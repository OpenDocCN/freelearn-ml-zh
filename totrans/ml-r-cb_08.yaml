- en: Chapter 8. Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying data with the bagging method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing cross-validation with the bagging method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying data with the boosting method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing cross-validation with the boosting method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying data with gradient boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating the margins of a classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating the error evolution of the ensemble method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying the data with random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the prediction errors of different classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble learning is a method to combine results produced by different learners
    into one format, with the aim of producing better classification results and regression
    results. In previous chapters, we discussed several classification methods. These
    methods take different approaches but they all have the same goal, that is, finding
    an optimum classification model. However, a single classifier may be imperfect,
    which may misclassify data in certain categories. As not all classifiers are imperfect,
    a better approach is to average the results by voting. In other words, if we average
    the prediction results of every classifier with the same input, we may create
    a superior model compared to using an individual method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In ensemble learning, bagging, boosting, and random forest are the three most
    common methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging is a voting method, which first uses Bootstrap to generate a different
    training set, and then uses the training set to make different base learners.
    The bagging method employs a combination of base learners to make a better prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting is similar to the bagging method. However, what makes boosting different
    is that it first constructs the base learning in sequence, where each successive
    learner is built for the prediction residuals of the preceding learner. With the
    means to create a complementary learner, it uses the mistakes made by previous
    learners to train the next base learner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest uses the classification results voted from many classification
    trees. The idea is simple; a single classification tree will obtain a single classification
    result with a single input vector. However, a random forest grows many classification
    trees, obtaining multiple results from a single input. Therefore, a random forest
    will use the majority of votes from all the decision trees to classify data or
    use an average output for regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following recipes, we will discuss how to use bagging and boosting to
    classify data. We can then perform cross-validation to estimate the error rate
    of each classifier. In addition to this, we'll introduce the use of a margin to
    measure the certainty of a model. Next, we cover random forests, similar to the
    bagging and boosting methods, and introduce how to train the model to classify
    data and use margins to estimate the model certainty. Lastly, we'll demonstrate
    how to estimate the error rate of each classifier, and use the error rate to compare
    the performance of different classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying data with the bagging method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `adabag` package implements both boosting and bagging methods. For the bagging
    method, the package implements Breiman's Bagging algorithm, which first generates
    multiple versions of classifiers, and then obtains an aggregated classifier. In
    this recipe, we will illustrate how to use the bagging method from `adabag` to
    generate a classification model using the telecom `churn` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we continue to use the telecom `churn` dataset as the input
    data source for the bagging method. For those who have not prepared the dataset,
    please refer to [Chapter 5](part0060_split_000.html#page "Chapter 5. Classification
    (I) – Tree, Lazy, and Probabilistic"), *Classification (I) – Tree, Lazy, and Probabilistic*,
    for detailed information.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to generate a classification model for the telecom
    `churn` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to install and load the `adabag` package (it might take a while
    to install `adabag`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can use the `bagging` function to train a training dataset (the result
    may vary during the training process):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Access the variable importance from the bagging result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After generating the classification model, you can use the predicted results
    from the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the predicted results, you can obtain a classification table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you can retrieve the average error of the bagging result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bagging is derived from the name Bootstrap aggregating, which is a stable,
    accurate, and easy to implement model for data classification and regression.
    The definition of bagging is as follows: given a training dataset of size *n*,
    bagging performs Bootstrap sampling and generates *m* new training sets, *Di*,
    each of size *n*. Finally, we can fit *m* Bootstrap samples to *m* models and
    combine the result by averaging the output (for regression) or voting (for classification):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An illustration of bagging method
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using bagging is that it is a powerful learning method, which
    is easy to understand and implement. However, the main drawback of this technique
    is that it is hard to analyze the result.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we use the boosting method from `adabag` to classify the telecom
    churn data. Similar to other classification methods discussed in previous chapters,
    you can train a boosting classifier with a formula and a training dataset. Additionally,
    you can set the number of iterations to 10 in the `mfinal` argument. Once the
    classification model is built, you can examine the importance of each attribute.
    Ranking the attributes by importance reveals that the number of customer service
    calls play a crucial role in the classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, with a fitted model, you can apply the `predict.bagging` function to predict
    the labels of the testing dataset. Therefore, you can use the labels of the testing
    dataset and predicted results to generate a classification table and obtain the
    average error, which is 0.045 in this example.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides `adabag`, the `ipred` package provides a bagging method for a classification
    tree. We demonstrate here how to use the bagging method of the `ipred` package
    to train a classification model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to install and load the `ipred` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then use the `bagging` method to fit the classification method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain an out of bag estimate of misclassification of the errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then use the `predict` function to obtain the predicted labels of the
    testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain the classification table from the labels of the testing dataset and
    prediction result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Performing cross-validation with the bagging method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To assess the prediction power of a classifier, you can run a cross-validation
    method to test the robustness of the classification model. In this recipe, we
    will introduce how to use `bagging.cv` to perform cross-validation with the bagging
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we continue to use the telecom `churn` dataset as the input
    data source to perform a k-fold cross-validation with the bagging method.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to retrieve the minimum estimation errors by performing
    cross-validation with the bagging method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we use `bagging.cv` to make a 10-fold classification on the training
    dataset with 10 iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then obtain the confusion matrix from the cross-validation results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can retrieve the minimum estimation errors from the cross-validation
    results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `adabag` package provides a function to perform the k-fold validation with
    either the bagging or boosting method. In this example, we use `bagging.cv` to
    make the k-fold cross-validation with the bagging method. We first perform a 10-fold
    cross validation with 10 iterations by specifying `v=10` and `mfinal=10`. Please
    note that this is quite time consuming due to the number of iterations. After
    the cross-validation process is complete, we can obtain the confusion matrix and
    average errors (0.058 in this case) from the cross-validation results.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For those interested in tuning the parameters of `bagging.cv`, please view
    the `bagging.cv` document by using the `help` function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Classifying data with the boosting method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to the bagging method, boosting starts with a simple or weak classifier
    and gradually improves it by reweighting the misclassified samples. Thus, the
    new classifier can learn from previous classifiers. The `adabag` package provides
    implementation of the **AdaBoost.M1** and **SAMME** algorithms. Therefore, one
    can use the boosting method in `adabag` to perform ensemble learning. In this
    recipe, we will use the boosting method in `adabag` to classify the telecom `churn`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue to use the telecom churn dataset as the input
    data source to perform classifications with the boosting method. Also, you need
    to have the `adabag` package loaded in R before commencing the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to classify the telecom `churn` dataset with the
    boosting method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the boosting function from the `adabag` package to train the classification
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then make a prediction based on the boosted model and testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can retrieve the classification table from the predicted results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you can obtain the average errors from the predicted results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea of boosting is to "boost" weak learners (for example, a single decision
    tree) into strong learners. Assuming that we have *n* points in our training dataset,
    we can assign a weight, *Wi* (0 <= i <n), for each point. Then, during the iterative
    learning process (we assume the number of iterations is *m*), we can reweigh each
    point in accordance with the classification result in each iteration. If the point
    is correctly classified, we should decrease the weight. Otherwise, we increase
    the weight of the point. When the iteration process is finished, we can then obtain
    the *m* fitted model, *f[i](x)* (0 <= i <n). Finally, we can obtain the final
    prediction through the weighted average of each tree''s prediction, where the
    weight, b, is based on the quality of each tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00133.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An illustration of boosting method
  prefs: []
  type: TYPE_NORMAL
- en: Both bagging and boosting are ensemble methods, which combine the prediction
    power of each single learner into a strong learner. The difference between bagging
    and boosting is that the bagging method combines independent models, but boosting
    performs an iterative process to reduce the errors of preceding models by predicting
    them with successive models.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to fit a classification model within the
    boosting method. Similar to bagging, one has to specify the formula and the training
    dataset used to train the classification model. In addition, one can specify parameters,
    such as the number of iterations (`mfinal`), the weight update coefficient (`coeflearn`),
    the weight of how each observation is used (`boos`), and the control for `rpart`
    (a single decision tree). In this recipe, we set the iteration to 10, using `Freund`
    (the AdaBoost.M1 algorithm implemented method) as `coeflearn`, `boos` set to false
    and max depth set to `3` for `rpart` configuration.
  prefs: []
  type: TYPE_NORMAL
- en: We use the boosting method to fit the classification model and then save it
    in `churn.boost`. We can then obtain predicted labels using the `prediction` function.
    Furthermore, we can use the `table` function to retrieve a classification table
    based on the predicted labels and testing the dataset labels. Lastly, we can get
    the average errors of the predicted results.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to using the boosting function in the `adabag` package, one can
    also use the `caret` package to perform a classification with the boosting method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, load the `mboost` and `pROC` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then set the training control with the `trainControl` function and use
    the `train` function to train the classification model with adaboost:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `summary` function to obtain the details of the classification model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `plot` function to plot the ROC curve within different iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![There''s more...](img/00134.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The repeated cross validation plot
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we can make predictions using the `predict` function and view the
    classification table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Performing cross-validation with the boosting method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to the `bagging` function, `adabag` provides a cross-validation function
    for the boosting method, named `boosting.cv`. In this recipe, we will demonstrate
    how to perform cross-validation using `boosting.cv` from the package, `adabag`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we continue to use the telecom `churn` dataset as the input
    data source to perform a k-fold cross-validation with the `boosting` method.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to retrieve the minimum estimation errors via cross-validation
    with the `boosting` method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can use `boosting.cv` to cross-validate the training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then obtain the confusion matrix from the boosting results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you can retrieve the average errors of the boosting method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to `bagging.cv`, we can perform cross-validation with the boosting method
    using `boosting.cv`. If `v` is set to `10` and `mfinal` is set to `5`, the `boosting`
    method will perform 10-fold cross-validations with five iterations. Also, one
    can set the control of the `rpart` fit within the parameter. We can set the complexity
    parameter to 0.01 in this example. Once the training is complete, the confusion
    matrix and average errors of the boosted results will be obtained.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For those who require more information on tuning the parameters of `boosting.cv`,
    please view the `boosting.cv` document by using the `help` function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Classifying data with gradient boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient boosting ensembles weak learners and creates a new base learner that
    maximally correlates with the negative gradient of the loss function. One may
    apply this method on either regression or classification problems, and it will
    perform well in different datasets. In this recipe, we will introduce how to use
    `gbm` to classify a telecom `churn` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we continue to use the telecom `churn` dataset as the input
    data source for the `bagging` method. For those who have not prepared the dataset,
    please refer to [Chapter 5](part0060_split_000.html#page "Chapter 5. Classification
    (I) – Tree, Lazy, and Probabilistic"), *Classification (I) – Tree, Lazy, and Probabilistic*,
    for detailed information.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to calculate and classify data with the gradient
    boosting method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install and load the package, `gbm`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `gbm` function only uses responses ranging from `0` to `1`; therefore,
    you should transform yes/no responses to numeric responses (0/1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can use the `gbm` function to train a training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can obtain the summary information from the fitted model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00135.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Relative influence plot of fitted model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can obtain the best iteration using cross-validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00136.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The performance measurement plot
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, you can retrieve the odd value of the log returned from the Bernoulli
    loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can plot the ROC curve and get the best cut off that will have the
    maximum accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00137.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The ROC curve of fitted model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can retrieve the best cut off with the `coords` function and use this cut
    off to obtain the predicted label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can obtain the classification table from the predicted results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The algorithm of gradient boosting involves, first, the process computes the
    deviation of residuals for each partition, and then, determines the best data
    partitioning in each stage. Next, the successive model will fit the residuals
    from the previous stage and build a new model to reduce the residual variance
    (an error). The reduction of the residual variance follows the functional gradient
    descent technique, in which it minimizes the residual variance by going down its
    derivative, as show here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00138.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Gradient descent method
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we use the gradient boosting method from `gbm` to classify the
    telecom churn dataset. To begin the classification, we first install and load
    the `gbm` package. Then, we use the `gbm` function to train the classification
    model. Here, as our prediction target is the `churn` attribute, which is a binary
    outcome, we therefore set the distribution as `bernoulli` in the `distribution`
    argument. Also, we set the 1000 trees to fit in the `n.tree` argument, the maximum
    depth of the variable interaction to `7` in `interaction.depth`, the learning
    rate of the step size reduction to 0.01 in `shrinkage`, and the number of cross-validations
    to `3` in `cv.folds`. After the model is fitted, we can use the summary function
    to obtain the relative influence information of each variable in the table and
    figure. The relative influence shows the reduction attributable to each variable
    in the sum of the square error. Here, we can find `total_day_minutes` is the most
    influential one in reducing the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use the `gbm.perf` function to find the optimum iteration. Here, we
    estimate the optimum number with cross-validation by specifying the `method` argument
    to `cv`. The function further generates two plots, where the black line plots
    the training error and the green one plots the validation error. The error measurement
    here is a `bernoulli` distribution, which we have defined earlier in the training
    stage. The blue dash line on the plot shows where the optimum iteration is.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we use the `predict` function to obtain the odd value of a log in each
    testing case returned from the Bernoulli loss function. In order to get the best
    prediction result, one can set the `n.trees` argument to an optimum iteration
    number. However, as the returned value is an odd value log, we still have to determine
    the best cut off to determine the label. Therefore, we use the `roc` function
    to generate an ROC curve and get the cut off with the maximum accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can use the function, `coords`, to retrieve the best cut off threshold
    and use the `ifelse` function to determine the class label from the odd value
    of the log. Now, we can use the `table` function to generate the classification
    table and see how accurate the classification model is.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to using the boosting function in the `gbm` package, one can also
    use the `mboost` package to perform classifications with the gradient boosting
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install and load the `mboost` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `mboost` function only uses numeric responses; therefore, you should transform
    yes/no responses to numeric responses (0/1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Also, you should remove nonnumerical attributes, such as `voice_mail_plan`
    and `international_plan`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then use `mboost` to train the classification model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `summary` function to obtain the details of the classification model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, use the `plot` function to draw a partial contribution plot of each
    attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![There''s more...](img/00139.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The partial contribution plot of important attributes
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Calculating the margins of a classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A margin is a measure of the certainty of classification. This method calculates
    the difference between the support of a correct class and the maximum support
    of an incorrect class. In this recipe, we will demonstrate how to calculate the
    margins of the generated classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have completed the previous recipe by storing a fitted bagging model
    in the variables, `churn.bagging` and `churn.predbagging`. Also, put the fitted
    boosting classifier in both `churn.boost` and `churn.boost.pred`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to calculate the margin of each ensemble learner:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, use the `margins` function to calculate the margins of the boosting
    classifiers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then use the `plot` function to plot a marginal cumulative distribution
    graph of the boosting classifiers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00140.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The margin cumulative distribution graph of using the boosting method
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can then calculate the percentage of negative margin matches training errors
    and the percentage of negative margin matches test errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Also, you can calculate the margins of the bagging classifiers. You might see
    the warning message showing "`no non-missing argument to min`". The message simply
    indicates that the min/max function is applied to the numeric of the 0 length
    argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then use the `plot` function to plot a margin cumulative distribution
    graph of the bagging classifiers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00141.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The margin cumulative distribution graph of the bagging method
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, you can then compute the percentage of negative margin matches training
    errors and the percentage of negative margin matches test errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A margin is the measurement of certainty of the classification; it is computed
    by the support of the correct class and the maximum support of the incorrect class.
    The formula of margins can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00142.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the margin of the xi sample equals the support of a correctly classified
    sample (*c* denotes the correct class) minus the maximum support of a sample that
    is classified to class *j* (where *j≠c* and *j=1…k*). Therefore, correctly classified
    examples will have positive margins and misclassified examples will have negative
    margins. If the margin value is close to one, it means that correctly classified
    examples have a high degree of confidence. On the other hand, examples of uncertain
    classifications will have small margins.
  prefs: []
  type: TYPE_NORMAL
- en: The `margins` function calculates the margins of AdaBoost.M1, AdaBoost-SAMME,
    or the bagging classifier, which returns a vector of a margin. To visualize the
    margin distribution, one can use a margin cumulative distribution graph. In these
    graphs, the x-axis shows the margin and the y-axis shows the percentage of observations
    where the margin is less than or equal to the margin value of the x-axis. If every
    observation is correctly classified, the graph will show a vertical line at the
    margin equal to 1 (where margin = 1).
  prefs: []
  type: TYPE_NORMAL
- en: For the margin cumulative distribution graph of the boosting classifiers, we
    can see that there are two lines plotted on the graph, in which the green line
    denotes the margin of the testing dataset, and the blue line denotes the margin
    of the training set. The figure shows about 6.39 percent of negative margins match
    the training error, and 6.26 percent of negative margins match the test error.
    On the other hand, we can find that 17.33% of negative margins match the training
    error and 4.3 percent of negative margins match the test error in the margin cumulative
    distribution graph of the bagging classifiers. Normally, the percentage of negative
    margins matching the training error should be close to the percentage of negative
    margins that match the test error. As a result of this, we should then examine
    the reason why the percentage of negative margins that match the training error
    is much higher than the negative margins that match the test error.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are interested in more details on margin distribution graphs, please
    refer to the following source: *Kuncheva LI (2004)*, *Combining Pattern Classifiers:
    Methods and Algorithms*, *John Wiley & Sons*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating the error evolution of the ensemble method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `adabag` package provides the `errorevol` function for a user to estimate
    the ensemble method errors in accordance with the number of iterations. In this
    recipe, we will demonstrate how to use `errorevol` to show the evolution of errors
    of each ensemble classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have completed the previous recipe by storing the fitted bagging
    model in the variable, `churn.bagging`. Also, put the fitted boosting classifier
    in `churn.boost`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to calculate the error evolution of each ensemble
    learner:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, use the `errorevol` function to calculate the error evolution of the
    boosting classifiers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00143.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Boosting error versus number of trees
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, use the `errorevol` function to calculate the error evolution of the
    bagging classifiers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00144.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Bagging error versus number of trees
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `errorest` function calculates the error evolution of AdaBoost.M1, AdaBoost-SAMME,
    or the bagging classifiers and returns a vector of error evolutions. In this recipe,
    we use the boosting and bagging models to generate error evolution vectors and
    graph the error versus number of trees.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting graph reveals the error rate of each iteration. The trend of the
    error rate can help measure how fast the errors reduce, while the number of iterations
    increases. In addition to this, the graphs may show whether the model is over-fitted.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If the ensemble model is over-fitted, you can use the `predict.bagging` and
    `predict.boosting` functions to prune the ensemble model. For more information,
    please use the help function to refer to `predict.bagging` and `predict.boosting`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Classifying data with random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forest is another useful ensemble learning method that grows multiple
    decision trees during the training process. Each decision tree will output its
    own prediction results corresponding to the input. The forest will use the voting
    mechanism to select the most voted class as the prediction result. In this recipe,
    we will illustrate how to classify data using the `randomForest` package.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue to use the telecom `churn` dataset as the input
    data source to perform classifications with the random forest method.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to classify data with random forest:'
  prefs: []
  type: TYPE_NORMAL
- en: First, you have to install and load the `randomForest` package;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then fit the random forest classifier with a training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, make predictions based on the fitted model and testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similar to other classification methods, you can obtain the classification
    table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can use the `plot` function to plot the mean square error of the forest
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00145.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The mean square error of the random forest
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can then examine the importance of each attribute within the fitted classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can use the `varImpPlot` function to obtain the plot of variable
    importance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00146.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The visualization of variable importance
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can also use the `margin` function to calculate the margins and plot the
    margin cumulative distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00147.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The margin cumulative distribution graph for the random forest method
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Furthermore, you can use a histogram to visualize the margin distribution of
    the random forest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00148.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The histogram of margin distribution
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can also use `boxplot` to visualize the margins of the random forest by
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00149.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Margins of the random forest by class
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The purpose of random forest is to ensemble weak learners (for example, a single
    decision tree) into a strong learner. The process of developing a random forest
    is very similar to the bagging method, assuming that we have a training set containing
    *N* samples with *M* features. The process first performs bootstrap sampling,
    which samples *N* cases at random, with the replacement as the training dataset
    of each single decision tree. Next, in each node, the process first randomly selects
    *m* variables (where *m << M*), then finds the predictor variable that provides
    the best split among m variables. Next, the process grows the full tree without
    pruning. In the end, we can obtain the predicted result of an example from each
    single tree. As a result, we can get the prediction result by taking an average
    or weighted average (for regression) of an output or taking a majority vote (for
    classification):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A random forest uses two parameters: **ntree** (the number of trees) and **mtry**
    (the number of features used to find the best feature), while the bagging method
    only uses ntree as a parameter. Therefore, if we set mtry equal to the number
    of features within a training dataset, then the random forest is equal to the
    bagging method.'
  prefs: []
  type: TYPE_NORMAL
- en: The main advantages of random forest are that it is easy to compute, it can
    efficiently process data, and is fault tolerant to missing or unbalanced data.
    The main disadvantage of random forest is that it cannot predict the value beyond
    the range of a training dataset. Also, it is prone to over-fitting of noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we employ the random forest method adapted from the `randomForest`
    package to fit a classification model. First, we install and load `randomForest`
    into an R session. We then use the random forest method to train a classification
    model. We set `importance = T`, which will ensure that the importance of the predictor
    is assessed.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the bagging and boosting methods, once the model is fitted, one can
    perform predictions using a fitted model on the testing dataset, and furthermore,
    obtain the classification table.
  prefs: []
  type: TYPE_NORMAL
- en: In order to assess the importance of each attribute, the `randomForest` package
    provides the importance and `varImpPlot` functions to either list the importance
    of each attribute in the fitted model or visualize the importance using either
    mean decrease accuracy or mean decrease `gini`.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to `adabag`, which contains a method to calculate the margins of the
    bagging and boosting methods, `randomForest` provides the `margin` function to
    calculate the margins of the forest object. With the `plot`, `hist`, and `boxplot`
    functions, you can visualize the margins in different aspects to the proportion
    of correctly classified observations.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apart from the `randomForest` package, the `party` package also provides an
    implementation of random forest. In the following steps, we illustrate how to
    use the `cforest` function within the `party` package to perform classifications:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install and load the `party` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then use the `cforest` function to fit the classification model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can make predictions based on the built model and the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, obtain the classification table from the predicted labels and the
    labels of the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Estimating the prediction errors of different classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of this chapter, we discussed why we use ensemble learning
    and how it can improve the prediction performance compared to using just a single
    classifier. We now validate whether the ensemble model performs better than a
    single decision tree by comparing the performance of each method. In order to
    compare the different classifiers, we can perform a 10-fold cross-validation on
    each classification method to estimate test errors using `erroreset` from the
    `ipred` package.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue to use the telecom `churn` dataset as the input
    data source to estimate the prediction errors of the different classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to estimate the prediction errors of each classification
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can estimate the error rate of the bagging model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then estimate the error rate of the boosting method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, estimate the error rate of the random forest model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, make a prediction function using `churn.predict`, and then use the
    function to estimate the error rate of the single decision tree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we estimate the error rates of four different classifiers using
    the `errorest` function from the `ipred` package. We compare the boosting, bagging,
    and random forest methods, and the single decision tree classifier. The `errorest`
    function performs a 10-fold cross-validation on each classifier and calculates
    the misclassification error. The estimation results from the four chosen models
    reveal that the boosting method performs the best with the lowest error rate (0.0475).
    The random forest method has the second lowest error rate (0.051), while the bagging
    method has an error rate of 0.0583\. The single decision tree classifier, `rpart`,
    performs the worst among the four methods with an error rate equal to 0.0674\.
    These results show that all three ensemble learning methods, boosting, bagging,
    and random forest, outperform a single decision tree classifier.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe we mentioned the `ada` package, which contains a method to perform
    stochastic boosting. For those interested in this package, please refer to: *Additive
    Logistic Regression: A Statistical View of Boosting by Friedman*, *et al. (2000)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
