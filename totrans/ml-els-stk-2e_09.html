<html><head></head><body>
		<div id="_idContainer171">
			<h1 id="_idParaDest-129"><em class="italic"><a id="_idTextAnchor131"/>Chapter 7</em>: AIOps and Root Cause Analysis</h1>
			<p>Up until this point, we have extensively explained the value of detecting anomalies across metrics and logs separately. This is extremely valuable, of course. In some cases, however, the knowledge that a particular metric or log file has gone awry may not tell the whole story of what is going on. It may, for example, be pointing to a symptom and not the cause of the problem. To have a better understanding of the full scope of an emerging problem, it is often helpful to look holistically at many aspects of a system or situation. This involves smartly analyzing multiple kinds of related datasets together.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Demystifying the term ''AIOps''</li>
				<li>Understanding the importance and limitations of KPIs</li>
				<li>Moving beyond KPIs</li>
				<li>Organizing data for better analysis</li>
				<li>Leveraging the contextual information</li>
				<li>Bringing it all together for RCA</li>
			</ul>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor132"/>Technical requirements</h1>
			<p>The information and examples demonstrated in this chapter are relevant as of v7.11 of the Elastic Stack and utilize sample datasets from the GitHub repo found at <a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition</a>. </p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor133"/>Demystifying the term ''AIOps''</h1>
			<p>We learned in <a href="B17040_01_Epub_AM.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning for IT</em>, that many <a id="_idIndexMarker456"/>companies are drowning in an ever-increasing cascade of IT data while simultaneously being asked to ''do more with less'' (fewer people, fewer costs, and so on). Some of that data is collected and/or stored in specialized tools, but some may be collected in general-purpose data platforms such as the Elastic Stack. But the question still remains: what percentage of that data is being paid attention to? By this, we mean the percentage of collected data that is actively inspected by humans or being watched by some type of automated means (defined alarms based on rules, thresholds, and so on). Even generous estimates might put the percentage in the range of single digits. So, with 90% or more data being collected going unwatched, what's being missed? The proper answer might be that we don't actually know.</p>
			<p>Before we admonish IT organizations for the sin of collecting piles of data but not watching it, we need to understand the magnitude of the challenge associated with such an operation. A typical user-facing application may do the following:</p>
			<ul>
				<li>Span hundreds of physical servers</li>
				<li>Have dozens (if not hundreds) of microservices, each of which may have dozens or hundreds of operational metrics or log entries that describe its operation</li>
			</ul>
			<p>The combinatorics of this <a id="_idIndexMarker457"/>can easily rise to a six- or seven-figure range of unique measurement points. Additionally, there may be dozens or even hundreds of such applications under the umbrella of management by the IT organization. It's no wonder that the amount of data being collected by these systems per day can easily be measured in terabytes.</p>
			<p>So, it is quite natural that the desired solution could involve a combination of automation and artificial intelligence to lessen the burden on human analysts. Some clever marketing person somewhere figured out that coining the term ''AIOps'' encapsulated a projected solution to the problem – augment what humans can't (or don't have the time or capacity to do manually) with some amount of intelligent automation. Now, what an AIOps solution actually does to accomplish that goal is often left to a discerning user to interpret.</p>
			<p>So, let's demystify the term by not focusing on the term itself (let's leave that to the marketing folks), but rather articulating the kinds of things we would want to have this intelligent technology do to help us in our situation:</p>
			<ul>
				<li>Autonomously inspect data and assess its relevance, importance, and notability based upon an automatically learned set of constraints, rules, and behavior.</li>
				<li>Filter out the noise of irrelevant behaviors so as to not distract human analysts from the things that actually matter.</li>
				<li>Obtain a certain amount of proactive early warnings regarding problems that may be brewing but have not necessarily caused an outage yet.</li>
				<li>Automatically gather related/correlated evidence <a id="_idIndexMarker458"/>around a problem to assist with <strong class="bold">Root</strong> <strong class="bold">Cause</strong> <strong class="bold">Analysis</strong> (<strong class="bold">RCA</strong>).</li>
				<li>Uncover operational inefficiencies in order to maximize infrastructure performance.</li>
				<li>Suggest an action or next step for remediation, based upon past remediations and their effectiveness.</li>
			</ul>
			<p>While this list is in <a id="_idIndexMarker459"/>no way comprehensive, we can see the gist of what we're getting at here – which is that intelligent automation and analysis can pay big dividends and allow IT departments to drive efficiencies and thus maximize business outcomes.</p>
			<p>Except for the suggested remediations mentioned in number six in the preceding list (at least at this moment), Elastic <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) can very <a id="_idIndexMarker460"/>much be an important part of all the other goals on this list. We've seen already how Elastic ML can automatically find anomalous behavior, forecast trends, proactively alert, and so on. But we must also recognize that Elastic ML is a generic ML platform – it is not purpose-built for IT operations/observability or security analytics. As such, there still needs to be an orientation of how Elastic ML is used in the context of operations, and that will be discussed throughout this chapter.</p>
			<p>It is also important to note that there are still a large number of IT operation groups that currently use no intelligent automation and analysis. They often claim that they would like to employ an AI-based approach to improve their current situation, but that they are not quite ready to take the plunge. So, let's challenge the notion that the only way to benefit from AI is to do every single thing that is possible on day 1. Let's instead build up some practical applications of Elastic ML in the context of IT operations and how it can be used to satisfy most of the goals articulated in the preceding list.</p>
			<p>We will first start with the <a id="_idIndexMarker461"/>notion of the <strong class="bold">Key</strong> <strong class="bold">Performance</strong> <strong class="bold">Indicator</strong> (<strong class="bold">KPI</strong>) and why it is the logical choice for the best place to get started with Elastic ML.</p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor134"/>Understanding the importance and limitations of KPIs</h1>
			<p>Because of the problem of scale and the desire to make some amount of progress in making the collected data actionable, it is natural that some of the first metrics to be tackled for active inspection are those that are the best indicators of performance or operation. The KPIs that an <a id="_idIndexMarker462"/>IT organization chooses for measurement, tracking, and flagging can span diverse indicators, including the following:</p>
			<ul>
				<li><strong class="bold">Customer experience</strong>: These metrics measure customer experience, such as application response times or error rates.</li>
				<li><strong class="bold">Availability</strong>: Metrics such as <a id="_idIndexMarker463"/>uptime or <strong class="bold">Mean</strong> <strong class="bold">Time</strong> <strong class="bold">to</strong> <strong class="bold">Repair</strong> (<strong class="bold">MTTR</strong>) are often important to track.</li>
				<li><strong class="bold">Business</strong>: Here we may have metrics that directly measure business performance, such as orders per minute or number of active users.</li>
			</ul>
			<p>As such, these types of metrics are usually displayed, front and center, on most high-level operational dashboards or on staff reports for employees ranging from technicians to executives. A quick Google image search for a KPI dashboard will return countless examples of charts, gauges, dials, maps, and other eye candy.</p>
			<p>While there is <a id="_idIndexMarker464"/>great value in such displays of information that can be consumed with a mere glance, there are still fundamental challenges with manual inspection:</p>
			<ul>
				<li><strong class="bold">Interpretation</strong>: There may be difficulty in understanding the difference between normal operation and abnormal, unless that difference is already intrinsically understood by the human.</li>
				<li><strong class="bold">Challenges of scale</strong>: Despite the fact that KPIs are already a distillation of all metrics down to a set of important ones, there still may be more KPIs to display than is feasible given the real estate of the screen that the dashboard is displayed upon. The end result may be crowded visualizations or lengthy dashboards that require scrolling/paging.</li>
				<li><strong class="bold">Lack of proactivity</strong>: Many dashboards such as this do not have their metrics also tied to alerts, thus requiring constant <a id="_idIndexMarker465"/>supervision if it's proactively known that a KPI that is faltering is important.</li>
				<li>The bottom line is that KPIs are an extremely important step in the process of identifying and tracking meaningful indicators of the health and behavior of an IT system. However, it should be obvious that the mere act of identifying and tracking a set of KPIs with a visual-only paradigm is going to leave some significant deficiencies in the strategy of a successful IT operations plan.</li>
			</ul>
			<p>It should be obvious <a id="_idIndexMarker466"/>that KPIs are a great candidate for metrics that can be tracked with Elastic ML's anomaly detection. For example, say we have some data that looks like the following (from the <strong class="source-inline">it_ops_kpi</strong> sample dataset in the GitHub repo):</p>
			<p class="source-code">      {</p>
			<p class="source-code">        ''_index'' : ''it_ops_kpi'',</p>
			<p class="source-code">        ''_type'' : ''_doc'',</p>
			<p class="source-code">        ''_id'' : ''UqUsMngBFOh8A28xK-E3'',</p>
			<p class="source-code">        ''_score'' : 1.0,</p>
			<p class="source-code">        ''_source'' : {</p>
			<p class="source-code">          ''@timestamp'' : ''2021-01-29T05:36:09.000Z'',</p>
			<p class="source-code">          ''events_per_min'' : 28,</p>
			<p class="source-code">          ''kpi_indicator'' : ''online_purchases''</p>
			<p class="source-code">        }</p>
			<p class="source-code">      },</p>
			<p>In this case, the KPI (the field called <strong class="source-inline">events_per_min</strong>) represents the summarized total number of purchases per minute for some online transaction processing system. We could easily track this KPI over time with an anomaly detection job with a <strong class="source-inline">sum</strong> function on the <strong class="source-inline">events_per_min</strong> field and a bucket span of 15 minutes. An unexpected dip in online sales (to a value of <strong class="source-inline">921</strong>) is detected and flagged as anomalous:</p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B17040_07_1.jpg" alt="Figure 7.1 – A KPI being analyzed with a typical anomaly detection job&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – A KPI being analyzed with a typical anomaly detection job</p>
			<p>In this case, the <a id="_idIndexMarker467"/>KPI is just a single, overall metric. If there was another categorical field in the data that allowed it to be segmented (for example, sales by product ID, product category, geographical region, and so on), then ML could easily split the analysis along that field to expand the analysis in a parallel fashion (as we saw in <a href="B17040_03_Epub_AM.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a>, <em class="italic">Anomaly Detection</em>). But let's not lose sight of what we're accomplishing here: a proactive analysis of a key metric that someone likely cares about. The number of online sales per unit of time is directly tied to incoming revenue and thus is an obvious KPI.</p>
			<p>However, despite the importance of knowing that something unusual is happening with our KPI, there is still no insight as to <em class="italic">why</em> it is happening. Is there an operational problem with one of the backend systems that supports this customer-facing application? Was there a user interface coding error in the latest release that makes it harder for users to complete the transaction? Is there a problem with the third-party payment processing provider that is relied upon? None of these questions can be answered by merely scrutinizing the KPI.</p>
			<p>To get that kind of insight, we will need to broaden our analysis to include other sets of relevant and related information.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor135"/>Moving beyond KPIs</h1>
			<p>The process of selecting KPIs, in general, should be relatively easy, as it is likely obvious what metrics are the best indicators (if online sales are down, then the application is likely not working). But if we want to get a more holistic view of what may be contributing to an operational problem, we must expand our analysis beyond the KPIs to indicators that emanate from the underlying systems and technology that support the application.</p>
			<p>Fortunately, there are a plethora of ways to collect all <a id="_idIndexMarker468"/>kinds of data for centralization in the Elastic Stack. The <strong class="bold">Elastic Agent</strong>, for example, is a <a id="_idIndexMarker469"/>single, unified agent that you can deploy to hosts or containers to collect data and send it to the Elastic Stack. Behind the scenes, the Elastic Agent runs the Beats shippers or Elastic Endpoint required for your configuration. Starting from version 7.11, the Elastic Agent is managed in Kibana <a id="_idIndexMarker470"/>in the <strong class="bold">Fleet</strong> user interface and can be used to add and manage integrations for popular services and platforms:</p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B17040_07_2.jpg" alt="Figure 7.2 – The Integrations section of the Fleet user interface in Kibana&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – The Integrations section of the Fleet user interface in Kibana</p>
			<p>Using these different integrations, the user <a id="_idIndexMarker471"/>can easily collect data and centralize it in the Elastic Stack. While this chapter is not meant to be a tutorial <a id="_idIndexMarker472"/>on Fleet and the Elastic Agent, the <a id="_idIndexMarker473"/>important point is that regardless of what tools you use to gather the underlying application and system data, one thing is likely true: there will be a lot of data when all is said and done. Remember that our ultimate goal is to proactively and holistically pay attention to a larger percentage of the overall dataset. To do that, we must first organize this data so that we can effectively analyze it with Elastic ML.</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor136"/>Organizing data for better analysis</h1>
			<p>One of the <a id="_idIndexMarker474"/>nicest things about ingesting data via the Elastic Agent is that by default, the data collected is normalized <a id="_idIndexMarker475"/>using the <strong class="bold">Elastic Common Schema</strong> (<strong class="bold">ECS</strong>). ECS is an open source specification that defines a common taxonomy and naming conventions across data that is stored in the Elastic Stack. As such, the data becomes easier to manage, analyze, visualize, and correlate across disparate data types – including across both performance metrics and log files.</p>
			<p>Even if you are not using the Elastic Agent or other legacy Elastic ingest tools (such as Beats and Logstash) and are instead relying on other, third-party data collection or ingest pipelines, it is still recommended that you conform your data to ECS because it will pay big dividends when users expect to use this data for queries, dashboards, and, of course, ML jobs.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">More information on ECS can be found in the <a id="_idIndexMarker476"/>reference section of the website at <a href="https://www.elastic.co/guide/en/ecs/current/ecs-reference.html">https://www.elastic.co/guide/en/ecs/current/ecs-reference.html</a>.</p>
			<p>Among many of the <a id="_idIndexMarker477"/>important fields within ECS is the <strong class="source-inline">host.name</strong> field, which defines which host the data was collected from. By default, most data collection strategies in the Elastic Stack involve putting data in indices that are oriented around the data type, and thus potentially contain interleaved documents from many different hosts. Perhaps some of our hosts in our environment support one application (that is, online purchases), but other hosts support a different application (such as invoice processing). With all hosts reporting their data into a single index, if we are interested in orienting our reporting and analysis of the data for one or both applications, it is obviously inappropriate to orient the analysis based solely on the index – we will need our analysis to be application-centric. </p>
			<p>In order to accomplish this, we have a few options:</p>
			<ul>
				<li>Modifying the base query of the anomaly detection job so that it filters the data for only the hosts associated with the application of interest</li>
				<li>Modifying the data on ingest to enrich it, to insert additional contextual information into each document, which will later be used to filter the query made by the anomaly detection job</li>
			</ul>
			<p>Both require customization of the datafeed query that the anomaly detection job makes to the raw data in the source indices. The first option may result in a relatively complex query and the second option requires an interstitial step of data enrichment using custom ingest pipelines. Let's briefly discuss each.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor137"/>Custom queries for anomaly detection datafeeds</h2>
			<p>When a new job is <a id="_idIndexMarker478"/>created in the anomaly detection UI, the first step is to choose either an index pattern or a Kibana saved search. If the former is chosen, then a <strong class="source-inline">{''match_all'':{}}</strong> Elasticsearch query (return every record in the index) is invoked. If the job is created via the API or the advanced job wizard, then the user can specify just about any valid Elasticsearch DSL for filtering the data. Free-form composing Elasticsearch DSL can be a little error-prone for non-expert users. Therefore, a more intuitive way would be to approach this from Kibana via saved searches.</p>
			<p>For example, let's say that we have an index of log files and the appropriate hosts associated with the application we would like to monitor and analyze consist of two servers, <strong class="source-inline">esxserver1.acme.com</strong> and <strong class="source-inline">esxserver2.acme.com</strong>. On Kibana's <strong class="bold">Discover</strong> page, we can build a filtered query using <strong class="bold">KQL</strong> using the search box at the top of the user interface:</p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B17040_07_3.jpg" alt="Figure 7.3 – Building a filtered query using KQL&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – Building a filtered query using KQL</p>
			<p>The text of this KQL query would be as follows:</p>
			<p class="source-code">physicalhost_name:''esxserver1.acme.com'' or physicalhost_name:''esxserver2.acme.com''</p>
			<p>If you were <a id="_idIndexMarker479"/>curious about the actual Elasticsearch DSL that is invoked by Kibana to get this filtered query, you could click the <strong class="bold">Inspect</strong> button in the top right and select the <strong class="bold">Request</strong> tab to see the Elasticsearch DSL:</p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B17040_07_4.jpg" alt="Figure 7.4 – Inspecting the Elasticsearch DSL that runs for the KQL filter&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Inspecting the Elasticsearch DSL that runs for the KQL filter</p>
			<p>It is probably worth noting that despite the way the KQL query gets translated to Elasticsearch DSL in this specific example (using <strong class="source-inline">match_phrase</strong>, for example), it is not the only way to achieve the desired results. A query filter using <strong class="source-inline">terms</strong> is yet another way, but assessing the merits of one over the other is beyond the scope of this book.</p>
			<p>Regardless of the <a id="_idIndexMarker480"/>Elasticsearch DSL that runs behind the scenes, the key thing is that we have a query that filters the raw data to identify only the servers of interest for the application we would like to analyze with Elastic ML. To keep this filtered search, a click of the <strong class="bold">Save</strong> button in the top right and naming the search is necessary:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B17040_07_5.jpg" alt="Figure 7.5 – Saving the search for later use in Elastic ML&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – Saving the search for later use in Elastic ML</p>
			<p>Later on, you could then select this saved search when configuring a new anomaly detection job:</p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/B17040_07_6.jpg" alt="Figure 7.6 – Leveraging a saved search in an anomaly detection job&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Leveraging a saved search in an anomaly detection job</p>
			<p>As such, our ML job <a id="_idIndexMarker481"/>will now only run for the hosts of interest for this specific application. Thus, we have been able to effectively limit and segment the data analysis to the hosts that we've defined to have made a contribution to this application.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor138"/>Data enrichment on ingest</h2>
			<p>Another option is to <a id="_idIndexMarker482"/>move the decision-making about which hosts belong to which applications further upstream to the time of ingest. If Logstash was part of the ingest pipeline, you could use a filter plugin to add additional fields to the data based upon a lookup against an asset list (file, database, and so on). Consult the Logstash documentation at <a href="https://www.elastic.co/guide/en/logstash/current/lookup-enrichment.html">https://www.elastic.co/guide/en/logstash/current/lookup-enrichment.html</a>, which shows you how to dynamically enrich the indexed documents with additional fields to provide context. If you were not using Logstash (merely using Beats/Elastic Agent and the ingest node), perhaps a simpler way would be to use the enrich processor instead. Consult the documentation at <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-enriching-data.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-enriching-data.html</a>.</p>
			<p>For example, you could have this enrichment add an <strong class="source-inline">application_name</strong> field and dynamically populate the value of this field with the appropriate name of the application, such as the following (truncated JSON here):</p>
			<p class="source-code"><strong class="bold">      </strong><strong class="bold">''host'': ''wasinv2.acme.com'',</strong></p>
			<p class="source-code"><strong class="bold">      ''application_name'': ''invoice_processing'',</strong></p>
			<p>Or you could have the following:</p>
			<p class="source-code"><strong class="bold">      ''host'': ''www3.acme.com'',</strong></p>
			<p class="source-code"><strong class="bold">      ''application_name'': ''online_purchases'',</strong></p>
			<p>Once the value of this <a id="_idIndexMarker483"/>field is set and inserted into the indexed documents, then you would use the <strong class="source-inline">application_name</strong> field, along with the ability to filter the query for the anomaly detection job (as previously described), to limit your data analysis to the pertinent application of interest. The addition of the data enrichment step may seem like a little more up-front effort, but it should pay dividends in the long term as it will be easier to maintain as asset names change or evolve, since the first method requires hardcoding the asset names into the searches of the ML jobs.</p>
			<p>Now that we have organized our data and perhaps even enriched it, let's now see how we can leverage that contextual information to make our anomaly detection jobs more effective.</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor139"/>Leveraging the contextual information</h1>
			<p>With our <a id="_idIndexMarker484"/>data organized and/or enriched, the two primary ways we can leverage contextual information is via analysis <strong class="bold">splits</strong> and statistical <strong class="bold">influencers</strong>.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor140"/>Analysis splits</h2>
			<p>We have <a id="_idIndexMarker485"/>already seen that an <strong class="bold">anomaly</strong> <strong class="bold">detection</strong> job can be split based on any categorical field. As such, we can individually model behavior separately for <a id="_idIndexMarker486"/>each instance of that field. This could be extremely valuable, especially in a case where each instance needs its own separate model.</p>
			<p>Take, for example, the case where we have data for different regions of the world: </p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/B17040_07_7.jpg" alt="Figure 7.7 – Differing data behaviors based on region&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Differing data behaviors based on region</p>
			<p>Whatever data this is (sales KPIs, utilization metrics, and so on), clearly it has very distinctive <a id="_idIndexMarker487"/>patterns that are unique to each region. In this case, it makes sense to split any analysis we do with anomaly detection for each region to capitalize on <a id="_idIndexMarker488"/>this uniqueness. We would be able to detect anomalies in the behavior that are specific to each region.</p>
			<p>Let's also imagine that, within each region, a fleet of servers support the application and transaction processing, but they are load-balanced and contribute equally to the performance/operation. In that way, there's nothing unique about each server's contribution to a region. As such, it probably doesn't make sense to split the analysis per server.</p>
			<p>We've naturally come to the conclusion that splitting by region is more effective than splitting by server. But what if a particular server within a region is having problems contributing to the <a id="_idIndexMarker489"/>anomalies that are being detected? Wouldn't we <a id="_idIndexMarker490"/>want to have this information available immediately, instead of having to manually diagnose further? This is possible to know via influencers.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor141"/>Statistical influencers</h2>
			<p>We introduced the concept of <a id="_idIndexMarker491"/>influencers in <a href="B17040_05_Epub_AM.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>, <em class="italic">Interpreting Results</em>. As a reminder, an influencer is a field that describes an entity where you would like to know whether it ''influences'' (is to blame for) the existence of the <a id="_idIndexMarker492"/>anomaly or at least had a significant contribution. Remember that any field chosen as a candidate to be an influencer doesn't need to be part of the detection logic, although it is natural to pick fields that are used as splits to also be influencers. It is also important that influencers are chosen when the anomaly detection jobs are created as they cannot be added to the configuration later.</p>
			<p>It is also key to understand that the process of finding potential influencers happens after the anomaly detection job finds the anomaly. In other words, it does not affect any of the probability calculations that are made as part of the detection. Once the anomaly has been determined, ML will systematically go through all instances of each candidate influencer field and remove that instance's contribution to the data in that time bucket. If, once removed, the remaining data is no longer anomalous, then via counterfactual reasoning, that instance's contribution must have been influential and is scored accordingly (with an <strong class="source-inline">influencer_score</strong> value in the results).</p>
			<p>What we will see in the next section, however, is how influencers can be leveraged when viewing the results of not just a single anomaly detection job, but potentially several related jobs. Let's now move on to discuss the process of grouping and viewing jobs together to assist with RCA.</p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor142"/>Bringing it all together for RCA</h1>
			<p>We are at the <a id="_idIndexMarker493"/>point now where we can now discuss how we can bring everything together. In our desire to increase our effectiveness in IT operations and look more holistically at application health, we now need to operationalize what we've prepared in the prior sections and configure our anomaly detection jobs accordingly. To that end, let's work through a real-life scenario in which Elastic ML <a id="_idIndexMarker494"/>helped us get to the root cause of an operational problem.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor143"/>Outage background</h2>
			<p>This scenario is loosely based on a real application outage, although the data has been somewhat <a id="_idIndexMarker495"/>simplified and sanitized to obfuscate the original customer. The problem was with a retail application that processed gift card transactions. Occasionally, the app would stop working and transactions could not be processed. This would only be discovered when individual stores called headquarters to complain. The root cause of the issue was unknown and couldn't be ascertained easily by the customer. Because they never got to the root cause, and because the problem could be fixed by simply rebooting the application servers, the problem would randomly reoccur and plagued them for months.</p>
			<p>The following data was collected and included in the analysis to help understand the origins of the problem. This data included the following (and is supplied in the GitHub repo):</p>
			<ul>
				<li>A summarized (1-minute) count of transaction volume (the main KPI)</li>
				<li>Application logs (semi-structured text-based messages) from the transaction processing engine</li>
				<li>SQL Server performance metrics from the database that backed the transaction processing engine</li>
				<li>Network utilization performance metrics from the network the transaction processing engine operates on</li>
			</ul>
			<p>As such, four ML jobs were configured against the data. They were as follows:</p>
			<ul>
				<li><strong class="bold">it_ops_kpi</strong>: Using <strong class="source-inline">sum</strong> on the number of transactions processed per minute</li>
				<li><strong class="bold">it_ops_logs</strong>: Using <strong class="source-inline">count</strong> by the <strong class="source-inline">mlcategory</strong> detector to count the number of log messages by type, but using dynamic ML-based categorization to delineate different message types</li>
				<li><strong class="bold">it_ops_sql</strong>: Simple <strong class="source-inline">mean</strong> analysis of every SQL Server metric in the index </li>
				<li><strong class="bold">it_ops_network</strong>: Simple <strong class="source-inline">mean</strong> analysis of every network performance metric in the index</li>
			</ul>
			<p>These four jobs were <a id="_idIndexMarker496"/>configured and run on the data when the problem occurred in the application. Anomalies were found, especially in the KPI that tracked the number of transactions being processed. In fact, this is the same KPI that we saw at the beginning of this chapter, where an unexpected dip in order processing was the main indicator that a problem was occurring: </p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/B17040_07_8.jpg" alt="Figure 7.8 – The KPI of the number of transactions processed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – The KPI of the number of transactions processed</p>
			<p>However, the root cause wasn't understood until this KPI's anomaly was correlated with the anomalies in the other three ML jobs that were looking at the data in the underlying technology and infrastructure. Let's see how the power of visual correlation and shared influencers allowed the underlying cause to be discovered.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor144"/>Correlation and shared influencers</h2>
			<p>In addition to the <a id="_idIndexMarker497"/>anomaly in the transactions processed KPI (in which an unexpected dip occurs), the other three anomaly <a id="_idIndexMarker498"/>detection jobs (for the network metrics, the application logs, and the SQL database metrics) were superimposed onto the same time frame in the <strong class="bold">Anomaly Explorer</strong>. The following screenshot shows the results of this:</p>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/B17040_07_9.jpg" alt="Figure 7.9 – Anomaly Explorer showing results of multiple jobs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9 – Anomaly Explorer showing results of multiple jobs</p>
			<p>In particular, notice that the day the KPI was exhibiting problems (February 8, 2021, as shown in <em class="italic">Figure 7.8</em>), the three other jobs in <em class="italic">Figure 7.9</em> exhibit correlated anomalies, shown by the circled area. Upon closer inspection (by clicking on the red tile for the <strong class="source-inline">it_ops_sql</strong> job), you can see that there were issues with several of the SQL Server metrics going haywire at the <a id="_idIndexMarker499"/>same <a id="_idIndexMarker500"/>time:</p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/B17040_07_10.jpg" alt="Figure 7.10 – Anomaly Explorer showing anomalies for SQL Server&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10 – Anomaly Explorer showing anomalies for SQL Server</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The shaded area of the charts is highlighting the window of time associated with the width of the selected tile in the swim lane. This window of time might be larger than the bucket span of the analysis (as is the case here) and therefore the shaded area can contain many individual anomalies during that time frame.</p>
			<p>If we look at the anomalies in the anomaly detection job for the application log, there is an influx of errors all <a id="_idIndexMarker501"/>referencing the <a id="_idIndexMarker502"/>database (further corroborating an unstable SQL server):</p>
			<div>
				<div id="_idContainer167" class="IMG---Figure">
					<img src="image/B17040_07_11.jpg" alt="Figure 7.11 – Anomaly Explorer showing anomalies for the application log&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.11 – Anomaly Explorer showing anomalies for the application log</p>
			<p>However, interesting things were also happening on the network:</p>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="image/B17040_07_12.jpg" alt="Figure 7.12 – Anomaly Explorer showing anomalies for the network data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.12 – Anomaly Explorer showing anomalies for the network data</p>
			<p>Specifically, there was a large spike in network traffic (shown by the <strong class="bold">Out_Octets</strong> metric), and a high spike in packets getting dropped at <a id="_idIndexMarker503"/>the network <a id="_idIndexMarker504"/>interface (shown by the <strong class="bold">Out_Discards</strong> metric).</p>
			<p>At this point, there was clear suspicion that this network spike might have something to do with the database problem. And, while correlation is not always causation, it was enough of a clue to entice the operations team to look back over some historical data from prior outages. In every other outage, this large network spike and packet drops pattern also existed.</p>
			<p>The ultimate cause of the network spike was VMware's action of moving VMs to new ESX servers. Someone had misconfigured the network switch and VMware was sending this massive burst of traffic over the application VLAN instead of the management VLAN. When this occurred (randomly, of course), the transaction processing app would temporarily lose <a id="_idIndexMarker505"/>connection to the database and attempt to reconnect. However, there was a critical flaw in this reconnection <a id="_idIndexMarker506"/>code in that it would not attempt the reconnection to the database at the remote IP address that belonged to SQL Server. Instead, it attempted the reconnection to localhost (IP address <strong class="source-inline">127.0.01</strong>), where, of course, there was no such database. The clue to this bug was seen in one of the example log lines that Elastic ML displayed in the <strong class="bold">Examples</strong> section (circled in the following screenshot):</p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/B17040_07_13.jpg" alt="Figure 7.13 – Anomaly Explorer showing the root cause of the reconnection problem&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.13 – Anomaly Explorer showing the root cause of the reconnection problem</p>
			<p>Once the problem occurred, the connection to SQL Server was therefore only possible if the application server was completely rebooted, the startup configuration files were reread, and the IP address of SQL Server was relearned. This was why a full reboot always fixed the problem.</p>
			<p>One key thing to <a id="_idIndexMarker507"/>notice is how the influencers in the user interface also assist with narrowing down the scope of who's at <a id="_idIndexMarker508"/>fault for the anomalies:</p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="image/B17040_07_14.jpg" alt="Figure 7.14 – Anomaly Explorer showing the top influencers&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.14 – Anomaly Explorer showing the top influencers</p>
			<p>The top-scoring influencers over the time span selected in the dashboard are listed in the <strong class="bold">Top influencers</strong> section on the left. For each influencer, the maximum influencer score (in any bucket) is displayed, together with the total influencer score over the dashboard time range (summed across all buckets). And, if multiple jobs are being displayed together, then those influencers that are common across jobs have higher sums, thus pushing their ranking higher.</p>
			<p>This is a very <a id="_idIndexMarker509"/>key point because now it is very easy to see commonalities in offending entities across jobs. If <strong class="source-inline">esxserver1.acme.com</strong> is the only physical host that surfaces as an influencer when viewing multiple jobs, then <a id="_idIndexMarker510"/>we immediately know which machine to focus on; we know it is not a widespread problem.</p>
			<p>In the end, the customer was able to mitigate the system by both correcting the network misconfiguration and addressing the bug in the database reconnection code. They were able to narrow in on this root cause quite quickly because Elastic ML allowed them to narrow the focus of their investigation, thus saving time and preventing future occurrences.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor145"/>Summary</h1>
			<p>Elastic ML can certainly boost the amount of data that IT organizations <em class="italic">pay attention to</em>, and thus get more insight and proactive value out of their data. The ability to organize, correlate, and holistically view related anomalies across data types is critical to problem isolation and root cause identification. It reduces application downtime and limits the possibility of problem recurrence.</p>
			<p>In the next chapter, we will see how other apps within the Elastic Stack (APM, Security, and Logs) take advantage of Elastic ML to provide an out-of-the-box experience that's custom-tailored for specific use cases.</p>
		</div>
	</body></html>