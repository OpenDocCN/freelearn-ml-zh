- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Compute and Networking
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算和网络
- en: Several large and small organizations run workloads on AWS using AWS compute.
    Here, AWS **Compute** refers to a set of services on AWS that help you build and
    deploy your own solutions and services; this can include workloads as diverse
    as websites, data analytics engines, **Machine Learning** (**ML**), **High-Performance
    Computing** (**HPC**), and more. Being one of the first services to be released,
    **Amazon Elastic Compute Cloud** (**EC2**) is sometimes used synonymously with
    the term *compute* and offers a wide variety of instance types, processors, memory,
    and storage configurations for your workloads.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 许多大型和小型组织在AWS上运行工作负载，使用AWS计算服务。在这里，AWS **计算**指的是AWS上的一组服务，帮助您构建和部署自己的解决方案和服务；这可以包括网站、数据分析引擎、**机器学习**（**ML**）、**高性能计算**（**HPC**）等多种多样的工作负载。作为最早发布的服务之一，**Amazon
    Elastic Compute Cloud**（**EC2**）有时与术语*计算*同义使用，并为您的各种工作负载提供广泛的实例类型、处理器、内存和存储配置。
- en: Apart from EC2, compute services that are suited to some specific types of workloads
    include Amazon **Elastic Container Service** (**ECS**), **Elastic Kubernetes Service**
    (**EKS**), Batch, Lambda, Wavelength, and Outposts. **Networking on AWS** refers
    to foundational networking services, including Amazon **Virtual Private Cloud**
    (**VPC**), AWS Transit Gateway, and AWS PrivateLink. These services, along with
    the various compute services, enable you to build solutions with the most secure
    and performant networked systems at a global scale. AWS compute and networking
    concepts are two broad topics and are important to understand many concepts that
    will be discussed in the following chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 除了EC2之外，适合某些特定类型工作负载的计算服务还包括Amazon **弹性容器服务**（**ECS**）、**弹性Kubernetes服务**（**EKS**）、Batch、Lambda、Wavelength和Outposts。**AWS上的网络**指的是基础网络服务，包括Amazon
    **虚拟私有云**（**VPC**）、AWS Transit Gateway和AWS PrivateLink。这些服务以及各种计算服务，使您能够在全球范围内构建最安全、性能最高的网络化系统解决方案。AWS计算和网络概念是两个广泛的主题，对于理解以下章节中将要讨论的许多概念非常重要。
- en: Compute and networking also form two important pillars of HPC, along with data
    management, which was discussed in the last chapter. Every application of HPC
    is generally optimized for high levels of distributed compute, which depends on
    networking.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 计算和网络也是HPC的两个重要支柱，与上一章讨论的数据管理一样。HPC的每个应用通常都针对高水平的分布式计算进行优化，这依赖于网络。
- en: In this chapter, you will learn about the different services AWS offers for
    compute and networking, how these services are used for different types of computing
    workloads, and lastly, best practices for the HPC type of workloads on AWS, which
    goes beyond the AWS Well-Architected Framework.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解AWS提供的不同计算和网络服务，这些服务如何用于不同类型的计算工作负载，以及最后，AWS上HPC类型工作负载的最佳实践，这超出了AWS
    Well-Architected Framework的范围。
- en: 'Specifically, in this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中，我们将涵盖以下主题：
- en: Introducing the AWS compute ecosystem
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍AWS计算生态系统
- en: Networking on AWS
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS上的网络
- en: Selecting the right compute for HPC workloads
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择适合HPC工作负载的正确计算资源
- en: Best practices for HPC workloads
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HPC工作负载的最佳实践
- en: Introducing the AWS compute ecosystem
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍AWS计算生态系统
- en: Compute lies at the foundation of every HPC application that you will read about
    in and outside of this book. In AWS and other clouds in general, compute refers
    to a group of services that offer the basic building blocks of performing a computation
    or some business logic. This can range from basic data computations to ML.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 计算是您将在本书内外阅读的每个HPC应用的基础。在AWS和其他云服务中，计算指的是一组提供执行计算或某些业务逻辑基本构建块的服务。这可以包括从基本数据计算到机器学习（ML）的各种应用。
- en: 'The basic units of measuring compute power on AWS (regardless of the service
    we are talking about) are as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS上衡量计算能力的基准单位（无论我们讨论的是哪种服务）如下：
- en: '*Processing units* – this can be measured as the number of **Central Processing
    Units** (**CPUs**), **Virtual CPUs** (**vCPUs**), or **Graphics Processing** **Units**
    (**GPUs**)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*处理单元* - 这可以衡量为中央处理单元（**CPUs**）、虚拟中央处理单元（**vCPUs**）或图形处理单元（**GPUs**）的数量'
- en: '*Memory* – this is the total requested or allocated memory for the application
    measured in units of bytes'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*内存* - 这是应用程序请求或分配的总内存，以字节为单位'
- en: Typical HPC applications access multiple instances and hence can take advantage
    of pooled compute and memory resources for larger workloads.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的HPC应用访问多个实例，因此可以利用池化的计算和内存资源来处理更大的工作负载。
- en: The foundational service that provides compute resources for customers to build
    their applications on AWS is called Amazon EC2\. Amazon EC2 provides customers
    with a choice of about 500 instance types (at the time of writing this book and
    according to public documentation). Customers can then tailor the right combination
    of instance types for their business applications.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为客户提供计算资源以在 AWS 上构建应用程序的基础服务称为 Amazon EC2。Amazon EC2 为客户提供约 500 种实例类型的选择（根据撰写本书时的公开文档）。然后，客户可以为他们的业务应用程序定制合适的实例类型组合。
- en: 'Amazon EC2 provides five types of instances:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon EC2 提供五种类型的实例：
- en: General purpose instances
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用型实例
- en: Compute optimized instances
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算优化实例
- en: Accelerated computing instances
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速计算实例
- en: Memory optimized instances
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存优化实例
- en: Storage optimized instances
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储优化实例
- en: 'Each of the instance types listed here is actually a family of instances, as
    shown in *Figure 3**.1*:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列出的每种实例类型实际上都是一个实例系列，如图 *3.1* 所示：
- en: '![Figure 3.1 – Amazon EC2 instance types](img/B18493_03_001.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – Amazon EC2 实例类型](img/B18493_03_001.jpg)'
- en: Figure 3.1 – Amazon EC2 instance types
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – Amazon EC2 实例类型
- en: In the following section, we will highlight some important facts about these
    instance types.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将突出介绍这些实例类型的一些重要事实。
- en: General purpose instances
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用型实例
- en: 'General purpose instances can be used for a variety of workloads. They have
    the right balance of compute, memory, and storage for most typical applications
    that customers have on AWS. On AWS, there are several types of general purpose
    instances:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通用型实例可用于各种工作负载。它们具有计算、内存和存储之间的平衡，适合大多数典型的 AWS 客户应用程序。在 AWS 上，有几种类型的通用型实例：
- en: '**T-type instances**: T instances, for example, the T2 instance, are *burstable*
    instances that provide a basic level of compute for low compute- and memory-footprint
    workloads. The following figure shows what a typical workload that is suited for
    T-type instances might look like – most of the time is spent under the baseline
    CPU utilization level, with a need to *burst* above this baseline occasionally.
    With non-burstable instances, application owners need to over-provision for the
    burst CPU levels, and therefore pay more while utilizing very little. With burstable
    T instances, credits are accrued for utilization under the baseline (white areas
    below the baseline), and these credits can be used when the application is experiencing
    higher loads; see the gray filled-in areas in the following graph:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**T 型实例**：T 型实例，例如 T2 实例，是*可爆增*实例，为低计算和内存占用工作负载提供基本计算能力。以下图示展示了适合 T 型实例的典型工作负载可能的样子——大部分时间都低于基线
    CPU 利用率，偶尔需要*爆增*超过这个基线。对于非可爆增实例，应用程序所有者需要为爆增 CPU 级别进行过度配置，因此即使利用率很低，也需要支付更多费用。对于可爆增
    T 型实例，当利用率低于基线（基线以下的白色区域）时，会累积信用，这些信用可以在应用程序遇到更高负载时使用；请参见以下图中灰色填充的区域：'
- en: '![Figure 3.2 – CPU utilization versus time for burstable T-type instances on
    AWS](img/B18493_03_002.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – AWS 上可爆增 T 型实例的 CPU 利用率与时间对比](img/B18493_03_002.jpg)'
- en: Figure 3.2 – CPU utilization versus time for burstable T-type instances on AWS
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – AWS 上可爆增 T 型实例的 CPU 利用率与时间对比
- en: '**M-type instances**: M instances (like the M4, M5, and M6) can be used for
    a variety of workloads that need a balance of compute, memory, and networking,
    including (but not limited to) web and application servers, and small to medium-sized
    database workloads. Special versions of M5 and M6 instances are offered that are
    suitable for certain workloads. For example, the M5zn instance types can be used
    for applications that demand extremely high single-threaded performance, such
    as HPC, simulations, and gaming.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**M 型实例**：M 型实例（如 M4、M5 和 M6）可用于需要计算、内存和网络平衡的各种工作负载，包括但不限于 Web 和应用程序服务器，以及小型到中型数据库工作负载。M5
    和 M6 实例的特殊版本适用于某些工作负载。例如，M5zn 实例类型可用于需要极高单线程性能的应用程序，如高性能计算、模拟和游戏。'
- en: '**A-type instances**: A1 instances are used to run **Advanced RISC Machine**
    or **ARM**-based applications, such as microservices and web servers powered by
    the AWS Graviton ARM processors.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A 型实例**：A1 实例用于运行基于**高级精简指令集机器**或**ARM**的应用程序，例如由 AWS Graviton ARM 处理器提供动力的微服务和
    Web 服务器。'
- en: '**Mac-type instances**: Mac1 instances are powered by Apple’s Mac Mini computers
    and provide very high network and storage bandwidth. They are typically used for
    building and testing Apple applications for the iPhone, Mac, and so on.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mac型实例**：Mac1实例由苹果的Mac Mini电脑供电，提供非常高的网络和存储带宽。它们通常用于构建和测试适用于iPhone、Mac等设备的苹果应用。'
- en: In the following section, we will discuss compute optimized instances on AWS.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论AWS上的计算优化实例。
- en: Compute optimized instances
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算优化实例
- en: Many HPC applications that will be described in this book take advantage of
    the high-performance, compute optimized instance types on AWS.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将描述的许多HPC应用都利用了AWS上高性能、计算优化的实例类型。
- en: 'There are several types of compute optimized instances:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 存在几种类型的计算优化实例：
- en: '**C5 instances**: C5 and C5n instances provide low-cost, high-performance compute
    for typical HPC, gaming, batch processing, and modeling. C5 instances use Intel
    Xeon processors (first and second generation) and provide upward of 3.4 GHz clock
    speeds on a single core. C5a instances also provide AMD processors for high performance
    at an even lower cost. C5n instances are well suited to HPC applications since
    they support **Elastic Fabric Adapter** (**EFA**) and can deliver up to 100 gigabits
    per second of networking throughput. For more information about EFA, please visit
    [https://aws.amazon.com/hpc/efa/](https://aws.amazon.com/hpc/efa/).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C5实例**：C5和C5n实例提供低成本、高性能的计算，适用于典型的HPC、游戏、批处理和建模。C5实例使用英特尔Xeon处理器（第一代和第二代），在单个核心上提供超过3.4GHz的时钟速度。C5a实例还提供AMD处理器，以更低成本实现高性能。C5n实例非常适合HPC应用，因为它们支持**弹性布线适配器**（**EFA**）并可实现高达每秒100吉比特的网络吞吐量。有关EFA的更多信息，请访问[https://aws.amazon.com/hpc/efa/](https://aws.amazon.com/hpc/efa/)。'
- en: '**C6 instances**: C6g instances are ARM-based instances based on the AWS Graviton
    processor. They are ideal for running HPC workloads, ad serving, and game servers.
    C6g instances are available with local **Non-Volatile Memory express** or **NVMe**-based
    high performance, low latency SSD storage with 100 gigabits per second networking,
    and support for EFA. On the other hand, the C6i class of instances is Intel Xeon-based
    and can provide up to 128 vCPUs per instance for typical HPC workloads.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C6实例**：C6g实例是基于AWS Graviton处理器的ARM架构实例。它们非常适合运行高性能计算（HPC）工作负载、广告服务和游戏服务器。C6g实例提供基于本地**非易失性内存表达式（NVMe**）的高性能、低延迟SSD存储，网络速度可达每秒100吉比特，并支持EFA。另一方面，C6i类实例基于英特尔Xeon处理器，每个实例可提供高达128个vCPU，适用于典型的HPC工作负载。'
- en: '**HPC instances**: The HPC6a instance type is powered by third-generation AMD
    processors for lower cost-to-performance ratios for typical HPC workloads. These
    instance types also provide 96 CPU cores and 384 GB of RAM for memory-intensive
    applications. HPC6a instances also support EFA-based networking for up to 100
    gigabits per second of throughput.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HPC实例**：HPC6a实例类型由第三代AMD处理器提供动力，以实现典型HPC工作负载的成本性能比。这些实例类型还提供96个CPU核心和384GB的RAM，适用于内存密集型应用。HPC6a实例还支持基于EFA的网络，吞吐量可达每秒100吉比特。'
- en: In the following section, we will discuss accelerated compute instances on AWS.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论AWS上的加速计算实例。
- en: Accelerated compute instances
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速计算实例
- en: Accelerated computing instances use co-processors such as GPUs to accelerate
    performance for workloads such as floating point number calculations useful for
    ML, deep learning, and graphics processing.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 加速计算实例使用如GPU这样的协处理器来加速工作负载的性能，例如用于机器学习、深度学习和图形处理的浮点数计算。
- en: 'Accelerated compute instances use hardware-based compute accelerators such
    as the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 加速计算实例使用以下基于硬件的计算加速器：
- en: GPUs
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU
- en: '**Field Programmable Gate** **Arrays** (**FPGAs**)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现场可编程门阵列**（**FPGA**）'
- en: AWS Inferentia
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Inferentia
- en: GPUs
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPU
- en: GPUs were originally used for 3D graphics but are now being used as general-purpose
    co-processors for various applications such as HPC and deep learning. HPC applications
    are computation and bandwidth-heavy. Several types of NVIDIA GPUs are available
    on AWS, and detailed information can be found at the following link, [https://aws.amazon.com/nvidia/](https://aws.amazon.com/nvidia/).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: GPU最初用于3D图形，但现在被用作各种应用的通用协处理器，如HPC和深度学习。HPC应用计算和带宽密集。AWS上提供了多种类型的NVIDIA GPU，详细信息可在以下链接找到：[https://aws.amazon.com/nvidia/](https://aws.amazon.com/nvidia/)。
- en: 'Let’s dive into the basics of how GPUs help with compute-heavy calculations.
    Imagine adding a list of numbers to another list of the same size. Visually, this
    looks like the following diagram:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解 GPU 如何帮助进行计算密集型计算的基础。想象一下将一个数字列表添加到另一个相同大小的数字列表中。直观上看，这就像以下图示：
- en: '![Figure 2.3 – Adding two arrays](img/B18493_03_003.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 添加两个数组](img/B18493_03_003.jpg)'
- en: Figure 2.3 – Adding two arrays
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 添加两个数组
- en: 'The naïve way of adding these to arrays is to loop through all elements of
    each array and add each corresponding number from the top and bottom arrays. This
    may be fine for small arrays, but what about arrays that are millions of elements
    long? To do this on a GPU, we first allocate memory for these two very long arrays
    and then use *threads* to parallelize these computations. Adding these arrays
    using a single thread on a single GPU is the same as our earlier naïve approach.
    Using multiple threads (say 256) can help parallelize this operation by allocating
    a part of the work to each thread. For example, the first few elements (the total
    size divided by 256 in this case) will be done by the first thread, and so on.
    This speeds up the operation by letting each thread focus on a smaller portion
    of the work and do each of these split-up addition operations in parallel; see
    the shaded region in the following diagram:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些添加到数组中的简单方法是遍历每个数组的所有元素，并将顶部和底部数组中相应的数字相加。这对于小型数组来说可能没问题，但对于数百万个元素的数组呢？要在
    GPU 上完成这项工作，我们首先为这两个非常长的数组分配内存，然后使用 *线程* 来并行化这些计算。使用单个 GPU 上的单个线程添加这些数组与我们的早期简单方法相同。使用多个线程（例如
    256 个）可以通过将部分工作分配给每个线程来帮助并行化此操作。例如，前几个元素（在这种情况下是总大小除以 256）将由第一个线程完成，依此类推。这通过让每个线程专注于较小的工作部分，并并行执行这些分割的加法操作来加快操作速度；请参见以下图中的阴影区域：
- en: '![Figure 3.4 – Multiple threads handling a portion of the computation](img/B18493_03_004.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 多个线程处理计算的一部分](img/B18493_03_004.jpg)'
- en: Figure 3.4 – Multiple threads handling a portion of the computation
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 多个线程处理计算的一部分
- en: 'GPUs today are architected in a way that allows even higher levels of parallelism
    – multiple processing threads make up a block, and there are usually multiple
    blocks in a GPU. Each block can run concurrently in a **Streaming Multiprocessor**
    (**SM**) and process the same set of computations or *kernels*. Visually, this
    looks like the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的 GPU 架构允许更高的并行级别——多个处理线程组成一个块，通常 GPU 中有多个块。每个块可以在 **流式多处理器** (**SM**) 中并发运行，并处理相同的一组计算或
    *内核*。直观上看，这就像以下图示：
- en: '![Figure 3.5 – Multiple blocks in a GPU](img/B18493_03_005.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – GPU 中的多个块](img/B18493_03_005.jpg)'
- en: Figure 3.5 – Multiple blocks in a GPU
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – GPU 中的多个块
- en: 'To give you an idea of what you can access on AWS, consider the **P4d.24xlarge**
    instance. This instance has eight GPUs, as seen in the following figure, each
    of which is an NVIDIA A100 housing 108 SMs, with each SM capable of running 2,048
    threads in parallel:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您了解在 AWS 上可以访问的内容，考虑一下 **P4d.24xlarge** 实例。这个实例有八个 GPU，如下面的图所示，每个 GPU 都是一个包含
    108 个 SM 的 NVIDIA A100，每个 SM 能够并行运行 2,048 个线程：
- en: '![Figure 3.6 – A single instance with multiple GPUs](img/B18493_03_006.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 单个实例中的多个 GPU](img/B18493_03_006.jpg)'
- en: Figure 3.6 – A single instance with multiple GPUs
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 单个实例中的多个 GPU
- en: On AWS, P4d instances can be used to provision a supercomputer or an EC2 Ultracluster
    with more than 4,000 A100 GPUs, Petabit-scale networking, and scalable, shared
    high throughput storage on Amazon FSx for Lustre ([https://aws.amazon.com/fsx/lustre/](https://aws.amazon.com/fsx/lustre/)).
    Application and package developers use the NVIDIA CUDA library to build massively
    parallel applications for HPC and deep learning. For example, PyTorch, a popular
    ML library, uses NVIDIA’s CUDA GPU programming library for training large-scale
    models. Another example is Ansys Fluent, a popular **Computational Fluid Dynamics**
    (**CFD**) simulation software that uses GPU cores to accelerate fluid flow computations.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 上，P4d 实例可用于部署超级计算机或具有超过 4,000 个 A100 GPU 的 EC2 Ultracluster，以及 Amazon
    FSx for Lustre 上的可扩展、共享的高吞吐量存储（[https://aws.amazon.com/fsx/lustre/](https://aws.amazon.com/fsx/lustre/））。应用程序和包开发者使用
    NVIDIA CUDA 库构建用于 HPC 和深度学习的海量并行应用程序。例如，PyTorch，一个流行的 ML 库，使用 NVIDIA 的 CUDA GPU
    编程库进行大规模模型的训练。另一个例子是 Ansys Fluent，一个流行的 **计算流体动力学** (**CFD**) 模拟软件，它使用 GPU 内核来加速流体流动计算。
- en: 'On AWS, there are several families of GPU instances:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 上，有几个系列的 GPU 实例：
- en: '**G family of instances**: G2, G3, G4, and G5 type instances on AWS provide
    cost-effective access to GPU resources. Each G-type instance mentioned here comes
    with a different NVIDIA GPU – for example, the latest G5 instances come with NVIDIA
    A10G GPUs, G5g instances with NVIDIA T4G GPUs, and G4Dn with the NVIDIA Tesla
    GPUs. AMD-based GPUs are also available – for example, the G4ad instances use
    the AMD Radeon Pro V520 GPUs.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**G 系列实例**：AWS 上的 G2、G3、G4 和 G5 类型实例提供经济高效的 GPU 资源访问。这里提到的每种 G 类型实例都配备了不同的
    NVIDIA GPU – 例如，最新的 G5 实例配备了 NVIDIA A10G GPU，G5g 实例配备了 NVIDIA T4G GPU，而 G4Dn 则配备了
    NVIDIA Tesla GPU。AMD 基础的 GPU 也可用 – 例如，G4ad 实例使用 AMD Radeon Pro V520 GPU。'
- en: '**P family of instances**: These instances provide extremely high-performance
    GPUs for single-instance and distributed applications. The P2 instances provide
    access to the NVIDIA K80 GPUs, P3 instances have the NVIDIA Tesla V100 GPUs, and
    the P4d instances have the A10 GPUs.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**P 系列实例**：这些实例为单实例和分布式应用提供了高性能 GPU。P2 实例提供对 NVIDIA K80 GPU 的访问，P3 实例配备 NVIDIA
    Tesla V100 GPU，而 P4d 实例则配备 A10 GPU。'
- en: '**VT1 instances**: These provide access to the Xilinx Alveo U30 media accelerator
    cards that are primarily used for video transcoding applications. More information
    about VT1 instances can be found here: [https://aws.amazon.com/ec2/instance-types/vt1/](https://aws.amazon.com/ec2/instance-types/vt1/).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VT1 实例**：这些实例提供访问 Xilinx Alveo U30 媒体加速卡，主要用于视频转码应用。有关 VT1 实例的更多信息，请参阅此处：[https://aws.amazon.com/ec2/instance-types/vt1/](https://aws.amazon.com/ec2/instance-types/vt1/)。'
- en: '**AWS Inferentia**: These instances are specifically designed to provide cost-effective
    and low latency ML inference capability and are a custom-made chip created by
    AWS. A typical workflow that customers could follow using these *Inf1* instances
    is to use an ML framework like TensorFlow to train a model on another EC2 instance
    or SageMaker training instances, then use Amazon SageMaker’s compilation feature
    *Neo* to compile the model for use with the Inf1 instances. You can also make
    use of the AWS Neuron SDK to profile and deploy deep learning models onto *Inf1*
    instances.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Inferentia**：这些实例专门设计用于提供经济高效且低延迟的机器学习推理能力，并由 AWS 定制制造。客户可以使用这些 *Inf1*
    实例遵循的典型工作流程是使用 TensorFlow 等机器学习框架在另一个 EC2 实例或 SageMaker 训练实例上训练模型，然后使用 Amazon
    SageMaker 的编译功能 *Neo* 将模型编译用于与 Inf1 实例一起使用。您还可以使用 AWS Neuron SDK 对深度学习模型进行性能分析并将其部署到
    *Inf1* 实例。'
- en: FPGA instances
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FPGA 实例
- en: Amazon EC2 F1 instances allow you to develop and deploy hardware-accelerated
    applications easily on the cloud. Example applications include (but are not limited
    to) big data analytics, genomics, and simulation-related applications. Developers
    can use high-level C/C++ code to program their applications, register the FPGA
    as an **Amazon FPGA Image** (**AFI**), and deploy the application to an F1 instance.
    For more information on F1 instances, please refer to the links in the Reference
    section at the end of this chapter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon EC2 F1 实例允许您在云上轻松开发并部署硬件加速应用。示例应用包括（但不限于）大数据分析、基因组学和与模拟相关的应用。开发者可以使用高级
    C/C++ 代码来编写他们的应用，将 FPGA 注册为 **Amazon FPGA Image**（**AFI**），并将应用部署到 F1 实例。有关 F1
    实例的更多信息，请参阅本章末尾参考部分中的链接。
- en: In the following section, we will discuss memory optimized compute instances
    on AWS.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论 AWS 上的内存优化计算实例。
- en: Memory optimized instances
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存优化实例
- en: 'Memory optimized instances on AWS are suited to run applications that require
    storage of extremely large data in memory. Typical applications that fall into
    this category are in-memory databases, HPC applications, simulation, and **Electronic
    Design Automation** (**EDA**) applications. On AWS, there are several types of
    memory optimized instances:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 上的内存优化实例适用于运行需要将极大量数据存储在内存中的应用。典型的此类应用包括内存数据库、高性能计算应用、模拟和 **电子设计自动化**（**EDA**）应用。在
    AWS 上，有几种类型的内存优化实例：
- en: '**R5 instances**: The R5 family of instances (such as the R5, R5a, R5b, and
    R5n instance types) is a great choice for relational databases such as MySQL,
    MongoDB, and Cassandra, for in-memory databases such as Redis and Memcached, and
    business intelligence applications, such as SAP HANA and HPC applications. The
    R5 metal instance type also provides direct access to processors and memory on
    the physical server that the instance is based on.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R5型实例**：R5型实例系列（如R5、R5a、R5b和R5n实例类型）是关系数据库（如MySQL、MongoDB和Cassandra）、内存数据库（如Redis和Memcached）以及商业智能应用程序（如SAP
    HANA和HPC应用程序）的理想选择。R5金属实例类型还提供了对基于物理服务器的处理器和内存的直接访问。'
- en: '**R6 instances**: R6 instances such as R6g and R6gd are based on ARM-based
    AWS Gravitron2 processors and can provide better price-to-performance ratios compared
    to R5 instances. Application developers can use these instance types to develop
    or support ARM-based applications that need a high memory footprint.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R6型实例**：R6型实例（如R6g和R6gd）基于ARM架构的AWS Gravitron2处理器，与R5实例相比，可以提供更好的价格性能比。应用开发者可以使用这些实例类型开发或支持需要高内存占用脚本的ARM架构应用程序。'
- en: '**U instances**: These instances are extremely high memory instances – they
    can offer anywhere from 6 to 24 TB of memory per instance. They are typically
    used to run large in-memory applications such as databases and SAP HANA and are
    powered by Intel Xeon Platinum 8176M processors.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**U型实例**：这些实例是极高性能的内存实例 – 每个实例可以提供从6到24 TB的内存。它们通常用于运行大型内存应用程序，如数据库和SAP HANA，并由Intel
    Xeon Platinum 8176M处理器提供动力。'
- en: '**X instances**: X-type instances (such as X1, X1e, and X1gd instance types)
    are designed for large-scale in-memory applications in the cloud. Each X1 instance
    is powered by four Intel Xeon E8880 processors with up to 128 vCPUs, and up to
    1,952 GB of memory. X1 instances are picked by developers for their low price-to-performance
    ratio given the amount of memory provided, compared to other families of instances.
    X1e instances provide even higher memory (up to 3,904 GB) and support production-grade
    SAP workloads. Both X1 and X1e instance types provide up to 25 gigabits per second
    of network bandwidth when used with an **Elastic Network Adapter** (**ENA**).
    Finally, X1gd and X2gd are AWS Graviton2-based ARM instances that provide better
    price performance compared to x86-based X1 instances.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X型实例**：X型实例（如X1、X1e和X1gd实例类型）是为云中的大规模内存应用程序设计的。每个X1实例由四个Intel Xeon E8880处理器提供动力，最高可达128个vCPU，以及最高1,952
    GB的内存。X1实例因其提供的内存量与其它实例系列相比具有较低的价格性能比而受到开发者的青睐。X1e实例提供更高的内存（高达3,904 GB）并支持生产级SAP工作负载。X1和X1e实例类型在使用**弹性网络适配器**（**ENA**）时提供高达每秒25千兆位的网络带宽。最后，X1gd和X2gd是基于AWS
    Graviton2的ARM实例，与基于x86的X1实例相比，提供更好的价格性能。'
- en: '**Z instance types**: The Z1d instance type provides both high performance
    and high memory for typical data analytics, financial services, and HPC applications.
    Z1d instances are well suited for applications that need very high single-threaded
    performance, with an added dependence on high memory. Z1d instances come in seven
    different sizes, and up to 48 vCPUs and 384 GB of RAM, so customers can choose
    the right instance size for their application.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Z型实例**：Z1d实例类型既提供高性能又提供高内存，适用于典型的数据分析、金融服务和HPC应用程序。Z1d实例非常适合需要非常高的单线程性能且对高内存有额外依赖的应用程序。Z1d实例有七种不同的大小，最高可达48个vCPU和384
    GB的RAM，因此客户可以根据其应用程序选择合适的实例大小。'
- en: Storage optimized instances
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储优化实例
- en: 'Storage optimized instances are well suited for applications that need frequent,
    sequential reads and writes from local storage by providing very high **I/O**
    **Operations Per Second** (**IOPS**). There are several storage optimized instances
    on AWS:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 存储优化实例非常适合需要频繁、顺序地从本地存储进行读取和写入的应用程序，通过提供非常高的每秒**I/O操作数**（**IOPS**）。在AWS上存在几个存储优化实例：
- en: '**D-type instances**: Instances such as D2, D3, and D3en provide high-performance
    local storage and can be used for MapReduce-style operations (with Hadoop or Spark),
    log processing, and other big data workloads that do not require all the data
    to be held in memory but require very fast, on-demand access to this data.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**D型实例**：D2、D3和D3en等实例提供高性能本地存储，可用于MapReduce风格的操作（使用Hadoop或Spark）、日志处理以及其他大数据工作负载，这些工作负载不需要将所有数据都保留在内存中，但需要非常快速、按需访问这些数据。'
- en: '**H-type instances**: The H1 instance is typically used for MapReduce applications
    and distributed file storage, and other data-intensive applications.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**H型实例**：H1实例通常用于MapReduce应用、分布式文件存储和其他数据密集型应用。'
- en: '**I-type instances**: I type instances such as I3 and I3en are well suited
    for relational and non-relational databases, in-memory caches, and other big data
    applications. The I3 instances are NVMe **Solid State Drive** (**SSD**) -based
    instances that can provide up to 25 GB of network bandwidth and 14 gigabits per
    second of dedicated bandwidth to attached **Elastic Block Store** (**EBS**) volumes.
    The Im4gn and Is4gen type instances can be used for relational and *NoSQL* databases,
    streaming applications, and distributed file applications.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**I型实例**：I型实例，如I3和I3en，非常适合关系型和非关系型数据库、内存缓存和其他大数据应用。I3实例是基于NVMe **固态硬盘**（**SSD**）的实例，可以提供高达25
    GB的网络带宽和每秒14千兆位的专用带宽，用于连接的**弹性块存储**（**EBS**）卷。Im4gn和Is4gen类型的实例可用于关系型和非*NoSQL*数据库、流式应用和分布式文件应用。'
- en: Amazon Machine Images (AMIs)
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊机器镜像（AMIs）
- en: 'Now that we have discussed different instance types that you can choose for
    your applications on AWS, we can move on to the topic of **Amazon Machine Images**
    (**AMIs**). AMIs contain all the information needed to launch an instance. This
    includes the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了您可以在AWS上为应用程序选择的不同实例类型，我们可以继续讨论**亚马逊机器镜像**（**AMIs**）的主题。AMIs包含启动实例所需的所有信息。这包括以下内容：
- en: The description of the operating system to use, the architecture (32 or 64-bit),
    any applications to be included along with an application server, and EBS snapshots
    to be attached before launch
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述要使用的操作系统、架构（32位或64位）、要包含的应用程序以及应用程序服务器，以及启动前要附加的EBS快照
- en: Block-device mapping that defines which volumes to attach to the instance on
    launch
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在启动时定义要附加到实例上的卷的块设备映射
- en: Launch permissions that control which AWS accounts can use this AMI
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动权限，控制哪些AWS账户可以使用此AMIs
- en: You can create your own AMI, or buy, share, or sell your AMIs on the AWS Marketplace.
    AWS maintains Amazon Linux-based AMIs that are stable and secure, updated and
    maintained on a regular basis, and includes several AWS tools and packages. Furthermore,
    these AMIs are provided free of charge to AWS customers.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以创建自己的AMIs，或者在AWS市场上购买、共享或出售您的AMIs。AWS维护基于Amazon Linux的稳定且安全的AMIs，这些AMIs定期更新和维护，并包括几个AWS工具和包。此外，这些AMIs免费提供给AWS客户。
- en: Containers on AWS
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS上的容器
- en: In the previous section, we spoke about AMIs on AWS that can help isolate and
    replicate applications across several instances and instance types. Containers
    can be used to further isolate and launch one or more applications onto instances.
    The most popular flavor of containers is called Docker. **Docker** is an open
    platform for developing, shipping, and running applications. Docker provides the
    ability to package and run an application in a loosely isolated environment called
    a container. Docker containers are definitions of runnable images, and these images
    can be run locally on your computer, on virtual machines, or in the cloud. Docker
    containers can be run on any host operating system, and as such are extremely
    portable, as long as Docker is running on the host system.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了AWS上的AMIs，可以帮助在多个实例和实例类型之间隔离和复制应用程序。容器可以用来进一步隔离并在实例上启动一个或多个应用程序。最受欢迎的容器类型被称为Docker。**Docker**是一个用于开发、运输和运行应用程序的开源平台。Docker提供了在称为容器的松散隔离环境中打包和运行应用程序的能力。Docker容器是可运行镜像的定义，这些镜像可以在您的计算机上本地运行，在虚拟机上运行，或在云中运行。只要在主机系统上运行Docker，Docker容器就可以在任何主机操作系统上运行，因此它们非常便携。
- en: 'A Docker container contains everything that is needed to run the applications
    that are defined inside it – this includes configuration information, directory
    structure, software dependencies, binaries, and packages. This may sound complicated,
    but it is actually very easy to define a Docker image; this is done in a Dockerfile
    that may look similar to this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Docker容器包含了运行其中定义的应用程序所需的一切——这包括配置信息、目录结构、软件依赖、二进制文件和包。这可能听起来很复杂，但实际上定义一个Docker镜像非常简单；这是在Dockerfile中完成的，其外观可能类似于以下内容：
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding file named `Dockerfile` defines the Docker image to run a sample
    Python application using the popular `Gunicorn` package (see the last line in
    the file). Before we can run the application, we tell Docker to use the Python-3.7
    base image (`FROM python:3.7-alpine`), copy all the required files from the host
    system to a folder called `app`, and install requirements or dependencies for
    that application to run successfully (`RUN pip install -r requirements.txt`).
    Now you can test out this application locally before deploying it at scale on
    the cloud.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的文件名为 `Dockerfile`，它定义了运行一个使用流行的 `Gunicorn` 包的 Python 示例应用程序的 Docker 镜像（请参阅文件中的最后一行）。在我们能够运行应用程序之前，我们告诉
    Docker 使用 Python-3.7 基础镜像（`FROM python:3.7-alpine`），将所有必需的文件从主机系统复制到名为 `app` 的文件夹中，并安装该应用程序成功运行所需的要求或依赖项（`RUN
    pip install -r requirements.txt`）。现在你可以在云上大规模部署之前在本地测试这个应用程序。
- en: 'On AWS, you can run containers on EC2 instances of your choice or make use
    of the many container services available:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 上，你可以在你选择的 EC2 实例上运行容器，或利用许多可用的容器服务：
- en: When you need to run containers with server-level control, you can directly
    run the images that you define on EC2\. Furthermore, running these containers
    on EC2 *Spot* instances can save you up to 90% of the cost over on-demand instances.
    For more information on *Spot* instances please refer to [https://aws.amazon.com/ec2/spot/](https://aws.amazon.com/ec2/spot/).
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你需要以服务器级控制运行容器时，你可以直接在 EC2 上运行你定义的镜像。此外，在 EC2 *Spot* 实例上运行这些容器可以节省高达 90% 的成本，与按需实例相比。有关
    *Spot* 实例的更多信息，请参阅[https://aws.amazon.com/ec2/spot/](https://aws.amazon.com/ec2/spot/)。
- en: At the opposite end of the spectrum, you can use a service like AWS Fargate
    to run containers without managing servers. Fargate removes all the operational
    overhead of maintaining server-level software so you can focus on just the application
    at hand. With Fargate, you only pay for what you use – for example, if you create
    an application that downloads data files from Amazon S3, processes these files,
    and writes output files back to S3, and this process takes 30 minutes to finish,
    you only pay for the time and resources (vCPUs) used to complete the task.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在光谱的另一端，你可以使用像 AWS Fargate 这样的服务来运行容器，而不需要管理服务器。Fargate 去除了维护服务器级软件的所有运营开销，这样你就可以专注于手头的应用程序。使用
    Fargate，你只需为所使用的资源付费——例如，如果你创建了一个从 Amazon S3 下载数据文件、处理这些文件并将输出文件写回 S3 的应用程序，并且这个过程需要
    30 分钟来完成，你只需为完成任务所使用的时时间和资源（vCPUs）付费。
- en: When you have multiple, complex, container-based applications, managing and
    orchestrating these applications is an important task. On AWS, container management
    and orchestration can be achieved using services like Amazon **Elastic Container
    Registry** (**ECR**), Amazon **Elastic Container Service** (**ECS**), and Amazon
    **Elastic Kubernetes Service** (**EKS**). A detailed discussion about these services
    is outside the scope of this book but if you are interested, you can refer to
    the links mentioned in the *References* section to learn more.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你拥有多个复杂、基于容器的应用程序时，管理和编排这些应用程序是一项重要任务。在 AWS 上，可以使用像 Amazon **弹性容器注册库**（**ECR**）、Amazon
    **弹性容器服务**（**ECS**）和 Amazon **弹性 Kubernetes 服务**（**EKS**）这样的服务来实现容器管理和编排。关于这些服务的详细讨论超出了本书的范围，但如果你感兴趣，可以参考
    *参考文献* 部分中提到的链接以了解更多信息。
- en: Serverless compute on AWS
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS 上的无服务器计算
- en: 'In the previous section, you read about AWS Fargate, which lets you run applications
    and code based on Docker containers, without the need to manage infrastructure.
    This is an example of a serverless service on AWS. AWS offers serverless services
    that have the following features in common:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你了解了 AWS Fargate，它允许你基于 Docker 容器运行应用程序和代码，而不需要管理基础设施。这是一个 AWS 上的无服务器服务的例子。AWS
    提供的无服务器服务具有以下共同特征：
- en: No infrastructure to manage
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需管理基础设施
- en: Automatic scaling
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动扩展
- en: Built-in high availability
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内置高可用性
- en: Pay-per-use billing
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按使用付费计费
- en: Serverless compute technologies on AWS are AWS Lambda and Fargate. AWS Lambda
    is a serverless computing service that lets you run any code that can be triggered
    by over 200 services and SaaS applications. Code can be written in popular languages
    such as Python, [Node.js](http://Node.js), Go, and Java or can be packaged as
    Docker containers, as described earlier. With AWS Lambda, you only pay for the
    number of milliseconds that your code runs, beyond a very generous free tier of
    over a million free requests. AWS Lambda supports the creation of a wide variety
    of applications including file processing, streaming, web applications, IoT backend
    applications, and mobile app backends.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: AWS上的无服务器计算技术包括AWS Lambda和Fargate。AWS Lambda是一种无服务器计算服务，允许您运行由超过200个服务和SaaS应用程序触发的任何代码。代码可以用Python、[Node.js](http://Node.js)、Go和Java等流行语言编写，或者可以像之前描述的那样打包成Docker容器。使用AWS
    Lambda，您只需为代码运行的毫秒数付费，超过一百万次免费请求的非常慷慨的免费层。AWS Lambda支持创建各种应用程序，包括文件处理、流媒体、Web应用程序、物联网后端应用程序和移动应用后端。
- en: For more information on serverless computing on AWS, please refer to the links
    included in the *References* section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解AWS上无服务器计算的更多信息，请参阅*参考文献*部分包含的链接。
- en: In the next section, we will cover basic concepts around networking on AWS.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍AWS上网络的基本概念。
- en: Networking on AWS
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS上的网络
- en: Networking on AWS is a vast topic that is out of the scope of this book. However,
    in order to easily explain some of the sections and chapters that follow, we will
    attempt to provide a brief overview here. First, AWS has a concept called **regions**,
    which are physical areas around the world where AWS places clusters of data centers.
    Each region contains multiple logically separated, groups of data centers called
    **availability zones**. Each availability zone has independent power, cooling,
    and physical security. Availability zones are connected via redundant and ultra-low
    latency AWS Networks. At the time of writing this chapter, AWS has 26 regions
    and 84 availability zones.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: AWS上的网络是一个庞大的主题，超出了本书的范围。然而，为了方便解释接下来的某些章节和部分，我们将在此尝试提供一个简要概述。首先，AWS有一个称为**区域**的概念，这是AWS在全球范围内放置数据中心集群的物理区域。每个区域包含多个逻辑上分离的数据中心组，称为**可用区**。每个可用区都有独立的电源、冷却和物理安全。可用区通过冗余和超低延迟的AWS网络连接。在撰写本章时，AWS有26个区域和84个可用区。
- en: 'The next foundational concept we will discuss here is a **Virtual Private Cloud**
    (**VPC**). A VPC is a logical partition that lets you launch and group AWS resources.
    In the following diagram, we can see that a region has multiple availability zones
    that can span multiple VPCs:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将讨论的基础概念是**虚拟私有云**（**VPC**）。VPC是一个逻辑分区，允许您启动和分组AWS资源。在下面的图中，我们可以看到区域包含多个可用区，这些可用区可以跨越多个VPC：
- en: '![Figure 3.7 – Relationship between regions, VPCs, and availability zones](img/B18493_03_007.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图3.7 – 区域、VPC和可用区之间的关系](img/B18493_03_007.jpg)'
- en: Figure 3.7 – Relationship between regions, VPCs, and availability zones
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 – 区域、VPC和可用区之间的关系
- en: A **subnet** is a range of IP addresses associated with the VPC you have defined.
    A **route table** is a set of rules that determine how traffic will flow within
    the VPC. Every subnet you create in a VPC is automatically associated with the
    main route table of the VPC. A **VPC endpoint** lets you connect resources from
    one VPC to another and to other services.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**子网**是与您定义的VPC关联的IP地址范围。**路由表**是一组规则，用于确定VPC内流量如何流动。在VPC中创建的每个子网都会自动与VPC的主路由表关联。**VPC端点**允许您将一个VPC中的资源连接到另一个VPC和其他服务。'
- en: Next, we will discuss **Classless Inter-Domain Routing** (**CIDR**) blocks and
    routing.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论**无类域间路由**（**CIDR**）块和路由。
- en: CIDR blocks and routing
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CIDR块和路由
- en: 'CIDR is a set of standards that is useful for assigning IP addresses to a device
    or group of devices. A CIDR block looks like the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: CIDR是一组标准，用于将IP地址分配给设备或设备组。CIDR块看起来如下所示：
- en: '[PRE1]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This defines the starting IP, and the number of IP addresses in the block. Here,
    the 16 means that there are 2^(32-16) or 65,536 unique addresses. When you create
    a CIDR block, you have to make sure that all IP addresses are contiguous, the
    block size is a power of 2, and IPs range from `0.0.0.0` to `256.256.256.256`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这定义了起始IP和块中的IP地址数量。在这里，16表示有2^(32-16)或65,536个唯一的地址。当您创建CIDR块时，必须确保所有IP地址都是连续的，块大小是2的幂，IP地址范围从`0.0.0.0`到`256.256.256.256`。
- en: 'For example, the CIDR block `10.117.50.0/22` has a total of 2^(32-22), or 1,024
    addresses. Now, if we would like to partition this network into four more networks
    with 256 addresses each, we could use the following CIDR blocks:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，CIDR 块 `10.117.50.0/22` 总共有 2^(32-22)，或 1,024 个地址。现在，如果我们想将这个网络划分为四个具有 256
    个地址的网络，我们可以使用以下 CIDR 块：
- en: '| `10.117.50.0.22` | `10.117.50.0/24` | 256 addresses |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| `10.117.50.0.22` | `10.117.50.0/24` | 256 个地址 |'
- en: '| `10.117.51.0/24` | 256 addresses |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| `10.117.51.0/24` | 256 个地址 |'
- en: '| `10.117.52.0/24` | 256 addresses |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| `10.117.52.0/24` | 256 个地址 |'
- en: '| `10.117.53.0/24` | 256 addresses |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| `10.117.53.0/24` | 256 个地址 |'
- en: Figure 3.8 – Example of using CIDR blocks to create four partitions on the network
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – 使用 CIDR 块在网络上创建四个分区的示例
- en: Great, now that we know how CIDR blocks work, let us apply the same to VPCs
    and subnets.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，现在我们知道了 CIDR 块是如何工作的，让我们将其应用到 VPC 和子网上。
- en: Networking for HPC workloads
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HPC 工作负载的联网
- en: 'Referring back to *Figure 3**.8*, we have made a few modifications to show
    CIDR blocks that define two subnets within **VPC1** in the following diagram:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾 *图 3.8*，我们在以下图中对 CIDR 块进行了一些修改，以显示定义 **VPC1** 内两个子网的 CIDR 块：
- en: '![Figure 3.9 – CIDR blocks used to define two subnets within VPC1](img/B18493_03_009.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.9 – 用于在 VPC1 中定义两个子网的 CIDR 块](img/B18493_03_009.jpg)'
- en: Figure 3.9 – CIDR blocks used to define two subnets within VPC1
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 用于在 VPC1 中定义两个子网的 CIDR 块
- en: 'As we can see in *Figure 3**.9*, VPC 1 has a CIDR block of `10.0.0.0/16` (amounting
    to 65,536 addresses), and the two subnets (`/24`) have allocated 256 addresses
    each. As you have already noticed, there are several unallocated addresses in
    this VPC, which can be used in the future for more subnets. Routing decisions
    are defined using a route table, as shown in the figure. Here, each subnet is
    considered to be private, as traffic originating from within the VPC cannot leave
    the VPC. This also means that resources within this VPC cannot, by default, access
    the internet. One way to allow resources from within a subnet to access the internet
    is to add an internet gateway. For allowing only outbound internet connection
    from a private subnet, you can use an NAT gateway. This is often a requirement
    for security-sensitive workloads. This modification results in the following change
    to our network diagram:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 3.9* 所示，VPC 1 有一个 CIDR 块 `10.0.0.0/16`（相当于 65,536 个地址），两个子网（`/24`）各自分配了
    256 个地址。如您已经注意到的，在这个 VPC 中有几个未分配的地址，将来可以用于更多子网。路由决策是通过路由表定义的，如图所示。这里，每个子网都被认为是私有的，因为来自
    VPC 内部的流量不能离开 VPC。这也意味着默认情况下，VPC 内部的资源不能访问互联网。允许来自子网内的资源访问互联网的一种方法是通过添加互联网网关。为了仅从私有子网允许出站互联网连接，您可以使用
    NAT 网关。这通常是安全敏感型工作负载的要求。这种修改导致我们的网络图发生了以下变化：
- en: '![Figure 3.10 – Adding an internet gateway to Subnet 1](img/B18493_03_010.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10 – 向子网 1 添加互联网网关](img/B18493_03_010.jpg)'
- en: Figure 3.10 – Adding an internet gateway to Subnet 1
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 向子网 1 添加互联网网关
- en: 'The main route table is associated with all subnets in the VPC, but we can
    also define custom route tables for each subnet. This defines whether the subnet
    is private, public, or VPN only. Now, if we need resources in **Subnet 2** to
    only access VPN resources in a corporate network via a **Virtual Private Gateway**
    (**VGW**) in *Figure 3**.11*, we can create two route tables and associate them
    with **Subnet 1** and **Subnet 2**, as shown in the following diagram:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 主要路由表与 VPC 中的所有子网相关联，但我们可以为每个子网定义自定义路由表。这定义了子网是私有、公共还是仅 VPN。现在，如果我们需要 **子网 2**
    中的资源仅通过 *图 3.11* 中的 **虚拟专用网关**（**VGW**）访问企业网络中的 VPN 资源，我们可以创建两个路由表并将它们与 **子网 1**
    和 **子网 2** 关联，如下面的图所示：
- en: '![Figure 3.11 – Adding a VGW to connect to on-premises resources](img/B18493_03_011.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.11 – 添加 VGW 以连接到本地资源](img/B18493_03_011.jpg)'
- en: Figure 3.11 – Adding a VGW to connect to on-premises resources
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 – 添加 VGW 以连接到本地资源
- en: 'A feature called **VPC peering** can be used in order to privately access resources
    in another VPC on AWS. With VPC peering, you can use a private networking connection
    between two VPCs to enable communication between them. For more information, you
    can visit [https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html](https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html).
    As shown in the following diagram, VPC peering allows resources in **VPC 1** and
    **VPC 2** to communicate with each other as though they are in the same network:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用一个名为 **VPC 对等连接** 的功能来私有地访问 AWS 上另一个 VPC 的资源。使用 VPC 对等连接，您可以在两个 VPC 之间使用私有网络连接来启用它们之间的通信。有关更多信息，您可以访问
    [https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html](https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html)。如图所示，VPC
    对等连接允许 **VPC 1** 和 **VPC 2** 中的资源相互通信，就像它们在同一个网络中一样：
- en: '![Figure 3.12 – Adding VPC peering and VPC endpoints](img/B18493_03_012.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.12 – 添加 VPC 对等连接和 VPC 端点](img/B18493_03_012.jpg)'
- en: Figure 3.12 – Adding VPC peering and VPC endpoints
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 – 添加 VPC 对等连接和 VPC 端点
- en: VPC peering can be done within VPCs in the same region or VPCs in different
    regions. A VPC endpoint allows resources from within a VPC (here, VPC 2) to access
    AWS services privately. Here, an **EC2** instance can make private API calls to
    services such as **Amazon** **S3**, **Kinesis**, or **SageMaker**. These are called
    **interface-type endpoints**. Gateway-type VPC endpoints are also available for
    Amazon S3 and DynamoDB, where you can further customize access control using policies
    (for example, bucket policies for Amazon S3).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: VPC 对等连接可以在同一区域的 VPC 内或不同区域的 VPC 内进行。VPC 端点允许 VPC 内部（此处为 VPC 2）的资源私有地访问 AWS
    服务。在此，一个 **EC2** 实例可以对 **Amazon** **S3**、**Kinesis** 或 **SageMaker** 等服务进行私有 API
    调用。这些被称为 **接口类型端点**。对于 Amazon S3 和 DynamoDB，也提供了网关类型 VPC 端点，您可以使用策略（例如，Amazon
    S3 的存储桶策略）进一步自定义访问控制。
- en: 'Large enterprise customers with workloads that run on-premises, as well as
    on the cloud, may have a setup similar to *Figure 3**.13*:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地和云上运行的工作负载的大型企业客户可能有一个类似于 *图 3**.13* 的设置：
- en: '![Figure 3.13 – Enterprise network architecture example](img/B18493_03_013.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.13 – 企业网络架构示例](img/B18493_03_013.jpg)'
- en: Figure 3.13 – Enterprise network architecture example
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 – 企业网络架构示例
- en: Each corporate location may be connected to AWS by using **Direct Connect**
    (a service for creating dedicated network connections to AWS with a VPN backup.
    Private subnets may host single or clusters of EC2 instances for large, permanent
    workloads. The cluster of EC2 instances is placed in a multi-AZ autoscaling group
    so that the workload can recover from the unlikely event of an AZ failure, and
    a minimum number of EC2 instances is maintained.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 每个企业位置都可以通过使用 **Direct Connect**（一个用于创建与 AWS 的专用网络连接并带有 VPN 备份的服务）连接到 AWS。私有子网可以托管大型、永久性工作负载的单个或集群的
    EC2 实例。EC2 实例集群放置在多可用区自动扩展组中，以便工作负载可以从 AZ 故障的不太可能事件中恢复，并保持最小数量的 EC2 实例。
- en: For ephemeral workloads, managed services such as EKS, Glue, or SageMaker can
    be used. In the preceding diagram, a private **EKS** cluster is placed in VPC
    2\. Since internet access is disabled by default, all container images must be
    local to the VPC or copied onto an ECR repository; that is, you cannot use an
    image from Docker Hub. To publish logs and save checkpoints, VPC endpoints are
    required in VPC 2 to connect to the Amazon S3 and CloudWatch services. Data stores
    and databases are not discussed in this diagram but are important considerations
    in hybrid architectures. This is because some data cannot leave the corporate
    network but may be anonymized and replicated on AWS temporarily.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于短暂的工作负载，可以使用 EKS、Glue 或 SageMaker 等托管服务。在前面的图中，一个私有的 **EKS** 集群放置在 VPC 2 中。由于默认情况下禁用了互联网访问，所有容器镜像都必须位于
    VPC 内或复制到 ECR 存储库中；也就是说，您不能使用 Docker Hub 上的镜像。要发布日志和保存检查点，需要在 VPC 2 中使用 VPC 端点连接到
    Amazon S3 和 CloudWatch 服务。数据存储和数据库在此图中未讨论，但在混合架构中是重要的考虑因素。这是因为某些数据不能离开企业网络，但可以在
    AWS 上临时匿名化和复制。
- en: Typically, this temporary data on AWS is used for analytics purposes before
    getting deleted. Lastly, hybrid architectures may also involve **AWS Outposts**,
    which is a fully managed service that extends AWS services, such as EC2, ECS,
    EKS, S3, EMR, **Relational Database Service** (**RDS**) and so on, to on-premises.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些在 AWS 上的临时数据用于分析目的，然后被删除。最后，混合架构也可能涉及 **AWS Outposts**，这是一个完全托管的服务，它扩展了
    AWS 服务，例如 EC2、ECS、EKS、S3、EMR、**关系数据库服务** (**RDS**) 等等，到本地。
- en: Selecting the right compute for HPC workloads
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have learned about the foundations of compute and network on AWS,
    we are ready to explore some typical architectural patterns for compute on AWS.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'Selecting the right compute for HPC and ML applications involves considering
    the rest of the architecture you are designing, and therefore involves all aspects
    of the Well-Architected Framework:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Operational excellence
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliability
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance efficiency
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost optimization
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cover best practices across these pillars at the end of this section, but
    first, we will start with the most basic pattern of computing on AWS and add complexity
    as we progress.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Pattern 1 – a standalone instance
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many HPC applications that are built for simulations, financial services, CFD,
    or genomics can run on a single EC2 instance as long as the right instance type
    is selected. We discussed many of these instance-type options in the *Introducing
    AWS compute ecosystem* section. As shown in the following diagram, a **CloudFormation
    Template** can be used to launch an **EC2 Instance** in a VPC, and **Secure Shell**
    (**SSH**) access can be provided to the user for installing and using software
    on this instance:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – CloudFormation Template used to launch an EC2 Instance inside
    a VPC](img/B18493_03_014.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – CloudFormation Template used to launch an EC2 Instance inside
    a VPC
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will describe a pattern that uses AWS ParallelCluster.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Pattern 2 – using AWS ParallelCluster
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**AWS ParallelCluster** can be used to provision a cluster with head and worker
    nodes for massive-scale parallel processing or HPC. ParallelCluster, once launched,
    will be similar to on-premises HPC clusters with the added benefits of security
    and scalability in the cloud. These clusters can be permanent or provisioned and
    de-provisioned on an as-needed basis. On AWS, a user can use the AWS ParallelCluster
    **Command-Line Interface** (**CLI**) to create a cluster of EC2 instances on the
    fly. AWS CloudFormation is used to launch the infrastructure, including required
    networking, storage, and AMI configurations. As the user (or multiple users) submit
    jobs through the job scheduler, more instances are provisioned and de-provisioned
    in the autoscaling group, as shown in the following diagram:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Using AWS ParallelCluster for distributed workloads on AWS](img/B18493_03_015.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Using AWS ParallelCluster for distributed workloads on AWS
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Once the user is done with using the cluster for their HPC workloads, they can
    use the CLI or CloudFormation APIs to delete all resources created. As a modification
    to what is suggested in the following architecture, you can replace the **head/master**
    EC2 node with an Amazon SQS queue to get a queue-based architecture for typical
    HPC workloads.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss how you can use AWS Batch.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Pattern 3 – using AWS Batch
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS Batch helps run HPC and big data-based applications that are based on unconnected
    input configurations or files without the need to manage infrastructure. To submit
    a job to AWS batch, you package your application as a container and use the CLI
    or supported APIs to define and submit a job. With AWS Batch, you can get started
    quickly by using default job configurations, a built-in job queue, and integration
    with workflow services such as AWS Step Functions and Luigi.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Batch可以帮助运行基于未连接输入配置或文件的HPC和大数据应用程序，无需管理基础设施。要将作业提交到AWS批处理，您需要将应用程序打包为容器，并使用CLI或支持的API定义和提交作业。使用AWS
    Batch，您可以通过使用默认作业配置、内置作业队列以及与AWS Step Functions和Luigi等工作流服务的集成来快速开始。
- en: 'As you can see in the following screenshot, the user first defines a **Docker
    image** (much like the image we discussed in the section on containers) and then
    registers this image with **Amazon ECR**. Then, the user can create a job definition
    in **AWS Batch** and submit one or more jobs to the job queue. Input data can
    be pulled from **Amazon S3**, and output data can be written to a different location
    on Amazon S3:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下截图所示，用户首先定义一个**Docker镜像**（类似于我们在容器部分讨论的镜像），然后将其注册到**Amazon ECR**。然后，用户可以在**AWS
    Batch**中创建一个作业定义，并将一个或多个作业提交到作业队列。输入数据可以从**Amazon S3**中提取，输出数据可以写入Amazon S3上的不同位置：
- en: '![Figure 3.16 – Using AWS Batch along with AWS EC2 instances for batch workloads](img/B18493_03_016.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图3.16 – 使用AWS Batch和AWS EC2实例进行批量工作负载](img/B18493_03_016.jpg)'
- en: Figure 3.16 – Using AWS Batch along with AWS EC2 instances for batch workloads
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 – 使用AWS Batch和AWS EC2实例进行批量工作负载
- en: Next, we will discuss patterns that help with hybrid architectures on AWS.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论有助于AWS混合架构的模式。
- en: Pattern 4 – hybrid architecture
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式4 – 混合架构
- en: 'Customers who have already invested in large on-premises clusters, and who
    also want to make use of the on-demand, highly scalable, and secure AWS environment
    for their jobs, generally opt for a hybrid approach. In this approach, organizations
    decide to do one of the following:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 已经投资于大型本地集群的客户，同时希望利用按需、高度可扩展和安全的AWS环境进行工作，通常会选择混合方法。在这种方法中，组织决定执行以下操作之一：
- en: Run a particular job type on AWS and keep the rest on-premises
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在AWS上运行特定类型的作业，其余的保持在本地
- en: Use AWS for overflow/excess capacity
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS处理溢出/多余容量
- en: Use on-premises as primary data storage for security reasons, or place only
    the scheduler or job monitors on-premises with all of the compute being done on
    AWS
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于安全原因，将本地作为主要数据存储，或者仅在本地放置调度器或作业监控器，所有计算都在AWS上完成
- en: Run small, test, or development jobs on-premises, but larger production jobs
    using high-performance or high-memory instances on AWS
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地运行小型、测试或开发作业，但使用AWS上的高性能或高内存实例运行较大的生产作业
- en: 'On-premises data can be transferred to Amazon S3 using a software agent called
    DataSync (see [https://docs.aws.amazon.com/datasync/latest/userguide/working-with-agents.html](https://docs.aws.amazon.com/datasync/latest/userguide/working-with-agents.html)).
    Clusters that use Lustre’s shared high-performance file system on-premises can
    make use of Amazon FSx for Lustre on AWS (for more information, see [https://aws.amazon.com/fsx/lustre/](https://aws.amazon.com/fsx/lustre/)).
    The following diagram is a reference architecture for hybrid workloads:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用名为DataSync的软件代理将本地数据传输到Amazon S3（有关信息，请参阅[https://docs.aws.amazon.com/datasync/latest/userguide/working-with-agents.html](https://docs.aws.amazon.com/datasync/latest/userguide/working-with-agents.html)）。在本地使用Lustre共享高性能文件系统的集群可以利用AWS上的Amazon
    FSx for Lustre（更多信息，请参阅[https://aws.amazon.com/fsx/lustre/](https://aws.amazon.com/fsx/lustre/)）。以下图是混合工作负载的参考架构：
- en: '![Figure 3.17 – Using FSx, S3, and AWS DataSync for hybrid architectures](img/B18493_03_017.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图3.17 – 使用FSx、S3和AWS DataSync进行混合架构](img/B18493_03_017.jpg)'
- en: Figure 3.17 – Using FSx, S3, and AWS DataSync for hybrid architectures
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 – 使用FSx、S3和AWS DataSync进行混合架构
- en: Next, we will discuss patterns for container-based distributed processing
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论基于容器的分布式处理模式
- en: Pattern 5 – Container-based distributed processing
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式5 – 基于容器的分布式处理
- en: 'The following diagram is a reference architecture for container-based distributed
    processing workflows that are suited for HPC and other related applications:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图是适合HPC和其他相关应用的基于容器的分布式处理工作流的参考架构：
- en: '![Figure 3.18 – EKS-based architecture for distributed computing](img/B18493_03_018.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图3.18 – 基于EKS的分布式计算架构](img/B18493_03_018.jpg)'
- en: Figure 3.18 – EKS-based architecture for distributed computing
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 – 基于EKS的分布式计算架构
- en: Admins can use command-line tools such as *eksctl* or CloudFormation to provision
    resources. Pods that are one or more containers can be run on managed EC2 nodes
    of your choice or via the AWS Fargate service. EMR on EKS can also be used to
    run open source, big data applications (for example, based on Spark) directly
    on EKS-managed nodes. In all of the preceding cases, containers that are provided
    by AWS can be used as a baseline, or completely custom containers that you build
    and push to ECR may be used. Applications running in EKS pods can access data
    from Amazon S3, Redshift, DynamoDB, or a host of other services and applications.
    To learn more about EKS, Fargate, or EMR on EKS, please take a look at the links
    provided in the *References* section.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 管理员可以使用如 *eksctl* 或 CloudFormation 这样的命令行工具来配置资源。可以运行一个或多个容器的Pod可以在您选择的托管EC2节点上运行，或者通过AWS
    Fargate服务运行。EKS上的EMR也可以用来在EKS管理的节点上直接运行开源的大数据应用（例如，基于Spark）。在所有上述情况下，都可以使用AWS提供的容器作为基础，或者使用您构建并推送到ECR的完全定制的容器。在EKS
    Pod中运行的应用可以访问Amazon S3、Redshift、DynamoDB或其他众多服务和应用的数据。要了解更多关于EKS、Fargate或EKS上的EMR的信息，请参阅*参考*部分提供的链接。
- en: Pattern 6 – serverless architecture
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式6 – 无服务器架构
- en: 'The following diagram is an example of serverless architecture that can be
    used for real-time, serverless processing, analytics, and business intelligence:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示是一个用于实时、无服务器处理、分析和商业智能的无服务器架构示例：
- en: '![Figure 3.19 – Architecture for real-time, serverless processing and business
    analytics](img/B18493_03_019.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图3.19 – 实时、无服务器处理和商业分析架构](img/B18493_03_019.jpg)'
- en: Figure 3.19 – Architecture for real-time, serverless processing and business
    analytics
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.19 – 实时、无服务器处理和商业分析架构
- en: First, Kinesis Data Streams captures data from one or more data producers. Next,
    **Kinesis Data Analytics** can be used to build real-time applications for transforming
    this incoming data using SQL, Java, Python, or Scala. Data can also be interactively
    processed using managed **Apache Zeppelin** notebooks ([https://zeppelin.apache.org/](https://zeppelin.apache.org/)).
    In this case, a **Lambda Function** is being used to continuously post-process
    the output of the **Kinesis Analytics** application before dropping a filtered
    set of results into the serverless, NoSQL database **DynamoDB**.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Kinesis Data Streams从一个或多个数据生产者捕获数据。接下来，可以使用**Kinesis Data Analytics**来构建实时应用，使用SQL、Java、Python或Scala转换这些传入的数据。数据还可以通过管理的**Apache
    Zeppelin**笔记本（[https://zeppelin.apache.org/](https://zeppelin.apache.org/)）进行交互式处理。在这种情况下，正在使用**Lambda函数**来持续后处理**Kinesis
    Analytics**应用程序的输出，然后将过滤后的结果集放入无服务器、NoSQL数据库**DynamoDB**中。
- en: Simultaneously, the **Kinesis Firehose** component is being used to save incoming
    data into S3, which is then processed by several other serverless components such
    as **AWS Glu** and **AWS Lambda**, and orchestrated using **AWS Step Functions**.
    With AWS Glue, you can run serverless **Extract-Transform-Load** (**ETL**) applications
    that are written in familiar languages such as SQL or Spark. You can then save
    the output of Glue transform jobs to data stores such as Amazon S3 or Amazon Redshift.
    ML applications that run on Amazon SageMaker can also make use of the output data
    from real-time streaming analytics.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，**Kinesis Firehose**组件被用来将传入的数据保存到S3，然后由其他几个无服务器组件如**AWS Glu**和**AWS Lambda**进行处理，并使用**AWS
    Step Functions**进行编排。使用AWS Glu，您可以运行用熟悉的语言如SQL或Spark编写的无服务器**提取-转换-加载**（**ETL**）应用。然后，您可以将Glue转换作业的输出保存到数据存储如Amazon
    S3或Amazon Redshift。在Amazon SageMaker上运行的ML应用也可以利用实时流分析输出的数据。
- en: Once the data is transformed, it is ready to be queried interactively using
    **Amazon Athena**. Amazon Athena makes it possible for you to query data that
    resides in Amazon S3 using standard SQL commands. Athena is also directly integrated
    with the Glue Data Catalog, which makes it much easier to work with these two
    services without the additional burden of writing ETL jobs or scripts to enable
    this connection. Athena is built on the open source library **Presto** ([https://prestodb.io/](https://prestodb.io/))
    and can be used to query a variety of standard formats such as CSV, JSON, Parquet,
    and Avro. With Athena Federated data sources, you can use a visualization tool
    such as **Amazon QuickSight** to run complex SQL queries.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被转换，就可以使用 **Amazon Athena** 进行交互式查询。Amazon Athena 使你能够使用标准 SQL 命令查询存储在 Amazon
    S3 中的数据。Athena 还直接集成到 Glue 数据目录中，这使得在没有额外负担编写 ETL 作业或脚本以启用此连接的情况下，与这两个服务一起工作变得更加容易。Athena
    基于开源库 **Presto** ([https://prestodb.io/](https://prestodb.io/))，可以用于查询各种标准格式，如
    CSV、JSON、Parquet 和 Avro。使用 Athena 联邦数据源，你可以使用可视化工具如 **Amazon QuickSight** 来运行复杂的
    SQL 查询。
- en: Rather than using a dataset to visualize outputs, QuickSight, when configured
    correctly, can directly send these SQL queries to Athena. The results of the query
    can then be directly visualized interactively using multiple chart types and organized
    into a dashboard. These dashboards can then be shared with business analysts for
    further research.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用数据集来可视化输出不同，当配置正确时，QuickSight 可以直接将这些 SQL 查询发送到 Athena。查询的结果可以随后通过多种图表类型直接进行交互式可视化，并组织到仪表板中。然后，这些仪表板可以与业务分析师共享以进行进一步研究。
- en: In this section, we have covered various patterns around the topic of compute
    on AWS. Although this is not an exhaustive list of patterns, this should give
    you a basic idea of the components or services used and how these components are
    connected to each other to achieve different requirements. Next, we will describe
    some best practices related to HPC on AWS.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了围绕 AWS 计算主题的各种模式。尽管这不是模式的完整列表，但这应该能给你一个基本的概念，了解所使用的组件或服务以及这些组件是如何相互连接以实现不同需求的。接下来，我们将描述一些与
    AWS 上 HPC 相关的最佳实践。
- en: Best practices for HPC workloads
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HPC 工作负载的最佳实践
- en: 'The AWS Well-Architected Framework helps with the architecting of secure, cost-effective,
    resilient, and high-performing applications and workloads on the cloud. It is
    the go-to reference when building any application. Details about the AWS Well-Architected
    Framework can be obtained at [https://aws.amazon.com/architecture/well-architected/](https://aws.amazon.com/architecture/well-architected/).
    However, applications in certain domains and verticals require further scrutiny
    and have details that need to be handled differently from the generic guidance
    that the AWS Well-Architected Framework provides. Thus, we have many other documents
    called *lenses* that provide best practice guidance; some of these lenses that
    are relevant to our current discussion are listed as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 架构良好框架有助于在云上构建安全、经济高效、弹性好和性能高的应用程序和工作负载。它是构建任何应用程序时的首选参考资料。有关 AWS 架构良好框架的详细信息，请参阅
    [https://aws.amazon.com/architecture/well-architected/](https://aws.amazon.com/architecture/well-architected/)。然而，某些领域和垂直领域的应用程序需要进一步的审查，并且需要以与
    AWS 架构良好框架提供的通用指南不同的方式处理详细信息。因此，我们有许多其他称为 *透镜* 的文档，提供最佳实践指导；以下列出了一些与我们当前讨论相关的透镜：
- en: '*Data Analytics Lens* – well-architected lens for data analytics workloads
    ([https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/analytics-lens.html?did=wp_card&trk=wp_card](https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/analytics-lens.html?did=wp_card&trk=wp_card))'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据分析透镜* – 适用于数据分析工作负载的架构良好透镜 ([https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/analytics-lens.html?did=wp_card&trk=wp_card](https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/analytics-lens.html?did=wp_card&trk=wp_card))'
- en: '*Serverless Lens* – focusing on architecting serverless applications on AWS
    ([https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/welcome.html](https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/welcome.html))'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无服务器透镜* – 专注于在 AWS 上构建无服务器应用程序 ([https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/welcome.html](https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/welcome.html))'
- en: '*ML Lens* – for ML workloads on AWS ([https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/welcome.html?did=wp_card&trk=wp_card](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/welcome.html?did=wp_card&trk=wp_card))'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ML视角* – 用于AWS上的ML工作负载 ([https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/welcome.html?did=wp_card&trk=wp_card](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/welcome.html?did=wp_card&trk=wp_card))'
- en: '*HPC Lens* – focusing on HPC workloads on AWS ([https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/welcome.html?did=wp_card&trk=wp_card](https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/welcome.html?did=wp_card&trk=wp_card))'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HPC视角* – 关注AWS上的HPC工作负载 ([https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/welcome.html?did=wp_card&trk=wp_card](https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/welcome.html?did=wp_card&trk=wp_card))'
- en: 'While it is out of the scope of this book to go over best practices from the
    generic AWS Well-Architected Framework, as well as these individual lenses, we
    will list some common, important design considerations that are relevant to our
    current topic of HPC and ML:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书的范围不包括概述通用的AWS Well-Architected框架的最佳实践，以及这些个别视角，但我们将列出一些与当前主题HPC和ML相关的重要设计考虑因素：
- en: Both HPC and ML applications evolve over time. Organizations that freeze an
    architecture for several years in advance tend to be the ones that resist change
    and are later impacted by even larger costs to accommodate new requirements. In
    general, it is best practice to avoid static architectures, as the original requirements
    may evolve quickly. When there is a need to run more training jobs, or more HPC
    simulations, the architecture must allow scaling out and increase overall performance,
    but also return back to a steady, low-cost state when the demand is lower.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HPC和ML应用程序都会随着时间的推移而发展。那些提前几年冻结架构的组织往往是对变化持抵制态度的，并且后来会受到更大的成本影响，以适应新的需求。一般来说，避免静态架构是最佳实践，因为原始需求可能会迅速变化。当需要运行更多训练作业或更多HPC模拟时，架构必须允许扩展并提高整体性能，但在需求较低时也能回到稳定、低成本的状态。
- en: On AWS, compute clusters can be right-sized at any given point in time, and
    the use of managed services can help with provisioning resources on the fly. For
    example, Amazon SageMaker allows users to provision various instance types for
    training without the undifferentiated heavy lifting of maintaining clusters or
    infrastructure. Customers only need to choose the framework of interest, point
    to training data in Amazon S3, and use the APIs to start, monitor, and stop training
    jobs. Customers only pay for what they use and don’t pay for any idle time.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS上，计算集群可以在任何给定时间点进行适当规模调整，使用托管服务可以帮助动态配置资源。例如，Amazon SageMaker允许用户为训练配置各种实例类型，无需承担维护集群或基础设施的繁重工作。客户只需选择感兴趣的框架，指向Amazon
    S3中的训练数据，并使用API启动、监控和停止训练作业。客户只需为使用的部分付费，无需为任何闲置时间付费。
- en: 'Architecting to encourage and enable collaboration can make a significant difference
    to the productivity of the team running HPC and ML workloads on AWS. With teams
    that are becoming remote and global, the importance of effective collaboration
    cannot be understated. To improve collaboration, it is important to do the following:'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建旨在鼓励和启用协作的架构，可以显著提高在AWS上运行HPC和ML工作负载的团队的生产力。随着团队变得越来越远程和全球化，有效协作的重要性不容低估。为了提高协作，以下事项很重要：
- en: Track experiments using an experiment tracking tool.
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用实验跟踪工具跟踪实验。
- en: Enable sharing of resources such as configuration files, CloudFormation templates,
    pipeline definitions, code, notebooks, and data.
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用资源共享，例如配置文件、CloudFormation模板、管道定义、代码、笔记本和数据。
- en: Enable automation and use tools for continuous integration, continuous delivery,
    continuous monitoring, continuous training, and continuous improvement.
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用自动化，并使用工具进行持续集成、持续交付、持续监控、持续训练和持续改进。
- en: Make sure that work is reproducible – this means that the inputs, environmental
    configuration, and packages can be easily reused and the outputs of a batch process
    can be verified. This helps to track changes, with audits, and to maintain high
    standards.
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保工作可重复 – 这意味着输入、环境配置和包可以轻松重用，批量处理的输出可以验证。这有助于跟踪变更，进行审计，并保持高标准。
- en: Use ephemeral resources from managed services when possible. Again, this is
    applicable to both HPC and ML. When considering hybrid architectures or when migrating
    an on-premises workload to AWS, it is no longer necessary to completely replicate
    the workload on AWS. For example, running Spark-based workloads can be done on
    AWS Glue without the need to provision an entire EMR cluster. Similarly, you can
    run ML training or inference without handling the underlying clusters using Amazon
    SageMaker APIs.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider both performance and cost when right-sizing your resources. For workloads
    that are not time-sensitive, using Spot instance on AWS is the simplest cost optimization
    strategy to follow. For HPC applications, running workloads on EC2 spot instances
    or using spot fleets for containerized workloads on EKS or Fargate can provide
    a discount of up to 90% over on-demand instances of the same type.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On SageMaker, using Spot instances is very simple – you just need to pass an
    argument to supported training APIs. On the other hand, for high-performance workloads,
    it is important to prioritize on-demand instances over spot instances so that
    the results of simulations or ML training jobs can be returned and analyzed in
    a timely manner. When choosing services or applications to use for your HPC or
    ML workloads, prefer pay-as-you-go pricing over licensing and upfront costs.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Consider cost optimization and performance for the entire pipeline. It is typical
    for both HPC and ML applications to be designed over a pipeline – for example,
    data transfer, pre-processing, training or simulation, post-processing, and visualization.
    It is possible that some steps require less compute than others. Also, making
    a decision upfront about data formats or locations may force downstream steps
    to be more expensive in terms of time, processing resources, or cost.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on making small and frequent changes, building modular components, and
    testing in an automated fashion. Reducing the level of manual intervention and
    architecting the workload so that it does not require any downtown for maintenance
    is a best practice.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For both HPC and ML, use software packages and tools that provide good documentation
    and support. This choice needs to be made carefully upfront, since several architectural,
    design, and team decisions may need to change based on this. For example, when
    choosing an ML framework such as PyTorch, it is important to be familiar with
    services on AWS that support this framework and hire a team that is well-versed
    in this particular framework to ensure success.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly in HPC, the choice of software that does molecular dynamics simulations
    will decide the scale of simulations that can be done, which services are compatible
    with the package on AWS, and which team members are trained and ready to make
    use of this software set up on AWS.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Prioritize security and establish security best practices before beginning
    to develop multiple workloads or applications. These best practice areas under
    security are discussed in great detail in the AWS Well-Architected Framework and
    several of the lenses. Here, we outline the major sub-topics for completeness:'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identity and Access** **Management** (**IAM**)'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Detective controls
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Preventive controls
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure protection
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data protection
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Incident response
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is normal to expect a complex architecture system to fail, but the best practice
    is to respond quickly and recover from these failures. Checkpointing is a common
    feature that is built into HPC and ML applications. A common idea is to checkpoint
    data or progress (or both) to a remote S3 location where a simulation or a training
    job can pick up after a failure. Checkpointing becomes even more important when
    using spot instances. When managing infrastructure on your own, you have the flexibility
    to deploy the application to multiple availability zones when extremely low latency
    requirements do not need to be met. Managed services take care of maintaining
    and updating instances and containers that run on these instances.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure the cluster is dynamic, can be used by multiple users simultaneously,
    and is designed to work over large amounts of data. In order to design the cluster
    successfully, use cloud native technologies to test applications and packages
    over a meaningful use case and not a toy problem. With the cloud, you have the
    ability to spin up and spin down ephemeral clusters to test out your use case
    at a low cost, while also making sure that a production-sized workload will work
    smoothly and as expected.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we have listed some best practices for HPC workloads on AWS.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first described the AWS Compute ecosystem, including the
    various types of EC2 instances, as well as container-based services (Fargate,
    ECS, and EKS), and serverless compute options (AWS Lambda). We then introduced
    networking concepts on AWS and applied them to typical workloads using a visual
    walk-through. To help guide you through selecting the right compute for HPC workloads,
    we described several typical patterns including standalone, self-managed instances,
    AWS ParallelCluster, AWS Batch, hybrid architectures, container-based architectures,
    and completely serverless architectures for HPC. Lastly, we discussed various
    best practices that may further help you right-size your instances and clusters
    and apply the Well-Architected Framework to your workloads.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will outline the various storage services that can be
    used on AWS for HPC and ML workloads.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For additional information on the topics covered in this chapter, please navigate
    to the following pages:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/products/compute/](https://aws.amazon.com/products/compute/)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/what-is/compute/](https://aws.amazon.com/what-is/compute/)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/hpc/?pg=ln&sec=uc](https://aws.amazon.com/hpc/?pg=ln&sec=uc)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.amazonaws.cn/en/ec2/instance-types/](https://www.amazonaws.cn/en/ec2/instance-types/)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-explorer/](https://aws.amazon.com/ec2/instance-explorer/)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/hpc6/](https://aws.amazon.com/ec2/instance-types/hpc6/)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/hpc/parallelcluster/](https://aws.amazon.com/hpc/parallelcluster/)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/c5/](https://aws.amazon.com/ec2/instance-types/c5/)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/c6g/](https://aws.amazon.com/ec2/instance-types/c6g/)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/c6i/](https://aws.amazon.com/ec2/instance-types/c6i/)'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/m5/](https://aws.amazon.com/ec2/instance-types/m5/)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html#gpu-instances](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html#gpu-instances)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/aws/aws-neuron-sdk](https://github.com/aws/aws-neuron-sdk)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instances-and-amis.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instances-and-amis.html)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/](https://aws.amazon.com/ec2/instance-types/)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/a1/](https://aws.amazon.com/ec2/instance-types/a1/)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://developer.nvidia.com/blog/even-easier-introduction-cuda/](https://developer.nvidia.com/blog/even-easier-introduction-cuda/)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/p4/](https://aws.amazon.com/ec2/instance-types/p4/)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/](https://aws.amazon.com/ec2/instance-types/)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/p4/](https://aws.amazon.com/ec2/instance-types/p4/)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/ansys-fluent/](https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/ansys-fluent/)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.nvidia.com/en-in/data-center/a100/](https://www.nvidia.com/en-in/data-center/a100/)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://pytorch.org/docs/stable/notes/cuda.html](https://pytorch.org/docs/stable/notes/cuda.html)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/f1/](https://aws.amazon.com/ec2/instance-types/f1/)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/aws/aws-fpga](https://github.com/aws/aws-fpga)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage-optimized-instances.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage-optimized-instances.html)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/i3/](https://aws.amazon.com/ec2/instance-types/i3/)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/r6g/](https://aws.amazon.com/ec2/instance-types/r6g/)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/high-memory/](https://aws.amazon.com/ec2/instance-types/high-memory/)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/x1/](https://aws.amazon.com/ec2/instance-types/x1/)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/x1e/](https://aws.amazon.com/ec2/instance-types/x1e/)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/x2g/](https://aws.amazon.com/ec2/instance-types/x2g/)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.hpcworkshops.com/](https://www.hpcworkshops.com/)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/instance-types/z1d/](https://aws.amazon.com/ec2/instance-types/z1d/)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/containers/](https://aws.amazon.com/containers/)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ec2/?c=cn&sec=srv](https://aws.amazon.com/ec2/?c=cn&sec=srv)'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/containers/?nc1=f_cc](https://aws.amazon.com/containers/?nc1=f_cc)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/fargate/?c=cn&sec=srv](https://aws.amazon.com/fargate/?c=cn&sec=srv)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/serverless/?nc2=h_ql_prod_serv](https://aws.amazon.com/serverless/?nc2=h_ql_prod_serv)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/eks/?c=cn&sec=srv](https://aws.amazon.com/eks/?c=cn&sec=srv)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/ecs/?c=cn&sec=srv](https://aws.amazon.com/ecs/?c=cn&sec=srv)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/lambda/](https://aws.amazon.com/lambda/)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/big-data/accessing-and-visualizing-data-from-multiple-data-sources-with-amazon-athena-and-amazon-quicksight/](https://aws.amazon.com/blogs/big-data/accessing-and-visualizing-data-from-multiple-data-sources-with-amazon-athena-and-amazon-quicksight/)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/glue/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc](https://aws.amazon.com/glue/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-output-lambda.html](https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-output-lambda.html)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/kinesis/data-analytics/](https://aws.amazon.com/kinesis/data-analytics/)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/athena/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc](https://aws.amazon.com/athena/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/emr/features/eks/](https://aws.amazon.com/emr/features/eks/)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/pod-templates.html](https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/pod-templates.html)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks.html](https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks.html)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/big-data/orchestrate-an-amazon-emr-on-amazon-eks-spark-job-with-aws-step-functions/](https://aws.amazon.com/blogs/big-data/orchestrate-an-amazon-emr-on-amazon-eks-spark-job-with-aws-step-functions/)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/about-aws/global-infrastructure/regions_az/](https://aws.amazon.com/about-aws/global-infrastructure/regions_az/)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://d1.awsstatic.com/whitepapers/computational-fluid-dynamics-on-aws.pdf?cmptd_hpc3](https://d1.awsstatic.com/whitepapers/computational-fluid-dynamics-on-aws.pdf?cmptd_hpc3)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/general-design-principles.html](https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/general-design-principles.html)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/well-architected-machine-learning-design-principles.html](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/well-architected-machine-learning-design-principles.html)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/outposts/latest/userguide/what-is-outposts.html](https://docs.aws.amazon.com/outposts/latest/userguide/what-is-outposts.html)'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html)'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat.html](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat.html)'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/vpc/latest/userguide/security.html](https://docs.aws.amazon.com/vpc/latest/userguide/security.html)'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
