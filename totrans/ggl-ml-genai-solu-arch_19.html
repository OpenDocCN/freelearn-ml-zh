<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer207" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-321"><a id="_idTextAnchor383" class="calibre6 pcalibre pcalibre1"/>16</h1>
<h1 id="_idParaDest-322" class="calibre5"><a id="_idTextAnchor384" class="calibre6 pcalibre pcalibre1"/>Advanced Generative AI Concepts and Use Cases</h1>
<p class="calibre3">Now that we’ve covered the<a id="_idIndexMarker1897" class="calibre6 pcalibre pcalibre1"/> basics of <strong class="bold">generative AI</strong> (<strong class="bold">GenAI</strong>), it’s time to start diving deeper. In this chapter, we will cover more advanced topics in the field of GenAI. We’ll begin by learning about some techniques to tune and optimize generative models for specific domains or tasks. Then, we’ll dive into more detail on the important topics of embeddings and vector databases and how they relate to a relatively new pattern <a id="_idIndexMarker1898" class="calibre6 pcalibre pcalibre1"/>of using <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) to ground our <strong class="bold">large language model</strong> (<strong class="bold">LLM</strong>) responses <a id="_idIndexMarker1899" class="calibre6 pcalibre pcalibre1"/>in our own data. Next, we’ll discuss multimodal models, how they differ from standard, text-based LLMs, and the kinds of use cases they support. Finally, we will introduce LangChain, which is a popular framework that enables us to design complex applications that can build on the functionalities provided by LLMs. Specifically, this chapter covers the <span>following topics:</span></p>
<ul class="calibre16">
<li class="calibre8">Advanced tuning and <span>optimization techniques</span></li>
<li class="calibre8">Embeddings and <span>vector databases</span></li>
<li class="calibre8"><span>RAG</span></li>
<li class="calibre8"><span>Multimodal models</span></li>
<li class="calibre8">GenAI <span>model evaluation</span></li>
<li class="calibre8"><span>LangChain</span></li>
</ul>
<p class="calibre3">Let’s dive right in and start learning about advanced tuning and <span>optimization techniques!</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The term <em class="italic">LLM</em> has become almost synonymous with the field of GenAI in general, but considering that <strong class="bold">LLM</strong> stands for <strong class="bold">large language model</strong>, the term technically relates them to language processing. It’s important to note that there are other types of GenAI models, such as some image generation and multimodal models, that are not strictly language models. Additionally, a more recent concept of <strong class="bold">large action models</strong> or <strong class="bold">large agentic models</strong> (<strong class="bold">LAMs</strong>) has <a id="_idIndexMarker1900" class="calibre6 pcalibre pcalibre1"/>emerged, which combine the language understanding and generation capabilities of LLMs with the ability to perform actions (such as planning and booking a vacation) by using tools and agents to interact with their environment. However, for simplicity, throughout the rest of this book, I will use the terms <em class="italic">LLM</em> and <em class="italic">GenAI </em><span><em class="italic">model</em></span><span> interchangeably.</span></p>
<h1 id="_idParaDest-323" class="calibre5"><a id="_idTextAnchor385" class="calibre6 pcalibre pcalibre1"/>Advanced tuning and optimization techniques</h1>
<p class="calibre3">At the end of the <a id="_idIndexMarker1901" class="calibre6 pcalibre pcalibre1"/>previous chapter, we discussed LLMs and how they are trained and tuned. I mentioned some of the tuning approaches at a high level, and in this section, we will dive deeper into how we can tune LLMs to more adequately address our specific needs. Let’s set the stage by outlining how we interact with LLMs in the first place, which we generally <a id="_idIndexMarker1902" class="calibre6 pcalibre pcalibre1"/>do <span>via </span><span><strong class="bold">prompts</strong></span><span>.</span></p>
<p class="callout-heading">Definition</p>
<p class="callout">A prompt is a piece of text or instruction that we provide to an LLM to guide its response or output. It tells the LLM what to do and, in some cases, provides guidance on how to do it; for example, “<strong class="source-inline1">summarize this financial document, specifically focusing on details relating to company performance in </strong><span><strong class="source-inline1">Q4, 2023</strong></span><span>.”</span></p>
<p class="calibre3">The first LLM tuning technique we’ll explore is <span><strong class="bold">prompt engineering</strong></span><span>.</span></p>
<h2 id="_idParaDest-324" class="calibre9"><a id="_idTextAnchor386" class="calibre6 pcalibre pcalibre1"/>Prompt engineering</h2>
<p class="calibre3">Prompts are<a id="_idIndexMarker1903" class="calibre6 pcalibre pcalibre1"/> the most straightforward method we<a id="_idIndexMarker1904" class="calibre6 pcalibre pcalibre1"/> can use to tune an LLM’s outputs to our specific needs. In fact, during the early days of the GenAI popularity explosion that happened in late 2022 and early 2023, you may recall seeing news headlines about a new type of extremely high-demand role that was emerging, called <strong class="bold">prompt engineer</strong>. In this section, we’ll<a id="_idIndexMarker1905" class="calibre6 pcalibre pcalibre1"/> discuss what prompt engineering is and how it can be used to improve the outputs of GenAI models, which will help provide context on why there was such a sudden increase in demand for talented people in <span>this space.</span></p>
<p class="calibre3">To begin our discussion, consider the <span>following prompt:</span></p>
<pre class="source-code">
Write a poem about nature.</pre> <p class="calibre3">This is a very simple prompt, and it does not provide much information to the LLM about what kind of poem we would like it to write for us. If we want a specific type of output, we could include additional instructions in our prompt, such as <span>the following:</span></p>
<pre class="source-code">
Write a poem about nature with the following properties:
Format: sonnet
Theme: how the changing seasons affect human emotions
Primary emotion: the poem should focus particularly on the transition from summer to autumn and the sense of longing that some humans feel when summer is coming to an end.</pre> <p class="calibre3">The latter prompt will produce a much more specific poem that abides by the parameters we’ve outlined. Feel free to go ahead and try this out for yourself. Visit the following URL, enter each of those prompts, and see how different the responses <span>appear: </span><a href="https://gemini.google.com" class="calibre6 pcalibre pcalibre1"><span>https://gemini.google.com</span></a><span>.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">You can use the aforementioned URL to test all of the prompts in <span>this book.</span></p>
<p class="calibre3">This is a<a id="_idIndexMarker1906" class="calibre6 pcalibre pcalibre1"/> rudimentary form of prompt engineering that<a id="_idIndexMarker1907" class="calibre6 pcalibre pcalibre1"/> simply includes additional instructions in the prompt to help the LLM better understand how it should respond to our request. I’ll explain additional prompt engineering techniques in this section, beginning by outlining some standard best practices for crafting <span>effective prompts.</span></p>
<h3 class="calibre11">Core prompt design principles</h3>
<p class="calibre3">The following are <a id="_idIndexMarker1908" class="calibre6 pcalibre pcalibre1"/>some principles we should keep in mind when <span>creating prompts:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Be clear</strong>: We should be clear about what we want the LLM to do and avoid using <span>ambiguous language</span></li>
<li class="calibre8"><strong class="bold">Be specific</strong>: Using specific instructions can help us get <span>better-quality results</span></li>
<li class="calibre8"><strong class="bold">Provide adequate context</strong>: Without appropriate context, the LLM’s responses might not be as relevant to what we are trying <span>to achieve</span></li>
</ul>
<p class="calibre3">I included some of those principles in the latter example prompt in the previous section, and I’ll outline them more explicitly here. In most cases, it’s important for prompts to be as clear and specific as possible due to the sheer and ever-growing versatility of LLMs. For example, if I ask an LLM to write a story about happiness, there’s almost no way of knowing what kinds of details it will come up with. While this is one of the wonderful properties of LLMs, it can be challenging when we want an LLM to provide results within <span>specific parameters.</span></p>
<p class="calibre3">Before I provide additional examples regarding clarity and specificity, consider use cases in which we might not want to be clear and specific, such as when we’re not yet sure of our exact <a id="_idIndexMarker1909" class="calibre6 pcalibre pcalibre1"/>requirements, and we want the LLM to help us explore various potential options. In that case, we could start by sending a broad prompt to the LLM, such as <span>the following:</span></p>
<pre class="source-code">
Suggest ideas for a new business that will help me make money.</pre> <p class="calibre3">In response to a broad prompt such as this one, the LLM will likely come up with all kinds of random ideas, from baking cakes to developing video games that help kids <span>learn mathematics.</span></p>
<p class="calibre3">After getting some initial outputs from the LLM, we could then iteratively refine the process by diving in on certain ideas more specifically. For example, if we really like the idea of developing video games for kids, we could follow up with a more specific prompt, such as <span>the following:</span></p>
<pre class="source-code">
I want to develop a video game that helps kids to learn mathematics. The target audience will be kids between ten and fifteen years old, and the game will focus on algebra and precalculus. List and explain the steps I need to take to start this business.</pre> <p class="calibre3">As we can see, this prompt is much more specific than the previous prompt and will likely provide more targeted and actionable responses. This updated prompt abides by all of the principles we’ve outlined because it’s specific in terms of exactly what we want to achieve (starting a business to create a video game), it provides context such as the intended content of the game and the target audience, and it’s clear in terms of how we want the LLM to respond (that is, list and explain <span>the steps).</span></p>
<p class="calibre3">Sometimes, we need to provide large amounts of context, and we will discuss how to do that later in this<a id="_idIndexMarker1910" class="calibre6 pcalibre pcalibre1"/> chapter. Next, however, let’s explore how we can use chaining to refine the outputs we get from <span>an LLM.</span></p>
<h3 class="calibre11">Chaining</h3>
<p class="calibre3">The process I described <a id="_idIndexMarker1911" class="calibre6 pcalibre pcalibre1"/>in the previous section, in which <a id="_idIndexMarker1912" class="calibre6 pcalibre pcalibre1"/>we use the output of the first prompt as an input to a subsequent prompt, is called <strong class="bold">prompt chaining</strong>. This can be done via an interactive process such as a chat interface, or in an automated fashion using tools I will describe later, such as LangChain. There’s also a framework called ReAct, in which we can chain multiple actions together to achieve a broader goal, which I’ll <span>describe next.</span></p>
<h4 class="calibre20">ReAct and agents</h4>
<p class="calibre3">I briefly touched <a id="_idIndexMarker1913" class="calibre6 pcalibre pcalibre1"/>upon the concept of LAMs at the beginning of this chapter, and I’ll cover it in more <span>detail here.</span></p>
<p class="calibre3"><strong class="bold">ReAct</strong> stands for <strong class="bold">Reasoning and Acting</strong>, and it’s a framework for combining the reasoning abilities of LLMs with the ability to take actions and interact with external tools or environments, which helps us build solutions that go beyond simply generating content to achieve more complex goals. Examples of external tools include software, APIs, code interpreters, search engines, or <span>custom extensions.</span></p>
<p class="calibre3">By using ReAct to go beyond generating content, we can build what are referred to as <strong class="bold">agents</strong>. The<a id="_idIndexMarker1914" class="calibre6 pcalibre pcalibre1"/> role of an agent is to interpret the user’s desired outcome and decide which tools to utilize to achieve the intended goal. Here, I will again highlight the example of planning and booking a vacation, which might include booking flights, accommodation, restaurant reservations, and excursions, among <span>other tasks.</span></p>
<p class="calibre3">Chaining is not to be confused with another prompt engineering approach, called <strong class="bold">chain-of-thought</strong> (<strong class="bold">CoT</strong>) prompting, which we can use to help LLMs work through complex reasoning tasks, which I’ll <span>describe next.</span></p>
<h4 class="calibre20">CoT prompting</h4>
<p class="calibre3">CoT prompting<a id="_idIndexMarker1915" class="calibre6 pcalibre pcalibre1"/> involves creating prompts in a way that guides<a id="_idIndexMarker1916" class="calibre6 pcalibre pcalibre1"/> the model through a step-by-step reasoning process before arriving at a final answer, much like a human might do when solving a problem. Interestingly, simply adding the words <em class="italic">step by step</em> to a prompt can cause the model to respond differently and work through a problem in a logical manner, while it might not have done so in the absence of those words. We can also help the model if we describe tasks in a step-by-step manner. For example, consider the following prompt as a baseline, which does not implement <span>CoT prompting:</span></p>
<pre class="source-code">
If an investor places $10,000 in a savings account with an annual interest rate of 5% and the interest is compounded annually, how much money will be in the account after 3 years?</pre> <p class="calibre3">We know that LLMs are large language models and not large mathematics models, but we may be able to teach an LLM how to perform complex mathematical calculations by teaching it the step-by-step process. If the LLM were struggling to provide an accurate answer, we could explain the steps <span>as follows:</span></p>
<pre class="source-code">
Annually compounded interest means the interest is added to the principal amount at the end of each year, and in the next year, interest is earned on the new principal. Based on that information, work through the following process step by step: If an investor places $10,000 in a savings account with an annual interest rate of 5% and the interest is compounded annually, how much money will be in the account after 3 years?</pre> <p class="calibre3">While the model may not have been able to immediately provide the requested answer, we can help it to get there by teaching it about what should happen in each step, and then it can chain those steps together to get to the end result. To learn more about this fascinating topic, I recommend reading the research paper titled <em class="italic">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em> (Wei et al., 2022), available at the <span>following URL:</span></p>
<p class="calibre3"><a href="https://doi.org/10.48550/arXiv.2201.11903" class="calibre6 pcalibre pcalibre1"><span>https://doi.org/10.48550/arXiv.2201.11903</span></a></p>
<p class="calibre3">Next, I’ll cover another popular prompt engineering technique, which I briefly mentioned in <a href="B18143_15.xhtml#_idTextAnchor371" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 15</em></span></a>, called <span><strong class="bold">few-shot prompting</strong></span><span>.</span></p>
<h3 class="calibre11">Few-shot prompting</h3>
<p class="calibre3">All of the <a id="_idIndexMarker1917" class="calibre6 pcalibre pcalibre1"/>prompts I’ve included so far in this chapter <a id="_idIndexMarker1918" class="calibre6 pcalibre pcalibre1"/>are examples <a id="_idIndexMarker1919" class="calibre6 pcalibre pcalibre1"/>of <strong class="bold">zero-shot prompting</strong> because I simply asked the LLM to do something without providing any referenceable examples of how to do <span>that thing.</span></p>
<p class="calibre3">Few-shot prompting is a rather simple concept, as it just means that we are providing some examples in our prompt that teach the LLM how to perform the task we are requesting. An example of few-shot prompting would be <span>the following:</span></p>
<pre class="source-code">
The following are some examples of product reviews, as well as the categorized customer sentiment associated with those reviews:
Review: "Sending this back because the strap broke the first time I used it." Sentiment: Negative. Properties: construction.
Review: "Love the color! It's the perfect size and seems well-made." Sentiment: Positive. Properties: appearance, construction.
Review: "Disappointed. The zipper gets stuck constantly." Sentiment: Negative. Properties: construction.
Based on those reviews and their associated sentiment categorizations, categorize the sentiment of the following review:
"The fabric feels flimsy, and I'm worried it might tear easily."</pre> <p class="calibre3">As we can see in the prompt, I’ve instructed the LLM on how to categorize the sentiment of a review, and I provided detailed examples of the format in which I want the LLM to respond, including specific product properties that the sentiment appears <span>to reference.</span></p>
<p class="calibre3">In the case of few-shot prompting, we’re providing the LLM with a mini “training set” within the prompt itself, and it can use these examples to understand the expected mapping between the inputs and the desired outputs. This helps the LLM to understand what it should focus on and how it should structure its responses, hopefully providing results that more closely match the specific needs of our <span>use case.</span></p>
<p class="calibre3">Although the examples can be seen as a small training set included in our prompt, it’s important to note that the underlying LLM’s weights are not changed by the examples we provide. This is true for most prompt engineering techniques, whereby the responses may change significantly based on our inputs, but the underlying LLM’s weights <span>remain unchanged.</span></p>
<p class="calibre3">Few-shot prompting and prompt engineering, in general, can be surprisingly effective, but in some cases, we need to provide many examples to the LLM, and we want to perform incremental training (that is, updating weights), especially for complex tasks. I will outline how we can approach those scenarios shortly, but first, I’ll introduce the important<a id="_idIndexMarker1920" class="calibre6 pcalibre pcalibre1"/> emerging <a id="_idIndexMarker1921" class="calibre6 pcalibre pcalibre1"/>field of <span>prompt management.</span></p>
<h3 class="calibre11">Prompt management practices and tooling</h3>
<p class="calibre3">While new<a id="_idIndexMarker1922" class="calibre6 pcalibre pcalibre1"/> technologies bring new functionality and opportunities, they often also introduce new challenges. As prompt engineering continues to develop, a common challenge companies encounter is the need to store and manage their prompts effectively. For example, if a company’s employees are coming up with awesome prompts that provide wonderful results, they will want to track and reuse those prompts. They may also want to update the prompts over time as the LLMs and other components of their solutions evolve, in which case they will want to track different versions of prompts and <span>their results.</span></p>
<p class="calibre3">In software development, we use versioning systems to track and manage updates to our application code. We can apply the same principles to prompt engineering, using versioning systems and templates to help us efficiently develop, reuse, and share prompts. Companies performing advanced prompt engineering practices will likely end up curating prompt libraries and repositories for <span>these purposes.</span></p>
<p class="calibre3"><strong class="bold">Prompt templates</strong> can be <a id="_idIndexMarker1923" class="calibre6 pcalibre pcalibre1"/>used to standardize the structures of effective prompts so that we don’t need to keep using trial and error and reinventing the wheel to create prompts that are best suited for specific use cases. For example, imagine that we work in the marketing department and we run a monthly report to measure the success of our marketing campaigns. We may want to use an LLM to summarize the reports, and there are likely specific pieces of information that certain team members want to review each time. We could create the following prompt template to formulate <span>those requirements:</span></p>
<pre class="source-code">
Summarize the [Report Name] report for [Target Audience], considering that [Target Audience] is particularly interested in reviewing increases or decreases in [Metric 1] and [Metric 2].</pre> <p class="calibre3">Now, the marketing team members simply need to use this prompt template and fill in their desired values for each of the placeholders in the template. As is the case with individual prompts, we could maintain different versions of our prompt templates and evaluate how each <span>version performs.</span></p>
<p class="calibre3">We will likely see many more prompt engineering and prompt management approaches being developed in the coming years. In addition to this, we can even apply <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) techniques <a id="_idIndexMarker1924" class="calibre6 pcalibre pcalibre1"/>to find or suggest the best prompts for our use cases, either by asking an LLM to suggest the best prompts or by using traditional ML optimization approaches such as classification and regression to find or suggest prompts that will likely lead to the best outcome for a given use case. Expect to continue seeing interesting approaches emerging in <span>this field.</span></p>
<p class="calibre3">Going beyond <a id="_idIndexMarker1925" class="calibre6 pcalibre pcalibre1"/>prompt engineering and management, the next section describes larger-scale LLM tuning techniques, which can mainly be categorized under the umbrella of <strong class="bold">transfer </strong><span><strong class="bold">learning</strong></span><span> (</span><span><strong class="bold">TL</strong></span><span>).</span></p>
<h2 id="_idParaDest-325" class="calibre9"><a id="_idTextAnchor387" class="calibre6 pcalibre pcalibre1"/>TL</h2>
<p class="calibre3">In <a href="B18143_15.xhtml#_idTextAnchor371" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 15</em></span></a>, I mentioned <a id="_idIndexMarker1926" class="calibre6 pcalibre pcalibre1"/>at a high level that LLMs <a id="_idIndexMarker1927" class="calibre6 pcalibre pcalibre1"/>usually go through an unsupervised pre-training phase followed by a supervised tuning phase. This section describes in more detail some of the supervised training techniques that are used to fine-tune LLMs. Let’s begin with a definition <span>of TL.</span></p>
<p class="callout-heading">Definition</p>
<p class="callout">TL is an ML approach that uses a model that has already been trained on a certain task (or set of tasks) as a starting point for a new model that will perform a similar task (or set of tasks) with different but somewhat related parameters <span>or data.</span></p>
<p class="calibre3">An example of TL would be to take a model that was pre-trained on generic image recognition tasks and then fine-tune it to identify objects that are specifically relevant to driving scenarios by incrementally training it on datasets containing <span>road scenes.</span></p>
<p class="calibre3">TL approaches can be seen as a kind of spectrum, where some TL techniques and use cases only require updating a small portion of the model weights, while others involve much more extensive updates. Different points on this spectrum represent a trade-off between customizability and computational expense. For example, updating a lot of model weights provides more customizability but is more computationally expensive, while updating a small number of model weights offers less customizability but is also less <span>computationally expensive.</span></p>
<p class="calibre3">Let’s begin our<a id="_idIndexMarker1928" class="calibre6 pcalibre pcalibre1"/> discussion at one extreme of the <a id="_idIndexMarker1929" class="calibre6 pcalibre pcalibre1"/>spectrum, in which we will update all or a large portion of the original model’s weights, which we refer to as <span><strong class="bold">full fine-tuning</strong></span><span>.</span></p>
<h3 class="calibre11">Full fine-tuning</h3>
<p class="calibre3">In the case of fully<a id="_idIndexMarker1930" class="calibre6 pcalibre pcalibre1"/> fine-tuning LLMs, we could begin <a id="_idIndexMarker1931" class="calibre6 pcalibre pcalibre1"/>with a model that has been pre-trained on an enormous body of data and has learned a broad understanding of the concepts on which it was trained. We then introduce the model to a new dataset specific to the task at hand (for example, understanding the rules of the road). This dataset is usually smaller and more focused than the data used in the initial pre-training phase. During the fine-tuning process, the weights across all layers of the model are updated to minimize the loss in relation to our <span>new task.</span></p>
<p class="calibre3">A couple of things to bear in mind are that full fine-tuning can require a lot of computational resources, and there’s also a risk that the model might forget some of the useful representations it learned during pre-training (sometimes referred to as <strong class="bold">catastrophic forgetting</strong>), especially<a id="_idIndexMarker1932" class="calibre6 pcalibre pcalibre1"/> if the new task is considerably different from the <span>pre-trained task.</span></p>
<p class="calibre3">Remember that LLMs are large—in fact, they’re huge! With that in mind, many companies may find full fine-tuning infeasible due to the sheer amount of data, computational resources, and expense it <span>often requires.</span></p>
<p class="calibre3">Rather than tuning all of the weights in an enormous LLM, research has found that sometimes, we can get effective improvements on specific tasks by only changing some weights. In such cases, we can “freeze” some of the model’s weights (or parameters) to ensure they do not get updated while allowing others to be updated. These approaches can be more efficient because they require fewer parameters to be updated and are therefore referred to<a id="_idIndexMarker1933" class="calibre6 pcalibre pcalibre1"/> as <strong class="bold">parameter-efficient fine-tuning</strong> (<strong class="bold">PEFT</strong>), which I will describe in<a id="_idIndexMarker1934" class="calibre6 pcalibre pcalibre1"/> more detail in the following subsections, starting<a id="_idIndexMarker1935" class="calibre6 pcalibre pcalibre1"/> with <span><strong class="bold">adapter tuning</strong></span><span>.</span></p>
<h3 class="calibre11">Adapter tuning</h3>
<p class="calibre3">In the case of<a id="_idIndexMarker1936" class="calibre6 pcalibre pcalibre1"/> adapter tuning, the original model layers<a id="_idIndexMarker1937" class="calibre6 pcalibre pcalibre1"/> remain unchanged, but we <a id="_idIndexMarker1938" class="calibre6 pcalibre pcalibre1"/>add <strong class="bold">adapter layers</strong> or <strong class="bold">adapter modules</strong>, which<a id="_idIndexMarker1939" class="calibre6 pcalibre pcalibre1"/> are small <strong class="bold">neural networks</strong> (<strong class="bold">NNs</strong>) inserted <a id="_idIndexMarker1940" class="calibre6 pcalibre pcalibre1"/>between the layers of a pre-trained model, consisting of just a few learnable parameters. As input data is fed into the model, it flows through the original pre-trained layers and the newly inserted adapter modules, and the adapters slightly alter the data processing flow, which introduces additional transformation steps in some of <span>the layers.</span></p>
<p class="calibre3">The loss calculation steps that are performed after the data passes through the network are the same steps we learned about in our chapter on NNs, and the calculated loss is used to compute gradients for the model parameters. However, unlike traditional full-model fine-tuning, the gradients are only applied to update the weights of the adapter modules, while the weights of the pre-trained model itself remain frozen and unchanged from their <span>initial values.</span></p>
<p class="calibre3">Another popular PEFT technique is <strong class="bold">low-rank adaptation</strong> (<strong class="bold">LoRA</strong>), which I’ll <span>describe next.</span></p>
<h3 class="calibre11">LoRA</h3>
<p class="calibre3">LoRA is<a id="_idIndexMarker1941" class="calibre6 pcalibre pcalibre1"/> based<a id="_idIndexMarker1942" class="calibre6 pcalibre pcalibre1"/> on the premise that not all parameters in a NN are equally important for transferring learned knowledge to a new task, and that, by identifying and changing only a small subset of the model’s parameters, it’s possible to adapt the model for a specific task without completely retraining the model. Instead of modifying the original weight matrices directly, LoRA creates low-rank matrices that can be thought of as a simplified or compressed version of the full matrix that captures the most important properties with <span>fewer parameters.</span></p>
<p class="calibre3">This is what makes LoRA more parameter-efficient. As with adapter tuning, during backpropagation, only the parameters of the low-rank matrices are updated, and the original model parameters remain unchanged. Since the low-rank matrices have fewer parameters than the full matrices they represent, the training and updating process can be much <span>more efficient.</span></p>
<p class="calibre3">In addition to improving a model’s performance on specific tasks, there are also more subtle performance improvement practices that are just as important, such as aligning with human <a id="_idIndexMarker1943" class="calibre6 pcalibre pcalibre1"/>values <a id="_idIndexMarker1944" class="calibre6 pcalibre pcalibre1"/>and expectations. Let’s explore that concept in more <span>detail next.</span></p>
<h2 id="_idParaDest-326" class="calibre9"><a id="_idTextAnchor388" class="calibre6 pcalibre pcalibre1"/>Aligning with human values and expectations</h2>
<p class="calibre3">Have you ever <a id="_idIndexMarker1945" class="calibre6 pcalibre pcalibre1"/>met somebody who is highly intelligent and talented, but has poor communication skills? Such a person might perform amazingly when it comes to troubleshooting and fixing technology issues, for example, but they may not be a suitable person to lead a customer meeting. They may say things that could be considered rude or inappropriate, or perhaps just a bit odd, due to their poor communication skills. This is the analogy I like to use to explain the concept of tuning GenAI models to align with human values and expectations because such values and expectations can often be more subtle than scientific and, therefore, require tailored approaches. For example, in addition to a model’s outputs being accurate, human expectations could require the model’s outputs to be friendly, safe, and unbiased, among other subtle, nuanced qualities. I’ll refer to this concept from here onward as <strong class="bold">alignment</strong>, and<a id="_idIndexMarker1946" class="calibre6 pcalibre pcalibre1"/> in this section I’ll describe two approaches that are commonly used for human value alignment today, starting with <strong class="bold">reinforcement learning from human </strong><span><strong class="bold">feedback</strong></span><span> (</span><span><strong class="bold">RLHF</strong></span><span>).</span></p>
<h3 class="calibre11">RLHF</h3>
<p class="calibre3">We explored <a id="_idIndexMarker1947" class="calibre6 pcalibre pcalibre1"/>the concept of <strong class="bold">reinforcement learning</strong> (<strong class="bold">RL</strong>) in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a> of this book, and I explained that in RL, the model<a id="_idIndexMarker1948" class="calibre6 pcalibre pcalibre1"/> learns from rewards or penalties based on interactions with its environment as it pursues a specific goal. RLHF is one of those examples in which the name of the technology is highly descriptive and accurately captures what the technology entails. As the name suggests, RLHF is an extension of RL, in which feedback to the model is provided <span>by humans.</span></p>
<p class="calibre3">In the case of RLHF, we start with an LLM that has already been pre-trained on a large dataset. The model generates multiple possible responses to a prompt, and humans evaluate the responses based on various preferences. The human feedback data is then used to train a separate model<a id="_idIndexMarker1949" class="calibre6 pcalibre pcalibre1"/> called the <strong class="bold">reward model</strong>, which learns to predict the kinds of responses humans are likely to prefer, based on the feedback given. This process is intended to capture human preferences in a <span>quantifiable way.</span></p>
<p class="calibre3">The LLM then uses this feedback (like a reward signal) to update its parameters through RL techniques, making it more likely to generate responses that humans find desirable in <span>the future.</span></p>
<p class="calibre3">Another technique that can be used to align with human values and expectations is <strong class="bold">Direct Preference Optimization</strong> (<strong class="bold">DPO</strong>). Let’s discuss <span>that next.</span></p>
<h3 class="calibre11">DPO</h3>
<p class="calibre3">DPO also<a id="_idIndexMarker1950" class="calibre6 pcalibre pcalibre1"/> uses human feedback to improve a model’s performance in aligning with human values and expectations. In the case of DPO, again, the model may provide multiple responses to a prompt, and a human can pick which response they prefer (similar to RLHF). However, DPO does not involve training a separate reward model. Instead, it uses pairwise comparisons as the optimization signal, and it directly optimizes policies based on user preferences rather than predefined reward functions, which is especially useful in cases where it’s difficult to define an explicit <span>reward function.</span></p>
<p class="calibre3">It’s important to note that, while RLHF and DPO are valuable and important techniques, there are some challenges that human interaction inherently brings into scope. For example, any process that requires human interaction can be difficult to scale. This means that it can be difficult to gather large amounts of data via feedback from humans. Also, humans can make mistakes or introduce conscious or unconscious bias into the results. These are some factors you would need to monitor and mitigate if you implement an RLHF or <span>DPO solution.</span></p>
<p class="calibre3">Other LLM tuning techniques continue to emerge in addition to the tuning approaches I’ve covered here, and a lot of research is being invested in this field. This is another space in which you<a id="_idIndexMarker1951" class="calibre6 pcalibre pcalibre1"/> can expect to continue seeing groundbreaking leaps forward <span>in development.</span></p>
<p class="calibre3">Next, let’s dive deeper into the role of embeddings and vector databases <span>in GenAI.</span></p>
<h1 id="_idParaDest-327" class="calibre5"><a id="_idTextAnchor389" class="calibre6 pcalibre pcalibre1"/>Embeddings and vector databases</h1>
<p class="calibre3">In <a href="B18143_15.xhtml#_idTextAnchor371" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 15</em></span></a>, we<a id="_idIndexMarker1952" class="calibre6 pcalibre pcalibre1"/> discussed the importance of embeddings and latent spaces, and I explained<a id="_idIndexMarker1953" class="calibre6 pcalibre pcalibre1"/> how they can be created in different ways. One way is when a generative model learns them intrinsically during its training process, and another is when we use specific types of models to create <span>them explicitly.</span></p>
<p class="calibre3">I also touched on why we would want to explicitly create them since they can be processed more efficiently and are a more suitable format for ML use cases. In this context, when we create an embedding for something, we are simply creating a vector of numeric values to represent it (how we actually do that is a more advanced topic we will <span>cover shortly).</span></p>
<p class="calibre3">Another concept I briefly touched upon was the importance of relationships between embeddings in the vector space. For example, the proximity of embeddings to each other in the vector space can reflect the similarity between the concepts they represent. Let’s examine this relationship in <span>more detail.</span></p>
<h2 id="_idParaDest-328" class="calibre9"><a id="_idTextAnchor390" class="calibre6 pcalibre pcalibre1"/>Embeddings and similarity of concepts</h2>
<p class="calibre3">A well-constructed<a id="_idIndexMarker1954" class="calibre6 pcalibre pcalibre1"/> embedding contains everything needed to describe and explain the concept it represents—that is, the “meaning” of the concept. That may seem somewhat abstract to us because we don’t often think of everything that goes into the meaning of something. For example, when I utter the word “car,” a certain image might appear in your mind, and a lot of other information is immediately contextualized by that word. You know that you can drive a car; you know that they are usually relatively expensive; you know that they have wheels and windows, and they’re traditionally made from some kind of metal. You know lots of pieces of information about the concept of a car, and this helps you to understand what it is. Imagine that all of the information required to conjure up the idea of a car is stored as a vector in your mind. Now, imagine that you have never heard of a truck before, and I suddenly show you a picture of a truck. It has wheels and windows; it’s made from steel; it looks like something you could possibly drive. Although you’ve never seen anything like it before, you can understand that this new object is similar to a car. That’s because the pieces of information (that is, the vector of information) associated with it are very similar to that of <span>a car.</span></p>
<p class="calibre3">How do we make these kinds of associations? It feels somewhat intuitive, and most lay people (including myself) don’t understand how this all really happens in our brains. However, in the case of embeddings, it’s a lot easier to understand because our good old friend, mathematics (much loved by computers and ML models), comes to the rescue. As I mentioned already, we can mathematically compare different vectors by calculating the distances between them in their vector space, using well-established distance metrics such as Euclidean distance or <span>cosine similarity.</span></p>
<p class="calibre3">If concepts are close in the vector space, they are likely close in meaning, in which case, we can say that they <a id="_idIndexMarker1955" class="calibre6 pcalibre pcalibre1"/>are <strong class="bold">semantically similar</strong>. For example, in a text-based vector database, the <a id="_idIndexMarker1956" class="calibre6 pcalibre pcalibre1"/>phrases “The cat sat on the mat” and “The feline rested on the rug” would have very similar vector embeddings, even though their exact words differ. A query for one of these vectors is likely to also identify the other one as a highly relevant result. A classic example of this concept is the representation of the words “man,” “woman,” “king,” and “queen,” as represented in <span><em class="italic">Figure 16</em></span><span><em class="italic">.1</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer203">
<img alt="Figure 16.1: Embeddings and semantic similarity" src="image/B18143_16_1.jpg" class="calibre190"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 16.1: Embeddings and semantic similarity</p>
<p class="calibre3">As depicted in <span><em class="italic">Figure 16</em></span><em class="italic">.1</em>, depending on how embeddings of items are captured in a vector space, we may see somewhat consistent projections in the vector representations and distances between vectors. The example shows that the concept of binary gender (or some kind of consistent semantic relationship) is captured in the words “man,” “woman,” “king,” and “queen,” and moreover, the distance and direction from “man” to “woman” is similar to that as from “king” to “queen.” Additionally, the distance and direction from “man” to “king” is similar to that from “woman” to “queen.” In this scenario, hypothetically, you could infer the value “queen” by performing the following mathematical operation in the <span>vector space:</span></p>
<p class="author-quote">king – man + woman = queen</p>
<p class="calibre3">That is, if you take the concept of a king, subtract the male gender, and add the female gender, you end <a id="_idIndexMarker1957" class="calibre6 pcalibre pcalibre1"/>up with the concept of <span>a queen.</span></p>
<p class="calibre3">We can also apply this approach more broadly. For example, in a vector database storing image embeddings, a picture of a tangerine may lie in close proximity to a picture of a clementine. These objects have similar properties, such as size, shape, and color, and a vector representing a banana would likely not be as close to them as they are to each other because a banana does not share as many similarities. However, from a multimodal perspective, which we will describe later in this chapter, considering that they are all types of fruit, the banana vector may still be closer to them than a vector representing a car or an umbrella, for example. This concept is depicted in a simplified manner in <span><em class="italic">Figure 16</em></span><span><em class="italic">.2</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer204">
<img alt="Figure 16.2: Elaborated example of embeddings and semantic similarity" src="image/B18143_16_2.jpg" class="calibre191"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 16.2: Elaborated example of embeddings and semantic similarity</p>
<p class="calibre3">Note that, in reality, the embedding space consists of many abstract dimensions and is not usually visualizable or directly interpretable to humans. Embeddings can be seen as a way to translate the messy complexity of the real world into a numerical language that ML models <span>can understand.</span></p>
<p class="calibre3">If we create <a id="_idIndexMarker1958" class="calibre6 pcalibre pcalibre1"/>embeddings, we might want somewhere to store them and easily find them when needed. This is where vector databases come into the picture. Let’s dive into this in more detail and explain what vector <span>databases are.</span></p>
<h2 id="_idParaDest-329" class="calibre9"><a id="_idTextAnchor391" class="calibre6 pcalibre pcalibre1"/>Vector databases</h2>
<p class="calibre3">Vector databases<a id="_idIndexMarker1959" class="calibre6 pcalibre pcalibre1"/> are specialized databases designed to store and manage vector embeddings. Unlike traditional databases that focus on exact matches (for example, finding a customer with a specific ID), vector databases are more suitable for use cases such as <strong class="bold">similarity search</strong>, in which they use distance metrics to determine how close and, therefore, how similar two vector embeddings are in the latent space <span>they occupy.</span></p>
<p class="calibre3">This approach significantly changes how we can retrieve and analyze data. While traditional methods rely on keywords or predefined attributes, vector databases allow us to perform semantic search, in which we can query based on meaning, and this enables new applications. If I search for “trousers,” for example, the responses can also include jeans, slacks, and chinos because these are all semantically similar concepts. Most modern search engines, such as Google, for example, use sophisticated mechanisms such as semantic search (and a combination of other techniques), rather than simple <span>keyword matching.</span></p>
<p class="calibre3">Semantic search provides a much better customer experience, as well as a potential increase in revenue for the company implementing it. If you run a retail website, for example, bear in mind that customers will not buy a product if they cannot find it, and their business may go to your competitors instead. By implementing semantic search, we can understand the meaning and the intent of the user and, therefore, present the most relevant products <span>to them.</span></p>
<p class="calibre3">These new search<a id="_idIndexMarker1960" class="calibre6 pcalibre pcalibre1"/> capabilities are also important in the context of GenAI for reasons I will explain shortly. First, however, let’s dive into more detail on how vector <span>databases work.</span></p>
<h2 id="_idParaDest-330" class="calibre9"><a id="_idTextAnchor392" class="calibre6 pcalibre pcalibre1"/>How vector databases work</h2>
<p class="calibre3">As a precursor to<a id="_idIndexMarker1961" class="calibre6 pcalibre pcalibre1"/> explaining how vector databases work, let’s briefly talk about how databases work in general. The main purpose of any database is to store and organize data in a way that makes it easy to find as quickly as possible. Most databases make use of indexes for this purpose, which I’ll <span>describe next.</span></p>
<h3 class="calibre11">Indexing and neighbor search</h3>
<p class="calibre3">To understand the <a id="_idIndexMarker1962" class="calibre6 pcalibre pcalibre1"/>role of indexes, imagine I handed you a book and asked you to find information in the book about “anthropomorphism” as quickly as possible. Without any mechanisms to assist you, you would have to read through every word on every page until you found that word. This, of course, would be a very inefficient process, and it would likely take you a long time to find the information unless it happened to appear near the beginning of the book. This is referred to<a id="_idIndexMarker1963" class="calibre6 pcalibre pcalibre1"/> as a <strong class="bold">full table scan</strong> in the database world, which we usually want to avoid, if possible. This is where indexes come into play—if the book I handed you contains an index, you could look in the index to see which pages contain references to “anthropomorphism” and then go directly to those pages, cutting out a lot of <span>inefficient searching.</span></p>
<p class="calibre3">Relational databases typically use tree-like (for example, B-trees) or hashing-based indexes for rapid lookups, but vector database indexes can be more complex due to the complexities of the embedding space and how each vector is represented in potentially high-dimensional space. Such indexes can resemble graph-like structures that map relationships between embeddings to enable <span>proximity-based searches.</span></p>
<p class="calibre3">The equivalent of a full table scan in the vector database world is<a id="_idIndexMarker1964" class="calibre6 pcalibre pcalibre1"/> called a <strong class="bold">brute-force</strong> search, which borrows its name from cybersecurity and relates to the practice of trying every possible combination of inputs to find the desired result. This is a common way in which bad actors try to guess a person’s password—they try every possible combination of characters (within certain parameters). A longer password makes it difficult for them to guess it via brute force because each additional character exponentially increases the number of <span>potential combinations.</span></p>
<p class="calibre3">In the case of vector databases, our queries generally try to find items near the queried item in the vector space. If we have a lot of vectors in the database, then brute force would be impractical, especially in high-dimensional spaces where the <strong class="bold">curse of dimensionality</strong> (<strong class="bold">CoD</strong>) makes <a id="_idIndexMarker1965" class="calibre6 pcalibre pcalibre1"/>exact searches computationally expensive. Fortunately, brute force is usually unnecessary because it seeks to precisely find the vector that is closest to the vector being queried (referred to as the <strong class="bold">exact nearest neighbor</strong>), but we don’t always need the exact nearest neighbor. For <a id="_idIndexMarker1966" class="calibre6 pcalibre pcalibre1"/>example, use cases such as recommendation engines, which recommend items that are similar to a particular item, generally just need to find vectors that are in the close vicinity of the vector being queried, but it doesn’t have to be the exact nearest neighbor of the vector being queried. This is <a id="_idIndexMarker1967" class="calibre6 pcalibre pcalibre1"/>referred to as an <strong class="bold">approximate nearest neighbor</strong> (<strong class="bold">ANN</strong>) search, and it’s extremely useful in the context of <a id="_idIndexMarker1968" class="calibre6 pcalibre pcalibre1"/>vector databases because it allows a trade-off between the accuracy of the search results and the speed of the query – that is, for these kinds of use cases, it’s better to quickly get results that are close enough to the queried vector rather than spending a lot of time finding the exact <span>best result.</span></p>
<p class="callout-heading">Note – the CoD and vector proximity</p>
<p class="callout">In high-dimensional spaces, points become increasingly spread out, and as the number of dimensions increases, the distance between the nearest and farthest points becomes diluted, which makes it harder to distinguish between truly similar and dissimilar items based on <span>distance metrics.</span></p>
<p class="calibre3">Some vector databases implement hybrid approaches that combine ANN with other indexing or search strategies, such as hierarchical indexing structures, multi-index hashing, and tree-based methods to improve accuracy, reduce search latency, or optimize resource usage. Graph-based methods such as <strong class="bold">Navigable Small World</strong> (<strong class="bold">NSW</strong>) graphs, <strong class="bold">Hierarchical Navigable Small World</strong> (<strong class="bold">HNSW</strong>) graphs, and others create a <a id="_idIndexMarker1969" class="calibre6 pcalibre pcalibre1"/>graph where <a id="_idIndexMarker1970" class="calibre6 pcalibre pcalibre1"/>nodes represent vectors, and queries then traverse the graph to find the nearest neighbors. Some vector databases also use partitioning or clustering algorithms to divide the dataset into smaller, more manageable chunks or clusters, and indexing can then be performed within these partitions or clusters, often using a combination of methods to first identify the relevant partition and then perform an ANN search <span>within it.</span></p>
<p class="calibre3">Also, we can use approaches such as product quantization or scalar quantization to compress vectors by mapping them to a finite set of reference vectors, which reduces the dimensionality and storage requirements of the vectors before indexing and enables faster searches by<a id="_idIndexMarker1971" class="calibre6 pcalibre pcalibre1"/> approximating distances in the <span>compressed space.</span></p>
<p class="calibre3">In <a href="B18143_17.xhtml#_idTextAnchor408" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 17</em></span></a>, we’ll walk through the various vector database offerings in Google Cloud, but for now, let’s move on to discuss how we can create embeddings in the <span>first place.</span></p>
<h2 id="_idParaDest-331" class="calibre9"><a id="_idTextAnchor393" class="calibre6 pcalibre pcalibre1"/>Creating embeddings</h2>
<p class="calibre3">We’ve already <a id="_idIndexMarker1972" class="calibre6 pcalibre pcalibre1"/>discussed <strong class="bold">autoencoders</strong> (<strong class="bold">AEs</strong>) in <a href="B18143_15.xhtml#_idTextAnchor371" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 15</em></span></a>, and we learned how they can be used to create latent space representations of their inputs. There are many more ways in which we can create embeddings, and I’ll describe some of the more popular methods in this section, starting with one of the most famous word embedding <span>models, Word2Vec.</span></p>
<h3 class="calibre11">Word2Vec</h3>
<p class="calibre3">Word2Vec (short for “word to vector”) is a <a id="_idIndexMarker1973" class="calibre6 pcalibre pcalibre1"/>group of algorithms <a id="_idIndexMarker1974" class="calibre6 pcalibre pcalibre1"/>invented by Google (Mikolov et al., 2013) to learn representations of words as vectors. It’s often considered to be the grandfather of word embedding approaches, and although it wasn’t the first word embedding technique to be invented, it promoted the idea of representing words as dense vectors that capture the semantic meaning and relationships between words in a high-dimensional space. It works by building a vocabulary of unique words and learning a vector representation for each word, where words with similar meanings have <span>similar vectors.</span></p>
<p class="calibre3">The two main approaches used in Word2Vec are the <strong class="bold">Continuous Bag of Words </strong>(<strong class="bold">CBOW</strong>) model, which <a id="_idIndexMarker1975" class="calibre6 pcalibre pcalibre1"/>predicts a target word based on its surrounding <a id="_idIndexMarker1976" class="calibre6 pcalibre pcalibre1"/>words, and <strong class="bold">skip-gram</strong>, which predicts surrounding words based on a given <span>target word.</span></p>
<p class="calibre3">While Word2Vec has been a popular and useful approach, newer, transformer-based approaches <a id="_idIndexMarker1977" class="calibre6 pcalibre pcalibre1"/>have <a id="_idIndexMarker1978" class="calibre6 pcalibre pcalibre1"/>emerged that provide more advanced capabilities. Let’s take a look at <span>those next.</span></p>
<h3 class="calibre11">Transformers</h3>
<p class="calibre3">We’ve already<a id="_idIndexMarker1979" class="calibre6 pcalibre pcalibre1"/> introduced transformers in previous chapters, and in this section, I will briefly describe transformer-based approaches to create embeddings. While there are many options to choose from, I’ll focus on the famous <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>) and <span>its derivatives.</span></p>
<h4 class="calibre20">BERT</h4>
<p class="calibre3">Earlier in this<a id="_idIndexMarker1980" class="calibre6 pcalibre pcalibre1"/> book, I mentioned that, every now and then, there’s a significant step forward in AI/ML research, and it’s well established that the invention of the transformer architecture by Google in 2017 (Vaswani et al., 2017) was one of those significant leaps. BERT, which was invented by Google in 2018 (Devlin et al., 2018), was another <span>significant step.</span></p>
<p class="calibre3">As indicated in the name, BERT is based on the transformer architecture, and it is pre-trained on a large dataset of text and code. During its training, it learns to model complex patterns and relationships between words and understand nuances such as context, grammar, and how different parts of a sentence relate to <span>each other.</span></p>
<p class="calibre3">The two main approaches used in <a id="_idIndexMarker1981" class="calibre6 pcalibre pcalibre1"/>BERT are <strong class="bold">masked language modeling</strong> (<strong class="bold">MLM</strong>), which predicts missing words within a sentence based on the surrounding <a id="_idIndexMarker1982" class="calibre6 pcalibre pcalibre1"/>context, and <strong class="bold">next sentence prediction</strong> (<strong class="bold">NSP</strong>), which tries to determine if two sentences are logically connected. These are examples of the pre-training phase described in <a href="B18143_15.xhtml#_idTextAnchor371" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 15</em></span></a>. By combining these two approaches, BERT develops an understanding of <span>language structure.</span></p>
<p class="calibre3">At the heart of BERT are the transformer layers, which take the text we give it and produce word embeddings that capture meaning depending on the surrounding words, which is a major improvement over static embeddings such <span>as Word2Vec.</span></p>
<p class="calibre3">BERT was such an important breakthrough that many variants have been created since its initial release, such as DistilBERT and ALBERT, which are smaller, distilled versions of BERT with fewer parameters (trading off some accuracy for computational efficiency), as well as domain-specific variants, such as SciBERT (trained on scientific publications) and FinBERT (fine-tuned for financial industry <span>use cases).</span></p>
<p class="calibre3">There are also more targeted models that build on top of transformer-based models such as BERT. For example, rather than focusing on individual words, <strong class="bold">Sentence Transformers</strong> use <a id="_idIndexMarker1983" class="calibre6 pcalibre pcalibre1"/>pooling strategies such as mean pooling or max pooling to create semantically meaningful embeddings of <span>entire sentences.</span></p>
<p class="calibre3">For image embeddings, while we’ve <a id="_idIndexMarker1984" class="calibre6 pcalibre pcalibre1"/>discussed <strong class="bold">convolutional NNs</strong> (<strong class="bold">CNNs</strong>) in previous chapters for use cases such as image classification, it’s important to note that CNNs can also be used in creating <span>image embeddings.</span></p>
<p class="calibre3">Now that we know about some options for creating embeddings, as well as options for storing<a id="_idIndexMarker1985" class="calibre6 pcalibre pcalibre1"/> embeddings in vector databases, let’s explore a relatively new pattern that is becoming increasingly popular in the industry and that combines these topics, referred to <span>as RAG.</span></p>
<h1 id="_idParaDest-332" class="calibre5"><a id="_idTextAnchor394" class="calibre6 pcalibre pcalibre1"/>RAG</h1>
<p class="calibre3">While LLMs are <a id="_idIndexMarker1986" class="calibre6 pcalibre pcalibre1"/>clearly impressive and powerful technologies, they have some limitations. Firstly, LLMs are only as good as the data on which they were trained, and they can be expensive and time-consuming to train, so for most companies, it would not be feasible to retrain them with new information every day. For this reason, in many cases, updated versions of LLMs tend to be released every few months. This means that the information provided in their responses depends on their most recent training dataset. If you want to ask about something that happened yesterday, but the LLM was last updated a month ago, it simply will not have any information on that topic. This can be quite limiting in today’s fast-paced business world, where people and applications constantly need up-to-date knowledge at <span>their fingertips.</span></p>
<p class="calibre3">Secondly, unless you are one of the large corporations that have created the popular LLMs being used today, you most likely did not train the LLM yourself from scratch, so it has not been trained on your specific data. Imagine you are a retail company that wants to use an LLM to enable customers to find out about your products via a chat interface. Since the LLM was not trained specifically on your product catalog, it will not be familiar with the details of <span>your products.</span></p>
<p class="calibre3">To combat these kinds of challenges, rather than relying solely on the data that was used to train an LLM, we can insert additional data into the context of the prompt being sent to the LLM. We already touched on this in the <em class="italic">Prompt engineering</em> section of this chapter, in which we provided additional information in the prompts in order to guide the LLM’s responses to more accurately match our required outputs. That approach works great for small amounts of data, but if we consider the aforementioned use case of getting the LLM to answer questions about a product catalog, we cannot include the entire product catalog in every prompt. This is where RAG <span>can help.</span></p>
<p class="calibre3">With RAG, we can <em class="italic">augment</em> the responses <em class="italic">generated</em> by LLMs by first <em class="italic">retrieving</em> relevant information from a data store and then including that information in the prompt being sent<a id="_idIndexMarker1987" class="calibre6 pcalibre pcalibre1"/> to the LLM. That data store can contain whatever information we want, such as the contents of our product catalog. Let’s take a look at this pattern in <span>more detail.</span></p>
<h2 id="_idParaDest-333" class="calibre9"><a id="_idTextAnchor395" class="calibre6 pcalibre pcalibre1"/>How RAG works</h2>
<p class="calibre3">To set the context <a id="_idIndexMarker1988" class="calibre6 pcalibre pcalibre1"/>for explaining how RAG works, let’s level-set the discussion by reviewing how LLM interactions typically work without RAG. This is very straightforward and is depicted in <span><em class="italic">Figure 16</em></span><span><em class="italic">.3</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer205">
<img alt="Figure 16.3: LLM prompt and response" src="image/B18143_16_3.jpg" class="calibre192"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 16.3: LLM prompt and response</p>
<p class="calibre3">As we can see, the typical LLM interaction simply consists of sending a prompt and getting a response. Note that LLMs can only understand numerical values, so behind the scenes, the textual prompt sent by the user is broken down into tokens, and the tokens are encoded into vectors that the LLM can understand. The reverse process is performed when<a id="_idIndexMarker1989" class="calibre6 pcalibre pcalibre1"/> sending the response to the user. Next, let’s look at how RAG changes the process, as depicted in <span><em class="italic">Figure 16</em></span><span><em class="italic">.4</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer206">
<img alt="Figure 16.4: RAG" src="image/B18143_16_4.jpg" class="calibre193"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 16.4: RAG</p>
<p class="calibre3"><span><em class="italic">Figure 16</em></span><em class="italic">.4</em> outlines how RAG works at a high level. The steps are <span>as follows:</span></p>
<ol class="calibre7">
<li class="calibre8">The user or client application sends a textual request <span>or prompt.</span></li>
<li class="calibre8">The contents of the prompt are converted to <span>vectors (embeddings).</span></li>
<li class="calibre8">During the “retrieval” part of the process, the prompt embeddings are used to find similar embeddings in the vector database. These embeddings represent the data that we want to use to augment the prompt (for example, embeddings that represent data from our <span>product catalog).</span></li>
<li class="calibre8">The embeddings from our vector database are combined with the embeddings of the user prompt and sent to <span>the LLM.</span></li>
<li class="calibre8">The LLM uses the combined embeddings of the user prompt and the retrieved embeddings to augment its response, aiming to provide a response that is not only based on the data on which it was originally trained but that is also relevant in the context of the data retrieved from our <span>vector database.</span></li>
<li class="calibre8">The response embeddings are decoded to provide a text response back to the user or <span>client application.</span></li>
</ol>
<p class="calibre3">We will implement<a id="_idIndexMarker1990" class="calibre6 pcalibre pcalibre1"/> RAG in a later chapter in this book. Next, let’s begin to explore the topic of <span>multimodal models.</span></p>
<h1 id="_idParaDest-334" class="calibre5"><a id="_idTextAnchor396" class="calibre6 pcalibre pcalibre1"/>Multimodal models</h1>
<p class="calibre3">Given that LLMs, as the<a id="_idIndexMarker1991" class="calibre6 pcalibre pcalibre1"/> name suggests, focus on language, the primary modality for many of the popular LLMs you are likely interacting with at the time of writing this in early 2024 is likely to be text. However, the concept of GenAI also goes beyond text and expands into other modalities, such as pictures, sound, <span>and video.</span></p>
<p class="calibre3">The term <em class="italic">multimodal</em> is becoming ever more prominent within the field of GenAI. In this section, we explore what that means, starting with a definition <span>of “modality.”</span></p>
<p class="callout-heading">Definition</p>
<p class="callout">The Oxford English Dictionary defines <span>modality as:</span></p>
<p class="callout">“<em class="italic">Those aspects of a thing which relate to its mode, or manner or state of being, as distinct from its substance or identity; the non-essential aspect or attributes of a concept or entity. Also: a particular quality or attribute denoting the mode or manner of being </em><span><em class="italic">of something.</em></span><span>”</span></p>
<p class="callout"><em class="italic">Oxford English Dictionary</em>, <em class="italic">s.v. “modality (n.), sense 1.a,”</em> <em class="italic">December </em><span><em class="italic">2023</em></span><span>, </span><a href="https://doi.org/10.1093/OED/1055677936" class="calibre6 pcalibre pcalibre1"><span>https://doi.org/10.1093/OED/1055677936</span></a><span>.</span></p>
<p class="calibre3">Considering this formal definition of modalities, unlike traditional models that focus on a single data type, multimodal models are designed to process information from multiple modalities or data<a id="_idIndexMarker1992" class="calibre6 pcalibre pcalibre1"/> types. This is another one of those important leaps forward in AI research because it opens up new use cases and applications for AI. Let’s dive into this in <span>more detail.</span></p>
<h2 id="_idParaDest-335" class="calibre9"><a id="_idTextAnchor397" class="calibre6 pcalibre pcalibre1"/>Why multimodality matters</h2>
<p class="calibre3">In the section on<a id="_idIndexMarker1993" class="calibre6 pcalibre pcalibre1"/> RAG in this chapter, I mentioned an example of a company using an LLM to help customers explore product details via a chat interface and how that would require interacting with data from the company’s product catalog. Diving into this example in more detail, consider that items in a product catalog can have many different types of information associated with them. This may consist of structured data with standardized fields such as product dimensions, color, price, and other attributes. It may be associated with semi-structured or unstructured data such as customer reviews as well as product images. Imagine how much of a more detailed understanding our model could gain regarding the items in our catalog if it could ingest and understand all of those data modalities. If the written product description was somehow missing details regarding the color of the product, the model could instead learn that information from pictures of the product. Also, in addition to written customer reviews, some websites allow customers to post videos in their reviews. The model could learn more information from those videos. Overall, this could provide a much richer end-user experience. This is just one example of the power of multimodality, and this concept becomes even more important when creating models that need to interact with the physical world, such as robots and <span>self-driving vehicles.</span></p>
<p class="calibre3">Multimodal implementations are also important in AI research because one of the main challenges in creating useful and powerful models is getting access to relevant data. Considering that there is a finite amount of text that has ever been created in the world, if text remained the only modality, it would eventually become a limiting factor in AI development. As we can all clearly see on various social media platforms and other websites, video, audio, and pictures have equaled or surpassed text as the primary modalities in user-generated content, and we can expect those trends to continue. We’ve all heard the expression, “A picture is worth a thousand words,” and this becomes ever more true (in addition to other modalities) when training and interacting with large <span>generative models!</span></p>
<p class="calibre3">Having expressed the<a id="_idIndexMarker1994" class="calibre6 pcalibre pcalibre1"/> importance and advantages of multimodal model implementations, I’ll balance the discussion by highlighting some of the related <span>challenges next.</span></p>
<h2 id="_idParaDest-336" class="calibre9"><a id="_idTextAnchor398" class="calibre6 pcalibre pcalibre1"/>Multimodality challenges</h2>
<p class="calibre3">I’ll begin this section<a id="_idIndexMarker1995" class="calibre6 pcalibre pcalibre1"/> with a challenge that consistently emerges in many contexts that we’ve discussed throughout this book. Generally, creating more powerful models requires more computing resources. Multimodal approaches are no exception here, and training multimodal models often requires significant <span>computational resources.</span></p>
<p class="calibre3">Another challenge is the complexity—combining data from different modalities can be complex and require careful preprocessing, especially considering that the feature representations in different modalities usually have very different feature spaces, and some modalities have richer or more sparse data than others. For example, text is usually sequential, images are spatial, and audio and video have temporal properties, so aligning all of these for the model to build a unified understanding can <span>be difficult.</span></p>
<p class="calibre3">In previous chapters, we looked at examples of data quality issues and the kinds of data preparation steps and pipelines required to address those issues. The tricky thing, however, is that data in different modalities has different kinds of potential quality issues, preparation steps, and pipelines that are required to get it ready for <span>training models.</span></p>
<p class="calibre3">Despite the challenges, multimodality is a rapidly developing space that will continue to get significant attention for the <span>foreseeable future.</span></p>
<p class="calibre3">While this section focused mainly on training GenAI models (specifically, multimodal approaches), other parts of the model development life cycle also have unique approaches that must be factored in when dealing with GenAI models rather than traditional AI models. The next important topic to highlight is how we evaluate GenAI models and how this can differ in some cases from evaluating traditional <span>AI models.</span></p>
<h1 id="_idParaDest-337" class="calibre5"><a id="_idTextAnchor399" class="calibre6 pcalibre pcalibre1"/>GenAI model evaluation</h1>
<p class="calibre3">While evaluating <a id="_idIndexMarker1996" class="calibre6 pcalibre pcalibre1"/>traditional AI models usually involves measuring the model’s outputs against a ground truth dataset and calculating established, objective metrics such as accuracy, precision, recall, F1 score, <strong class="bold">Mean Squared Error</strong> (<strong class="bold">MSE</strong>), and others we’ve covered in this book, evaluating<a id="_idIndexMarker1997" class="calibre6 pcalibre pcalibre1"/> generative models is not always <span>as straightforward.</span></p>
<p class="calibre3">When evaluating a GenAI model, we might want to focus on different factors, such as the model’s ability to produce creative and human-like outputs while staying relevant to the task at hand. An extra challenge in this case is that evaluating these properties can be somewhat vague and subjective. For example, if I ask a generative model to write a poem or create a photorealistic picture of a cat in a meadow, the quality of that poem or the realness of the picture is not always easy to represent with a mathematically calculated number, although some formulaic measurements do exist, which I will describe in this section. However, let’s first discuss <span>human evaluation.</span></p>
<h2 id="_idParaDest-338" class="calibre9"><a id="_idTextAnchor400" class="calibre6 pcalibre pcalibre1"/>Human evaluation</h2>
<p class="calibre3">Perhaps the most <a id="_idIndexMarker1998" class="calibre6 pcalibre pcalibre1"/>straightforward but also slow, cumbersome, and potentially expensive approach is for human evaluators to assess generated outputs for quality, creativity, coherence, and how well they align with the task requirements. There are tools, frameworks, services, and entire companies dedicated to performing these kinds of evaluations. Some common challenges with this approach are that the evaluations are only as good as the instructions provided to the evaluators, so it’s necessary to clearly understand and explain how the responses should be evaluated, and the evaluations can be subjective, based on the personal interpretations of <span>the evaluator.</span></p>
<p class="calibre3">Next, I’ll describe some of the formalized methods that can be used in evaluating generative models, starting with metrics for textual language models such as <strong class="bold">BiLingual Evaluation Understudy</strong> (<strong class="bold">BLEU</strong>) and <strong class="bold">Recall-Oriented Understudy for Gisting </strong><span><strong class="bold">Evaluation</strong></span><span> (</span><span><strong class="bold">ROUGE</strong></span><span>).</span></p>
<h2 id="_idParaDest-339" class="calibre9"><a id="_idTextAnchor401" class="calibre6 pcalibre pcalibre1"/>BLEU</h2>
<p class="calibre3">BLEU is <a id="_idIndexMarker1999" class="calibre6 pcalibre pcalibre1"/>used<a id="_idIndexMarker2000" class="calibre6 pcalibre pcalibre1"/> mainly for evaluating machine translation use cases, and it works by measuring how similar the generated text is to human-written text, so when we’re using BLEU, we need a set of reference, human-written text examples, which are compared against the generated text. More specifically, BLEU measures something called <strong class="bold">n-gram overlap</strong>, which is the overlap in sequences of words<a id="_idIndexMarker2001" class="calibre6 pcalibre pcalibre1"/> between <a id="_idIndexMarker2002" class="calibre6 pcalibre pcalibre1"/>the generated text and the <span>reference text.</span></p>
<h2 id="_idParaDest-340" class="calibre9"><a id="_idTextAnchor402" class="calibre6 pcalibre pcalibre1"/>ROUGE</h2>
<p class="calibre3">ROUGE also <a id="_idIndexMarker2003" class="calibre6 pcalibre pcalibre1"/>measures the overlap between generated <a id="_idIndexMarker2004" class="calibre6 pcalibre pcalibre1"/>text and human-written reference examples, but it focuses on summarization use cases by measuring the <strong class="bold">recall</strong>, which represents how well a generated summary captures important information from the <span>reference summaries.</span></p>
<p class="calibre3">In addition to overlap-based evaluations, we can also evaluate text generation performance with metrics such<a id="_idIndexMarker2005" class="calibre6 pcalibre pcalibre1"/> as <strong class="bold">perplexity</strong> and <strong class="bold">negative log-likelihood</strong> (<strong class="bold">NLL</strong>), which<a id="_idIndexMarker2006" class="calibre6 pcalibre pcalibre1"/> can be used to evaluate a model’s performance in predicting sequences of words, such as predicting the next word in <span>a sentence.</span></p>
<p class="calibre3">Next, let’s consider some approaches for evaluating image generation models, such as the <strong class="bold">Inception Score</strong> (<strong class="bold">IS</strong>) and <strong class="bold">Fréchet Inception </strong><span><strong class="bold">Distance</strong></span><span> (</span><span><strong class="bold">FID</strong></span><span>).</span></p>
<h2 id="_idParaDest-341" class="calibre9"><a id="_idTextAnchor403" class="calibre6 pcalibre pcalibre1"/>IS</h2>
<p class="calibre3">IS is used to<a id="_idIndexMarker2007" class="calibre6 pcalibre pcalibre1"/> measure the quality and <strong class="bold">diversity</strong> of generated <a id="_idIndexMarker2008" class="calibre6 pcalibre pcalibre1"/>images, in which a good image generation model should be able to generate a wide variety of images, not just variations of the same thing, and the images should look clear and realistic. We can calculate the IS by using a pre-trained image classification model to classify images generated by a <span>generative model.</span></p>
<h2 id="_idParaDest-342" class="calibre9"><a id="_idTextAnchor404" class="calibre6 pcalibre pcalibre1"/>FID</h2>
<p class="calibre3">While the IS<a id="_idIndexMarker2009" class="calibre6 pcalibre pcalibre1"/> evaluates generated images independently, FID <a id="_idIndexMarker2010" class="calibre6 pcalibre pcalibre1"/>was designed to improve on it by measuring how similar a distribution of generated images is to a distribution of real images (it gets its name from using the Fréchet distance to measure the distance between <span>two distributions).</span></p>
<p class="calibre3">For the purpose of this chapter, we mainly need to be aware that specific metrics exist for evaluating some generative models, but going into the mathematical detail of how these metrics are calculated would be more information than is required at this point. There are additional approaches and metrics beyond the ones we’ve covered in this section, but this section describes some popular ones to consider. Next, I’ll introduce a somewhat different <a id="_idIndexMarker2011" class="calibre6 pcalibre pcalibre1"/>approach to evaluating generative<a id="_idIndexMarker2012" class="calibre6 pcalibre pcalibre1"/> models, which is to use <span>an </span><span><strong class="bold">auto-rater</strong></span><span>.</span></p>
<h2 id="_idParaDest-343" class="calibre9"><a id="_idTextAnchor405" class="calibre6 pcalibre pcalibre1"/>Auto-raters and side-by-side evaluations</h2>
<p class="calibre3">At a high level, an<a id="_idIndexMarker2013" class="calibre6 pcalibre pcalibre1"/> auto-rater<a id="_idIndexMarker2014" class="calibre6 pcalibre pcalibre1"/> is an ML model that rates or evaluates the outputs of models. There<a id="_idIndexMarker2015" class="calibre6 pcalibre pcalibre1"/> are different ways in which this can be done, and I will explain the concept in the context of Google Cloud <span>Vertex AI.</span></p>
<p class="calibre3">Google Cloud Vertex AI recently launched a service called <strong class="bold">Automatic side-by-side</strong> (<strong class="bold">AutoSxS</strong>), which <a id="_idIndexMarker2016" class="calibre6 pcalibre pcalibre1"/>can be used to evaluate pre-generated predictions or GenAI models in the Vertex AI Model Registry. This means that, in addition to Vertex AI foundation models, we can also use it to evaluate third-party <span>language models.</span></p>
<p class="calibre3">In order to compare the results of two models against each other, it uses an auto-rater to decide which model gives the better response to a prompt. The same prompt is sent to both models, and the auto-rater evaluates the responses from each model against various criteria, such as relevance, comprehensiveness, the model’s ability to follow instructions, and whether the responses are grounded in <span>established facts.</span></p>
<p class="calibre3">As I mentioned at the beginning of this section, evaluating generative models often requires some subjectivity and more flexibility than the simple objective evaluations of traditional ML models. The various criteria supported by Google Cloud Vertex AI AutoSxS provide such <span>additional flexibility.</span></p>
<p class="calibre3">Now that we have a high-level understanding of how the evaluation of GenAI models differs in some ways from that of traditional ML models, let’s move on to introduce another important topic <span>in GenAI—LangChain.</span></p>
<h1 id="_idParaDest-344" class="calibre5"><a id="_idTextAnchor406" class="calibre6 pcalibre pcalibre1"/>LangChain</h1>
<p class="calibre3">By sending <a id="_idIndexMarker2017" class="calibre6 pcalibre pcalibre1"/>a prompt to an LLM, we can achieve amazing feats and get useful information. However, we may want to build applications that implement more complex logic than we could achieve in a single prompt, and these applications may require interacting with multiple systems in addition to <span>an LLM.</span></p>
<p class="calibre3">LangChain is a popular framework for developing applications using LLMs, and it enables us to combine multiple steps into a chain, in which each step implements some logic, such as reading from a data store, sending a prompt to an LLM, taking the outputs from the LLM, and using them in <span>subsequent steps.</span></p>
<p class="calibre3">One of LangChain’s advantages is that it uses a modular approach, so we can build complex workflows by combining smaller, simpler modules of logic. It provides tools for useful tasks such as creating prompt templates and managing context for interactive integrations with LLMs, and we can easily find integrations for accessing information from sources such as Google Search, knowledge bases, or custom <span>data stores.</span></p>
<p class="calibre3">We will use LangChain<a id="_idIndexMarker2018" class="calibre6 pcalibre pcalibre1"/> later in this book to orchestrate a number of different steps in a workflow. For now, let’s summarize what we covered in this chapter before we move on to the <span>next one.</span></p>
<h1 id="_idParaDest-345" class="calibre5"><a id="_idTextAnchor407" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter on advanced GenAI concepts and use cases, we started by diving into techniques for tuning and optimizing LLMs. We learned how prompt engineering practices can affect model outputs, and how tuning approaches such as full fine-tuning, adapter tuning, and LoRA enable pre-trained models to be adapted for specific domains <span>or tasks.</span></p>
<p class="calibre3">Next, we dived into embeddings and vector databases, including how they represent the meanings of concepts, and enable similarity-based searches. We looked into specific embedding models such as Word2Vec and <span>transformer-based encodings.</span></p>
<p class="calibre3">We then moved on to describe how RAG can help us to combine information from custom data stores into prompts being sent to an LLM, thereby enabling the LLM to modify its responses in alignment with the contents of our <span>data stores.</span></p>
<p class="calibre3">After that, we discussed multimodal models and how they can open up additional use cases beyond textual language. We then moved on to discuss how the evaluation of GenAI models differs in some ways from that of traditional ML models, and we introduced some new approaches and metrics for evaluating <span>GenAI models.</span></p>
<p class="calibre3">Finally, we introduced the important topic of LangChain and how it can help us build applications that implement complex logic by chaining simple modules or building <span>blocks together.</span></p>
<p class="calibre3">In the next chapter, we will learn about the various GenAI offerings and implementations on <span>Google Cloud.</span></p>
</div>
</div></body></html>