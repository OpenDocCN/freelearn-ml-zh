<html><head></head><body>
		<div id="_idContainer043">
			<h1 id="_idParaDest-33"><em class="italic"><a id="_idTextAnchor032"/>Chapter 2</em>: AWS Application Services for AI/ML</h1>
			<p>In this chapter, we will learn about the AWS AI services for building chatbots, advanced text analysis, document analysis, transcription, and so on. This chapter has been designed in such a way that you can solve different use cases by integrating AWS AI services and get an idea of how they work. AWS is growing every day and they are adding new AI services regularly. </p>
			<p>In this chapter, you will approach different use cases programmatically or from the console. This will help you understand different APIs and their usages. We will use S3 for storage and AWS Lambda to execute any code. The examples in this chapter are in Python, but you can use other supported languages such as Java, Node.js, .NET, PowerShell, Ruby, and so on. </p>
			<p>We are going to cover the following topics:</p>
			<ul>
				<li>Analyzing images and videos with Amazon Rekognition</li>
				<li>Text to speech with Amazon Polly</li>
				<li>Speech to text with Amazon Transcribe</li>
				<li>Implementing natural language processing with Amazon Comprehend</li>
				<li>Translating documents with Amazon Translate</li>
				<li>Extracting text from documents with Amazon Textract</li>
				<li>Creating chatbots on Amazon Lex</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>Technical requirements</h1>
			<p>All you will need for this chapter is an AWS account.</p>
			<p>You can download the code examples for this chapter from GitHub at <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2</a>.</p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Analyzing images and videos with Amazon Rekognition</h1>
			<p>If you need to add powerful <a id="_idIndexMarker097"/>visual analysis to your applications, then <strong class="bold">Amazon Rekognition</strong> is the service to opt for. <strong class="bold">Rekognition Image</strong> lets <a id="_idIndexMarker098"/>you easily build powerful applications to search, verify, and organize millions of images. <strong class="bold">Rekognition Video</strong> lets you extract motion-based context from<a id="_idIndexMarker099"/> stored or live stream videos, and helps you analyze them. Rekognition Video also allows you to index metadata such as objects, activities, scene, celebrities, and faces, making video searches easy. Rekognition Image uses deep neural network models to detect and label thousands of objects and scenes in your images. It helps you capture text in an image, a bit like <strong class="bold">Optical Character Recognition</strong> (<strong class="bold">OCR</strong>). A perfect <a id="_idIndexMarker100"/>example is a T-shirt with quotes on it. If you were to take a picture of one and ask Amazon Rekognition to extract the text from it, it would be able to tell you what the text says. You can also perform celebrity recognition using Amazon Rekognition. I am not a celebrity, so I won't be using the celebrity recognition API for my face; instead, I will use the face comparison API. </p>
			<p>The official documentation, available at <a href="https://aws.amazon.com/rekognition/faqs/">https://aws.amazon.com/rekognition/faqs/</a>, states the following:</p>
			<p class="author-quote">"With Rekognition Image, you only pay for the images you analyze and the face metadata you store. You will not be charged for the compute resources if, at any point of time, your training fails."</p>
			<p>Some common uses of Amazon Rekognition include the following:</p>
			<ul>
				<li>Image and video analysis</li>
				<li>Searchable image library</li>
				<li>Face-based user verification</li>
				<li>Sentiment analysis</li>
				<li>Text in image</li>
				<li>Facial recognition</li>
				<li>Image moderation</li>
				<li>Search index for video archives</li>
				<li>Easy filtering for videos, for explicit and suggestive content </li>
				<li>Examples of explicit nudity – sexual activity, graphical nudity, adult toys, and so on</li>
				<li>Examples of suggestive content – partial<a id="_idIndexMarker101"/> nudity, swimwear or underwear, and so on</li>
			</ul>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Exploring the benefits of Amazon Rekognition</h2>
			<p>Let's look at some of the benefits of<a id="_idIndexMarker102"/> using Amazon Rekognition:</p>
			<ul>
				<li>AWS manages the infrastructure it runs on. In short, just use the API for your image analysis. We just need to focus on building and managing our deep learning pipelines. <p>With or without knowledge of image processing, you can perform image and video analysis just by using the APIs provided in Amazon Rekognition, which can be used for any application or service on several platforms.</p></li>
				<li>The Labels API's response will identify real-world entities within an image through the DetectLabels API. These labels include city, town, table, home, garden, animal, pets, food, drink, electronics, flowers, and more. The entities are classified based on their <strong class="bold">Confidence</strong> score, which indicates the probability that a given prediction is correct: the higher the better. Similarly, we can use the DetectText API to extract the text in an image. Amazon Rekognition may detect multiple lines based on the gap between words. Periods don't represent the end of a line. </li>
				<li>Amazon Rekognition can be integrated with AWS Kinesis Video Stream, AWS S3, and AWS Lambda for seamless and affordable image and video analysis. With the AWS IAM service, Amazon Rekognition API calls can easily be secured and controlled.</li>
				<li>Low cost. You only pay for<a id="_idIndexMarker103"/> the images and videos that are analyzed. </li>
				<li>Through AWS CloudTrail, all the API calls for Amazon Rekognition can be captured as events. It captures all calls made from the console, or the CLI, or code calls for APIs, which further enables the user to create Amazon SNS notifications based on CloudTrail events.</li>
				<li>You can create a VPC Endpoint policy for specific API calls to establish a private connection between your VPC and Amazon Rekognition. This helps you leverage enhanced security. As per the AWS shared responsibility model, AWS takes care of the security of the infrastructure and software, and we have to take care of the security of our content in the cloud.</li>
			</ul>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Getting hands-on with Amazon Rekognition</h2>
			<p>In this section, we will learn <a id="_idIndexMarker104"/>how to integrate AWS Lambda with Amazon Rekognition to detect the labels in our image (uploaded at <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Rekognition%20Demo/images">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Rekognition%20Demo/images</a>) and print the detected objects in the CloudWatch console. We will use the <strong class="source-inline">detect_labels</strong> API from Amazon Rekognition in the code.</p>
			<p>We will begin by creating an IAM role for Lambda:</p>
			<ol>
				<li>Navigate to the IAM console page.</li>
				<li>Select <strong class="bold">Roles</strong> from the left-hand menu.</li>
				<li>Select <strong class="bold">Create role</strong>.</li>
				<li>Select <strong class="bold">Lambda</strong> from the <strong class="bold">Choose a use case</strong> section.</li>
				<li>Add the following managed policies:<ul><li><strong class="source-inline">AmazonS3ReadOnlyAccess </strong></li><li><strong class="source-inline">AmazonRekognitionFullAccess </strong></li><li><strong class="source-inline">CloudWatchLogsFullAccess </strong></li></ul></li>
				<li>Name the <a id="_idIndexMarker105"/>role <strong class="source-inline">rekognition-lambda-role</strong>:<div id="_idContainer012" class="IMG---Figure"><img src="image/B16735_02_001.jpg" alt="Figure 2.1 – The Create role dialog&#13;&#10;"/></div><p class="figure-caption">Figure 2.1 – The Create role dialog</p><p>Next, we will create a Lambda function.</p></li>
				<li>Navigate to the AWS Lambda console page.</li>
				<li>Select <strong class="bold">Create function</strong>.</li>
				<li>Create a function:<ul><li>Select <strong class="bold">Author from scratch</strong>.</li><li>Give the function a name, such as <strong class="source-inline">lambda-rekognition</strong>.</li><li>Choose <strong class="source-inline">Python 3.6</strong> from the <strong class="bold">Runtime</strong> dropdown.</li><li>Select <strong class="bold">Use an existing role</strong>. Add the name<a id="_idIndexMarker106"/> of the role you created previously; that is, <strong class="source-inline">rekognition-lambda-role</strong>:</li></ul></li>
			</ol>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B16735_02_002.jpg" alt="Figure 2.2 – Creating the Lambda function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Creating the Lambda function</p>
			<ol>
				<li value="10">Enter the following code in <strong class="source-inline">lambda_function.py</strong>:<p class="source-code">from __future__ import print_function</p><p class="source-code">import boto3</p><p class="source-code">def lambda_handler(event, context):</p><p class="source-code">    print("========lambda_handler started=======")</p><p class="source-code">    # read the bucket name (key) from the event</p><p class="source-code">    name_of_the_bucket=event['Records'][0]['s3']['bucket']</p><p class="source-code">['name']</p><p class="source-code">    # read the object from the event</p><p class="source-code">    name_of_the_photo=event['Records'][0]['s3']['object']['key']</p><p class="source-code">    detect_labels(name_of_the_photo,name_of_the_bucket)</p><p class="source-code">    print("Labels detected Successfully")</p><p class="source-code">def detect_labels(photo, bucket):</p><p class="source-code">    client=boto3.client('rekognition')</p><p class="source-code">  response=client.detect_labels(Image={'S3Object':{'Bucket':bucket,'Name':photo}})</p><p class="source-code">    print('Detected labels for ' + photo) </p><p class="source-code">    print('==============================')   </p><p class="source-code">    for label in response['Labels']:</p><p class="source-code">        print ("Label: " + label['Name'])</p><p class="source-code">        print ("Confidence: " + </p><p class="source-code">str(label['Confidence']))</p><p class="source-code">        print ("Instances:")</p><p class="source-code">        for instance in label['Instances']:</p><p class="source-code">            print ("  Bounding box")</p><p class="source-code">            print ("Top: </p><p class="source-code">"+str(instance['BoundingBox']['Top']))</p><p class="source-code">            print ("Left: \</p><p class="source-code">"+str(instance['BoundingBox']['Left']))</p><p class="source-code">            print ("Width: \</p><p class="source-code">"+str(instance['BoundingBox']['Width']))</p><p class="source-code">            print ("Height: \</p><p class="source-code">"+str(instance['BoundingBox']['Height']))</p><p class="source-code">            print ("Confidence: </p><p class="source-code">"+str(instance['Confidence']))</p><p class="source-code">            print()</p><p class="source-code">        print ("Parents:")</p><p class="source-code">        for parent in label['Parents']:</p><p class="source-code">            print ("   " + parent['Name'])</p><p class="source-code">        print ("----------")</p><p class="source-code">        print('==============================') </p><p class="source-code">    return response </p><p>Now, we will create a trigger for the Lambda Function.</p></li>
				<li>Navigate to the AWS S3 console page. Create a <a id="_idIndexMarker107"/>bucket, for example, <strong class="source-inline">rekognition-test-baba</strong>, as shown in the following screenshot:<div id="_idContainer014" class="IMG---Figure"><img src="image/B16735_02_003.jpg" alt="Figure 2.3 – AWS S3 console page&#13;&#10;"/></div><p class="figure-caption">Figure 2.3 – AWS S3 console page</p></li>
				<li>Click on <strong class="bold">Create folder</strong> and name it <strong class="source-inline">images</strong>. Click <strong class="bold">Save</strong>.</li>
				<li>Click the <strong class="bold">Properties</strong> tab of our bucket.</li>
				<li>Scroll to <strong class="bold">Events</strong> for that <a id="_idIndexMarker108"/>bucket.</li>
				<li>Inside the <strong class="bold">Events</strong> window, select <strong class="bold">Add notification</strong> and set the following properties:<ul><li><strong class="bold">Name</strong>: <strong class="source-inline">rekognition_event</strong></li><li><strong class="bold">Events</strong>: <strong class="source-inline">All object create events</strong></li><li><strong class="bold">Prefix</strong>: <strong class="source-inline">images</strong><em class="italic">/</em></li><li><strong class="bold">Send to</strong>: <strong class="source-inline">Lambda Function</strong></li><li><strong class="bold">Lambda</strong>: <strong class="source-inline">lambda-rekognition</strong>:</li></ul></li>
			</ol>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B16735_02_004.jpg" alt="Figure 2.4 –S3 bucket Events window&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 –S3 bucket Events window</p>
			<p>Next, we will upload the image<a id="_idIndexMarker109"/> from the shared GitHub repository to the S3 bucket <strong class="source-inline">images</strong> folder.</p>
			<ol>
				<li value="16">As soon as you upload, you can check the <strong class="bold">Monitoring</strong> tab in the Lambda console to monitor the events, as shown in the following screenshot:<div id="_idContainer016" class="IMG---Figure"><img src="image/B16735_02_005.jpg" alt="Figure 2.5 – CloudWatch monitoring the event in the Lambda console&#13;&#10;"/></div><p class="figure-caption">Figure 2.5 – CloudWatch monitoring the event in the Lambda console</p></li>
				<li>Navigate to <strong class="bold">CloudWatch &gt; CloudWatch Logs &gt; Log groups &gt; /aws/lambda/lambda-rekognition</strong>. Select the latest stream from all the streams and scroll<a id="_idIndexMarker110"/> down in the logs to see your output, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B16735_02_006.jpg" alt="Figure 2.6 – CloudWatch logs &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – CloudWatch logs </p>
			<p>In this section, we learned<a id="_idIndexMarker111"/> how to implement the Amazon Rekognition AI service to detect objects in an image and get a confidence score for each. We will see more use cases for Amazon Rekognition in the upcoming sections, when we detect text in images. In the next section, we will learn about Amazon's text-to-speech service and implement it.</p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Text to speech with Amazon Polly</h1>
			<p><strong class="bold">Amazon Polly</strong> is all about<a id="_idIndexMarker112"/> converting text into speech, and does so using pretrained deep learning models. It's a fully managed service, so<a id="_idIndexMarker113"/> we don't have to do anything. You provide the plain text as input for synthesizing or in <strong class="bold">Speech Synthesis Markup Language</strong> (<strong class="bold">SSML</strong>) format so that an audio stream is returned. It also gives you different <a id="_idIndexMarker114"/>languages and voices to choose from, with both male and female options. The output audio from Amazon Polly can be saved in MP3 format for further usage in the application (web or mobile) or can be a JSON output for written speech.</p>
			<p>For example, if we were to input the text "Baba went to the library" into Amazon Polly, the output speech mark object would look as follows:</p>
			<p class="source-code">{"time":370,"type":"word","start":5,"end":9,"value":"went"}</p>
			<p>The word <strong class="source-inline">"went"</strong> begins 370 milliseconds after the audio stream begins, and starts at byte 5 and ends at byte 9 of the given input text.</p>
			<p>It also returns output in <strong class="source-inline">ogg_vorbis</strong> and <strong class="source-inline">pcm</strong> format. When <strong class="source-inline">pcm</strong> is used, the content that's returned is<a id="_idIndexMarker115"/> audio/pcm in a signed 16-bit, 1 <a id="_idIndexMarker116"/>channel (mono), little-endian format.</p>
			<p>Some common uses of <a id="_idIndexMarker117"/>Amazon Polly include the following:</p>
			<ul>
				<li>Can be used as an accessibility tool for reading web content.</li>
				<li>Can be integrated with Amazon Rekognition to help visually impaired people read signs. You can click a picture of the sign with text and feed it to Amazon Rekognition to extract text. The output text can be used as input for Polly, and it will return a voice as output.</li>
				<li>Can be used in a public address system, where the admin team can just pass on the text to be announced and Amazon Polly does the magic.</li>
				<li>By combining Amazon Polly with <strong class="bold">Amazon Connect</strong> (telephony backend service), you can <a id="_idIndexMarker118"/>build an <strong class="bold">audio/video receiver</strong> (<strong class="bold">AVR</strong>) system.</li>
				<li>Smart devices such as smart tvs, smart watches, and <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>) devices can use this for audio output.</li>
				<li>Narration generation.</li>
				<li>When combined with Amazon Lex, full-blown voice user interfaces for applications can be developed.</li>
			</ul>
			<p>Now, let's explore the benefits of Amazon Polly.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>Exploring the benefits of Amazon Polly</h2>
			<p>Some of the benefits of<a id="_idIndexMarker119"/> using Amazon Polly include the following:</p>
			<ul>
				<li>This service is fully managed and doesn't require any admin cost to maintain or manage resources.</li>
				<li>It provides an instant speech corrections and enhancements facility.</li>
				<li>You can develop your own access layer using the HTTP API from Amazon Polly. Development is easy due to the huge amount of language support that's available, such as Python, Ruby, Go, C++, Java, and Node.js.</li>
				<li>For certain neural voices, speech can be synthesized by using the Newscaster style, to make them sound like a TV or radio broadcaster. </li>
				<li>Amazon Polly also allows you to modify the pronunciation of particular words or the use of new words. </li>
			</ul>
			<p>Next, we'll get hands-on <a id="_idIndexMarker120"/>with Amazon Polly.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Getting hands-on with Amazon Polly</h2>
			<p>In this section, we will <a id="_idIndexMarker121"/>build a pipeline where we can integrate AWS Lambda with Amazon Polly to read a text file, and then generate an <strong class="bold">MP3</strong> file of the same to another folder in the same bucket. We will monitor the task's progress in CloudWatch logs.</p>
			<p>We will begin by creating an IAM Role for Lambda. Let's get started:</p>
			<ol>
				<li value="1">Navigate to the IAM console page.</li>
				<li>Select <strong class="bold">Roles</strong> from the left-hand menu.</li>
				<li>Select <strong class="bold">Create role</strong>.</li>
				<li>Select <strong class="bold">Lambda</strong> as the trusted entity.</li>
				<li>Add the following managed policies:<ul><li><strong class="source-inline">AmazonS3FullAccess</strong></li><li><strong class="source-inline">AmazonPollyFullAccess</strong></li><li><strong class="source-inline">CloudWatchFullAccess</strong></li></ul></li>
				<li>Save the role as <strong class="source-inline">polly-lambda-role</strong>. </li>
			</ol>
			<p>Next, we will create a Lambda function:</p>
			<ol>
				<li value="1">Navigate to <strong class="bold">Lambda &gt; Functions &gt; Create Function</strong>.<ul><li>Name the function <strong class="source-inline">polly-lambda</strong></li><li>Set the runtime to <strong class="source-inline">python 3.6</strong>.</li><li>Use an existing role; that is, <strong class="source-inline">polly-lambda-role</strong></li></ul></li>
				<li>Paste the code at <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Rekognition%20Demo/lambda_code">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Rekognition%20Demo/lambda_code</a> into your lambda <a id="_idIndexMarker122"/>function and check its progress in the CloudWatch console. We will be using the <strong class="source-inline">start_speech_synthesis_task</strong> API from Amazon Polly for this code; it is an asynchronous synthesis task.</li>
				<li>Scroll down and in <strong class="bold">Basic Settings</strong> section, change <strong class="bold">Timeout</strong> to <strong class="source-inline">59</strong><strong class="bold"> sec</strong>, as shown in the following screenshot, and click <strong class="bold">Save</strong>:<p class="callout-heading">Important note</p><p class="callout">The default is 3 sec. Since this is an asynchronous operation, any retried attempts will create more files.</p><div id="_idContainer018" class="IMG---Figure"><img src="image/B16735_02_007.jpg" alt="Figure 2.7 – Edit basic settings window&#13;&#10;"/></div><p class="figure-caption">Figure 2.7 – Edit basic settings window</p><p>Now, we will create a bucket to trigger an event. </p></li>
				<li>Navigate to the<a id="_idIndexMarker123"/> AWS S3 console and create a bucket called <strong class="source-inline">polly-test-baba</strong>.</li>
				<li>Create a folder called <strong class="source-inline">input-text</strong> (in this example, we will only upload .<strong class="source-inline">txt</strong> files).</li>
				<li>Navigate to <strong class="bold">Properties &gt; Events &gt; Add notification</strong>. Fill in the required fields, as shown here, and click on <strong class="bold">Save</strong>:<ul><li><strong class="bold">Name</strong>: <strong class="source-inline">polly_event</strong></li><li><strong class="bold">Events</strong>: <strong class="source-inline">All object create events</strong></li><li><strong class="bold">Prefix</strong>:<em class="italic"> </em><strong class="source-inline">input-text/</strong></li><li><strong class="bold">Suffix</strong>:<em class="italic"> .</em><strong class="source-inline">txt</strong></li><li><strong class="bold">Send to</strong>: <strong class="source-inline">Lambda Function</strong></li><li><strong class="bold">Lambda</strong>: <strong class="source-inline">polly-lambda</strong></li></ul></li>
				<li>Next, we will upload a file in order to trigger an event and check its progress in CloudWatchUpload, in this case, a<a id="_idIndexMarker124"/> file <strong class="source-inline">test_file.txt</strong> in <strong class="source-inline">input-text</strong>, as shown in the following screenshot. You can download the sample file from this book's GitHub repository at <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Polly%20Demo/text_file">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Polly%20Demo/text_file</a>:<div id="_idContainer019" class="IMG---Figure"><img src="image/B16735_02_008.jpg" alt="Figure 2.8 – The S3 bucket after uploading a text file for further processing&#13;&#10;"/></div><p class="figure-caption">Figure 2.8 – The S3 bucket after uploading a text file for further processing</p></li>
				<li>This will trigger the<a id="_idIndexMarker125"/> lambda function. You can monitor your logs by going to <strong class="bold">CloudWatch&gt; CloudWatch Logs&gt; Log groups&gt; /aws/lambda/polly-lambda</strong>:<div id="_idContainer020" class="IMG---Figure"><img src="image/B16735_02_009.jpg" alt="Figure 2.9 – Log groups in the CloudWatch console&#13;&#10;"/></div><p class="figure-caption">Figure 2.9 – Log groups in the CloudWatch console</p></li>
				<li>Click on the<a id="_idIndexMarker126"/> latest stream; the log will look as follows:<p class="source-code">File Content:  Hello Everyone, Welcome to Dublin. How </p><p class="source-code">are you doing today?</p><p class="source-code">{'ResponseMetadata': {'RequestId': '74ca4afd-5844-</p><p class="source-code">47d8-9664-3660a26965e4', 'HTTPStatusCode': 200, </p><p class="source-code">'HTTPHeaders': {'x-amzn-requestid': '74ca4afd-5844-</p><p class="source-code">47d8-9664-3660a26965e4', 'content-type': </p><p class="source-code">'application/json', 'content-length': '471', 'date': </p><p class="source-code">'Thu, 24 Sep 2020 18:50:57 GMT'}, 'RetryAttempts': 0}, </p><p class="source-code">'SynthesisTask': {'Engine': 'standard', 'TaskId': </p><p class="source-code">'57548c6b-d21a-4885-962f-450952569dc7', 'TaskStatus': </p><p class="source-code">'scheduled', 'OutputUri': 'https://s3.us-east-</p><p class="source-code">1.amazonaws.com/polly-test-baba/output-</p><p class="source-code">audio/.57548c6b-d21a-4885-962f-450952569dc7.mp3', </p><p class="source-code">'CreationTime': datetime.datetime(2020, 9, 24, 18, 50, </p><p class="source-code">57, 769000, tzinfo=tzlocal()), 'RequestCharacters': </p><p class="source-code">59, 'OutputFormat': 'mp3', 'TextType': 'text', </p><p class="source-code">'VoiceId': 'Aditi', 'LanguageCode': 'en-GB'}}</p><p>The logs sample is shown<a id="_idIndexMarker127"/> in the following screenshot:</p><div id="_idContainer021" class="IMG---Figure"><img src="image/B16735_02_010.jpg" alt="Figure 2.10 – The logs in the CloudWatch console&#13;&#10;"/></div><p class="figure-caption">Figure 2.10 – The logs in the CloudWatch console</p></li>
				<li>It will create output in MP3 format, as shown in the following screenshot. Download and listen to it:</li>
			</ol>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B16735_02_011.jpg" alt="Figure 2.11 – The output file that was created in the S3 bucket&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.11 – The output file that was created in the S3 bucket</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The most scalable and <a id="_idIndexMarker128"/>cost-effective way for your mobile apps or web apps is to generate an AWS pre-signed URL for S3 buckets and provide it to your users. These S3 Put events asynchronously invoke downstream AI workflows to generate results and send a response to the end users. Many users can be served at the same time through this approach, and it may increase performance and throughput.</p>
			<p>In this section, we learned <a id="_idIndexMarker129"/>how to implement text to speech. In the next section, we will learn about Amazon Transcribe, a speech-to-text AI service.</p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Speech to text with Amazon Transcribe</h1>
			<p>In the previous section, we<a id="_idIndexMarker130"/> learned about text to speech. In this section, we will learn about speech to text and the service that provides this: <strong class="bold">Amazon Transcribe</strong>. It is an automatic <a id="_idIndexMarker131"/>speech recognition service that uses pre-trained deep learning models, which means that we don't have to train on petabytes of data to produce a model; Amazon does this for<a id="_idIndexMarker132"/> us. We just have to use the APIs that are available to transcribe audio files or video files; it supports a number of different languages and custom vocabulary too. Accuracy is the key and through custom vocabulary, you can enhance it based on the desired domain or industry:</p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B16735_02_012.jpg" alt="Figure 2.12 – Block diagram of Amazon Transcribe's input and output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.12 – Block diagram of Amazon Transcribe's input and output</p>
			<p>Some common uses of <a id="_idIndexMarker133"/>Amazon Transcribe include the following:</p>
			<ul>
				<li>Real-time audio streaming and transcription.</li>
				<li>Transcripting pre-recorded audio files.</li>
				<li>Enable text searching from a media file by combining AWS Elasticsearch and Amazon Transcribe.</li>
				<li>Performing sentiment analysis on recorded audio files for voice helpdesk (contact center analytics).</li>
				<li>Channel identification separation.</li>
			</ul>
			<p>Next, we'll explore the <a id="_idIndexMarker134"/>benefits of Amazon Transcribe.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Exploring the benefits of Amazon Transcribe</h2>
			<p>Let's look at some of the benefits of using Amazon Transcribe:</p>
			<ul>
				<li><em class="italic">Content Redaction</em>: Customer privacy can be ensured by instructing Amazon Transcribe to<a id="_idIndexMarker135"/> identify and redact <strong class="bold">personally identifiable information</strong> (<strong class="bold">PII</strong>) from the language transcripts. You <a id="_idIndexMarker136"/>can filter unwanted words from your transcript by supplying a list of unwanted words with <strong class="source-inline">VocabularyFilterName</strong> and <strong class="source-inline">VocabularyFilterMethod</strong>, which are provided by the <strong class="source-inline">StratTranscriptionJob</strong> operation. For example, in financial organizations, this can be used to redact a caller's details.</li>
				<li><em class="italic">Language Identification</em>: It can automatically identify the most used language in an audio file and generate transcriptions. If you have several audio files, then this service would help you classify them by language.</li>
				<li><em class="italic">Streaming Transcription</em>: You can send recorded audio files or live audio streams to Amazon Transcribe and output a stream of text in real time.</li>
				<li><em class="italic">Custom Vocabulary or Customized Transcription</em>: You can use your custom vocabulary list as per your custom needs to generate accurate transcriptions. </li>
				<li><em class="italic">TimeStamp Generation</em>: If you want to build or add subtitles for your videos, then Amazon Transcribe can return the timestamp for each word or phrase from the audio.</li>
				<li><em class="italic">Cost Effectiveness</em>: Being a managed service, there is no infrastructure cost.</li>
			</ul>
			<p>Now, let's get hands-on with Amazon Transcribe.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Getting hands-on with Amazon Transcribe</h2>
			<p>In this section, we<a id="_idIndexMarker137"/> will build a pipeline where we can integrate AWS Lambda with Amazon Transcribe to read an audio file stored in a folder in an S3 bucket, and then store the output JSON file in another S3 bucket. We will monitor the task's progress in CloudWatch logs too. We will use the <strong class="source-inline">start_transcription_job</strong> asynchronous function to start our job and we will constantly monitor the job through <strong class="source-inline">get_transcription_job</strong> until its status becomes <strong class="source-inline">COMPLETED</strong>. Let's get started:</p>
			<ol>
				<li value="1">First, create an IAM role called <strong class="source-inline">transcribe-demo-role</strong> for the Lambda function to execute. Ensure that it can read and write from/to S3, use Amazon Transcribe, and print the output in CloudWatch logs. Add the following policies to the IAM role:<ul><li><strong class="source-inline">AmazonS3FullAccess</strong></li><li><strong class="source-inline">CloudWatchFullAccess</strong></li><li><strong class="source-inline">AmazonTranscribeFullAccess</strong></li></ul></li>
				<li>Now, we will create a Lambda function called <strong class="source-inline">transcribe-lambda</strong> with our existing IAM role, <strong class="source-inline">transcribe-demo-role</strong>, and save it. <p>Please make sure you change the default timeout to a higher value in the<strong class="bold"> Basic settings</strong> section of your lambda function. I have set it to 10 min and 20 sec to avoid timeout errors. We will be using an asynchronous API call called <strong class="source-inline">start_transcription_job</strong> to start the task and monitor the same by using the <strong class="source-inline">get_transcription_job</strong> API. </p></li>
				<li>Paste the code available at <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/blob/master/Chapter-2/Amazon%20Transcribe%20Demo/lambda_function/lambda_function.py">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/blob/master/Chapter-2/Amazon%20Transcribe%20Demo/lambda_function/lambda_function.py</a> and click on <strong class="bold">Deploy</strong>.<p>This should give us the following output:</p><div id="_idContainer024" class="IMG---Figure"><img src="image/B16735_02_013.jpg" alt="Figure 2.13 – The Basic settings section of our created lambda function&#13;&#10;"/></div><p class="figure-caption">Figure 2.13 – The Basic settings section of our created lambda function</p></li>
				<li>Next, we will be creating the <a id="_idIndexMarker138"/>an S3 bucket called <strong class="source-inline">transcribe-demo-101</strong> and a folder called <strong class="source-inline">input</strong>. Create an event by going to the <strong class="bold">Properties</strong> tab of the <strong class="bold">Create event notification</strong> section. Enter the following details:<ul><li><strong class="bold">Name</strong>: <strong class="source-inline">audio-event</strong></li><li><strong class="bold">Events</strong>: <strong class="source-inline">All object create events</strong></li><li><strong class="bold">Prefix</strong>: <strong class="source-inline">input/</strong></li><li><strong class="bold">Destination</strong>: <strong class="source-inline">Lambda Function</strong></li><li><strong class="bold">Lambda</strong>: <strong class="source-inline">transcribe-lambda</strong></li></ul></li>
				<li>Upload the audio file in <strong class="source-inline">.mp4</strong><em class="italic"> </em>format to the <strong class="source-inline">input</strong> folder. This will trigger the Lambda function. As per the code, the output will be stored in the S3 bucket in JSON format, which you can then use to read the contents of the file.</li>
				<li>Navigate to <strong class="bold">CloudWatch &gt; CloudWatch Logs &gt; Log groups &gt; aws/lambda/transcribe-lambda</strong>. Choose the<a id="_idIndexMarker139"/> latest stream from the list. It will look as follows:<div id="_idContainer025" class="IMG---Figure"><img src="image/B16735_02_014.jpg" alt="Figure 2.14 – The logs in a Log Stream for the specified log groups in the CloudWatch console&#13;&#10;"/></div><p class="figure-caption">Figure 2.14 – The logs in a Log Stream for the specified log groups in the CloudWatch console</p></li>
				<li>The output is saved to the S3 bucket in JSON format, as per the jobname mentioned in your <a id="_idIndexMarker140"/>code (you can use the the S3 <strong class="source-inline">getObject</strong> API to download<a id="_idIndexMarker141"/> and read it):</li>
			</ol>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B16735_02_015.jpg" alt="Figure 2.15 – The output JSON file in an S3 bucket&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.15 – The output JSON file in an S3 bucket</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">It is a best practice not to overprovision your function's timeout settings. Always understand your code performance and set a function timeout accordingly. Overprovisioning a function timeout results in Lambda functions running longer and it causes unexpected costs. If you're using asynchronous API calls in your Lambda function, then it's good to write them into SNS topics on success and trigger another Lambda function from that. If it needs human intervention, then it is suggested that you use AWS Step Functions.</p>
			<p>In this section, we learned and applied Amazon Transcribe to convert speech into text. In the next section, we will learn about one of the most powerful AWS AI services we can use to get the maximum amount of insight from our text data.</p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/>Implementing natural language processing with Amazon Comprehend </h1>
			<p>This service helps you extract <a id="_idIndexMarker142"/>insights from unstructured text. Unstructured text information is growing exponentially. A few data<a id="_idIndexMarker143"/> source examples are as follows:</p>
			<ul>
				<li><em class="italic">Customer engagement</em>: Call center, issue triage, customer surveys, and product reviews</li>
				<li><em class="italic">Business processes</em>: Customer/vendor emails, product support messages, and operation support feedback</li>
				<li><em class="italic">Records and research</em>: Whitepapers and medical records</li>
				<li> <em class="italic">News and social media</em>: Social media analytics, brand trends, and correlated events</li>
			</ul>
			<p>Now, the question is, what can we do with this data? How can we analyze it and extract any value out of it? The answer is Amazon Comprehend, which is used to get insights from your unstructured data.</p>
			<p>Some common uses of Amazon Comprehend<a id="_idIndexMarker144"/> include the following:</p>
			<ul>
				<li>Information management system</li>
				<li>More accurate search system on organized topics</li>
				<li>Sentiment analysis of users</li>
				<li>Support ticket classification</li>
				<li>Language detection from a document and then translating it into English using Amazon Translate</li>
				<li>Creating a system to label unstructured clinical data to assist in research and analysis purposes</li>
				<li>Extracting topics from the saved audio files of company meetings or TV news</li>
			</ul>
			<p>Next, we'll explore the benefits of<a id="_idIndexMarker145"/> Amazon Comprehend.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Exploring the benefits of Amazon Comprehend</h2>
			<p>Some of the <a id="_idIndexMarker146"/>advantages of using Comprehend can be seen in the following image:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B16735_02_016.jpg" alt="Figure 2.16 – A block diagram showing Amazon Comprehend's capabilities&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.16 – A block diagram showing Amazon Comprehend's capabilities</p>
			<p>Let's look at these in more<a id="_idIndexMarker147"/> detail:</p>
			<ul>
				<li>It detects the language of the text and extract key phrases. Amazon Comprehend can be used for sentiment analysis and topic modeling too. </li>
				<li>Amazon Comprehend Medical can be used to extract medical information.</li>
				<li>You pay for what you <a id="_idIndexMarker148"/>use since this is a fully managed service; you do not have to pay for the infrastructure. You don't need to train, develop, and deploy your own model.</li>
				<li>The topic modeling service works by extracting up to 100 topics. A topic is a keyword bucket so that you can see what's in the actual corpus of documents.</li>
				<li>It's accurate, continuously trained, and easy to use.</li>
			</ul>
			<p>Next, we'll get hands-on with Amazon Comprehend.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>Getting hands-on with Amazon Comprehend</h2>
			<p>In this section, we<a id="_idIndexMarker149"/> will build a pipeline where we can integrate AWS Lambda with Amazon Rekognition and Amazon Comprehend. We will then read an image file stored in an S3 bucket and detect the language of the text that's been extracted from the image. We will also use CloudWatch to print out the output. The following is a diagram of our use case:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B16735_02_017.jpg" alt="Figure 2.17 – Architecture diagram of the required use case&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.17 – Architecture diagram of the required use case</p>
			<p>Let's begin by creating an IAM Role:</p>
			<ol>
				<li value="1">Navigate to the IAM console page.</li>
				<li>Select <strong class="bold">Roles</strong> from the left-hand menu.</li>
				<li>Select <strong class="bold">Create role</strong>.</li>
				<li>Select <strong class="bold">Lambda </strong>as the trusted entity.</li>
				<li>Add the following managed <strong class="source-inline">policies</strong>:<ul><li><strong class="source-inline">AmazonS3ReadOnlyAccess</strong></li><li><strong class="source-inline">AmazonRekognitionFullAccess</strong></li><li><strong class="source-inline">ComprehendFullAccess</strong></li><li><strong class="source-inline">CloudWatchFullAccess</strong></li></ul></li>
				<li>Save the role as <strong class="source-inline">language-detection-from-image-role</strong>.</li>
				<li>Now, let's create the Lambda <a id="_idIndexMarker150"/>function. Navigate to <strong class="bold">Lambda &gt; Functions &gt; Create Function</strong>.</li>
				<li>Name the function <strong class="source-inline">language-detection-from-image</strong>.</li>
				<li>Set the runtime to <strong class="source-inline">Python 3.6</strong>.</li>
				<li>Use our existing role; that is, <strong class="source-inline">language-detection-from-image-role</strong>.</li>
				<li>Download the code from <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Transcribe%20Demo/lambda_function">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Transcribe%20Demo/lambda_function</a>, paste it into the function, and click <strong class="bold">Deploy</strong>. <p>This code will read the text from the image that you uploaded and detect the language of the text. We have used the <strong class="source-inline">detect_text</strong> API from Amazon Rekognition to detect text from an image and the <strong class="source-inline">batch_detect_dominant_language</strong> API from Amazon Comprehend to detect the language of the text. </p></li>
				<li>Now, go to your AWS S3 console and create a bucket called <em class="italic">language-detection-image</em>.</li>
				<li>Create a folder called <strong class="source-inline">input-image</strong> (in this example, we will only upload <strong class="source-inline">.jpg</strong> files).</li>
				<li>Navigate to <strong class="bold">Properties &gt; Events&gt; Add notification</strong>.</li>
				<li>Fill in the required<a id="_idIndexMarker151"/> fields in the <strong class="bold">Events</strong> section with the following information; then, click on <strong class="bold">Save</strong>:<ul><li><strong class="bold">Name</strong>: <strong class="source-inline">image-upload-event</strong></li><li><strong class="bold">Events</strong>: <strong class="source-inline">All object create events</strong></li><li><strong class="bold">Prefix</strong>: <strong class="source-inline">input-image/</strong></li><li><strong class="bold">Suffix</strong>: <strong class="source-inline">.jpg</strong></li><li><strong class="bold">Send to</strong>: <strong class="source-inline">Lambda Function</strong></li><li><strong class="bold">Lambda</strong>: <strong class="source-inline">language-detection-from-image</strong></li></ul></li>
				<li>Navigate to <strong class="bold">Amazon S3&gt;language-detection-image&gt;input-image</strong>. Upload the <strong class="source-inline">sign-image.jpg</strong> image in the folder. (This file is available in this book's GitHub repository at <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Comprehend%20Demo/input_image">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Comprehend%20Demo/input_image</a>).</li>
				<li>This file upload will trigger the lambda function. You can monitor the logs from <strong class="bold">CloudWatch&gt; CloudWatch Logs&gt; Log groups&gt; /aws/lambda/language-detection-from-image</strong>.</li>
				<li>Click on the streams and select the latest one. The detected language is printed in the logs, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B16735_02_018.jpg" alt="Figure 2.18 – The logs in CloudWatch for verifying the output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.18 – The logs in CloudWatch for verifying the output</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">It is suggested that you use batch operations such as <strong class="source-inline">BatchDetectSentiment</strong>or <strong class="source-inline">BatchDetectDominantLanguage</strong> in your production environment. This is because single API operations can cause API-level throttling. More details are available here: <a href="https://docs.aws.amazon.com/comprehend/latest/dg/functionality.html">https://docs.aws.amazon.com/comprehend/latest/dg/functionality.html</a>.</p>
			<p>In this section, we<a id="_idIndexMarker152"/> learned how to use Amazon Comprehend to detect the language of texts. The text is extracted into our Lambda function using Amazon Rekognition. In the next section, we will learn about translating the same text into English via Amazon Translate.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Translating documents with Amazon Translate</h1>
			<p>Most of the time, people<a id="_idIndexMarker153"/> prefer to communicate in their own language, even on digital platforms. Amazon Translate is a text translation service. We can provide documents or strings of text in various languages and get it back in a different language. It uses pre-trained deep learning techniques, so we shouldn't be worried about the models, nor how they are managed. We can make API<a id="_idIndexMarker154"/> requests and get the results back.</p>
			<p>Some common uses of <a id="_idIndexMarker155"/>Amazon Translate include the following: </p>
			<ul>
				<li>If there's an organization-wide requirement to prepare documents in different languages, then Translate is the solution for converting one language into many.</li>
				<li>Online chat applications can be translated in real time to provide a better customer experience.</li>
				<li>To localize website content faster and more affordably into more languages.</li>
				<li>Sentiment analysis can be applied to different languages once they have been translated.</li>
				<li>To provide non-English language support for a news publishing website.</li>
			</ul>
			<p>Next, we'll explore the benefits of Amazon Translate.</p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor047"/>Exploring the benefits of Amazon Translate</h2>
			<p>Some of the benefits of <a id="_idIndexMarker156"/>using Amazon Translate include the following:</p>
			<ul>
				<li>It uses neural machine translation, which mimics the way the human brain works. </li>
				<li>No need to maintain your resources. </li>
				<li>Produces high-quality results and maintains their consistency.</li>
				<li>You can customize your brand names and model names, and any other unique terms get translated using the Custom Terminology Feature.</li>
				<li>Can be easily integrated with applications through APIs.</li>
				<li>Amazon Translate scales itself when you need it to do more.</li>
			</ul>
			<p>Next, we will get hands-on with Amazon Translate.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/>Getting hands-on with Amazon Translate</h2>
			<p>In this section, we will build a<a id="_idIndexMarker157"/> product by integrating AWS Lambda with Amazon Rekognition, Amazon Comprehend, and Amazon Translate to read an image file stored in an S3 bucket. Then, we will detect the language of the text that's been extracted from the image so that we can translate it into English. We will also use CloudWatch to print the translated output. The following is a diagram of our use case:</p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B16735_02_019.jpg" alt="Figure 2.19 – Architecture diagram of the required use case&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.19 – Architecture diagram of the required use case</p>
			<p>Let's start by creating an IAM role:</p>
			<ol>
				<li value="1">Navigate to the IAM console page.</li>
				<li>Select <strong class="bold">Roles</strong> from the left-hand menu.</li>
				<li>Select <strong class="bold">Create role</strong>.</li>
				<li>Select <strong class="bold">Lambda</strong> as the trusted entity.</li>
				<li>Add the following managed policies:<ul><li><strong class="source-inline">AmazonS3ReadOnlyAccess</strong></li><li><strong class="source-inline">AmazonRekognitionFullAccess</strong></li><li><strong class="source-inline">ComprehendFullAccess</strong></li><li><strong class="source-inline">CloudWatchFullAccess</strong></li><li><strong class="source-inline">TranslateFullAccess</strong></li></ul></li>
				<li>Save the role as <strong class="source-inline">language-translation-from-image</strong>.</li>
				<li>The next immediate step is<a id="_idIndexMarker158"/> to create a Lambda function. Navigate to <strong class="bold">Lambda &gt; Functions &gt; Create Function</strong>.</li>
				<li>Name the function <strong class="source-inline">language-detection-from-image</strong>.</li>
				<li>Set the runtime to <strong class="source-inline">Python 3.6</strong>.</li>
				<li>Use an existing role; that is, <strong class="source-inline">language-detection-from-image-role</strong>. </li>
				<li>Paste the code available at <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/blob/master/Chapter-2/Amazon%20Translate%20Demo/lambda_function/lambda_function.py">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/blob/master/Chapter-2/Amazon%20Translate%20Demo/lambda_function/lambda_function.py</a> and click <strong class="bold">Deploy</strong>. We will use the <strong class="source-inline">translate_text</strong> API to translate the input text.</li>
				<li>The next step is to create a bucket called <strong class="source-inline">language-translation-from-image</strong>.<p>Create a folder named <strong class="source-inline">image</strong>. Then, navigate to <strong class="bold">Properties &gt; Events&gt; Add notification</strong>. </p></li>
				<li>Fill in the required fields, as shown here, and click on <strong class="bold">Save</strong> (please make sure you select <strong class="source-inline">.jpg</strong> as the suffix; otherwise, it will trigger the lambda function for any object creation process):<ul><li><strong class="bold">Name</strong>: <em class="italic">translate-language-image</em></li><li><strong class="bold">Events</strong>: <strong class="source-inline">All object create events</strong></li><li><strong class="bold">Prefix</strong>: <strong class="source-inline">image/</strong></li><li><strong class="bold">Suffix</strong>: <strong class="source-inline">.jpg</strong></li><li><strong class="bold">Send to</strong>: <strong class="source-inline">Lambda Function</strong></li><li><strong class="bold">Lambda</strong>: <strong class="source-inline">language-translation-from-image</strong></li></ul></li>
				<li>Navigate to <strong class="bold">Amazon S3&gt;language-detection-image&gt;input-image</strong>. Upload the <strong class="source-inline">sign-image.jpg</strong> image into the folder. This file is available in this book's GitHub repository: <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Translate%20Demo/input_image">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Translate%20Demo/input_image</a>.</li>
				<li>Uploading this image<a id="_idIndexMarker159"/> will trigger the lambda function. You can monitor the logs by going to <strong class="bold">CloudWatch&gt; CloudWatch Logs&gt; Log groups&gt; /aws/lambda/language-translation-from-image</strong>.</li>
				<li>Click on the streams and select the latest one. It will look as follow:</li>
			</ol>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B16735_02_020.jpg" alt="Figure 2.20 – The logs in CloudWatch for verifying the output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.20 – The logs in CloudWatch for verifying the output</p>
			<p>The translation is as follows:</p>
			<p class="source-code">Translation of the text from the Image :   </p>
			<p class="source-code">{'PREVENCION DEL COVID-19': 'PREVENTION OF COVID-19', </p>
			<p class="source-code">'LAVATE LAS MANOS EVITA EL CONTACTO NO TE TOQUES oJOs, </p>
			<p class="source-code">EVITA': 'WASHE HANDS AVOID CONTACT DO NOT TOUCH EYES', </p>
			<p class="source-code">'60 SEGUNDOS CON CONTAGIADOS NARIZ O BOCA </p>
			<p class="source-code">AGLOMERACIONES': '60 SECONDS WITH CONTAGIOUS NOSE OR </p>
			<p class="source-code">MOUTH AGGLOMERATIONS', 'NO COMPARTAS NO VIAJES A MENOS </p>
			<p class="source-code">SI TE PONES ENFERMO': "DON'T SHARE NOT TRAVEL UNLESS </p>
			<p class="source-code">YOU GET SICK", 'CUBIERTOS NI COMIDA QUE SEA NECESARIO </p>
			<p class="source-code">BUSCA AYUDA MEDICA': 'CUTLERY OR FOOD NEEDED SEEK </p>
			<p class="source-code">MEDICAL HELP'}</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">For production use cases, it is recommended to use AWS Lambda with AWS Step Function if you have dependent services or a chain of services.</p>
			<p class="callout">Using the same S3 bucket to store input and output objects is not recommended. Output object creation in the same bucket may trigger recursive Lambda invocation. If you're using same bucket, then we recommend that you use a prefix and suffix to trigger events. Similarly, we recommend using a prefixto store output objects.</p>
			<p>In this section, we learned <a id="_idIndexMarker160"/>how to combine multiple services and chain their output to achieve a particular use case outcome. We learned how to integrate Amazon Rekognition to detect text in an image. The language can then be detected by using Amazon Comprehend. Then, we used the same input and translated it into English with the help of Amazon Translate. The translated output was then printed on CloudWatch logs for verification. In the next section, we will learn about Amazon Textract, which we can use to extract text from a document.</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor049"/>Extracting text from documents with Amazon Textract</h1>
			<p>Manually extracting information from <a id="_idIndexMarker161"/>documents is slow, expensive, and prone to errors. Traditional optical character recognition software needs a lot of customization, and it will still give erroneous output. To avoid such manual processes and errors, you should use <strong class="bold">Amazon Textract</strong>. Generally, we <a id="_idIndexMarker162"/>convert the documents into images in order to detect bounding boxes around the texts in images. We then apply character recognition technique to read the text from it. Textract does all this for you, and also extracts text, tables, forms, and other data for you with minimal effort. If you get low-confidence results from Amazon Textract, then Amazon A2I is the best solution.</p>
			<p>Textract reduces the manual effort of extracting text from millions of scanned document pages. Once the information has been captured, actions can be taken on the text, such as storing it in different data stores, analyzing sentiments, or searching for keywords. The following diagram shows how Amazon Textract works:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B16735_02_021.jpg" alt="Figure 2.21 – Block diagram representation of Amazon Textract and how it stores its output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.21 – Block diagram representation of Amazon Textract and how it stores its output</p>
			<p> Some common uses of<a id="_idIndexMarker163"/> Amazon Textract include the following:</p>
			<ul>
				<li>Documenting processing workflows to extract tables or forms. </li>
				<li>Creating search indexes from documents using Amazon Elasticsearch.</li>
				<li>Redacting personally identifiable information in a workflow; Textract identifies data types and form labels automatically.</li>
			</ul>
			<p>Next, we'll explore the benefits of Amazon Textract.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Exploring the benefits of Amazon Textract</h2>
			<p>There are<a id="_idIndexMarker164"/> several reasons to use Textract, as follows:</p>
			<ul>
				<li>Zero infrastructure cost.</li>
				<li>Fully managed service (reduced development and management overhead).</li>
				<li>Helps you extract both structured and unstructured data.</li>
				<li>Handwritten reviews can be analyzed.</li>
				<li>Amazon Textract performs better than OCR apps, which use flat bag of words.</li>
				<li>Next, we'll get hands-on with Amazon Textract.</li>
			</ul>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor051"/>Getting hands-on with Amazon Textract</h2>
			<p>In this section, we will use<a id="_idIndexMarker165"/> the Amazon Textract API to read an image file from our S3 bucket and print the "FORM" details on Cloudwatch. The same can be stored in S3 in your desired format for further use or can be stored in DynamoDB as a key-value pair. Let's get started:</p>
			<ol>
				<li value="1">First, create an IAM role called <strong class="source-inline">textract-use-case-role</strong> with the following policies. This will allow the Lambda function to execute so that it can read from S3, use Amazon Textract, and print the output in CloudWatch logs:<ul><li><strong class="source-inline">CloudWatchFullAccess</strong></li><li><strong class="source-inline">AmazonTextractFullAccess</strong></li><li><strong class="source-inline">AmazonS3ReadOnlyAccess</strong></li></ul></li>
				<li>Let's create an S3 bucket called <strong class="source-inline">textract-document-analysis</strong> and upload the <strong class="source-inline">receipt.png</strong> image file. This will be used to contain the FORM details that will be extracted. The image file is available here at <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Textract%20Demo/input_doc">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Textract%20Demo/input_doc</a>:<div id="_idContainer033" class="IMG---Figure"><img src="image/B16735_02_022.jpg" alt="Figure 2.22 – An S3 bucket with an image (.png) file uploaded to the input folder&#13;&#10;"/></div><p class="figure-caption">Figure 2.22 – An S3 bucket with an image (.png) file uploaded to the input folder</p></li>
				<li>The next step<a id="_idIndexMarker166"/> is to create a Lambda function called <strong class="source-inline">read-scanned-doc</strong>, as shown in the following screenshot, with an existing execution role called <strong class="source-inline">textract-use-case-role</strong>: <div id="_idContainer034" class="IMG---Figure"><img src="image/B16735_02_023.jpg" alt="Figure 2.23 – The AWS Lambda Create function dialog&#13;&#10;"/></div><p class="figure-caption">Figure 2.23 – The AWS Lambda Create function dialog</p></li>
				<li>Once the function has been created, paste<a id="_idIndexMarker167"/> the following code and deploy it. Scroll down to <strong class="bold">Basic Settings</strong> to change the default timeout to a higher value (40 secs) to avoid timeout errors. We have used the <strong class="source-inline">analyze_document</strong> API from Amazon Textract to get the <strong class="source-inline">Table and Form</strong> details via the <strong class="source-inline">FeatureTypes</strong> parameter of the API:<p class="source-code">import boto3</p><p class="source-code">import time</p><p class="source-code">from trp import Document</p><p class="source-code">textract_client=boto3.client('textract')</p><p class="source-code"> </p><p class="source-code">def lambda_handler(event, context): </p><p class="source-code">    print("- - - Amazon Textract Demo - - -") </p><p class="source-code">    # read the bucket name from the event </p><p class="source-code">    name_of_the_bucket=event['Records'][0]['s3']['bucket'] ['name'] </p><p class="source-code">    # read the object from the event </p><p class="source-code">    name_of_the_doc=event['Records'][0]['s3']['object']['key']</p><p class="source-code">    print(name_of_the_bucket)</p><p class="source-code">    print(name_of_the_doc)</p><p class="source-code">    response = </p><p class="source-code">textract_client.analyze_document(Document={'S3Object': </p><p class="source-code">{'Bucket': name_of_the_bucket,'Name': </p><p class="source-code">name_of_the_doc}},FeatureTypes=["TABLES","FORMS"])</p><p class="source-code">    print(str(response))</p><p class="source-code">    doc=Document(response)</p><p class="source-code">    for page in doc.pages:</p><p class="source-code">        # Print tables</p><p class="source-code">        for table in page.tables:</p><p class="source-code">            for r, row in enumerate(table.rows):</p><p class="source-code">                for c, cell in enumerate(row.cells):</p><p class="source-code">                    print("Table[{}][{}] = </p><p class="source-code">{}".format(r, c, cell.text))</p><p class="source-code">    for page in doc.pages:</p><p class="source-code">        # Print fields</p><p class="source-code">        print("Fields:")</p><p class="source-code">        for field in page.form.fields:</p><p class="source-code">            print("Key: {}, Value: </p><p class="source-code">{}".format(field.key, field.value))</p><p>Unlike the previous <a id="_idIndexMarker168"/>examples, we will create a test configuration to run our code.</p></li>
				<li>Click on the dropdown left of the <strong class="bold">Test</strong> button.</li>
				<li>Select <strong class="bold">Configure test events</strong> and choose <strong class="bold">Create new test event</strong>.</li>
				<li>Select <strong class="bold">Amazon S3 Put</strong> from the <strong class="bold">Event template</strong> dropdown.</li>
				<li>In the JSON body, change the highlighted values as per our bucket name and key, as shown here:<div id="_idContainer035" class="IMG---Figure"><img src="image/B16735_02_024.jpg" alt="Figure 2.24 – The Event template for testing the lambda function&#13;&#10;"/></div><p class="figure-caption">Figure 2.24 – The Event template for testing the lambda function</p></li>
				<li>In the <strong class="bold">Event name</strong> field, name<a id="_idIndexMarker169"/> the test configuration<em class="italic"> </em><strong class="source-inline">TextractDemo</strong>. </li>
				<li>Click <strong class="bold">Save</strong>. </li>
				<li>Select your test configuration (<strong class="source-inline">TextractDemo</strong>) and click on <strong class="bold">Test</strong>:<div id="_idContainer036" class="IMG---Figure"><img src="image/B16735_02_025.jpg" alt="Figure 2.25 – Selecting the test configuration before running your test&#13;&#10;"/></div><p class="figure-caption">Figure 2.25 – Selecting the test configuration before running your test</p></li>
				<li>This will trigger the lambda function. You can monitor the logs from <strong class="bold">CloudWatch&gt; CloudWatch Logs&gt; Log groups&gt; /aws/lambda/ read-scanned-doc</strong>.</li>
				<li>Click on the<a id="_idIndexMarker170"/> streams and select the latest one. It will look as follows; the key-value pairs can be seen in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B16735_02_026.jpg" alt="Figure 2.26 – The logs in CloudWatch for verifying the output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.26 – The logs in CloudWatch for verifying the output</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The most scalable and cost-effective way to generate S3 Put events for asynchronously invocating downstream AI workflows via Lambda is to generate an AWS Pre-Signed URL, and then provide it to your mobile or web application users. Many users can be served at the same time via this approach, and it may increase performance and throughput.</p>
			<p class="callout">Considering the<a id="_idIndexMarker171"/> same region for your AWS AI services and S3 bucket may improve performance and reduce network latency. AWS VPC endpoints can leverage enhanced security without using the public internet. You can store the AWS AI results in an AWS S3 bucket and encrypt the rest to attain better security.</p>
			<p>In this section, we learned how to extract text from a scanned document and print the form data out of it. Unlike the other sections, we used the testing feature of a lambda function by creating<a id="_idIndexMarker172"/> a test configuration that includes an event template. In the next section, we will learn about creating a chatbot for organizations and learn how to use it.</p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/>Creating chatbots on Amazon Lex </h1>
			<p>Most of the features that <a id="_idIndexMarker173"/>are available in Alexa are powered by <strong class="bold">Amazon Lex</strong>. You can easily build a chatbot using Amazon Lex. It uses natural language<a id="_idIndexMarker174"/> understanding and automatic speech recognition behind the scenes. Through SLU, Amazon Lex takes natural language speech and text input, understands the intent, and fulfills the intent of the user. An Amazon Lex bot can be created either from the console or via APIs. Its basic requirements are shown in the upcoming diagram.</p>
			<p>Some common uses of<a id="_idIndexMarker175"/> Amazon Lex include the following:</p>
			<ul>
				<li>Apps that both listen and take input as text.</li>
				<li>Chatbots.</li>
				<li>Conversational AI products to provide a better customer and sales experience.</li>
				<li>Custom business bots for assistance through AWS Lambda functions.</li>
				<li>Voice assistants for your call center, which can speak to a user, schedule a meeting, or request details of your account.</li>
				<li>By integrating with Amazon Cognito, you can control user management, authentication, and sync across all your devices.</li>
			</ul>
			<p>Next, we will explore the benefits of Amazon Lex.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor053"/>Exploring the benefits of Amazon Lex</h2>
			<p>Some reasons for<a id="_idIndexMarker176"/> using Lex include the following:</p>
			<ul>
				<li>Chatbots can be directly built and tested from the AWS management console. These chatbots can be easily integrated into Facebook Messenger, Slack, and Twilio SMS via its rich formatting capabilities. </li>
				<li>Conversation logs can be stored in Amazon CloudWatch for further analysis. You can use them to monitor your bot and derive insights to improve your user experience.</li>
				<li>Amazon Lex can be integrated into other AWS services such as Amazon Cognito, AWS Lambda, Amazon DynamoDB, Amazon CloudWatch, and AWS Mobile Hub to leverage application security, monitoring, user authentication, business logic, storage, and mobile app development in AWS platforms.</li>
				<li>Amazon Lex chatbots <a id="_idIndexMarker177"/>can be integrated into your custom web applications too. You just need to build a chatbot widget and integrate it into your UI.</li>
			</ul>
			<p>Next, we'll get hands-on with Amazon Lex.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor054"/>Getting hands-on with Amazon Lex</h2>
			<p>Let's get <a id="_idIndexMarker178"/>started:</p>
			<ol>
				<li value="1">Log into <a href="https://console.aws.amazon.com/lex/">https://console.aws.amazon.com/lex/</a>.</li>
				<li>Click on <strong class="bold">Get Started</strong> and select <strong class="bold">Custom bot</strong>.</li>
				<li>Fill in the following details and click on <strong class="bold">Create</strong>:<div id="_idContainer038" class="IMG---Figure"><img src="image/B16735_02_027.jpg" alt="Figure 2.27 – The Create dialog of Amazon Lex&#13;&#10;"/></div><p class="figure-caption">Figure 2.27 – The Create dialog of Amazon Lex</p></li>
				<li>Click on <strong class="bold">Create Intent</strong>. A dialog will appear. Select <strong class="bold">Create Intent</strong>.</li>
				<li>Name the new intent <strong class="source-inline">MovieIntent</strong> and click on <strong class="bold">Add</strong>.</li>
				<li>Go to the <strong class="bold">Slots</strong> section and<a id="_idIndexMarker179"/> use the following details:<ul><li>Name: <strong class="source-inline">movie_type</strong></li><li>Slot type: <strong class="source-inline">AMAZON.Genre</strong></li><li>Prompt: <strong class="source-inline">Which movie do you like?</strong></li></ul></li>
				<li>Click on the <strong class="bold">+</strong> in <strong class="bold">Settings</strong>.<p>Some sample utterances can be seen in the following screenshot. In this example, <strong class="source-inline">movie_type</strong> is my variable:</p><div id="_idContainer039" class="IMG---Figure"><img src="image/B16735_02_028.jpg" alt="Figure 2.28 – The Sample utterances section&#13;&#10;"/></div><p class="figure-caption">Figure 2.28 – The Sample utterances section</p></li>
				<li>Scroll down to the <strong class="bold">Response</strong> section to <a id="_idIndexMarker180"/>add a message:<div id="_idContainer040" class="IMG---Figure"><img src="image/B16735_02_029.jpg" alt="Figure 2.29 – The Response section of Amazon Lex&#13;&#10;"/></div><p class="figure-caption">Figure 2.29 – The Response section of Amazon Lex</p></li>
				<li>Scroll down to <strong class="bold">Save Intent</strong> and <a id="_idIndexMarker181"/>click on <strong class="bold">Build</strong>. Upon successfully building the prompt, the following success message will appear:<div id="_idContainer041" class="IMG---Figure"><img src="image/B16735_02_030.jpg" alt="Figure 2.30 – The Response section of Amazon Lex&#13;&#10;"/></div><p class="figure-caption">Figure 2.30 – The Response section of Amazon Lex</p></li>
				<li>Now, you can test<a id="_idIndexMarker182"/> your bot, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B16735_02_031.jpg" alt="Figure 2.31 – The Test bot dialog&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.31 – The Test bot dialog</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Not all Amazon Polly features are <a id="_idIndexMarker183"/>available within Alexa – particularly the Amazon Polly SSML features – which makes Amazon Polly and Alexa different.</p>
			<p>That concludes this chapter's introduction to the various AWS application services that are available.</p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/>Summary</h1>
			<p>In this chapter, we learned about a few of the AWS AI services that can be used to solve various problems. We used the Amazon Rekognition service, which detects objects and faces (including celebrity faces), and can also extract text from images. For text to speech, we used Amazon Polly, while for speech to text, we used Amazon Transcribe. Toward the end of this chapter, we built a chatbot in Amazon Lex.</p>
			<p>For language detection and translation in an image, we used Amazon Rekognition, Amazon Comprehend, and Amazon Translate. We learned how to combine all of them into one Lambda function to solve our problem. </p>
			<p>For the certification exam, you don't need to remember all the APIs we used in this chapter. There may be questions on a few of the best practices that we learned or on the names of services that solve a specific problem. It is always good to practice using these AWS AI services as it will enhance your architecting skills.</p>
			<p>In the next chapter, we will learn about data preparation and transformation, which is the most important aspect of machine learning. </p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/>Questions</h2>
			<ol>
				<li value="1">Using pre-defined logic and rules to make product recommendations to online shoppers is an example of machine learning.<p>a. TRUE</p><p>b. FALSE</p></li>
				<li>Which level of the ML stack helps you build custom ML models without managing infrastructure?<p>a. Top level (the AI services)</p><p>b. Middle level (Amazon SageMaker)</p><p>c. Bottom level (ML frameworks and infrastructure)</p><p>d. Your own infrastructure and code level</p></li>
				<li>Which of the following can you do with Amazon Textract?<p>a. Detect key-value pairs in documents</p><p>b. Build a custom ML model for text extraction</p><p>c. Send text extraction with low confidence scores for human review</p><p>d. Translate the detected text into English</p></li>
				<li>With Amazon Comprehend, a new model can be trained to help you extract custom entities from text.<p>a. FALSE</p><p>b. TRUE</p></li>
				<li>Which of the following is an example of the type of data Amazon Comprehend is designed to analyze?<p>a. Social media posts</p><p>b. Data in a table</p><p>c. Log files</p><p>d. GPS data</p><p class="callout-heading">Answer</p><p class="callout">For log files, we can use CloudWatch Log Insights.</p></li>
				<li>When calling the <strong class="source-inline">DetectKeyPhrases</strong> API, which of the following is not returned by Amazon Comprehend?<p>a. The key phrases</p><p>b. The count of each key phrase</p><p>c. The confidence level for each key phrase</p><p>d. The sentiment of each key phrase</p><p class="callout-heading">Answer</p><p class="callout">It has nothing to do with sentiment.</p></li>
				<li>You want to create a Lex bot that can help you order pizza. Why is it important to add slots as part of intent configuration?<p>a. So you can customize your orders with different pizza sizes and toppings.</p><p>b. So you can account for different ways you might convey your intent to order pizza.</p><p>c. So that a lambda function can be automatically set up for you to fulfill the intent.</p></li>
				<li>Let's say you're responsible for building a system that analyzes the sentiment of a customer chat. Which service should you integrate with Amazon Lex to do this?<p>a. Amazon Transcribe</p><p>b. Amazon Comprehend</p><p>c. Amazon Translate</p><p>d. Amazon Textract</p></li>
				<li>In which situation would an Amazon Lex fallback intent help?<p>a. When a user orders pizza but, due to background noise, the bot needs the user to repeat what they said.</p><p>b. When a bot has to use a previous exchange with a user to pretend to understand an unclear message from that user.</p><p>c. When a bot is asked a question that is not programmed to answer.</p><p class="callout-heading">Answer</p><p class="callout">Fallback intent is meant for those inputs that a bot doesn't expect.</p></li>
				<li>Which three of the following options can Amazon Textract handle that traditional OCR methods are not able to? <p>a. Extracting words and lines from documents</p><p>b. Extracting forms (key/values) from documents without using any templates</p><p>c. Handling non-textual content such as radio buttons and checkboxes</p><p>d. Preserving the composition of data stored in tables </p></li>
				<li>Which of the following is a common use case for integrating Amazon Textract with Amazon A2I (human review)?<p>a. You want to identify form labels and values from an image or PDF document.</p><p>b. You are extracting data from a document that requires review due to regulatory requirements or sensitive business decisions.</p><p>c. You have tables in your document, and you need to preserve the composition of the data stored in those tables.</p></li>
				<li>You are looking to extract form or table data in a document. You need to do this synchronously because your use is latency-sensitive, such as mobile capture. What API should you use?<p>a. AnalyzeDocument</p><p>b. DetectDocumentText</p><p>c. StartDocumentAnalysis</p><p>d. GetDocumentTextDetection</p></li>
			</ol>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor057"/>Answers</h2>
			<p>1. B</p>
			<p>2. B</p>
			<p>3. A, C</p>
			<p>4. B</p>
			<p>5. A</p>
			<p>6. D</p>
			<p>7. A</p>
			<p>8. B</p>
			<p>9. C</p>
			<p>10. B, C, D</p>
			<p>11. B</p>
			<p>12. A</p>
		</div>
	</body></html>