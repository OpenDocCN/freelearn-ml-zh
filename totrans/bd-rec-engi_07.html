<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Building Real-Time Recommendation Engines with Spark"><div class="titlepage" id="aid-1GKCM2"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Building Real-Time Recommendation Engines with Spark</h1></div></div></div><p>In this day and age, the need to build scalable real-time recommendations is increasing day by day. With more internet users using e-commerce sites for their purchases, these e-commerce sites have realized the potential of understanding the patterns of the users' purchase behavior to improve their business, and to serve their customers on a very personalized level. To build a system which caters to a huge user base and generates recommendations in real time, we need a modern, fast scalable system. <span class="strong"><strong>Apache Spark</strong></span>, which is a special framework designed for distributed in-memory data processing, comes to our rescue. Spark applies a set of transformations and actions to distributed data to build real-time data mining applications.</p><p>In the previous chapters, we learned about implementing similarity-based collaborative filtering approaches, such as user-based collaborative filtering and content-based collaborative filtering. Though the similarity-based approaches are a huge success in commercial applications, there came into existence model-based recommender models, such as matrix factorization models, which have improved the performance of recommendation engine models. In this chapter, we will learn about the model-based approach of collaborative filtering, moving away from the heuristic-based similarity approaches. Also, we will focus on implementing the model-based collaborative filtering approach using Spark.</p><p>In this chapter, we will learn about the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">What is in Spark 2.0</li><li class="listitem">Setting up the pyspark environment</li><li class="listitem">Basic Spark concepts</li><li class="listitem">The MLlib recommendation engine module</li><li class="listitem">The Alternating Least Squares algorithm</li><li class="listitem">Data exploration of the Movielens-100k dataset</li><li class="listitem">Building model-based recommendation engines using ALS</li><li class="listitem">Evaluating the recommendation engine model</li><li class="listitem">Parameter tuning</li></ul></div><div class="section" title="About Spark 2.0"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec44"/>About Spark 2.0</h1></div></div></div><p><span class="strong"><strong>Apache Spark</strong></span> is a fast, powerful, easy-to-use, distributed, in-memory, and open source cluster computing framework built to perform advanced analytics. It was originally developed at UC Berkeley in 2009. Spark has been widely adopted by enterprises across a wide range of industries since its inception.</p><p>One of the main advantages of Spark is that it takes all the complexities away from us, such as resources scheduling, job submissions, executions, tracking, between-node communication, fault tolerance, and all low-level operations that are inherent features of parallel processing. The Spark framework helps us write programs to run on the clusters in parallel.</p><p>Spark can be run both as a standalone mode and as a cluster mode. Spark can be easily integrated with Hadoop platforms.</p><p>As a general-purpose computing engine, Spark with its in-memory data processing capability and easy-to-use APIs allows us to efficiently work on a wide range of large-scale data processing tasks, such as streaming applications, machine learning, or interactive SQL queries over large datasets that require iterative access.</p><p>Spark can be easily integrated with many applications, data sources, storage platforms, and environments, and exposes high-level APIs in Java, Python, and R to work with. Spark has proved to be broadly useful for a wide range of large-scale data processing tasks, over and above machine learning and iterative analytics.</p><div class="mediaobject"><img src="../Images/image00398.jpeg" alt="About Spark 2.0"/><div class="caption"><p>Credits: Databricks</p></div></div><p style="clear:both; height: 1em;"> </p><div class="section" title="Spark architecture"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec80"/>Spark architecture</h2></div></div></div><p>The Apache Spark ecosystem contains many components to work with distributed, in-memory, and machine-learning data processing tools. The main components of Spark are discussed in the following sub-sections. Spark works on a master-slave architecture; a high-level architecture is shown in the following diagram:</p><div class="mediaobject"><img src="../Images/image00399.jpeg" alt="Spark architecture"/><div class="caption"><p>Credits: Databricks</p></div></div><p style="clear:both; height: 1em;"> </p><p>The Spark cluster works on the master-slave architecture. The Spark Core execution engine accepts requests from clients and passes them to the master node. The driver program in the master communicates with the worker node executors to get the work done as shown in the following diagram:</p><div class="mediaobject"><img src="../Images/image00400.jpeg" alt="Spark architecture"/></div><p style="clear:both; height: 1em;"> </p><p><span class="strong"><strong>Spark driver program</strong></span>: The driver program acts as a master node in a Spark cluster, which hosts the SparkContext for Spark applications. It receives the client request and co-ordinates with the cluster manager which manages the worker nodes. The driver program splits the original request into tasks and schedules them to run on executors in worker nodes. All the processes in Spark are Java processes. The SparkContext creates <span class="strong"><strong>Resilient Distributed Datasets</strong></span> (<span class="strong"><strong>RDD</strong></span>), an immutable, distributable collection of datasets partitioned across nodes, and performs a series of transformations and actions to compute the final output. We will learn more about RDDs in the latter sections.</p><p><span class="strong"><strong>Worker nodes</strong></span>: A worker contains executors, where the actual task execution happens in the form of Java processes. Each worker runs its own Spark instance and is the main compute node in Spark. When a SparkContext is created, each worker node starts its own executors to receive the tasks.</p><p><span class="strong"><strong>Executors</strong></span>: These are the main task executioners of Spark applications.</p></div><div class="section" title="Spark components"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec81"/>Spark components</h2></div></div></div><p>In this section, we will see the core components of the Spark ecosystem. The following diagram shows the Apache Spark Ecosystem:</p><div class="mediaobject"><img src="../Images/image00401.jpeg" alt="Spark components"/><div class="caption"><p>Credits:Databricks</p></div></div><p style="clear:both; height: 1em;"> </p></div><div class="section" title="Spark Core"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec82"/>Spark Core</h2></div></div></div><p><span class="strong"><strong>Spark Core</strong></span> is the core part of the Spark platform: the execution engine. All other functionalities are built on top of Spark Core. This provides all the capabilities of Spark, such as in-memory distributed computation, and fast, easy-to-use APIs.</p><div class="section" title="Structured data with Spark SQL"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec21"/>Structured data with Spark SQL</h3></div></div></div><p><span class="strong"><strong>Spark SQL</strong></span> is a component on top of Spark Core. It is a spark module that provides support for structured and semi-structured data.</p><p>Spark SQL provides a unified approach, to allow users to query the data objects in an interactive SQL type, such as applying select, where you can group data objects by the kind of operations through data abstraction APIs, such as DataFrames.</p><p>A considerable amount of time will be dedicated to data exploration, exploratory analysis, and SQL-like interactions. Spark SQL, which provides DataFrames, also acts as a distributed SQL query engine; for instance, in R, the DataFrames in Spark 2.0, the data is stored as rows and columns, access to which is allowed as an SQL table with all the structural information, such as data types.</p></div><div class="section" title="Streaming analytics with Spark Streaming"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec22"/>Streaming analytics with Spark Streaming</h3></div></div></div><p><span class="strong"><strong>Spark Streaming</strong></span> is another Spark module that enables users to process and analyze both batch and streaming data in real time, to perform interactive and analytical applications. Spark Streaming provides <span class="strong"><strong>Discretized Stream</strong></span> (<span class="strong"><strong>DStream</strong></span>), a high-level abstraction, to represent a continuous stream of data.</p><p>The main features of Spark Streaming API are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Scalable</li><li class="listitem">High throughput</li><li class="listitem">Fault-tolerant</li><li class="listitem">Processes live stream of incoming data</li><li class="listitem">Can connect to real-time data sources and process real-time incoming data on the go</li><li class="listitem">Can apply complex machine learning and graph processing algorithms on streaming data</li></ul></div><div class="mediaobject"><img src="../Images/image00402.jpeg" alt="Streaming analytics with Spark Streaming"/></div><p style="clear:both; height: 1em;"> </p></div><div class="section" title="Machine learning with MLlib"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec23"/>Machine learning with MLlib</h3></div></div></div><p><span class="strong"><strong>MLlib</strong></span> is another module in Spark, built on top of Spark Core. This machine learning library is developed with an objective to make practical machine learning scalable and easy to use.</p><p>This library provides tools for data scientists, such as the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Machine learning algorithms for regression, classification, clustering, and recommendation engines</li><li class="listitem">Feature extraction, feature transformation, dimensionality reduction, and feature selection</li><li class="listitem">Pipeline tools for streamlining machine learning processes for construction, evaluation, and tuning the over process of solving a machine learning problem</li><li class="listitem">Persistence of storing and loading machine learning models and pipelines</li><li class="listitem">Utilities, such as linear algebra and statistical tasks</li></ul></div><p>When starting Spark 2.0, the old MLlib model is replaced with ML library, which is built with DataFrames APIs, providing more optimizations and making uniform APIs across all languages.</p></div><div class="section" title="Graph computation with GraphX"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec24"/>Graph computation with GraphX</h3></div></div></div><p><span class="strong"><strong>GraphX</strong></span> is a new Spark API for building graph-based systems. It is a graph-parallel processing computation engine and distributed framework, which is built on top of Spark Core. This project was started with the objective of unifying the graph-parallel and data distribution framework into a single Spark API. GraphX enables users to process data both as RDDs and graphs.</p><p>GraphX provides many features, such as the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Property graphs</li><li class="listitem">Graph-based algorithms, such as PageRank, connected components, and Graph Builders, which are used to build graphs</li><li class="listitem">Basic graph computational components, such as subgraph, joinVertices, aggregateMessages, Pregel API, and so on</li></ul></div><p>Though graph models are not within the scope of this book, we will learn some fundamentals of graph applications in <a class="link" title="Chapter 8.  Building Real-Time Recommendations with Neo4j" href="part0057.xhtml#aid-1MBG21">Chapter 8</a>, <span class="emphasis"><em>Building Real-Time Recommendations with Neo4j</em></span>.</p></div></div><div class="section" title="Benefits of Spark"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec83"/>Benefits of Spark</h2></div></div></div><p>The main advantages of Spark are, it is fast, has in-memory framework, contains several APIs, making it very easy to use, its Unified Engine for large quantities of data, and its machine learning components. Unlike Map-Reduce model with its batch mode, which is slower and contains lot of programming, Spark is faster, with real-time and easy to code framework.</p><p>The following diagram shows the above mentioned benefits:</p><div class="mediaobject"><img src="../Images/image00403.jpeg" alt="Benefits of Spark"/></div><p style="clear:both; height: 1em;"> </p></div><div class="section" title="Setting up Spark"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec84"/>Setting up Spark</h2></div></div></div><p>Spark runs on both Windows and UNIX-like systems (for example, Linux, Mac OS). It's easy to run locally on one machine; all you need is to have Java installed on your system PATH or the <code class="literal">JAVA_HOME</code> environment variable pointing to a Java installation.</p><p>Spark runs on Java 7+, Python 2.6+/3.4+ and R 3.1+. For the Scala API, Spark 2.0.0 uses Scala 2.11. You will need to use a compatible Scala version (2.11.x).</p><p>Get Spark from the downloads page of the project website:</p><p><a class="ulink" href="http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.7.tgz">http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.7.tgz</a></p><p>Spark needs to be built against a specific version of Hadoop in order to access <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>), as well as standard and custom Hadoop input sources.</p><p>Spark requires the Scala programming language (version 2.10.4 at the time of writing this book) in order to run. Fortunately, the prebuilt binary package comes with the Scala runtime packages included, so you don't need to install Scala separately in order to get started. However, you will need to have a <span class="strong"><strong>Java Runtime Environment</strong></span> (<span class="strong"><strong>JRE</strong></span>) or <span class="strong"><strong>Java Development Kit</strong></span> (<span class="strong"><strong>JDK</strong></span>) installed (take a look at the software and hardware list in this book's code bundle for installation instructions).</p><p>Once you have downloaded the Spark binary package, unpack the contents of the package and change it into the newly created directory by running the following commands:</p><pre class="programlisting">
<span class="strong"><strong>tar xfvz spark-2.0.0-bin-hadoop2.7.tgz &#13;
cd spark-2.0.0-bin-hadoop2.7</strong></span>
</pre><p>Spark places user scripts to run Spark in the bin directory. You can test whether everything is working correctly by running one of the example programs included in Spark:</p><pre class="programlisting">
<span class="strong"><strong>------------------ &#13;
./bin/run-example org.apache.spark.examples.SparkPi &#13;
16/09/26 15:20:36 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.845103 s &#13;
Pi is roughly 3.141071141071141 &#13;
--------------------</strong></span>
</pre><p>You can run Spark interactively with Scala using the following command:</p><pre class="programlisting">
<span class="strong"><strong>./bin/spark-shell --master local[2]</strong></span>
</pre><p>The <code class="literal">--master</code> option specifies the master URL for a distributed cluster, or you can use <code class="literal">local</code> to run locally with one thread or <code class="literal">local[N]</code> to run locally with N threads. You should start by using <code class="literal">local</code> for testing. For a full list of options, run the Spark shell with the <code class="literal">--help</code> option.</p><p>Source:<a class="ulink" href="http://spark.apache.org/docs/latest/">http://spark.apache.org/docs/latest/</a></p></div><div class="section" title="About SparkSession"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec85"/>About SparkSession</h2></div></div></div><p>From Spark 2.0, the <code class="literal">SparkSession</code> will be the entry point for Spark applications. The <code class="literal">SparkSession</code> serves as the main interactive access point for underlying Spark functionalities and Spark programming capabilities, such as DataFrames API and Dataset API. We use <code class="literal">SparkSession</code> to create DataFrame objects.</p><p>In the earlier versions of Spark, we used to create <code class="literal">SparkConf</code>, <code class="literal">SparkContext</code>, or <code class="literal">SQLContext</code> to interact with Spark, but since Spark 2.0, this has been taken care of by <code class="literal">SparkSession</code> by encapsulating <code class="literal">SparkConf</code>, <code class="literal">SparkContext</code> automatically.</p><p>When you start Spark in the shell command, <code class="literal">SparkSession</code> is created automatically as <code class="literal">spark</code></p><p>We can programmatically create SparkSession, as follows:</p><pre class="programlisting">
<span class="strong"><strong>spark = SparkSession\ &#13;
    .builder\ &#13;
    .appName("recommendationEngine")\ &#13;
     config("spark.some.config.option", "some-value")\ &#13;
    .getOrCreate() &#13;
</strong></span>
</pre></div><div class="section" title="Resilient Distributed Datasets (RDD)"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec86"/>Resilient Distributed Datasets (RDD)</h2></div></div></div><p>The core of Spark is Resilient Distributed Datasets, in short, RDD. RDD is an immutable distributed collection of objects of some datatype of your data, partitioned across nodes on your cluster. This RDD is fault-tolerant, that is, a property of the system that is able to operate continuously, even in the event of failure by reconstructing the failed partition.</p><p>In short, we can say that RDD is a distributed dataset abstraction, which allows iterative operations on very large-scale cluster systems in a fault-tolerant way.</p><p>RDDs can be created in a number of ways, such as parallelizing an existing collection of data objects, or referencing an external file system, such as HDFS:</p><p>Creating RDD from an existing data object:</p><pre class="programlisting">coll = List("a", "b", "c", "d", "e") &#13;
 &#13;
rdd_from_coll = sc.parallelize(coll) &#13;
 &#13;
</pre><p>Creating RDD from a referenced file:</p><pre class="programlisting">rdd_from_Text_File = sc.textFile("testdata.txt") &#13;
</pre><p>RDD supports two types of operations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Transformations</strong></span>: This operation creates new RDDs from existing RDDs, which are immutable</li><li class="listitem"><span class="strong"><strong>Actions</strong></span>: This operation returns values after performing computation on the dataset</li></ul></div><p>These RDD transformations are executed lazily only when the final results are required. We can recreate or recompute the RDDs any number of times, or we can persist them by caching them in memory if we know we may need them in the future.</p></div><div class="section" title="About ML Pipelines"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec87"/>About ML Pipelines</h2></div></div></div><p>The ML Pipelines API in Spark 2.0 is the way to use a standard workflow when solving machine learning problems. Every machine learning problem will undergo a sequence of steps, such as the following:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Loading data.</li><li class="listitem">Feature extraction.</li><li class="listitem">Model training.</li><li class="listitem">Evaluation.</li><li class="listitem">Predictions.</li><li class="listitem">Model tuning.</li></ol><div style="height:10px; width: 1px"/></div><p>If we closely observe the aforementioned steps, we can see the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The ML process follows a series of steps as if it is a workflow.</li><li class="listitem">Often, we require more than one algorithm while solving a machine learning problem; for example, a text classification problem might require a feature extraction algorithm for feature extraction.</li><li class="listitem">Generating predictions on test data may require many data transformations or data-preprocessing steps, which are used during model training. For example, in text classification problems, making predictions on test data involves data pre-processing steps, such as tokenization and feature extraction, before applying them to a generated classification model, which was used during model creation on training data.</li></ul></div><p>The preceding steps form one of the main motivating factors behind introducing the ML Pipeline API. The ML Pipeline module allows users to define a sequence of stages, so that it's easy to use. The API framework allows the ML process to scale on a distributed platform and accommodate very large datasets, reuse some components, and so on.</p><p>The components of the ML Pipeline module are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>DataFrame</strong></span>: As mentioned earlier, DataFrame is a way of representing the data in the Spark framework.</li><li class="listitem"><span class="strong"><strong>Transformers</strong></span>: Transformers take input DataFrames and transform data into new DataFrames. Transformation classes contain the <code class="literal">transform()</code> method to do the transformations.</li><li class="listitem"><span class="strong"><strong>Estimators</strong></span>: Estimators compute the final results. An Estimator class makes use of the <code class="literal">fit()</code> method to compute results.</li><li class="listitem"><span class="strong"><strong>Pipeline</strong></span>: This is a set of Transformers and Estimators stacked as a workflow.</li><li class="listitem"><span class="strong"><strong>Parameters</strong></span>: This refers to the set of parameters that may be used by both Transformers and Estimators.</li></ul></div><p>We'll illustrate this for the simple text document workflow. The following figure is for the training time usage of a pipeline:</p><div class="mediaobject"><img src="../Images/image00404.jpeg" alt="About ML Pipelines"/></div><p style="clear:both; height: 1em;"> </p><p>Find below the explanation for the steps shown in the preceding figure. The blue boxes are Transformers and the red box is the Estimator.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The Tokenizer Transformer takes the text column of a DataFrame as input and returns a new DataFrame column containing tokens.</li><li class="listitem">The HashingTF Transformer takes in the tokens DataFrame from the previous step as input and creates new DataFrame features as output.</li><li class="listitem">Now the LogisticRegression Estimator takes in the features DataFrame, fits a logistic regression model, and creates a PipelineModel transformer.</li></ol><div style="height:10px; width: 1px"/></div><p>First we build a pipeline, which is an Estimator, then on this Pipeline we apply <code class="literal">fit()</code> method which produces a PipelineModel, a Transformer, that can be used on test data or at prediction time.</p><p>Source:<a class="ulink" href="http://spark.apache.org/docs/latest/ml-guide.html">http://spark.apache.org/docs/latest/ml-guide.html</a></p><p>The following figure illustrates this usage:</p><div class="mediaobject"><img src="../Images/image00405.jpeg" alt="About ML Pipelines"/></div><p style="clear:both; height: 1em;"> </p><p>In the preceding figure, when we want to make predictions on the test data, we observe that first the test data has to pass through a series of data-preprocessing steps, which are very much identical to the aforementioned training step. After the pre-processing step is completed, the features of the test data are applied to the logistic regression model.</p><p>In order to make the data-preprocessing and feature extraction steps identical, we will pass the test data to the PipelineModel (logistic regression model) by calling the <code class="literal">transform()</code> to generate the predictions.</p></div></div></div>
<div class="section" title="Collaborative filtering using Alternating Least Square"><div class="titlepage" id="aid-1HIT82"><div><div><h1 class="title"><a id="ch07lvl1sec45"/>Collaborative filtering using Alternating Least Square</h1></div></div></div><p>In this section, let's explain the Matrix Factorization Model (MF) and the Alternating Least Squares method. Before we get to know about the Matrix Factorization Model, we'll define the objective once again. Imagine we have ratings given to items by a number of users. Let's define the ratings given by users on items in a matrix form given by <span class="emphasis"><em>R</em></span>, as shown in the following diagram:</p><div class="mediaobject"><img src="../Images/image00406.jpeg" alt="Collaborative filtering using Alternating Least Square"/></div><p style="clear:both; height: 1em;"> </p><p>In the preceding diagram, we observe that user Ted has rated items B and D as 4 and 3 respectively. In a collaborative filtering approach, the first step before generating recommendations is to fill the empty spaces, that is, to predict the non-rated items. Once the non-rated item ratings are filled, we suggest new items to the users by ranking the newly filled items.</p><p>In the previous chapters, we have seen neighbouring methods using Euclidean distances and cosine distances to predict the missing values. In this section, we will adopt a new method to fill the missing non-rated items. This approach is called the matrix factorization method. This is a mathematical approach, which uses matrix decomposition methods. This method is explained as follows:</p><p>A matrix can be decomposed into two low rank matrices, which, when multiplied back, will result in a single matrix approximately equal to the original matrix.</p><p>Let's say a rating matrix <span class="emphasis"><em>R</em></span> of size <span class="emphasis"><em>U X M</em></span> can be decomposed into two low rank matrices <span class="emphasis"><em>P</em></span> and <span class="emphasis"><em>Q</em></span> of size <span class="emphasis"><em>U X K</em></span> and <span class="emphasis"><em>M X K</em></span> respectively, where <span class="emphasis"><em>K</em></span> is called the rank of the matrix.</p><p>In the following example, let's say the original matrix of size <span class="emphasis"><em>4 X 4</em></span> is decomposed into two matrices: <span class="emphasis"><em>P (4 X 2)</em></span> and <span class="emphasis"><em>Q (4 X 2)</em></span>. Multiplying back <span class="emphasis"><em>P</em></span> and <span class="emphasis"><em>Q</em></span> will give us the original matrix of size <span class="emphasis"><em>4 X 4</em></span> and values approximately equal to those of the original matrix:</p><div class="mediaobject"><img src="../Images/image00407.jpeg" alt="Collaborative filtering using Alternating Least Square"/></div><p style="clear:both; height: 1em;"> </p><p>The principle of matrix factorization is used in recommendation engines, to fill the non-rated items. The assumption in applying the aforementioned principle to recommendation engines is that the ratings given by users on items are based on some latent features. These latent features are applicable to both users and items, that is, a user rates an item because of some of his personal preferences for it. Also, the user rates items because of certain features of the items.</p><p>Using this assumption, when the matrix factorization method is applied to the ratings matrix, we decompose the original ratings matrix into two matrices follows as user-latent factor matrix, P, and item-latent factor matrix:</p><div class="mediaobject"><img src="../Images/image00408.jpeg" alt="Collaborative filtering using Alternating Least Square"/></div><p style="clear:both; height: 1em;"> </p><p>Now, let's come back to the machine learning approach; you must be wondering what the learning in this approach is. Observe the following formula:</p><div class="mediaobject"><img src="../Images/image00409.jpeg" alt="Collaborative filtering using Alternating Least Square"/></div><p style="clear:both; height: 1em;"> </p><p>We have learnt that when we multiply back the two latent factor matrices, we get the approximate original matrix. Now, in order to improve the accuracy of the model, that is, to learn the optimal factor vectors, P and Q, we define an optimization function, shown in the preceding formula, which minimizes the regularized squared error between the original ratings matrix and the resultant after the product of the latent matrices. The latter part of the preceding equation is the regularization imposed to avoid over-fitting.</p><p><span class="strong"><strong>Alternating Least Squares</strong></span> is the optimization technique to minimize the aforementioned loss function. In general, we use stochastic gradient descent to optimize the loss function. For the Spark recommendation module, the ALS technique has been used for minimizing the loss function.</p><p>In the ALS method, we calculate the optimal latent factor vectors alternatively by fixing one of the two factor vectors, that is, we calculate the user latent vector by fixing the item-latent feature vector as constant and vice versa.</p><p>The main benefits of the ALS approach are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">This approach can be easily parallelized</li><li class="listitem">In most cases, we deal with sparse datasets in recommendation engine problems, and ALS is more efficient in handling sparsity compared to the stochastic gradient descent</li></ul></div><p>The Spark implementation of the recommendation engine module in spark.ml has the following parameters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>numBlocks</strong></span>: This is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10)</li><li class="listitem"><span class="strong"><strong>rank</strong></span>: This refers to the number of latent factors in the model (defaults to 10)</li><li class="listitem"><span class="strong"><strong>maxIter</strong></span>: This is the maximum number of iterations to run (defaults to 10)</li><li class="listitem"><span class="strong"><strong>regParam</strong></span>: This parameter specifies the regularization parameter in ALS (defaults to 0.1)</li><li class="listitem"><span class="strong"><strong>implicitPrefs</strong></span>: This parameter specifies whether to use the explicit feedback ALS variant or the one adapted for implicit feedback data (defaults to false, which means it's using explicit feedback)</li><li class="listitem"><span class="strong"><strong>alpha</strong></span>: This is a parameter applicable to the implicit feedback variant of ALS, which governs the baseline confidence in preference observations (defaults to 1.0)</li><li class="listitem"><span class="strong"><strong>nonnegative</strong></span>: This parameter specifies whether or not to use non-negative constraints for least squares (defaults to false)</li></ul></div></div>
<div class="section" title="Model based recommender system using pyspark" id="aid-1IHDQ1"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec46"/>Model based recommender system using pyspark</h1></div></div></div><p>Software details for the use case are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Spark 2.0</li><li class="listitem">Python API: pyspark</li><li class="listitem">Centos 6</li><li class="listitem">Python 3.4</li></ul></div><p>Start the Spark session using pyspark, as follows:</p><pre class="programlisting">pyspark</pre><p>The following screenshot shows the Spark session created by running the above <code class="literal">pyspark</code> command:</p><div class="mediaobject"><img src="../Images/image00410.jpeg" alt="Model based recommender system using pyspark"/></div><p style="clear:both; height: 1em;"> </p><p>To build the recommendation engine using Spark, we make use of Spark 2.0 capabilities, such as DataFrames, RDD, Pipelines, and Transforms available in Spark MLlib, which has was explained earlier.</p><p>Unlike earlier heurist approaches, such as k-nearest neighboring approaches used for building recommendation engines, in Spark, matrix factorization methods are used for building recommendation engines and the Alternating Least Squares (ALS) method is used for generating model-based collaborative filtering.</p></div>
<div class="section" title="MLlib recommendation engine module" id="aid-1JFUC1"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec47"/>MLlib recommendation engine module</h1></div></div></div><p>In this section, let's learn about the different methods present in the MLlib recommendation engine module. The current recommendation engine module helps us build the model-based collaborative filtering approach using the Alternating Least Squares matrix factorization model to generate recommendations.</p><p>The main methods available for building collaborative filtering are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">ALS()</code>: The <code class="literal">ALS()</code> constructor is invoked and its instance is created with all the required parameters, such as user column name, item column name, rating column name, rank, regularization parameter (regParam), maximum iterations (maxIter), and so on supplied.</li><li class="listitem"><code class="literal">fit()</code>: The <code class="literal">fit()</code> method is used to generate the model. This method takes the following parameters:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">dataset</code>: input dataset of type <code class="literal">pyspark.sql.DataFrame</code>(<a class="ulink" href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame</a>)</li><li class="listitem"><code class="literal">params</code>: this is an optional param map which contains required parameters listed above.</li><li class="listitem">Returns: The <code class="literal">fit()</code> method returns fitted models.</li></ul></div></li><li class="listitem"><code class="literal">Transform()</code>: The <code class="literal">transform()</code> method is used to generate the predictions. The <code class="literal">transform()</code> method takes in the following:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Test data (DataFrame datatype)</li><li class="listitem">Optional additional parameters that embed previously defined parameters.</li><li class="listitem">Returns a predictions (DataFrame object)</li></ul></div></li></ul></div></div>
<div class="section" title="The recommendation engine approach"><div class="titlepage" id="aid-1KEEU2"><div><div><h1 class="title"><a id="ch07lvl1sec48"/>The recommendation engine approach</h1></div></div></div><p>Now let's get into the actual implementation of the recommendation engine. We use the following approach to build the recommendation engine using Spark:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the Spark environment.</li><li class="listitem">Load the data.</li><li class="listitem">Explore the data source.</li><li class="listitem">Use the MLlib recommendation engine module to generate the recommendations using ALS instance.</li><li class="listitem">Generate the recommendations.</li><li class="listitem">Evaluate the model.</li><li class="listitem">Using the cross_validation approach, apply the parameter tuning model to tune the parameter and select the best model, and then generate recommendations.</li></ol><div style="height:10px; width: 1px"/></div><div class="section" title="Implementation"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec88"/>Implementation</h2></div></div></div><p>Like any other recommendation engine, the first step is to load the data into the analytics environment (into the Spark environment in our case). When we start the Spark environment in the 2.0 version, SparkContext and SparkSession will be created at the load time.</p><p>Before we get into the implementation part, let's review the data for a while. In this chapter, we use the MovieLens 100K Dataset to build collaborative filtering recommendation engines, both user-based and item-based. The dataset contains 943 user ratings on 1,682 movies. The ratings are on a scale of 1-5.</p><p>As a first step, we shall make use of SparkContext (sc) to load the data into the Spark environment.</p><div class="section" title="Data loading"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec25"/>Data loading</h3></div></div></div><p>To load the data, run the below command:</p><pre class="programlisting">data = sc.textFile("~/ml-100k/udata.csv")</pre><p>Loaded data will be a spark RDD type-run the below command to find out the data type of the data object:</p><pre class="programlisting">type(data)&#13;
&lt;class 'pyspark.rdd.RDD'&gt;&#13;
</pre><p>Total length of the data loaded is given by:</p><pre class="programlisting">data.count()&#13;
100001</pre><p>To load the first record in the loaded data:</p><pre class="programlisting">data.first()&#13;
'UserID\tItemId \tRating\tTimestamp'&#13;
</pre><p>We can see that the header information is located as the first row in the data object separated by <code class="literal">\t</code>; the column names of the data object are <code class="literal">UserID</code>, <code class="literal">ItemId</code>, <code class="literal">Rating</code>, and <code class="literal">Timestamp</code>.</p><p>For our purposes, we don't require Timestamp information, so we can remove this field from the data RDD:</p><p>To check the first 5 rows of the data RDD, we use take() action method:</p><pre class="programlisting">data.take(5)&#13;
['UserID\tItemId \tRating\tTimestamp', '196\t242\t3\t881250949', '186\t302\t3\t891717742', '22\t377\t1\t878887116', '244\t51\t2\t880606923']&#13;
&#13;
</pre><p>The MLlib recommendation engine module expects the data to be without any header information. So let's remove the header information, that is, remove the first line from the data RDD object, as follows:</p><p>Extract the first row from the data RDD object:</p><pre class="programlisting">header = data.first()</pre><p>Use <code class="literal">filter()</code> method and lambda expression to remove the first header row from the data. The lambda expression below is applied for each row and each row is compared with the header to check if the extracted row is the header or not. If the extracted row is found to be the header then that row is filtered:</p><pre class="programlisting">data = data.filter(lambda l:l!=header)</pre><p>Now let us check the count of the data RDD object; it has reduced from 100001 to 100000:</p><pre class="programlisting">data.count()&#13;
100000</pre><p>Now let us check the first row, we can observe that the header has been successfully removed:</p><pre class="programlisting">data.first()&#13;
'196\t242\t3\t881250949'</pre><p>Now that we have loaded the data into the Spark environment, let's format the data into a proper shape, as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Load the required functions for building the recommendation engine, such as ALS, the Matrix Factorization Model, and the Rating function from the MLlib recommendation module.</li><li class="listitem">Extract each row from the data RDD and split by <code class="literal">\t</code> to separate each column using the <code class="literal">map()</code> and lambda expressions.</li><li class="listitem">In the resultant set, let's create a Rating row object for each of the lines extracted in the previous step</li><li class="listitem">When the following expression is applied on the entire dataset, a pipelined RDD object is created:<div class="mediaobject"><img src="../Images/image00411.jpeg" alt="Data loading"/></div><p style="clear:both; height: 1em;"> </p></li></ol><div style="height:10px; width: 1px"/></div><p>Check the data type of ratings object using type:</p><pre class="programlisting">type(ratings)&#13;
&lt;class 'pyspark.rdd.PipelinedRDD'&gt;</pre><p>Check the first 5 records of the ratings PipelinedRDD object by running the following code:</p><pre class="programlisting">ratings.take(5)&#13;
    &#13;
[Rating(user=196, product=242, rating=3.0), Rating(user=186, product=302, rating=3.0), Rating(user=22, product=377, rating=1.0), Rating(user=244, product=51, rating=2.0), Rating(user=166, product=346, rating=1.0)]</pre><p>We can observe from the preceding result that each row in the original raw data RDD object turns into a kind of list of Rating row objects stacked into PipelinedRDD.</p></div><div class="section" title="Data exploration"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec26"/>Data exploration</h3></div></div></div><p>Now that we have loaded the data, let's spend some time exploring the data. Let's use the Spark 2.0 DataFrame API capabilities to explore the data:</p><p>Compute the total number of unique users by first selecting the <code class="literal">'user'</code> column and then using <code class="literal">distinct()</code> function to remove the duplicate <code class="literal">userId</code>:</p><pre class="programlisting">df.select('user').distinct().show(5)</pre><p>The following screenshot shows the results of the previous query:</p><div class="mediaobject"><img src="../Images/image00412.jpeg" alt="Data exploration"/></div><p style="clear:both; height: 1em;"> </p><p>Total number of unique users:</p><pre class="programlisting">df.select('user').distinct().count()&#13;
943</pre><p>Total number of unique items:</p><pre class="programlisting">df.select('product').distinct().count()&#13;
1682</pre><p>Display first 5 unique products:</p><pre class="programlisting">df.select('product').distinct().show(5)</pre><p>The following screenshot shows the results of the previous query:</p><div class="mediaobject"><img src="../Images/image00413.jpeg" alt="Data exploration"/></div><p style="clear:both; height: 1em;"> </p><p>Number of rated products by each user:</p><pre class="programlisting">df.groupBy("user").count().take(5)&#13;
    &#13;
[Row(user=26, count=107), Row(user=29, count=34), Row(user=474, count=327), Row(user=191, count=27), Row(user=65, count=80)]</pre><p>The previous results explain that User 26 has rated 107 movies and user 29 has rated 34 movies.</p><p>Number of records for each rating type:</p><pre class="programlisting">df.groupBy("rating").count().show()</pre><p>The following screenshot shows the results of the previous query:</p><div class="mediaobject"><img src="../Images/image00414.jpeg" alt="Data exploration"/></div><p style="clear:both; height: 1em;"> </p><p>In the following code, we make use of the <code class="literal">numpy</code> scientific computing package in Python, used for working with arrays: <code class="literal">matplotlibe</code> - a visualizing package in Python:</p><pre class="programlisting">import numpy as np &#13;
import matplotlib.pyplot as plt&#13;
n_groups = 5 &#13;
x = df.groupBy("rating").count().select('count') &#13;
xx = x.rdd.flatMap(lambda x: x).collect() &#13;
fig, ax = plt.subplots() &#13;
index = np.arange(n_groups) &#13;
bar_width = 1 &#13;
opacity = 0.4 &#13;
rects1 = plt.bar(index, xx, bar_width, &#13;
                 alpha=opacity, &#13;
                 color='b', &#13;
                 label='ratings') &#13;
plt.xlabel('ratings') &#13;
plt.ylabel('Counts') &#13;
plt.title('Distribution of ratings') &#13;
plt.xticks(index + bar_width, ('1.0', '2.0', '3.0', '4.0', '5.0')) &#13;
plt.legend() &#13;
plt.tight_layout() &#13;
plt.show() &#13;
</pre><div class="mediaobject"><img src="../Images/image00415.jpeg" alt="Data exploration"/></div><p style="clear:both; height: 1em;"> </p><p>Statistics of ratings per user:</p><pre class="programlisting">df.groupBy("UserID").count().select('count').describe().show()</pre><div class="mediaobject"><img src="../Images/image00416.jpeg" alt="Data exploration"/></div><p style="clear:both; height: 1em;"> </p><p>Individual counts of ratings per user:</p><pre class="programlisting"> df.stat.crosstab("UserID", "Rating").show() &#13;
</pre><div class="mediaobject"><img src="../Images/image00417.jpeg" alt="Data exploration"/></div><p style="clear:both; height: 1em;"> </p><p>Average rating given by each user:</p><pre class="programlisting">df.groupBy('UserID').agg({'Rating': 'mean'}).take(5)&#13;
 &#13;
[Row(UserID=148, avg(Rating)=4.0), Row(UserID=463, avg(Rating)=2.8646616541353382), Row(UserID=471, avg(Rating)=3.3870967741935485), Row(UserID=496, avg(Rating)=3.0310077519379846), Row(UserID=833, avg(Rating)=3.056179775280899)] &#13;
</pre><p>Average rating per movie:</p><pre class="programlisting">df.groupBy('ItemId ').agg({'Rating': 'mean'}).take(5)&#13;
 &#13;
[Row(ItemId =496, avg(Rating)=4.121212121212121), Row(ItemId =471, avg(Rating)=3.6108597285067874), Row(ItemId =463, avg(Rating)=3.859154929577465), Row(ItemId =148, avg(Rating)=3.203125), Row(ItemId =1342, avg(Rating)=2.5)] &#13;
</pre></div><div class="section" title="Building the basic recommendation engine"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec27"/>Building the basic recommendation engine</h3></div></div></div><p>Divide the original data into training and test datasets randomly as follows, using the <code class="literal">randomSplit()</code> method:</p><p>(training, test) = ratings.randomSplit([0.8, 0.2])</p><p>Counting the number of instances in the training dataset:</p><pre class="programlisting">training.count()&#13;
80154    &#13;
</pre><p>Counting the number of instances in the test set:</p><pre class="programlisting">test.count()&#13;
19846</pre><p>Let's now build a recommendation engine model using the ALS algorithm available in the MLlib library of Spark.</p><p>For this, we use the following methods and parameters:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Load the ALS module into the Spark environment.</li><li class="listitem">Call the <code class="literal">ALS.train()</code> method to train the model.</li><li class="listitem">Pass the required parameters, such as rank, number of iterations (maxIter), and training data to the <code class="literal">ALS.train()</code> method.</li></ol><div style="height:10px; width: 1px"/></div><p>Let's understand the parameters now:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Rank</strong></span>: This parameter is the number of latent factors of users and items to be used in the model. The default is 10.</li><li class="listitem"><span class="strong"><strong>maxIter</strong></span>: This is the number of iterations the model has to run. The default is 10.</li></ul></div><p>Build the recommendation model using Alternating Least Squares:</p><p>Setting rank and maxIter parameters:</p><pre class="programlisting">rank = 10&#13;
numIterations = 10&#13;
</pre><p>Calling <code class="literal">train()</code> method with training data, rank, maxIter params, <code class="literal">model = ALS.train</code>(training, rank, numIterations):</p><pre class="programlisting">16/10/04 11:01:34 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS&#13;
16/10/04 11:01:34 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS&#13;
16/10/04 11:01:34 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK&#13;
16/10/04 11:01:34 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK&#13;
16/10/04 11:01:37 WARN Executor: 1 block locks were not released by TID = 122:&#13;
[rdd_221_0]&#13;
16/10/04 11:01:37 WARN Executor: 1 block locks were not released by TID = 123:&#13;
[rdd_222_0]&#13;
16/10/04 11:01:37 WARN Executor: 1 block locks were not released by TID = 124:&#13;
[rdd_221_0]&#13;
16/10/04 11:01:37 WARN Executor: 1 block locks were not released by TID = 125:&#13;
[rdd_222_0]&#13;
</pre><p>Checking the model as below, we observe that <code class="literal">Matrixfactorizationmodel</code> object is created:</p><pre class="programlisting">model&#13;
</pre></div><div class="section" title="Making predictions"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec28"/>Making predictions</h3></div></div></div><p>Now that we have created the model, let's predict the values of ratings on the test set we created earlier.</p><p>The ALS module has provided many methods discussed in the following sections for making predictions, recommending users, and recommending items to users, user features, item features, and so on. Let's run the methods one by one.</p><p>Before we proceed to predictions, we shall first create test data in a way that is acceptable to the prediction methods, as follows:</p><p>The following code extracts each row in the test data and extracts <code class="literal">userID, ItemID</code> and puts it in testdata <code class="literal">PipelinedRDD</code> object:</p><pre class="programlisting">testdata = test.map(lambda p: (p[0], p[1]))&#13;
&#13;
type(testdata)&#13;
&lt;class 'pyspark.rdd.PipelinedRDD'&gt;&#13;
</pre><p>The following code shows the original test data sample:</p><pre class="programlisting">test.take(5)&#13;
&#13;
[Rating(user=119, product=392, rating=4.0), Rating(user=38, product=95, rating=5.0), Rating(user=63, product=277, rating=4.0), Rating(user=160, product=234, rating=5.0), Rating(user=225, product=193, rating=4.0)]&#13;
</pre><p>The following code displays the formatted data required for making predictions:</p><pre class="programlisting">testdata.take(5)&#13;
&#13;
[(119, 392), (38, 95), (63, 277), (160, 234), (225, 193)]&#13;
</pre><p>The prediction methods are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">predict()</code>: The predict method will the predict rating for a given user and item and is given as follows:</li></ul></div><p>This method is used when we want to make predictions for a combination of user and item:</p><pre class="programlisting">pred_ind = model.predict(119, 392) &#13;
 &#13;
</pre><p>We can observe below that the prediction value for a user <code class="literal">119</code> and movie <code class="literal">392</code> is <code class="literal">4.3926091845289275</code>: just see above the original value for the same combination in test data:</p><pre class="programlisting">pred_ind &#13;
 &#13;
4.3926091845289275 &#13;
</pre><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">predictall()</code>: This method is used when we want to predict values for all the test data in one go, given as follows:</li></ul></div><pre class="programlisting">predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2])) &#13;
</pre><p>Use the following code to check the data type:</p><pre class="programlisting">type(predictions) &#13;
&lt;class 'pyspark.rdd.PipelinedRDD'&gt; &#13;
 &#13;
</pre><p>Use the following code displays the first five predictions:</p><pre class="programlisting"> predictions.take(5) &#13;
 &#13;
[((268, 68), 3.197299431949281), ((200, 68), 3.6296857016488357), ((916, 68), 3.070451877410571), ((648, 68), 2.165520614428771), ((640, 68), 3.821666263132798)] &#13;
</pre></div></div><div class="section" title="User-based collaborative filtering"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec89"/>User-based collaborative filtering</h2></div></div></div><p>Now let's recommend items (movies) to users. The ALS recommendation module contains the <code class="literal">recommendProductsForUsers()</code> method to generate the top-N item recommendations for users.</p><p>The <code class="literal">recommendProductsForUsers()</code> method takes integers as the input parameter, which indicates the top-N recommendations; for example, to generate the top 10 recommendations to the users, we pass 10 as value to the <code class="literal">recommendProductsForUsers()</code> method, as follows:</p><pre class="programlisting">recommedItemsToUsers = model.recommendProductsForUsers(10)</pre><p>Use the following code shows that recommendations are generated for all the 943 users:</p><pre class="programlisting">recommedItemsToUsers.count()  &#13;
943</pre><p>Let us see the recommendations for the first two users: 96 and 784:</p><pre class="programlisting">recommedItemsToUsers.take(2)&#13;
&#13;
[&#13;
(96, (Rating(user=96, product=1159, rating=11.251653489172302), Rating(user=96, product=962, rating=11.1500279633824), Rating(user=96, product=534, rating=10.527262244626867), Rating(user=96, product=916, rating=10.066351313580977), Rating(user=96, product=390, rating=9.976996795233937), Rating(user=96, product=901, rating=9.564128162876036), Rating(user=96, product=1311, rating=9.13860044421153), Rating(user=96, product=1059, rating=9.081563794413025), Rating(user=96, product=1178, rating=9.028685203289745), Rating(user=96, product=968, rating=8.844312806737918)&#13;
)),&#13;
 (784, (Rating(user=784, product=904, rating=5.975314993539809), Rating(user=784, product=1195, rating=5.888552423210881), Rating(user=784, product=1169, rating=5.649927493462845), Rating(user=784, product=1446, rating=5.476279163198376), Rating(user=784, product=1019, rating=5.303140289874016), Rating(user=784, product=1242, rating=5.267858336331315), Rating(user=784, product=1086, rating=5.264190584020031), Rating(user=784, product=1311, rating=5.248377920702441), Rating(user=784, product=816, rating=5.173286729120303), Rating(user=784, product=1024, rating=5.1253425029498985)&#13;
))&#13;
]&#13;
</pre></div><div class="section" title="Model evaluation"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec90"/>Model evaluation</h2></div></div></div><p>Now let's evaluate the model accuracy. For this, we choose the Root Mean Squared Error method to calculate the model accuracy. We can do it either manually, as shown next, or call a defined function available in the Spark MLlib:
Create a <code class="literal">ratesAndPreds</code> object by joining the original ratings and predictions:</p><pre class="programlisting">ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)</pre><p>The following code will calculate the mean squared error:</p><pre class="programlisting">MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()&#13;
&#13;
[Stage 860:&gt;                                               (0 + 4) / 6]&#13;
&#13;
Mean Squared Error = 1.1925845065690288  &#13;
&#13;
from math import sqrt&#13;
&#13;
rmse = sqrt(MSE)&#13;
rmse&#13;
1.092055175606539&#13;
</pre></div><div class="section" title="Model selection and hyperparameter tuning"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec91"/>Model selection and hyperparameter tuning</h2></div></div></div><p>The most important step in any machine learning task is to use model evaluation or model selection to find the optimal parameters that the fit the data. Spark provides infrastructure to tune and model evaluation, for individual algorithms or for the entire model building pipeline. Users may tune the entire pipeline model or tune individual components of the pipeline. MLlib provides model selection tools such as <code class="literal">CrossValidator</code> class and <code class="literal">TrainValidationSplit</code> class.</p><p>The above mentioned classes require the following items:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Estimator</strong></span> algorithm or Pipeline to tune</li><li class="listitem"><span class="strong"><strong>Set of ParamMaps</strong></span>: parameters to choose from, sometimes called a <span class="emphasis"><em>parameter grid</em></span> to search over</li><li class="listitem"><span class="strong"><strong>Evaluator</strong></span>: metric to measure how well a fitted model does on held-out test data</li></ul></div><p>At a high level, these model selection tools work as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">They split the input data into separate training and test datasets</li><li class="listitem">For each (training and test) pair, they iterate through the set of <code class="literal">ParamMaps</code></li><li class="listitem">For each <code class="literal">ParamMap</code>, they fit the Estimator using those parameters, get the fitted Model, and evaluate the Model's performance using the Evaluator</li><li class="listitem">They select the Model produced by the best-performing set of parameters</li></ul></div><p>The MLlib supports various evaluation classes for performing evaluation tasks, such as the <code class="literal">RegressionEvaluator</code> class for regression based problems, the <code class="literal">BinaryClassificationEvaluator</code> class for binary classification problems, and the <code class="literal">MulticlassClassificationEvaluator</code> class for multiclass classification problems. For constructing a parameter grid, we can use the <code class="literal">paramGridBuilder</code> class.</p><div class="section" title="Cross-Validation"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec29"/>Cross-Validation</h3></div></div></div><p>The <span class="strong"><strong>Cross-Validation</strong></span> approach is one of the most popular approaches in evaluating the datamining models and in choosing optimal parameters for building the best estimation model. MLlib offers two types of evaluation classes: the <code class="literal">CrossValidator</code> and <code class="literal">TrainValidationSplit</code> classes.</p></div><div class="section" title="CrossValidator"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec30"/>CrossValidator</h3></div></div></div><p>The <code class="literal">CrossValidator</code> class takes the input dataset and splits it into multiple dataset folds, which can be used as training and test sets. Using these datasets, the <code class="literal">CrossValidator</code> class builds multiple models and finds optimal parameters and stores in <code class="literal">ParamMap</code>. After identifying the best <code class="literal">ParamMap</code>, the <code class="literal">CrossValidator</code> class finally computes the best model using the entire dataset. For example, let's say we choose the five-fold cross-validation; the <code class="literal">CrossValidator</code> class splits the original dataset into five sub-datasets with each sub-dataset containing training and test sets. The <code class="literal">CrossValidator</code> class chooses each fold set at a time and estimates the model parameters. Finally, <code class="literal">CrossValidator</code> computes the average of the evaluation metric to store the best parameters in <code class="literal">ParamMap</code>.</p></div><div class="section" title="Train-Validation Split"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec31"/>Train-Validation Split</h3></div></div></div><p>Spark MLlib provides another class for estimating the optimal parameters using <code class="literal">TrainValidationSplit</code>. Unlike <code class="literal">CrossValidator</code>, this class estimates the optimal parameters on a single dataset. For example, the <code class="literal">TrainValidatorSplit</code> class divides the input data into trainset and test sets of size 3/4 and 1/4, and optimal parameters are chosen using these sets.</p><p>Now, let's understand the recommendation engine model we built earlier.</p><p>The tuning model is present in the MLlib of Spark 2.0 and makes use of DataFrame API features. So in order to accommodate this, our first step is to convert the original dataset ratings to DataFrame.</p><p>For conversion, we use the <code class="literal">sqlContext</code> object and the <code class="literal">createDataFrame()</code> method to convert the ratings RDD object to a <code class="literal">DataFrame</code> object, as follows:</p><pre class="programlisting">type(ratings) &#13;
&lt;class 'pyspark.rdd.PipelinedRDD'&gt; &#13;
</pre><p>SQL Context object is created when starting the spark session using pyspark:</p><pre class="programlisting">sqlContext &#13;
&lt;pyspark.sql.context.SQLContext object at 0x7f24c94f7d68&gt; &#13;
</pre><p>Creating a DataFrame object from the ratings rdd object as follows:</p><pre class="programlisting">df = sqlContext.createDataFrame(ratings) &#13;
 &#13;
type(df) &#13;
 &#13;
&lt;class 'pyspark.sql.dataframe.DataFrame'&gt; &#13;
</pre><p>Display first 20 records of <code class="literal">dataframe</code> object:</p><pre class="programlisting">df.show() &#13;
</pre><p>The following screenshot shows the results of the previous query:</p><div class="mediaobject"><img src="../Images/image00418.jpeg" alt="Train-Validation Split"/></div><p style="clear:both; height: 1em;"> </p><p>Creating random samples of training set and test set using <code class="literal">randomSplit()</code> method:</p><pre class="programlisting">(training, test) = df.randomSplit([0.8, 0.2]) &#13;
</pre><p>Load modules required for running parameter <code class="literal">tuningmodel</code>:</p><pre class="programlisting">from pyspark.ml.recommendation import ALS &#13;
</pre><p>Call the ALS method available in MLlib to building the recommendation engine. The following method <code class="literal">ALS()</code> takes only column values of training data, such as UserID, ItemId, and Rating. The other parameters, such as rank, number of iterations, learning parameters, and so on will be passed as the <code class="literal">ParamGridBuilder</code> object to the cross-validation method.</p><p>As mentioned earlier, the model tuning pipeline require Estimators, a set of ParamMaps, and Evaluators. Let's create each one of them, as follows:</p><p>Estimator objects as stated earlier, estimators take algorithm or pipeline objects as input. Let us build one pipeline object as follows:</p><p>Calling ALS algorithm:</p><pre class="programlisting">als = ALS(userCol="user", itemCol="product", ratingCol="rating") &#13;
als &#13;
 &#13;
ALS_45108d6e011beae88f4c &#13;
</pre><p>Checking the type of <code class="literal">als</code> object:</p><pre class="programlisting">type(als) &#13;
&lt;class 'pyspark.ml.recommendation.ALS'&gt; &#13;
</pre><p>Let us see how the default parameters are set for the ALS model:</p><pre class="programlisting">als.explainParams()&#13;
"alpha: alpha for implicit preference (default: 1.0)\ncheckpointInterval: set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. (default: 10)\nfinalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\nimplicitPrefs: whether to use implicit preference (default: False)\nintermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\nitemCol: column name for item ids. Ids must be within the integer value range. (default: item, current: ItemId )\nmaxIter: max number of iterations (&gt;= 0). (default: 10)\nnonnegative: whether to use nonnegative constraint for least squares (default: False)\nnumItemBlocks: number of item blocks (default: 10)\nnumUserBlocks: number of user blocks (default: 10)\npredictionCol: prediction column name. (default: prediction)\nrank: rank of the factorization (default: 10)\nratingCol: column name for ratings (default: rating, current: Rating)\nregParam: regularization parameter (&gt;= 0). (default: 0.1)\nseed: random seed. (default: -1517157561977538513)\nuserCol: column name for user ids. Ids must be within the integer value range. (default: user, current: UserID)"&#13;
</pre><p>From the preceding result, we observe that the model is set to its default values for rank as 10, maxIter as 10, and blocksize as 10:</p><p>Create pipeline object and setting the created als model as a stage in the pipeline:</p><pre class="programlisting">from pyspark.ml import Pipeline&#13;
&#13;
pipeline = Pipeline(stages=[als])&#13;
 type(pipeline)&#13;
&lt;class 'pyspark.ml.pipeline.Pipeline'&gt;&#13;
</pre></div><div class="section" title="Setting the ParamMaps/parameters"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec32"/>Setting the ParamMaps/parameters</h3></div></div></div><p>Let's observe the <code class="literal">ALS()</code> method closely and logically and infer the parameters that can be used for parameter tuning:</p><p><span class="strong"><strong>rank:</strong></span> We know that rank is the number of latent features for users and items, which, by default, is 10, but if we do not have the optimal number of latent features for a given dataset, this parameter can be taken up for tuning the model by giving a range of values between 8 and 12 - the choice is left to the users. Due to the computational cost, we restrict the values to 8 -12, but readers are free to try other values.</p><p><span class="strong"><strong>MaxIter:</strong></span> MaxIter is the number of times the model is made to run; it's set to a default 10. We can select this parameter also for tuning as we do not know the optimal iterations at which the model performs well; we select between 10 and 15.</p><p><span class="strong"><strong>reqParams:</strong></span> regParams is the learning parameter set between 1 and 10.</p><p>Loading <code class="literal">CrossValidation</code> and <code class="literal">ParamGridBuilder</code> modules to create range of parameters:</p><pre class="programlisting">from pyspark.ml.tuning import CrossValidator, ParamGridBuilder&#13;
&#13;
paramMapExplicit = ParamGridBuilder() \&#13;
                    .addGrid(als.rank, [8, 12]) \&#13;
                    .addGrid(als.maxIter, [10, 15]) \&#13;
                    .addGrid(als.regParam, [1.0, 10.0]) \&#13;
                    .build()&#13;
</pre></div><div class="section" title="Setting the evaluator object"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec33"/>Setting the evaluator object</h3></div></div></div><p>As stated earlier, the evaluator object sets the evaluation metric to evaluate the model during multiple runs in the cross validation method:</p><p>Loading the <code class="literal">RegressionEvaluator</code> model:</p><pre class="programlisting">from pyspark.ml.evaluation import RegressionEvaluator&#13;
&#13;
</pre><p>Calling <code class="literal">RegressionEvaluator()</code> method with evaluation metric set to <code class="literal">rmse</code> and <code class="literal">evaluation</code> column set to Rating:</p><pre class="programlisting">evaluatorR = RegressionEvaluator(metricName="rmse", labelCol="rating")&#13;
</pre><p>Now that we have prepared all the required objects for running the cross-validation method, that is, <code class="literal">Estimator</code>, <code class="literal">paramMaps</code>, and <code class="literal">Evaluator</code>, let's run the model.</p><p>The cross-validation method gives us the best optimal model out of all the executed models:</p><pre class="programlisting">cvExplicit = CrossValidator(estimator=als, estimatorParamMaps=paramMap, evaluator=evaluatorR,numFolds=5)&#13;
&#13;
</pre><p>Running the model using <code class="literal">fit()</code> method:</p><pre class="programlisting">cvModel = cvExplicit.fit(training)&#13;
&#13;
[Stage 897:============================&gt;                           (5 + 4) / 10]&#13;
[Stage 938:==================================================&gt;     (9 + 1) / 10]&#13;
[Stage 1004:&gt;(0 + 4) / 10][Stage 1005:&gt; (0 + 0) / 2][Stage 1007:&gt;(0 + 0) / 10]  &#13;
[Stage 1008:&gt;                                                     (3 + 4) / 200]&#13;
&#13;
&#13;
preds = cvModel.bestModel.transform(test)&#13;
evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating",predictionCol="prediction")&#13;
rmse = evaluator.evaluate(pred)&#13;
print("Root-mean-square error = " + str(rmse))&#13;
 rmse                                                                        &#13;
&#13;
0.924617823674082&#13;
</pre></div></div></div>
<div class="section" title="Summary" id="aid-1LCVG1"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec49"/>Summary</h1></div></div></div><p>In this chapter, we learned about model-based collaborative filtering using matrix factorization methods using ALS. We used the Python API to access the Spark framework and ran the ALS collaborative filtering. In the beginning of the chapter, we refreshed our knowledge of Spark with all the basics that are required to run the recommendation engines, such as what Spark is, the Spark ecosystem, components of Spark, SparkSession, DataFrames, RDD, and so on. As explained then, we explored the MovieLens data, built a basic recommendation engine, evaluated the model, and used parameter tuning to improve the model. In the next chapter, we shall learn about building recommendations using Graph database - Neo4j.</p></div></body></html>