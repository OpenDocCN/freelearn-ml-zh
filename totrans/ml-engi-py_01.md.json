["```py\ngit clone https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition.git \n```", "```py\nconda env create –f mlewp-chapter01.yml \n```", "```py\nmake\npython3 -m pip install poetry\npython3 -m poetry build\npython3 -m pip install dist/*.whl \n```", "```py\ndyld[29330]: Library not loaded: '@rpath/libtbb.dylib' \n```", "```py\ncd /opt/homebrew/Caskroom/miniforge/base/envs/mlewp-chapter01/lib/python3.10/site-packages/prophet/stan_model/\ninstall_name_tool -add_rpath @executable_path/cmdstan-2.26.1/stan/lib/stan_math/lib/tbb prophet_model.bin \n```", "```py\npython3 clustering_example.py \n```", "```py\n    import numpy as np\n    from numpy.random import MT19937\n    from numpy.random import RandomState, SeedSequence\n    rs = RandomState(MT19937(SeedSequence(123456789)))\n\n    # Define simulate ride data function\n    def simulate_ride_distances():\n        ride_dists = np.concatenate(\n            (\n                10 * np.random.random(size=370),\n                30 * np.random.random(size=10), # long distances\n                10 * np.random.random(size=10), # same distance\n                10 * np.random.random(size=10) # same distance\n            )\n        )\n        return ride_dists \n    ```", "```py\n    def simulate_ride_speeds():\n        ride_speeds = np.concatenate(\n            (\n                np.random.normal(loc=30, scale=5, size=370),\n                np.random.normal(loc=30, scale=5, size=10),\n                np.random.normal(loc=50, scale=10, size=10),\n                np.random.normal(loc=15, scale=4, size=10) \n            )\n        )\n        return ride_speeds \n    ```", "```py\n    def simulate_ride_data():\n        # Simulate some ride data …\n        ride_dists = simulate_ride_distances()\n        ride_speeds = simulate_ride_speeds()\n        ride_times = ride_dists/ride_speeds\n        # Assemble into Data Frame\n        df = pd.DataFrame(\n            {\n              'ride_dist': ride_dists,\n              'ride_time': ride_times,\n              'ride_speed': ride_speeds\n            }\n        )\n        ride_ids = datetime.datetime.now().strftime(\"%Y%m%d\") +\\\n                   df.index.astype(str)\n        df['ride_id'] = ride_ids\n        return df \n    ```", "```py\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.cluster import DBSCAN\n    from sklearn import metrics \n\n    def cluster_and_label(data, create_and_show_plot=True):\n        data = StandardScaler().fit_transform(data)\n        db = DBSCAN(eps=0.3, min_samples=10).fit(data)\n        # Find labels from the clustering\n        core_samples_mask = np.zeros_like(db.labels_,dtype=bool)\n        core_samples_mask[db.core_sample_indices_] = True\n        labels = db.labels_\n        # Number of clusters in labels, ignoring noise if present.\n        n_clusters_ = len(set(labels)) -\n                             (1 if -1 in labels else 0)\n        n_noise_ = list(labels).count(-1)\n        run_metadata = {\n            'nClusters': n_clusters_,\n            'nNoise': n_noise_,\n            'silhouetteCoefficient':\n             metrics.silhouette_score(data, labels),\n            'labels': labels,\n        }\n        if create_and_show_plot:\n            plot_cluster_results(data, labels, core_samples_mask,\n                                 n_clusters_)\n        else:\n            pass\n        return run_metadata \n    ```", "```py\n    import matplotlib.pyplot as plt\n\n    def plot_cluster_results(data, labels, core_samples_mask,\n                             n_clusters_):\n        fig = plt.figure(figsize=(10, 10))\n        # Black removed and is used for noise instead.\n        unique_labels = set(labels)\n        colors = [plt.cm.cool(each) for each in np.linspace(0, 1,\n                  len(unique_labels))]\n        for k, col in zip(unique_labels, colors):\n            if k == -1:\n                # Black used for noise.\n                col = [0, 0, 0, 1]\n            class_member_mask = (labels == k)\n            xy = data[class_member_mask & core_samples_mask]\n            plt.plot(xy[:, 0], xy[:, 1], 'o',\n                     markerfacecolor=tuple(col),\n                     markeredgecolor='k', markersize=14)\n            xy = data[class_member_mask & ~core_samples_mask]\n            plt.plot(xy[:, 0], xy[:, 1], '^',\n                     markerfacecolor=tuple(col),\n                     markeredgecolor='k', markersize=14)\n        plt.xlabel('Standard Scaled Ride Dist.')\n        plt.ylabel('Standard Scaled Ride Time')\n        plt.title('Estimated number of clusters: %d' % n_clusters_)\n        plt.savefig('taxi-rides.png') \n    ```", "```py\n    import logging\n    logging.basicConfig()\n    logging.getLogger().setLevel(logging.INFO)\n\n    if __name__ == \"__main__\":\n        import os\n        # If data present, read it in\n        file_path = 'taxi-rides.csv'\n        if os.path.exists(file_path):\n            df = pd.read_csv(file_path)\n        else:\n            logging.info('Simulating ride data')\n            df = simulate_ride_data()\n            df.to_csv(file_path, index=False)\n        X = df[['ride_dist', 'ride_time']]\n\n        logging.info('Clustering and labelling')\n        results = cluster_and_label(X, create_and_show_plot=True)\n        df['label'] = results['labels']\n\n        logging.info('Outputting to json ...')\n        df.to_json('taxi-labels.json', orient='records') \n    ```", "```py\npython3 forecasting_example.py \n```", "```py\n    import pandas as pd\n\n    if __name__ == \"__main__\":\n        import os\n        file_path = train.csv\n        if os.path.exists(file_path):\n            df = pd.read_csv(file_path)\n        else:\n            download_kaggle_dataset()\n            df = pd.read_csv(file_path) \n    ```", "```py\n    import kaggle\n\n    def download_kaggle_dataset( kaggle_dataset: str =\"pratyushakar/\n                                 rossmann-store-sales\" ) -> None:\n        api = kaggle.api\n        kaggle.api.dataset_download_files(kaggle_dataset, path=\"./\",\n                                          unzip=True, quiet=False) \n    ```", "```py\n    def prep_store_data(df: pd.DataFrame, \n                        store_id: int = 4, \n                        store_open: int = 1) -> pd.DataFrame:\n        df['Date'] = pd.to_datetime(df['Date'])\n        df.rename(columns= {'Date':'ds','Sales':'y'}, inplace=True)\n        df_store = df[\n            (df['Store'] == store_id) & \n            (df['Open'] == store_open)\n            ].reset_index(drop=True)\n        return df_store.sort_values('ds', ascending=True) \n    ```", "```py\n    seasonality = {\n        'yearly': True,\n        'weekly': True,\n        'daily': False\n    }\n    predicted, df_train, df_test, train_index = train_predict(\n        df = df,\n        train_fraction = 0.8,\n        seasonality=seasonality\n    ) \n    ```", "```py\n    def train_predict(df: pd.DataFrame, train_fraction: float, \n                      seasonality: dict) -> tuple[\n                          pd.DataFrame,pd.DataFrame,pd.DataFrame, int]:\n        train_index = int(train_fraction*df.shape[0])\n        df_train = df.copy().iloc[0:train_index]\n        df_test = df.copy().iloc[train_index:]\n        model=Prophet(\n            yearly_seasonality=seasonality['yearly'],\n            weekly_seasonality=seasonality['weekly'],\n            daily_seasonality=seasonality['daily'],\n            interval_width = 0.95\n        )\n        model.fit(df_train)\n        predicted = model.predict(df_test)\n        return predicted, df_train, df_test, train_index \n    ```", "```py\n    plot_forecast(df_train, df_test, predicted) \n    ```", "```py\npython3 classify_example.py \n```", "```py\n    if __name__ == \"__main__\":\n        X_train, X_test, y_train, y_test = ingest_and_prep_data()\n        X_balanced, y_balanced = rebalance_classes(X_train, y_train)\n        rf_random = get_randomised_rf_cv(\n                      random_grid=get_hyperparam_grid()\n                      )\n        rf_random.fit(X_balanced, y_balanced) \n    ```", "```py\n    def ingest_and_prep_data(\n            bank_dataset: str = 'bank_data/bank.csv'\n            ) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame,\n                       pd.DataFrame]:\n        df = pd.read_csv('bank_data/bank.csv', delimiter=';',\n                          decimal=',')\n\n        feature_cols = ['job', 'marital', 'education', 'contact',\n                        'housing', 'loan', 'default', 'day']\n        X = df[feature_cols].copy()\n        y = df['y'].apply(lambda x: 1 if x == 'yes' else 0).copy()\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_\n                                              size=0.2, random_state=42)\n        enc = OneHotEncoder(handle_unknown='ignore')\n        X_train = enc.fit_transform(X_train)\n        return X_train, X_test, y_train, y_test \n    ```", "```py\n    def rebalance_classes(X: pd.DataFrame, y: pd.DataFrame\n                          ) -> tuple[pd.DataFrame, pd.DataFrame]:\n        sm = SMOTE()\n        X_balanced, y_balanced = sm.fit_resample(X, y)\n        return X_balanced, y_balanced \n    ```", "```py\n    def get_hyperparam_grid() -> dict:\n        n_estimators = [int(x) for x in np.linspace(start=200,\n                        stop=2000, num=10)]\n        max_features = ['auto', 'sqrt']\n        max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n        max_depth.append(None)\n        min_samples_split = [2, 5, 10]\n        min_samples_leaf = [1, 2, 4]\n        bootstrap = [True, False]  # Create the random grid\n        random_grid = {\n            'n_estimators': n_estimators,\n            'max_features': max_features,\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf': min_samples_leaf,\n            'bootstrap': bootstrap\n        }\n        return random_grid \n    ```", "```py\n    def get_randomised_rf_cv(random_grid: dict) -> sklearn.model_\n                                   selection._search.RandomizedSearchCV:\n        rf = RandomForestClassifier()\n        rf_random = RandomizedSearchCV(\n            estimator=rf,\n            param_distributions=random_grid,\n            n_iter=100,\n            cv=3,\n            verbose=2,\n            random_state=42,\n            n_jobs=-1,\n            scoring='f1'\n        )\n        return rf_random \n    ```"]