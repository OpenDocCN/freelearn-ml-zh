<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer125">
<h1 id="_idParaDest-92"><em class="italic"><a id="_idTextAnchor091"/>Chapter 7</em>: Linear Regression Models</h1>
<p>Linear regression is <a id="_idIndexMarker610"/>perhaps the most well-known machine learning algorithm, having origins in statistical learning at least 200 years ago. If you took a statistics, econometrics, or psychometrics course in college, you were likely introduced to linear regression, even if you took that course long before machine learning was taught in undergraduate courses. As it turns out, many social and physical phenomena can be successfully modeled as a function of a linear combination of predictor variables. This is as useful for machine learning as it has been for statistical learning all these years, though, with machine learning, we care much less about the parameter values than we do about predictions.</p>
<p>Linear regression is a very good choice for modeling a continuous target, assuming that our features and target have certain qualities. In this chapter, we will go over the assumptions of linear regression models and construct a model using data that is largely consistent with these assumptions. However, we will also explore alternative approaches, such as nonlinear regression, which we use when these assumptions do not hold. We will conclude this chapter by looking at techniques that address the possibility of overfitting, such as lasso regression.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Key concepts</li>
<li>Linear regression and gradient descent</li>
<li>Using classical linear regression </li>
<li>Using lasso regression</li>
<li>Using non-linear regression</li>
<li>Regression with gradient descent</li>
</ul>
<h1 id="_idParaDest-93"><a id="_idTextAnchor092"/>Technical requirements</h1>
<p>In this chapter, we will stick to the libraries that are available with most scientific distributions of Python – NumPy, pandas, and scikit-learn. The code for this chapter can be found in this book’s GitHub repository at <a href="https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning">https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning</a>.</p>
<h1 id="_idParaDest-94"><a id="_idTextAnchor093"/>Key concepts</h1>
<p>The typical analyst who has been doing predictive modeling for a while has constructed tens, perhaps hundreds, of linear regression models over the years. If you worked for a large accounting firm in the late 1980s, as I did, and you were doing forecasting, you may have spent your whole day, every day, specifying linear models. You would have run all conceivable permutations of independent variables and transformations of dependent variables, and diligently looked for evidence of heteroscedasticity (non-constant variance in residuals) or multicollinearity (highly correlated features). But most of all, you worked hard to identify key predictor variables and address any bias in your parameter estimates (your coefficients or weights).</p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor094"/>Key assumptions of linear regression models</h2>
<p>Much <a id="_idIndexMarker611"/>of that effort still applies today, though there is now much more emphasis on the accuracy of predictions than on parameter estimates. We worry about overfitting now, in a way that we did not 30 years ago. We are also more likely to seek alternatives when the assumptions of linear regression models are violated. These assumptions are as follows:</p>
<ul>
<li>There there is a linear relationship between features (independent variables) and the target (dependent variable)</li>
<li>That the residuals (the difference between actual and predicted values) are normally distributed</li>
<li>That the residuals are independent across observations</li>
<li>That the variance of residuals is constant</li>
</ul>
<p>It is<a id="_idIndexMarker612"/> not unusual for one or more of these assumptions to be violated with real-world data. The relationship between a feature and target is often not linear. The influence of the feature may vary across the range of that feature. Anyone familiar with the expression “<em class="italic">too many cooks in the kitchen</em>” likely appreciates that the marginal increase in productivity with the fifth cook may not be as great as with the second or third.</p>
<p>Our residuals are sometimes not normally distributed. This can indicate that our model is less accurate along certain ranges of our target. For example, it is not unusual to have smaller residuals along the middle of the target’s range, say the 25th to 75th percentile, and higher residuals at the extremes. This can happen when the relationship with the target is nonlinear.</p>
<p>There are several reasons why residuals may not be independent. This is often the case with time series data. For a model of daily stock price, the residuals may be correlated for adjacent days. This is referred to as autocorrelation. This can also be a problem with longitudinal or repeated measures data. For example, we may have test scores for 600 students in 20 different classrooms or annual wage income for 100 people. Our residuals would not be independent if our model failed to account for there being no variation in some features across a group – the classroom-determined and person-determined features in these examples. </p>
<p>Finally, it is not uncommon for our residuals to have greater variability along different ranges of a feature. If we are predicting temperatures at weather stations around the world, and latitude is one of the features we are using, there is a chance that there will be greater residuals at higher latitude values. This is known as heteroscedasticity. This may also be an indicator that our model has omitted important predictors.</p>
<p>Beyond these four key assumptions, another common challenge with linear regression is the high correlation among features. This is known as multicollinearity. As we discussed in <a href="B17978_05_ePub.xhtml#_idTextAnchor058"><em class="italic">Chapter 5</em></a>, <em class="italic">Feature Selection</em>, we likely increase the risk of overfitting when our model struggles to isolate the independent effect of a particular feature because it moves so much with another feature. This will be familiar to any of you who have spent weeks building a model where the coefficients shift dramatically with each new specification.</p>
<p>When one or more of these assumptions is violated, we may still be able to use a traditional regression model. However, we may need to transform the data in some way. We will discuss techniques for identifying violations of these assumptions, the implications<a id="_idIndexMarker613"/> of those violations for model performance, and possible ways to address these issues throughout this chapter.</p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor095"/>Linear regression and ordinary least squares</h2>
<p>The most <a id="_idIndexMarker614"/>common estimation technique for linear<a id="_idIndexMarker615"/> regression is <strong class="bold">ordinary least squares</strong> (<strong class="bold">OLS</strong>). OLS selects coefficients that minimize the sum of the squared distance between the actual target values and the predicted values. More precisely, OLS minimizes the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer104">
<img alt="" height="225" src="image/B17978_07_001.jpg" width="372"/>
</div>
</div>
<p>Here, <img alt="" height="38" src="image/B17978_07_002.png" width="40"/> is the actual value at the ith observation and <img alt="" height="47" src="image/B17978_07_003.png" width="38"/> is the predicted value. As we have discussed, the difference between the actual target value and the predicted target value, <img alt="" height="45" src="image/B17978_07_004.png" width="132"/>, is known as the residual.</p>
<p>Graphically, OLS fits a line through our data that minimizes the vertical distance of data points from that line. The following plot illustrates a model with one feature, known as simple linear regression, with made-up data points. The vertical distance between each data point and the regression line is the residual, which can be positive or negative: </p>
<div>
<div class="IMG---Figure" id="_idContainer108">
<img alt="Figure 7.1 – Ordinary least squares regression line " height="628" src="image/B17978_07_0011.jpg" width="840"/>
</div>
</div>
<p class="figure-caption">Figure 7.1 – Ordinary least squares regression line</p>
<p>The line,<img alt="" height="47" src="image/B17978_07_005.png" width="288"/>, gives us the predicted value of <em class="italic">y</em> for each value of <em class="italic">x</em>. It is equal to the estimated intercept, <img alt="" height="46" src="image/B17978_07_006.png" width="45"/>, plus the estimated coefficient for the feature times the feature value, <img alt="" height="44" src="image/B17978_07_007.png" width="70"/>. This is the OLS line. Any other straight line through the data would result in a <a id="_idIndexMarker616"/>higher sum of squared residuals. This can be <a id="_idIndexMarker617"/>extended to multiple linear regression models – that is, those with more than one feature:</p>
<div>
<div class="IMG---Figure" id="_idContainer112">
<img alt="" height="58" src="image/B17978_07_008.jpg" width="1023"/>
</div>
</div>
<p>Here, <em class="italic">y</em> is the target, each <em class="italic">x</em> is a feature, each <img alt="" height="48" src="image/B17978_07_009.png" width="30"/> is a coefficient (or the intercept), <em class="italic">n</em> is the number of features, and <em class="italic">ɛ</em> is an error term. Each coefficient is the estimated change in the target from a 1-unit change in the associated feature. This is a good place to notice that the coefficient is constant across the whole range of each feature; that is, an increase in the feature from 0 to 1 is assumed to have the same impact on the target as from 999 to 1000. However, this does not always make sense. Later in this chapter, we will discuss how to use transformations when the relationship between a feature and the target is not linear.</p>
<p>An important advantage<a id="_idIndexMarker618"/> of linear regression is that it is not as computationally expensive as other supervised regression algorithms. When linear regression performs well, based on metrics such as those we discussed in the previous chapter, it is a good choice. This is particularly true when you have large amounts of data to train or your business process does not permit large blocks of time for model training. The efficiency of the algorithm can also make it feasible to use more resource-intensive feature selection techniques, such as wrapper methods, which we discussed in <a href="B17978_05_ePub.xhtml#_idTextAnchor058"><em class="italic">Chapter 5</em></a>, <em class="italic">Feature Selection</em>. As we saw there, you may not want to use exhaustive feature selection with a decision tree regressor. However, it may be perfectly fine with a linear regression model.</p>
<h1 id="_idParaDest-97"><a id="_idTextAnchor096"/>Linear regression and gradient descent</h1>
<p>We can use gradient descent, rather than ordinary least squares, to estimate our linear regression parameters. Gradient descent <a id="_idIndexMarker619"/>iterates over possible coefficient values to find those that minimize the residual sum of squares. It starts with random <a id="_idIndexMarker620"/>coefficient values and calculates the sum of squared errors for that iteration. Then, it generates new values for coefficients that yield smaller residuals than those from the previous step. We specify a learning rate when using gradient descent. The learning rate determines the amount of improvement in residuals at each step.</p>
<p>Gradient descent can often be a good choice when working with very large datasets. It may be the only choice if the full dataset does not fit into your machine’s memory. We will use both OLS and gradient descent to estimate our parameters in the next section.</p>
<h1 id="_idParaDest-98"><a id="_idTextAnchor097"/>Using classical linear regression</h1>
<p>In this section, we <a id="_idIndexMarker621"/>will specify a fairly straightforward linear model. We will use it to predict the implied gasoline tax of a country based on several national economic and political measures. But before we specify our model, we need to do the pre-processing tasks we discussed in the first few chapters of this book.</p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor098"/>Pre-processing the data for our regression model</h2>
<p>We will use<a id="_idIndexMarker622"/> pipelines to pre-process our data in this chapter, and throughout the rest of this book. We need to impute values where they are missing, identify and handle outliers, and encode and scale our data. We also need to do this in a way that avoids data leakage and cleans the training data without peeking ahead to the testing data. As we saw in <a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for Model Evaluation</em>, scikit-learn’s pipelines can help with these tasks.</p>
<p>The dataset we will use contains the implied gasoline tax for each country and some possible predictors, including national income per capita, government debt, fuel income dependency, extent of car use, and measures of democratic processes and government effectiveness.</p>
<p class="callout-heading">Note</p>
<p class="callout">This dataset on implied gasoline tax by country is available for public use on the Harvard Dataverse at <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/RX4JGK">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/RX4JGK</a>. It was compiled by <em class="italic">Paasha Mahdavi</em>, <em class="italic">Cesar B. Martinez-Alvarez</em>, and <em class="italic">Michael L. Ross</em>. The implied gasoline tax is calculated based on the difference between the world benchmark price and the local price for a liter of gas. A local price above the benchmark price represents a tax. When the benchmark price is higher, it can be considered a subsidy. We will use 2014 data for each country for this analysis.</p>
<p>Let’s start by pre-processing the data:</p>
<ol>
<li>First, we load many of the libraries we worked with in the last few chapters. However, we also need two new libraries to build the pipeline for our data – <strong class="source-inline">ColumnTransformer</strong> and <strong class="source-inline">TransformedTargetRegressor</strong>. These libraries allow us to build a pipeline that does different pre-processing on numerical and categorical features, and that also transforms our target:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.linear_model import LinearRegression</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.compose import TransformedTargetRegressor</p><p class="source-code">from sklearn.feature_selection import RFE</p><p class="source-code">from sklearn.impute import KNNImputer</p><p class="source-code">from sklearn.model_selection import cross_validate, KFold</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
<li>We can <a id="_idIndexMarker623"/>extend the functionality of a scikit-learn pipeline by adding our own classes. Let’s add a class to handle extreme values called <strong class="source-inline">OutlierTrans</strong>. </li>
</ol>
<p>To include this class in a pipeline, it must inherit from the <strong class="source-inline">BaseEstimator</strong> class. We must also inherit from <strong class="source-inline">TransformerMixin</strong>, though there are other possibilities. Our class needs the <strong class="source-inline">fit</strong> and <strong class="source-inline">transform</strong> methods. We can put code for assigning extreme values as missing in the <strong class="source-inline">transform</strong> method.</p>
<p>But before we can use our class, we need to import it. To import it, we need to append the <strong class="source-inline">helperfunctions</strong> subfolder, since that is where we have placed the <strong class="source-inline">preprocfunc</strong> module that contains our class:</p>
<p class="source-code">import os</p>
<p class="source-code">import sys</p>
<p class="source-code">sys.path.append(os.getcwd() + “/helperfunctions”)</p>
<p class="source-code">from preprocfunc import OutlierTrans</p>
<p>This<a id="_idIndexMarker624"/> imports the <strong class="source-inline">OutlierTrans</strong> class, which we can add to the pipelines we create:</p>
<p class="source-code">class OutlierTrans(BaseEstimator,TransformerMixin):</p>
<p class="source-code">  def __init__(self,threshold=1.5):</p>
<p class="source-code">    self.threshold = threshold</p>
<p class="source-code">  </p>
<p class="source-code">  def fit(self,X,y=None):</p>
<p class="source-code">    return self</p>
<p class="source-code">  </p>
<p class="source-code">  def transform(self,X,y=None):</p>
<p class="source-code">    Xnew = X.copy()</p>
<p class="source-code">    for col in Xnew.columns:</p>
<p class="source-code">      thirdq, firstq = Xnew[col].quantile(0.75),\</p>
<p class="source-code">        Xnew[col].quantile(0.25)</p>
<p class="source-code">      interquartilerange = self.threshold*(thirdq-firstq)</p>
<p class="source-code">      outlierhigh, outlierlow = interquartilerange+thirdq,\</p>
<p class="source-code">        firstq-interquartilerange</p>
<p class="source-code">      Xnew.loc[(Xnew[col]&gt;outlierhigh) | \</p>
<p class="source-code">        (Xnew[col]&lt;outlierlow),col] = np.nan</p>
<p class="source-code">    return Xnew.values</p>
<p>The <strong class="source-inline">OutlierTrans</strong> class uses a fairly standard univariate approach for identifying an outlier. It calculates<a id="_idIndexMarker625"/> the <strong class="bold">interquartile range</strong> (<strong class="bold">IQR</strong>) for each feature, then sets any value that is more than 1.5 times the IQR above the third quartile or below the first quartile to missing. We can change the threshold to something other than 1.5, such as 2.0, if we want to be more conservative. (We discussed this technique for identifying outliers in <a href="B17978_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Examining the Distribution of Features and Targets</em>.)</p>
<ol>
<li value="3">Next, we <a id="_idIndexMarker626"/>load the gasoline tax data for 2014. There are 154 rows – one for each country in the DataFrame. A few features have some missing values, but only one, <strong class="source-inline">motorization_rate</strong>, has double-digit missings. <strong class="source-inline">motorization_rate</strong> is the number of cars per person:<p class="source-code">fftaxrate14 = pd.read_csv(“data/fossilfueltaxrate14.csv”)</p><p class="source-code">fftaxrate14.set_index(‘countrycode’, inplace=True)</p><p class="source-code">fftaxrate14.info()</p><p class="source-code"><strong class="bold">&lt;class ‘pandas.core.frame.DataFrame’&gt;</strong></p><p class="source-code"><strong class="bold">Index: 154 entries, AFG to ZWE</strong></p><p class="source-code"><strong class="bold">Data columns (total 19 columns):</strong></p><p class="source-code"><strong class="bold">#   Column                     Non-Null Count  Dtype</strong></p><p class="source-code"><strong class="bold">                               --------------  -----  </strong></p><p class="source-code"><strong class="bold">0   country                    154 non-null    object</strong></p><p class="source-code"><strong class="bold">1   region                     154 non-null    object</strong></p><p class="source-code"><strong class="bold">2   region_wb                  154 non-null    object</strong></p><p class="source-code"><strong class="bold">3   year                       154 non-null    int64</strong></p><p class="source-code"><strong class="bold">4   gas_tax_imp                154 non-null    float64</strong></p><p class="source-code"><strong class="bold">5   bmgap_diesel_spotprice_la  146 non-null    float64</strong></p><p class="source-code"><strong class="bold">6   fuel_income_dependence     152 non-null    float64</strong></p><p class="source-code"><strong class="bold">7   national_income_per_cap    152 non-null    float64</strong></p><p class="source-code"><strong class="bold">8   VAT_Rate                   151 non-null    float64</strong></p><p class="source-code"><strong class="bold">9   gov_debt_per_gdp           139 non-null    float64</strong></p><p class="source-code"><strong class="bold">10  polity                     151 non-null    float64</strong></p><p class="source-code"><strong class="bold">11  democracy_polity           151 non-null    float64</strong></p><p class="source-code"><strong class="bold">12  autocracy_polity           151 non-null    float64</strong></p><p class="source-code"><strong class="bold">13  goveffect                  154 non-null    float64</strong></p><p class="source-code"><strong class="bold">14  democracy_index            152 non-null    float64</strong></p><p class="source-code"><strong class="bold">15  democracy                  154 non-null    int64</strong></p><p class="source-code"><strong class="bold">16  nat_oil_comp               152 non-null    float64</strong></p><p class="source-code"><strong class="bold">17  nat_oil_comp_state         152 non-null    float64</strong></p><p class="source-code"><strong class="bold">18  motorization_rate          127 non-null    float64</strong></p><p class="source-code"><strong class="bold">dtypes: float64(14), int64(2), object(3)</strong></p><p class="source-code"><strong class="bold">memory usage: 24.1+ KB</strong></p></li>
<li>Let’s<a id="_idIndexMarker627"/> separate the features into numerical and binary features. We will put <strong class="source-inline">motorization_rate</strong> into a special category because we anticipate having to do a little more with it than with the other features:<p class="source-code">num_cols = [‘fuel_income_dependence’, </p><p class="source-code">  ’national_income_per_cap’, ‘VAT_Rate’,  </p><p class="source-code">  ‘gov_debt_per_gdp’, ’polity’, ’goveffect’,</p><p class="source-code">  ‘democracy_index’]</p><p class="source-code">dummy_cols = ‘democracy_polity’,’autocracy_polity’,</p><p class="source-code">  ‘democracy’,’nat_oil_comp’,’nat_oil_comp_state’]</p><p class="source-code">spec_cols = [‘motorization_rate’]</p></li>
<li>We should look at some descriptives for the numeric features and the target. Our target, <strong class="source-inline">gas_tax_imp</strong>, has a median value of 0.52. Notice that some of the features have a very different range. More than half of the countries have a <strong class="source-inline">polity</strong> score of 7 or higher; 10 is the highest possible <strong class="source-inline">polity</strong> score, meaning most democratic. Most countries have a negative value for government effectiveness. <strong class="source-inline">democracy_index</strong> is a very similar measure to <strong class="source-inline">polity</strong>, though there <a id="_idIndexMarker628"/>is more variation:<p class="source-code">fftaxrate14[[‘gas_tax_imp’] + num_cols + spec_cols].\</p><p class="source-code">  agg([‘count’,’min’,’median’,’max’]).T</p><p class="source-code"><strong class="bold">                      count min    median   max</strong></p><p class="source-code"><strong class="bold">gas_tax_imp             154 -0.80  0.52     1.73</strong></p><p class="source-code"><strong class="bold">fuel_income_dependence  152 0.00   0.14     34.43</strong></p><p class="source-code"><strong class="bold">national_income_per_cap 152 260.00 6,050.00 104,540.00</strong></p><p class="source-code"><strong class="bold">VAT_Rate                151 0.00   16.50    27.00</strong></p><p class="source-code"><strong class="bold">gov_debt_per_gdp        139 0.55   39.30    194.76</strong></p><p class="source-code"><strong class="bold">polity                  151 -10.00 7.00     10.00</strong></p><p class="source-code"><strong class="bold">goveffect               154 -2.04  -0.15    2.18</strong></p><p class="source-code"><strong class="bold">democracy_index         152 0.03   0.57     0.93</strong></p><p class="source-code"><strong class="bold">motorization_rate       127 0.00   0.20     0.81</strong></p></li>
<li>Let’s also look at the distribution of the binary features. We must set <strong class="source-inline">normalize</strong> to <strong class="source-inline">True</strong> to generate ratios rather than counts. The <strong class="source-inline">democracy_polity</strong> and <strong class="source-inline">autocracy_polity</strong> features are just binarized versions of the <strong class="source-inline">polity</strong> feature; very high <strong class="source-inline">polity</strong> scores get <strong class="source-inline">democracy_polity</strong> values of 1, while very low <strong class="source-inline">polity</strong> scores get <strong class="source-inline">autocracy_polity</strong> values of 1. Similarly, <strong class="source-inline">democracy</strong> is a dummy feature for those countries with high <strong class="source-inline">democracy_index</strong> values. Interestingly, nearly half of the countries (0.46) have a national oil company, and almost a quarter (0.23) have a state-owned national oil company:<p class="source-code">fftaxrate14[dummy_cols].apply(pd.value_counts, normalize=True).T</p><p class="source-code"><strong class="bold">                               0               1</strong></p><p class="source-code"><strong class="bold">democracy_polity               0.41            0.59</strong></p><p class="source-code"><strong class="bold">autocracy_polity               0.89            0.11</strong></p><p class="source-code"><strong class="bold">democracy                      0.42            0.58</strong></p><p class="source-code"><strong class="bold">nat_oil_comp                   0.54            0.46</strong></p><p class="source-code"><strong class="bold">nat_oil_comp_state             0.77            0.23</strong></p></li>
</ol>
<p>This all looks to be in fairly good shape. However, we will need to do some work on the missing values for several features. We also need to do some scaling, but there is no need to do any encoding because we can use the binary features as they are. Some features are correlated, so we need to do some feature elimination.</p>
<ol>
<li value="7">We begin our <a id="_idIndexMarker629"/>pre-processing by creating training and testing DataFrames. We will only reserve 20% for testing:<p class="source-code">target = fftaxrate14[[‘gas_tax_imp’]]</p><p class="source-code">features = fftaxrate14[num_cols + dummy_cols + spec_cols]</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(features,\</p><p class="source-code">  target, test_size=0.2, random_state=0)</p></li>
<li>We need to build a pipeline with column transformations so that we can do different pre-processing on numeric and categorical data. We will construct a pipeline, <strong class="source-inline">standtrans</strong>, for all of the numeric columns in <strong class="source-inline">num_cols</strong>. First, we want to set outliers to missing. We will define an outlier value as one that is more than two times the interquartile range above the third quartile, or below the first quartile, for that feature. We will use <strong class="source-inline">SimpleImputer</strong> to set missing values to the median.</li>
</ol>
<p>We do not want to scale the binary features in <strong class="source-inline">dummy_cols</strong>, but we do want to use <strong class="source-inline">SimpleImputer</strong> to set missing values to the most frequent value for each categorical feature.</p>
<p>We <a id="_idIndexMarker630"/>won’t use <strong class="source-inline">SimpleImputer</strong> for <strong class="source-inline">motorization_rate</strong>. Remember that <strong class="source-inline">motorization_rate</strong> is not in the <strong class="source-inline">num_cols</strong> list – it is in the <strong class="source-inline">spec_cols</strong> list. We set up a special pipeline, <strong class="source-inline">spectrans</strong>, for features in <strong class="source-inline">spec_cols</strong>. We will use <strong class="bold">K-Nearest Neighbor</strong> (<strong class="bold">KNN</strong>) imputation later to handle missing <strong class="source-inline">motorization_rate</strong> values:</p>
<p class="source-code">standtrans = make_pipeline(OutlierTrans(2), </p>
<p class="source-code">  SimpleImputer(strategy=”median”), StandardScaler())</p>
<p class="source-code">cattrans = make_pipeline(</p>
<p class="source-code">  SimpleImputer(strategy=”most_frequent”))</p>
<p class="source-code">spectrans = make_pipeline(OutlierTrans(2), </p>
<p class="source-code">  StandardScaler())</p>
<p class="source-code">coltrans = ColumnTransformer(</p>
<p class="source-code">  transformers=[</p>
<p class="source-code">    (“stand”, standtrans, num_cols),</p>
<p class="source-code">    (“cat”, cattrans, dummy_cols),</p>
<p class="source-code">    (“spec”, spectrans, spec_cols)</p>
<p class="source-code">  ]</p>
<p class="source-code">)</p>
<p>This sets up all of the pre-processing we want to do on the gasoline tax data. To do the transformations, all we need to do is call the <strong class="source-inline">fit</strong> method of the column transformer. However, we will not do that yet because we also want to add feature selection to the pipeline and get it to run a linear regression. We will do that in the next few steps.</p>
<h2 id="_idParaDest-100"><a id="_idTextAnchor099"/>Running and evaluating our linear model</h2>
<p>We will use <strong class="bold">recursive feature elimination</strong> (<strong class="bold">RFE</strong>) to <a id="_idIndexMarker631"/>select features for our model. RFE has the advantages of wrapper feature selection methods – it evaluates features based on a selected algorithm, and it considers multivariate relationships in that assessment. However, it can also be computationally expensive. Since we do not have many features or observations, that is not much of a problem in this case.</p>
<p>After selecting the features, we<a id="_idIndexMarker632"/> run a linear regression model and take a <a id="_idIndexMarker633"/>look at our predictions. Let’s get started:</p>
<ol>
<li value="1">First, we create linear regression and recursive feature elimination instances and add them to the pipeline. We also create a <strong class="source-inline">TransformedTargetRegressor</strong> object since we still need to transform the target. We pass our pipeline to the regressor parameter of <strong class="source-inline">TransformedTargetRegressor</strong>. </li>
</ol>
<p>Now, we can call the target regressor’s <strong class="source-inline">fit</strong> method. After that, the <strong class="source-inline">support_</strong> attribute of the pipeline’s <strong class="source-inline">rfe</strong> step will give us the selected features. Similarly, we can get the coefficients by getting the <strong class="source-inline">coef_</strong> value of the <strong class="source-inline">linearregression</strong> step. The key here is that referencing <strong class="source-inline">ttr.regressor</strong> gets us to the pipeline:</p>
<p class="source-code">lr = LinearRegression()</p>
<p class="source-code">rfe = RFE(estimator=lr, n_features_to_select=7)</p>
<p class="source-code">pipe1 = make_pipeline(coltrans, </p>
<p class="source-code">  KNNImputer(n_neighbors=5), rfe, lr)</p>
<p class="source-code">ttr=TransformedTargetRegressor(regressor=pipe1,</p>
<p class="source-code">  transformer=StandardScaler())</p>
<p class="source-code">ttr.fit(X_train, y_train)</p>
<p class="source-code">selcols = X_train.columns[ttr.regressor_.named_steps[‘rfe’].support_]</p>
<p class="source-code">coefs = ttr.regressor_.named_steps[‘linearregression’].coef_</p>
<p class="source-code">np.column_stack((coefs.ravel(),selcols))</p>
<p class="source-code"><strong class="bold">array([[0.44753064726665703, ‘VAT_Rate’],</strong></p>
<p class="source-code"><strong class="bold">       [0.12368913577287821, ‘gov_debt_per_gdp’],</strong></p>
<p class="source-code"><strong class="bold">       [0.17926454403985687, ‘goveffect’],</strong></p>
<p class="source-code"><strong class="bold">       [-0.22100930246392841, ‘autocracy_polity’],</strong></p>
<p class="source-code"><strong class="bold">       [-0.15726572731003752, ‘nat_oil_comp’],</strong></p>
<p class="source-code"><strong class="bold">       [-0.7013454686632653, ‘nat_oil_comp_state’],</strong></p>
<p class="source-code"><strong class="bold">       [0.13855012574945422, ‘motorization_rate’]], dtype=object)</strong></p>
<p>Our feature <a id="_idIndexMarker634"/>selection identified the VAT rate, government<a id="_idIndexMarker635"/> debt, a measure of government effectiveness (<strong class="source-inline">goveffect</strong>), whether the country is in the autocracy category, whether there is a national oil company and one that is state-owned, and the motorization rate as the top seven features. The number of features indicated is an example of a hyperparameter, and our choice of seven here is fairly arbitrary. We will discuss techniques for hyperparameter tuning in the next section.</p>
<p>Notice that of the several autocracy/democracy measures in the dataset, the one that seems to matter most is the autocracy dummy, which has a value of 1 for countries with very low <strong class="source-inline">polity</strong> scores. It is estimated to have a negative effect gasoline taxes; that is, to reduce them.</p>
<ol>
<li value="2">Let’s take a look at the <a id="_idIndexMarker636"/>predictions and the residuals. We <a id="_idIndexMarker637"/>can pass the features from the testing data to the transformer’s/pipeline’s <strong class="source-inline">predict</strong> method to generate the predictions. There is a little positive skew and some overall bias; the residuals are negative overall:<p class="source-code">pred = ttr.predict(X_test)</p><p class="source-code">preddf = pd.DataFrame(pred, columns=[‘prediction’],</p><p class="source-code">  index=X_test.index).join(X_test).join(y_test)</p><p class="source-code">preddf[‘resid’] = preddf.gas_tax_imp-preddf.prediction</p><p class="source-code">preddf.resid.agg([‘mean’,’median’,’skew’,’kurtosis’])</p><p class="source-code"><strong class="bold">mean                -0.09</strong></p><p class="source-code"><strong class="bold">median              -0.13</strong></p><p class="source-code"><strong class="bold">skew                 0.61</strong></p><p class="source-code"><strong class="bold">kurtosis             0.04</strong></p><p class="source-code"><strong class="bold">Name: resid, dtype: float64</strong></p></li>
<li>Let’s also generate some overall model evaluation statistics. We get a mean absolute error of <strong class="source-inline">0.23</strong>. That’s not a great average error, given that the median value for the gas tax price is <strong class="source-inline">0.52</strong>. The r-squared is decent, however:<p class="source-code">print(“Mean Absolute Error: %.2f, R-squared: %.2f” % </p><p class="source-code">  (skmet.mean_absolute_error(y_test, pred),</p><p class="source-code">  skmet.r2_score(y_test, pred)))</p><p class="source-code"><strong class="bold">Mean Absolute Error: 0.23, R-squared: 0.75</strong></p></li>
<li>It is usually <a id="_idIndexMarker638"/>helpful to look at a plot of the residuals. Let’s<a id="_idIndexMarker639"/> also draw a red dashed line at the average value of the residuals:<p class="source-code">plt.hist(preddf.resid, color=”blue”, bins=np.arange(-0.5,1.0,0.25))</p><p class="source-code">plt.axvline(preddf.resid.mean(), color=’red’, linestyle=’dashed’, linewidth=1)</p><p class="source-code">plt.title(“Histogram of Residuals for Gax Tax Model”)</p><p class="source-code">plt.xlabel(“Residuals”)</p><p class="source-code">plt.ylabel(“Frequency”)</p><p class="source-code">plt.xlim()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer114">
<img alt="Figure 7.2 – Gas tax model residuals " height="442" src="image/B17978_07_002.jpg" width="569"/>
</div>
</div>
<p class="figure-caption">Figure 7.2 – Gas tax model residuals</p>
<p>This plot shows the positive skew. Moreover, our model is somewhat more likely to over-predict the gas tax than under-predict it. (The residual is negative when the prediction is greater than the actual target value.)</p>
<ol>
<li value="5">Let’s also<a id="_idIndexMarker640"/> look at a scatterplot of the predictions <a id="_idIndexMarker641"/>against the residuals. Let’s draw a red dashed line at 0 on the <em class="italic">Y</em>-axis:<p class="source-code">plt.scatter(preddf.prediction, preddf.resid, color=”blue”)</p><p class="source-code">plt.axhline(0, color=’red’, linestyle=’dashed’, linewidth=1)</p><p class="source-code">plt.title(“Scatterplot of Predictions and Residuals”)</p><p class="source-code">plt.xlabel(“Predicted Gax Tax”)</p><p class="source-code">plt.ylabel(“Residuals”)</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer115">
<img alt="Figure 7.3 – Scatter plot of predictions against residuals " height="445" src="image/B17978_07_003.jpg" width="582"/>
</div>
</div>
<p class="figure-caption">Figure 7.3 – Scatter plot of predictions against residuals</p>
<p>Here, overprediction <a id="_idIndexMarker642"/>occurs throughout the range of predicted <a id="_idIndexMarker643"/>values, but there are no underpredictions (positive residuals) with predictions below 0 or above 1. This should give us some doubts about our assumption of linearity.</p>
<h2 id="_idParaDest-101"><a id="_idTextAnchor100"/>Improving our model evaluation</h2>
<p>One problem <a id="_idIndexMarker644"/>with how we have evaluated our model so far is that we are not making great use of the data. We are only training on about 80% of the data. Our metrics are also quite dependent on the testing data being representative of the real world we want to predict. However, it might not be. We can improve our odds with k-fold cross-validation, as we discussed in the previous chapter.</p>
<p>Since we have been using pipelines for our analysis, we have already done much of the work we need for k-fold cross-validation. Recall from the previous chapter that the k-fold model evaluation divides our data into k equal parts. One of the folds is designated for testing and the rest for training. This is repeated k times, with a different fold being used for testing each time.</p>
<p>Let’s try<a id="_idIndexMarker645"/> k-fold cross-validation<a id="_idIndexMarker646"/> with our linear regression model:</p>
<ol>
<li value="1">We will start by creating new training and testing DataFrames, leaving just 10% for later validation. We do not need to retain as much data for validation, though it is a good idea to always hold a little back:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(features,\</p><p class="source-code">  target, test_size=0.1, random_state=1)</p></li>
<li>We also need to instantiate <strong class="source-inline">KFold</strong> and <strong class="source-inline">LinearRegression</strong> objects:<p class="source-code">kf = KFold(n_splits=3, shuffle=True, random_state=0)</p></li>
<li>Now, we are ready to run our k-fold cross validation. We indicate that we want both r-squared and mean absolute error for each split. <strong class="source-inline">cross_validate</strong> automatically gives us fit and score times for each fold:<p class="source-code">scores = cross_validate(ttr, X=X_train, y=y_train,</p><p class="source-code">  cv=kf, scoring=(‘r2’, ‘neg_mean_absolute_error’), n_jobs=1)</p><p class="source-code">print(“Mean Absolute Error: %.2f, R-squared: %.2f” % </p><p class="source-code">  (scores[‘test_neg_mean_absolute_error’].mean(),</p><p class="source-code">  scores[‘test_r2’].mean()))</p><p class="source-code"><strong class="bold">Mean Absolute Error: -0.25, R-squared: 0.62</strong></p></li>
</ol>
<p>These scores are <a id="_idIndexMarker647"/>not very impressive. We do<a id="_idIndexMarker648"/> not end up explaining as much of the variance as we would like. R-squared scores average about 0.62 across the three folds. This is partly because the testing DataFrames of each fold are quite small, with about 40 observations in each. Nonetheless, we should explore modifications of the classical linear regression approach, such as regularization and non-linear regression. </p>
<p>One advantage of regularization is that we may be able to get similar results without going through a computationally expensive feature selection process. Regularization can also help us avoid overfitting. We will explore lasso regression with the same data in the next section. We will also look into non-linear regression strategies.</p>
<h1 id="_idParaDest-102"><a id="_idTextAnchor101"/>Using lasso regression</h1>
<p>A key characteristic <a id="_idIndexMarker649"/>of OLS is that it produces the parameter estimates with the least bias. However, OLS estimates may have a higher variance than we want. We need to be careful about overfitting when we use a classical linear regression model. One strategy to reduce the likelihood of overfitting is to use regularization. Regularization may also allow us to combine feature selection and model training. This may matter for datasets with a large number of features or observations.</p>
<p>Whereas OLS minimizes mean squared error, regularization techniques seek both minimal error and a reduced number of features. Lasso regression, which we explore in this section, uses L1 regularization, which<a id="_idIndexMarker650"/> penalizes the absolute value<a id="_idIndexMarker651"/> of the coefficients. Ridge regression is similar. It uses L2 regularization, which<a id="_idIndexMarker652"/> penalizes the squared values of the coefficients. Elastic net regression uses both L1 and L2 regularization.</p>
<p>Once again, we will work with the gasoline tax data from the previous section:</p>
<ol>
<li value="1">We will start by importing the same libraries as in the previous section, except we will import the <strong class="source-inline">Lasso</strong> module rather than the <strong class="source-inline">linearregression</strong> module:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.linear_model import Lasso</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.compose import TransformedTargetRegressor</p><p class="source-code">from sklearn.model_selection import cross_validate, KFold</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
<li>We will <a id="_idIndexMarker653"/>also need the <strong class="source-inline">OutlierTrans</strong> class that we created:<p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + “/helperfunctions”)</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>Now, let’s load the gasoline tax data and create testing and training DataFrames:<p class="source-code">fftaxrate14 = pd.read_csv(“data/fossilfueltaxrate14.csv”)</p><p class="source-code">fftaxrate14.set_index(‘countrycode’, inplace=True)</p><p class="source-code">num_cols = [‘fuel_income_dependence’,’national_income_per_cap’,</p><p class="source-code">  ‘VAT_Rate’,  ‘gov_debt_per_gdp’,’polity’,’goveffect’,</p><p class="source-code">  ‘democracy_index’]</p><p class="source-code">dummy_cols = [‘democracy_polity’,’autocracy_polity’,</p><p class="source-code">‘democracy’,’nat_oil_comp’,’nat_oil_comp_state’]</p><p class="source-code">spec_cols = [‘motorization_rate’]</p><p class="source-code">target = fftaxrate14[[‘gas_tax_imp’]]</p><p class="source-code">features = fftaxrate14[num_cols + dummy_cols + spec_cols]</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(features,\</p><p class="source-code">  target, test_size=0.2, random_state=0)</p></li>
<li>We also<a id="_idIndexMarker654"/> need to set up the column transformations:<p class="source-code">standtrans = make_pipeline(</p><p class="source-code">  OutlierTrans(2), SimpleImputer(strategy=”median”),</p><p class="source-code">  StandardScaler())</p><p class="source-code">cattrans = make_pipeline(SimpleImputer(strategy=”most_frequent”))</p><p class="source-code">spectrans = make_pipeline(OutlierTrans(2), StandardScaler())</p><p class="source-code">coltrans = ColumnTransformer(</p><p class="source-code">  transformers=[</p><p class="source-code">    (“stand”, standtrans, num_cols),</p><p class="source-code">    (“cat”, cattrans, dummy_cols),</p><p class="source-code">    (“spec”, spectrans, spec_cols)</p><p class="source-code">  ]</p><p class="source-code">)</p></li>
<li>Now, we<a id="_idIndexMarker655"/> are ready to fit our model. We will start with a fairly conservative alpha of 0.1. The higher the alpha, the greater the penalties for our coefficients. At 0, we get the same results as with linear regression. In addition to column transformation and lasso regression, our pipeline uses KNN imputation for missing values. We will also use the target transformer to scale the gasoline tax target. We will pass the pipeline we just created to the regressor parameter of the target transformer before we fit it:<p class="source-code">lasso = Lasso(alpha=0.1,fit_intercept=False)</p><p class="source-code">pipe1 = make_pipeline(coltrans, KNNImputer(n_neighbors=5), lasso)</p><p class="source-code">ttr=TransformedTargetRegressor(regressor=pipe1,transformer=StandardScaler())</p><p class="source-code">ttr.fit(X_train, y_train)</p></li>
<li>Let’s take a look at the coefficients from lasso regression. If we compare them to the coefficients from linear regression in the previous section, we notice that we end up selecting the same features. Those features that were eliminated with recursive feature selection are largely the same ones that get near zero values with lasso regression:<p class="source-code">coefs = ttr.regressor_[‘lasso’].coef_</p><p class="source-code">np.column_stack((coefs.ravel(), num_cols + dummy_cols + spec_cols))</p><p class="source-code"><strong class="bold">array([[‘-0.0026505240129231175’, ‘fuel_income_dependence’],</strong></p><p class="source-code"><strong class="bold">       [‘0.0’, ‘national_income_per_cap’],</strong></p><p class="source-code"><strong class="bold">       [‘0.43472262042825915’, ‘VAT_Rate’],</strong></p><p class="source-code"><strong class="bold">       [‘0.10927136643326674’, ‘gov_debt_per_gdp’],</strong></p><p class="source-code"><strong class="bold">       [‘0.006825858127837494’, ‘polity’],</strong></p><p class="source-code"><strong class="bold">       [‘0.15823493727828816’, ‘goveffect’],</strong></p><p class="source-code"><strong class="bold">       [‘0.09622123660935211’, ‘democracy_index’],</strong></p><p class="source-code"><strong class="bold">       [‘0.0’, ‘democracy_polity’],</strong></p><p class="source-code"><strong class="bold">       [‘-0.0’, ‘autocracy_polity’],</strong></p><p class="source-code"><strong class="bold">       [‘0.0’, ‘democracy’],</strong></p><p class="source-code"><strong class="bold">       [‘-0.0’, ‘nat_oil_comp’],</strong></p><p class="source-code"><strong class="bold">       [‘-0.2199638245781246’, ‘nat_oil_comp_state’],</strong></p><p class="source-code"><strong class="bold">       [‘0.016680304258453165’, ‘motorization_rate’]], dtype=’&lt;U32’)</strong></p></li>
<li>Let’s<a id="_idIndexMarker656"/> look at the predictions and residuals for this model. The residuals look decent, with little bias and not much skew:<p class="source-code">pred = ttr.predict(X_test)</p><p class="source-code">preddf = pd.DataFrame(pred, columns=[‘prediction’],</p><p class="source-code">  index=X_test.index).join(X_test).join(y_test)</p><p class="source-code">preddf[‘resid’] = preddf.gas_tax_imp-preddf.prediction</p><p class="source-code">preddf.resid.agg([‘mean’,’median’,’skew’,’kurtosis’])</p><p class="source-code"><strong class="bold">mean                 -0.06</strong></p><p class="source-code"><strong class="bold">median               -0.07</strong></p><p class="source-code"><strong class="bold">skew                  0.33</strong></p><p class="source-code"><strong class="bold">kurtosis              0.10</strong></p><p class="source-code">Name: resid, dtype: float64</p></li>
<li>Let’s also<a id="_idIndexMarker657"/> generate the mean absolute error and r-squared. These are not impressive scores. The r-squared is lower than with linear regression, but the mean absolute error is about the same:<p class="source-code">print(“Mean Absolute Error: %.2f, R-squared: %.2f” % </p><p class="source-code">  (skmet.mean_absolute_error(y_test, pred),</p><p class="source-code">  skmet.r2_score(y_test, pred)))</p><p class="source-code"><strong class="bold">Mean Absolute Error: 0.24, R-squared: 0.68</strong></p></li>
<li>We should look at a histogram of the residuals. The distribution of the residuals is quite similar to the linear regression model:<p class="source-code">plt.hist(preddf.resid, color=”blue”, bins=np.arange(-0.5,1.0,0.25))</p><p class="source-code">plt.axvline(preddf.resid.mean(), color=’red’, linestyle=’dashed’, linewidth=1)</p><p class="source-code">plt.title(“Histogram of Residuals for Gax Tax Model”)</p><p class="source-code">plt.xlabel(“Residuals”)</p><p class="source-code">plt.ylabel(“Frequency”)</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<img alt="Figure 7.4 – Gas tax model residuals " height="446" src="image/B17978_07_004.jpg" width="551"/>
</div>
</div>
<p class="figure-caption">Figure 7.4 – Gas tax model residuals</p>
<ol>
<li value="10">Let’s also <a id="_idIndexMarker658"/>look at a scatter plot of the predicted values on the residuals. Our model is likely to over-predict at the lower ranges and under-predict at the upper ranges. This is a change from the linear model, where we consistently over predicted at both extremes:<p class="source-code">plt.scatter(preddf.prediction, preddf.resid, color=”blue”)</p><p class="source-code">plt.axhline(0, color=’red’, linestyle=’dashed’, linewidth=1)</p><p class="source-code">plt.title(“Scatterplot of Predictions and Residuals”)</p><p class="source-code">plt.xlabel(“Predicted Gax Tax”)</p><p class="source-code">plt.ylabel(“Residuals”)</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer117">
<img alt="Figure 7.5 – Scatter plot of predictions against residuals " height="442" src="image/B17978_07_005.jpg" width="577"/>
</div>
</div>
<p class="figure-caption">Figure 7.5 – Scatter plot of predictions against residuals</p>
<ol>
<li value="11">We will <a id="_idIndexMarker659"/>conclude by performing k-fold cross-validation on the model. The scores are lower than but close to those of the linear regression model:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(features,\</p><p class="source-code">  target, test_size=0.1, random_state=22)</p><p class="source-code">kf = KFold(n_splits=4, shuffle=True, random_state=0)</p><p class="source-code">scores = cross_validate(ttr, X=X_train, y=y_train,</p><p class="source-code">  cv=kf, scoring=(‘r2’, ‘neg_mean_absolute_error’), n_jobs=1)</p><p class="source-code">print(“Mean Absolute Error: %.2f, R-squared: %.2f” % </p><p class="source-code">  (scores[‘test_neg_mean_absolute_error’].mean(),</p><p class="source-code">  scores[‘test_r2’].mean()))</p><p class="source-code"><strong class="bold">Mean Absolute Error: -0.27, R-squared: 0.57</strong></p></li>
</ol>
<p>This gives us a model that is not any better than our original model, but it at least handles the feature selection process more efficiently. It is also possible that we could get better results if we tried different values for the alpha hyperparameter. Why not 0.05 or 1.0 instead? We will try to answer that in the next two steps.</p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor102"/>Tuning hyperparameters with grid searches</h2>
<p>Figuring out the best<a id="_idIndexMarker660"/> value for a hyperparameter, such<a id="_idIndexMarker661"/> as the alpha value in the previous example, is known as hyperparameter tuning. One tool in scikit-learn for hyperparameter tuning is <strong class="source-inline">GridSearchCV</strong>. The CV suffix is for cross-validate.</p>
<p>Using <strong class="source-inline">GridSearchCV</strong> is <a id="_idIndexMarker662"/>very straightforward. If we already have a pipeline, as we do in this case, we pass it to a <strong class="source-inline">GridSearchCV</strong> object, along with a dictionary of parameters. <strong class="source-inline">GridSearchCV</strong> will try all combinations of parameters and return the best one. Let’s try it on our lasso regression model:</p>
<ol>
<li value="1">First, we will instantiate a <strong class="source-inline">lasso</strong> object and create a dictionary with the hyperparameters to be tuned. The dictionary, <strong class="source-inline">lasso_params</strong>, indicates that we want to try all the alpha values between 0.05 and 0.9 at 0.5 intervals. We cannot choose any name we want for the dictionary key. <strong class="source-inline">regressor__lasso__alpha</strong> is based on the names of the steps in the pipeline. Also, notice that we are using double underscores. Single underscores will return an error:<p class="source-code">lasso = Lasso()</p><p class="source-code">lasso_params = {‘regressor__lasso__alpha’: np.arange(0.05, 1, 0.05)}</p></li>
<li>Now, we <a id="_idIndexMarker663"/>can run the grid search. We will <a id="_idIndexMarker664"/>pass the pipeline, which is a <strong class="source-inline">TransformedTargetRegressor</strong> in this case, and the dictionary to <strong class="source-inline">GridSearchCV</strong>. The <strong class="source-inline">best_params_</strong> attribute indicates that the best alpha is <strong class="source-inline">0.05</strong>. When we use that value, we get an r-squared of <strong class="source-inline">0.60</strong>:<p class="source-code">gs = GridSearchCV(ttr,param_grid=lasso_params, cv=5)</p><p class="source-code">gs.fit(X_train, y_train)</p><p class="source-code">gs.best_params_</p><p class="source-code"><strong class="bold">{‘regressor__lasso__alpha’: 0.05}</strong></p><p class="source-code">gs.best_score_</p><p class="source-code"><strong class="bold">0.6028804486340877</strong></p></li>
</ol>
<p>The lasso regression model comes close to but does not do quite as well as the linear model in terms of mean absolute error and r-squared. One benefit of lasso regression is that we do not need to do a separate feature selection step before training our model. (Recall that for wrapper feature selection methods, the model needs to be trained during feature selection as well as after, as we discussed in <a href="B17978_05_ePub.xhtml#_idTextAnchor058"><em class="italic">Chapter 5</em></a>, <em class="italic">Feature Selection</em>.)</p>
<h1 id="_idParaDest-104"><a id="_idTextAnchor103"/>Using non-linear regression</h1>
<p>Linear regression assumes <a id="_idIndexMarker665"/>that the relationship of a feature to the target is constant across the range of the feature. You may recall that the simple linear regression equation that we discussed at the beginning of this chapter had one slope estimate for each feature:</p>
<div>
<div class="IMG---Figure" id="_idContainer118">
<img alt="" height="57" src="image/B17978_07_010.jpg" width="336"/>
</div>
</div>
<p>Here, <em class="italic">y</em> is the target, each <em class="italic">x</em> is a feature, and each β is a coefficient (or the intercept). If the true relationships between features and targets are nonlinear, our model will likely perform poorly.</p>
<p>Fortunately, we<a id="_idIndexMarker666"/> can still make good use of OLS when we cannot assume a linear relationship between the features and the target. We can use the same linear regression algorithm that we used in the previous section, but with a polynomial transformation of the features. This is referred to as polynomial regression.</p>
<p>We add a power to the feature to run a polynomial regression. This gives us the following equation:</p>
<div>
<div class="IMG---Figure" id="_idContainer119">
<img alt="" height="71" src="image/B17978_07_011.jpg" width="1176"/>
</div>
</div>
<p>The following plot compares predicted values for linear versus polynomial regression. The polynomial curve seems to fit the fictional data points better than the linear regression line:</p>
<div>
<div class="IMG---Figure" id="_idContainer120">
<img alt="Figure 7.6 – Illustration of the polynomial equation curve and linear equation line " height="540" src="image/B17978_07_006.jpg" width="700"/>
</div>
</div>
<p class="figure-caption">Figure 7.6 – Illustration of the polynomial equation curve and linear equation line</p>
<p>In this section, we <a id="_idIndexMarker667"/>will experiment with both a linear model and a non-linear model of average annual temperatures at weather stations across the world. We will use latitude and elevation as features. First, we will predict temperature using multiple linear regression, and then try a model with polynomial transformations. Follow these steps:</p>
<ol>
<li value="1">We will start by importing the necessary libraries. These libraries will be familiar if you have been working through this chapter:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler, PolynomialFeatures</p><p class="source-code">from sklearn.linear_model import LinearRegression</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.model_selection import cross_validate</p><p class="source-code">from sklearn.model_selection import KFold</p><p class="source-code">from sklearn.impute import KNNImputer</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
<li>We also need <a id="_idIndexMarker668"/>to import the module that contains our class for identifying outlier values:<p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + “/helperfunctions”)</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>We load the land temperature data, identify the features we want, and generate some descriptive statistics. There are a few missing values for elevation and some extreme negative values for average annual temperature. The range of values for the target and features is very different, so we will probably want to scale our data:<p class="source-code">landtemps = pd.read_csv(“data/landtempsb2019avgs.csv”)</p><p class="source-code">landtemps.set_index(‘locationid’, inplace=True)</p><p class="source-code">feature_cols = [‘latabs’,’elevation’]</p><p class="source-code">landtemps[[‘avgtemp’] + feature_cols].\</p><p class="source-code">  agg([‘count’,’min’,’median’,’max’]).T</p><p class="source-code"><strong class="bold">             count      min      median    max</strong></p><p class="source-code"><strong class="bold">avgtemp      12,095    -60.82    10.45     33.93</strong></p><p class="source-code"><strong class="bold">latabs       12,095     0.02     40.67     90.00</strong></p><p class="source-code"><strong class="bold">elevation    12,088    -350.00   271.30    4,701.00</strong></p></li>
<li>Next, we create the training and testing DataFrames:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(landtemps[feature_cols],\</p><p class="source-code">  landtemps[[‘avgtemp’]], test_size=0.1, random_state=0)</p></li>
<li>Now, we <a id="_idIndexMarker669"/>build a pipeline to handle our pre-processing – set outlier values to missing, do KNN imputation for all missing values, and scale the features – and then run a linear model. We do k-fold cross-validation with 10 folds and get an average r-squared of 0.79 and a mean absolute error of -2.8:<p class="source-code">lr = LinearRegression()</p><p class="source-code">knnimp = KNNImputer(n_neighbors=45)</p><p class="source-code">pipe1 = make_pipeline(OutlierTrans(3),knnimp,</p><p class="source-code">  StandardScaler(), lr)</p><p class="source-code">ttr=TransformedTargetRegressor(regressor=pipe1,</p><p class="source-code">  transformer=StandardScaler())</p><p class="source-code">kf = KFold(n_splits=10, shuffle=True, random_state=0)</p><p class="source-code">scores = cross_validate(ttr, X=X_train, y=y_train,</p><p class="source-code">  cv=kf, scoring=(‘r2’, ‘neg_mean_absolute_error’), n_jobs=1)</p><p class="source-code">scores[‘test_r2’].mean(), scores[‘test_neg_mean_absolute_error’].mean()</p><p class="source-code"><strong class="bold">(0.7933471824738406, -2.8047627785750913)</strong></p></li>
</ol>
<p>Notice that we are quite conservative with our identification of outliers. We pass a threshold value of 3, meaning that a value needs to be more than three times the interquartile range above or below that range. Obviously, we would typically give much more thought to the identification of outliers. We demonstrate here only how to handle outliers in a pipeline once we have decided that that makes sense.</p>
<ol>
<li value="6">Let’s see the <a id="_idIndexMarker670"/>predictions and the residuals. There is almost no bias overall (the average of the residuals is 0), but there is some negative skew:<p class="source-code">ttr.fit(X_train, y_train)</p><p class="source-code">pred = ttr.predict(X_test)</p><p class="source-code">preddf = pd.DataFrame(pred, columns=[‘prediction’],</p><p class="source-code">  index=X_test.index).join(X_test).join(y_test)</p><p class="source-code">preddf.resid.agg([‘mean’,’median’,’skew’,’kurtosis’])</p><p class="source-code"><strong class="bold">mean                 0.00</strong></p><p class="source-code"><strong class="bold">median               0.50</strong></p><p class="source-code"><strong class="bold">skew                -1.13</strong></p><p class="source-code"><strong class="bold">kurtosis             3.48</strong></p><p class="source-code">Name: resid, dtype: float64</p></li>
<li>It is easy <a id="_idIndexMarker671"/>to see this skew if we create a histogram of the residuals. There are some extreme negative residuals – that is, times when we over-predict the average temperature by a lot:<p class="source-code">plt.hist(preddf.resid, color=”blue”)</p><p class="source-code">plt.axvline(preddf.resid.mean(), color=’red’, </p><p class="source-code">  linestyle=’dashed’, linewidth=1)</p><p class="source-code">plt.title(“Histogram of Residuals for Linear Model of Temperature”)</p><p class="source-code">plt.xlabel(“Residuals”)</p><p class="source-code">plt.ylabel(“Frequency”)</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<img alt="Figure 7.7 – Temperature model residuals " height="445" src="image/B17978_07_007.jpg" width="564"/>
</div>
</div>
<p class="figure-caption">Figure 7.7 – Temperature model residuals</p>
<ol>
<li value="8">It can also be helpful to plot the predicted values against the residuals:<p class="source-code">plt.scatter(preddf.prediction, preddf.resid, color=”blue”)</p><p class="source-code">plt.axhline(0, color=’red’, linestyle=’dashed’, linewidth=1)</p><p class="source-code">plt.title(“Scatterplot of Predictions and Residuals”)</p><p class="source-code">plt.xlabel(“Predicted Temperature”)</p><p class="source-code">plt.ylabel(“Residuals”)</p><p class="source-code">plt.xlim(-20,40)</p><p class="source-code">plt.ylim(-27,10)</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This <a id="_idIndexMarker672"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer122">
<img alt="Figure 7.8 – Scatter plot of predictions against residuals " height="447" src="image/B17978_07_0081.jpg" width="575"/>
</div>
</div>
<p class="figure-caption">Figure 7.8 – Scatter plot of predictions against residuals</p>
<p>Our model over-predicts all predictions above approximately 28 degrees Celsius. It is also likely to underpredict when it predicts values between 18 and 28.</p>
<p>Let’s see if we get any<a id="_idIndexMarker673"/> better results with polynomial regression:</p>
<ol>
<li value="9">First, we will create a <strong class="source-inline">PolynomialFeatures</strong> object with a <strong class="source-inline">degree</strong> of <strong class="source-inline">4</strong> and fit it. We can pass the original feature names to the <strong class="source-inline">get_feature_names</strong> method to get the names of the columns that will be returned after the transformation. Second, third, and fourth power values of each feature are created, as well as interaction effects between variables (such as <strong class="source-inline">latabs</strong> * <strong class="source-inline">elevation</strong>). We do not need to run the fit here since that will happen in the pipeline. We are just doing it here to get a feel for how it works:<p class="source-code">polytrans = PolynomialFeatures(degree=4, include_bias=False)</p><p class="source-code">polytrans.fit(X_train.dropna())</p><p class="source-code">featurenames = polytrans.get_feature_names(feature_cols)</p><p class="source-code">featurenames</p><p class="source-code">[‘latabs’,</p><p class="source-code"> ‘elevation’,</p><p class="source-code"> ‘latabs^2’,</p><p class="source-code"> ‘latabs elevation’,</p><p class="source-code"> ‘elevation^2’,</p><p class="source-code"> ‘latabs^3’,</p><p class="source-code"> ‘latabs^2 elevation’,</p><p class="source-code"> ‘latabs elevation^2’,</p><p class="source-code"> ‘elevation^3’,</p><p class="source-code"> ‘latabs^4’,</p><p class="source-code"> ‘latabs^3 elevation’,</p><p class="source-code"> ‘latabs^2 elevation^2’,</p><p class="source-code"> ‘latabs elevation^3’,</p><p class="source-code"> ‘elevation^4’]</p></li>
<li>Next, we <a id="_idIndexMarker674"/>will create a pipeline for our polynomial regression. The pipeline is pretty much the same as with linear regression, except we insert the polynomial transformation step after the KNN imputation:<p class="source-code">pipe2 = make_pipeline(OutlierTrans(3), knnimp,</p><p class="source-code">  polytrans, StandardScaler(), lr)</p><p class="source-code">ttr2 = TransformedTargetRegressor(regressor=pipe2,\</p><p class="source-code">  transformer=StandardScaler())</p></li>
<li>Now, let’s create predictions and residuals based on the polynomial model. There is a little less skew in the residuals than with the linear model:<p class="source-code">ttr2.fit(X_train, y_train)</p><p class="source-code">pred = ttr2.predict(X_test)</p><p class="source-code">preddf = pd.DataFrame(pred, columns=[‘prediction’],</p><p class="source-code">  index=X_test.index).join(X_test).join(y_test)</p><p class="source-code">preddf[‘resid’] = preddf.avgtemp-preddf.prediction</p><p class="source-code">preddf.resid.agg([‘mean’,’median’,’skew’,’kurtosis’])</p><p class="source-code"><strong class="bold">mean                 0.01</strong></p><p class="source-code"><strong class="bold">median               0.20</strong></p><p class="source-code"><strong class="bold">skew                -0.98</strong></p><p class="source-code"><strong class="bold">kurtosis             3.34</strong></p><p class="source-code"><strong class="bold">Name: resid, dtype: float64</strong></p></li>
<li>We should <a id="_idIndexMarker675"/>take a look at a histogram of the residuals:<p class="source-code">plt.hist(preddf.resid, color=”blue”)</p><p class="source-code">plt.axvline(preddf.resid.mean(), color=’red’, linestyle=’dashed’, linewidth=1)</p><p class="source-code">plt.title(“Histogram of Residuals for Temperature Model”)</p><p class="source-code">plt.xlabel(“Residuals”)</p><p class="source-code">plt.ylabel(“Frequency”)</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer123">
<img alt="Figure 7.9 – Temperature model residuals " height="449" src="image/B17978_07_009.jpg" width="564"/>
</div>
</div>
<p class="figure-caption">Figure 7.9 – Temperature model residuals</p>
<ol>
<li value="13">Let’s also do <a id="_idIndexMarker676"/>another scatter plot of predicted values against residuals. These look a little bit better than the residuals with the linear model, particularly at the upper ranges of the predictions:<p class="source-code">plt.scatter(preddf.prediction, preddf.resid, color=”blue”)</p><p class="source-code">plt.axhline(0, color=’red’, linestyle=’dashed’, linewidth=1)</p><p class="source-code">plt.title(“Scatterplot of Predictions and Residuals”)</p><p class="source-code">plt.xlabel(“Predicted Temperature”)</p><p class="source-code">plt.ylabel(“Residuals”)</p><p class="source-code">plt.xlim(-20,40)</p><p class="source-code">plt.ylim(-27,10)</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<img alt="Figure 7.10 – Scatter plot of predictions against residuals " height="451" src="image/B17978_07_0101.jpg" width="575"/>
</div>
</div>
<p class="figure-caption">Figure 7.10 – Scatter plot of predictions against residuals</p>
<ol>
<li value="14">Let’s do <a id="_idIndexMarker677"/>k-fold cross-validation once again and take the average r-squared value across the folds. There are improvements in both the r-squared and mean absolute error compared to the linear model:<p class="source-code">scores = cross_validate(ttr2, X=X_train, y=y_train,</p><p class="source-code">  cv=kf, scoring=(‘r2’, ‘neg_mean_absolute_error’),</p><p class="source-code">  n_jobs=1)</p><p class="source-code">scores[‘test_r2’].mean(), scores[‘test_neg_mean_absolute_error’].mean()</p><p class="source-code"><strong class="bold">(0.8323274036342788, -2.4035803290965507)</strong></p></li>
</ol>
<p>The<a id="_idIndexMarker678"/> polynomial transformation improved our overall results, particularly within certain ranges of our predictors. The residuals at high temperatures were noticeably lower. It is often a good idea to try a polynomial transformation when our residuals suggest that there might be a nonlinear relationship between our features and our target.</p>
<h1 id="_idParaDest-105"><a id="_idTextAnchor104"/>Regression with gradient descent</h1>
<p>Gradient<a id="_idIndexMarker679"/> descent can be a good alternative to ordinary least squares <a id="_idIndexMarker680"/>for optimizing the loss function of a linear model. This is particularly true when working with very large datasets. In this section, we will use gradient descent with the land temperatures dataset, mainly to demonstrate how to use it and to give us another opportunity to explore exhaustive grid searches. Let’s get started:</p>
<ol>
<li value="1">First, we will load the same libraries we have been working with so far, plus the stochastic gradient descent regressor from scikit-learn:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.linear_model import SGDRegressor</p><p class="source-code">from sklearn.compose import TransformedTargetRegressor</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.impute import KNNImputer</p><p class="source-code">from sklearn.model_selection import GridSearchCV</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + “/helperfunctions”)</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>Then, we will load the land temperatures dataset again and create training and testing DataFrames:<p class="source-code">landtemps = pd.read_csv(“data/landtempsb2019avgs.csv”)</p><p class="source-code">landtemps.set_index(‘locationid’, inplace=True)</p><p class="source-code">feature_cols = [‘latabs’,’elevation’]</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(landtemps[feature_cols],\</p><p class="source-code">  landtemps[[‘avgtemp’]], test_size=0.1, random_state=0)</p></li>
<li>Next, we <a id="_idIndexMarker681"/>will set up a pipeline to deal with outliers, impute <a id="_idIndexMarker682"/>values for missings, and scale the data before running the gradient descent:<p class="source-code">knnimp = KNNImputer(n_neighbors=45)</p><p class="source-code">sgdr = SGDRegressor()</p><p class="source-code">pipe1 = make_pipeline(OutlierTrans(3),knnimp,StandardScaler(), sgdr)</p><p class="source-code">ttr=TransformedTargetRegressor(regressor=pipe1,transformer=StandardScaler())</p></li>
<li>Now, we need to create a dictionary to indicate the hyperparameters we want to tune and the values to try. We want to try values for alpha, the loss function, epsilon, and the penalty. We will give each key in the dictionary a prefix of <strong class="source-inline">regressor__sgdregressor__</strong> because that is where the stochastic gradient descent regressor can be found in the pipeline.</li>
</ol>
<p>The <strong class="source-inline">alpha</strong> parameter determines the size of the penalty. The default is <strong class="source-inline">0.0001</strong>. We can choose L1, L2, or elastic net regularization. We will select <strong class="source-inline">huber</strong> and <strong class="source-inline">epsilon_insensitive</strong> as loss functions to include in the search. The default loss function is <strong class="source-inline">squared_error</strong>, but that would just give us ordinary least squares again.</p>
<p>The <strong class="source-inline">huber</strong> loss<a id="_idIndexMarker683"/> function is less sensitive to outliers <a id="_idIndexMarker684"/>than is OLS. How sensitive it is, is based on the value of epsilon we specify. With the <strong class="source-inline">epsilon_insensitive</strong> loss function, errors within a given range (epsilon) are not penalized (we will construct models with epsilon-insensitive tubes in the next chapter, where we’ll examine support vector regression):</p>
<p class="source-code">sgdr_params = {</p>
<p class="source-code"> ‘regressor__sgdregressor__alpha’: 10.0 ** -np.arange(1, 7),</p>
<p class="source-code"> ‘regressor__sgdregressor__loss’: [‘huber’,’epsilon_insensitive’],</p>
<p class="source-code"> ‘regressor__sgdregressor__penalty’: [‘l2’, ‘l1’, ‘elasticnet’],</p>
<p class="source-code"> ‘regressor__sgdregressor__epsilon’: np.arange(0.1, 1.6, 0.1)</p>
<p class="source-code">}</p>
<ol>
<li value="5">Now, we are ready to run an exhaustive grid search. The best parameters are <strong class="source-inline">0.001</strong> for alpha, <strong class="source-inline">1.3</strong> for epsilon, <strong class="source-inline">huber</strong> for the loss function, and elastic net regularization:<p class="source-code">gs = GridSearchCV(ttr,param_grid=sgdr_params, cv=5, scoring=”r2”)</p><p class="source-code">gs.fit(X_train, y_train)</p><p class="source-code">gs.best_params_</p><p class="source-code"><strong class="bold">{‘regressor__sgdregressor__alpha’: 0.001,</strong></p><p class="source-code"><strong class="bold"> ‘regressor__sgdregressor__epsilon’: 1.3000000000000003,</strong></p><p class="source-code"><strong class="bold"> ‘regressor__sgdregressor__loss’: ‘huber’,</strong></p><p class="source-code"><strong class="bold"> ‘regressor__sgdregressor__penalty’: ‘elasticnet’}</strong></p><p class="source-code">gs.best_score_</p><p class="source-code"><strong class="bold">0.7941051735846133</strong></p></li>
<li>I usually find <a id="_idIndexMarker685"/>it helpful to look at the hyperparameters <a id="_idIndexMarker686"/>for some of the other grid search iterations. Huber loss models, with either elastic net or L2 regularization, perform best:<p class="source-code">Results = \</p><p class="source-code">  pd.DataFrame(gs.cv_results_[‘mean_test_score’], \</p><p class="source-code">    columns=[‘meanscore’]).\</p><p class="source-code">  join(pd.DataFrame(gs.cv_results_[‘params’])).\</p><p class="source-code">  sort_values([‘meanscore’], ascending=False)</p><p class="source-code">results.head(3).T</p><p class="source-code"><strong class="bold">                                         254      252      534</strong></p><p class="source-code"><strong class="bold">meanscore                           0.794105 0.794011 0.794009</strong></p><p class="source-code"><strong class="bold">regressor__sgdregressor__alpha      0.001000 0.001000 0.000001</strong></p><p class="source-code"><strong class="bold">regressor__sgdregressor__epsilon    1.300000 1.300000 1.500000</strong></p><p class="source-code"><strong class="bold">regressor__sgdregressor__loss          huber    huber    huber</strong></p><p class="source-code"><strong class="bold">regressor__sgdregressor__penalty  elasticnet       l2       l2</strong></p></li>
</ol>
<p>Stochastic gradient descent<a id="_idIndexMarker687"/> is a generalized approach to optimization and can be applied to many machine learning problems. It is often quite efficient, as we can see here. We were able to run an exhaustive grid search on penalty, penalty size, epsilon, and loss function fairly quickly.</p>
<h1 id="_idParaDest-106"><a id="_idTextAnchor105"/>Summary</h1>
<p>This chapter allowed us to explore a very well-known machine learning algorithm: linear regression. We examined the qualities of a feature space that makes it a good candidate for a linear model. We also explored how to improve a linear model, when necessary, with regularization and with transformations. Then, we looked at stochastic gradient descent as an alternative to OLS optimization. We also learned how to add our own classes to a pipeline and how to do hyperparameter tuning.</p>
<p>In the next chapter, we will explore support vector regression.</p>
</div>
</div>
</body></html>