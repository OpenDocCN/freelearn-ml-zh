<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;12.&#xA0;Recommendation Systems"><div class="book" id="2IV0U2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch12" class="calibre1"/>Chapter 12. Recommendation Systems</h1></div></div></div><p class="calibre8">In our final chapter, we'll tackle one of the most ubiquitous problems prevalent in the e-commerce world: making effective product recommendations to customers. Recommendation systems, also referred to as recommender systems, often rely on the notion of similarity between objects in an approach known as collaborative filtering. Its basic premise is that customers can be considered similar to each other if they share most of the products that they have purchased; equally, items can be considered similar to each other if they share a large number of customers who purchased them.</p><p class="calibre8">There are a number of different ways to quantify this notion of similarity, and we will present some of the commonly used alternatives. Whether we want to recommend movies, books, hotels, or restaurants, building a recommender system often involves dealing with very large datasets.</p></div>

<div class="book" title="Chapter&#xA0;12.&#xA0;Recommendation Systems">
<div class="book" title="Rating matrix"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch12lvl1sec80" class="calibre1"/>Rating matrix</h1></div></div></div><p class="calibre8">A <span class="strong"><strong class="calibre2">recommendation system</strong></span> usually involves having a set of users, <span class="strong"><em class="calibre9">U = {u<sub class="calibre14">1</sub></em></span>
<span class="strong"><em class="calibre9">, u<sub class="calibre14">2</sub></em></span>
<span class="strong"><em class="calibre9">, …, u<sub class="calibre14">m</sub></em></span>
<span class="strong"><em class="calibre9">},</em></span> that have <a id="id871" class="calibre1"/>varying preferences on a set of items, <span class="strong"><em class="calibre9">I = {i<sub class="calibre14">1</sub>, i<sub class="calibre14">2</sub>, …, i<sub class="calibre14">n</sub></em></span>
<span class="strong"><em class="calibre9">}</em></span>. The number of users, <span class="strong"><em class="calibre9">|U| = m</em></span>, is usually different from the number of items, <span class="strong"><em class="calibre9">|I| = n</em></span>. In addition, users <a id="id872" class="calibre1"/>can often express their preference by rating items on some scale. As an example, we can think of users as being restaurant patrons in a city, and the items being the restaurants that they visit. Under this setup, the preferences of the users could be expressed as ratings on a five-star scale. Of course, our generalization does not require that the items be physical items or that the users be actual people—this is simply an abstraction that is commonly used for the recommender system problem.</p><p class="calibre8">As an illustration, think of a dating website in which users rate other users; here, the <span class="strong"><em class="calibre9">items</em></span> that are being rated are the profiles of the actual users themselves. Let's return to our example of a restaurant recommender system and build some example data. A natural data structure that is popular for recommendation systems is the <span class="strong"><strong class="calibre2">rating matrix</strong></span>. This is an <span class="strong"><em class="calibre9">m × n</em></span> matrix where the rows represent the users and the columns represent the items. Each entry, <span class="strong"><em class="calibre9">e<sub class="calibre14">i</sub>, <sub class="calibre14">j</sub></em></span>, of the matrix represents the rating made by the user <span class="strong"><em class="calibre9">i</em></span> for item <span class="strong"><em class="calibre9">j</em></span>. What follows is a simple example:</p><div class="informalexample"><pre class="programlisting">&gt;oliver&lt;- c(1,1,2,5,7,8,9,7)
&gt;thibault&lt;- c(5,9,4,1,1,7,5,9)
&gt;maria&lt;- c(1,4,2,5,8,6,2,8)
&gt;pedro&lt;- c(2,6,7,2,6,1,8,9)
&gt;ines&lt;- c(1,3,2,4,8,9,7,7)
&gt;gertrude&lt;- c(1,6,5,7,3,2,5,5)
&gt;ratingMatrix&lt;- rbind(oliver, thibault, maria, pedro, ines,  
gertrude)
&gt;colnames(ratingMatrix) &lt;- c("Berny's", "La Traviata", "El Pollo
  Loco", "Joey's Pizza", "The Old West", "Jake and Jill", "Full 
  Moon", "Acropolis")
&gt;ratingMatrix
         Berny's La Traviata El Pollo Loco Joey's Pizza
oliver         1           1             2            5
thibault       5           9             4            1
maria          1           4             2            5
pedro          2           6             7            2
ines           1           3             2            4
gertrude       1           6             5            7
         The Old West Jake and Jill Full Moon Acropolis
oliver              7             8         9         7
thibault            1             7         5         9
maria               8             6         2         8
pedro               6             1         8         9
ines                8             9         7         7
gertrude            3             2         5         5</pre></div><p class="calibre8">Here, we have used a 10-point scale as a rating system, where 10 is the highest rating and 1 is the <a id="id873" class="calibre1"/>lowest. An alternative rating scale is a binary rating scale, where 1 indicates a positive rating and 0 indicates a negative rating. This second approach would yield a binary rating matrix. How might we be able to use this rating matrix in order to inform a simple recommender system for other users?</p><p class="calibre8">Concretely, suppose that a new user, Silvan, has rated a few restaurants and we would like to make a recommendation for a suitable restaurant to which he has not been. Alternatively, we might want to propose a list of the top three restaurants or even predict whether Silvan will like a specific restaurant he is currently considering. </p><p class="calibre8">One way to think about this problem is to find users that have similar views to Silvan on the restaurants that he has already rated. Then, we could use their ratings on restaurants that Silvan has not yet rated in order to predict Silvan's rating for those restaurants. This seems promising, but we should first think about how we might quantify this notion of similarity between two users based on their item ratings.</p></div></div>

<div class="book" title="Chapter&#xA0;12.&#xA0;Recommendation Systems">
<div class="book" title="Rating matrix">
<div class="book" title="Measuring user similarity"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch12lvl2sec100" class="calibre1"/>Measuring user similarity</h2></div></div></div><p class="calibre8">Even <a id="id874" class="calibre1"/>with a very large database of users, chances are that, for a real-world recommender system, it will be rare—if not massively unlikely—to find two people who would rate all the items in our item set with the exact same score. That being said, we can still say that some users are more similar than others based on how they rate different items. For example, in our restaurant rating matrix, we can see that Ines and Oliver rated the first four restaurants poorly and the last four restaurants highly, and so their tastes can be considered far more similar compared to a pair such as Thibault and Pedro, who sometimes agree and sometimes have completely opposite views on a particular restaurant.</p><p class="calibre8">By representing a user as their particular row in the rating matrix, we can think of a user as being a vector in an <span class="strong"><em class="calibre9">n</em></span> dimensional space, <span class="strong"><em class="calibre9">n</em></span> being the number of items. Thus, we can use different distance measures appropriate for vectors in order to measure the similarity of two different users. Note that the notion of distance is inversely proportional to the notion of similarity, and we can thus use measures of distance as measures of similarity by interpreting a large distance between two vectors as analogous to a low similarity score.</p><p class="calibre8">The most <a id="id875" class="calibre1"/>familiar distance metric for two vectors, <span class="strong"><em class="calibre9">a</em></span> and <span class="strong"><em class="calibre9">b,</em></span> is the <span class="strong"><strong class="calibre2">Euclidean distance</strong></span>:</p><div class="mediaobject"><img src="../images/00198.jpeg" alt="Measuring user similarity" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We can use R's built-in <code class="email">dist()</code> function to compute all the pair-wise distances in our rating matrix as follows:</p><div class="informalexample"><pre class="programlisting">&gt;dist(ratingMatrix, method = 'euclidean')
oliverthibaultmariapedroines
thibault 12.529964                                        
maria     8.000000 11.000000                              
pedro    10.723805  9.899495 10.246951                    
ines      3.316625 11.224972  6.082763 10.583005          
gertrude 10.488088 10.344080  8.717798  8.062258 10.440307</pre></div><p class="calibre8">The result is a lower triangular matrix because the Euclidean distance is a symmetric function. Thus, the entry for (<code class="email">maria</code>, <code class="email">pedro</code>) is exactly the same as for (<code class="email">pedro</code>, <code class="email">maria</code>), and so we only need to display one of these. Here, we can explicitly see that Ines and Oliver are the two most similar users as the distance between them is the smallest. Note that we can also talk about the distances between items in terms of the similarity of the ratings they received from different users. All we have to do to compute this is to transpose the rating matrix:</p><div class="informalexample"><pre class="programlisting">&gt;dist(t(ratingMatrix), method = 'euclidean')
                Berny's La Traviata El Pollo Loco Joey's Pizza
La Traviata    8.366600                                       
El Pollo Loco  6.708204    5.744563                           
Joey's Pizza   9.643651    9.949874      7.745967             
The Old West  13.038405   12.247449     10.535654     7.810250
Jake and Jill 12.000000   11.575837     12.449900     9.848858
Full Moon     12.369317   10.246951      8.717798     9.486833
Acropolis     14.212670    8.831761     10.723805    11.789826
              The Old West Jake and Jill Full Moon
La Traviata
El Pollo Loco                                     
Joey's Pizza                                      
The Old West                                      
Jake and Jill     8.246211                        
Full Moon         8.062258      9.110434          
Acropolis         8.831761      9.273618  7.549834</pre></div><p class="calibre8">As we <a id="id876" class="calibre1"/>can see, the two most dissimilar restaurants (that is to say, those with the largest difference between them) are the <span class="strong"><em class="calibre9">Acropolis</em></span> and <span class="strong"><em class="calibre9">Berny's</em></span>. Looking back at the rating matrix, we should easily see why this is the case. The former restaurant has received largely positive reviews across our user base, whereas the reviews have been poor for the latter.</p><p class="calibre8">A commonly <a id="id877" class="calibre1"/>used alternative to the Euclidean distance (or L2 norm, as it is also known) is the <span class="strong"><strong class="calibre2">cosine distance</strong></span>. This metric measures the cosine of the smallest angle between two vectors. If the vectors are parallel to each other, meaning that their angle is 0, then the cosine distance is 0 as well. If the two vectors are at a right angle to each other, then they have the largest distance according to this metric. The cosine distance is given by:</p><div class="mediaobject"><img src="../images/00199.jpeg" alt="Measuring user similarity" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, the numerator is the dot product between the two vectors and the denominator is the product of the magnitudes (typically computed via the L2 norm) of the two vectors. The cosine distance isn't available as a method in the <code class="email">dist()</code> function of R's base distribution, but we can install the <code class="email">proxy</code> package, which enhances this function with a number of new distance metrics in order to compute the cosine distances for our rating matrix:</p><div class="informalexample"><pre class="programlisting">&gt; library("proxy")
&gt;dist(ratingMatrix, method = 'cosine')
oliverthibaultmariapedroines
thibault 0.28387670                                            
maria    0.12450495 0.23879093                                 
pedro    0.20947046 0.17687385 0.20854178                      
ines     0.02010805 0.22821528 0.06911870 0.20437426           
gertrude 0.22600742 0.21481973 0.19156876 0.12227138 0.22459114</pre></div><p class="calibre8">Suppose <a id="id878" class="calibre1"/>instead that our users rated restaurants on a binary scale. We can convert our rating matrix into a binary rating matrix by considering all ratings above 5 to be positive and assigning them a new score of 1. The remaining <a id="id879" class="calibre1"/>ratings are all converted to a score of 0. For two binary vectors, the <span class="strong"><strong class="calibre2">Jaccard similarity</strong></span> is given by the cardinality of the logical intersection divided by the cardinality of the logical union. The Jaccard distance is then computed as 1 minus this:</p><div class="mediaobject"><img src="../images/00200.jpeg" alt="Measuring user similarity" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In a nutshell, what this is computing is one minus the ratio of the number of positions in which the two vectors both have a positive rating over the total number of positions in which either of the two vectors have a positive rating. Two binary vectors that agree in all their positive positions will be identical and thus have a distance of 0. Using the <code class="email">proxy</code> package, we can show the <code class="email">Jaccard</code> distance for our restaurant patrons as follows:</p><div class="informalexample"><pre class="programlisting">&gt;binaryRatingMatrix&lt;- ratingMatrix&gt; 5
&gt;dist(binaryRatingMatrix, method = 'jaccard')
oliverthibaultmariapedroines
thibault 0.6000000                                        
maria    0.2500000 0.5000000                              
pedro    0.5000000 0.6666667 0.6666667                    
ines     0.0000000 0.6000000 0.2500000 0.5000000          
gertrude 1.0000000 0.7500000 1.0000000 0.8333333 1.0000000</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note47" class="calibre1"/>Note</h3><p class="calibre8">The study of measurement and distance metrics is broad and there are many suitable metrics that have been applied to the recommender system setting. The definitive reference for distance metrics is the <span class="strong"><em class="calibre9">Encyclopedia of Distances</em></span>, <span class="strong"><em class="calibre9">Michel Marie Deza and Elena Deza</em></span>, <span class="strong"><em class="calibre9">Springer</em></span>.</p></div></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Collaborative filtering"><div class="book" id="2JTHG2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch12lvl1sec81" class="calibre1"/>Collaborative filtering</h1></div></div></div><p class="calibre8">Having <a id="id880" class="calibre1"/>covered distances, we are ready to delve into the topic of <span class="strong"><strong class="calibre2">collaborative filtering</strong></span>, which will help us to define a strategy for making recommendations. Collaborative filtering describes an algorithm, or more precisely a family of algorithms, that aims to create recommendations for a test user given only information about the ratings of other users via the rating matrix, as well as any ratings that the test user has already made.</p><p class="calibre8">There are <a id="id881" class="calibre1"/>two very common variants of collaborative <a id="id882" class="calibre1"/>filtering, <span class="strong"><strong class="calibre2">memory-based collaborative filtering</strong></span> and <span class="strong"><strong class="calibre2">model-based collaborative filtering</strong></span>. With memory-based collaborative filtering, the entire history of all the ratings made by all the users is remembered <a id="id883" class="calibre1"/>and must be processed in order to make a recommendation. The prototypical memory-based collaborative filtering method is <span class="strong"><strong class="calibre2">user-based collaborative filtering</strong></span>. Although this approach uses all the ratings available, the downside is that it can be computationally expensive as the entire database is used in order to make rating predictions for our test user.</p><p class="calibre8">The alternative <a id="id884" class="calibre1"/>approach to this is embodied in model-based collaborative filtering. Here, we first create a model of the rating preferences of our users, such as a set of clusters of users who like similar items, and then use the model to generate recommendations. We will study <span class="strong"><strong class="calibre2">item-based collaborative filtering</strong></span>, which is the most well-known, model-based collaborative filtering method.</p></div>

<div class="book" title="Collaborative filtering">
<div class="book" title="User-based collaborative filtering"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch12lvl2sec101" class="calibre1"/>User-based collaborative filtering</h2></div></div></div><p class="calibre8">User-based <a id="id885" class="calibre1"/>collaborative filtering is commonly described as a memory-based or lazy learning approach. Unlike most of the models we have built in this book, which assume that we will fit the data to a particular model and then use that model to make predictions, lazy learning simply uses the training data itself to make predictions directly. We saw an example of lazy learning with k-nearest neighbors in <a class="calibre1" title="Chapter 1. Gearing Up for Predictive Modeling" href="part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7">Chapter 1</a>, <span class="strong"><em class="calibre9">Gearing Up for Predictive Modeling</em></span>. In fact, the premise of the user-based collaborative filtering approach builds directly on the k-nearest neighbors approach.</p><p class="calibre8">However, in user-based collaborative filtering, when we want to make recommendations for a new user, we will first pick a set of similar users using a particular distance metric. Then, we try to infer the ratings that our target user would assign to items that he or she has not yet rated as an average of the ratings made by similar users on those items. We usually refer <a id="id886" class="calibre1"/>to this set of similar users as the user's <span class="strong"><strong class="calibre2">neighborhood</strong></span>. Thus, the idea is that a user will prefer items that their neighborhood prefers.</p><p class="calibre8">Typically, there are two ways to define the user's neighborhood. We can compute a fixed neighborhood by finding the k-nearest neighbors. These are the <span class="strong"><em class="calibre9">k</em></span> users in our database that have the smallest distance between them and our target user.</p><p class="calibre8">Alternatively, we can specify a similarity threshold and pick all the users in our database whose <a id="id887" class="calibre1"/>distance from our target user does not exceed this threshold. This second approach has the advantage that we will be making recommendations by employing users that are as close to our target user as we want, and therefore our confidence in our recommendation can be high. On the other hand, there might be very few users that satisfy our requirement, meaning that we will be relying on the recommendations of these few users. Worse, there might be no users in our database who are sufficiently similar to our target user and we might not be able to actually make a recommendation at all. If we don't mind our method sometimes failing to make a recommendation, for example because we have a backup plan to handle these cases, the second approach might be a good choice.</p><p class="calibre8">Another important consideration to make in a real-world setting is the problem of sparse ratings. In our simple restaurant example, every user had rated every restaurant. This rarely, if ever, happens in a real situation, simply because the number of items is usually too big for a user to rate them all. If we think of e-commerce websites such as Amazon.com, for example, it is easy to imagine that the maximum number of products that any user has rated is still only a small fraction of the overall number of products on sale.</p><p class="calibre8">To compute distance metrics between users in order to determine similarity, we usually incorporate only the items that both users have rated. Consequently, in practice, we often make comparisons between users in a smaller number of dimensions.</p><p class="calibre8">Once we have decided on a distance metric and how to form a neighborhood of users that are similar to our test user, we then use this neighborhood to compute the missing ratings for the test user. The easiest way to do this is to simply compute the average rating for each item in the user neighborhood and report this value. Thus, for test user <span class="strong"><em class="calibre9">t</em></span> and an item <span class="strong"><em class="calibre9">j</em></span>, for which the test user has not yet made a rating, we can predict the test user's rating for that item,<span class="strong"><img src="../images/00201.jpeg" alt="User-based collaborative filtering" class="calibre26"/></span> as follows:</p><div class="mediaobject"><img src="../images/00202.jpeg" alt="User-based collaborative filtering" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This equation expresses the simple idea that the predicted rating of our test user <span class="strong"><em class="calibre9">t</em></span> for item <span class="strong"><em class="calibre9">j</em></span> is just the average of the ratings made by the test user's neighborhood on this item. Suppose we had a new user for our restaurant scenario and this user had already rated a few of the restaurants. Then, imagine that, from those ratings, we discovered that our new user's neighborhood comprised Oliver and Thibault. If we wanted to make a prediction for what rating the test user would make on the restaurant <span class="strong"><em class="calibre9">El Pollo Loco</em></span>, this would be done by averaging the ratings of Oliver and Thibault for this restaurant, which in this case would be the average of 2 and 4, yielding a rating of 3.</p><p class="calibre8">If our objective was to obtain a top-N list of recommendations for our user, we would repeat <a id="id888" class="calibre1"/>this process for all the items in the database, sort them in descending order so that the highest rated items appeared first, and then pick out the top <span class="strong"><em class="calibre9">N</em></span> items from this list. In practice, we would only need to check the items that at least one of the users in the new user's neighborhood has rated in order to simplify this computation.</p><p class="calibre8">We can make some improvements to this very simple approach. A first possible improvement comes from the observation that some users will tend to consistently rate items more strictly or more leniently than other users, and we would like to smooth out this variation. In practice, we often use <span class="strong"><em class="calibre9">Z</em></span>-score normalization, which takes into account the variance of ratings. We can also center each rating made by a user by subtracting that user's average rating across all the items. In the rating matrix, this means subtracting the mean of each row from the elements of the row. Let's apply this last transformation to our restaurant rating matrix and see the results:</p><div class="informalexample"><pre class="programlisting">&gt;centered_rm&lt;- t(apply(ratingMatrix, 1, function(x) x - mean(x)))
&gt;centered_rm
         Berny's La Traviata El Pollo Loco Joey's Pizza
oliver     -4.00       -4.00         -3.00          0.0
thibault   -0.12        3.88         -1.12         -4.1
maria      -3.50       -0.50         -2.50          0.5
pedro      -3.12        0.88          1.88         -3.1
ines       -4.12       -2.12         -3.12         -1.1
gertrude   -3.25        1.75          0.75          2.8
         The Old West Jake and Jill Full Moon Acropolis
oliver           2.00           3.0      4.00      2.00
thibault        -4.12           1.9     -0.12      3.88
maria            3.50           1.5     -2.50      3.50
pedro            0.88          -4.1      2.88      3.88
ines             2.88           3.9      1.88      1.88
gertrude        -1.25          -2.2      0.75      0.75</pre></div><p class="calibre8">Even though both Ines and Gertrude originally rated <code class="email">Berny's</code> with the same rating of 1, the centering operation has Ines rating this restaurant with a lower score than Gertrude. This is because Ines tends to make higher ratings on average than Gertrude, and so the rating of 1 for Ines could be interpreted as a stronger negative rating than Gertrude's.</p><p class="calibre8">Another area of improvement concerns the way in which we incorporate the ratings of our new user's neighborhood to create the final ratings. By treating the ratings of all the neighboring users as equal, we are ignoring the fact that our distance metric may show that certain users in the neighborhood of the new user are more similar to the new user than others.</p><p class="calibre8">As we <a id="id889" class="calibre1"/>have already seen in the example on Jaccard similarity and Jaccard distance, we can often define a similarity metric from a distance metric by inverting it in some way, such as subtracting from 1 or taking the reciprocal. Consequently, for the distance metric of our choice, we can define its corresponding similarity metric and denote it with <span class="strong"><em class="calibre9">sim(u,t)</em></span>. A user similarity metric takes high values for similar users, which are users for whom a distance metric takes low values.</p><p class="calibre8">With this clarification established, we can incorporate the similarity between users <span class="strong"><em class="calibre9">u</em></span> and <span class="strong"><em class="calibre9">t</em></span> in our previous equation by taking a weighted average of the ratings made by the neighboring users of the new user as follows:</p><div class="mediaobject"><img src="../images/00203.jpeg" alt="User-based collaborative filtering" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Other reasons why we might want to incorporate weights in the ratings made by other users include trust. For example, we might trust a user that has been using our restaurant recommendation service for a long time more who a more recent user. Equally, we might also want to consider the total number of items that a user has rated in common with the new user. For example, if a user has only rated two items in common with the new user, then even, if the corresponding ratings made are identical, the evidence that these two users are indeed very similar is limited.</p><p class="calibre8">All in all, the single largest difficulty with user-based collaborative filtering is that making recommendations for a test user requires access to the whole database of users in order to determine the user neighborhood. This is done by performing a similarity computation between the test user and every other user, an expensive process computationally. Next, we'll look at item-based collaborative filtering, which attempts to ameliorate this situation.</p></div></div>

<div class="book" title="Collaborative filtering">
<div class="book" title="Item-based collaborative filtering"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch12lvl2sec102" class="calibre1"/>Item-based collaborative filtering</h2></div></div></div><p class="calibre8">Item-based <a id="id890" class="calibre1"/>collaborative filtering is a model-based approach to collaborative filtering. The central idea underlying this method is that, instead of looking at other users similar to the test user, we will directly recommend items that are similar to the items that have received a high rating by the test user. As we are directly comparing items, instead of first comparing users, to recommend items, we can build up a model to describe the similarity of items and then use the model rather than the entire database to make recommendations.</p><p class="calibre8">The process of building an item-based similarity model involves computing a similarity matrix for all pairs of items in our database. If we have <span class="strong"><em class="calibre9">N</em></span> items, then we will end up with a similarity matrix with <span class="strong"><em class="calibre9">N<sup class="calibre15">2</sup></em></span> elements in total. To reduce the size of our model, we can store a list of the similarity values of the top <span class="strong"><em class="calibre9">k</em></span> most similar items for every item in the database.</p><p class="calibre8">As <span class="strong"><em class="calibre9">k</em></span> will be far smaller than <span class="strong"><em class="calibre9">N</em></span>, we will have a very substantial reduction in the size of the data that we need to keep for our model. For every item in our database, this list of the <span class="strong"><em class="calibre9">k</em></span> most similar items is analogous to the neighborhood of users for the user-based collaborative filtering approach. The same discussion regarding normalizing with respect to the bias and variance of user ratings in user-based collaborative filtering can be applied here. That is, we can compute item-to-item similarities after we normalize our rating matrix.</p><p class="calibre8">This approach is not without its shortcomings. In the memory-based recommender, a new user rating can automatically be incorporated into the recommendation process because that approach uses the entire database (rating matrix). Model-based collaborative filtering requires us to periodically retrain the model to incorporate information from these new ratings. In addition, the fact that the modeling process discards some information from the original rating matrix means that it can sometimes make non-optimal recommendations.</p><p class="calibre8">Despite these drawbacks, the space and time performance of item-based collaborative filtering means that it has been very successfully applied in a number of real-world settings. Model retraining can be done offline and automatically scheduled, and the non-optimality of recommendations can often be tolerated.</p><p class="calibre8">We can devise an analogous equation to what we saw for user-based collaborative filtering that explains how to predict a new rating using the item-based collaborative filtering model. Suppose we want to estimate the rating that our test user, <span class="strong"><em class="calibre9">t</em></span>, would give to an item, <span class="strong"><em class="calibre9">I</em></span>. Suppose also that we have already chosen a similarity function, <span class="strong"><em class="calibre9">sim(i,j)</em></span>, between a pair of items, <span class="strong"><em class="calibre9">i</em></span> and <span class="strong"><em class="calibre9">j</em></span>, and from this we constructed our model. Using the model, we can retrieve the stored item neighborhood for the item in which we are interested, <span class="strong"><em class="calibre9">S(i)</em></span>. To compute the predicted rating that our test user will make on this item, we calculate the weighted sum of the ratings our user has made on items that are similar to it:</p><div class="mediaobject"><img src="../images/00204.jpeg" alt="Item-based collaborative filtering" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">While <a id="id891" class="calibre1"/>this approach won't work if the user hasn't rated any items similar to the item in question, it does not require finding users that have similar preferences to the test user.</p></div></div>
<div class="book" title="Singular value decomposition"><div class="book" id="2KS222-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch12lvl1sec82" class="calibre1"/>Singular value decomposition</h1></div></div></div><p class="calibre8">In a <a id="id892" class="calibre1"/>real-world recommender system, the rating matrix will eventually become very large as more users are added to the system and the list of items being offered grows. As a result, we may want to apply a dimensionality reduction technique to this matrix. Ideally, we would like to retain as much information as possible from the original matrix while doing this. One such method that has applications across a wide range of disciplines uses <span class="strong"><strong class="calibre2">singular value decomposition</strong></span>, or <span class="strong"><strong class="calibre2">SVD</strong></span> as it is commonly abbreviated to.</p><p class="calibre8">SVD is a matrix factorization technique that has a number of useful applications, one of which is dimensionality reduction. It is related to the PCA method of dimensionality reduction that we saw in <a class="calibre1" title="Chapter 1. Gearing Up for Predictive Modeling" href="part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7">Chapter 1</a>, <span class="strong"><em class="calibre9">Gearing Up for Predictive Modeling</em></span>, and many people confuse the two. SVD actually describes just a mathematical method of factorizing matrices. In fact, some implementations of PCA use SVD to compute the principal components.</p><p class="calibre8">Let's begin by looking at how this process works. SVD is a matrix factorization process, so we start with an original matrix representing our data and express this as a product of matrices. In a dimensionality reduction scenario, our input data matrix would be the matrix where the rows are data points and the columns are the features; thus, in R, this would just be a data frame. In our recommender systems scenario, the matrix we use is our rating matrix. Suppose that we call our rating matrix <span class="strong"><em class="calibre9">D</em></span> and we have <span class="strong"><em class="calibre9">m</em></span> users (rows) rating <span class="strong"><em class="calibre9">n</em></span> items (columns). The SVD factorization of this matrix is given by:</p><div class="mediaobject"><img src="../images/00205.jpeg" alt="Singular value decomposition" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In the previous equation, <span class="strong"><em class="calibre9">U</em></span> and <span class="strong"><em class="calibre9">V</em></span> are square matrices and the matrix <span class="strong"><em class="calibre9">Σ</em></span> is a matrix with the same dimensionality as our input matrix, <span class="strong"><em class="calibre9">D</em></span>. In addition, it is a diagonal matrix, meaning that all the elements of the matrix are zero except those on the leading diagonal. These elements are conventionally ordered from largest to smallest and are known as the <span class="strong"><strong class="calibre2">singular values</strong></span> of <a id="id893" class="calibre1"/>the matrix <span class="strong"><em class="calibre9">D</em></span>, giving rise to the name singular value decomposition.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note48" class="calibre1"/>Note</h3><p class="calibre8">Readers familiar with linear algebra will know that the eigenvalues of a matrix are often also described as containing information about the important dimensions of that matrix. It turns out that the eigenvalues of a matrix are related to the singular values through the following relationship—the singular values of a matrix, <span class="strong"><em class="calibre9">D,</em></span> are the same as the square roots of the eigenvalues of the matrix product <span class="strong"><em class="calibre9">D</em></span> × <span class="strong"><em class="calibre9">D<sup class="calibre15">T</sup></em></span>.</p></div><p class="calibre8">We can <a id="id894" class="calibre1"/>easily perform SVD on a matrix in R via the <code class="email">svd()</code> function, which is available with R's <code class="email">base</code> package. Let's see this using our existing <code class="email">ratingMatrix</code>:</p><div class="informalexample"><pre class="programlisting">&gt; options(digits = 2)
&gt; (rm_svd&lt;- svd(ratingMatrix))
$d
[1] 35.6 10.6  7.5  5.7  4.7  1.3
$u
      [,1]  [,2]   [,3]   [,4]   [,5]   [,6]
[1,] -0.44  0.48 -0.043 -0.401  0.315  0.564
[2,] -0.41 -0.56  0.703 -0.061  0.114  0.099
[3,] -0.38  0.24  0.062  0.689 -0.494  0.273
[4,] -0.43 -0.40 -0.521 -0.387 -0.483 -0.033
[5,] -0.44  0.42  0.170 -0.108 -0.003 -0.764
[6,] -0.33 -0.26 -0.447  0.447  0.641 -0.114
$v
      [,1]   [,2]  [,3]    [,4]   [,5]   [,6]
[1,] -0.13 -0.255  0.30 -0.0790  0.013  0.301
[2,] -0.33 -0.591  0.16  0.3234  0.065 -0.486
[3,] -0.25 -0.382 -0.36 -0.0625 -0.017 -0.200
[4,] -0.27  0.199 -0.36  0.5796  0.578  0.284
[5,] -0.38  0.460 -0.30  0.1412 -0.556 -0.325
[6,] -0.39  0.401  0.68  0.0073  0.239 -0.226
[7,] -0.42  0.044 -0.26 -0.7270  0.369 -0.047
[8,] -0.52 -0.161  0.11  0.0279 -0.398  0.628</pre></div><p class="calibre8">The singular values are returned as a vector <code class="email">d</code>, from which we can easily construct the diagonal matrix using the <code class="email">diag()</code> function. To verify that this factorization really is the one that we expected, we can reconstruct our original rating matrix by simply multiplying the matrix factors that we have obtained:</p><div class="informalexample"><pre class="programlisting">&gt;reconstructed_rm&lt;- rm_svd$u %*% diag(rm_svd$d) %*% t(rm_svd$v)
&gt;reconstructed_rm
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    1    2    5    7    8    9    7
[2,]    5    9    4    1    1    7    5    9
[3,]    1    4    2    5    8    6    2    8
[4,]    2    6    7    2    6    1    8    9
[5,]    1    3    2    4    8    9    7    7
[6,]    1    6    5    7    3    2    5    5</pre></div><p class="calibre8">One thing to note here is that, if we were to attempt a direct equality check with our original matrix, we would most likely fail. This is due to rounding errors that are introduced when we store the factorized matrices. We can check that our two matrices are nearly equal using the <code class="email">all.equal()</code> function:</p><div class="informalexample"><pre class="programlisting">&gt;all.equal(ratingMatrix, reconstructed_rm, tolerance = 0.000001,
check.attributes = F)
[1] TRUE</pre></div><p class="calibre8">The reader <a id="id895" class="calibre1"/>is encouraged to decrease the size of the tolerance and note that, after several decimal points, the equality check fails. Though the two matrices are not exactly equal, the difference is so small that this will not impact us in any significant way. Now, once we have this factorization, let's investigate our singular values. The first singular value of 35.6 is much larger than the smallest singular value of 1.3.</p><p class="calibre8">We can perform dimensionality reduction by keeping the top singular values and throwing the rest away. To do this, we'd like to know how many singular values we should keep and <a id="id896" class="calibre1"/>how many we should discard. One approach to this problem is to compute the square of the singular values, which can be thought of as the vector of <span class="strong"><strong class="calibre2">matrix energy</strong></span>, and then pick the top singular values that preserve at least 90 % of the overall energy of the original matrix. This is easy to do with R as we can use the <code class="email">cumsum()</code> function for creating a cumulative sum and the singular values are already ordered from largest to smallest:</p><div class="informalexample"><pre class="programlisting">&gt; energy &lt;- rm_svd$d ^ 2
&gt;cumsum(energy) / sum(energy)
[1] 0.85 0.92 0.96 0.98 1.00 1.00</pre></div><p class="calibre8">Keeping the first two singular values will retain 92 % of the energy of our original matrix. Using just two values, we can reconstruct our rating matrix and observe the differences:</p><div class="informalexample"><pre class="programlisting">&gt;d92&lt;- c(rm_svd$d[1:2], rep(0, length(rm_svd$d) - 2))
&gt;reconstructed92_rm&lt;- rm_svd$u %*% diag(d92) %*% t(rm_svd$v)
&gt;reconstructed92_rm
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,] 0.68  2.0  1.9  5.1  8.3  8.0  6.7  7.2
[2,] 3.37  8.3  5.9  2.7  2.9  3.3  5.9  8.6
[3,] 1.10  3.0  2.4  4.1  6.4  6.3  5.9  6.7
[4,] 3.02  7.5  5.4  3.2  3.9  4.2  6.2  8.6
[5,] 0.87  2.5  2.2  5.1  8.1  7.9  6.8  7.5
[6,] 2.20  5.5  4.0  2.6  3.3  3.5  4.9  6.6</pre></div><p class="calibre8">As we <a id="id897" class="calibre1"/>can see, there are a few differences in the absolute values, but most of the patterns of different users have been retained to a large extent. Discarding singular values effectively introduces zeros in the leading diagonal of matrix <span class="strong"><em class="calibre9">D</em></span> in the factorization, so that this matrix ends up with entire rows and columns that only contain zeros. Consequently, we can truncate not only this matrix, but rows from the matrices <span class="strong"><em class="calibre9">U</em></span> and <span class="strong"><em class="calibre9">V</em></span>.</p><p class="calibre8">Thus, we reduce the size of the data that we have to store.</p></div>
<div class="book" title="Predicting recommendations for movies and jokes" id="2LQIK1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch12lvl1sec83" class="calibre1"/>Predicting recommendations for movies and jokes</h1></div></div></div><p class="calibre8">In this <a id="id898" class="calibre1"/>chapter, we will focus on building recommender systems using two different datasets. To do this, we shall use the <code class="email">recommenderlab</code> package. This provides us with not only the algorithms to perform the recommendations, but also with the data structures to store the sparse rating matrices efficiently. The first datasets we will use contains anonymous user reviews for jokes from the <span class="strong"><em class="calibre9">Jester Online Joke recommender system</em></span>.</p><p class="calibre8">The joke ratings fall on a continuous scale (-10 to +10). A number of datasets collected from the <a id="id899" class="calibre1"/>Jester system can be found at <a class="calibre1" href="http://eigentaste.berkeley.edu/dataset/">http://eigentaste.berkeley.edu/dataset/</a>. We will use the datasets labeled on the website as <span class="strong"><em class="calibre9">Dataset 2+</em></span>. This datasets contains ratings made by 50,692 users on 150 jokes. As is typical with a real-world application, the rating matrix is very sparse in that each user rated only a fraction of all the jokes; the minimum number of ratings made by a user is 8. We will refer to this data set as the jester datasets.</p><p class="calibre8">The second datasets can be found at <a class="calibre1" href="http://grouplens.org/datasets/movielens/">http://grouplens.org/datasets/movielens/</a>. This website contains data on user ratings for movies that were made on the <span class="strong"><em class="calibre9">MovieLens</em></span> website at <a class="calibre1" href="http://movielens.org">http://movielens.org</a>. Again, there is more than one datasets on the website; we will use the one labeled <span class="strong"><em class="calibre9">MovieLens1M</em></span>. This contains ratings on a five-point scale (1-5) made by 6,040 users on 3,706 movies. The minimum number of movie ratings per user is 20. We will refer to this datasets as the movie dataset.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip20" class="calibre1"/>Tip</h3><p class="calibre8">These two datasets are actually very well-known open source datasets, to the point that the <code class="email">recommenderlab</code> package itself includes smaller versions of them as part of the package. Readers who would like to skip the process of loading and preprocessing the data, or who would like to run the examples that follow on smaller datasets due to computational constraints, are encouraged to try them out using <code class="email">data(Jester5k)</code> or <code class="email">data(MovieLense)</code>.</p></div></div>
<div class="book" title="Loading and pre-processing the data" id="2MP361-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch12lvl1sec84" class="calibre1"/>Loading and pre-processing the data</h1></div></div></div><p class="calibre8">Our first <a id="id900" class="calibre1"/>goal in building our recommender systems is to load <a id="id901" class="calibre1"/>the data in R, preprocess it, and convert it into a rating matrix. More precisely, in each case, we will be creating a <code class="email">realRatingMatrix</code> object, which is the specific data structure that the <code class="email">recommenderlab</code> package uses to store numerical ratings. We will start with the jester datasets. If we download and unzip the archive from the website, we'll see that the file <code class="email">jesterfinal151cols.csv</code> contains the ratings. More specifically, each row in this file corresponds to the ratings made by a particular user, and each column corresponds to a particular joke.</p><p class="calibre8">The columns are comma-separated and there is no header row. In fact, the format is almost already a rating matrix, were it not for the fact that the first column is a special column and contains the total number of ratings made by a particular user. We will load this data into a data table using the function <code class="email">fread()</code>, which is a fast implementation of <code class="email">read.table()</code> and efficiently loads a data file into a data table. We'll then drop the first column efficiently using the <code class="email">data.table</code> syntax:</p><div class="informalexample"><pre class="programlisting">&gt; library(data.table)
&gt; jester &lt;- fread("jesterfinal151cols.csv", sep = ",", header = F)
&gt; jester[, V1 := NULL]</pre></div><p class="calibre8">The last line used the assignment operator <code class="email">:=</code> to set the first column, <code class="email">V1</code>, to <code class="email">NULL</code>, which is how we drop a column on a data table. We now have one final preprocessing step left to do on our data table, <code class="email">jester</code>, before we are ready to convert it to a <code class="email">realRatingMatrix</code> object. Specifically, we will convert this into a matrix and replace all occurrences of the rating of 99 with <code class="email">NA</code>, as 99 was the special rating used to represent missing values:</p><div class="informalexample"><pre class="programlisting">&gt;jester_m&lt;- as.matrix(jester) 
&gt;jester_m&lt;- ifelse(jester_m == 99, NA, jester_m)
&gt; library(recommenderlab)
&gt;jester_rrm&lt;- as(jester_m, "realRatingMatrix")</pre></div><p class="calibre8">Depending on the computational resources of the computer available to us (most notably, the available memory), we may want to try to process a single datasets in its entirety instead of loading both datasets at once. Here, we have chosen to work with the two datasets in parallel in order to showcase the main steps in the analysis and highlight any differences or particularities of an individual datasets with respect to a particular step.</p><p class="calibre8">Let's move on to the MovieLens data. Downloading the MovieLens1M archive and unzipping reveals three main data files. The <code class="email">users.dat</code> file contains background information about the users, such as age and gender. The <code class="email">movies.dat</code> data file, in turn, contains information about the movies being rated, namely the title and a list of genres (for example, <span class="strong"><em class="calibre9">comedy</em></span>) to which the movie belongs.</p><p class="calibre8">We are <a id="id902" class="calibre1"/>mainly interested in the <code class="email">ratings.dat</code> file, which contains the ratings themselves. Unlike the raw jester data, here each line corresponds to a single <a id="id903" class="calibre1"/>rating made by a user. The line format contains the User ID, Movie ID, rating, and timestamp, all separated by two colon characters, <code class="email">::</code>. Unfortunately, <code class="email">fread()</code> requires a separator with a single character, so we will specify a single colon. The double-colon separator in the raw data results in us creating extra columns with <code class="email">NA</code> values that we will have to remove, as well as the final column that contains the timestamp:</p><div class="informalexample"><pre class="programlisting">&gt; movies &lt;- fread("ratings.dat", sep = ":", header = F)
&gt; movies[, c("V2", "V4", "V6", "V7") := NULL]
&gt; head(movies)
V1V3V5
1:  1 1193  5
2:  1  661  3
3:  1  914  3
4:  1 3408  4
5:  1 2355  5
6:  1 1197  3</pre></div><p class="calibre8">As we can see, we are now left with three columns, where the first is the <code class="email">UserID</code>, the second is the <code class="email">MovieID</code>, and the last is the rating. We will now aggregate all the ratings made by a user in order to form an object that can be interpreted as or converted to, a rating matrix. We should aggregate the data in a way that minimizes memory usage. We will do this by building a sparse matrix using the <code class="email">sparseMatrix()</code> command from the <code class="email">Matrix</code> package.</p><p class="calibre8">This package is loaded automatically when we use the <code class="email">recommenderlab</code> package, as, it is one of its dependencies. To build a sparse matrix using this function, we can simply specify a vector of row coordinates, a vector of matching column coordinates, and a vector with the nonzero values that fill up the sparse matrix. Remember, as our matrix is sparse, all we need are the locations and values for entries that are nonzero.</p><p class="calibre8">Right now, it is slightly inconvenient that we cannot directly interpret the User IDs and Movie IDs as coordinates. This is because, if we have a user with a User ID value of 1 and a user with a User ID value of 3, R will automatically create a user with a User ID value of 2 and create an empty row, even though that user does not actually exist in the training data. The situation is similar for columns. Consequently, we must first make factors out of our <code class="email">UserID</code> and <code class="email">MovieID</code> columns before proceeding to create our rating matrix as described earlier. Here is the code for building our rating matrix for the MovieLens data:</p><div class="informalexample"><pre class="programlisting">&gt;userid_factor&lt;- as.factor(movies[, V1])
&gt;movieid_factor&lt;- as.factor(movies[, V3])
&gt;movies_sm&lt;- sparseMatrix(i = as.numeric(userid_factor), j = 
as.numeric(movieid_factor), x = as.numeric(movies[,V5]))
&gt;movies_rrm&lt;- new("realRatingMatrix", data = movies_sm)
&gt;colnames(movies_rrm) &lt;- levels(movieid_factor)
&gt;rownames(movies_rrm) &lt;- levels(userid_factor)
&gt; dim(movies_rrm)
[1] 6040 3706</pre></div><p class="calibre8">It is a <a id="id904" class="calibre1"/>good exercise to check that the dimensions of the result <a id="id905" class="calibre1"/>correspond to our expectations on the number of users and movies respectively.</p></div>

<div id="page" style="height:0pt"/><div class="book" title="Exploring the data"><div class="book" id="2NNJO2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch12lvl1sec85" class="calibre1"/>Exploring the data</h1></div></div></div><p class="calibre8">Before <a id="id906" class="calibre1"/>building and evaluating recommender systems using the two datasets we have loaded, it is a good idea to get a feel for the data. For one thing, we can make use of the <code class="email">getRatings()</code> function to retrieve the ratings from a rating matrix. This is useful in order to construct a histogram of item ratings. Additionally, we can also normalize the ratings with respect to each user, as we discussed earlier. The following code snippet shows how we can compute ratings and normalized ratings for the jester data. We can then do the same for the MovieLens data and produce histograms for the ratings:</p><div class="informalexample"><pre class="programlisting">&gt;jester_ratings&lt;- getRatings(jester_rrm)
&gt;jester_normalized_ratings&lt;- getRatings(normalize(jester_rrm, 
                                          method = "Z-score"))</pre></div><p class="calibre8">The following plot shows the different histograms:</p><div class="mediaobject"><img src="../images/00206.jpeg" alt="Exploring the data" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In the Jester data, we can see that ratings above zero are more prominent than ratings below zero, and the most common rating is 10, the maximum rating. The normalized ratings create <a id="id907" class="calibre1"/>a more symmetric distribution centered on zero. For the MovieLens data with the 5-point rating scale, 4 is the most prominent rating and higher ratings are far more common than low ratings.</p><p class="calibre8">We can also look for the distribution of the number of items rated per user and the average rating per item by looking at the row counts and the column means of the rating matrix respectively. Again, the following code snippet shows how to compute these for the jester data, and we follow up with histograms showing the results for both datasets:</p><div class="informalexample"><pre class="programlisting">&gt;jester_items_rated_per_user&lt;- rowCounts(jester_rrm)
&gt;jester_average_item_rating_per_item&lt;- colMeans(jester_rrm)</pre></div><p class="calibre8">The histograms of the Jester and MovieLens data are shown here:</p><div class="mediaobject"><img src="../images/00207.jpeg" alt="Exploring the data" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Both datasets show a curve in the average ratings per user that looks like a power curve. Most of the users have rated very few items, but a small number of very committed users have actually rated a large number of items. In the Jester case, some have rated the maximum number of jokes in the datasets. This is an exception and only occurs because the number of items (jokes) in this datasets is relatively small. The distribution of the average joke rating is between -3 and 4, but for movies we see the whole range of the spectrum, indicating that some users have rated all the movies they considered completely awful or totally great. We can find the average of these distributions in order to determine the average number of items rated per user and the average rating of each item.</p><p class="calibre8">Note that we need to remove <code class="email">NA</code> values from consideration in the Jester datasets, as some columns may not have ratings in them:</p><div class="informalexample"><pre class="programlisting">&gt; (jester_avg_items_rated_per_user&lt;- mean(rowCounts(jester_rrm)))
[1] 34.10493
&gt; (jester_avg_item_rating&lt;- mean(colMeans(jester_rrm), na.rm = T))
[1] 1.633048
&gt; (movies_avg_items_rated_per_user&lt;- mean(rowCounts(movies_rrm)))
[1] 165.5975
&gt; (movies_avg_item_rating&lt;- mean(colMeans(movies_rrm)))
[1] 3.238892 </pre></div></div>

<div class="book" title="Exploring the data">
<div class="book" title="Evaluating binary top-N recommendations"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch12lvl2sec103" class="calibre1"/>Evaluating binary top-N recommendations</h2></div></div></div><p class="calibre8">We now <a id="id908" class="calibre1"/>have some sense of what our data looks like for both datasets so we can start building some models. We will begin by looking at the problem of making top-N recommendations for a binary recommender system, which is simpler to do than when we have more granular data for ratings. Recall that a top-N recommendation is nothing but a list of <span class="strong"><em class="calibre9">N</em></span> recommendations that are more likely to interest a user. To do this, we will use the jester datasets and create a binary version of our rating matrix. We'll call any rating that is 5 or above a positive rating. As this may result in some users having no positive ratings, we'll also prune the rating matrix and keep only users with at least 10 positive ratings under this scheme:</p><div class="informalexample"><pre class="programlisting">&gt;jester_bn&lt;- binarize(jester_rrm, minRating = 5)
&gt;jester_bn&lt;- jester_bn[rowCounts(jester_bn) &gt; 1]
&gt; dim(jester_bn)
[1] 13789   150</pre></div><p class="calibre8">One of the advantages of the <code class="email">recommenderlab</code> package is that it makes it very easy for us to compare results from several algorithms. The process of training and evaluating multiple algorithms for top-N recommendations begins by creating a list containing the definitions of the algorithms that we want to use. Each element in the list is given a name of our choice but must itself be a list containing a set of parameters for configuring a known algorithm. Concretely, the <code class="email">name</code> parameter of this inner parameter list must be one that the <code class="email">recommenderlab</code> package recognizes. It is possible to create and register one's own algorithm with this package, but our focus will be on existing implementations that more than suffice for our intents and purposes:</p><div class="informalexample"><pre class="programlisting">&gt; algorithms &lt;- list(
"Random" = list(name = "RANDOM", param = NULL),
"Popular" = list(name = "POPULAR", param = NULL),
"UserBasedCF_COS" = list(name = "UBCF", 
param = list(method = "Cosine", nn = 50)),
"UserBasedCF_JAC" = list(name = "UBCF", 
param = list(method = "Jaccard", nn = 50))
 )</pre></div><p class="calibre8">The RANDOM algorithm is a baseline algorithm that makes recommendations randomly. The POPULAR algorithm is another baseline algorithm that can sometimes be tough to beat. This proposes items in descending order of global popularity, so that for a top-1 recommendation, it will recommend the item with the highest average rating in the datasets. We have chosen to try out two variants of user-based collaborative filtering for this example. The first one uses the cosine distance and specifies 50 as the number of nearest neighbors to use. The second one is identical but uses the Jaccard distance instead.</p><p class="calibre8">Next, we define an evaluation scheme via the function <code class="email">evaluationScheme()</code>. This function records how we will split our data into training and test sets, the number of ratings we will <a id="id909" class="calibre1"/>take as given from our test users via the <code class="email">given</code> parameter, and how many runs we want to execute. We will do a straight 80-20 split for our training and test set, consider 10 ratings from our test users as known ratings, and evaluate over a single run:</p><div class="informalexample"><pre class="programlisting">&gt;jester_split_scheme&lt;- evaluationScheme(jester_bn, method = 
"split", train = 0.8, given = 10, k = 1)</pre></div><p class="calibre8">Note that the <code class="email">given</code> parameter must be at least as large as the smallest number of items rated by a user in our datasets. We previously filtered the datasets to ensure we have 10 items per user, so we are covered in our case. Finally, we will evaluate our list of algorithms in turn with our evaluation scheme using the <code class="email">evaluate()</code> function. Aside from an evaluation scheme and a list of algorithms, we will also specify the range of <span class="strong"><em class="calibre9">N</em></span> values to use when making top-N recommendations via the <code class="email">n</code> parameter. We will do this for values 1 through 20:</p><div class="informalexample"><pre class="programlisting">&gt;jester_split_eval&lt;- evaluate(jester_split_scheme, algorithms, 
                                n = 1 : 20)
RANDOM run 
  1  [0.015sec/1.87sec] 
POPULAR run 
  1  [0.006sec/12.631sec] 
UBCF run 
  1  [0.001sec/36.862sec] 
UBCF run 
  1  [0.002sec/36.342sec]</pre></div><p class="calibre8">We now have a list of four objects that represent the evaluation results of each algorithm on our data. We can get important measures such as precision by looking at the confusion matrices. Note that, as we have run this experiment for the top-N recommendations, where <span class="strong"><em class="calibre9">N</em></span> is in the range 1-20, we expect to have 20 such confusion matrices for each algorithm. </p><p class="calibre8">The function <code class="email">getConfusionMatrix()</code>, when applied to one of these objects, can be used to retrieve the folded confusion matrices in tabular format so that each row represents the confusion matrix for a particular value of <span class="strong"><em class="calibre9">N</em></span>:</p><div class="informalexample"><pre class="programlisting">&gt; options(digits = 4)
&gt;getConfusionMatrix(jester_split_eval[[4]])
[[1]]
TP      FP    FN    TN precision  recall     TPRFPR
1  0.5181  0.4819 18.47 120.5    0.5181 0.06272 0.06272 0.003867
2  1.0261  0.9739 17.96 120.0    0.5131 0.12042 0.12042 0.007790
3  1.4953  1.5047 17.49 119.5    0.4984 0.16470 0.16470 0.012011
4  1.9307  2.0693 17.06 118.9    0.4827 0.20616 0.20616 0.016547
5  2.3575  2.6425 16.63 118.4    0.4715 0.24215 0.24215 0.021118
6  2.7687  3.2313 16.22 117.8    0.4614 0.27509 0.27509 0.025791
7  3.1530  3.8470 15.83 117.2    0.4504 0.30508 0.30508 0.030709
8  3.5221  4.4779 15.46 116.5    0.4403 0.33216 0.33216 0.035735
9  3.8999  5.1001 15.09 115.9    0.4333 0.36069 0.36069 0.040723
10 4.2542  5.7458 14.73 115.3    0.4254 0.38723 0.38723 0.045890
11 4.6037  6.3963 14.38 114.6    0.4185 0.40927 0.40927 0.051036
12 4.9409  7.0591 14.04 114.0    0.4117 0.43368 0.43368 0.056345
13 5.2534  7.7466 13.73 113.3    0.4041 0.45345 0.45345 0.061856
14 5.5638  8.4362 13.42 112.6    0.3974 0.47248 0.47248 0.067360
15 5.8499  9.1501 13.14 111.9    0.3900 0.48907 0.48907 0.073066
16 6.1298  9.8702 12.86 111.1    0.3831 0.50604 0.50604 0.078836
17 6.4090 10.5910 12.58 110.4    0.3770 0.52151 0.52151 0.084592
18 6.6835 11.3165 12.30 109.7    0.3713 0.53664 0.53664 0.090384
19 6.9565 12.0435 12.03 109.0    0.3661 0.55187 0.55187 0.096198
20 7.2165 12.7835 11.77 108.2    0.3608 0.56594 0.56594 0.102095</pre></div><p class="calibre8">To visualize <a id="id910" class="calibre1"/>this data and compare our algorithms, we can try plotting the results directly using the <code class="email">plot()</code> function. For our <a id="id911" class="calibre1"/>evaluation results, the default is a plot of the <span class="strong"><strong class="calibre2">true positive rate</strong></span> (<span class="strong"><strong class="calibre2">TPR</strong></span>) versus the <span class="strong"><strong class="calibre2">false positive rate</strong></span>(<span class="strong"><strong class="calibre2">FPR</strong></span>). This is simply the <a id="id912" class="calibre1"/>ROC curve, as we know from <a class="calibre1" title="Chapter 4. Generalized Linear Models" href="part0035_split_000.html#11C3M1-c6198d576bbb4f42b630392bd61137d7">Chapter 4</a>, <span class="strong"><em class="calibre9">Neural Networks</em></span>.</p><div class="informalexample"><pre class="programlisting">&gt; plot(jester_split_eval, annotate = 2, legend = "topright")
&gt; title(main = "TPR vs FPR For Binary Jester Data")</pre></div><p class="calibre8">Here is the ROC curve for the binary Jester data:</p><div class="mediaobject"><img src="../images/00208.jpeg" alt="Evaluating binary top-N recommendations" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The graph <a id="id913" class="calibre1"/>shows that the user-based collaborative filtering algorithms perform better than the two baseline algorithms, but there is very little to separate these two, with the cosine distance marginally outperforming the Jaccard distance. We can complement this view of our results by also plotting a precision recall curve:</p><div class="informalexample"><pre class="programlisting">&gt; plot(jester_split_eval, "prec/rec", annotate = 2, 
       legend = "bottomright")
&gt; title(main = "Precision versus Recall Binary Jester Data")</pre></div><p class="calibre8">The following is the precision recall curve for the binary Jester data:</p><div class="mediaobject"><img src="../images/00209.jpeg" alt="Evaluating binary top-N recommendations" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The precision recall curve paints a similar picture, with the user-based collaborative filtering algorithm that uses the cosine distance coming out as the winner. Note that the trade-off <a id="id914" class="calibre1"/>between precision and recall surfaces in a top-N recommender system via the number of recommendations that the system makes. The way our evaluation scheme works is that we treat users in the test data as new users in the system that just contributed a certain number of ratings. We hold out as many ratings as the <code class="email">given</code> parameter allows. Then, we apply our model to see if the ratings we suggest will agree with the ratings that remain. We order our suggestions in descending order of confidence so that in a top-1 recommendation system, we will suggest the item we believe has the best chance of interesting the user. Increasing <span class="strong"><em class="calibre9">N,</em></span> therefore, is like casting a wider net. We will be less precise in our suggestions but are more likely to find something the user will like.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note49" class="calibre1"/>Note</h3><p class="calibre8">An excellent and freely available resource for recommendation systems is <span class="strong"><em class="calibre9">Chapter 9</em></span> from the online textbook <span class="strong"><em class="calibre9">Mining of Massive Datasets</em></span> by <span class="strong"><em class="calibre9">Jure Leskovec</em></span>, <span class="strong"><em class="calibre9">Anand Rajaraman</em></span>, and <span class="strong"><em class="calibre9">Jeffrey David Ullman</em></span>. The website is <a class="calibre1" href="http://www.mmds.org/">http://www.mmds.org/</a>.</p></div></div></div>

<div class="book" title="Exploring the data">
<div class="book" title="Evaluating non-binary top-N recommendations"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch12lvl2sec104" class="calibre1"/>Evaluating non-binary top-N recommendations</h2></div></div></div><p class="calibre8">In this <a id="id915" class="calibre1"/>section, we will use the movies dataset to see how we perform in the non-binary scenario. First, we will define our algorithms as before:</p><div class="informalexample"><pre class="programlisting">&gt;normalized_algorithms&lt;- list(
"Random" = list(name = "RANDOM", param = list(normalize =  
"Z-score")),
"Popular" = list(name = "POPULAR", param = list(normalize = 
"Z-score")),
"UserBasedCF" = list(name = "UBCF", param = list(normalize = 
"Z-score", method = "Cosine", nn = 50)),
"ItemBasedCF" = list(name = "IBCF", param = list(normalize = 
"Z-score")),
"SVD" = list(name = "SVD", param = list(categories = 30, 
                      normalize = "Z-score", treat_na = "median"))
 )</pre></div><p class="calibre8">This time, our algorithms will work with normalized ratings by specifying the <code class="email">normalize</code> parameter. We will only be using the cosine distance for user-based collaborative filtering as the Jaccard distance only applies in the binary setting. Furthermore, we will also try out item-based collaborative filtering as well as SVD-based recommendations. Instead of directly splitting our data, we demonstrate how we can perform ten-fold cross-validation by modifying our evaluation scheme. We will continue to investigate making top-N recommendations in the range of 1 to 20. Evaluating a moderately-sized dataset with five algorithms using 10-fold cross-validation means that we can expect this process to take quite a long time to finish, depending on the computing power we have at our disposal:</p><div class="informalexample"><pre class="programlisting">&gt;movies_cross_scheme&lt;- evaluationScheme(movies_rrm, method = 
"cross-validation", k = 10, given = 10, goodRating = 4)
&gt;movies_cross_eval&lt;- evaluate(movies_cross_scheme, 
normalized_algorithms, n = 1 : 20)</pre></div><p class="calibre8">To conserve space, we have truncated the output that shows us the amount of time spent running each iteration for the different algorithms. Note that the most expensive algorithm during training is the item-based collaborative filtering algorithm, as this is building a model and not just performing lazy learning. Once the process terminates, we can plot the results in the same way as we did for our binarized Jester dataset to compare the performance of our algorithms:</p><div class="informalexample"><pre class="programlisting">&gt; plot(movies_cross_eval, annotate = 4, legend = "topright")
&gt; title(main = "TPR versus FPR For Movielens Data")</pre></div><p class="calibre8">Here is the ROC curve for the MovieLens data:</p><div class="mediaobject"><img src="../images/00210.jpeg" alt="Evaluating non-binary top-N recommendations" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As we <a id="id916" class="calibre1"/>can see, user-based collaborative filtering is the clear winner here. SVD performs in a similar manner to the POPULAR baseline, though the latter starts to become better when <span class="strong"><em class="calibre9">N</em></span> is high. Finally, we see item-based collaborative filtering performing far worse than these, outperforming only the random baseline. What is clear from these experiments is that tuning recommendation systems can often be a very time-consuming, resource-intensive endeavor.</p><p class="calibre8">All the algorithms that we specified can be tuned in various ways and we have explored a number of parameters, from the size of the neighborhood to the similarity metric, that will influence the results. In addition, we've seen that, even for the top-N scenario alone, there are several ways that we can evaluate our recommendation system; thus, if we want to try out a number of these for comparison, we will again need to spend more time on model training.</p><p class="calibre8">The reader is encouraged to repeat these experiments using different parameters and evaluation schemes in order to get a feel for the process of designing and training recommendation systems. In addition, by visiting the websites of our two datasets, the reader can find additional links to similar datasets commonly used for learning about recommendation systems, such as the book-crossing datasets.</p><p class="calibre8">For <a id="id917" class="calibre1"/>completeness, we will plot the precision recall curve for the MovieLens data:</p><div class="informalexample"><pre class="programlisting">&gt; plot(movies_split_eval, "prec/rec", annotate = 3, 
       legend = "bottomright")
&gt; title(main = "Precision versus Recall For Movielens Data")</pre></div><p class="calibre8">Here is the precision recall curve for the MovieLens data:</p><div class="mediaobject"><img src="../images/00211.jpeg" alt="Evaluating non-binary top-N recommendations" class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div class="book" title="Exploring the data">
<div class="book" title="Evaluating individual predictions"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch12lvl2sec105" class="calibre1"/>Evaluating individual predictions</h2></div></div></div><p class="calibre8">Another <a id="id918" class="calibre1"/>way to evaluate a recommendation system is to ask it to predict the specific values of a portion of the known ratings made <a id="id919" class="calibre1"/>by a set of test users using the remainder of their ratings. In this way, we can measure accuracy by taking average distance measures <a id="id920" class="calibre1"/>over the predicted ratings. These include the <span class="strong"><strong class="calibre2">mean squared error</strong></span> (<span class="strong"><strong class="calibre2">MSE</strong></span>) and the <span class="strong"><strong class="calibre2">Root Mean Square Error</strong></span> (<span class="strong"><strong class="calibre2">RMSE</strong></span>), which we have seen before, and the <span class="strong"><strong class="calibre2">mean average error</strong></span>(<span class="strong"><strong class="calibre2">MAE</strong></span>), which is just the average of the absolute <a id="id921" class="calibre1"/>errors. We will do this for the regular (unbinarized) Jester datasets. </p><p class="calibre8">We begin as before by defining an evaluation scheme:</p><div class="informalexample"><pre class="programlisting">&gt;jester_split_scheme&lt;- evaluationScheme(jester_rrm, method = 
"split", train = 0.8, given = 5, goodRating = 5)</pre></div><p class="calibre8">Next, we will define individual user- and item-based collaborative filtering recommenders using the <code class="email">Recommender()</code> and <code class="email">getData()</code> functions. The logic behind these is that the <code class="email">getData()</code> function will extract the ratings set aside for training by the evaluation scheme and the <code class="email">Recommender()</code> function will use the data to train a model:</p><div class="informalexample"><pre class="programlisting">&gt;jester_ubcf_srec&lt;- Recommender(getData(jester_split_scheme, 
"train"), "UBCF")
&gt;jester_ibcf_srec&lt;- Recommender(getData(jester_split_scheme, 
"train"), "IBCF")</pre></div><p class="calibre8">We can then use these models to predict those ratings that were classified as known (there are as many of these as the <code class="email">given</code> parameter specifies) in our test data:</p><div class="informalexample"><pre class="programlisting">&gt;jester_ubcf_known&lt;- predict(jester_ubcf_srec, 
getData(jester_split_scheme, "known"), type="ratings")
&gt;jester_ibcf_known&lt;- predict(jester_ibcf_srec, 
getData(jester_split_scheme, "known"), type="ratings") </pre></div><p class="calibre8">Finally, we can use the known ratings to compute prediction accuracy on the ratings kept for testing:</p><div class="informalexample"><pre class="programlisting">&gt; (jester_ubcf_acc&lt;- calcPredictionAccuracy(jester_ubcf_known, 
getData(jester_split_scheme, "unknown")))
RMSEMSE      MAE 
 4.70765 22.16197  3.54130 
&gt; (jester_ibcf_acc&lt;- calcPredictionAccuracy(jester_ibcf_known, 
getData(jester_split_scheme, "unknown")))
RMSEMSE       MAE 
 5.012211 25.122256  3.518815</pre></div><p class="calibre8">We can see that the performance of the two algorithms is fairly close. User-based collaborative filtering performs better when we penalize larger errors (via the RMSE and MSE) through squaring. From the perspective of the mean average error, item-based collaborative filtering is marginally better.</p><p class="calibre8">Consequently, in this case, we might make our decision as to which type of recommendation system to use on the basis of the error behavior that more closely matches our business <a id="id922" class="calibre1"/>needs. In this section, we used the default parameter values for the two algorithms, but by using the <code class="email">parameter</code> parameter in the <code class="email">Recommender()</code> function, we can play around with different configurations as we did before. This is left as an exercise for the reader.</p></div></div>
<div class="book" title="Other approaches to recommendation systems" id="2OM4A1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch12lvl1sec86" class="calibre1"/>Other approaches to recommendation systems</h1></div></div></div><p class="calibre8">In this <a id="id923" class="calibre1"/>chapter, we concentrated our efforts on building recommendation systems by following the collaborative filtering paradigm. This is a very popular approach due to its many advantages. By essentially mimicking word-of-mouth recommendations, it requires virtually no knowledge about the items being recommended nor any background about the users in question.</p><p class="calibre8">Moreover, collaborative filtering systems incorporate new ratings as they arise, either through a memory approach, or via the regular retraining of a model-based approach. Thus, they naturally become better for their users over time as they learn more information and adapt to changing preferences. On the other hand, they are not without their disadvantages, not the least of which is the fact that they will not take into account any information about the items and their content even when it is available.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Content-based recommendation systems</strong></span> try to suggest items to users that are similar to those <a id="id924" class="calibre1"/>that users like on the basis of content. The key premise behind this idea is that, if it is known that a user happens to like novels by <span class="strong"><em class="calibre9">George R. R. Martin</em></span>, a fantasy and science fiction author, it makes sense that a recommendation service for books might suggest a similar author, such as <span class="strong"><em class="calibre9">Robert Jordan</em></span>, for example.</p><p class="calibre8">Collaborative <a id="id925" class="calibre1"/>filtering systems, by their nature, require some sort of <a id="id926" class="calibre1"/>feedback system in order for the recommender to record a particular rating. In particular, they are ideal for leveraging <span class="strong"><strong class="calibre2">explicit feedback</strong></span>, whereby the user logs an actual rating or score. <span class="strong"><strong class="calibre2">Implicit feedback</strong></span> is indirect feedback, such as believing that a user likes a particular movie solely on the basis that they chose to rent that movie. Content-based recommendation systems are better suited to implicit feedback as they will use information about the content of the items to improve their knowledge about the user's preferences.</p><p class="calibre8">In addition, content-based recommendation systems often make use of a user profile in which the user may record what he or she likes in the form of a list of keywords, for example. Moreover, preference keywords can be learned from queries made by the user in the item database, if searching is supported.</p><p class="calibre8">Certain types of content are more amenable to the content-based approach. The classic scenario for a content-based recommender is when the content is in the form of text. Examples include book and news article recommendation systems. With text-based content, we can use techniques from the field of information retrieval in order to build up an understanding of how different items are similar to each other. For example, we have seen ways to analyze text using the bag of words feature when we looked at sentiment analysis in <a class="calibre1" title="Chapter 8. Dimensionality Reduction" href="part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7">Chapter 8</a>, <span class="strong"><em class="calibre9">Probabilistic Graphical Models</em></span>, and topic modeling in <a class="calibre1" title="Chapter 10. Probabilistic Graphical Models" href="part0076_split_000.html#28FAO2-c6198d576bbb4f42b630392bd61137d7">Chapter 10</a>, <span class="strong"><em class="calibre9">Topic Modeling</em></span>.</p><p class="calibre8">Of course, content such as images and video is much less amenable to this method. For general products, the content-based approach requires textual descriptions of all the items in the database, which is one of its drawbacks. Furthermore, with content-based recommendations, we are often likely to consistently suggest items that are too similar; that is to say that our recommendations might not be sufficiently varied. For instance, we might consistently recommend books by the same author or news articles with the same topic precisely because their content is so similar.</p><p class="calibre8">By contrast, the collaborative filtering paradigm uses empirically found relationships between <a id="id927" class="calibre1"/>users and items on the basis of preferences alone. Consequently, it can be far less predictable (though, in some contexts, this is not necessarily good).</p><p class="calibre8">One of the classic difficulties faced by collaborative filtering and content-based recommendation <a id="id928" class="calibre1"/>systems alike is the <span class="strong"><strong class="calibre2">cold start problem</strong></span>. If we are basing the recommendations we supply using ratings made by users or on the content that they somehow indicated they like, how do we deal with new users and new items for which we have no ratings at all? One way to handle this is to use heuristics or rules of thumb, for example, by suggesting items that most users will like, just as the POPULAR algorithm does.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Knowledge-based recommendation systems</strong></span> avoid this issue entirely by basing their recommendations on rules and other sources of information about users and items. These systems <a id="id929" class="calibre1"/>usually behave quite predictably, have reliable quality, and can enforce a particular business practice, such as a sales-driven policy, with regard to making recommendations. Such recommenders often ask users specific questions in an interactive attempt to learn their preferences and use rules or constraints to identify items that should be recommended.</p><p class="calibre8">Often, this results in a system that, although predictable, can explain its output. This means that it can justify its recommendations to a user, which is a property that most examples of recommenders that follow the other paradigms lack. One important drawback of the knowledge-based paradigm, besides the initial effort necessary to design it, is that it is static and cannot adapt to changes or trends in user behavior.</p><p class="calibre8">Finally, it is well worth mentioning that we can design hybrid recommendation systems that incorporate more than one approach. An example of this is a recommender that uses collaborative <a id="id930" class="calibre1"/>filtering for most users but has a knowledge-based component for making recommendations to users that are new to the system. Another possibility for a hybrid recommendation system is to build a number of recommenders and integrate them into an ensemble using a voting scheme for the final recommendation.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note50" class="calibre1"/>Note</h3><p class="calibre8">A good all-round book that covers a wide variety of different recommender system paradigms and examples is <span class="strong"><em class="calibre9">Recommender Systems: An Introduction</em></span> by <span class="strong"><em class="calibre9">Dietmar Jannach</em></span> and others. This is published by <span class="strong"><em class="calibre9">Cambridge University Press</em></span>.</p></div></div>
<div class="book" title="Summary" id="2PKKS1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch12lvl1sec87" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we explored the process of building and evaluating recommender systems in R using the <code class="email">recommenderlab</code> package. We focused primarily on the paradigm of collaborative filtering, which in a nutshell formalizes the idea of recommending items to users through word-of-mouth. As a general rule, we found that user-based collaborative filtering performs quite quickly but requires all the data to make predictions. Item-based collaborative filtering can be slow to train a model but makes predictions very quickly once the model is trained. It is useful in practice because it does not require us to store all the data. In some scenarios, the trade-off in accuracy between these two can be high, but in others the difference is acceptable.</p><p class="calibre8">The process of training recommendation systems is quite resource-intensive and a number of important parameters come into play in the design, such as the metrics used to quantify similarity and distance between items and users. Finally, we touched upon alternatives to the collaborative filtering paradigm. Content-based recommendation systems are designed to leverage similarity between items on the basis of their content. As such, they are ideally suited to the domain of text. Knowledge-based recommendation systems are designed to make recommendations to users on the basis of a set of rules or constraints that have been designed by experts. These can be combined with the other approaches in order to address the cold-start problem for new users or items.</p><p class="calibre8">In the next chapter, we will cover the idea of applying the techniques and practices already covered in this book to very large data sources and point out the specific challenges encountered when working with big data.</p></div></body></html>