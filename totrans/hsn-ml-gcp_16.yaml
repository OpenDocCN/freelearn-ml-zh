- en: Generative Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In recent times, neural networks have been used as generative models: algorithms
    able to replicate the distribution of data in input to then be able to generate
    new values starting from that distribution. Usually, an image dataset is analyzed,
    and we try to learn the distribution associated with the pixels of the images
    to produce shapes similar to the original ones. Much work is ongoing to get neural
    networks to create novels, articles, art, and music.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Artificial intelligence** (**AI**) researchers are interested in generative
    models because they represent a springboard towards the construction of AI systems
    able to use raw data from the world and automatically extract knowledge. These
    models seem to be a way to train computers to understand the concepts without
    the need for researchers to teach such concepts a priori. It would be a big step
    forward compared to current systems, which are only able to learn from training
    data accurately labeled by competent human beings.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will touch one of the most exciting research avenues on
    generating models with neural networks. First, we will get an introduction to
    unsupervised learning algorithms; then an overview of generative models will be
    proposed. We will also discover the most common generative models and show how
    to implement a few examples. Finally, we will introduce the reader to the Nsynth
    dataset and the Google Magenta project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered are:'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative model introduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restricted Boltzmann machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Boltzmann machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variational autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative adversarial network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the chapter, the reader will learn how to extract the content
    generated within the neural net with different types of content.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning is a machine learning technique that, starting from a
    series of inputs (system experience), is able to reclassify and organize on the
    basis of common characteristics to try to make predictions on subsequent inputs.
    Unlike supervised learning, only unlabeled examples are provided to the learner
    during the learning process, as the classes are not known a priori but must be
    learned automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows three groups labeled from raw data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd2095b9-045f-48fc-86e2-e6df1f973034.png)'
  prefs: []
  type: TYPE_IMG
- en: From this diagram, it is possible to notice that the system has identified three
    groups on the basis of a similarity, which in this case is due to proximity. In
    general, unsupervised learning tries to identify the internal structure of data
    to reproduce it.
  prefs: []
  type: TYPE_NORMAL
- en: Typical examples of these algorithms are search engines. These programs, given
    one or more keywords, are able to create a list of links that lead to pages that
    the search algorithm considers relevant to the research carried out. The validity
    of these algorithms depends on the usefulness of the information that they can
    extract from the database.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning techniques work by comparing data and looking for similarities
    or differences. As is known, machine learning algorithms try to imitate the functioning
    of an animal's nervous system. For this purpose, we can hypothesize that neural
    processes are guided by mechanisms that optimize the unknown objective they pursue.
    Each process evolves from an initial situation associated with a stimulus to a
    terminal in which there is an answer, which is the result of the process itself.
    It is intuitive that, in this evolution, there is a transfer of information. In
    fact, the stimulus provides the information necessary to obtain the desired response.
    Therefore, it is important that this information is transmitted as faithfully
    as possible until the process is completed. A reasonable criterion for interpreting
    the processes that take place in the nervous system is, therefore, to consider
    them as transfers of information with maximum preservation of the same.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms are based on these concepts. It is a question
    of using learning theory techniques to measure the loss of information that has
    occurred in the transfer. The process under consideration is considered as the
    transmission of a signal through a noisy channel, using well-known techniques
    developed in the field of communications. It is possible, however, to follow a
    different approach based on a geometric representation of the process. In fact,
    both the stimulus and the response are characterized by an appropriate number
    of components, which in a space correspond to a point. Thus, the process can be
    interpreted as a geometric transformation of the input space to the output space.
    The exit space has a smaller size than the input space, as the stimulus contains
    the information necessary to activate many simultaneous processes. Compared to
    only one, it is redundant. This means that there is always a redundancy reduction
    operation in the transformation under consideration.
  prefs: []
  type: TYPE_NORMAL
- en: In the entry and exit space, typical regions are formed, with which the information
    is associated. The natural mechanism that controls the transfer of information
    must therefore identify, in some way, these important regions for the process
    under consideration, and make sure that they correspond in the transformation.
    Thus, a data grouping operation is present in the process in question; this operation
    can be identified with the acquisition of experience. The two previous operations
    of grouping and reduction of redundancy are typical of optimal signal processing,
    and there is biological evidence of their existence in the functioning of the
    nervous system. It is interesting to note that these two operations are automatically
    achieved in the case of non-supervised learning based on experimental principles,
    such as competitive learning.
  prefs: []
  type: TYPE_NORMAL
- en: Generative models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A generative model aims to generate all the values of a phenomenon, both those
    that can be observed (input) and those that can be calculated from the ones observed
    (target). We try to understand how such a model can succeed in this goal by proposing
    a first distinction between generative and discriminative models.
  prefs: []
  type: TYPE_NORMAL
- en: Often, in machine learning, we need to predict the value of a target vector
    *y* given the value of an input *x* vector. From a probabilistic perspective,
    the goal is to find the conditional probability distribution *p(y|x)*.
  prefs: []
  type: TYPE_NORMAL
- en: The conditional probability of an event *y* with respect to an event *x* is
    the probability that *y* occurs, knowing that *x* is verified. This probability,
    indicated by *p(y|x)*, expresses a correction of expectations for *y*, dictated
    by the observation of *x*.
  prefs: []
  type: TYPE_NORMAL
- en: The most common approach to this problem is to represent the conditional distribution
    using a parametric model, and then determine the parameters using a training set
    consisting of pairs (*xn*, *yn*) that contain both the values ​​of the input variables
    and the relative vectors of corresponding outputs. The resulting conditional distribution
    can be used to make predictions of the target (*y*) for new input values ​​(*x*).
    This is known as a **discriminatory approach**, since the conditional distribution
    discriminates directly between the different values ​​of *y*.
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative to this approach, we can look for the joint probability distribution
    *p(x∩ y)*, and then use this joint distribution to evaluate the conditional probability
    *p(y | x)* in order to make predictions of *y* for new values ​​of *x*. This is
    known as **generative approach**, because by sampling from the joint distribution,
    it is possible to generate synthetic examples of the vector of characteristics
    *x*.
  prefs: []
  type: TYPE_NORMAL
- en: The joint probability distribution *p(x, y)* is a probability distribution that
    gives the probability that each of *x*, *y* vectors falls in any particular range
    or discrete set of values specified for that variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'A generative approach, regardless of the type of data and the theoretical model
    used, is divided into two basic steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step involves the construction of the generative model. The input
    data is processed with the aim of deducing their distribution. To do this, input
    data can simply be reorganized into a different structure, or it can represent
    new information extracted from input data from specific algorithms. The result
    of the construction of the generative model is the presentation of data according
    to the distribution to which it has been approximated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the generative model has been built on the input data, this allows sampling,
    which leads to the formation of new data that shares the same distribution with
    the input data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The construction of a generative model allows highlighting features and properties
    implicitly present in the initial data. The individual approaches are then distinguished
    by the type of processing performed on the data to explain these characteristics,
    and consequently for the type of variables on which an approximate data distribution
    is obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why are AI researchers so excited about generative models? Let''s take a simple
    example: suppose we provide the system with a series of images of cats. Suppose
    then, that after seeing these images, the computer is able to generate new photos
    of cats in a completely independent manner. If the computer were able to do it
    and the images that were produced had the right number of legs, tails, ears, and
    so on, it would be easy to prove that the computer knows which parts make up the
    cat, even if no one has ever explained cat anatomy to it. So, in a sense, a good
    generative model is proof of the basic knowledge of concepts by computers.'
  prefs: []
  type: TYPE_NORMAL
- en: This is why researchers are so enthusiastic about building generative models.
    These models seem to be a way to train computers to understand concepts without
    the need for researchers to teach them a priori concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Restricted Boltzmann machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Boltzmann machine is a probabilistic graphic model that can be interpreted
    as a stochastic neural network. Boltzmann machines were first introduced in 1985
    by Geoffrey Hinton and Terry Sejnowski. **Stochastic** is due to the behavior
    of the neurons; within them, in the activation function, they will have a probabilistic
    value that will influence the activation of the neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, a Boltzmann machine is a model (including a certain number of
    parameters) that, when applied to a data distribution, is able to provide a representation.
    This model can be used to extract important aspects of an unknown distribution
    (target distribution) starting only from a sample of the latter. The data samples
    referred to by a Boltzmann Machine are also called **training data**. The following
    diagram shows a Boltzmann machine''s architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e44f820-27f1-481c-88ad-ebe5c3822a5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Training a Boltzmann machine means adapting its parameters so that the probability
    distribution represented by it interpolates the training data as best as possible.
    The training of a Boltzmann machine is a rather demanding work from a computational
    point of view. However, this problem can be made easier by imposing restrictions
    on the topology of the network on which you are working; this defines **Restricted
    Boltzmann machines** (**RBM**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In Boltzmann machines, there are two types of units:'
  prefs: []
  type: TYPE_NORMAL
- en: Visible units (or neurons since, as we said, a Boltzmann machine can be interpreted
    as a neural network)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden units (or neurons)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even in RBMs, there are both of these types of units and we can imagine them
    as arranged on two levels:'
  prefs: []
  type: TYPE_NORMAL
- en: Visible units are the components of an observation (for example, if our data
    consists of images, we can associate a visible unit with each pixel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden units instead give us a model of the dependencies that exist between
    the components of our observation (for example, the dependency relationships that
    exist between the pixels of an image)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hidden units can therefore be seen as detectors of data characteristics. In
    the RBM graph, every neuron is connected to all the neurons of the other level,
    while there are no connections between neurons of the same level; it is precisely
    this restriction that gives the RBM its name, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5015ef9-9fd2-4a71-9c23-0cb45759b829.png)'
  prefs: []
  type: TYPE_IMG
- en: After successful training, an RBM provides a very good representation of the
    distribution that underlies training data. It is a generative model that allows
    sampling new data starting from the learned distribution; for example, new image
    structures can be generated starting from studied images. Having a generative
    model makes useful applications possible. For example, you can think of integrating
    some visible units corresponding to a partial observation (that is, you fix the
    values of the observed variables and consider them constant) and then produce
    the remaining ones visible units to complete the observation; in the image analysis
    example, this can be useful for an image completion task.
  prefs: []
  type: TYPE_NORMAL
- en: 'As generative models, RBMs can also be used as classifiers. Consider an application
    of this type:'
  prefs: []
  type: TYPE_NORMAL
- en: RBM is trained to learn the joint probability distribution of the input data
    (explanatory variables) and the corresponding labels (response/output variables),
    both represented in the graph of the network, from the visible units of the RBM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subsequently, a new input pattern, this time without labels, can be linked to
    the visible variables. The corresponding labels can be predicted by sampling directly
    from the Boltzmann machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Boltzmann machine is able to complete partial patterns of data on visible
    units. If we divide the visible units into units of input and output, given the
    input pattern, the Boltzmann machine completes it by producing the outputs (classification).
    Otherwise, it works as associative memory, returning the most similar pattern
    among those learned to the (partial) data.
  prefs: []
  type: TYPE_NORMAL
- en: Boltzmann machine architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Boltzmann machine architecture is based on input, output, and hidden nodes.
    The connection weights are symmetrical:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6acae0d1-92b9-42d1-978d-72cb6705d75f.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on this assumption, Boltzmann machines are highly recurrent, and this
    recurrence eliminates any basic difference between input and output nodes, which
    can be considered as input or output when needed. The Boltzmann machine is a network
    of units with an **energy** defined for the overall network. Its units produce
    binary results ((1,0) values). Outputs are computed probabilistically, and depend
    upon the temperature variable *T*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The consensus function of the Boltzmann machine is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14f55f75-1a92-4109-9e2f-16d8b98a6736.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous formula, the terms are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*S[i]* is the state of unit *i(1,0)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w[ij]* is the connection strength between unit *j* and unit *i*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*u[j]* is the output of unit *j*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The calculation proceeds within the machine in a stochastic manner so that the
    consent is increased. Thus, if *w[ij]* is positive, there is a tendency to have
    units *i* and *j* both activated or both deactivated, while if the weight is negative,
    there is a tendency to have them with different activations (one activated and
    the other not). When a weight is positive, it is called **excitatory**; otherwise,
    it is called **inhibitory**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each binary unit makes a stochastic decision to be either 1 (with probability
    *p[i]*) or 0 (with probability *1- p[i]*). This probability is given by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d3077fe-3819-4235-b6cc-1247b5eab5f7.png)'
  prefs: []
  type: TYPE_IMG
- en: At the equilibrium state of the network, the likelihood is defined as the exponentiated
    negative energy, known as the **Boltzmann distribution**. You can imagine that
    by administering energy, you can get the system out of the local minima. This
    must be done slowly, because a violent shock can drive the system away from the
    global minimum. The best method is to give energy and then slowly reduce it. This
    concept is used in metallurgy, where an ordered state of the metal is obtained
    first by melting, and then slowly the temperature is reduced. The reduction in
    temperature as the process is under way is called **simulated annealing**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method can be reproduced by adding a probabilistic update rule to the
    Hopfield network (refer to [Chapter 13](0fa9cfb2-9e84-4f95-b287-c28f1805cc97.xhtml),
    *Beyond Feedforward Networks – CNN and RNN*); the network that reproduces it is
    called **Boltzmann machine**. There will be a parameter that varies: the temperature.
    So at high *T*, the probability of jumping to a higher energy is much greater
    than at low temperatures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the temperature drops, the probability of assuming the correct minimum
    energy status approaches 1, and the network reaches the thermal equilibrium. Each
    unit of the network makes an energy leap given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdcee90a-5723-4c44-b7d3-fffdd91cdc9c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The system changes to a state of lower energy according to the following probabilistic
    rule (transition function):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3a92cb1-11e1-4512-98d1-fe158d8c23f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is seen that the probability of transition to a higher energy state is greater
    at high *T* than at low *T*. The network can assume a configuration of stable
    states according to the following Boltzmann distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ba15569-d320-436d-a1b8-13c948c30e70.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, it depends on the energy of the state and temperature of the system.
    Lower energy states are more likely; in fact if *E[a] < E[b]*, then *P[a]/P[b]
    > 1*, because of which *P[a]>P[b]*. So the system tends toward a state of minimum
    energy.
  prefs: []
  type: TYPE_NORMAL
- en: Boltzmann machine disadvantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Numerous problems have emerged in the use of algorithms based on Boltzmann
    machines. The following are some of the problems encountered:'
  prefs: []
  type: TYPE_NORMAL
- en: Weight adjustment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The time needed to collect statistics in order to calculate probabilities,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many weights change at a time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to adjust the temperature during simulated annealing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to decide when the network has reached the equilibrium temperature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main disadvantage is that Boltzmann learning is significantly slower than
    backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Boltzmann machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another type of Boltzmann Machine is **Deep Boltzmann machine** (**DBM**).
    This is a neural network similar to RBM, but instead of having only one layer
    of hidden nodes, DBMs have many. Each layer of neurons is connected only to those
    adjacent (the one immediately preceding and immediately following); here also,
    the neurons of the same layer are not interconnected. This structure allows the
    emergence of particular statistics from each layer that can capture new data features.
    The following diagram shows a DBM model with one visible layer and two hidden
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cec3294c-9538-4bc9-9c3d-cbcd6597215e.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, connections are only between units in neighboring layers. Like
    RBMs and DBMs contain only binary units.
  prefs: []
  type: TYPE_NORMAL
- en: 'The DBMs model assigns the following probability to a visible vector *v*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/246b4b27-7a6d-4e45-aefe-fb19b9e2d4b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous formula, the terms are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*v* is the visible vector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*θ = (W(1),W(2))* are the model parameters, representing visible-to-hidden
    and hidden-to-hidden symmetric interaction terms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*h^((1))* and *h^((2))* are hidden stochastic binary variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Z(θ)* is the partition function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBMs are particularly useful in the case of the recognition of objects or words.
    This is due to the great ability to learn complex and abstract internal representations
    using little labeled input data, instead of exploiting a large amount of unlabeled
    input data. However, unlike deep convolutional neural networks, DBMs adopt the
    inference and training procedure in both directions to better detect representations
    of input structures.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An autoencoder is a neural network whose purpose is to code its input into
    small dimensions and the result obtained, to be able to reconstruct the input
    itself. Autoencoders are made up of the union of the following two subnets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Encoder, which calculates the function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*z = ϕ(x)*'
  prefs: []
  type: TYPE_NORMAL
- en: Given an input *x*, the encoder encodes it in a variable *z*, also called **latent
    variable**. *z* usually has much smaller dimensions than *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decoder, which calculates the following function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x'' = ψ(z)*'
  prefs: []
  type: TYPE_NORMAL
- en: Since *z* is the code of *x* produced by the encoder, the decoder must decode
    it so that *x'* is similar to *x*.
  prefs: []
  type: TYPE_NORMAL
- en: The training of autoencoders is intended to minimize the mean square error between
    the input and the result.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean Squared Error** (**MSE**) is the average squared difference between
    the outputs and targets. Lower values are indicative of better results. Zero means
    no error.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For n observations, *MSE* is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c0e222d-4550-4c01-ba34-712bc3d26588.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we can summarize that the encoder encodes the input in a compressed
    representation and the decoder returns from it a reconstruction of the input,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4d01427-e444-48e4-9ff6-19e7f4da2887.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s define the following terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '*W*: input → hidden weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*V*: hidden → output weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The previous formulas become:'
  prefs: []
  type: TYPE_NORMAL
- en: '*z = ϕ(W* x)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'And they also become:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x'' = ψ(V*W1* x)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the training of autoencoders is intended to minimize the following
    quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/599757b5-83ce-4735-9e7d-76dd298d25f6.png)'
  prefs: []
  type: TYPE_IMG
- en: The purpose of autoencoders is not simply to perform a sort of compression of
    the input or to look for an approximation of the identity function. There are
    techniques that allow, starting from a hidden layer of reduced dimensions, to
    direct the model to give greater importance to some data properties, thus giving
    rise to different representations based on the same data.
  prefs: []
  type: TYPE_NORMAL
- en: Variational autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Variational autoencoder** (**VAE**) are inspired by the concept of Autoencoder:
    a model consisting of two neural networks called **encoders** and **decoders**.
    As we have seen, the encoder network tries to code its input in a compressed form,
    while the network decoder tries to reconstruct the initial input, starting from
    the code returned by the encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the functioning of the VAE is very different than that of simple autoencoders.
    VAEs allow not only coding/decoding of input but also generating new data. To
    do this, they treat both the code *z* and the reconstruction/generation *x'* as
    if they belonged to a certain probability distribution. In particular, the VAEs
    are the result of the combination of deep learning and Bayesian inference, in
    the sense that they consist of a neural network trained with the backpropagation
    algorithm modified with a technique called **re-parameterization**. While deep
    learning has proven to be very effective in the approximation of complex functions,
    the Bayesian statistics allow managing the uncertainty derived from a random generation
    in the form of probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The VAE uses the same structure to generate new images, similar to those belonging
    to the training set. In this case, the encoder does not directly produce a code
    for a given input but calculates the mean and variance of a normal distribution.
    A value is taken from this distribution and it is decoded by the decoder. The
    training consists of modifying the encoder and decoder parameters so that the
    result of the decoded so carried out is as similar as possible to the starting
    image. At the end of the training, we have that starting from the normal distribution
    with mean and variance produced by the encoder; the decoder will be able to produce
    images similar to those belonging to the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the following terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X*: Input data vector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*z*: Latent variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(X)* : Probability distribution of the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(z)*: Probability distribution of the latent variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(X|z)*: Posterior probability, that is, the distribution of generating data
    given the latent variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The posterior probability *P(X|z)* is the probability of the condition *X* given
    the evidence *z*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to generate data according to the characteristics contained in
    the latent variable, so we want to find *P(X)*. For this purpose, we can use the
    law of total probability according to the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b70972e-9595-4b55-9d90-53a6db072c2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To understand how we arrived at this formulation, we reason by step. Our first
    task in defining the model is to infer good values of the latent variables starting
    from the observed data, or to calculate the posterior *p(z|X)*. To do this, we
    can use the Bayes theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d46bbe0-263d-468d-add4-a5eb6764734d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous formula, the *P(X)* term appears. In the context of Bayesian
    statistics, it may also be referred to as the evidence or model evidence. The
    evidence can be calculated by marginalizing out the latent variables. This brings
    us to the starting formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0fe3410-7faa-4200-a3e5-2578aa13fff0.png)'
  prefs: []
  type: TYPE_IMG
- en: The computational estimate of this integral requires an exponential time as
    it must be evaluated on all the configurations of latent variables. To reduce
    the computational cost, we are forced to approximate the estimate of the posterior
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: In VAE, as the name suggests, we deduce *p(z | X)* using a method called **variational
    inference** (**VI**). VI is one of the most used methods in Bayesian inference.
    This technique considers inference as an optimization problem. In doing this,
    we use a simpler distribution that is easy to evaluate (for example, Gaussian)
    and minimize the difference between these two distributions using the **Kullback-Leibler
    divergence metric**.
  prefs: []
  type: TYPE_NORMAL
- en: Kullback-Leibler divergence metric is a non-symmetric measure of the difference
    between two probability distributions *P* and *Q*. Specially, the Kullback-Leibler
    divergence of *Q* from *P*, denoted by **DKL** *(P ||Q)*, is the measurement of
    the information lost when *Q* is used to approximate *P*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For discrete probability distributions *P* and *Q*, the Kullback-Leibler divergence
    from *Q* to *P* is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57583f96-c918-48a5-9f01-1f29c375b290.png)'
  prefs: []
  type: TYPE_IMG
- en: Analyzing the formula makes it evident that the divergence of Kullback-Leibler
    is the expectation of the logarithmic difference between the probabilities *P*
    and *Q*, where the expectation is taken using the probability *P*.
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative adversarial network** (**GAN**) is a generative model consisting
    of two networks that are jointly trained, called **generator** and **discriminator**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dynamics between these two networks are like those between a forger and
    an investigator. The forger tries to produce faithful imitations of authentic
    works of art, while the investigator tries to distinguish the fakes from the originals.
    In this analogy, the forger represents the generator and the investigator represents
    the discriminator. The generator accepts input values ​​belonging to a fixed distribution
    and tries to produce images similar to those of the dataset. The discriminator
    tries to distinguish the data created by the generator from those belonging to
    the dataset. These two networks are jointly coached:'
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator tries to return output = 1 if the input belongs to the dataset
    and returns 0 if its input was generated by the generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generator instead tries to maximize the possibility that the discriminator
    will make mistakes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The generator acquires a random input noise and tries to create a sample of
    data, while the discriminator takes input from either real-world examples or the
    generator, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc17de32-bdec-4266-9b6a-3cd00a7a346a.png)'
  prefs: []
  type: TYPE_IMG
- en: For simplicity, the two opposing networks are of the multilayer perceptron type;
    however, the same structure can be modeled with deep networks. For example, to
    generate new images, instead of sampling data from a complex distribution, the
    approach used in these networks is to start from values belonging to a simple
    distribution or from random values. Subsequently, they are mapped through a second
    distribution that will be learned during the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In such a system, training leads to constant competition between generator
    and discriminator. Under these conditions, the optimization process can be carried
    out independently on both sides. Naming *G(z)* the generator and *D(x)* the discriminator,
    the training of the model aims to maximize the probability of the discriminator
    to assign 1 to values coming from the training set, instead of 0 to those produced
    by the generator. On the other hand, we want to teach the generator to minimize
    the following quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6001f561-7c54-47ba-bee9-755125f7f352.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The training is then performed by applying the gradient descent technique to
    the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f02edaee-d830-4920-a698-7b99d1410767.png)'
  prefs: []
  type: TYPE_IMG
- en: This method originates from game theory, in particular from the method called
    **two-player minimax game**. The algorithms of this type adopt the strategy of
    minimizing the maximum possible loss resulting from the choice of a player. It
    can happen that, in the training process, the discriminator is not able to classify
    examples generated by real ones.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Adversarial autoencode**r (**AAE**) is a generative model produced by the
    union of VAE and GAN. To explain the model, we start by defining the following
    terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x*: Autoencoder input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'z: Code produced from *x*,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p(z)*: The distribution we want to impose'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*q(z|x)*: Distribution learned from the encoder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p(x|z)*: Distribution learned from the decoder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*pdata*: Distribution of the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p(x)*: Distribution of the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We consider the function *q(z|x)* as a posterior distribution of *q(z)*, which
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09520493-7e28-4d45-86c5-e5a66cde4a86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We try to impose the equality *q(z)=p(z)* on the model. The difference with
    a VAE is due to the fact that what drives *q (z)* towards *p(z)* is an adversarial
    network. The encoder of the VAE is considered the generator of a GAN for which
    a discriminator can be used. This tries to distinguish data belonging to *q(z)*
    from that coming from *p(z)*. The following diagram shows an AAE architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e48353cc-d8e6-4f7b-b6de-566fa7919e91.png)'
  prefs: []
  type: TYPE_IMG
- en: The trainings of the adversarial network and of the autoencoder take place jointly,
    using stochastic gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction using RBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recently, several types of **artificial neural networks** (**ANNs**) have been
    applied to classify a specific dataset. However, most of these models use only
    a limited number of features as input, in which case there may not be enough information
    to make the prediction due to the complexity of the starting dataset. If you have
    more features, the run time of training would be increased and generalization
    performance would deteriorate due to the curse of dimesionality. In these cases,
    a tool to extract the characteristics would be particularly useful. RBM is a machine
    learning tool with a strong representation power, which is often used as a feature
    extractor in a wide variety of classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Breast cancer dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The breast is made up of a set of glands and adipose tissue, and is located
    between the skin and the chest wall. In fact, it is not a single gland but a set
    of glandular structures, called **lobules**, joined together to form a lobe. In
    a breast, there are 15 to 20 lobes. The milk reaches the nipple from the lobules
    through small tubes called **milk ducts**.
  prefs: []
  type: TYPE_NORMAL
- en: Breast cancer is a potentially serious disease if it is not detected and treated.
    It is caused by uncontrolled multiplication of some cells in the mammary gland
    that are transformed into malignant cells. This means that they have the ability
    to detach themselves from the tissue that has generated them to invade the surrounding
    tissues and eventually other organs of the body. In theory, cancers can be formed
    from all types of breast tissues, but the most common ones are from glandular
    cells or from those forming the walls of the ducts.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of this example is to identify each of a number of benign or malignant
    classes. To do this, we will use the data contained in the dataset named BreastCancer
    (Wisconsin Breast Cancer database). This data has been taken from the UCI Repository
    of machine learning databases as DNA samples arrive periodically, as Dr. Wolberg
    reports his clinical cases. The database therefore reflects this chronological
    grouping of the data. This grouping information appears immediately, having been
    removed from the data itself. Each variable, except for the first, was converted
    into 11 primitive numerical attributes with values ranging from zero through ten.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the data, we draw on the large collection of data available in the UCI
    Machine Learning Repository at the following link: [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).'
  prefs: []
  type: TYPE_NORMAL
- en: To load the dataset, we will use the `sklearn.datasets` module. It includes
    utilities to load datasets, including methods to load and fetch popular reference
    datasets. It also features some artificial data generators.
  prefs: []
  type: TYPE_NORMAL
- en: 'The breast cancer dataset is a classic and very easy binary classification
    dataset. The following table has some information about the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Classes | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Samples per class | 212(M), 357(B) |'
  prefs: []
  type: TYPE_TB
- en: '| Samples total | 569 |'
  prefs: []
  type: TYPE_TB
- en: '| Dimensionality | 30 |'
  prefs: []
  type: TYPE_TB
- en: '| Features | real and positive |'
  prefs: []
  type: TYPE_TB
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After introducing the breast cancer dataset, we can analyze the code that will
    allow us to classify the input data line by line. In the first part of the code,
    we import the libraries we will use later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For now, let''s limit ourselves to import; we will deepen them at the time
    of use. To start, we have to import the dataset; we will do so using the `sklearn.datasets`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This command loads and returns the breast cancer `wisconsin` dataset. The `sklearn.datasets`
    package embeds some small toy datasets. To evaluate the impact of the scale of
    the dataset (`n_samples` and `n_features`) while controlling the statistical properties
    of the data (typically the correlation and informativeness of the features), it
    is also possible to generate synthetic data. This package also features helpers
    to fetch larger datasets commonly used by the machine learning community to benchmark
    algorithm on data that comes from the real world. A dataset is a dictionary-like
    object that holds all the data and some metadata about the data. This data is
    stored in the data member, which is a `n_samples` and `n_features` array. In the
    case of a supervised problem, one or more response variables are stored in the
    target member.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data is returned in a `Bunch` object, a dictionary-like object that contains
    the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`data`: The data to learn'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target`: The classification labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_names`: The meaning of the labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_names`: The meaning of the features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DESCR`: The full description of the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To confirm the content of the data, let''s extract the dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To better understand the operations, we divide these data into `X` (predictors)
    and `Y` (target):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we extract a series of statistics from the predictors using the
    tools that make available to us the `pandas` library.
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas` is an open source, BSD-licensed library providing high-performance,
    easy-to-use data structures and data analysis tools for the Python programming
    language.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use this function, we have to convert the input data from `numpy.darray`
    to `pandas` dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Due to space constraints, we have reported only the results for the first five
    predictors. As we can see, the variables have different ranges. When the predictors
    have different ranges, the impact on response variables by the feature having
    a greater numeric range could be more than the one having a less numeric range,
    and this could, in turn, impact the prediction accuracy. Our goal is to improve
    predictive accuracy and not allow a particular feature to impact the prediction
    due to a large numeric value range. Thus, we may need to scale values under different
    features such that they fall under a common range. Through this statistical procedure,
    it is possible to compare identical variables belonging to different distributions
    and also different variables or variables expressed in different units.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, it is good practice to rescale the data before training a machine
    learning algorithm. With rescaling, data units are eliminated, allowing you to
    easily compare data from different locations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we will use the min-max method (usually called **feature scaling**)
    to get all the scaled data in the range (0, 1). The formula to achieve this is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d0fa157-043f-480a-8c67-3356ced2ab01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following command performs a feature scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`numpy.min()` and `numpy.max()` are used to calculate the minimum and maximum
    values of each database column.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now split the data for the training and the test models. Training and
    testing the model forms the basis for further usage of the model for prediction
    in predictive analytics. Given a dataset of 100 rows of data, which includes the
    predictor and response variables, we split the dataset into a convenient ratio
    (say 80:20), and allocate 80 rows for training and 20 rows for testing. The rows
    are selected at random to reduce bias. Once the training data is available, the
    data is fed to the machine learning algorithm to get the massive universal function
    in place. To split the dataset, we will use the `sklearn.model_selection.train_test_split()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train_test_split()` function splits arrays or matrices into random train
    and test subsets. The first two arguments are `X` (predictors) and `Y` (target)
    numpy arrays. Allowed inputs are lists, `numpy` arrays, scipy-sparse matrices,
    or `pandas` dataframes. Then two options are added:'
  prefs: []
  type: TYPE_NORMAL
- en: '`test_size`: This should be between 0.0 and 1.0 and represent the proportion
    of the dataset to include in the test split'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state`: This is the seed used by the random number generator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model fitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have previously said that RBM is often used as a feature in a wide variety
    of classification problems. It's time to see how to do it. The first thing to
    do is to use the `BernoulliRBM` function of the `sklearn.neural_network` module.
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn` is a free machine learning library for the Python programming language.
    It features various classification, regression, and clustering algorithms, including
    support vector machines, random forests, gradient boosting, k-means, and DBSCAN.
    And it is designed to interoperate with the Python numerical and scientific libraries
    NumPy and SciPy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `sklearn` library, the `sklearn.neural_network` module includes models
    based on neural networks. In this module, the `BernoulliRBM` function fits a Bernoulli
    RBM. An RBM with binary visible units and binary hidden units is returned. The
    parameters are estimated using **Stochastic Maximum Likelihood** (**SML**), also
    known as **Persistent Contrastive Divergence** (**PCD**). First, we will set the
    architecture of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will fit the model with the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `fit_transform` method fits the transformer to `X_train` and `Y_train` with
    optional parameter `fit_params`, and returns a transformed version of `X_train`.
    In this case, no optional parameters are used.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you remember, our purpose is to use the `Rbm` model to extract the features
    that will then be used by the logistic regression model to classify the data.
    So, the first part has already been performed—we already have the features extracted
    in the `FitRbmModel` variable. The time has come to create the logistic regression
    model. To do this, we will use `LogisticRegression` function of the `sklearn.linear_model`
    module, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We now set coefficients of the features in the decision function equal to the
    features extracted from the `rbm` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can build the classifier. To do this, we will use the `Pipeline` function
    of the `sklearn.pipeline` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The purpose of the `pipeline` is to assemble several steps that can be cross-validated
    together while setting different parameters. For this, it enables setting parameters
    of the various steps using their names, as in the previous code. A step''s estimator
    may be replaced entirely by setting the parameter with its name to another estimator,
    or a transformer removed by setting to `None`. The classifier is now ready; we
    just have to train it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'First, the logistic regression model is trained and then the classifier. We
    just have to make predictions. Recall that for doing this, we have an unused dataset
    available: `X_test` and `Y_test`. To check the performance of the classifier,
    we will compare the forecasts with the real data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the results returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45e4aa13-a412-47eb-b29b-72ce75cb5df6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, to better understand the model performance, we will calculate the
    confusion matrix. In a confusion matrix, our classification results are compared
    to real data. The strength of a confusion matrix is that it identifies the nature
    of the classification errors as well as their quantities. In this matrix, the
    diagonal cells show the number of cases that were correctly classified; all the
    other cells show the misclassified cases. To calculate the confusion matrix, we
    can use the `ConfusionMatrix()` function contained in pandas library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, the results returned by the `ConfusionMatrix()` function
    are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Several bits of information are returned; in particular, we can notice that
    the accuracy of the model is 0.85.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoder with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we said previously, an autoencoder is a neural network whose purpose is
    to code its input into small dimensions and the result obtained to be able to
    reconstruct the input itself. Autoencoders are made up of the union of the following
    two subnets: encoder and decoder. To these functions is added another; it''s a
    loss function calculated as the distance between the amount of information loss
    between the compressed representation of the data and the decompressed representation.
    The encoder and the decoder will be differentiable with respect to the distance
    function, so the parameters of the encoding/decoding functions can be optimized
    to minimize the loss of reconstruction, using the gradient stochastic.'
  prefs: []
  type: TYPE_NORMAL
- en: Load data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is a database of handwritten digits consisting of 60,000 28 x 28 grayscale
    images of the 10 digits, along with a test set of 10,000 images. This dataset
    is already available in the Keras library. The following diagram shows a sample
    of images of 0-8 from the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fec99017-84f1-44c8-b493-7ae8745a8ed6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As always, we will analyze the code line by line. In the first part of the
    code, we import the libraries we will use later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This code imports the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: The Input function is used to instantiate a Keras tensor. A Keras tensor is
    a tensor object from the underlying backend (Theano, TensorFlow, or CNTK). We
    augment it with certain attributes that allow us to build a Keras model just by
    knowing the inputs and outputs of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Dense function is used instantiate a regular densely connected NN layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Model function is used to define the model. The model is the thing that
    you can summarize, fit, evaluate, and use to make predictions. Keras provides
    a `Model` class that you can use to create a model from your created layers. It
    only requires that you specify the input and output layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To import the dataset, simply use this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following tuples are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x_train, x_test`: A `uint8` array of grayscale image data with shape (`num_samples`,
    28, 28)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_train, y_test`: A `uint8` array of digit labels (integers in range 0-9)
    with shape (`num_samples`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we have to normalize all values between 0 and 1\. The Mnist images are
    stored in pixel format, where each pixel (totally 28 x 28) is stored as an 8-bit
    integer giving a range of possible values from 0 to 255\. Typically, zero is taken
    to be black, and 255 is taken to be white. The values in between make up the different
    shades of gray. Now, to normalize all values between 0 and 1, simply divide each
    value by 255\. So the pixel containing the value 255 will become 1 and the one
    containing 0 will remain as such; in between lie all the other values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'By using the `astype()` function, we have converted the input data in `float32`
    (single precision float: sign bit, 8-bits exponent, 23 bits mantissa). As we said,
    each sample (image) consists of a 28 x 28 matrix. To reduce the dimensionality,
    we will flatten the 28 x 28 images into vectors of size 784:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `reshape()` function gives a new shape to an array without changing its
    data. The new shape should be compatible with the original shape. The first dimension
    of the new shape is the number of observations returned from the `len()` function
    (`len(x_train)` and `len(x_test)`). The second dimension represents the product
    of the last two dimensions of the starting data (28 x 28 = 784). To better understand
    this transformation, we print the shape of the starting dataset first and then
    the shape of the transformed dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the results before and after the dataset reshape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Keras model overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two types of models available in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: Sequential model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras functional API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us take a look at each one in detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Sequential` model is a linear stack of layers. We can create a `Sequential`
    model by passing a list of layer instances to the constructor as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also simply add layers via the `.add()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This type of model needs to know what input shape it should expect. For this
    reason, the first layer in a `Sequential` model needs to receive information about
    its input shape. There are several possible ways to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: Pass an `input_shape` argument to the first layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify of their input shape via the `input_dim` and `input_length` arguments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a `batch_size` argument to a layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras functional API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another way to define a model is the Keras functional API. The Keras functional
    API is the way to go for defining complex models, such as multi-output models,
    directed acyclic graphs, or models with shared layers. For example, to define
    a densely connected network, simply type the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the following section, we will dive deep into this type of model by applying
    it to our example.
  prefs: []
  type: TYPE_NORMAL
- en: Define model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will build the model using the Keras functional API. As we saw before,
    first we have to define the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns a tensor that represents our input placeholder. Later, we will
    use this placeholder to define a `Model`. At this point, we can add layers to
    the architecture of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The Dense class is used to define a fully connected layer. We have specified
    the number of neurons in the layer as the first argument (32), the activation
    function using the activation argument (`relu`), and finally the input tensor
    (`InputModel`) of the layer.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that given an input `x`, the encoder encodes it in a variable `z`,
    also called **latent variable**. `z` usually has much smaller dimensions than
    `x`; in our case, we have passed from 784 to 32 with a compression factor of 24.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s add the decoding layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This layer is the lossy reconstruction of the input. For another time, we have
    used the Dense class with 784 neurons (dimensionality of the output space), the
    `sigmoid` activation function, and `EncodedLayer` output as input. Now we have
    to instantiate a model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This model will include all layers required in the computation of `DecodedLayer`
    (output) given `InputModel` (input). In the following are listed some useful attributes
    of Model class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model.layers` is a flattened list of layers comprising the model graph'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model.inputs` is the list of input tensors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model.outputs` is the list of output tensors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, we have to configure the model for training. To do this, we will use the
    `compile` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This method configures the model for training. Only two arguments are used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`optimizer`: String (name of optimizer) or optimizer instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss`: String (name of objective function) or objective function. If the model
    has multiple outputs, you can use a different loss on each output by passing a
    dictionary or a list of losses. The loss value that will be minimized by the model
    will then be the sum of all individual losses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have used adadelta optimizer. This method dynamically adapts over time, using
    only first-order information, and has minimal computational overhead beyond vanilla
    stochastic gradient descent. The method requires no manual tuning of the learning
    rate and appears robust to noisy gradient information, different model architecture
    choices, various data modalities, and selection of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we have used `binary_crossentropy` as a `loss` function. Loss functions
    are computationally feasible functions representing the price paid for inaccuracy
    of predictions in classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The fit method trains the model for a fixed number of epochs (iterations on
    a dataset). In the following, the arguments passed are explained to better understand
    the meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`: A Numpy array of training data (if the model has a single input), or list
    of Numpy arrays (if the model has multiple inputs). If input layers in the model
    are named, you can also pass a dictionary mapping input names to Numpy arrays.
    `x` can be None (default) if feeding from framework-native tensors (for example,.
    TensorFlow data tensors).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y`: A Numpy array of target (label) data if the model has a single output,
    or a list of Numpy arrays if the model has multiple outputs. If output layers
    in the model are named, you can also pass a dictionary mapping output names to
    Numpy arrays. `y` can be `None` (default) if feeding from framework-native tensors
    (for example, TensorFlow data tensors).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: `Integer` or `None`. This is the number of samples per gradient
    update. If unspecified, `batch_size` will default to `32`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epochs`: An Integer. It is the number of epochs to train the model. An epoch
    is an iteration over the entire `x` and `y` data provided. Note that in conjunction
    with `initial_epoch`, `epochs` is to be understood as the final epoch. The model
    is not trained for a number of iterations given by epochs, but merely until the
    epoch of index epochs is reached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shuffle`: A boolean to decide whether to shuffle the training data before
    each epoch or `str` (for `batch`). `batch` is a special option for dealing with
    the limitations of HDF5 data; it shuffles in batch-sized chunks. It has no effect
    when `steps_per_epoch` is anything other than None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validation_data`: A tuple (`x_val` and `y_val`) or tuple (`x_val`, `y_val`,
    and `val_sample_weights`) on which to evaluate the loss and any model metrics
    at the end of each epoch. The model will not be trained on this data. `validation_data`
    will override `validation_split`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `History` object is returned. Its `history.history` attribute is a record
    of training loss values and metrics values at successive epochs, as well as validation
    loss values and validation metrics values (if applicable).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our model is now ready, so we can use it to automatically rebuild the handwritten
    digits. To do this, we will use the `predict` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This method generates output predictions for the input samples (`x_test`).
    Running this example, you should see a message for each of the 100 epochs, printing
    the loss and accuracy for each, followed by a final evaluation of the trained
    model on the training dataset. This is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8bf13071-a06e-4ec3-9d31-595c5063f0f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To get an idea of how the `loss` function varies during the epochs, it can
    be useful create a plot of loss on the training and validation datasets over training
    epochs. To do this, we will use the `Matplotlib` library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'A plot of loss on the training and validation datasets over training epochs
    is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddd7f4d5-cff7-4b35-8021-5803be3c2b4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our work is done; we just have to verify the results obtained. We can print
    on screen the starting handwriting digits and those reconstructed from our model.
    Of course, we will do it only for some of the 60,000 digits contained in the dataset;
    in fact, we will limit ourselves to display the first five. We will also use the
    `Matplotlib` library in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10667a27-bb86-4860-8a5c-3e006d6b793d.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the result is very close to the original, meaning that the model
    works well.
  prefs: []
  type: TYPE_NORMAL
- en: Magenta
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On June 1, 2016, Google launched the Magenta project, a research project that
    aims to allow the creation of art and music in an autonomous way through the use
    of AI. Based on the TensorFlow platform, Magenta aims to publish code in open
    source mode on GitHub to allow developers to achieve increasingly striking and
    advanced results.
  prefs: []
  type: TYPE_NORMAL
- en: The project is a brainchild of the Google Brain team, a deep learning AI research
    team at Google. It combines open-ended machine learning research with system engineering
    and Google-scale computing resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Magenta project has set itself two ambitious goals: to develop machine
    learning for art and music, and to build a community of people interested in this
    topic. Machine learning has long been used in different contexts, in particular
    for speech recognition and translation of languages. Magenta was created to concentrate
    activity on previously unexplored fields such as the generation of art in the
    broad sense. To do this, Magenta wanted to create a physical place, where all
    people united by the same interest (that is, generation of art) could exchange
    ideas and products. In other words, a community of artists, programmers, and researchers
    of machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, refer to the official website of the project at the following
    URL: [https://magenta.tensorflow.org/](https://magenta.tensorflow.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: The NSynth dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From reading the previous chapters, we have now understood that, to correctly
    train a machine learning algorithm, it is necessary to have a dataset containing
    an important number of observations. Recently, the increased use of generative
    models has been applied to images thanks to the availability of high-quality image
    datasets, which therefore correspond to a significant data set. With this in mind,
    the Google Brain team has made NSynth available. It's a large-scale, high-quality
    set of musical notes that is an order of magnitude larger than comparable public
    datasets. The aim is to have a significant audio dataset in order to develop generative
    models with better performance.
  prefs: []
  type: TYPE_NORMAL
- en: The NSynth dataset was introduced by Jesse Engel et al. in the article named
    *Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders*.
  prefs: []
  type: TYPE_NORMAL
- en: NSynth is an audio dataset containing 305,979 musical notes, each with a unique
    tone, tone, and envelope. For 1,006 tools from commercial sample libraries, the
    Google Brain team generated 4 seconds of 16 kHz monophonic audio fragments, called
    **notes**, spanning each step of a standard MIDI piano or (21-108) and five different
    speeds (25, 50, 75, 100, and 127). The note was kept for the first 3 seconds and
    allowed to fall for the final second.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Google Brain team also annotated each of the notes with three additional
    pieces of information based on a combination of human evaluation and heuristic
    algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Source**: The method of sound production for the note''s instrument. This
    can be one of acoustic or electronic for instruments that were recorded from acoustic
    or electronic instruments respectively, or synthetic for synthesized instruments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Family**: The high-level family of which the note’s instrument is a member.
    Each instrument is a member of exactly one family.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Qualities**: Sonic qualities of the note. Each note is annotated with zero
    or more qualities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The NSynth dataset can be downloaded in two formats:'
  prefs: []
  type: TYPE_NORMAL
- en: TFRecord files of serialized TensorFlow example protocol buffers with one Example
    proto per note
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSON files containing non-audio features alongside 16-bit PCM WAV audio files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The full dataset is split into three sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train**: A training set with 289,205 examples. Instruments do not overlap
    with valid or test.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Valid**: A validation set with 12,678 examples. Instruments do not overlap
    with train.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test**: A test set with 4,096 examples. Instruments do not overlap with train.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more information and to download the dataset, refer to the official website
    of the project at the following URL: [https://magenta.tensorflow.org/datasets/nsynth](https://magenta.tensorflow.org/datasets/nsynth).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored one of the most interesting research sites on modeling
    with neural networks. First we saw an introduction to unsupervised learning algorithms.
    Unsupervised learning is a machine learning technique that, starting from a series
    of inputs (system experience), will be able to reclassify and organize on the
    basis of common characteristics to try to make predictions on subsequent inputs.
    Unlike supervised learning, only unlabeled examples are provided to the learner
    during the learning process, as the classes are not known a priori but must be
    learned automatically.
  prefs: []
  type: TYPE_NORMAL
- en: So, we analyzed different types of generative models. A Boltzmann machine is
    a probabilistic graphic model that can be interpreted as a stochastic neural network.
    In practice, a Boltzmann machine is a model (including a certain number of parameters)
    that, when applied to a data distribution, is able to provide a representation.
    This model can be used to extract important aspects of an unknown distribution
    (target distribution) starting only from a sample of the latter.
  prefs: []
  type: TYPE_NORMAL
- en: An autoencoder is a neural network whose purpose is to code its input into small
    dimensions and the result obtained to be able to reconstruct the input itself.
    The purpose of autoencoders is not simply to perform a sort of compression of
    the input or look for an approximation of the identity function; but there are
    techniques that allow us to direct the model (starting from a hidden layer of
    reduced dimensions) to give greater importance to some data properties. Thus they
    give rise to different representations based on the same data.
  prefs: []
  type: TYPE_NORMAL
- en: GAN is a generative model consisting of two networks that are jointly trained,
    called **generator** and **discriminator**. The dynamics between these two networks
    is like a forger and an investigator. The forger tries to produce faithful imitations
    of authentic works of art while the investigator tries to distinguish the fakes
    from the originals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we showed how to implement some examples: feature extraction using RBM
    and autoencoder with Keras. Finally, we introduced the Nsynth dataset and the
    Google Magenta project.'
  prefs: []
  type: TYPE_NORMAL
