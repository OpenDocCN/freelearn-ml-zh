- en: Generative Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成神经网络
- en: 'In recent times, neural networks have been used as generative models: algorithms
    able to replicate the distribution of data in input to then be able to generate
    new values starting from that distribution. Usually, an image dataset is analyzed,
    and we try to learn the distribution associated with the pixels of the images
    to produce shapes similar to the original ones. Much work is ongoing to get neural
    networks to create novels, articles, art, and music.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，神经网络已被用作生成模型：能够复制输入数据的分布，然后能够从这个分布中生成新值的算法。通常，分析图像数据集，并尝试学习与图像像素相关的分布，以产生与原始图像相似的形状。正在进行大量工作，以使神经网络能够创建小说、文章、艺术和音乐。
- en: '**Artificial intelligence** (**AI**) researchers are interested in generative
    models because they represent a springboard towards the construction of AI systems
    able to use raw data from the world and automatically extract knowledge. These
    models seem to be a way to train computers to understand the concepts without
    the need for researchers to teach such concepts a priori. It would be a big step
    forward compared to current systems, which are only able to learn from training
    data accurately labeled by competent human beings.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能**（**AI**）研究人员对生成模型感兴趣，因为它们代表了一个跳板，可以构建能够使用世界原始数据并自动提取知识的AI系统。这些模型似乎是一种训练计算机理解概念的方法，无需研究人员事先教授这些概念。与当前系统相比，这将是一个巨大的进步，因为当前系统只能从由有能力的自然人准确标记的训练数据中学习。'
- en: In this chapter, we will touch one of the most exciting research avenues on
    generating models with neural networks. First, we will get an introduction to
    unsupervised learning algorithms; then an overview of generative models will be
    proposed. We will also discover the most common generative models and show how
    to implement a few examples. Finally, we will introduce the reader to the Nsynth
    dataset and the Google Magenta project.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将触及生成模型中最激动人心的研究途径之一。首先，我们将介绍无监督学习算法；然后提出生成模型的概述。我们还将发现最常见的生成模型，并展示如何实现一些示例。最后，我们将向读者介绍Nsynth数据集和Google
    Magenta项目。
- en: 'The topics covered are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 涵盖的主题是：
- en: Unsupervised learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Generative model introduction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成模型介绍
- en: Restricted Boltzmann machine
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机
- en: Deep Boltzmann machines
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度玻尔兹曼机
- en: Autoencoder
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器
- en: Variational autoencoder
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分自动编码器
- en: Generative adversarial network
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: Adversarial autoencoder
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗性自动编码器
- en: At the end of the chapter, the reader will learn how to extract the content
    generated within the neural net with different types of content.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，读者将学习如何从神经网络中提取不同类型的内容生成的内容。
- en: Unsupervised learning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Unsupervised learning is a machine learning technique that, starting from a
    series of inputs (system experience), is able to reclassify and organize on the
    basis of common characteristics to try to make predictions on subsequent inputs.
    Unlike supervised learning, only unlabeled examples are provided to the learner
    during the learning process, as the classes are not known a priori but must be
    learned automatically.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是一种机器学习技术，它从一系列输入（系统经验）开始，能够根据共同特征重新分类和组织，以尝试对后续输入进行预测。与监督学习不同，在学习过程中，只向学习者提供未标记的示例，因为类别不是事先已知的，而是必须自动学习。
- en: 'The following diagram shows three groups labeled from raw data:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了从原始数据中标记的三个组：
- en: '![](img/cd2095b9-045f-48fc-86e2-e6df1f973034.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/cd2095b9-045f-48fc-86e2-e6df1f973034.png)'
- en: From this diagram, it is possible to notice that the system has identified three
    groups on the basis of a similarity, which in this case is due to proximity. In
    general, unsupervised learning tries to identify the internal structure of data
    to reproduce it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张图中，我们可以注意到系统基于相似性识别了三个组，在这个例子中，这种相似性是由于邻近性。一般来说，无监督学习试图识别数据的内部结构以重现它。
- en: Typical examples of these algorithms are search engines. These programs, given
    one or more keywords, are able to create a list of links that lead to pages that
    the search algorithm considers relevant to the research carried out. The validity
    of these algorithms depends on the usefulness of the information that they can
    extract from the database.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法的典型例子是搜索引擎。这些程序，给定一个或多个关键词，能够创建一个链接列表，这些链接指向搜索算法认为与所进行的研究相关的页面。这些算法的有效性取决于它们可以从数据库中提取的信息的有用性。
- en: Unsupervised learning techniques work by comparing data and looking for similarities
    or differences. As is known, machine learning algorithms try to imitate the functioning
    of an animal's nervous system. For this purpose, we can hypothesize that neural
    processes are guided by mechanisms that optimize the unknown objective they pursue.
    Each process evolves from an initial situation associated with a stimulus to a
    terminal in which there is an answer, which is the result of the process itself.
    It is intuitive that, in this evolution, there is a transfer of information. In
    fact, the stimulus provides the information necessary to obtain the desired response.
    Therefore, it is important that this information is transmitted as faithfully
    as possible until the process is completed. A reasonable criterion for interpreting
    the processes that take place in the nervous system is, therefore, to consider
    them as transfers of information with maximum preservation of the same.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习技术通过比较数据和寻找相似性或差异来工作。众所周知，机器学习算法试图模仿动物神经系统的功能。为此，我们可以假设神经过程是由优化他们追求的未知目标的机制所引导的。每个过程都从与刺激相关联的初始情况发展到终端，其中有一个答案，这是过程本身的结果。直观地讲，在这个过程中，存在信息传递。事实上，刺激提供了获得所需响应所需的信息。因此，在过程完成之前，尽可能忠实地传输这些信息是很重要的。因此，解释神经系统发生的过程的合理标准是，将它们视为信息传递，同时最大限度地保留相同的信息。
- en: Unsupervised learning algorithms are based on these concepts. It is a question
    of using learning theory techniques to measure the loss of information that has
    occurred in the transfer. The process under consideration is considered as the
    transmission of a signal through a noisy channel, using well-known techniques
    developed in the field of communications. It is possible, however, to follow a
    different approach based on a geometric representation of the process. In fact,
    both the stimulus and the response are characterized by an appropriate number
    of components, which in a space correspond to a point. Thus, the process can be
    interpreted as a geometric transformation of the input space to the output space.
    The exit space has a smaller size than the input space, as the stimulus contains
    the information necessary to activate many simultaneous processes. Compared to
    only one, it is redundant. This means that there is always a redundancy reduction
    operation in the transformation under consideration.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法基于这些概念。这是一个使用学习理论技术来衡量在传输过程中发生的信息损失的问题。考虑的过程被视为通过通信领域开发出的已知技术来传输信号的噪声信道。然而，也可以遵循基于过程几何表示的不同方法。实际上，刺激和响应都由适当数量的组件表征，这些组件在空间中对应于一个点。因此，这个过程可以解释为输入空间到输出空间的几何变换。输出空间的大小小于输入空间，因为刺激包含了激活许多同时进行的过程所需的信息。与只有一个相比，它是冗余的。这意味着在考虑的变换中始终存在冗余减少操作。
- en: In the entry and exit space, typical regions are formed, with which the information
    is associated. The natural mechanism that controls the transfer of information
    must therefore identify, in some way, these important regions for the process
    under consideration, and make sure that they correspond in the transformation.
    Thus, a data grouping operation is present in the process in question; this operation
    can be identified with the acquisition of experience. The two previous operations
    of grouping and reduction of redundancy are typical of optimal signal processing,
    and there is biological evidence of their existence in the functioning of the
    nervous system. It is interesting to note that these two operations are automatically
    achieved in the case of non-supervised learning based on experimental principles,
    such as competitive learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入和输出空间中，形成了典型的区域，信息与之相关联。因此，控制信息传输的自然机制必须以某种方式识别考虑过程中的这些重要区域，并确保它们在变换中相对应。因此，在所讨论的过程中存在数据分组操作；这个操作可以与经验的获得相等同。前两个分组和冗余减少操作是典型信号处理中的操作，有生物学证据表明它们存在于神经系统的功能中。值得注意的是，这两种操作在基于实验原则的非监督学习中是自动实现的，例如竞争学习。
- en: Generative models
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型
- en: A generative model aims to generate all the values of a phenomenon, both those
    that can be observed (input) and those that can be calculated from the ones observed
    (target). We try to understand how such a model can succeed in this goal by proposing
    a first distinction between generative and discriminative models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型旨在生成现象的所有值，包括可观察到的（输入）和可以从观察到的值中计算出的（目标）。我们试图通过提出生成模型和判别模型之间的第一个区别来理解这种模型如何实现这一目标。
- en: Often, in machine learning, we need to predict the value of a target vector
    *y* given the value of an input *x* vector. From a probabilistic perspective,
    the goal is to find the conditional probability distribution *p(y|x)*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们通常需要根据输入 *x* 向量的值预测目标向量 *y* 的值。从概率的角度来看，目标是找到条件概率分布 *p(y|x)*。
- en: The conditional probability of an event *y* with respect to an event *x* is
    the probability that *y* occurs, knowing that *x* is verified. This probability,
    indicated by *p(y|x)*, expresses a correction of expectations for *y*, dictated
    by the observation of *x*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 事件 *y* 关于事件 *x* 的条件概率是在已知 *x* 已验证的情况下 *y* 发生的概率。这个概率，用 *p(y|x)* 表示，表达了由 *x*
    的观察所决定的 *y* 的期望修正。
- en: The most common approach to this problem is to represent the conditional distribution
    using a parametric model, and then determine the parameters using a training set
    consisting of pairs (*xn*, *yn*) that contain both the values ​​of the input variables
    and the relative vectors of corresponding outputs. The resulting conditional distribution
    can be used to make predictions of the target (*y*) for new input values ​​(*x*).
    This is known as a **discriminatory approach**, since the conditional distribution
    discriminates directly between the different values ​​of *y*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的最常见方法是用参数模型表示条件分布，然后使用由包含输入变量的值和相应输出的相对向量的对 (*xn*, *yn*) 组成的训练集来确定参数。得到的条件分布可以用来对新输入值
    (*x*) 的目标 (*y*) 进行预测。这被称为**判别方法**，因为条件分布直接区分了 *y* 的不同值。
- en: As an alternative to this approach, we can look for the joint probability distribution
    *p(x∩ y)*, and then use this joint distribution to evaluate the conditional probability
    *p(y | x)* in order to make predictions of *y* for new values ​​of *x*. This is
    known as **generative approach**, because by sampling from the joint distribution,
    it is possible to generate synthetic examples of the vector of characteristics
    *x*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这种方法的替代方案，我们可以寻找联合概率分布 *p(x∩ y)*，然后使用这个联合分布来评估条件概率 *p(y | x)* 以便对新值 *x* 的
    *y* 进行预测。这被称为**生成方法**，因为通过从联合分布中采样，可以生成特征向量 *x* 的合成示例。
- en: The joint probability distribution *p(x, y)* is a probability distribution that
    gives the probability that each of *x*, *y* vectors falls in any particular range
    or discrete set of values specified for that variable.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 联合概率分布 *p(x, y)* 是一个概率分布，它给出了 *x* 和 *y* 向量中的每一个落在为该变量指定的任何特定范围或离散值集中的概率。
- en: 'A generative approach, regardless of the type of data and the theoretical model
    used, is divided into two basic steps:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 生成方法，无论数据类型和使用的理论模型如何，都分为两个基本步骤：
- en: The first step involves the construction of the generative model. The input
    data is processed with the aim of deducing their distribution. To do this, input
    data can simply be reorganized into a different structure, or it can represent
    new information extracted from input data from specific algorithms. The result
    of the construction of the generative model is the presentation of data according
    to the distribution to which it has been approximated.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步涉及生成模型的构建。输入数据被处理，目的是推导它们的分布。为此，输入数据可以简单地重新组织成不同的结构，或者它可以代表从输入数据中提取的新信息，这些信息来自特定的算法。生成模型构建的结果是根据其近似分布呈现数据。
- en: Once the generative model has been built on the input data, this allows sampling,
    which leads to the formation of new data that shares the same distribution with
    the input data.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦在输入数据上构建了生成模型，这允许采样，从而导致形成与输入数据具有相同分布的新数据。
- en: The construction of a generative model allows highlighting features and properties
    implicitly present in the initial data. The individual approaches are then distinguished
    by the type of processing performed on the data to explain these characteristics,
    and consequently for the type of variables on which an approximate data distribution
    is obtained.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型的构建允许突出显示初始数据中隐含的特征和属性。然后，根据对数据进行解释以说明这些特征的类型以及因此获得的近似数据分布的变量类型，区分不同的方法。
- en: 'Why are AI researchers so excited about generative models? Let''s take a simple
    example: suppose we provide the system with a series of images of cats. Suppose
    then, that after seeing these images, the computer is able to generate new photos
    of cats in a completely independent manner. If the computer were able to do it
    and the images that were produced had the right number of legs, tails, ears, and
    so on, it would be easy to prove that the computer knows which parts make up the
    cat, even if no one has ever explained cat anatomy to it. So, in a sense, a good
    generative model is proof of the basic knowledge of concepts by computers.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么人工智能研究人员对生成模型如此兴奋？让我们举一个简单的例子：假设我们向系统提供一系列猫的图片。假设在这些图片看过之后，计算机能够以完全独立的方式生成新的猫的照片。如果计算机能够做到这一点，并且产生的图像具有正确的腿、尾巴、耳朵等数量，那么很容易证明计算机知道哪些部分构成了猫，即使没有人向它解释过猫的解剖结构。因此，从某种意义上说，一个好的生成模型是计算机对概念基本知识的证明。
- en: This is why researchers are so enthusiastic about building generative models.
    These models seem to be a way to train computers to understand concepts without
    the need for researchers to teach them a priori concepts.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么研究人员对构建生成模型如此热情的原因。这些模型似乎是一种训练计算机理解概念的方法，无需研究人员事先教授它们概念。
- en: Restricted Boltzmann machine
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制性玻尔兹曼机
- en: A Boltzmann machine is a probabilistic graphic model that can be interpreted
    as a stochastic neural network. Boltzmann machines were first introduced in 1985
    by Geoffrey Hinton and Terry Sejnowski. **Stochastic** is due to the behavior
    of the neurons; within them, in the activation function, they will have a probabilistic
    value that will influence the activation of the neuron.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 玻尔兹曼机是一种概率图模型，可以解释为随机神经网络。玻尔兹曼机首次由杰弗里·辛顿和特里·谢诺夫斯基于1985年提出。"随机"一词源于神经元的行为；在它们内部，在激活函数中，它们将具有一个概率值，这将影响神经元的激活。
- en: 'In practice, a Boltzmann machine is a model (including a certain number of
    parameters) that, when applied to a data distribution, is able to provide a representation.
    This model can be used to extract important aspects of an unknown distribution
    (target distribution) starting only from a sample of the latter. The data samples
    referred to by a Boltzmann Machine are also called **training data**. The following
    diagram shows a Boltzmann machine''s architecture:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，玻尔兹曼机是一个模型（包括一定数量的参数），当应用于数据分布时，能够提供一种表示。该模型可以用来从目标分布（目标分布）的样本中提取未知分布的重要方面。玻尔兹曼机所引用的数据样本也称为**训练数据**。以下图显示了玻尔兹曼机的架构：
- en: '![](img/8e44f820-27f1-481c-88ad-ebe5c3822a5f.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8e44f820-27f1-481c-88ad-ebe5c3822a5f.png)'
- en: Training a Boltzmann machine means adapting its parameters so that the probability
    distribution represented by it interpolates the training data as best as possible.
    The training of a Boltzmann machine is a rather demanding work from a computational
    point of view. However, this problem can be made easier by imposing restrictions
    on the topology of the network on which you are working; this defines **Restricted
    Boltzmann machines** (**RBM**).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 训练玻尔兹曼机意味着调整其参数，以便它所表示的概率分布尽可能好地插值训练数据。从计算角度来看，玻尔兹曼机的训练是一项相当繁重的工作。然而，通过在工作网络的拓扑结构上施加限制，可以简化这个问题；这定义了**限制性玻尔兹曼机**（**RBM**）。
- en: 'In Boltzmann machines, there are two types of units:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在玻尔兹曼机中，有两种类型的单元：
- en: Visible units (or neurons since, as we said, a Boltzmann machine can be interpreted
    as a neural network)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可见单元（或神经元，因为正如我们所说，玻尔兹曼机可以解释为神经网络）
- en: Hidden units (or neurons)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏单元（或神经元）
- en: 'Even in RBMs, there are both of these types of units and we can imagine them
    as arranged on two levels:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在RBMs中，也存在这两种类型的单元，我们可以想象它们被安排在两个层面上：
- en: Visible units are the components of an observation (for example, if our data
    consists of images, we can associate a visible unit with each pixel)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可见单元是观察的组成部分（例如，如果我们的数据由图像组成，我们可以将一个可见单元与每个像素关联）
- en: Hidden units instead give us a model of the dependencies that exist between
    the components of our observation (for example, the dependency relationships that
    exist between the pixels of an image)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏单元为我们提供了一个关于观察（例如，图像像素之间的依赖关系）的组件之间存在的依赖关系的模型
- en: 'Hidden units can therefore be seen as detectors of data characteristics. In
    the RBM graph, every neuron is connected to all the neurons of the other level,
    while there are no connections between neurons of the same level; it is precisely
    this restriction that gives the RBM its name, as shown in the following diagram:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，隐藏单元可以被视为数据特征的检测器。在RBM图中，每个神经元都与另一层的所有神经元相连，而同一层的神经元之间没有连接；正是这种限制使得RBM得名，如下面的图所示：
- en: '![](img/b5015ef9-9fd2-4a71-9c23-0cb45759b829.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b5015ef9-9fd2-4a71-9c23-0cb45759b829.png)'
- en: After successful training, an RBM provides a very good representation of the
    distribution that underlies training data. It is a generative model that allows
    sampling new data starting from the learned distribution; for example, new image
    structures can be generated starting from studied images. Having a generative
    model makes useful applications possible. For example, you can think of integrating
    some visible units corresponding to a partial observation (that is, you fix the
    values of the observed variables and consider them constant) and then produce
    the remaining ones visible units to complete the observation; in the image analysis
    example, this can be useful for an image completion task.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功训练后，RBM提供了对训练数据下分布的非常好的表示。它是一个生成模型，允许从学习到的分布中采样新的数据；例如，可以从研究过的图像生成新的图像结构。拥有生成模型使得有用的应用成为可能。例如，你可以考虑整合一些对应于部分观察的可见单元（即，固定观察变量的值并认为它们是常数）然后产生剩余的可见单元以完成观察；在图像分析示例中，这可以用于图像补全任务。
- en: 'As generative models, RBMs can also be used as classifiers. Consider an application
    of this type:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作为生成模型，RBM也可以用作分类器。考虑这种类型的应用：
- en: RBM is trained to learn the joint probability distribution of the input data
    (explanatory variables) and the corresponding labels (response/output variables),
    both represented in the graph of the network, from the visible units of the RBM.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBM被训练来学习输入数据（解释变量）和相应的标签（响应/输出变量）的联合概率分布，这两个变量都在网络图中表示，从RBM的可见单元中学习。
- en: Subsequently, a new input pattern, this time without labels, can be linked to
    the visible variables. The corresponding labels can be predicted by sampling directly
    from the Boltzmann machine.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随后，可以链接一个新的输入模式，这次没有标签，到可见变量。相应的标签可以通过直接从霍尔兹曼机采样来预测。
- en: The Boltzmann machine is able to complete partial patterns of data on visible
    units. If we divide the visible units into units of input and output, given the
    input pattern, the Boltzmann machine completes it by producing the outputs (classification).
    Otherwise, it works as associative memory, returning the most similar pattern
    among those learned to the (partial) data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 霍尔兹曼机能够完成可见单元上的部分数据模式。如果我们把可见单元分为输入单元和输出单元，给定输入模式，霍尔兹曼机通过产生输出（分类）来完成它。否则，它作为关联记忆工作，返回学习到的模式中最相似的模式到（部分）数据。
- en: Boltzmann machine architecture
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 霍尔兹曼机架构
- en: 'Boltzmann machine architecture is based on input, output, and hidden nodes.
    The connection weights are symmetrical:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 霍尔兹曼机架构基于输入、输出和隐藏节点。连接权重是对称的：
- en: '![](img/6acae0d1-92b9-42d1-978d-72cb6705d75f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6acae0d1-92b9-42d1-978d-72cb6705d75f.png)'
- en: Based on this assumption, Boltzmann machines are highly recurrent, and this
    recurrence eliminates any basic difference between input and output nodes, which
    can be considered as input or output when needed. The Boltzmann machine is a network
    of units with an **energy** defined for the overall network. Its units produce
    binary results ((1,0) values). Outputs are computed probabilistically, and depend
    upon the temperature variable *T*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个假设，霍尔兹曼机高度递归，这种递归消除了输入节点和输出节点之间的任何基本差异，在需要时可以被视为输入或输出。霍尔兹曼机是一个为整个网络定义了**能量**的单元网络。其单元产生二元结果（(1,0)值）。输出是概率性地计算的，并依赖于温度变量*T*。
- en: 'The consensus function of the Boltzmann machine is given by the following formula:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 玻尔兹曼机的共识函数由以下公式给出：
- en: '![](img/14f55f75-1a92-4109-9e2f-16d8b98a6736.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/14f55f75-1a92-4109-9e2f-16d8b98a6736.png)'
- en: 'In the previous formula, the terms are defined as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，项的定义如下：
- en: '*S[i]* is the state of unit *i(1,0)*'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S[i]*是单元*i(1,0)*的状态'
- en: '*w[ij]* is the connection strength between unit *j* and unit *i*'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w[ij]*是单元*j*和单元*i*之间的连接强度'
- en: '*u[j]* is the output of unit *j*'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*u[j]*是单元*j*的输出'
- en: The calculation proceeds within the machine in a stochastic manner so that the
    consent is increased. Thus, if *w[ij]* is positive, there is a tendency to have
    units *i* and *j* both activated or both deactivated, while if the weight is negative,
    there is a tendency to have them with different activations (one activated and
    the other not). When a weight is positive, it is called **excitatory**; otherwise,
    it is called **inhibitory**.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 计算在机器中以随机方式进行，以便增加一致性。因此，如果*w[ij]*是正的，那么单元*i*和*j*同时激活或同时失活的趋势会增加，而如果权重是负的，那么它们具有不同激活（一个激活，另一个不激活）的趋势。当一个权重是正的时，它被称为**兴奋性**；否则，它被称为**抑制性**。
- en: 'Each binary unit makes a stochastic decision to be either 1 (with probability
    *p[i]*) or 0 (with probability *1- p[i]*). This probability is given by the following
    formula:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每个二元单元都会随机决定是1（概率*p[i]*）或0（概率*1- p[i]*）。这个概率由以下公式给出：
- en: '![](img/3d3077fe-3819-4235-b6cc-1247b5eab5f7.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3d3077fe-3819-4235-b6cc-1247b5eab5f7.png)'
- en: At the equilibrium state of the network, the likelihood is defined as the exponentiated
    negative energy, known as the **Boltzmann distribution**. You can imagine that
    by administering energy, you can get the system out of the local minima. This
    must be done slowly, because a violent shock can drive the system away from the
    global minimum. The best method is to give energy and then slowly reduce it. This
    concept is used in metallurgy, where an ordered state of the metal is obtained
    first by melting, and then slowly the temperature is reduced. The reduction in
    temperature as the process is under way is called **simulated annealing**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络的平衡状态下，似然被定义为指数化的负能量，称为**玻尔兹曼分布**。你可以想象，通过施加能量，你可以使系统摆脱局部最小值。这必须缓慢进行，因为剧烈的冲击可能会使系统远离全局最小值。最佳方法是先施加能量，然后缓慢减少。这个概念在冶金学中得到了应用，首先通过熔化获得金属的有序状态，然后缓慢降低温度。在过程进行中的温度降低被称为**模拟退火**。
- en: 'This method can be reproduced by adding a probabilistic update rule to the
    Hopfield network (refer to [Chapter 13](0fa9cfb2-9e84-4f95-b287-c28f1805cc97.xhtml),
    *Beyond Feedforward Networks – CNN and RNN*); the network that reproduces it is
    called **Boltzmann machine**. There will be a parameter that varies: the temperature.
    So at high *T*, the probability of jumping to a higher energy is much greater
    than at low temperatures.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以通过向Hopfield网络添加概率更新规则来重现（参见第13章，*超越前馈网络 - CNN和RNN*）；重现它的网络被称为**玻尔兹曼机**。将有一个参数会变化：温度。因此，在高温*T*下，跃迁到更高能量的概率远大于在低温下。
- en: 'When the temperature drops, the probability of assuming the correct minimum
    energy status approaches 1, and the network reaches the thermal equilibrium. Each
    unit of the network makes an energy leap given by the following formula:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当温度下降时，假设正确最小能量状态的概率接近1，网络达到热平衡。网络中的每个单元都会根据以下公式进行能量跃迁：
- en: '![](img/bdcee90a-5723-4c44-b7d3-fffdd91cdc9c.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bdcee90a-5723-4c44-b7d3-fffdd91cdc9c.png)'
- en: 'The system changes to a state of lower energy according to the following probabilistic
    rule (transition function):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 系统根据以下概率规则（转换函数）转变为更低能量的状态：
- en: '![](img/d3a92cb1-11e1-4512-98d1-fe158d8c23f5.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d3a92cb1-11e1-4512-98d1-fe158d8c23f5.png)'
- en: 'It is seen that the probability of transition to a higher energy state is greater
    at high *T* than at low *T*. The network can assume a configuration of stable
    states according to the following Boltzmann distribution:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，在高温*T*下，向更高能量状态的转换概率高于低温*T*。网络可以根据以下玻尔兹曼分布假设稳定状态配置：
- en: '![](img/5ba15569-d320-436d-a1b8-13c948c30e70.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5ba15569-d320-436d-a1b8-13c948c30e70.png)'
- en: That is, it depends on the energy of the state and temperature of the system.
    Lower energy states are more likely; in fact if *E[a] < E[b]*, then *P[a]/P[b]
    > 1*, because of which *P[a]>P[b]*. So the system tends toward a state of minimum
    energy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 即，它取决于状态的能量和系统的温度。低能量状态更可能；事实上，如果 *E[a] < E[b]*，则 *P[a]/P[b] > 1*，因此 *P[a]>P[b]*。所以系统倾向于向最低能量状态转变。
- en: Boltzmann machine disadvantages
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玻尔兹曼机缺点
- en: 'Numerous problems have emerged in the use of algorithms based on Boltzmann
    machines. The following are some of the problems encountered:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 基于玻尔兹曼机的算法在使用过程中出现了许多问题。以下是一些遇到的问题：
- en: Weight adjustment
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重调整
- en: The time needed to collect statistics in order to calculate probabilities,
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集统计信息以计算概率所需的时间，
- en: How many weights change at a time
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一次改变多少权重
- en: How to adjust the temperature during simulated annealing
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在模拟退火过程中调整温度
- en: How to decide when the network has reached the equilibrium temperature.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何决定网络何时达到平衡温度。
- en: The main disadvantage is that Boltzmann learning is significantly slower than
    backpropagation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 主要缺点是玻尔兹曼学习比反向传播慢得多。
- en: Deep Boltzmann machines
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度玻尔兹曼机
- en: 'Another type of Boltzmann Machine is **Deep Boltzmann machine** (**DBM**).
    This is a neural network similar to RBM, but instead of having only one layer
    of hidden nodes, DBMs have many. Each layer of neurons is connected only to those
    adjacent (the one immediately preceding and immediately following); here also,
    the neurons of the same layer are not interconnected. This structure allows the
    emergence of particular statistics from each layer that can capture new data features.
    The following diagram shows a DBM model with one visible layer and two hidden
    layers:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种类型的玻尔兹曼机是**深度玻尔兹曼机**（**DBM**）。这是一个类似于RBM的神经网络，但它不仅仅只有一个隐藏层节点，DBM有很多。每个神经层只与相邻层（立即前一个和立即后一个）连接；在这里，同一层的神经元也不相互连接。这种结构使得每个层都能产生特定的统计信息，从而能够捕捉新的数据特征。以下图显示了具有一个可见层和两个隐藏层的DBM模型：
- en: '![](img/cec3294c-9538-4bc9-9c3d-cbcd6597215e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cec3294c-9538-4bc9-9c3d-cbcd6597215e.png)'
- en: As we can see, connections are only between units in neighboring layers. Like
    RBMs and DBMs contain only binary units.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，连接仅存在于相邻层之间的单元之间。像RBM和DBM只包含二进制单元。
- en: 'The DBMs model assigns the following probability to a visible vector *v*:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: DBMs模型为可见向量 *v* 分配以下概率：
- en: '![](img/246b4b27-7a6d-4e45-aefe-fb19b9e2d4b6.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/246b4b27-7a6d-4e45-aefe-fb19b9e2d4b6.png)'
- en: 'In the previous formula, the terms are defined as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，术语定义如下：
- en: '*v* is the visible vector'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*v* 是可见向量'
- en: '*θ = (W(1),W(2))* are the model parameters, representing visible-to-hidden
    and hidden-to-hidden symmetric interaction terms'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*θ = (W(1),W(2))* 是模型参数，代表可见到隐藏和隐藏到隐藏的对称交互项'
- en: '*h^((1))* and *h^((2))* are hidden stochastic binary variables'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h^((1))* 和 *h^((2))* 是隐藏的随机二进制变量'
- en: '*Z(θ)* is the partition function'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Z(θ)* 是配分函数'
- en: DBMs are particularly useful in the case of the recognition of objects or words.
    This is due to the great ability to learn complex and abstract internal representations
    using little labeled input data, instead of exploiting a large amount of unlabeled
    input data. However, unlike deep convolutional neural networks, DBMs adopt the
    inference and training procedure in both directions to better detect representations
    of input structures.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别对象或单词的情况下，DBMs特别有用。这是由于使用少量标记输入数据学习复杂和抽象的内部表示的强大能力，而不是利用大量未标记的输入数据。然而，与深度卷积神经网络不同，DBMs在双向推理和训练过程中都采用，以更好地检测输入结构的表示。
- en: Autoencoder
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器
- en: 'An autoencoder is a neural network whose purpose is to code its input into
    small dimensions and the result obtained, to be able to reconstruct the input
    itself. Autoencoders are made up of the union of the following two subnets:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一种神经网络，其目的是将输入编码成小维度，并得到的结果能够重建输入本身。自编码器由以下两个子网组成：
- en: 'Encoder, which calculates the function:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器，它计算以下函数：
- en: '*z = ϕ(x)*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*z = ϕ(x)*'
- en: Given an input *x*, the encoder encodes it in a variable *z*, also called **latent
    variable**. *z* usually has much smaller dimensions than *x*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入 *x*，编码器将其编码到变量 *z* 中，也称为**潜在变量**。*z*通常比 *x* 的维度小得多。
- en: 'Decoder, which calculates the following function:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器，它计算以下函数：
- en: '*x'' = ψ(z)*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*x'' = ψ(z)*'
- en: Since *z* is the code of *x* produced by the encoder, the decoder must decode
    it so that *x'* is similar to *x*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *z* 是编码器产生的 *x* 的代码，解码器必须将其解码，以便 *x'* 与 *x* 相似。
- en: The training of autoencoders is intended to minimize the mean square error between
    the input and the result.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的训练旨在最小化输入和结果之间的均方误差。
- en: '**Mean Squared Error** (**MSE**) is the average squared difference between
    the outputs and targets. Lower values are indicative of better results. Zero means
    no error.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）是输出和目标之间的平均平方差。较低的值表示更好的结果。零表示没有错误。'
- en: 'For n observations, *MSE* is given by the following formula:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 n 个观测值，*MSE* 由以下公式给出：
- en: '![](img/9c0e222d-4550-4c01-ba34-712bc3d26588.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c0e222d-4550-4c01-ba34-712bc3d26588.png)'
- en: 'Finally, we can summarize that the encoder encodes the input in a compressed
    representation and the decoder returns from it a reconstruction of the input,
    as shown in the following diagram:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以总结说，编码器将输入编码为压缩表示，解码器从它返回输入的重建，如下面的图所示：
- en: '![](img/b4d01427-e444-48e4-9ff6-19e7f4da2887.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4d01427-e444-48e4-9ff6-19e7f4da2887.png)'
- en: 'Let’s define the following terms:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义以下术语：
- en: '*W*: input → hidden weights'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W*: 输入 → 隐藏权重'
- en: '*V*: hidden → output weights'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V*: 隐藏 → 输出权重'
- en: 'The previous formulas become:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的公式变为：
- en: '*z = ϕ(W* x)*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*z = ϕ(W* x)*'
- en: 'And they also become:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 并且它们也变为：
- en: '*x'' = ψ(V*W1* x)*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*x'' = ψ(V*W1* x)*'
- en: 'Finally, the training of autoencoders is intended to minimize the following
    quantity:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，自编码器的训练旨在最小化以下量：
- en: '![](img/599757b5-83ce-4735-9e7d-76dd298d25f6.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/599757b5-83ce-4735-9e7d-76dd298d25f6.png)'
- en: The purpose of autoencoders is not simply to perform a sort of compression of
    the input or to look for an approximation of the identity function. There are
    techniques that allow, starting from a hidden layer of reduced dimensions, to
    direct the model to give greater importance to some data properties, thus giving
    rise to different representations based on the same data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的目的不仅仅是执行一种对输入的压缩或寻找恒等函数的近似。有一些技术可以从一个降低维度的隐藏层开始，指导模型给予某些数据属性更大的重要性，从而基于相同的数据产生不同的表示。
- en: Variational autoencoder
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: '**Variational autoencoder** (**VAE**) are inspired by the concept of Autoencoder:
    a model consisting of two neural networks called **encoders** and **decoders**.
    As we have seen, the encoder network tries to code its input in a compressed form,
    while the network decoder tries to reconstruct the initial input, starting from
    the code returned by the encoder.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**变分自编码器**（**VAE**）受自编码器概念的影响：由两个称为**编码器**和**解码器**的神经网络组成的模型。正如我们所见，编码器网络试图以压缩的形式编码其输入，而解码器网络则试图从编码器返回的代码开始重建初始输入。'
- en: However, the functioning of the VAE is very different than that of simple autoencoders.
    VAEs allow not only coding/decoding of input but also generating new data. To
    do this, they treat both the code *z* and the reconstruction/generation *x'* as
    if they belonged to a certain probability distribution. In particular, the VAEs
    are the result of the combination of deep learning and Bayesian inference, in
    the sense that they consist of a neural network trained with the backpropagation
    algorithm modified with a technique called **re-parameterization**. While deep
    learning has proven to be very effective in the approximation of complex functions,
    the Bayesian statistics allow managing the uncertainty derived from a random generation
    in the form of probabilities.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，变分自编码器（VAE）的功能与简单的自编码器非常不同。VAE不仅允许对输入进行编码/解码，还可以生成新的数据。为此，它们将代码 *z* 和重建/生成
    *x'* 视为属于某个概率分布的一部分。特别是，VAE是深度学习和贝叶斯推理相结合的结果，因为它们由一个使用称为**重参数化**技术的反向传播算法修改后的神经网络训练而成。虽然深度学习已被证明在复杂函数逼近方面非常有效，但贝叶斯统计允许以概率的形式管理随机生成的不确定性。
- en: The VAE uses the same structure to generate new images, similar to those belonging
    to the training set. In this case, the encoder does not directly produce a code
    for a given input but calculates the mean and variance of a normal distribution.
    A value is taken from this distribution and it is decoded by the decoder. The
    training consists of modifying the encoder and decoder parameters so that the
    result of the decoded so carried out is as similar as possible to the starting
    image. At the end of the training, we have that starting from the normal distribution
    with mean and variance produced by the encoder; the decoder will be able to produce
    images similar to those belonging to the training set.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 使用与训练集相似的相同结构来生成新的图像。在这种情况下，编码器不会直接为给定的输入生成一个代码，而是计算正态分布的均值和方差。从这个分布中取一个值，然后由解码器进行解码。训练包括修改编码器和解码器参数，以便解码的结果尽可能接近起始图像。训练结束时，我们有从编码器产生的均值和方差的正态分布开始；解码器将能够生成与训练集相似的图像。
- en: 'Let''s define the following terms:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义以下术语：
- en: '*X*: Input data vector'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X*: 输入数据向量'
- en: '*z*: Latent variable'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*z*: 潜在变量'
- en: '*P(X)* : Probability distribution of the data'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(X)*: 数据的概率分布'
- en: '*P(z)*: Probability distribution of the latent variable'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(z)*: 潜在变量的概率分布'
- en: '*P(X|z)*: Posterior probability, that is, the distribution of generating data
    given the latent variable'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(X|z)*: 后验概率，即给定潜在变量生成数据的分布'
- en: The posterior probability *P(X|z)* is the probability of the condition *X* given
    the evidence *z*.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 后验概率 *P(X|z)* 是在证据 *z* 下 *X* 的概率。
- en: 'Our goal is to generate data according to the characteristics contained in
    the latent variable, so we want to find *P(X)*. For this purpose, we can use the
    law of total probability according to the following formula:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是根据潜在变量中包含的特征生成数据，因此我们想要找到 *P(X)*。为此，我们可以使用以下公式的全概率定律：
- en: '![](img/8b70972e-9595-4b55-9d90-53a6db072c2a.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b70972e-9595-4b55-9d90-53a6db072c2a.png)'
- en: 'To understand how we arrived at this formulation, we reason by step. Our first
    task in defining the model is to infer good values of the latent variables starting
    from the observed data, or to calculate the posterior *p(z|X)*. To do this, we
    can use the Bayes theorem:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解我们是如何得到这个公式的，我们逐步进行推理。在定义模型的第一项任务是从观察数据开始推断潜在变量的良好值，或者计算后验 *p(z|X)*。为此，我们可以使用贝叶斯定理：
- en: '![](img/4d46bbe0-263d-468d-add4-a5eb6764734d.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d46bbe0-263d-468d-add4-a5eb6764734d.png)'
- en: 'In the previous formula, the *P(X)* term appears. In the context of Bayesian
    statistics, it may also be referred to as the evidence or model evidence. The
    evidence can be calculated by marginalizing out the latent variables. This brings
    us to the starting formula:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，出现了 *P(X)* 项。在贝叶斯统计的背景下，它也可能被称为证据或模型证据。证据可以通过对潜在变量进行边缘化来计算。这使我们回到了起始公式：
- en: '![](img/e0fe3410-7faa-4200-a3e5-2578aa13fff0.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0fe3410-7faa-4200-a3e5-2578aa13fff0.png)'
- en: The computational estimate of this integral requires an exponential time as
    it must be evaluated on all the configurations of latent variables. To reduce
    the computational cost, we are forced to approximate the estimate of the posterior
    probability.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个积分的计算估计需要指数级的时间，因为它必须在所有潜在变量的配置上进行评估。为了降低计算成本，我们被迫对后验概率的估计进行近似。
- en: In VAE, as the name suggests, we deduce *p(z | X)* using a method called **variational
    inference** (**VI**). VI is one of the most used methods in Bayesian inference.
    This technique considers inference as an optimization problem. In doing this,
    we use a simpler distribution that is easy to evaluate (for example, Gaussian)
    and minimize the difference between these two distributions using the **Kullback-Leibler
    divergence metric**.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在变分自编码器（VAE）中，正如其名所示，我们使用一种称为**变分推断**（**VI**）的方法来推断 *p(z | X)*。VI 是贝叶斯推理中最常用的方法之一。这种技术将推断视为一个优化问题。在这样做的时候，我们使用一个简单且易于评估的分布（例如高斯分布），并使用
    **Kullback-Leibler 散度度量**来最小化这两个分布之间的差异。
- en: Kullback-Leibler divergence metric is a non-symmetric measure of the difference
    between two probability distributions *P* and *Q*. Specially, the Kullback-Leibler
    divergence of *Q* from *P*, denoted by **DKL** *(P ||Q)*, is the measurement of
    the information lost when *Q* is used to approximate *P*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Kullback-Leibler 散度度量是两个概率分布 *P* 和 *Q* 之间非对称差异的度量。特别地，从 *P* 到 *Q* 的 Kullback-Leibler
    散度，记为 **DKL** *(P ||Q)*，是当使用 *Q* 来近似 *P* 时损失的信息的度量。
- en: 'For discrete probability distributions *P* and *Q*, the Kullback-Leibler divergence
    from *Q* to *P* is defined as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离散概率分布*P*和*Q*，从*Q*到*P*的Kullback-Leibler散度定义为以下：
- en: '![](img/57583f96-c918-48a5-9f01-1f29c375b290.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57583f96-c918-48a5-9f01-1f29c375b290.png)'
- en: Analyzing the formula makes it evident that the divergence of Kullback-Leibler
    is the expectation of the logarithmic difference between the probabilities *P*
    and *Q*, where the expectation is taken using the probability *P*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 分析公式可以清楚地看出，Kullback-Leibler散度是概率*P*和*Q*之间对数差异的期望，这里的期望是使用概率*P*来计算的。
- en: Generative adversarial network
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: '**Generative adversarial network** (**GAN**) is a generative model consisting
    of two networks that are jointly trained, called **generator** and **discriminator**.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GAN**）是由两个共同训练的网络组成的生成模型，称为**生成器**和**判别器**。'
- en: 'The dynamics between these two networks are like those between a forger and
    an investigator. The forger tries to produce faithful imitations of authentic
    works of art, while the investigator tries to distinguish the fakes from the originals.
    In this analogy, the forger represents the generator and the investigator represents
    the discriminator. The generator accepts input values ​​belonging to a fixed distribution
    and tries to produce images similar to those of the dataset. The discriminator
    tries to distinguish the data created by the generator from those belonging to
    the dataset. These two networks are jointly coached:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个网络之间的动力学类似于伪造者和调查者之间的关系。伪造者试图制作对真实艺术作品的忠实模仿，而调查者则试图区分赝品和真品。在这个类比中，伪造者代表生成器，调查者代表判别器。生成器接受属于固定分布的输入值，并试图生成与数据集相似的图像。判别器试图区分生成器创建的数据与属于数据集的数据。这两个网络是共同训练的：
- en: The discriminator tries to return output = 1 if the input belongs to the dataset
    and returns 0 if its input was generated by the generator
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果输入属于数据集，判别器试图返回输出=1，如果其输入是由生成器生成的，则返回0
- en: The generator instead tries to maximize the possibility that the discriminator
    will make mistakes
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器则试图最大化判别器犯错的概率
- en: 'The generator acquires a random input noise and tries to create a sample of
    data, while the discriminator takes input from either real-world examples or the
    generator, as shown in the following diagram:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器获取随机输入噪声并试图创建数据样本，而判别器从现实世界的示例或生成器获取输入，如下面的图所示：
- en: '![](img/bc17de32-bdec-4266-9b6a-3cd00a7a346a.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc17de32-bdec-4266-9b6a-3cd00a7a346a.png)'
- en: For simplicity, the two opposing networks are of the multilayer perceptron type;
    however, the same structure can be modeled with deep networks. For example, to
    generate new images, instead of sampling data from a complex distribution, the
    approach used in these networks is to start from values belonging to a simple
    distribution or from random values. Subsequently, they are mapped through a second
    distribution that will be learned during the training.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，这两个对抗网络是多层感知器类型；然而，可以使用深度网络来模拟相同的结构。例如，为了生成新的图像，而不是从复杂的分布中采样数据，这些网络中使用的方法是从属于简单分布的值或从随机值开始。随后，它们通过将在训练过程中学习的第二个分布进行映射。
- en: 'In such a system, training leads to constant competition between generator
    and discriminator. Under these conditions, the optimization process can be carried
    out independently on both sides. Naming *G(z)* the generator and *D(x)* the discriminator,
    the training of the model aims to maximize the probability of the discriminator
    to assign 1 to values coming from the training set, instead of 0 to those produced
    by the generator. On the other hand, we want to teach the generator to minimize
    the following quantity:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的系统中，训练会导致生成器和判别器之间持续的竞争。在这些条件下，优化过程可以在双方独立进行。将生成器命名为*G(z*)，判别器命名为*D(x*)，模型的训练目标是最大化判别器将1分配给来自训练集的值的概率，而不是将0分配给生成器产生的值。另一方面，我们希望教会生成器最小化以下量：
- en: '![](img/6001f561-7c54-47ba-bee9-755125f7f352.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6001f561-7c54-47ba-bee9-755125f7f352.png)'
- en: 'The training is then performed by applying the gradient descent technique to
    the following expression:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 训练是通过将梯度下降技术应用于以下表达式来进行的：
- en: '![](img/f02edaee-d830-4920-a698-7b99d1410767.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f02edaee-d830-4920-a698-7b99d1410767.png)'
- en: This method originates from game theory, in particular from the method called
    **two-player minimax game**. The algorithms of this type adopt the strategy of
    minimizing the maximum possible loss resulting from the choice of a player. It
    can happen that, in the training process, the discriminator is not able to classify
    examples generated by real ones.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法源于博弈论，特别是称为**两人最小-最大博弈**的方法。这类算法采用最小化玩家选择可能造成的最大损失的策略。在训练过程中，判别器可能无法对由真实数据生成的示例进行分类。
- en: Adversarial autoencoder
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗自编码器
- en: '**Adversarial autoencode**r (**AAE**) is a generative model produced by the
    union of VAE and GAN. To explain the model, we start by defining the following
    terms:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**对抗自编码器**（**AAE**）是VAE和GAN结合产生的生成模型。为了解释该模型，我们首先定义以下术语：'
- en: '*x*: Autoencoder input'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*：自编码器输入'
- en: 'z: Code produced from *x*,'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: z：从 *x* 生成的代码，
- en: '*p(z)*: The distribution we want to impose'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(z)*：我们想要施加的分布'
- en: '*q(z|x)*: Distribution learned from the encoder'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*q(z|x)*：从编码器学习到的分布'
- en: '*p(x|z)*: Distribution learned from the decoder'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(x|z)*：从解码器学习到的分布'
- en: '*pdata*: Distribution of the data'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*pdata*：数据的分布'
- en: '*p(x)*: Distribution of the model'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(x)*：模型的分布'
- en: 'We consider the function *q(z|x)* as a posterior distribution of *q(z)*, which
    is defined as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将函数 *q(z|x)* 视为 *q(z)* 的后验分布，其定义如下：
- en: '![](img/09520493-7e28-4d45-86c5-e5a66cde4a86.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/09520493-7e28-4d45-86c5-e5a66cde4a86.png)'
- en: 'We try to impose the equality *q(z)=p(z)* on the model. The difference with
    a VAE is due to the fact that what drives *q (z)* towards *p(z)* is an adversarial
    network. The encoder of the VAE is considered the generator of a GAN for which
    a discriminator can be used. This tries to distinguish data belonging to *q(z)*
    from that coming from *p(z)*. The following diagram shows an AAE architecture:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图将等式 *q(z)=p(z)* 强加于模型。与VAE的不同之处在于，驱动 *q (z)* 向 *p(z)* 靠近的是对抗网络。VAE的编码器被视为GAN的生成器，对于该生成器可以使用判别器。这试图区分属于
    *q(z)* 的数据与来自 *p(z)* 的数据。以下图显示了AAE架构：
- en: '![](img/e48353cc-d8e6-4f7b-b6de-566fa7919e91.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e48353cc-d8e6-4f7b-b6de-566fa7919e91.png)'
- en: The trainings of the adversarial network and of the autoencoder take place jointly,
    using stochastic gradient descent.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗网络和自编码器的训练是联合进行的，使用随机梯度下降。
- en: Feature extraction using RBM
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RBM进行特征提取
- en: Recently, several types of **artificial neural networks** (**ANNs**) have been
    applied to classify a specific dataset. However, most of these models use only
    a limited number of features as input, in which case there may not be enough information
    to make the prediction due to the complexity of the starting dataset. If you have
    more features, the run time of training would be increased and generalization
    performance would deteriorate due to the curse of dimesionality. In these cases,
    a tool to extract the characteristics would be particularly useful. RBM is a machine
    learning tool with a strong representation power, which is often used as a feature
    extractor in a wide variety of classification problems.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，几种类型的**人工神经网络**（**ANNs**）已被应用于对特定数据集进行分类。然而，这些模型中的大多数只使用有限数量的特征作为输入，在这种情况下，由于起始数据集的复杂性，可能没有足够的信息来做出预测。如果你有更多特征，训练的运行时间会增加，并且由于维度诅咒，泛化性能会下降。在这些情况下，一个用于提取特征的工具会特别有用。RBM是一种具有强大表示能力的机器学习工具，通常用作各种分类问题中的特征提取器。
- en: Breast cancer dataset
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 乳腺癌数据集
- en: The breast is made up of a set of glands and adipose tissue, and is located
    between the skin and the chest wall. In fact, it is not a single gland but a set
    of glandular structures, called **lobules**, joined together to form a lobe. In
    a breast, there are 15 to 20 lobes. The milk reaches the nipple from the lobules
    through small tubes called **milk ducts**.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 乳房由一组腺体和脂肪组织组成，位于皮肤和胸壁之间。实际上，它不是一个单一的腺体，而是一组腺体结构，称为**小叶**，它们联合起来形成一个叶。在乳房中，有15到20个小叶。牛奶通过称为**乳管**的小管道从小叶流向乳头。
- en: Breast cancer is a potentially serious disease if it is not detected and treated.
    It is caused by uncontrolled multiplication of some cells in the mammary gland
    that are transformed into malignant cells. This means that they have the ability
    to detach themselves from the tissue that has generated them to invade the surrounding
    tissues and eventually other organs of the body. In theory, cancers can be formed
    from all types of breast tissues, but the most common ones are from glandular
    cells or from those forming the walls of the ducts.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌如果未被发现和治疗，可能是一种严重的疾病。它是由乳腺中某些细胞不受控制地增殖并转化为恶性细胞引起的。这意味着它们具有从产生它们的组织中脱离并侵犯周围组织以及最终侵犯身体其他器官的能力。理论上，癌症可以由所有类型的乳腺组织形成，但最常见的是由腺细胞或形成导管壁的细胞形成。
- en: The objective of this example is to identify each of a number of benign or malignant
    classes. To do this, we will use the data contained in the dataset named BreastCancer
    (Wisconsin Breast Cancer database). This data has been taken from the UCI Repository
    of machine learning databases as DNA samples arrive periodically, as Dr. Wolberg
    reports his clinical cases. The database therefore reflects this chronological
    grouping of the data. This grouping information appears immediately, having been
    removed from the data itself. Each variable, except for the first, was converted
    into 11 primitive numerical attributes with values ranging from zero through ten.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 本例的目标是识别多个良性或恶性类别中的每一个。为此，我们将使用名为“乳腺癌”（威斯康星乳腺癌数据库）的数据集中的数据。这些数据是从UCI机器学习数据库仓库中获取的，因为DNA样本定期到达，正如Wolberg博士报告他的临床病例一样。因此，数据库反映了这种数据的按时间顺序分组。这种分组信息立即出现，因为已经被从数据本身中移除。除了第一个变量之外，每个变量都被转换成了11个原始数值属性，其值从零到十不等。
- en: 'To get the data, we draw on the large collection of data available in the UCI
    Machine Learning Repository at the following link: [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取数据，我们借鉴了以下链接中UCI机器学习仓库的大量数据集：[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)。
- en: To load the dataset, we will use the `sklearn.datasets` module. It includes
    utilities to load datasets, including methods to load and fetch popular reference
    datasets. It also features some artificial data generators.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载数据集，我们将使用`sklearn.datasets`模块。它包括用于加载数据集的实用工具，包括加载和检索流行参考数据集的方法。它还提供了一些人工数据生成器。
- en: 'The breast cancer dataset is a classic and very easy binary classification
    dataset. The following table has some information about the dataset:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌数据集是一个经典且非常容易的二分类数据集。以下表格提供了一些关于数据集的信息：
- en: '| Classes | 2 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 2 |'
- en: '| Samples per class | 212(M), 357(B) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 每类样本数 | 212(M), 357(B) |'
- en: '| Samples total | 569 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 样本总数 | 569 |'
- en: '| Dimensionality | 30 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 维度 | 30 |'
- en: '| Features | real and positive |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 实数和正数 |'
- en: Data preparation
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'After introducing the breast cancer dataset, we can analyze the code that will
    allow us to classify the input data line by line. In the first part of the code,
    we import the libraries we will use later:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了乳腺癌数据集之后，我们可以分析代码，这将使我们能够逐行对输入数据进行分类。在代码的第一部分，我们导入稍后将要使用的库：
- en: '[PRE0]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For now, let''s limit ourselves to import; we will deepen them at the time
    of use. To start, we have to import the dataset; we will do so using the `sklearn.datasets`
    package:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，让我们仅限于导入；我们将在使用时进一步深入研究。首先，我们必须导入数据集；我们将使用`sklearn.datasets`包来完成：
- en: '[PRE1]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This command loads and returns the breast cancer `wisconsin` dataset. The `sklearn.datasets`
    package embeds some small toy datasets. To evaluate the impact of the scale of
    the dataset (`n_samples` and `n_features`) while controlling the statistical properties
    of the data (typically the correlation and informativeness of the features), it
    is also possible to generate synthetic data. This package also features helpers
    to fetch larger datasets commonly used by the machine learning community to benchmark
    algorithm on data that comes from the real world. A dataset is a dictionary-like
    object that holds all the data and some metadata about the data. This data is
    stored in the data member, which is a `n_samples` and `n_features` array. In the
    case of a supervised problem, one or more response variables are stored in the
    target member.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令加载并返回乳腺癌`wisconsin`数据集。`sklearn.datasets`包嵌入了一些小型玩具数据集。为了评估数据集的规模（`n_samples`和`n_features`）对数据统计属性（通常是特征的相关性和信息性）的影响，同时生成合成数据也是可能的。此包还提供了一些辅助工具，用于获取机器学习社区常用的大型数据集，以在来自真实世界的数据上基准测试算法。数据集是一个类似字典的对象，包含所有数据和有关数据的元数据。这些数据存储在数据成员中，它是一个`n_samples`和`n_features`数组。在监督问题的情况下，一个或多个响应变量存储在目标成员中。
- en: 'Data is returned in a `Bunch` object, a dictionary-like object that contains
    the following attributes:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以`Bunch`对象的形式返回，这是一个类似字典的对象，包含以下属性：
- en: '`data`: The data to learn'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data`: 要学习的数据'
- en: '`target`: The classification labels'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target`: 分类标签'
- en: '`target_names`: The meaning of the labels'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_names`: 标签的含义'
- en: '`feature_names`: The meaning of the features'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_names`: 特征的含义'
- en: '`DESCR`: The full description of the dataset'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DESCR`: 数据集的完整描述'
- en: 'To confirm the content of the data, let''s extract the dimensions:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认数据的内容，让我们提取维度：
- en: '[PRE2]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The results are listed as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '[PRE3]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To better understand the operations, we divide these data into `X` (predictors)
    and `Y` (target):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解操作，我们将这些数据分为`X`（预测变量）和`Y`（目标）：
- en: '[PRE4]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At this point, we extract a series of statistics from the predictors using the
    tools that make available to us the `pandas` library.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们使用提供给我们`pandas`库的工具从预测变量中提取一系列统计信息。
- en: '`pandas` is an open source, BSD-licensed library providing high-performance,
    easy-to-use data structures and data analysis tools for the Python programming
    language.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas`是一个开源的、BSD许可的库，为Python编程语言提供高性能、易于使用的数据结构和数据分析工具。'
- en: 'To use this function, we have to convert the input data from `numpy.darray`
    to `pandas` dataframe:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此功能，我们必须将输入数据从`numpy.darray`转换为`pandas`数据框：
- en: '[PRE5]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The results are shown as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '[PRE6]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Due to space constraints, we have reported only the results for the first five
    predictors. As we can see, the variables have different ranges. When the predictors
    have different ranges, the impact on response variables by the feature having
    a greater numeric range could be more than the one having a less numeric range,
    and this could, in turn, impact the prediction accuracy. Our goal is to improve
    predictive accuracy and not allow a particular feature to impact the prediction
    due to a large numeric value range. Thus, we may need to scale values under different
    features such that they fall under a common range. Through this statistical procedure,
    it is possible to compare identical variables belonging to different distributions
    and also different variables or variables expressed in different units.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '由于空间限制，我们只报告了前五个预测变量的结果。正如我们所看到的，变量有不同的范围。当预测变量有不同的范围时，具有较大数值范围的特征的响应变量的影响可能比具有较小数值范围的特征的影响更大，这反过来又可能影响预测精度。我们的目标是提高预测精度，不允许某个特征由于较大的数值范围而影响预测。因此，我们可能需要将不同特征下的值缩放到一个共同的范围内。通过这个统计过程，可以比较属于不同分布的相同变量，也可以比较不同的变量或以不同单位表示的变量。 '
- en: Remember, it is good practice to rescale the data before training a machine
    learning algorithm. With rescaling, data units are eliminated, allowing you to
    easily compare data from different locations.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在训练机器学习算法之前对数据进行缩放是一种良好的实践。通过缩放，消除了数据单位，使得您能够轻松地比较来自不同位置的数据。
- en: 'In this case, we will use the min-max method (usually called **feature scaling**)
    to get all the scaled data in the range (0, 1). The formula to achieve this is:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用最小-最大方法（通常称为**特征缩放**）来获取所有缩放数据在范围（0，1）内。实现此目的的公式是：
- en: '![](img/4d0fa157-043f-480a-8c67-3356ced2ab01.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4d0fa157-043f-480a-8c67-3356ced2ab01.png)'
- en: 'The following command performs a feature scaling:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行特征缩放：
- en: '[PRE7]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`numpy.min()` and `numpy.max()` are used to calculate the minimum and maximum
    values of each database column.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`numpy.min()` 和 `numpy.max()` 用于计算每个数据库列的最小值和最大值。'
- en: 'Let''s now split the data for the training and the test models. Training and
    testing the model forms the basis for further usage of the model for prediction
    in predictive analytics. Given a dataset of 100 rows of data, which includes the
    predictor and response variables, we split the dataset into a convenient ratio
    (say 80:20), and allocate 80 rows for training and 20 rows for testing. The rows
    are selected at random to reduce bias. Once the training data is available, the
    data is fed to the machine learning algorithm to get the massive universal function
    in place. To split the dataset, we will use the `sklearn.model_selection.train_test_split()`
    function:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将数据分割成训练和测试模型。训练和测试模型是进一步使用模型进行预测分析的基础。给定一个包含预测变量和响应变量的100行数据集，我们将数据集分割成方便的比例（比如说80:20），并分配80行用于训练，20行用于测试。行是随机选择的，以减少偏差。一旦有了训练数据，数据就被输入到机器学习算法中，以获得大规模的通用函数。为了分割数据集，我们将使用
    `sklearn.model_selection.train_test_split()` 函数：
- en: '[PRE8]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `train_test_split()` function splits arrays or matrices into random train
    and test subsets. The first two arguments are `X` (predictors) and `Y` (target)
    numpy arrays. Allowed inputs are lists, `numpy` arrays, scipy-sparse matrices,
    or `pandas` dataframes. Then two options are added:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_test_split()` 函数将数组或矩阵分割成随机的训练和测试子集。前两个参数是 `X`（预测变量）和 `Y`（目标）numpy 数组。允许的输入包括列表、`numpy`
    数组、scipy 稀疏矩阵或 `pandas` 数据框。然后添加了两个选项：'
- en: '`test_size`: This should be between 0.0 and 1.0 and represent the proportion
    of the dataset to include in the test split'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_size`：这应该在0.0和1.0之间，并代表要包含在测试分割中的数据集比例'
- en: '`random_state`: This is the seed used by the random number generator'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_state`：这是随机数生成器使用的种子'
- en: Model fitting
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型拟合
- en: We have previously said that RBM is often used as a feature in a wide variety
    of classification problems. It's time to see how to do it. The first thing to
    do is to use the `BernoulliRBM` function of the `sklearn.neural_network` module.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到 RBM 经常被用作各种分类问题中的特征。现在是时候看看如何做了。首先要做的是使用 `sklearn.neural_network` 模块的
    `BernoulliRBM` 函数。
- en: '`sklearn` is a free machine learning library for the Python programming language.
    It features various classification, regression, and clustering algorithms, including
    support vector machines, random forests, gradient boosting, k-means, and DBSCAN.
    And it is designed to interoperate with the Python numerical and scientific libraries
    NumPy and SciPy.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn` 是一个用于Python编程语言的免费机器学习库。它具有各种分类、回归和聚类算法，包括支持向量机、随机森林、梯度提升、k-means和DBSCAN。它旨在与Python数值和科学库NumPy和SciPy交互操作。'
- en: 'In the `sklearn` library, the `sklearn.neural_network` module includes models
    based on neural networks. In this module, the `BernoulliRBM` function fits a Bernoulli
    RBM. An RBM with binary visible units and binary hidden units is returned. The
    parameters are estimated using **Stochastic Maximum Likelihood** (**SML**), also
    known as **Persistent Contrastive Divergence** (**PCD**). First, we will set the
    architecture of the model:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `sklearn` 库中，`sklearn.neural_network` 模块包括基于神经网络的模型。在这个模块中，`BernoulliRBM`
    函数拟合一个伯努利RBM。返回一个具有二元可见单元和二元隐藏单元的RBM。参数使用**随机最大似然**（**SML**），也称为**持久对比散度**（**PCD**）进行估计。首先，我们将设置模型的架构：
- en: '[PRE9]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we will fit the model with the training data:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用训练数据拟合模型：
- en: '[PRE10]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `fit_transform` method fits the transformer to `X_train` and `Y_train` with
    optional parameter `fit_params`, and returns a transformed version of `X_train`.
    In this case, no optional parameters are used.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit_transform` 方法将转换器拟合到 `X_train` 和 `Y_train` 上，并可选地使用参数 `fit_params`，然后返回
    `X_train` 的转换版本。在这种情况下，没有使用可选参数。'
- en: 'If you remember, our purpose is to use the `Rbm` model to extract the features
    that will then be used by the logistic regression model to classify the data.
    So, the first part has already been performed—we already have the features extracted
    in the `FitRbmModel` variable. The time has come to create the logistic regression
    model. To do this, we will use `LogisticRegression` function of the `sklearn.linear_model`
    module, as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，我们的目的是使用`Rbm`模型提取特征，然后这些特征将被逻辑回归模型用于分类数据。所以，第一部分已经完成——我们已经在`FitRbmModel`变量中提取了特征。现在是时候创建逻辑回归模型了。为此，我们将使用`sklearn.linear_model`模块中的`LogisticRegression`函数，如下所示：
- en: '[PRE11]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We now set coefficients of the features in the decision function equal to the
    features extracted from the `rbm` model:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将决策函数中特征系数设置为从`rbm`模型中提取的特征：
- en: '[PRE12]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we can build the classifier. To do this, we will use the `Pipeline` function
    of the `sklearn.pipeline` module:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以构建分类器了。为此，我们将使用`sklearn.pipeline`模块中的`Pipeline`函数：
- en: '[PRE13]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The purpose of the `pipeline` is to assemble several steps that can be cross-validated
    together while setting different parameters. For this, it enables setting parameters
    of the various steps using their names, as in the previous code. A step''s estimator
    may be replaced entirely by setting the parameter with its name to another estimator,
    or a transformer removed by setting to `None`. The classifier is now ready; we
    just have to train it:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipeline`的目的在于组装可以一起进行交叉验证的多个步骤，同时设置不同的参数。为此，它允许使用步骤的名称来设置各个步骤的参数，就像之前的代码中那样。可以通过将参数名称设置为另一个估计器来完全替换一个步骤的估计器，或者通过将其设置为`None`来移除一个转换器。现在分类器已经准备好了；我们只需要对其进行训练：'
- en: '[PRE14]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'First, the logistic regression model is trained and then the classifier. We
    just have to make predictions. Recall that for doing this, we have an unused dataset
    available: `X_test` and `Y_test`. To check the performance of the classifier,
    we will compare the forecasts with the real data:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，训练逻辑回归模型，然后训练分类器。我们只需要进行预测。回想一下，为了做到这一点，我们有一个未使用的数据集可用：`X_test`和`Y_test`。为了检查分类器的性能，我们将预测与真实数据进行比较：
- en: '[PRE15]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following screenshot shows the results returned:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了返回的结果：
- en: '![](img/45e4aa13-a412-47eb-b29b-72ce75cb5df6.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/45e4aa13-a412-47eb-b29b-72ce75cb5df6.png)'
- en: 'Finally, to better understand the model performance, we will calculate the
    confusion matrix. In a confusion matrix, our classification results are compared
    to real data. The strength of a confusion matrix is that it identifies the nature
    of the classification errors as well as their quantities. In this matrix, the
    diagonal cells show the number of cases that were correctly classified; all the
    other cells show the misclassified cases. To calculate the confusion matrix, we
    can use the `ConfusionMatrix()` function contained in pandas library as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了更好地理解模型性能，我们将计算混淆矩阵。在混淆矩阵中，我们的分类结果与真实数据进行比较。混淆矩阵的优势在于它不仅识别了分类错误的性质，还识别了它们的数量。在这个矩阵中，对角线单元格显示了正确分类的案例数量；所有其他单元格显示了错误分类的案例。要计算混淆矩阵，我们可以使用pandas库中包含的`ConfusionMatrix()`函数，如下所示：
- en: '[PRE16]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In the following code, the results returned by the `ConfusionMatrix()` function
    are shown:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，展示了`ConfusionMatrix()`函数返回的结果：
- en: '[PRE17]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Several bits of information are returned; in particular, we can notice that
    the accuracy of the model is 0.85.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 返回了一些信息；特别是，我们可以注意到模型的准确率为0.85。
- en: Autoencoder with Keras
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras自编码器
- en: 'As we said previously, an autoencoder is a neural network whose purpose is
    to code its input into small dimensions and the result obtained to be able to
    reconstruct the input itself. Autoencoders are made up of the union of the following
    two subnets: encoder and decoder. To these functions is added another; it''s a
    loss function calculated as the distance between the amount of information loss
    between the compressed representation of the data and the decompressed representation.
    The encoder and the decoder will be differentiable with respect to the distance
    function, so the parameters of the encoding/decoding functions can be optimized
    to minimize the loss of reconstruction, using the gradient stochastic.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说的，自编码器是一种神经网络，其目的是将输入编码成小维度，并能够重建输入本身。自编码器由以下两个子网络的并集组成：编码器和解码器。此外，还有一个损失函数，它是数据压缩表示和分解表示之间信息损失量的距离。编码器和解码器将与距离函数可微分，因此编码/解码函数的参数可以通过梯度随机优化来最小化重建损失。
- en: Load data
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'This is a database of handwritten digits consisting of 60,000 28 x 28 grayscale
    images of the 10 digits, along with a test set of 10,000 images. This dataset
    is already available in the Keras library. The following diagram shows a sample
    of images of 0-8 from the MNIST dataset:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含60,000个28 x 28灰度图像的手写数字数据库，这些图像是10个数字的，还有一个包含10,000个图像的测试集。这个数据集已经在Keras库中可用。以下图表显示了MNIST数据集中0-8的图像样本：
- en: '![](img/fec99017-84f1-44c8-b493-7ae8745a8ed6.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fec99017-84f1-44c8-b493-7ae8745a8ed6.png)'
- en: 'As always, we will analyze the code line by line. In the first part of the
    code, we import the libraries we will use later:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，我们将逐行分析代码。在代码的第一部分，我们导入稍后将要使用的库：
- en: '[PRE18]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This code imports the following function:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码导入了以下函数：
- en: The Input function is used to instantiate a Keras tensor. A Keras tensor is
    a tensor object from the underlying backend (Theano, TensorFlow, or CNTK). We
    augment it with certain attributes that allow us to build a Keras model just by
    knowing the inputs and outputs of the model.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Input函数来实例化一个Keras张量。Keras张量是从底层后端（Theano、TensorFlow或CNTK）的张量对象。我们通过添加某些属性来增强它，这些属性允许我们仅通过知道模型的输入和输出就构建一个Keras模型。
- en: The Dense function is used instantiate a regular densely connected NN layer.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Dense函数实例化一个常规密集连接神经网络层。
- en: The Model function is used to define the model. The model is the thing that
    you can summarize, fit, evaluate, and use to make predictions. Keras provides
    a `Model` class that you can use to create a model from your created layers. It
    only requires that you specify the input and output layers.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Model函数来定义模型。模型是你可以总结、拟合、评估并用于做出预测的东西。Keras提供了一个`Model`类，你可以用它从创建的层中创建模型。它只需要你指定输入和输出层。
- en: 'To import the dataset, simply use this code:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 要导入数据集，只需使用此代码：
- en: '[PRE19]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following tuples are returned:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 以下元组被返回：
- en: '`x_train, x_test`: A `uint8` array of grayscale image data with shape (`num_samples`,
    28, 28)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x_train, x_test`：一个`uint8`数组，包含灰度图像数据，形状为(`num_samples`, 28, 28)'
- en: '`y_train, y_test`: A `uint8` array of digit labels (integers in range 0-9)
    with shape (`num_samples`)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_train, y_test`：一个`uint8`数组，包含数字标签（范围在0-9的整数），形状为(`num_samples`)'
- en: 'Now we have to normalize all values between 0 and 1\. The Mnist images are
    stored in pixel format, where each pixel (totally 28 x 28) is stored as an 8-bit
    integer giving a range of possible values from 0 to 255\. Typically, zero is taken
    to be black, and 255 is taken to be white. The values in between make up the different
    shades of gray. Now, to normalize all values between 0 and 1, simply divide each
    value by 255\. So the pixel containing the value 255 will become 1 and the one
    containing 0 will remain as such; in between lie all the other values:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须将所有值归一化到0到1之间。Mnist图像以像素格式存储，其中每个像素（总共28 x 28）存储为一个8位整数，其值范围从0到255。通常，0被认为是黑色，255被认为是白色。介于两者之间的值构成了不同的灰色阴影。现在，为了将所有值归一化到0到1之间，只需将每个值除以255。因此，包含值255的像素将变为1，包含0的像素将保持原样；介于两者之间的是所有其他值：
- en: '[PRE20]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'By using the `astype()` function, we have converted the input data in `float32`
    (single precision float: sign bit, 8-bits exponent, 23 bits mantissa). As we said,
    each sample (image) consists of a 28 x 28 matrix. To reduce the dimensionality,
    we will flatten the 28 x 28 images into vectors of size 784:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`astype()`函数，我们已经将输入数据从`float32`（单精度浮点数：符号位，8位指数，23位尾数）转换过来。正如我们所说的，每个样本（图像）由一个28
    x 28的矩阵组成。为了降低维度，我们将28 x 28的图像展平成大小为784的向量：
- en: '[PRE21]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `reshape()` function gives a new shape to an array without changing its
    data. The new shape should be compatible with the original shape. The first dimension
    of the new shape is the number of observations returned from the `len()` function
    (`len(x_train)` and `len(x_test)`). The second dimension represents the product
    of the last two dimensions of the starting data (28 x 28 = 784). To better understand
    this transformation, we print the shape of the starting dataset first and then
    the shape of the transformed dataset:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`reshape()`函数给数组赋予一个新形状，而不改变其数据。新形状应与原始形状兼容。新形状的第一个维度是`len()`函数返回的观测数（`len(x_train)`和`len(x_test)`）。第二个维度代表起始数据的最后两个维度的乘积（28
    x 28 = 784）。为了更好地理解这种转换，我们首先打印起始数据集的形状，然后打印转换后数据集的形状：'
- en: '[PRE22]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following are the results before and after the dataset reshape:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是数据集重塑前后的结果：
- en: '[PRE23]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Keras model overview
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras模型概述
- en: 'There are two types of models available in Keras:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中有两种类型的模型可用：
- en: Sequential model
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序模型
- en: Keras functional API
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras功能API
- en: Let us take a look at each one in detail in the following sections.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在以下各节中详细查看每个部分。
- en: Sequential model
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序模型
- en: 'The `Sequential` model is a linear stack of layers. We can create a `Sequential`
    model by passing a list of layer instances to the constructor as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sequential` 模型是层的线性堆叠。我们可以通过将层实例列表传递给构造函数来创建一个 `Sequential` 模型，如下所示：'
- en: '[PRE24]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can also simply add layers via the `.add()` method:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过 `.add()` 方法简单地添加层：
- en: '[PRE25]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This type of model needs to know what input shape it should expect. For this
    reason, the first layer in a `Sequential` model needs to receive information about
    its input shape. There are several possible ways to do this:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 此类模型需要知道它应该期望什么输入形状。因此，`Sequential` 模型的第一个层需要接收有关其输入形状的信息。有几种可能的方法可以实现这一点：
- en: Pass an `input_shape` argument to the first layer
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向第一个层传递 `input_shape` 参数
- en: Specify of their input shape via the `input_dim` and `input_length` arguments
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `input_dim` 和 `input_length` 参数指定它们的输入形状
- en: Pass a `batch_size` argument to a layer
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向一个层传递 `batch_size` 参数
- en: Keras functional API
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 功能 API
- en: 'Another way to define a model is the Keras functional API. The Keras functional
    API is the way to go for defining complex models, such as multi-output models,
    directed acyclic graphs, or models with shared layers. For example, to define
    a densely connected network, simply type the following code:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型的另一种方式是 Keras 功能 API。对于定义复杂模型，如多输出模型、有向无环图或具有共享层的模型，Keras 功能 API 是最佳选择。例如，要定义一个密集连接网络，只需输入以下代码：
- en: '[PRE26]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the following section, we will dive deep into this type of model by applying
    it to our example.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将通过将其应用于我们的示例来深入探讨此类模型。
- en: Define model architecture
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义模型架构
- en: 'Now we will build the model using the Keras functional API. As we saw before,
    first we have to define the input:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用 Keras 功能 API 来构建模型。正如我们之前看到的，首先我们必须定义输入：
- en: '[PRE27]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This returns a tensor that represents our input placeholder. Later, we will
    use this placeholder to define a `Model`. At this point, we can add layers to
    the architecture of our model:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个表示我们输入占位符的张量。稍后，我们将使用这个占位符来定义一个 `Model`。在这个阶段，我们可以向我们的模型架构中添加层：
- en: '[PRE28]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The Dense class is used to define a fully connected layer. We have specified
    the number of neurons in the layer as the first argument (32), the activation
    function using the activation argument (`relu`), and finally the input tensor
    (`InputModel`) of the layer.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Dense 类用于定义一个全连接层。我们指定层的神经元数量作为第一个参数（32），使用激活参数（`relu`）指定激活函数，最后指定层的输入张量（`InputModel`）。
- en: Remember that given an input `x`, the encoder encodes it in a variable `z`,
    also called **latent variable**. `z` usually has much smaller dimensions than
    `x`; in our case, we have passed from 784 to 32 with a compression factor of 24.5.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，给定一个输入 `x`，编码器将其编码到一个变量 `z` 中，也称为**潜在变量**。`z` 通常比 `x` 的维度小得多；在我们的例子中，我们从
    784 压缩到 32，压缩因子为 24.5。
- en: 'Now let''s add the decoding layer:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们添加解码层：
- en: '[PRE29]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This layer is the lossy reconstruction of the input. For another time, we have
    used the Dense class with 784 neurons (dimensionality of the output space), the
    `sigmoid` activation function, and `EncodedLayer` output as input. Now we have
    to instantiate a model as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 此层是对输入的损失重建。又一次，我们使用了具有 784 个神经元（输出空间的维度）的 Dense 类，`sigmoid` 激活函数，以及 `EncodedLayer`
    输出作为输入。现在我们必须如下实例化一个模型：
- en: '[PRE30]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This model will include all layers required in the computation of `DecodedLayer`
    (output) given `InputModel` (input). In the following are listed some useful attributes
    of Model class:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型将包括在给定 `InputModel`（输入）的情况下计算 `DecodedLayer`（输出）所需的全部层。以下是 Model 类的一些有用属性：
- en: '`model.layers` is a flattened list of layers comprising the model graph'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.layers` 是包含模型图的层的扁平化列表'
- en: '`model.inputs` is the list of input tensors'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.inputs` 是输入张量的列表'
- en: '`model.outputs` is the list of output tensors'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.outputs` 是输出张量的列表'
- en: 'So, we have to configure the model for training. To do this, we will use the
    `compile` method as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们必须为训练配置模型。为此，我们将使用 `compile` 方法，如下所示：
- en: '[PRE31]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This method configures the model for training. Only two arguments are used:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法配置模型以进行训练。仅使用两个参数：
- en: '`optimizer`: String (name of optimizer) or optimizer instance.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`：字符串（优化器名称）或优化器实例。'
- en: '`loss`: String (name of objective function) or objective function. If the model
    has multiple outputs, you can use a different loss on each output by passing a
    dictionary or a list of losses. The loss value that will be minimized by the model
    will then be the sum of all individual losses.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`: 字符串（目标函数的名称）或目标函数。如果模型有多个输出，可以通过传递字典或损失列表来使用不同的损失函数。然后，模型将最小化的损失值将是所有单个损失的加和。'
- en: We have used adadelta optimizer. This method dynamically adapts over time, using
    only first-order information, and has minimal computational overhead beyond vanilla
    stochastic gradient descent. The method requires no manual tuning of the learning
    rate and appears robust to noisy gradient information, different model architecture
    choices, various data modalities, and selection of hyperparameters.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了adadelta优化器。这种方法随时间动态调整，仅使用一阶信息，并且计算开销最小，超出了传统的随机梯度下降。该方法不需要手动调整学习率，并且对噪声梯度信息、不同的模型架构选择、各种数据模态和超参数的选择具有鲁棒性。
- en: Furthermore, we have used `binary_crossentropy` as a `loss` function. Loss functions
    are computationally feasible functions representing the price paid for inaccuracy
    of predictions in classification problems.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们使用了`binary_crossentropy`作为`loss`函数。损失函数是计算上可行的函数，表示在分类问题中对预测不准确性的代价。
- en: 'At this point, we can train the model:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以训练模型：
- en: '[PRE32]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The fit method trains the model for a fixed number of epochs (iterations on
    a dataset). In the following, the arguments passed are explained to better understand
    the meaning:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`方法用于在固定数量的epochs（在数据集上的迭代）上训练模型。以下是对传递的参数的解释，以便更好地理解其含义：'
- en: '`x`: A Numpy array of training data (if the model has a single input), or list
    of Numpy arrays (if the model has multiple inputs). If input layers in the model
    are named, you can also pass a dictionary mapping input names to Numpy arrays.
    `x` can be None (default) if feeding from framework-native tensors (for example,.
    TensorFlow data tensors).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x`: 如果模型有一个输入，则为训练数据的Numpy数组，或者如果有多个输入，则为Numpy数组的列表。如果模型中的输入层有名称，也可以通过将输入名称映射到Numpy数组来传递字典。如果从框架原生张量（例如，TensorFlow数据张量）中提供，则`x`可以是`None`（默认）。'
- en: '`y`: A Numpy array of target (label) data if the model has a single output,
    or a list of Numpy arrays if the model has multiple outputs. If output layers
    in the model are named, you can also pass a dictionary mapping output names to
    Numpy arrays. `y` can be `None` (default) if feeding from framework-native tensors
    (for example, TensorFlow data tensors).'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y`: 如果模型有一个输出，则为目标（标签）数据的Numpy数组，或者如果有多个输出，则为Numpy数组的列表。如果模型中的输出层有名称，也可以通过将输出名称映射到Numpy数组来传递字典。如果从框架原生张量（例如，TensorFlow数据张量）中提供，则`y`可以是`None`（默认）。'
- en: '`batch_size`: `Integer` or `None`. This is the number of samples per gradient
    update. If unspecified, `batch_size` will default to `32`.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`: `整数`或`None`。这是每次梯度更新时的样本数量。如果没有指定，`batch_size`将默认为`32`。'
- en: '`epochs`: An Integer. It is the number of epochs to train the model. An epoch
    is an iteration over the entire `x` and `y` data provided. Note that in conjunction
    with `initial_epoch`, `epochs` is to be understood as the final epoch. The model
    is not trained for a number of iterations given by epochs, but merely until the
    epoch of index epochs is reached.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`: 一个整数。这是训练模型的epoch数量。一个epoch是对整个`x`和`y`数据的迭代。注意，与`initial_epoch`结合使用时，`epochs`应理解为最终epoch。模型不是根据epoch的数量进行多次迭代训练，而是仅仅训练到epoch索引为epochs的那个epoch。'
- en: '`shuffle`: A boolean to decide whether to shuffle the training data before
    each epoch or `str` (for `batch`). `batch` is a special option for dealing with
    the limitations of HDF5 data; it shuffles in batch-sized chunks. It has no effect
    when `steps_per_epoch` is anything other than None.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shuffle`: 一个布尔值，用于决定在每个epoch之前是否对训练数据进行洗牌，或者`str`（用于`batch`）。`batch`是处理HDF5数据限制的特殊选项；它以批大小块进行洗牌。当`steps_per_epoch`不是`None`时，它没有效果。'
- en: '`validation_data`: A tuple (`x_val` and `y_val`) or tuple (`x_val`, `y_val`,
    and `val_sample_weights`) on which to evaluate the loss and any model metrics
    at the end of each epoch. The model will not be trained on this data. `validation_data`
    will override `validation_split`.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validation_data`: 一个元组（`x_val`和`y_val`）或元组（`x_val`，`y_val`，和`val_sample_weights`），用于在每个epoch结束时评估损失和任何模型度量。模型不会在此数据上训练。`validation_data`将覆盖`validation_split`。'
- en: A `History` object is returned. Its `history.history` attribute is a record
    of training loss values and metrics values at successive epochs, as well as validation
    loss values and validation metrics values (if applicable).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个`History`对象。其`history.history`属性是记录在连续的epoch中训练损失值和指标值，以及验证损失值和验证指标值（如果适用）。
- en: 'Our model is now ready, so we can use it to automatically rebuild the handwritten
    digits. To do this, we will use the `predict` method:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模式现在已准备就绪，因此我们可以使用它来自动重建手写数字。为此，我们将使用`predict`方法：
- en: '[PRE33]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This method generates output predictions for the input samples (`x_test`).
    Running this example, you should see a message for each of the 100 epochs, printing
    the loss and accuracy for each, followed by a final evaluation of the trained
    model on the training dataset. This is shown in the following screenshot:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法为输入样本（`x_test`）生成输出预测。运行此示例，你应该会看到每个100个epoch的消息，打印每个epoch的损失和准确率，然后是对训练数据集上训练模型的最终评估。如下所示：
- en: '![](img/8bf13071-a06e-4ec3-9d31-595c5063f0f2.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8bf13071-a06e-4ec3-9d31-595c5063f0f2.png)'
- en: 'To get an idea of how the `loss` function varies during the epochs, it can
    be useful create a plot of loss on the training and validation datasets over training
    epochs. To do this, we will use the `Matplotlib` library as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解`loss`函数在epoch中的变化情况，可以创建一个在训练和验证数据集上训练epoch的损失图。为此，我们将使用以下`Matplotlib`库：
- en: '[PRE34]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'A plot of loss on the training and validation datasets over training epochs
    is shown in the following graph:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图表中显示了训练和验证数据集上训练epoch的损失图：
- en: '![](img/ddd7f4d5-cff7-4b35-8021-5803be3c2b4b.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ddd7f4d5-cff7-4b35-8021-5803be3c2b4b.png)'
- en: 'Our work is done; we just have to verify the results obtained. We can print
    on screen the starting handwriting digits and those reconstructed from our model.
    Of course, we will do it only for some of the 60,000 digits contained in the dataset;
    in fact, we will limit ourselves to display the first five. We will also use the
    `Matplotlib` library in this case:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作已经完成；我们只需验证获得的结果。我们可以在屏幕上打印出原始的手写数字和从我们的模型重建的数字。当然，我们只会对数据集中包含的60000个数字中的某些进行操作；实际上，我们将仅显示前五个。在这种情况下，我们也将使用`Matplotlib`库：
- en: '[PRE35]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The results are shown in the following screenshot:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '![](img/10667a27-bb86-4860-8a5c-3e006d6b793d.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/10667a27-bb86-4860-8a5c-3e006d6b793d.png)'
- en: As you can see, the result is very close to the original, meaning that the model
    works well.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，结果非常接近原始数据，这意味着模型运行良好。
- en: Magenta
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Magenta
- en: On June 1, 2016, Google launched the Magenta project, a research project that
    aims to allow the creation of art and music in an autonomous way through the use
    of AI. Based on the TensorFlow platform, Magenta aims to publish code in open
    source mode on GitHub to allow developers to achieve increasingly striking and
    advanced results.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年6月1日，谷歌推出了Magenta项目，这是一个旨在通过使用人工智能以自主方式创作艺术和音乐的研究项目。基于TensorFlow平台，Magenta旨在以开源模式在GitHub上发布代码，以便开发者能够实现越来越引人注目和先进的结果。
- en: The project is a brainchild of the Google Brain team, a deep learning AI research
    team at Google. It combines open-ended machine learning research with system engineering
    and Google-scale computing resources.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目是谷歌大脑团队的一个创意，谷歌大脑团队是谷歌的一个深度学习人工智能研究团队。它将开放式的机器学习研究、系统工程和谷歌规模的计算资源相结合。
- en: 'The Magenta project has set itself two ambitious goals: to develop machine
    learning for art and music, and to build a community of people interested in this
    topic. Machine learning has long been used in different contexts, in particular
    for speech recognition and translation of languages. Magenta was created to concentrate
    activity on previously unexplored fields such as the generation of art in the
    broad sense. To do this, Magenta wanted to create a physical place, where all
    people united by the same interest (that is, generation of art) could exchange
    ideas and products. In other words, a community of artists, programmers, and researchers
    of machine learning.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: Magenta项目为自己设定了两个雄心勃勃的目标：开发艺术和音乐机器学习，并建立一个对这一主题感兴趣的人们的社区。机器学习长期以来在不同的环境中被使用，特别是在语音识别和语言翻译方面。Magenta的创建是为了集中活动在之前未探索的领域，如广义上的艺术生成。为此，Magenta希望创建一个物理场所，所有由相同兴趣（即艺术生成）联合的人可以交流思想和产品。换句话说，一个由艺术家、程序员和机器学习研究者组成的社区。
- en: 'For more information, refer to the official website of the project at the following
    URL: [https://magenta.tensorflow.org/](https://magenta.tensorflow.org/).'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参考以下URL的项目官方网站：[https://magenta.tensorflow.org/](https://magenta.tensorflow.org/).
- en: The NSynth dataset
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSynth数据集
- en: From reading the previous chapters, we have now understood that, to correctly
    train a machine learning algorithm, it is necessary to have a dataset containing
    an important number of observations. Recently, the increased use of generative
    models has been applied to images thanks to the availability of high-quality image
    datasets, which therefore correspond to a significant data set. With this in mind,
    the Google Brain team has made NSynth available. It's a large-scale, high-quality
    set of musical notes that is an order of magnitude larger than comparable public
    datasets. The aim is to have a significant audio dataset in order to develop generative
    models with better performance.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读前面的章节，我们现在已经了解到，为了正确训练机器学习算法，需要有一个包含重要数量观察的数据集。最近，由于高质量图像数据集的可用性，生成模型在图像上的使用增加，因此这对应着一个重要的数据集。考虑到这一点，Google
    Brain团队推出了NSynth。这是一个大规模、高质量的音符集，比可比的公共数据集大一个数量级。目标是拥有一个重要的音频数据集，以便开发性能更好的生成模型。
- en: The NSynth dataset was introduced by Jesse Engel et al. in the article named
    *Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders*.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: NSynth数据集由Jesse Engel等人介绍在名为《使用WaveNet自动编码器进行神经音频合成音符》的文章中。
- en: NSynth is an audio dataset containing 305,979 musical notes, each with a unique
    tone, tone, and envelope. For 1,006 tools from commercial sample libraries, the
    Google Brain team generated 4 seconds of 16 kHz monophonic audio fragments, called
    **notes**, spanning each step of a standard MIDI piano or (21-108) and five different
    speeds (25, 50, 75, 100, and 127). The note was kept for the first 3 seconds and
    allowed to fall for the final second.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: NSynth是一个包含305,979个音乐音符的音频数据集，每个音符都有独特的音调、音色和包络。对于来自商业样本库的1,006个工具，Google Brain团队生成了4秒的16
    kHz单声道音频片段，称为**音符**，跨越标准MIDI钢琴的每个步骤（21-108）和五种不同的速度（25、50、75、100和127）。音符保持前3秒，并在最后1秒允许其衰减。
- en: 'The Google Brain team also annotated each of the notes with three additional
    pieces of information based on a combination of human evaluation and heuristic
    algorithms:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: Google Brain团队还根据人类评估和启发式算法的组合，为每个音符标注了三条额外信息：
- en: '**Source**: The method of sound production for the note''s instrument. This
    can be one of acoustic or electronic for instruments that were recorded from acoustic
    or electronic instruments respectively, or synthetic for synthesized instruments.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源**：音符乐器的声音产生方法。这可以是声学或电子，对于分别从声学或电子乐器录制乐器的乐器，或合成，对于合成乐器。'
- en: '**Family**: The high-level family of which the note’s instrument is a member.
    Each instrument is a member of exactly one family.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**家族**：音符所属的高级别家族。每个乐器恰好属于一个家族。'
- en: '**Qualities**: Sonic qualities of the note. Each note is annotated with zero
    or more qualities.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**品质**：音符的音质。每个音符都标注了零个或多个品质。'
- en: 'The NSynth dataset can be downloaded in two formats:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: NSynth数据集可以以下两种格式下载：
- en: TFRecord files of serialized TensorFlow example protocol buffers with one Example
    proto per note
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow示例协议缓冲区的序列化TFRecord文件，每个note包含一个Example proto
- en: JSON files containing non-audio features alongside 16-bit PCM WAV audio files
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含非音频特征和16位PCM WAV音频文件的JSON文件
- en: 'The full dataset is split into three sets:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 整个数据集分为三个集合：
- en: '**Train**: A training set with 289,205 examples. Instruments do not overlap
    with valid or test.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练**：包含289,205个示例的训练集。乐器与有效集或测试集不重叠。'
- en: '**Valid**: A validation set with 12,678 examples. Instruments do not overlap
    with train.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有效**：包含12,678个示例的验证集。乐器与训练集不重叠。'
- en: '**Test**: A test set with 4,096 examples. Instruments do not overlap with train.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试**：包含4,096个示例的测试集。乐器与训练集不重叠。'
- en: 'For more information and to download the dataset, refer to the official website
    of the project at the following URL: [https://magenta.tensorflow.org/datasets/nsynth](https://magenta.tensorflow.org/datasets/nsynth).'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息和下载数据集，请参考以下URL的项目官方网站：[https://magenta.tensorflow.org/datasets/nsynth](https://magenta.tensorflow.org/datasets/nsynth).
- en: Summary
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored one of the most interesting research sites on modeling
    with neural networks. First we saw an introduction to unsupervised learning algorithms.
    Unsupervised learning is a machine learning technique that, starting from a series
    of inputs (system experience), will be able to reclassify and organize on the
    basis of common characteristics to try to make predictions on subsequent inputs.
    Unlike supervised learning, only unlabeled examples are provided to the learner
    during the learning process, as the classes are not known a priori but must be
    learned automatically.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用神经网络建模的最有趣的研究领域之一。首先，我们看到了无监督学习算法的介绍。无监督学习是一种机器学习技术，它从一系列输入（系统经验）开始，将能够根据共同特征重新分类和组织，以尝试对后续输入进行预测。与监督学习不同，在学习过程中，只向学习者提供未标记的示例，因为类别不是事先已知的，而是必须自动学习。
- en: So, we analyzed different types of generative models. A Boltzmann machine is
    a probabilistic graphic model that can be interpreted as a stochastic neural network.
    In practice, a Boltzmann machine is a model (including a certain number of parameters)
    that, when applied to a data distribution, is able to provide a representation.
    This model can be used to extract important aspects of an unknown distribution
    (target distribution) starting only from a sample of the latter.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们分析了不同类型的生成模型。玻尔兹曼机是一种概率图模型，可以解释为随机神经网络。在实践中，玻尔兹曼机是一个模型（包括一定数量的参数），当应用于数据分布时，能够提供一种表示。该模型可以用来从目标分布（未知分布）的样本中提取重要方面。
- en: An autoencoder is a neural network whose purpose is to code its input into small
    dimensions and the result obtained to be able to reconstruct the input itself.
    The purpose of autoencoders is not simply to perform a sort of compression of
    the input or look for an approximation of the identity function; but there are
    techniques that allow us to direct the model (starting from a hidden layer of
    reduced dimensions) to give greater importance to some data properties. Thus they
    give rise to different representations based on the same data.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器是一种神经网络，其目的是将输入编码成小维度，并能够重建输入本身。自动编码器的目的不仅仅是执行一种输入的压缩或寻找身份函数的近似；但有一些技术可以让我们指导模型（从减少维度的隐藏层开始）给予某些数据属性更大的重要性。因此，它们基于相同的数据产生不同的表示。
- en: GAN is a generative model consisting of two networks that are jointly trained,
    called **generator** and **discriminator**. The dynamics between these two networks
    is like a forger and an investigator. The forger tries to produce faithful imitations
    of authentic works of art while the investigator tries to distinguish the fakes
    from the originals.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 是由两个联合训练的网络组成的生成模型，称为 **生成器** 和 **判别器**。这两个网络之间的动态类似于伪造者和调查者。伪造者试图制作忠实于原作的仿制品，而调查者则试图区分赝品和真品。
- en: 'Then, we showed how to implement some examples: feature extraction using RBM
    and autoencoder with Keras. Finally, we introduced the Nsynth dataset and the
    Google Magenta project.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们展示了如何实现一些示例：使用 RBM 进行特征提取和 Keras 的自动编码器。最后，我们介绍了 Nsynth 数据集和 Google Magenta
    项目。
