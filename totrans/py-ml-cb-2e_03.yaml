- en: Predictive Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a linear classifier using **support vector machines** (**SVMs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a nonlinear classifier using SVMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tackling class imbalance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting confidence measurements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding optimal hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an event predictor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simplifying a machine learning workflow using TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the stacking method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To address the recipes in this chapter, you need the following files (available
    on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '`svm.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_multivar.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`svm_imbalance.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_multivar_imbalance.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`svm_confidence.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`perform_grid_search.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`building_event_binary.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`building_event_multiclass.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`` `event.py` ``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`traffic_data.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`traffic.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IrisTensorflow.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stacking.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Predictive modeling** is probably one of the most exciting fields in data
    analytics. It has gained a lot of attention in recent years due to massive amounts
    of data being available in many different verticals. It is very commonly used
    in areas concerning data mining to forecast future trends.'
  prefs: []
  type: TYPE_NORMAL
- en: Predictive modeling is an analysis technique that is used to predict the future
    behavior of a system. It is a collection of algorithms that can identify the relationship
    between independent input variables and the target responses. We create a mathematical
    model, based on observations, and then use this model to estimate what's going
    to happen in the future.
  prefs: []
  type: TYPE_NORMAL
- en: In predictive modeling, we need to collect data with known responses to train
    our model. Once we create this model, we validate it using some metrics, and then
    use it to predict future values. We can use many different types of algorithms
    to create a predictive model. In this chapter, we will use SVMs to build linear
    and nonlinear models.
  prefs: []
  type: TYPE_NORMAL
- en: A predictive model is built using a number of features that are likely to influence
    the behavior of the system. For example, to estimate weather conditions, we may
    use various types of data, such as temperature, barometric pressure, precipitation,
    and other atmospheric processes. Similarly, when we deal with other types of systems,
    we need to decide what factors are likely to influence its behavior and include
    them as part of the feature vector before training our model.
  prefs: []
  type: TYPE_NORMAL
- en: Building a linear classifier using SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SVMs are supervised learning models that we can use to create classifiers and
    regressors. An SVM solves a system of mathematical equations and finds the best
    separating boundary between two sets of points. Let's see how to build a linear
    classifier using an SVM.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s visualize our data to understand the problem at hand. We will use the `svm.py`
    file for this. Before we build the SVM, let''s understand our data. We will use
    the `data_multivar.txt` file that''s already provided to you. Let''s see how to
    to visualize the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and add the following lines to it (the full code is
    in the `svm.py` file which has already been provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We just imported a couple of packages and named the input file. Let''s look
    at the `load_data()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to separate the data into classes, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have separated the data, let''s plot it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2eaae6f7-a135-4d73-95d5-8f2391f2b33d.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding consists of two types of points—**solid squares** and **empty
    squares**. In machine learning lingo, we say that our data consists of two classes.
    Our goal is to build a model that can separate the solid squares from the empty
    squares.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will learn how to build a linear classifier using SVMs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to split our dataset into training and testing datasets. Add the following
    lines to the same Python file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s initialize the SVM object using a `linear` kernel. Add the following
    lines to the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to train the linear SVM classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now see how the classifier performs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f84ac53f-cb82-4d3a-8a16-0cf166e316fa.png)'
  prefs: []
  type: TYPE_IMG
- en: The `plot_classifier` function is the same as we discussed in [Chapter 1](f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml),
    *The Realm of Supervised Learning*. It has a couple of minor additions.
  prefs: []
  type: TYPE_NORMAL
- en: You can check out the `utilities.py` file already provided to you for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how this performs on the test dataset. Add the following lines to
    the `svm.py` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eddb9d17-57c0-4936-8873-7a962c1a6bfa.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the classifier boundaries on the input data are clearly identified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compute the accuracy for the training set. Add the following lines to
    the same file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following on your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2acae4ae-b68e-4230-bee1-b7a3ed3c7da8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, let''s see the classification report for the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following on the Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cfadb22c-c1e3-4aa0-a941-5f64a3c06a70.png)'
  prefs: []
  type: TYPE_IMG
- en: From the output screenshot where we visualized the data, we can see that the
    solid squares are completely surrounded by empty squares. This means that the
    data is not linearly separable. We cannot draw a nice straight line to separate
    the two sets of points! Hence, we need a nonlinear classifier to separate these
    datapoints.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SVMs are a set of supervised learning methods that can be used for both classification
    and regression. Given two classes of linearly separable multidimensional patterns,
    among all the possible separating hyperplanes, the SVM algorithm determines the
    one able to separate the classes with the greatest possible margin. The margin
    is the minimum distance of the points in the two classes in the training set from
    the hyperplane identified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Maximization of the margin is linked to generalization. If the training set
    patterns are classified with a large margin, you can hope that even test-set patterns
    close to the boundary between the classes are managed correctly. In the following,
    you can see three lines (**l1**, **l2**, and **l3**). Line **l1** does not separate
    the two classes, line **l2** separates them, but with a small margin, while line
    **l3** maximizes the distance between the two classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/204252af-0a4c-4aa1-ac10-b13f0d018eb1.png)'
  prefs: []
  type: TYPE_IMG
- en: SVMs can be used to separate classes that cannot be separated with a linear
    classifier. Object coordinates are mapped into a space called a **feature space**
    using non-linear functions, called **characteristic functions**. This space is
    highly multidimensional, in which the two classes can be separated with a linear
    classifier. So, the initial space is remapped in the new space, at which point
    the classifier is identified and then returned to the initial space.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**SVMs** constitute a class of learning machines recently introduced in the
    literature. SVMs derive from concepts concerning the statistical theory of learning
    and present theoretical generalization properties. The theory that governs the
    functioning mechanisms of SVMs was introduced by Vapnik in 1965 (statistical learning
    theory), and was more recently perfected, in 1995, by Vapnik himself, and others.
    SVMs are one of the most widely used tools for pattern classification. Instead
    of estimating the probability densities of classes, Vapnik suggests directly solving
    the problem of interest, that is, to determine the decisional surfaces between
    the classes (classification boundaries).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `sklearn.svm.SVC()` function: [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to *Support Vector Machine Tutorial* (from Columbia University): [http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf](http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to *Support Vector Machines* - Lecture notes (by Andrew Ng from Stanford
    University): [http://cs229.stanford.edu/notes/cs229-notes3.pdf](http://cs229.stanford.edu/notes/cs229-notes3.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tutorial on Support Vector Machine* (from Washington State University): [https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf](https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SVM Tutorial*: [http://web.mit.edu/zoya/www/SVM.pdf](http://web.mit.edu/zoya/www/SVM.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a nonlinear classifier using SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An SVM provides a variety of options to build a nonlinear classifier. We need
    to build a nonlinear classifier using various kernels. In this recipe, let's consider
    two cases here. When we want to represent a curvy boundary between two sets of
    points, we can either do this using a polynomial function or a radial basis function.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the same file used in the previous recipe, *Building
    a linear classifier using SVMs*, but in this case, we will use a different kernel
    to deal with a markedly nonlinear problem.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build a nonlinear classifier using SVMs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first case, let''s use a polynomial kernel to build a nonlinear classifier.
    In the same Python file (`svm.py`), search for the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace this line with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This means that we use a polynomial function with `degree` as `3`. If we increase
    the degree, this means we allow the polynomial to be curvier. However, curviness
    comes at a cost, in the sense that it will take more time to train because it's
    more computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run this code now, you will get the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/95a043a1-9e6c-4225-81bb-f7ae523c53bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will also see the following classification report printed on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/753776b5-ad66-441d-9c1f-e34492074839.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also use a radial basis function kernel to build a nonlinear classifier.
    In the same Python file, search for the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace this line with the following one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code now, you will get the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/75773f4a-5e75-462d-b1b6-c41949ef9137.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will also see the following classification report printed on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fded2682-2b0a-4fff-86dd-6a1691409256.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we have used an SVM classifier to find the best separating boundary
    between a dataset of points by solving a system of mathematical equations. To
    address a nonlinear problem, we used Kernel methods. Kernel methods are thus named
    for Kernel functions, which are used to operate in the feature space without calculating
    data coordinates in space, but rather by calculating the internal product between
    images of all copies of data in the function space. The calculation of the internal
    product is often computationally cheaper than the explicit calculation of the
    coordinates. This method is called the **Kernel stratagem**.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main point of the SVM is that a generic problem can always be solved as
    long as you carefully choose the kernel and all its parameters—for example, going
    to make a total overfitting of the input dataset. The problem with this method
    is that it scales quite badly with the size of the dataset, as it is classically
    attributed to a D2 factor, even if, in this sense, faster implementations can
    be obtained by optimizing this aspect. The problem is identifying the best kernel
    and providing it with the best parameters.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Support Vector Machines and Kernel Methods* (from Carnegie Mellon''s School
    of Computer Science): [https://www.cs.cmu.edu/~ggordon/SVMs/new-svms-and-kernels.pdf](https://www.cs.cmu.edu/~ggordon/SVMs/new-svms-and-kernels.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Support Vector Machines and Kernel Methods* (from the Department of Computer
    Science, National Taiwan University): [https://www.csie.ntu.edu.tw/~cjlin/talks/postech.pdf](https://www.csie.ntu.edu.tw/~cjlin/talks/postech.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tackling class imbalance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we dealt with problems where we had a similar number of datapoints
    in all our classes. In the real world, we might not be able to get data in such
    an orderly fashion. Sometimes, the number of datapoints in one class is a lot
    more than the number of datapoints in other classes. If this happens, then the
    classifier tends to get biased. The boundary won't reflect the true nature of
    your data, just because there is a big difference in the number of datapoints
    between the two classes. Therefore, it is important to account for this discrepancy
    and neutralize it so that our classifier remains impartial.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use a new dataset, named `data_multivar_imbalance.txt`, in
    which there are three values for each line; the first two represent the coordinates
    of the point, the third, the class to which the point belongs. Our aim is, once
    again, to build a classifier, but this time, we will have to face a data-balancing
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to tackle class imbalance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s load the data (`data_multivar_imbalance.txt`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s visualize the data. The code for visualization is exactly the same as
    it was in the previous recipe. You can also find it in the file named `svm_imbalance.py`,
    already provided to you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run it, you will see the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d0f0f3ab-87e7-4ebc-9320-b88eb858b978.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s build an SVM with a linear kernel. The code is the same as it was in
    the previous recipe, *Building a nonlinear classifier using SVM**s*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print a classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run it, you will see the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a24acb1e-3b1a-4706-8c62-81a0e68375eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You might wonder why there''s no boundary here! Well, this is because the classifier
    is unable to separate the two classes at all, resulting in 0% accuracy for `Class-0`.
    You will also see a classification report printed on your Terminal, as shown in
    the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/35bb2949-b30d-4e69-86b4-b332bcfc08d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we expected, `Class-0` has 0% precision, so let''s go ahead and fix this!
    In the Python file, search for the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace the preceding line with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `class_weight` parameter will count the number of datapoints in each class
    to adjust the weights so that the imbalance doesn't adversely affect the performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will get the following output once you run this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/24b86379-bcaf-4034-8595-1f224f6505a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2ef0355b-2065-4e8d-a8dd-c1445d6395ae.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, `Class-0` is now detected with nonzero percentage accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we have used a SVM classifier to find the best separating boundary
    between a dataset of points. To address a data-balancing problem, we once again
    used the linear Kernel method, but we implemented a `class_weight` keyword in
    the `fit` method. The `class_weight` variable is a dictionary in the form `{class_label:
    value}`, where `value` is a floating-point number greater than 0 that modifies
    the *C* parameter of the class `(class_label)`, setting it with a new value, obtained
    by multiplying the old *C* value with that specified in the value attribute (*C
    * value*).'
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*C* is a hyperparameter that determines the penalty for the incorrect classification
    of an observation. So, we used a weight for the classes to manage unbalanced classes. In
    this way, we will assign a new value of *C* to the classes, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0290b38f-9856-45fd-bee8-b57fb1163007.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *C* is the penalty, *w[i]* is a weight inversely proportional to class
    i's frequency, and *C[i]* is the *C* value for class *i*. This method suggests
    increasing the penalty to classify the less represented classes so as to prevent
    them from being outclassed by the most represented class.
  prefs: []
  type: TYPE_NORMAL
- en: In the `scikit-learn` library, when using SVC, we can set the values for *C[i]*
    automatically by setting `class_weight='balanced'`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Support Vector Machines*—official documentation of the `scikit-learn` library: [https://scikit-learn.org/stable/modules/svm.html](https://scikit-learn.org/stable/modules/svm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting confidence measurements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It would be nice to know the confidence with which we classify unknown data.
    When a new datapoint is classified into a known category, we can train the SVM
    to compute the confidence level of that output as well. A *confidence level* refers
    to the probability that the value of a parameter falls within a specified range
    of values.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use an SVM classifier to find the best separating boundary
    between a dataset of points. In addition, we will also perform a measure of the
    confidence level of the results obtained.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to extract confidence measurements:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code is given in the `svm_confidence.py` file, already provided to
    you. We will discuss the code of the recipe here. Let''s define some input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we split the data for training and testing, and then we will
    build the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the input datapoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s measure the distance from the boundary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following printed on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d4f5f260-388d-4a8a-bd54-361368f0bb5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The distance from the boundary gives us some information about the datapoint,
    but it doesn''t exactly tell us how confident the classifier is about the output
    tag. To do this, we need **Platt scaling**. This is a method that converts the
    distance measure into a probability measure between classes. Let''s go ahead and
    train an SVM using Platt scaling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `probability` parameter tells the SVM that it should train to compute the
    probabilities as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s train the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compute the confidence measurements for these input datapoints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `predict_proba` function measures the confidence value.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see the following on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6568daab-8c90-42f1-9377-1ee2b73da4e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s see where the points are with respect to the boundary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this, you will get the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9a4adf88-34c4-431f-ae20-d0e2f4b79423.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we built a classifier based on SVM. Once the classifier was
    obtained, we used a set of points to measure the distance of those points from
    the boundary and then measured the confidence levels for each of those points.
    When estimating a parameter, the simple identification of a single value is often
    not sufficient. It is therefore advisable to accompany the estimate of a parameter
    with a plausible range of values ​​for that parameter, which is defined as the
    confidence interval. It is therefore associated with a cumulative probability
    value that indirectly, in terms of probability, characterizes its amplitude with
    respect to the maximum values ​​assumed by the random variable that measures the
    probability that the random event described by that variable in question falls
    into this interval and is equal to this area graphically, subtended by the probability
    distribution curve of the random variable in that specific interval.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The confidence interval measures the reliability of a statistic, such as an
    opinion poll. For example, if 40% of the sample interviewed declare to choose
    a certain product, it can be inferred with a level of confidence of 99% that a
    percentage between 30 and 50 of the total consumer population will be expressed
    in favor of that product. From the same sample interviewed, with a 90% confidence
    interval, it can be assumed that the percentage of opinions favorable to that
    product is now between 37% and 43%.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `sklearn.svm.SVC.decision_` function: [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.decision_function](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.decision_function)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to *Probabilistic Outputs for Support Vector Machines and Comparisons
    to Regularized Likelihood Methods*: [https://www.researchgate.net/publication/2594015_Probabilistic_Outputs_for_Support_Vector_Machines_and_Comparisons_to_Regularized_Likelihood_Methods](https://www.researchgate.net/publication/2594015_Probabilistic_Outputs_for_Support_Vector_Machines_and_Comparisons_to_Regularized_Likelihood_Methods)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding optimal hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, hyperparameters are important for determining
    the performance of a classifier. Let's see how to extract optimal hyperparameters
    for SVMs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning algorithms, various parameters are obtained during the learning
    process. In contrast, hyperparameters are set before the learning process begins.
    Given these hyperparameters, the training algorithm learns the parameters from
    the data. In this recipe, we will extract hyperparameters for a model based on
    an SVM algorithm using the grid search method.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to find optimal hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code is given in the `perform_grid_search.py` file that''s already
    provided to you. We start importing the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we load the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We split the data into a train and test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will use cross-validation here, which we covered in the previous recipes.
    Once you load the data and split it into training and testing datasets, add the
    following to the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the metrics that we want to use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start the search for optimal hyperparameters for each of the metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print the best parameter set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in the preceding output, it searches for all the optimal hyperparameters.
    In this case, the hyperparameters are the type of `kernel,` the `C` value, and
    `gamma`. It will try out various combinations of these parameters to find the
    best parameters. Let''s test it out on the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b4a7a2e2-2389-453e-8e2f-402da58c1b62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have previously said that there are different techniques for optimizing
    hyperparameters. We''ll apply the `RandomizedSearchCV` method. To do this, just
    use the same data and change the classifier. To the code just seen, we add a further
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s test it out on the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2f2ff1b0-4f29-447e-9c83-094d99c10f74.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, *Building a nonlinear classifier using SVMs*, we repeatedly
    modified the kernel of the SVM algorithm to obtain an improvement in the classification
    of data. On the basis of the hyperparameter definition given at the beginning
    of the recipe, it is clear that the kernel represents a hyperparameter. In this
    recipe, we randomly set the value for this hyperparameter and checked the results
    to find out which value determines the best performance. However, a random selection
    of algorithm parameters may be inadequate.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, it is difficult to compare the performance of different algorithms
    by setting the parameters randomly, because an algorithm can perform better than
    another with a different set of parameters. And if the parameters are changed,
    the algorithm may have worse results than the other algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, the random selection of parameter values ​​is not the best approach
    we can take to find the best performance for our model. On the contrary, it would
    be advisable to develop an algorithm that automatically finds the best parameters
    for a particular model. There are several methods for searching for hyperparameters,
    such as the following: grid search, randomized search, and Bayesian optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: The grid search algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **grid search** algorithm does this by automatically looking for the set
    of hyperparameters that detracts from the best performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The `sklearn.model_selection.GridSearchCV()` function performs an exhaustive
    search over specified parameter values for an estimator. **Exhaustive search**
    (also named direct search, or brute force) is a comprehensive examination of all
    possibilities, and therefore represents an efficient solution method in which
    every possibility is tested to determine whether it is the solution.
  prefs: []
  type: TYPE_NORMAL
- en: The randomized search algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike the `GridSearchCV` method, not all parameter values are tested in this
    method, but the parameter settings are sampled in a fixed number. The parameter
    settings that are tested are set through the `n_iter` attribute. Sampling without
    replacement is performed if the parameters are presented as a list. If at least
    one parameter is supplied as a distribution, substitution sampling is used.
  prefs: []
  type: TYPE_NORMAL
- en: The Bayesian optimization algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The aim of a Bayesian hyperparameter optimizer is to construct a probability
    model of the objective function and use it to select the hyperparameters that
    work best for use in the real objective function. Bayesian statistics allow us
    to foresee not only a value, but a distribution, and this is the success of this
    methodology.
  prefs: []
  type: TYPE_NORMAL
- en: The Bayesian method, when compared with the two methods already dealt with (grid
    search and random search), stores the results of the past evaluation, which it
    uses to form a probabilistic model that associates the hyperparameters with a
    probability of a score on the objective function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This model is called a **surrogate** of the objective function and is much
    easier to optimize than the objective function itself. This result is obtained
    by following this procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: A surrogate probability model of the objective function is constructed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The hyperparameters that give the best results on the surrogate are searched.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These hyperparameters are applied to the real objective function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The surrogate model is updated by incorporating the new results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2–4 until you reach the pre-established iterations or the maximum
    time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this way, the surrogate probability model is updated after each evaluation
    of the objective function. To use a Bayesian hyperparameter optimizer, several
    libraries are available: `scikit-optimize`, `spearmint`, and `SMAC3`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Commonly, hyperparameters are all those values that can be freely set by the
    user, and that are generally optimized, maximizing the accuracy on the validation
    data with appropriate research. Even the choice of a technique rather than another
    can be seen as a categorical hyperparameter, which has as many values as the methods
    we can choose from.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official documentation of the `sklearn.model_selection.GridSearchCV()` function:[ https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hyperparameter optimization* (from Wikipedia): [https://en.wikipedia.org/wiki/Hyperparameter_optimization ](https://en.wikipedia.org/wiki/Hyperparameter_optimization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Spearmint Bayesian optimization* (from GitHub): [https://github.com/HIPS/Spearmint](https://github.com/HIPS/Spearmint)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The SMAC3 official documentation: [https://automl.github.io/SMAC3/stable/](https://automl.github.io/SMAC3/stable/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Tutorial on Bayesian Optimization for Machine Learning* (from the School
    of Engineering and Applied Sciences, Harvard University): [https://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf](https://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an event predictor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's apply all of this knowledge from this chapter to a real-world problem.
    We will build an SVM to predict the number of people going in and out of a building.
    The dataset is available at [https://archive.ics.uci.edu/ml/datasets/CalIt2+Building+People+Counts](https://archive.ics.uci.edu/ml/datasets/CalIt2+Building+People+Counts).
    We will use a slightly modified version of this dataset so that it's easier to
    analyze. The modified data is available in the `building_event_binary.txt` and
    the `building_event_multiclass.txt` files that are already provided to you. In
    this recipe, we will learn how to build an event predictor.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s understand the data format before we start building the model. Each
    line in `building_event_binary.txt` consists of six comma-separated strings. The
    ordering of these six strings is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Date
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of people going out of the building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of people coming into the building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The output indicating whether or not it''s an event**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first five strings form the input data, and our task is to predict whether
    or not an event is going on in the building.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each line in `building_event_multiclass.txt` consists of six comma-separated
    strings. This is more granular than the previous file, in the sense that the output
    is the exact type of event going on in the building. The ordering of these six
    strings is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Date
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of people going out of the building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of people coming into the building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The output indicating the type of event**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first five strings form the input data, and our task is to predict what
    type of event is going on in the building.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build an event predictor:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use `event.py` that''s already provided to you for reference. Create
    a new Python file, and add the following lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We just loaded all the data into `X`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s convert the data into numerical form:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s train the SVM using the radial basis function, Platt scaling, and class
    balancing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to perform cross-validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s test our SVM on a new datapoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following output on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'If you use the `building_event_multiclass.txt` file as the input data file
    instead of `building_event_binary.txt`, you will see the following output on your
    Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used data obtained from observations of people who flowed
    in and out of a building during 15 weeks, and at 48 time intervals per day. We
    therefore built a classifier able to predict the presence of an event such as
    a conference in the building, which determines an increase in the number of people
    present in the building for that period of time.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Later in the recipe, we used the same classifier on a different database to
    also predict the type of event that is held within the building.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official documentation of the `sklearn.svm.SVC()` function: [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.model_selection.cross_validate()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating traffic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An interesting application of SVMs is to predict traffic, based on related data.
    In the previous recipe, we used an SVM as a classifier. In this recipe, we will
    use an SVM as a regressor to estimate the traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the dataset available at [https://archive.ics.uci.edu/ml/datasets/Dodgers+Loop+Sensor](https://archive.ics.uci.edu/ml/datasets/Dodgers+Loop+Sensor).
    This is a dataset that counts the number of cars passing by during baseball games
    at the Los Angeles Dodgers home stadium. We will use a slightly modified form
    of that dataset so that it''s easier to analyze. You can use the `traffic_data.txt`
    file, already provided to you. Each line in this file contains comma-separated
    strings formatted in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: Day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The opponent team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether or not a baseball game is going on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of cars passing by
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to estimate traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how to build an SVM regressor. We will use `traffic.py` that''s
    already provided to you as a reference. Create a new Python file, and add the
    following lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We loaded all the input data into `X`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s encode this data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s build and train the SVM regressor using the radial basis function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding lines, the `C` parameter specifies the penalty for misclassification
    and `epsilon` specifies the limit within which no penalty is applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform cross-validation to check the performance of the regressor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s test it on a datapoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following printed on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used data collected by a sensor on the 101 North Highway
    in Los Angeles, near the stadium where the Dodgers play. This position is sufficiently
    close to the stadium to detect the increase in traffic that occurs during a match.
  prefs: []
  type: TYPE_NORMAL
- en: 'The observations were made over 25 weeks, over 288 time intervals per day (every
    5 minutes). We built a regressor based on the SVM algorithm to predict the presence
    of a baseball game at the Dodgers stadium. In particular, we can estimate the
    number of cars that pass that position on the basis of the value assumed by the
    following predictors: day, time, the opponent team, and whether or not a baseball
    game is going on.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Support vector regression** (**SVR**) is based on the same principles as
    SVMs. In fact, SVR is adapted from SVMs, where the dependent variable is numeric
    rather than categorical. One of the main advantages of using SVR is that it is
    a nonparametric technique.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.metrics.mean_absolute_error()` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Linear Regression and Support Vector Regression* (from the University of Adelaide):
    [https://cs.adelaide.edu.au/~chhshen/teaching/ML_SVR.pdf](https://cs.adelaide.edu.au/~chhshen/teaching/ML_SVR.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simplifying machine learning workflow using TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow** is an open source numerical calculation library. The library
    was created by Google programmers. It provides all the tools necessary to build
    deep learning models and offers developers a black-box interface to program.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will introduce the TensorFlow framework, using a simple
    neural network to classify the `iris` species. We will use the `iris` dataset,
    which has 50 samples from the following species:'
  prefs: []
  type: TYPE_NORMAL
- en: Iris setosa
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iris virginica
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iris versicolor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Four features are measured from each sample, namely the length and the width
    of the sepals and petals, in centimeters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following variables are contained:'
  prefs: []
  type: TYPE_NORMAL
- en: Sepal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class: `setosa`, `versicolor`, or `virginica`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to simplify machine learning workflow using TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start, as always, by importing the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The first two libraries are imported only to load and split the data. The third
    library loads the `tensorflow` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `iris` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Load and split the features and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The data is split into 70% for training and 30% for testing. The `random_state=1` parameter
    is the seed used by the random number generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will build a simple neural network with one hidden layer and 10 nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we fit the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then make the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will calculate the `accuracy` metric of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used the `tensorflow` library to build a simple neural network
    to classify iris species from four features measured. In this way, we saw how
    simple it is to implement a model based on a machine learning algorithm using
    the `tensorflow` library. This topic, and on deep neural networks in general,
    will be analyzed in detail in [Chapter 13](01c4a476-990c-40bf-8720-b8f71b2953d4.xhtml), *Deep
    Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow provides native APIs in Python, C, C++, Java, Go, and Rust. The third-party
    APIs available are in C#, R, and Scala. Since October 2017, it has integrated
    eager execution functionality which allows the immediate execution of the operations
    referred to by Python.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official documentation of the `tensorflow` library: [https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tensorflow for Deep Learning Research* (from Stanford University): [http://web.stanford.edu/class/cs20si/](http://web.stanford.edu/class/cs20si/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a stacking method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A combination of different approaches leads to better results: this statement
    works in different aspects of our life and also adapts to algorithms based on
    machine learning. Stacking is the process of combining various machine learning
    algorithms. This technique is due to David H. Wolpert, an American mathematician,
    physicist, and computer scientist.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to implement a stacking method.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the `heamy` library to stack the two models that we just used in
    the previous recipes. The `heamy` library is a set of useful tools for competitive
    data science.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to implement a stacking method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `boston` dataset, already used in [Chapter 1](f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml), *The
    Realm of Supervised Learning*, for the *Estimating housing prices* recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can build the two models that we will use in the stacking procedure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s time to stack these models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will train a `LinearRegression` model on stacked data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will calculate the results to validate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stacked generalization works by deducing the biases of the classifier/regressor
    relative to a supplied learning dataset. This deduction works by generalizing
    into a second space whose inputs are the hypotheses of the original generalizers
    and whose output is the correct hypothesis. When used with multiple generators,
    stacked generalization is an alternative to cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stacking tries to exploit the advantages of each algorithm by ignoring or correcting
    their disadvantages. It can be seen as a mechanism that corrects errors in your
    algorithms. Another library to perform a stacking procedure is StackNet.
  prefs: []
  type: TYPE_NORMAL
- en: '**StackNet** is a framework implemented in Java based on Wolpert''s stacked
    generalization on multiple levels to improve accuracy in machine learning predictive
    problems. The StackNet model functions as a neural network in which the transfer
    function takes the form of any supervised machine learning algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official documentation of the `heamy` library:[ https://heamy.readthedocs.io/en/latest/index.html](https://heamy.readthedocs.io/en/latest/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the `StackNet` framework: [https://github.com/kaz-Anova/StackNet](https://github.com/kaz-Anova/StackNet)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stacked Generalization* by David H. Wolpert: [http://www.machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf](http://www.machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
