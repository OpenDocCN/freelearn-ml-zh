["```py\n    > library(e1071)\n\n    ```", "```py\n    > model  = svm(churn~., data = trainset, kernel=\"radial\", cost=1, gamma = 1/ncol(trainset))\n\n    ```", "```py\n    > summary(model)\n\n    Call:\n    svm(formula = churn ~ ., data = trainset, kernel = \"radial\", cost = 1, gamma = 1/ncol(trainset))\n\n    Parameters:\n     SVM-Type:  C-classification \n     SVM-Kernel:  radial \n     cost:  1 \n     gamma:  0.05882353 \n\n    Number of Support Vectors:  691\n\n     ( 394 297 )\n\n    Number of Classes:  2 \n\n    Levels: \n     yes no\n\n    ```", "```py\n    > install.packages(\"klaR\")\n    > library(klaR)\n\n    ```", "```py\n    > getwd()\n\n    ```", "```py\n    > model.light  = svmlight(churn~., data = trainset, kernel=\"radial\", cost=1, gamma = 1/ncol(trainset))\n\n    ```", "```py\n    > iris.subset = subset(iris, select=c(\"Sepal.Length\", \"Sepal.Width\", \"Species\"), Species %in% c(\"setosa\",\"virginica\"))\n\n    ```", "```py\n    > plot(x=iris.subset$Sepal.Length,y=iris.subset$Sepal.Width, col=iris.subset$Species, pch=19)\n\n    ```", "```py\n    > svm.model = svm(Species ~ ., data=iris.subset, kernel='linear', cost=1, scale=FALSE)\n\n    ```", "```py\n    > points(iris.subset[svm.model$index,c(1,2)],col=\"blue\",cex=2)\n\n    ```", "```py\n    > w = t(svm.model$coefs) %*% svm.model$SV\n    > b = -svm.model$rho\n    > abline(a=-b/w[1,2], b=-w[1,1]/w[1,2], col=\"red\", lty=5)\n\n    ```", "```py\n    > plot(x=iris.subset$Sepal.Length,y=iris.subset$Sepal.Width, col=iris.subset$Species, pch=19)\n    > svm.model = svm(Species ~ ., data=iris.subset, type='C-classification', kernel='linear', cost=10000, scale=FALSE)\n    > points(iris.subset[svm.model$index,c(1,2)],col=\"blue\",cex=2)\n    > w = t(svm.model$coefs) %*% svm.model$SV\n    > b = -svm.model$rho\n    > abline(a=-b/w[1,2], b=-w[1,1]/w[1,2], col=\"red\", lty=5)\n\n    ```", "```py\n    > data(iris)\n    > model.iris  = svm(Species~., iris)\n    > plot(model.iris, iris, Petal.Width ~ Petal.Length, slice = list(Sepal.Width = 3, Sepal.Length = 4))\n\n    ```", "```py\n    > plot(model, trainset, total_day_minutes ~ total_intl_charge)\n\n    ```", "```py\n    > ?svm.plot\n\n    ```", "```py\n    > svm.pred = predict(model, testset[, !names(testset) %in% c(\"churn\")])\n\n    ```", "```py\n    > svm.table=table(svm.pred, testset$churn)\n    > svm.table\n\n    svm.pred yes  no\n     yes  70  12\n     no   71 865\n\n    ```", "```py\n    > classAgreement(svm.table)\n    $diag\n    [1] 0.9184676\n\n    $kappa\n    [1] 0.5855903\n\n    $rand\n    [1] 0.850083\n\n    $crand\n    [1] 0.5260472\n\n    ```", "```py\n    > library(caret)\n    > confusionMatrix(svm.table)\n    Confusion Matrix and Statistics\n\n    svm.pred yes  no\n     yes  70  12\n     no   71 865\n\n     Accuracy : 0.9185 \n     95% CI : (0.8999, 0.9345)\n     No Information Rate : 0.8615 \n     P-Value [Acc > NIR] : 1.251e-08 \n\n     Kappa : 0.5856 \n     Mcnemar's Test P-Value : 1.936e-10 \n\n     Sensitivity : 0.49645 \n     Specificity : 0.98632 \n     Pos Pred Value : 0.85366 \n     Neg Pred Value : 0.92415 \n     Prevalence : 0.13851 \n     Detection Rate : 0.06876 \n     Detection Prevalence : 0.08055 \n     Balanced Accuracy : 0.74139 \n\n     'Positive' Class : yes \n\n    ```", "```py\n    > library(car)\n    > data(Quartet)\n    > model.regression = svm(Quartet$y1~Quartet$x,type=\"eps-regression\")\n\n    ```", "```py\n    > predict.y = predict(model.regression, Quartet$x) \n    > predict.y\n     1        2        3        4        5        6        7        8 \n    8.196894 7.152946 8.807471 7.713099 8.533578 8.774046 6.186349 5.763689 \n     9       10       11 \n    8.726925 6.621373 5.882946 \n\n    ```", "```py\n    > plot(Quartet$x, Quartet$y1, pch=19)\n    > points(Quartet$x, predict.y, pch=15, col=\"red\")\n\n    ```", "```py\n    > tuned = tune.svm(churn~., data = trainset, gamma = 10^(-6:-1), cost = 10^(1:2))\n\n    ```", "```py\n    > summary(tuned)\n\n    Parameter tuning of 'svm':\n\n    - sampling method: 10-fold cross validation \n\n    - best parameters:\n     gamma cost\n     0.01  100\n\n    - best performance: 0.08077885 \n\n    - Detailed performance results:\n     gamma cost      error dispersion\n    1  1e-06   10 0.14774780 0.02399512\n    2  1e-05   10 0.14774780 0.02399512\n    3  1e-04   10 0.14774780 0.02399512\n    4  1e-03   10 0.14774780 0.02399512\n    5  1e-02   10 0.09245223 0.02046032\n    6  1e-01   10 0.09202306 0.01938475\n    7  1e-06  100 0.14774780 0.02399512\n    8  1e-05  100 0.14774780 0.02399512\n    9  1e-04  100 0.14774780 0.02399512\n    10 1e-03  100 0.11794484 0.02368343\n    11 1e-02  100 0.08077885 0.01858195\n    12 1e-01  100 0.12356135 0.01661508\n\n    ```", "```py\n    > model.tuned = svm(churn~., data = trainset, gamma = tuned$best.parameters$gamma, cost = tuned$best.parameters$cost)\n    > summary(model.tuned)\n\n    Call:\n    svm(formula = churn ~ ., data = trainset, gamma = 10^-2, cost = 100)\n\n    Parameters:\n     SVM-Type:  C-classification \n     SVM-Kernel:  radial \n     cost:  100 \n     gamma:  0.01 \n\n    Number of Support Vectors:  547\n\n     ( 304 243 )\n\n    Number of Classes:  2 \n\n    Levels: \n     yes no\n\n    ```", "```py\n    > svm.tuned.pred = predict(model.tuned, testset[, !names(testset) %in% c(\"churn\")])\n\n    ```", "```py\n    > svm.tuned.table=table(svm.tuned.pred, testset$churn)\n    > svm.tuned.table\n\n    svm.tuned.pred yes  no\n     yes  95  24\n     no   46 853\n\n    ```", "```py\n    > classAgreement(svm.tuned.table)\n    $diag\n    [1] 0.9312377\n\n    $kappa\n    [1] 0.691678\n\n    $rand\n    [1] 0.871806\n\n    $crand\n    [1] 0.6303615\n\n    ```", "```py\n    > confusionMatrix(svm.tuned.table)\n    Confusion Matrix and Statistics\n\n    svm.tuned.pred yes  no\n     yes  95  24\n     no   46 853\n\n     Accuracy : 0.9312 \n     95% CI : (0.9139, 0.946)\n     No Information Rate : 0.8615 \n     P-Value [Acc > NIR] : 1.56e-12 \n\n     Kappa : 0.6917 \n     Mcnemar's Test P-Value : 0.01207 \n\n     Sensitivity : 0.67376 \n     Specificity : 0.97263 \n     Pos Pred Value : 0.79832 \n     Neg Pred Value : 0.94883 \n     Prevalence : 0.13851 \n     Detection Rate : 0.09332 \n     Detection Prevalence : 0.11690 \n     Balanced Accuracy : 0.82320 \n\n     'Positive' Class : yes \n\n    ```", "```py\n    > ?svm.tune\n\n    ```", "```py\n    > data(iris)\n    > ind = sample(2, nrow(iris), replace = TRUE, prob=c(0.7, 0.3))\n    > trainset = iris[ind == 1,]\n    > testset = iris[ind == 2,]\n\n    ```", "```py\n    > install.packages(\"neuralnet\")\n    > library(neuralnet)\n\n    ```", "```py\n    > trainset$setosa = trainset$Species == \"setosa\"\n    > trainset$virginica = trainset$Species == \"virginica\"\n    > trainset$versicolor = trainset$Species == \"versicolor\"\n\n    ```", "```py\n    > network = neuralnet(versicolor + virginica + setosa~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, trainset, hidden=3)\n    > network\n    Call: neuralnet(formula = versicolor + virginica + setosa ~ Sepal.Length +     Sepal.Width + Petal.Length + Petal.Width, data = trainset,     hidden = 3)\n\n    1 repetition was calculated.\n\n     Error Reached Threshold Steps\n    1 0.8156100175    0.009994274769 11063\n\n    ```", "```py\n    > network$result.matrix\n     1\n    error                        0.815610017474\n    reached.threshold            0.009994274769\n    steps                    11063.000000000000\n    Intercept.to.1layhid1        1.686593311644\n    Sepal.Length.to.1layhid1     0.947415215237\n    Sepal.Width.to.1layhid1     -7.220058260187\n    Petal.Length.to.1layhid1     1.790333443486\n    Petal.Width.to.1layhid1      9.943109233330\n    Intercept.to.1layhid2        1.411026063895\n    Sepal.Length.to.1layhid2     0.240309549505\n    Sepal.Width.to.1layhid2      0.480654059973\n    Petal.Length.to.1layhid2     2.221435192437\n    Petal.Width.to.1layhid2      0.154879347818\n    Intercept.to.1layhid3       24.399329878242\n    Sepal.Length.to.1layhid3     3.313958088512\n    Sepal.Width.to.1layhid3      5.845670010464\n    Petal.Length.to.1layhid3    -6.337082722485\n    Petal.Width.to.1layhid3    -17.990352566695\n    Intercept.to.versicolor     -1.959842102421\n    1layhid.1.to.versicolor      1.010292389835\n    1layhid.2.to.versicolor      0.936519720978\n    1layhid.3.to.versicolor      1.023305801833\n    Intercept.to.virginica      -0.908909982893\n    1layhid.1.to.virginica      -0.009904635231\n    1layhid.2.to.virginica       1.931747950462\n    1layhid.3.to.virginica      -1.021438938226\n    Intercept.to.setosa          1.500533827729\n    1layhid.1.to.setosa         -1.001683936613\n    1layhid.2.to.setosa         -0.498758815934\n    1layhid.3.to.setosa         -0.001881935696\n\n    ```", "```py\n    > head(network$generalized.weights[[1]])\n\n    ```", "```py\n    > plot(network)\n\n    ```", "```py\n    > par(mfrow=c(2,2))\n    > gwplot(network,selected.covariate=\"Petal.Width\")\n    > gwplot(network,selected.covariate=\"Sepal.Width\")\n    > gwplot(network,selected.covariate=\"Petal.Length\")\n    > gwplot(network,selected.covariate=\"Petal.Width\")\n\n    ```", "```py\n    > ?gwplot\n\n    ```", "```py\n    > net.predict = compute(network, testset[-5])$net.result\n\n    ```", "```py\n    > net.prediction = c(\"versicolor\", \"virginica\", \"setosa\")[apply(net.predict, 1, which.max)]\n\n    ```", "```py\n    > predict.table = table(testset$Species, net.prediction)\n    > predict.table\n     prediction\n     setosa versicolor virginica\n     setosa         20          0         0\n     versicolor      0         19         1\n     virginica       0          2        16\n\n    ```", "```py\n    > classAgreement(predict.table)\n    $diag\n    [1] 0.9444444444\n\n    $kappa\n    [1] 0.9154488518\n\n    $rand\n    [1] 0.9224318658\n\n    $crand\n    [1] 0.8248251737\n\n    ```", "```py\n    > confusionMatrix(predict.table)\n    Confusion Matrix and Statistics\n\n     prediction\n     setosa versicolor virginica\n     setosa         20          0         0\n     versicolor      0         19         1\n     virginica       0          2        16\n\n    Overall Statistics\n\n     Accuracy : 0.9482759 \n     95% CI : (0.8561954, 0.9892035) \n     No Information Rate : 0.362069 \n     P-Value [Acc > NIR] : < 0.00000000000000022204\n\n     Kappa : 0.922252 \n     Mcnemar's Test P-Value : NA \n\n    Statistics by Class:\n\n     Class: setosa Class: versicolor Class: virginica\n    Sensitivity              1.0000000         0.9047619        0.9411765\n    Specificity              1.0000000         0.9729730        0.9512195\n    Pos Pred Value           1.0000000         0.9500000        0.8888889\n    Neg Pred Value           1.0000000         0.9473684        0.9750000\n    Prevalence               0.3448276         0.3620690        0.2931034\n    Detection Rate           0.3448276         0.3275862        0.2758621\n    Detection Prevalence     0.3448276         0.3448276        0.3103448\n    Balanced Accuracy        1.0000000         0.9388674        0.9461980\n\n    ```", "```py\n    > compute(network, testset[-5])\n\n    ```", "```py\n    > install.packages(\"nnet\")\n    > library(nnet)\n\n    ```", "```py\n    > data(iris)\n    > set.seed(2)\n    > ind = sample(2, nrow(iris), replace = TRUE, prob=c(0.7, 0.3))\n    > trainset = iris[ind == 1,]\n    > testset = iris[ind == 2,]\n\n    ```", "```py\n    > iris.nn = nnet(Species ~ ., data = trainset, size = 2, rang = 0.1, decay = 5e-4, maxit = 200)\n    # weights:  19\n    initial  value 165.086674 \n    iter  10 value 70.447976\n    iter  20 value 69.667465\n    iter  30 value 69.505739\n    iter  40 value 21.588943\n    iter  50 value 8.691760\n    iter  60 value 8.521214\n    iter  70 value 8.138961\n    iter  80 value 7.291365\n    iter  90 value 7.039209\n    iter 100 value 6.570987\n    iter 110 value 6.355346\n    iter 120 value 6.345511\n    iter 130 value 6.340208\n    iter 140 value 6.337271\n    iter 150 value 6.334285\n    iter 160 value 6.333792\n    iter 170 value 6.333578\n    iter 180 value 6.333498\n    final  value 6.333471 \n    converged\n\n    ```", "```py\n    > summary(iris.nn)\n    a 4-2-3 network with 19 weights\n    options were - softmax modelling  decay=0.0005\n     b->h1 i1->h1 i2->h1 i3->h1 i4->h1 \n     -0.38  -0.63  -1.96   3.13   1.53 \n     b->h2 i1->h2 i2->h2 i3->h2 i4->h2 \n     8.95   0.52   1.42  -1.98  -3.85 \n     b->o1 h1->o1 h2->o1 \n     3.08 -10.78   4.99 \n     b->o2 h1->o2 h2->o2 \n     -7.41   6.37   7.18 \n     b->o3 h1->o3 h2->o3 \n     4.33   4.42 -12.16 \n\n    ```", "```py\n    > iris.predict = predict(iris.nn, testset, type=\"class\")\n\n    ```", "```py\n    > nn.table = table(testset$Species, iris.predict)\n     iris.predict\n     setosa versicolor virginica\n     setosa         17          0         0\n     versicolor      0         14         0\n     virginica       0          1        14\n\n    ```", "```py\n    > confusionMatrix(nn.table)\n    Confusion Matrix and Statistics\n\n     iris.predict\n     setosa versicolor virginica\n     setosa         17          0         0\n     versicolor      0         14         0\n     virginica       0          1        14\n\n    Overall Statistics\n\n     Accuracy : 0.9782609 \n     95% CI : (0.8847282, 0.9994498) \n     No Information Rate : 0.3695652 \n     P-Value [Acc > NIR] : < 0.00000000000000022204\n\n     Kappa : 0.9673063 \n     Mcnemar's Test P-Value : NA \n\n    Statistics by Class:\n\n     Class: setosa Class: versicolor\n    Sensitivity              1.0000000         0.9333333\n    Specificity              1.0000000         1.0000000\n    Pos Pred Value           1.0000000         1.0000000\n    Neg Pred Value           1.0000000         0.9687500\n    Prevalence               0.3695652         0.3260870\n    Detection Rate           0.3695652         0.3043478\n    Detection Prevalence     0.3695652         0.3043478\n    Balanced Accuracy        1.0000000         0.9666667\n     Class: virginica\n    Sensitivity                 1.0000000\n    Specificity                 0.9687500\n    Pos Pred Value              0.9333333\n    Neg Pred Value              1.0000000\n    Prevalence                  0.3043478\n    Detection Rate              0.3043478\n    Detection Prevalence        0.3260870\n    Balanced Accuracy           0.9843750\n\n    ```", "```py\n    > head(predict(iris.nn, testset))\n\n    ```"]