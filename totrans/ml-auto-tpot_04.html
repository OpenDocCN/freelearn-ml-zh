<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer037">
			<h1 id="_idParaDest-37"><em class="italic"><a id="_idTextAnchor036"/>Chapter 2</em>: Deep Dive into TPOT</h1>
			<p>In this chapter, you'll learn everything about the theoretical aspects of the TPOT library and its underlying architecture. Topics such as architecture and <strong class="bold">genetic programming</strong> (<strong class="bold">GP</strong>) will be crucial to having a full grasp of the inner workings of the library.</p>
			<p>We will go through TPOT use cases and dive deep into different approaches to solve various machine learning problems. You can expect to learn the basics of automation in regression and classification tasks.</p>
			<p>We will also go through a complete environment setup for standalone Python installation and the Anaconda distribution and show you how to set up a virtual environment.</p>
			<p>This chapter will cover the following topics:</p>
			<ul>
				<li>Introducing TPOT </li>
				<li>Types of problems TPOT can solve</li>
				<li>Installing TPOT and setting up the environment</li>
			</ul>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Technical requirements</h1>
			<p>To complete this chapter, you only need a computer with Python installed. Both the standalone version and Anaconda are fine. We'll go through the installation for both through a virtual environment toward the end of the chapter.</p>
			<p>There is no code for this chapter.</p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Introducing TPOT</h1>
			<p><strong class="bold">TPOT</strong>, or <strong class="bold">Tree-based Pipeline Optimization Tool</strong>, is an open source library for performing machine learning in an automated fashion with the Python programming language. Below the surface, it uses the <a id="_idIndexMarker086"/>well-known <strong class="bold">scikit-learn</strong> machine learning library to perform <a id="_idIndexMarker087"/>data preparation, transformation, and machine learning. It also uses GP procedures to discover the best-performing pipeline for a given dataset. The concept of GP is covered in later sections.</p>
			<p>As a rule of thumb, you should use TPOT every time you need an automated machine learning pipeline. Data science is a broad field, and libraries such as TPOT enable you to spend much more time on data gathering and cleaning, as everything else is done automatically.</p>
			<p>The following figure shows what a typical machine learning pipeline looks like:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="Images/B16954_02_001.jpg" alt="Figure 2.1 – Example machine learning pipeline&#13;&#10;" width="1330" height="616"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – Example machine learning pipeline</p>
			<p>The preceding figure shows which parts of a machine learning process can and can't be automated. The data gathering phase (<strong class="bold">Raw data</strong>) is essential for any machine learning project. In this phase, you gather data that will serve as input to a machine learning model. If the input data isn't good enough, or there's not enough of it, machine learning algorithms can't produce good-quality models.</p>
			<p>Assuming there's enough data and you can access it, the next most significant problem is data cleaning. This step can't be automated, at least not entirely, for obvious reasons. Every dataset is different; hence there's no single approach to data cleaning. Missing and misformatted values are the most common and the most time-consuming types of problem, and they typically require a substantial amount of domain knowledge to address successfully.</p>
			<p>Once you have a fair amount of well-prepared data, TPOT can come into play. TPOT uses GP to find the best algorithm for a particular task, so there's no need to manually choose and optimize a single algorithm. The <em class="italic">Darwinian process of natural selection</em> inspires genetic algorithms, but more on that in a couple of sections.</p>
			<p>The TPOT pipeline <a id="_idIndexMarker088"/>has many parameters, depending on the type of problem you are trying to solve (regression or classification). All of the parameters are discussed later in the chapter, but these are the ones you should know regardless of the problem type:</p>
			<ul>
				<li><strong class="source-inline">generations</strong>: Represents the number of iterations the pipeline optimization process is run for</li>
				<li><strong class="source-inline">population_size</strong>: Represents the number of individuals to retain in the GP population in every generation</li>
				<li><strong class="source-inline">offspring_size</strong>: Represents the number of offspring to produce in each generation</li>
				<li><strong class="source-inline">mutation_rate</strong>: Tells the GP algorithm how many pipelines to apply random changes to every generation</li>
				<li><strong class="source-inline">crossover_rate</strong>: Tells the GP algorithm how many pipelines to breed every generation</li>
				<li><strong class="source-inline">cv</strong>: Cross-validation technique used for evaluating pipelines</li>
				<li><strong class="source-inline">scoring</strong>: A function that is used to evaluate the quality of a given pipeline</li>
			</ul>
			<p>Once TPOT finishes with the optimization, it returns Python code for the best pipeline it found so you can proceed with model evaluation and validation on your own. A simplified example of a TPOT pipeline is shown in the following figure:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="Images/B16954_02_002.jpg" alt="Figure 2.2 – Example TPOT pipeline&#13;&#10;" width="1311" height="522"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Example TPOT pipeline</p>
			<p>The TPOT library was built on top of <a id="_idIndexMarker089"/>Python's well-known machine learning package <strong class="source-inline">scikit-learn</strong>. As a result, TPOT has access to all of its classes and methods. The preceding figure shows <strong class="bold">PCA</strong> and <strong class="bold">Polynomial features</strong> as two possible feature preprocessing operations. TPOT isn't limited to these two, but instead can use any of the following:</p>
			<ul>
				<li><strong class="source-inline">PCA</strong></li>
				<li><strong class="source-inline">RandomizedPCA</strong></li>
				<li><strong class="source-inline">PolynomialFeatures</strong></li>
				<li><strong class="source-inline">Binarizer</strong></li>
				<li><strong class="source-inline">StandardScaler</strong></li>
				<li><strong class="source-inline">MinMaxScaler</strong></li>
				<li><strong class="source-inline">MaxAbsScaler</strong></li>
				<li><strong class="source-inline">RobustScaler</strong></li>
			</ul>
			<p>These are all classes built into <strong class="source-inline">scikit-learn</strong>, used to modify the dataset in some way and return a modified dataset. The next step involves some kind of feature selection. This step aims to select only the features with good predictive power and discard the others. By doing so, TPOT is reducing the dimensionality of the machine learning problem, which as an end result makes the problem easier to solve.</p>
			<p>The preceding figure<a id="_idIndexMarker090"/> hides this abstraction behind the <strong class="bold">Select best features</strong> step. To perform this step, TPOT can use one of the following algorithms:</p>
			<ul>
				<li><strong class="source-inline">VarianceThreshold</strong></li>
				<li><strong class="source-inline">SelectKBest</strong></li>
				<li><strong class="source-inline">SelectPercentile</strong></li>
				<li><strong class="source-inline">SelectFwe</strong></li>
				<li><strong class="source-inline">RecursiveFeatureElimination</strong></li>
			</ul>
			<p>As you can see, TPOT is very flexible when it comes to model training approaches. To understand further what's going on below the surface, we'll need to cover a bit of GP. The following section does that.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>A brief overview of genetic programming</h2>
			<p><strong class="bold">GP</strong> is a type of evolutionary <a id="_idIndexMarker091"/>algorithm, a subset of machine learning (<em class="italic">Genetic Programming page, GP Team; June 1, 2019</em>). Evolutionary algorithms are used for finding solutions to problems that we as humans don't know how to solve directly. These algorithms generate solutions that are, at worst, comparable to the best human solutions, and often better.</p>
			<p>In machine learning, GP can be used to discover the relationship between features in a dataset (<strong class="bold">regression</strong>), and to group data into categories (<strong class="bold">classification</strong>). In regular software engineering, GP is applied through code synthesis, genetic improvement, automatic bug fixing, and in developing game-playing strategies (<em class="italic">Genetic Programming page, GP Team; June 1, 2019</em>).</p>
			<p>GP is inspired by biological evolution and its mechanisms. It uses algorithms based on random mutation, crossover, fitness functions, and generations to solve the previously described regression and classification tasks for machine learning. These properties should sound familiar, as we covered them in the previous section.</p>
			<p>The idea behind GP is essential for advancements in machine learning because it is based on the <em class="italic">Darwinian process of natural selection</em>. In machine learning terms, these processes are used to generate<a id="_idIndexMarker092"/> optimal solutions – models and hyperparameters.</p>
			<p>Genetic algorithms have three <a id="_idIndexMarker093"/>properties:</p>
			<ul>
				<li><strong class="bold">Selection</strong>: Consists of a population of possible solutions and the fitness function. Each fit is evaluated at every iteration.</li>
				<li><strong class="bold">Crossover</strong>: The process of selecting the best (fittest) solution and performing crossover to create a new population.</li>
				<li><strong class="bold">Mutation</strong>: Taking the children from the previous point and mutating them with some random modifications until the best solution is obtained.</li>
			</ul>
			<p>It is always a good idea to know the basics and the underlying architecture of the language/library you are dealing with. TPOT is user-friendly and easy to use, so it doesn't require us to know everything about GP and genetic algorithms. For that reason, this chapter won't dive deeper into the topic. If you are interested in learning more about GP, you'll find useful links at the end of the chapter.</p>
			<p>We've discussed a lot about the good sides of machine learning automation, TPOT, and GP. But are there any downsides? The following section addresses a couple of them.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>TPOT limitations</h2>
			<p>Thus far, we have discussed only the<a id="_idIndexMarker094"/> good things about the TPOT library and the automation of machine learning processes in general. In this case, the pros outweigh the cons, but we should still talk about potential downsides. The first one is the execution time. It will vary based on the size of the dataset and the hardware specifications, but in general, it will take a lot of time to finish – hours or days for large datasets and minutes for smaller ones.</p>
			<p>It is essential to understand why. With the default TPOT settings – 100 generations with 100 population sizes – TPOT will evaluate 10,000 pipelines before finishing. That is equivalent to performing feature engineering and training of a machine learning model 10,000 times. Because of this, TPOT is expected to run slowly. </p>
			<p>Things get more complicated if you decide to bring <strong class="bold">cross-validation</strong> into the picture. This term represents a procedure where a <a id="_idIndexMarker095"/>machine learning model is trained <em class="italic">k</em> times on <em class="italic">k – 1</em> subsets and evaluated on a separate subset. The goal of cross-validation is to have a more accurate representation of the model's performance. The choice of <em class="italic">k</em> is arbitrary, but in practice, the most common value is 10.</p>
			<p>In practice, cross-validation<a id="_idIndexMarker096"/> makes TPOT significantly slower. When using cross-validation, TPOT will evaluate 100 generations with 100 population sizes and 10 cross-validation folds by default. This results in 100,000 different pipelines to evaluate before finishing.</p>
			<p>To address this issue, TPOT introduced the <strong class="source-inline">max_time_mins</strong> parameter. It is set to <strong class="source-inline">None</strong> by default, but you can set its value explicitly to any integer. For example, specifying <strong class="source-inline">max_time_mins=10</strong> would give TPOT only 10 minutes to optimize the pipeline. It's not an ideal solution if you want the best results, but it comes in handy when you are on a tight schedule.</p>
			<p>The second downside is that TPOT can sometimes recommend different solutions (pipelines) for the same dataset. This will often be a problem when the TPOT optimizer is run for a short amount of time. For example, if you have used the <strong class="source-inline">max_time_mins</strong> parameter to limit how long the optimizer would run, it's not a surprise that you will get a slightly different "optimal" pipeline every time.</p>
			<p>This isn't a reason to worry, as all pipelines should still outperform anything you can do manually in the same time frame, but it is essential to know why this happens. There are two possible reasons:</p>
			<ul>
				<li><em class="italic">The TPOT optimizer didn't converge</em>: This is the most likely scenario. TPOT wasn't able to find an optimal pipeline due to lack of time, or the dataset was too complex to optimize for in the given time period (or both).</li>
				<li><em class="italic">There are multiple "optimal" pipelines</em>: It's not uncommon to see numerous approaches working identically for some machine learning problems. This is a more likely scenario if the dataset is relatively small.</li>
			</ul>
			<p>This section briefly introduced the TPOT library and explained its benefits and shortcomings. The next section<a id="_idIndexMarker097"/> goes over the types of problems TPOT is solving and discusses the automation of regression and classification tasks in great detail.</p>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Types of problems TPOT can solve</h1>
			<p>The TPOT library<a id="_idIndexMarker098"/> was designed as a go-to tool for automating machine learning tasks; hence, it should be able to handle with ease anything you throw at it. We will start using TPOT in a practical sense soon. <a href="B16954_03_Final_SK_ePub.xhtml#_idTextAnchor051"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring before Regression</em>, shows how to use the library to handle practical tasks with many examples, and the following chapters focus on other types of tasks.</p>
			<p>In general, TPOT can be used to handle the following types of tasks:</p>
			<ul>
				<li><strong class="bold">Regression</strong>: Where the<a id="_idIndexMarker099"/> target variable is continuous, such as age, height, weight, score, or price. Refer to <a href="B16954_01_Final_SK_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning and the Idea of Automation</em>, for a brief overview of regression.</li>
				<li><strong class="bold">Classification</strong>: Where the <a id="_idIndexMarker100"/>target variable is categorical, such as sold/did not sell, churn/did not churn, or yes/no. Refer to <a href="B16954_01_Final_SK_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning and the Idea of Automation</em>, for a brief overview of classification.</li>
				<li><strong class="bold">Parallel training</strong>: TPOT can handle the <a id="_idIndexMarker101"/>training of machine learning models in a parallel manner through the <strong class="bold">Dask</strong> library. Please read <a href="B16954_05_Final_SK_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 5</em></a>, <em class="italic">Parallel Training with TPOT and Dask</em>, to get a full picture.</li>
				<li><strong class="bold">Neural networks</strong>: TPOT can even<a id="_idIndexMarker102"/> build models based on state-of-the-art neural network algorithms in a fully automated fashion. Please read <a href="B16954_06_Final_SK_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 6</em></a>,<em class="italic"> Getting Started with Deep Learning – Crash Course in Neural Networks</em>, for a quick crash course on neural networks, and <a href="B16954_07_Final_SK_ePub.xhtml#_idTextAnchor086"><em class="italic">Chapter 7</em></a>, <em class="italic">Neural Network Classifier with TPOT</em>, for the practical implementation of TPOT.</li>
			</ul>
			<p>The rest of this section briefly discusses how TPOT handles regression and classification tasks and spends a good <a id="_idIndexMarker103"/>amount of time exploring and explaining their parameters, attributes, and functions. You will learn how TPOT handles parallel training with Dask in <a href="B16954_05_Final_SK_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 5</em></a>, <em class="italic">Parallel Training with TPOT and Dask</em>, and how it handles neural networks in <a href="B16954_06_Final_SK_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 6</em></a>, <em class="italic">Getting Started with Deep Learning – Crash Course in Neural Networks</em>, because these topics require covering the prerequisites first.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>How TPOT handles regression tasks</h2>
			<p>The TPOT library handles <a id="_idIndexMarker104"/>regression tasks through the <strong class="source-inline">tpot.TPOTRegressor</strong> class. This class performs an intelligent search over machine learning pipelines containing supervised regression models, preprocessors, feature selection techniques, and any other estimator or transformer that follows the <strong class="source-inline">scikit-learn</strong> API (<em class="italic">TPOT Documentation page, TPOT Team; November 5, 2019</em>).</p>
			<p>The same class also performs a search over the hyperparameters of all objects in a pipeline. The <strong class="source-inline">tpot.TPOTRegressor</strong> class allows us to fully customize the models, transformers, and parameters searched over through the <strong class="source-inline">config_dict</strong> parameter.</p>
			<p>We will now go over the parameters that the <strong class="source-inline">tpot.TPOTRegressor</strong> class expects when instantiated:</p>
			<ul>
				<li><strong class="source-inline">generations</strong>: Integer or None (default = <strong class="source-inline">100</strong>). An optional parameter that specifies the number of iterations to run the pipeline optimization process. It must be positive. If it is not defined, the <strong class="source-inline">max_time_mins</strong> parameter must be specified instead.</li>
				<li><strong class="source-inline">population_size</strong>: Integer (default = <strong class="source-inline">100</strong>). An optional parameter that specifies the number of individuals to retain in the GP population in every generation. Must be a positive number.</li>
				<li><strong class="source-inline">offspring_size</strong>: Integer (default = the same as <strong class="source-inline">population_size</strong>). An optional parameter used to specify the number of offsprings to produce in each GP generation. Must be a positive number.</li>
				<li><strong class="source-inline">mutation_rate</strong>: Float (default = <strong class="source-inline">0.9</strong>). An optional parameter used to specify the mutation rate for the GP algorithm. Must be in the range [0.0, 1.0]. This parameter is used to instruct the algorithm on how many pipelines to apply random changes to every generation.</li>
				<li><strong class="source-inline">crossover_rate</strong>: Float (default = <strong class="source-inline">0.1</strong>). An optional parameter that instructs the GP algorithm on how many pipelines to "breed" every generation. Must be in the range [0.0, 1.0].</li>
				<li><strong class="source-inline">scoring</strong>: String or<a id="_idIndexMarker105"/> callable (default = <strong class="source-inline">neg_mean_squared_error</strong>). An optional parameter used to specify the function name for regression pipeline evaluation. Can be <strong class="source-inline">neg_median_abs_value</strong>, <strong class="source-inline">neg_mean_abs_error</strong>, <strong class="source-inline">neg_mean_squared_error</strong>, or <strong class="source-inline">r2</strong>.</li>
				<li><strong class="source-inline">cv</strong>: Integer, cross-validation generator, or an iterable (default = <strong class="source-inline">5</strong>). An optional parameter used to specify a cross-validation strategy for evaluating regression pipelines. If the passed value is an integer, it expects the number of folds. In other cases, it expects an object to be used as a cross-validation generator, or an iterable yielding train/test splits, respectively.</li>
				<li><strong class="source-inline">subsample</strong>: Float (default = <strong class="source-inline">1.0</strong>). An optional parameter used to specify a value for a fraction of training samples used in the optimization process. Must be in the range [0.0, 1.0].</li>
				<li><strong class="source-inline">n_jobs</strong>: Integer (default = <strong class="source-inline">1</strong>). An optional parameter used to specify the number of processes to use in parallel for the evaluation of pipelines during optimization. Set it to <strong class="source-inline">-1</strong> to use all CPU cores. Set it to <strong class="source-inline">-2</strong> to use all but one CPU core.</li>
				<li><strong class="source-inline">max_time_mins</strong>: Integer or None (default = <strong class="source-inline">None</strong>). An optional parameter used to specify how many minutes TPOT can perform the optimization. TPOT will optimize for less time only if all of the generations evaluate before the specified max time in minutes.</li>
				<li><strong class="source-inline">max_eval_time_mins</strong>: Float (default = <strong class="source-inline">5</strong>). An optional parameter used to specify how many minutes TPOT has to evaluate a single pipeline. If the parameter is set to a high enough value, TPOT will evaluate more complex pipelines. At the same time, it also makes TPOT run longer. </li>
				<li><strong class="source-inline">random_state</strong>: Integer or None (default = <strong class="source-inline">None</strong>). An optional parameter used to specify the seed for a pseudo-random number generator. Use it to get reproducible results.</li>
				<li><strong class="source-inline">config_dict</strong>: Dictionary, string, or None (default = <strong class="source-inline">None</strong>). An optional parameter used to specify <a id="_idIndexMarker106"/>a configuration dictionary for customizing the operators and parameters that TPOT searches during optimization. Possible inputs are as follows:<p>a) <em class="italic">None</em>: TPOT uses the default configuration.</p><p>b) <em class="italic">Python dictionary</em>: TPOT uses your configuration.</p><p>c) <em class="italic">'TPOT light'</em>: String; TPOT will use a built-in configuration with only fast models and preprocessors.</p><p>d) <em class="italic">'TPOT MDR'</em>: String; TPOT will use a built-in configuration specialized for genomic studies.</p><p>e) <em class="italic">'TPOT sparse'</em>: String; TPOT will use a configuration dictionary with a one-hot encoder and operators that support sparse matrices.</p></li>
				<li><strong class="source-inline">template</strong>: String (default = <strong class="source-inline">None</strong>). An optional parameter used to specify a template of a predefined pipeline. Used to specify the desired structure for the machine learning pipeline evaluated by TPOT.</li>
				<li><strong class="source-inline">warm_start</strong>: Boolean (default = <strong class="source-inline">False</strong>). An optional parameter used to indicate whether the current instance should reuse the population from previous calls to the <strong class="source-inline">fit()</strong> function. This function is discussed later in the chapter.</li>
				<li><strong class="source-inline">memory</strong>: A memory object or a string (default = <strong class="source-inline">None</strong>). An optional parameter used to cache each transformer after calling the <strong class="source-inline">fit()</strong> function. This function is discussed later in the chapter.</li>
				<li><strong class="source-inline">use_dask</strong>: Boolean (default = <strong class="source-inline">False</strong>). An optional parameter used to specify whether <em class="italic">Dask-ML's</em> pipeline optimizations should be used.</li>
				<li><strong class="source-inline">periodic_checkpoint_folder</strong>: Path string (default = <strong class="source-inline">None</strong>). An optional parameter <a id="_idIndexMarker107"/>used to specify in which folder TPOT will save pipelines while optimizing.</li>
				<li><strong class="source-inline">early_stop</strong>: Integer (default = <strong class="source-inline">None</strong>). An optional parameter used to specify after how many generations TPOT will stop optimizing if there's no improvement.</li>
				<li><strong class="source-inline">verbosity</strong>: Integer (default = <strong class="source-inline">0</strong>). An optional parameter used to specify how much information TPOT outputs to the console while running. Possible options are as follows:<p>a) <em class="italic">0</em>: TPOT doesn't print anything.</p><p>b) <em class="italic">1</em>: TPOT prints minimal information.</p><p>c) <em class="italic">2</em>: TPOT prints more information and provides a progress bar.</p><p>d) <em class="italic">3</em>: TPOT prints everything and provides a progress bar.</p></li>
				<li><strong class="source-inline">disable_update_check</strong>: Boolean (default = <strong class="source-inline">False</strong>). An optional parameter that indicates whether the TPOT version checker should be disabled. You can ignore this parameter because it only tells you whether a newer version of the library is available, and has nothing to do with the actual training.</li>
			</ul>
			<p>That's a lot of parameters you should know about if your goal is to truly master the library – at least the part of it that handles regression tasks. We've only covered parameters for the <strong class="source-inline">tpot.TPOTRegressor</strong> class and we will discuss attributes and functions next. Don't worry; there are only a couple of them available. </p>
			<p>Let's start with attributes. There are three in total. These become available once the pipeline is fitted:</p>
			<ul>
				<li><strong class="source-inline">fitted_pipeline_</strong>: Pipeline object from <strong class="source-inline">scikit-learn</strong>. Shows you the best pipeline that TPOT discovered <a id="_idIndexMarker108"/>during the optimization for a given training set.</li>
				<li><strong class="source-inline">pareto_front_fitted_pipelines_</strong>: Python dictionary. It contains all the pipelines on the TPOT Pareto front. The dictionary key is the string representing the pipeline, and the value is the corresponding pipeline. This argument is available only when the <strong class="source-inline">verbosity</strong> parameter is set to <strong class="source-inline">3</strong>.</li>
				<li><strong class="source-inline">evaluated_individuals_</strong>: Python dictionary. It contains all evaluated pipelines. The dictionary key is the string representing the pipeline, and the value is a tuple containing the number of steps in each pipeline and the corresponding accuracy metric.</li>
			</ul>
			<p>We will see how the mentioned attributes work in practice in the following chapters. The only thing left to discuss for this section are functions belonging to the <strong class="source-inline">tpot.TPOTRegressor</strong> class. There are four in total:</p>
			<ul>
				<li><strong class="source-inline">fit(features, target, sample_weight=None, groups=None)</strong>: This function is used to run the TPOT optimization process. The <strong class="source-inline">features</strong> parameter is an array of the features/predictors/attributes used for predicting the target variable. The <strong class="source-inline">target</strong> parameter is also an array that specifies the list of target labels for prediction. The other two parameters are optional. The <strong class="source-inline">sample_weights</strong> parameter is an array indicating per-sample weights. Higher weights indicate more importance. The last parameter, <strong class="source-inline">groups</strong>, is an array that specifies group labels for the samples used when performing cross-validation. It should only be used in conjunction with group cross-validation functions. The <strong class="source-inline">fit()</strong> function returns a copy of the fitted TPOT object.</li>
				<li><strong class="source-inline">predict(features)</strong>: This function is used to generate new predictions based on the <strong class="source-inline">features</strong> parameter. This parameter is an array containing features/predictors/attributes for predicting the target variable. The function returns an array of predictions.</li>
				<li><strong class="source-inline">score(testing_features, testing_target)</strong>: This function returns a score of the optimized pipeline on a given testing data. The function accepts two parameters. The first one is <strong class="source-inline">testing_features</strong>. It is an array/feature matrix of the testing set. The second one is <strong class="source-inline">testing_target</strong>. It is also an array, but of<a id="_idIndexMarker109"/> target labels for prediction in the training set. The function returns the accuracy score on the test set. </li>
				<li><strong class="source-inline">export(output_file_name)</strong>: This function is used to export the optimized pipeline as Python code. The function accepts a single parameter, <strong class="source-inline">output_file_name</strong>. It is used to specify the path and a filename where the Python code should be stored. If the value for the mentioned parameter isn't specified, the whole pipeline is returned as text.</li>
			</ul>
			<p>With this overview of parameters, attributes, and functions, you are ready to use TPOT's regression capabilities in practice. <a href="B16954_03_Final_SK_ePub.xhtml#_idTextAnchor051"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring before Regression</em>, is packed with regression examples, so don't hesitate to jump to it if you want to automate regression tasks.</p>
			<p>The next section of this chapter discusses how TPOT handles classification tasks.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>How TPOT handles classification tasks</h2>
			<p>The TPOT library <a id="_idIndexMarker110"/>handles classification tasks through the <strong class="source-inline">tpot.TPOTClassifer</strong> class. This class performs a search over machine learning pipelines containing supervised regression models, preprocessors, feature selection techniques, and any other estimator or transformer that follows the <strong class="source-inline">scikit-learn</strong> API (<em class="italic">TPOT Documentation page, TPOT Team; November 5, 2019</em>). The class also performs a search over the hyperparameters of all objects in the pipeline.</p>
			<p>The <strong class="source-inline">tpot.TPOTClassifier</strong> class allows us to fully customize the models, transformers, and parameters that will be searched over through the <strong class="source-inline">config_dict</strong> parameter.</p>
			<p>The <strong class="source-inline">tpot.TPOTClassifier</strong> class contains mostly the same parameters, attributes, and functions that the previously discussed <strong class="source-inline">tpot.TPOTRegressor</strong> has, so going over all of them again in <a id="_idIndexMarker111"/>detail would be redundant. Instead, we will just mention the identical parameters, attributes, and functions, and we will introduce and explain the ones that are unique for classification or work differently.</p>
			<p>First, let's go over the parameters:</p>
			<ul>
				<li><strong class="source-inline">generations</strong></li>
				<li><strong class="source-inline">population_size</strong></li>
				<li><strong class="source-inline">offspring_size</strong></li>
				<li><strong class="source-inline">mutation_rate</strong></li>
				<li><strong class="source-inline">crossover_rate</strong> </li>
				<li><strong class="source-inline">scoring</strong>: String or callable (default = <strong class="source-inline">accuracy</strong>). This is an optional parameter used to evaluate the quality of a given pipeline for the classification problem. The following scoring functions can be used: <strong class="source-inline">accuracy</strong>, <strong class="source-inline">adjusted_rand_score</strong>, <strong class="source-inline">average_precision</strong>, <strong class="source-inline">balanced_accuracy</strong>, <strong class="source-inline">f1</strong>, <strong class="source-inline">f1_macro</strong>, <strong class="source-inline">f1_micro</strong>, <strong class="source-inline">f1_samples</strong>, <strong class="source-inline">f1_weighted</strong>, <strong class="source-inline">neg_log_loss</strong>, <strong class="source-inline">precision</strong>, <strong class="source-inline">recall</strong>, <strong class="source-inline">recall_macro</strong>, <strong class="source-inline">recall_micro</strong>, <strong class="source-inline">recall_samples</strong>, <strong class="source-inline">recall_weighted</strong>, <strong class="source-inline">jaccard</strong>, <strong class="source-inline">jaccard_macro</strong>, <strong class="source-inline">jaccard_micro</strong>, <strong class="source-inline">jaccard_samples</strong>, <strong class="source-inline">jaccard_weighted</strong>, <strong class="source-inline">roc_auc</strong>, <strong class="source-inline">roc_auc_ovr</strong>, <strong class="source-inline">roc_auc_ovo</strong>, <strong class="source-inline">roc_auc_ovr_weighted</strong>, or <strong class="source-inline">roc_auc_ovo_weighted</strong>. If you want to use a custom scoring function, you can pass it as a function with the following signature: <strong class="source-inline">scorer(estimator, X, y)</strong>.</li>
				<li><strong class="source-inline">cv</strong></li>
				<li><strong class="source-inline">subsample</strong></li>
				<li><strong class="source-inline">n_jobs</strong></li>
				<li><strong class="source-inline">max_time_mins</strong></li>
				<li><strong class="source-inline">max_eval_time_mins</strong></li>
				<li><strong class="source-inline">random_state</strong></li>
				<li><strong class="source-inline">config_dict</strong></li>
				<li><strong class="source-inline">template</strong></li>
				<li><strong class="source-inline">warm_start</strong></li>
				<li><strong class="source-inline">memory</strong></li>
				<li><strong class="source-inline">use_dask</strong></li>
				<li><strong class="source-inline">periodic_checkpoint_folder</strong></li>
				<li><strong class="source-inline">early_stop</strong></li>
				<li><strong class="source-inline">verbosity</strong></li>
				<li><strong class="source-inline">disable_update_check</strong> </li>
				<li><strong class="source-inline">log_file</strong>: File-like class or string (default = <strong class="source-inline">None</strong>). This is an optional parameter used to save progress content to a file. If a string value is provided, it should be the path and the filename of the desired output file.</li>
			</ul>
			<p>As we can see, one parameter <a id="_idIndexMarker112"/>has changed, and one parameter is completely new. To repeat – please refer to the preceding subsection for detailed clarifications on what every parameter does.</p>
			<p>Next, we have to discuss the attributes of the <strong class="source-inline">tpot.TPOTClassifier</strong> class. These become available once the pipeline optimization process is finished. There are three in total, and all of them behave identically to the <strong class="source-inline">tpot.TPOTRegressor</strong> class:</p>
			<ul>
				<li><strong class="source-inline">fitted_pipeline_</strong></li>
				<li><strong class="source-inline">pareto_front_fitted_pipelines_</strong></li>
				<li><strong class="source-inline">evaluated_individuals_</strong></li>
			</ul>
			<p>Finally, we will discuss functions. As with the parameters, all are listed, but only the new and changed ones are discussed in detail. There are five functions in total:</p>
			<ul>
				<li><strong class="source-inline">fit(features, classes, sample_weight=None, groups=None)</strong>: This function behaves identically to the one in <strong class="source-inline">tpot.TPOTRegressor</strong>, but the second parameter is called <strong class="source-inline">classes</strong> instead of <strong class="source-inline">target</strong>. This parameter expects an array of class labels for prediction.</li>
				<li><strong class="source-inline">predict(features)</strong> </li>
				<li><strong class="source-inline">predict_proba(features)</strong>: This function does the same task as the <strong class="source-inline">predict()</strong> function but returns class probabilities instead of classes. You can see where the model was completely certain about predictions and where it wasn't so certain by examining the probabilities. You can also use class probabilities to adjust the decision threshold. You will learn how to do that in <a href="B16954_04_Final_SK_ePub.xhtml#_idTextAnchor058"><em class="italic">Chapter 4</em></a>, <em class="italic">Exploring before Classification</em>.</li>
				<li><strong class="source-inline">score(testing_features, testing_target)</strong></li>
				<li><strong class="source-inline">export(output_file_name)</strong> </li>
			</ul>
			<p>You are now ready<a id="_idIndexMarker113"/> to see how TPOT works in practice. Most of the time, there's no need to mess around with some of the listed parameters, but you need to know that they exist for more advanced use cases. <a href="B16954_04_Final_SK_ePub.xhtml#_idTextAnchor058"><em class="italic">Chapter 4</em></a>, <em class="italic">Exploring before Classification</em>, is packed with classification examples, so don't hesitate to jump to it if you want to learn how to automate classification tasks.</p>
			<p>The next section of this chapter discusses how to set up a TPOT environment through a virtual environment, both for standalone Python installation and installation through Anaconda.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Installing TPOT and setting up the environment</h1>
			<p>This section discusses the last required <a id="_idIndexMarker114"/>step before diving into the practical stuff – installation and environment setup. It is <a id="_idIndexMarker115"/>assumed that you have Python 3 installed, either through the standalone installation or through Anaconda.</p>
			<p>You will learn how to set up a virtual environment for TPOT for the following scenarios:</p>
			<ul>
				<li>Standalone Python</li>
				<li>Anaconda</li>
			</ul>
			<p>There's no need to read both installation sections, so just pick whichever suits you better. There shouldn't be any difference with regards to installation between operating systems. If you have Python installed as a standalone installation, you have access to <strong class="source-inline">pip</strong> through the terminal. If you have it installed through Anaconda, you have access to Anaconda Navigator.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>Installing and configuring TPOT with standalone Python installation </h2>
			<p>Before proceeding, make sure to have Python <a id="_idIndexMarker116"/>and <strong class="source-inline">pip</strong> (package manager for Python) installed. You<a id="_idIndexMarker117"/> can check whether <strong class="source-inline">pip</strong> is installed by entering the following line of code into the terminal:</p>
			<p class="source-code">&gt; pip</p>
			<p>If you see output like the one in the following figure, you are good to go:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="Images/B16954_02_003.jpg" alt="Figure 2.3 – Checking whether pip is installed&#13;&#10;" width="967" height="733"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – Checking whether pip is installed</p>
			<p>We can now <a id="_idIndexMarker118"/>proceed to the virtual environment setup:</p>
			<ol>
				<li>The first thing to do<a id="_idIndexMarker119"/> is to install the <strong class="source-inline">virtualenv</strong> package. To do so, execute this line from the terminal:<p class="source-code"><strong class="bold">&gt; pip install virtualenv</strong></p></li>
				<li>After a couple of seconds, you should see a success message, as shown in the following figure:<div id="_idContainer020" class="IMG---Figure"><img src="Images/B16954_02_004.jpg" alt="Figure 2.4 – virtualenv installation&#13;&#10;" width="1380" height="490"/></div><p class="figure-caption">Figure 2.4 – virtualenv installation</p></li>
				<li>The next step is to <a id="_idIndexMarker120"/>create a folder where the TPOT environment will be stored. Ours is located in the <strong class="source-inline">Documents</strong> folder, but you can store it anywhere. Here are the exact shell lines <a id="_idIndexMarker121"/>you need to execute to create the folder and install the Python virtual environment:<p class="source-code"><strong class="bold">&gt; cd Documents</strong></p><p class="source-code"><strong class="bold">&gt; mkdir python_venvs</strong></p><p class="source-code"><strong class="bold">&gt; virtualenv python_venvs/tpot_env</strong></p></li>
				<li>The execution and results are shown in the following figure:<div id="_idContainer021" class="IMG---Figure"><img src="Images/B16954_02_005.jpg" alt="Figure 2.5 – Creating a virtual environment&#13;&#10;" width="1387" height="513"/></div><p class="figure-caption">Figure 2.5 – Creating a virtual environment</p><p>The environment is successfully installed now. </p></li>
				<li>To activate the <a id="_idIndexMarker122"/>environment, you need to execute the following line from the terminal: <p class="source-code"><strong class="bold">&gt; source python_venvs/tpot_env/bin/activate</strong></p></li>
				<li>The text in parentheses confirms that the environment is activated. Take a look at the change from the <strong class="source-inline">base</strong> environment to <strong class="source-inline">tpot_env</strong> in the following figure:<div id="_idContainer022" class="IMG---Figure"><img src="Images/B16954_02_006.jpg" alt="Figure 2.6 – Activating a virtual environment&#13;&#10;" width="1357" height="128"/></div><p class="figure-caption">Figure 2.6 – Activating a virtual environment</p></li>
				<li>To deactivate<a id="_idIndexMarker123"/> the environment, enter the following line into the terminal:<p class="source-code"><strong class="bold">&gt; deactivate</strong></p><p>You can see the results in the following figure:</p><div id="_idContainer023" class="IMG---Figure"><img src="Images/B16954_02_007.jpg" alt="Figure 2.7 – Deactivating a virtual environment&#13;&#10;" width="959" height="65"/></div><p class="figure-caption">Figure 2.7 – Deactivating a virtual environment</p><p>We now have everything needed to begin with the library installation. Throughout the entire book, we will need the following:</p><ul><li><strong class="source-inline">jupyterlab</strong>: A notebook environment required for analyzing and exploring data and building machine learning models in an interactive way.</li><li><strong class="source-inline">numpy</strong>: Python's go-to library for numerical computations.</li><li><strong class="source-inline">pandas</strong>: A well-known library for data loading, processing, preparation, transformation, aggregation, and even visualization.</li><li><strong class="source-inline">matplotlib</strong>: Python's standard data visualization library. We will use it sometimes for basic plots.</li><li><strong class="source-inline">seaborn</strong>: A data visualization library<a id="_idIndexMarker124"/> with more aesthetically pleasing visuals than <strong class="source-inline">matplotlib</strong>.</li><li><strong class="source-inline">scikit-learn</strong>: Python's go-to library for machine learning and everything related to it.</li><li><strong class="source-inline">TPOT</strong>: Used to find optimal machine learning pipelines in an automated fashion.</li></ul></li>
				<li>To install every<a id="_idIndexMarker125"/> mentioned library, you can execute the following line from the opened terminal window:<p class="source-code"><strong class="bold">&gt; pip install jupyterlab numpy pandas matplotlib seaborn scikit-learn TPOT</strong></p><p>Python will immediately start downloading and installing libraries, as shown in the following figure:</p><div id="_idContainer024" class="IMG---Figure"><img src="Images/B16954_02_008.jpg" alt="Figure 2.8 – Installing libraries with pip&#13;&#10;" width="1379" height="624"/></div><p class="figure-caption">Figure 2.8 – Installing libraries with pip</p></li>
				<li>To test whether the<a id="_idIndexMarker126"/> environment was successfully configured, we can open <strong class="source-inline">JupyterLab</strong> from the terminal. Execute the following shell command once the libraries are installed:<p class="source-code"><strong class="bold">&gt; jupyter lab</strong></p><p>If you see something <a id="_idIndexMarker127"/>similar to the following, then everything went according to plan. The browser window with Jupyter should open immediately:</p><div id="_idContainer025" class="IMG---Figure"><img src="Images/B16954_02_009.jpg" alt="Figure 2.9 – Starting JupyterLab for standalone installation&#13;&#10;" width="1379" height="383"/></div><p class="figure-caption">Figure 2.9 – Starting JupyterLab for standalone installation</p></li>
				<li>For the final check, we<a id="_idIndexMarker128"/> will take a look at which Python version came with the environment. This can be done straight from the notebooks, as shown in the following figure:<div id="_idContainer026" class="IMG---Figure"><img src="Images/B16954_02_010.jpg" alt="Figure 2.10 – Checking the Python version for the standalone installation&#13;&#10;" width="950" height="182"/></div><p class="figure-caption">Figure 2.10 – Checking the Python version for the standalone installation</p></li>
				<li>Finally, we will see whether the TPOT library was installed by importing it and printing the version. This check can also be done from the notebooks. Follow the instructions in the following figure to see how:</li>
			</ol>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="Images/B16954_02_011.jpg" alt="Figure 2.11 – Checking the TPOT version for the standalone installation&#13;&#10;" width="363" height="189"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.11 – Checking the TPOT version for the standalone installation</p>
			<p>TPOT is now successfully installed in <a id="_idIndexMarker129"/>a virtual environment. The next section covers how to install and configure the environment with <a id="_idIndexMarker130"/>Anaconda.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Installing and configuring TPOT through Anaconda</h2>
			<p>Before proceeding, make <a id="_idIndexMarker131"/>sure you have Anaconda installed on your machine. We will use Anaconda to create and manage our <a id="_idIndexMarker132"/>environment and do the configurations from there:</p>
			<ol>
				<li value="1">To start, open up Anaconda Navigator:<div id="_idContainer028" class="IMG---Figure"><img src="Images/B16954_02_012.jpg" alt="Figure 2.12 – Anaconda Navigator&#13;&#10;" width="1319" height="699"/></div><p class="figure-caption">Figure 2.12 – Anaconda Navigator</p></li>
				<li>To create a new virtual environment, click on the <strong class="bold">Create</strong> button in the bottom-left portion of the<a id="_idIndexMarker133"/> screen:<div id="_idContainer029" class="IMG---Figure"><img src="Images/B16954_02_013.jpg" alt="Figure 2.13 – Creating a new environment in Anaconda&#13;&#10;" width="1312" height="704"/></div><p class="figure-caption">Figure 2.13 – Creating a new environment in Anaconda</p></li>
				<li>After clicking on the <strong class="bold">Create</strong> button, a modal window will pop up. In this window, you will<a id="_idIndexMarker134"/> have to specify the environment name and Python version. Stick to the newest version available, which is 3.8 at this point. How you will name the environment is up to you arbitrary; just make sure it is descriptive enough for you to remember:<div id="_idContainer030" class="IMG---Figure"><img src="Images/B16954_02_014.jpg" alt="Figure 2.14 – Configuring the virtual environment name and Python version&#13;&#10;" width="1303" height="695"/></div><p class="figure-caption">Figure 2.14 – Configuring the virtual environment name and Python version</p><p>After a couple of seconds, you will see <a id="_idIndexMarker135"/>the new environment<a id="_idIndexMarker136"/> listed below the <strong class="source-inline">base (root)</strong> environment. Here's how it should look:</p><div id="_idContainer031" class="IMG---Figure"><img src="Images/B16954_02_015.jpg" alt="Figure 2.15 – Listing of all virtual environments&#13;&#10;" width="1284" height="697"/></div><p class="figure-caption">Figure 2.15 – Listing of all virtual environments</p></li>
				<li>You are now ready to<a id="_idIndexMarker137"/> install libraries in your virtual environment. Anaconda makes it easy to open the environment from the terminal, by clicking<a id="_idIndexMarker138"/> on the play button and selecting the <strong class="bold">Open Terminal</strong> option. This is visible in the following figure:<div id="_idContainer032" class="IMG---Figure"><img src="Images/B16954_02_016.jpg" alt="Figure 2.16 – Opening the virtual environment from the terminal&#13;&#10;" width="1270" height="691"/></div><p class="figure-caption">Figure 2.16 – Opening the virtual environment from the terminal</p><p>Once the terminal window opens up, you <a id="_idIndexMarker139"/>are ready to install the libraries. Throughout the entire book, we will need the following:</p><ul><li><strong class="source-inline">jupyterlab</strong>: A notebook<a id="_idIndexMarker140"/> environment required for analyzing and exploring data and building machine learning models in an interactive way.</li><li><strong class="source-inline">numpy</strong>: Python's go-to library for numerical computations.</li><li><strong class="source-inline">pandas</strong>: A well-known library for data loading, processing, preparation, transformation, aggregation, and even visualization.</li><li><strong class="source-inline">matplotlib</strong>: Python's standard data visualization library. We will use it sometimes for basic plots.</li><li><strong class="source-inline">seaborn</strong>: A data visualization library with more aesthetically pleasing visuals than <strong class="source-inline">matplotlib</strong>.</li><li><strong class="source-inline">scikit-learn</strong>: Python's go-to library for machine learning and everything related to it.</li><li><strong class="source-inline">TPOT</strong>: Used to find optimal machine learning pipelines in an automated fashion.</li></ul></li>
				<li>To install every mentioned library, you <a id="_idIndexMarker141"/>can execute the following line from the opened terminal window:<p class="source-code"><strong class="bold">&gt; pip install jupyterlab numpy pandas matplotlib seaborn scikit-learn TPOT</strong></p><p>Python should immediately <a id="_idIndexMarker142"/>start downloading and installing the libraries, as shown in the following figure:</p><div id="_idContainer033" class="IMG---Figure"><img src="Images/B16954_02_017.jpg" alt="Figure 2.17 – Library installation through the terminal&#13;&#10;" width="1377" height="394"/></div><p class="figure-caption">Figure 2.17 – Library installation through the terminal</p></li>
				<li>To test whether the environment was successfully configured, we can open <strong class="source-inline">JupyterLab</strong> from the terminal. Execute the following shell command once the libraries are installed:<p class="source-code"><strong class="bold">&gt; jupyter lab</strong></p><p>If you see something similar to the following, then everything went according to plan. The browser window with Jupyter should open immediately:</p><div id="_idContainer034" class="IMG---Figure"><img src="Images/B16954_02_018.jpg" alt="Figure 2.18 – Starting JupyterLab from the terminal&#13;&#10;" width="1379" height="407"/></div><p class="figure-caption">Figure 2.18 – Starting JupyterLab from the terminal</p></li>
				<li>For the final check, we will take<a id="_idIndexMarker143"/> a look at which Python version came with the<a id="_idIndexMarker144"/> environment. This can be done straight from the notebooks, as shown in the following figure:<div id="_idContainer035" class="IMG---Figure"><img src="Images/B16954_02_019.jpg" alt="Figure 2.19 – Checking the Python version&#13;&#10;" width="931" height="184"/></div><p class="figure-caption">Figure 2.19 – Checking the Python version</p></li>
				<li>Finally, we will see whether the TPOT library was installed by importing it and printing the version. This check can also be done from the notebooks. Follow the instructions in the following figure to see how:</li>
			</ol>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="Images/B16954_02_020.jpg" alt="Figure 2.20 – Checking the TPOT version&#13;&#10;" width="185" height="103"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.20 – Checking the TPOT version</p>
			<p>We are now ready<a id="_idIndexMarker145"/> to proceed with the practical uses of TPOT.</p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Summary</h1>
			<p>You've learned a lot in this chapter – from how TPOT works and GP to setting up the environment with <strong class="source-inline">pip</strong> and Anaconda. You are now ready to tackle hands-on tasks in an automated way.</p>
			<p>The following chapter dives deep into handling regression tasks with TPOT with a couple of examples. Everything discussed during this chapter will become much clearer soon, after we get our hands dirty. Then, in <a href="B16954_04_Final_SK_ePub.xhtml#_idTextAnchor058"><em class="italic">Chapter 4</em></a>, <em class="italic">Exploring before Classification</em>, you will further reinforce your knowledge by solving classification tasks.</p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>Q&amp;A</h1>
			<ol>
				<li value="1">In your own words, define the TPOT library.</li>
				<li>Name and explain a couple of TPOT 's limitations.</li>
				<li>How would you limit the optimization time in TPOT? </li>
				<li>Briefly define the term "genetic programming."</li>
				<li>List and explain the five parameters of the <strong class="source-inline">tpot.TPOTRegressor</strong> class.</li>
				<li>List and explain the different and new parameters introduced in the <strong class="source-inline">tpot.TPOTClassifier</strong> class.</li>
				<li>What are virtual environments and why are they useful?</li>
			</ol>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor049"/>Further reading</h1>
			<p>Here are the sources we referenced in this chapter:</p>
			<ul>
				<li><em class="italic">Genetic programming page</em>: <a href="http://geneticprogramming.com/">http://geneticprogramming.com</a></li>
				<li><em class="italic">TPOT documentation page</em>: <a href="http://epistasislab.github.io/tpot/%20">http://epistasislab.github.io/tpot/</a></li>
			</ul>
		</div>
	</div></body></html>