- en: Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: How fast has the world been changing? Well, technology and data have been changing
    just as quickly. With the advent of the internet and social media, our entire
    outlook on data has changed. Initially, the scope of most data analytics revolved
    around structured data. However, due to so much unstructured data being pumped
    in through the internet and social media, the spectrum of analytics has broadened.
    Large amounts of text data, images, sound, and video data are being generated
    every second. They contain lots of information that needs to be synthesized for
    business. Natural language processing is a technique through which we enable a
    machine to understand text or speech. Although unstructured data has a wide range,
    the scope of this chapter will be to expose you to text analytics.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 世界变化有多快？好吧，技术和数据变化同样快。随着互联网和社交媒体的出现，我们对数据的整个看法已经改变。最初，大多数数据分析的范围围绕结构化数据。然而，由于互联网和社交媒体中涌入的大量非结构化数据，分析的范围已经扩大。每秒钟都在生成大量的文本数据、图像、声音和视频数据。它们包含大量需要商业综合的信息。自然语言处理是一种技术，通过它我们可以使机器理解文本或语音。尽管非结构化数据范围广泛，但本章的目的是让你接触文本分析。
- en: Structured data is typically made up of fixed observations and fixed columns
    set up in relational databases or in a spreadsheet, whereas unstructured data
    doesn't have any structure, and it can't be set up in a relational database; rather,
    it needs a NoSQL database, example, video, text, and so on.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据通常由关系数据库或电子表格中设置的固定观察值和固定列组成，而非结构化数据没有任何结构，不能在关系数据库中设置；相反，它需要一个NoSQL数据库，例如视频、文本等。
- en: 'In this chapter, you will learn about the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解以下主题：
- en: The document term matrix
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档-词矩阵
- en: Different approaches to looking at text
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察文本的不同方法
- en: Sentiment analysis
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析
- en: Topic modeling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模
- en: The Bayesian technique
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯技术
- en: Text corpus
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本语料库
- en: A text corpus is text data that forms out of a single document or group of documents
    and can come from any language, such as English, German, Hindi, and so on. In
    today's world, most of the textual data flows from social media, such as Facebook,
    Twitter, blogging sites, and other platforms. Mobile applications have now been
    added to the list of such sources. The larger size of a corpus, which is called
    **corpora,** makes the analytics more accurate.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 文本语料库是由单个文档或一组文档形成的文本数据，可以来自任何语言，如英语、德语、印地语等。在当今世界，大部分文本数据来自社交媒体，如Facebook、Twitter、博客网站和其他平台。移动应用程序现在也被列入此类来源。语料库的规模越大，即称为**语料库**，分析就越准确。
- en: Sentences
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子
- en: A corpus can be broken into units, which are called **sentences**. Sentences
    hold the meaning and context of the corpus, once we combine them together. Sentence
    formation takes place with the help of parts of speech. Every sentence is separated
    from other sentences by a delimiter, such as a period, which we can make use of
    to break it up further. This is called **sentence tokenization**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库可以被分解成称为**句子**的单位。句子承载着语料库的意义和上下文，一旦我们将它们组合在一起。句子的形成是在词性的帮助下进行的。每个句子都由分隔符（如句号）与其他句子分开，我们可以利用它来进一步分解。这被称为**句子标记化**。
- en: Words
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词
- en: Words are the smallest unit of corpuses and take the shape of sentences when
    we put them in order by following the parts of speech. When we break down the
    sentences into words, it is called **word tokenization**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 单词是语料库中最小的单位，当我们按照词性顺序排列时，它们就形成了句子。当我们把句子分解成单词时，这被称为**词标记化**。
- en: Bags of words
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词袋
- en: When we have text as input data, we can't go ahead and work with raw text. Hence,
    it's imperative for that text input data to get converted into numbers or vectors
    of numbers, in order to make it usable for a number of algorithms.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有文本作为输入数据时，我们不能直接处理原始文本。因此，将文本输入数据转换为数字或数字向量，以便使其可用于多种算法，这是至关重要的。
- en: A bag of words model is one of the ways to make the text usable for the algorithms.
    Essentially, it is a representation of text that works on the occurrence of words
    in the document. It has nothing to do with the structure, order, and location;
    this model only looks for the count of the words as a feature.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型是将文本用于算法的一种方法。本质上，它是对文档中单词出现情况的表示。它与结构、顺序和位置无关；此模型只寻找单词作为特征的数量。
- en: The thought process behind this model is that having similar content means having
    a similar document.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型背后的思维过程是：内容相似意味着文档相似。
- en: 'The different steps to be taken in the bag of words model are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在词袋模型中需要采取的不同步骤如下：
- en: '**Building the corpus**: In this step, the documents are collected and combined
    together to form a corpus. For example, the famous song from the TV series Friends
    has been used here as a corpus:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建语料库**：在这个步骤中，收集并合并文档以形成一个语料库。例如，这里使用了电视剧《老友记》中的著名歌曲作为语料库：'
- en: '*I will be there for you'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*我会为你守候'
- en: When the rain starts to pour
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当雨开始倾盆而下
- en: I will be there for you
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我会为你守候
- en: Like I have been there before
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我以前去过那里一样
- en: I will be there for you*
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我会为你守候*
- en: Let's consider each line of this song as a separate document.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑这首歌的每一行作为一个单独的文档。
- en: '**Vocabulary building**: In this step, we figure out the unique words in the
    corpus and create a list of them:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇构建**：在这个步骤中，我们确定语料库中的独特单词，并创建它们的列表：'
- en: I
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我
- en: will
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将
- en: be
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: be
- en: there
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: there
- en: for
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: for
- en: you
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: you
- en: when
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: when
- en: the
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: the
- en: rain
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: rain
- en: starts
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: starts
- en: to
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: to
- en: pour
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: pour
- en: like
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: like
- en: have
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: have
- en: been
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: been
- en: before
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: before
- en: '**Document vector creation**: Now, it''s time to convert each document of text
    into a vector.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档向量创建**：现在，是时候将每个文本文档转换为向量了。'
- en: The simple way to do this is through a Boolean route. This means that raw text
    will be transformed into a document vector, with the help of the presence/absence
    of that text in the respective document.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 做这件事的简单方法是通过布尔路由。这意味着原始文本将通过该文本在相应文档中的存在/不存在转换为文档向量。
- en: 'For example, if the first line of the song is turned into a document containing
    *I will be there for you*, then the document vector will turn out as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果歌曲的第一行被转换成一个包含*I will be there for you*的文档，那么文档向量将如下所示：
- en: '|  | **Document vector** |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | **文档向量** |'
- en: '| I | 1 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| I | 1 |'
- en: '| will | 1 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| will | 1 |'
- en: '| be | 1 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| be | 1 |'
- en: '| there | 1 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| there | 1 |'
- en: '| for | 1 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| for | 1 |'
- en: '| you | 1 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| you | 1 |'
- en: '| when | 0 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| when | 0 |'
- en: '| the | 0 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| the | 0 |'
- en: '| rain | 0 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| rain | 0 |'
- en: '| starts | 0 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| starts | 0 |'
- en: '| to | 0 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| to | 0 |'
- en: '| pour | 0 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| pour | 0 |'
- en: '| like | 0 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| like | 0 |'
- en: '| have | 0 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| have | 0 |'
- en: '| been | 0 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| been | 0 |'
- en: '| before | 0 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| before | 0 |'
- en: All the words that are present in the document are marked as 1, and the rest
    are marked as 0.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 文档中出现的所有单词都被标记为1，其余的都被标记为0。
- en: Hence, the document vector for the first sentence is *[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0]*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第一句话的文档向量是*[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0]*。
- en: Similarly, the document vector for the second sentence is *[0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0]*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，第二句话的文档向量是*[0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0]*。
- en: 'As the size of the corpus continues to increase, the number of zeros in the
    document vector will rise, as well. As a result of that, it induces sparsity in
    the vector and it becomes a sparse vector. Computing a sparse vector becomes really
    challenging for various algorithms. Data cleansing is one of the ways to counter
    it, to some extent:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 随着语料库的大小持续增加，文档向量中的零的数量也会增加。因此，这导致向量稀疏，成为一个稀疏向量。对于各种算法来说，计算稀疏向量变得非常具有挑战性。数据清洗是应对这一问题的方法之一，在一定程度上：
- en: '**Cleansing the text**: This would involve transforming all of the corpus into
    a single case (either upper (preferably) or lower). The punctuation must be taken
    out of the corpus. Stemming, which means finding the root words of the text, can
    be incorporated, and will be able to reduce the unique words in the corpus. Also,
    removal of stop words, such as *is* and *of*, might be able to abate the pain
    of sparsity.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本清洗**：这包括将所有语料库转换为单个大小写（最好是小写）。必须从语料库中去除标点符号。可以采用词干提取，即找到文本的词根，这将能够减少语料库中的独特单词。此外，去除诸如“is”和“of”之类的停用词，可能有助于减轻稀疏性的痛苦。'
- en: '**Count vector**: There is another way to create the document vector, with
    the help of the frequency of the words appearing in the document. Let''s suppose
    that there is a corpus comprised of N documents and T tokens (words) have been
    extracted. These T tokens will form our dictionary. Hence, the dimension of the
    count vector matrix will turn out to be N X T. Every row contains the frequency
    of tokens (words) in that respective document comprising the dictionary.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计数向量**：创建文档向量的另一种方法是利用文档中单词出现的频率。假设有一个由N个文档和T个标记（单词）组成的语料库。这些T个标记将形成我们的词典。因此，计数向量矩阵的维度将是N
    X T。每一行都包含该文档中词典中单词的频率。'
- en: 'For example, let''s suppose that we have three documents:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有三个文档：
- en: '**N1**: Count vector has got count in it'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**N1**: 计数向量中包含计数'
- en: '**N2**: Is count vector better than the Boolean way of creating feature vector?'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**N2**: 计数向量是否比布尔方式创建特征向量更好？'
- en: '**N3**: Creation of feature vector is very important'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**N3**: 特征向量的创建非常重要'
- en: 'After removing `stopwords`, the count vector matrix turns out like the following
    table:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在移除`停用词`后，计数向量矩阵如下表所示：
- en: '|  | **count** | **vector** | **got** | **it** | **better** | **than** | **Boolean**
    | **way** | **creating** | **feature** | **creation** | **important** |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | **count** | **vector** | **got** | **it** | **better** | **than** | **Boolean**
    | **way** | **creating** | **feature** | **creation** | **important** |'
- en: '| N1 | 2 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| N1 | 2 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
- en: '| N2 | 1 | 2 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| N2 | 1 | 2 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 |'
- en: '| N3 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| N3 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 |'
- en: Now, take a look at the matrix dimension carefully; since *N=3* and *T=12*,
    that makes this a matrix of 3 x 12.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，仔细看看矩阵维度；由于*N=3*和*T=12*，这使得这是一个3 x 12的矩阵。
- en: We will look at how the matrix formation has taken place. For document N1, the
    number of times the count has occurred in it is 2, the number of times the vector
    has come is 1, and so on. Taking these frequencies, we enter these values. A similar
    process has been completed for the other two documents, as well.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看看矩阵是如何形成的。对于文档*N1*，计数出现的次数是2，向量出现的次数是1，等等。将这些频率输入这些值。其他两个文档也完成了类似的过程。
- en: However, this has a drawback. A highly frequent word might start to dominate
    the document, and the corpus, too, which will result in having limited information
    extracted out of the features. To counter this, **term frequency inverse-document
    frequency** (**TF-IDF**) has been introduced.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这有一个缺点。一个高频词可能会开始主导文档，甚至整个语料库，这会导致从特征中提取的信息有限。为了解决这个问题，引入了**词频逆文档频率**（**TF-IDF**）。
- en: TF-IDF
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF
- en: As we understood the limitation of count vectorization that a highly frequent
    word might spoil the party. Hence, the idea is to penalize the frequent words
    occurring in most of the documents by assigning them a lower weight and increasing
    the weight of the words that appear in a subset of documents. This is the principle upon
    which TF-IDF works.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到计数向量化的局限性，即一个高频词可能会破坏整体效果。因此，我们的想法是对在大多数文档中频繁出现的词进行惩罚，给它们分配较低的权重，并增加在文档子集中出现的词的权重。这就是TF-IDF工作的原理。
- en: 'TF-IDF is a measure of how important a term is with respect to a document and
    the entire corpus (collection of documents):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF是衡量一个术语相对于文档和整个语料库（文档集合）重要性的度量：
- en: '*TF-IDF(term) = TF(term)* IDF(term)*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF-IDF(term) = TF(term)* IDF(term)*'
- en: '**Term frequency** (**TF**) is the frequency of the word appearing in the document
    out of all the words in the same document. For example, if there are 1,000 words
    in a document and we have to find out the *TF* of a word *NLP* that has appeared
    50 times in that very document, we use the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**词频**（**TF**）是单词在文档中出现的频率，相对于文档中所有单词的总数。例如，如果一个文档中有1,000个单词，我们需要找出该文档中出现了50次的单词*NLP*的*TF*，我们使用以下公式：'
- en: '*TF(NLP)= 50/1000=0.05*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF(NLP)= 50/1000=0.05*'
- en: 'Hence, we can conclude the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以得出以下结论：
- en: '*TF(term) = Number of times the term appears in the document/total number of
    terms in the document*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF(term) = 术语在文档中出现的次数/文档中术语的总数*'
- en: 'In the preceding example , comprised of three documents, *N1*, *N2*, and *N3*,
    if the *TF* of the term *count* in the document *N1* needs to be found, it will
    turn out to be like the following formula:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，包含三个文档*N1*、*N2*和*N3*，如果需要找到文档*N1*中术语*count*的*TF*，它将如下所示：
- en: '*TF(count) N1= 2/ (2+1+1+1) = 2/5 = 0.4*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF(count) N1= 2/ (2+1+1+1) = 2/5 = 0.4*'
- en: It indicates the contribution of words to the document.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 它表示单词对文档的贡献。
- en: 'However,  IDF is an indicator of how significant this term is for the entire
    corpus:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，IDF是衡量该词在整个语料库中重要性的指标：
- en: '*IDF("count") = log(Total number of documents/Number of documents containing
    the term "count")*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*IDF("count") = log(总文档数/包含术语"count"的文档数)*'
- en: '*IDF("count") = log(3/2)= 0.17*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*IDF("count") = log(3/2)= 0.17*'
- en: 'Now, let''s calculate the IDF for the term *vector*:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算术语*vector*的IDF：
- en: '*IDF("vector")=log(3/3)= 0*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*IDF("vector")=log(3/3)= 0*'
- en: How do we interpret this? It implies that if the same word has appeared in all
    of the documents, then it is not relevant to a particular document. But, if the
    word appears only in a subset of documents, this means that it holds some relevance
    to those documents in which it exists.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解释这个结果呢？它意味着如果一个词在所有文档中都出现过，那么它对这个特定文档来说并不相关。但是，如果一个词只出现在文档的子集中，这意味着它在存在的文档中具有一定的相关性。
- en: 'Let''s calculate the TF-IDF for *count* and *vector*, as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算*计数*和*向量*的TF-IDF，如下所示：
- en: '*TF-IDF(count) for Document N1= TF(count)*IDF(count)= 0.4 * 0.17 = 0.068*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*文档N1的TF-IDF计数 = TF计数 * IDF计数 = 0.4 * 0.17 = 0.068*'
- en: '*TF-IDF(vector) for Document N1 = TF(vector)* IDF(vector)= (1/5)*0 = 0*'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*文档N1的TF-IDF向量 = TF向量 * IDF向量 = (1/5) * 0 = 0*'
- en: It is quite evident that, since it assigns more weight to the *count* in *N1*,
    it is more important than the *vector*. The higher the weight value, the rarer
    the term. The smaller the weight, the more common the term. Search engines makes
    use of TF-IDF to retrieve the relevant documents pertaining to a query.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，由于它给N1中的*计数*分配了更多的权重，所以它比*向量*更重要。权重值越高，术语越稀有。权重值越小，术语越常见。搜索引擎利用TF-IDF检索与查询相关的相关文档。
- en: Now, we will look at how to execute the count vectorizer and TF-IDF vectorizer
    in Python.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨如何在Python中执行计数向量和TF-IDF向量器：
- en: Executing the count vectorizer
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行计数向量器
- en: 'The following are the steps for executing the `CountVectorizer`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 执行`CountVectorizer`的步骤如下：
- en: 'Import the library required for the count vectorizer:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的`CountVectorizer`库：
- en: '[PRE0]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Make a list of the text:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个文本列表：
- en: '[PRE1]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Tokenize the list of the text and build the vocabulary:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本列表分词并构建词汇表：
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You will get the following output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '![](img/c20b2298-018c-4ca4-8e27-761cbdd6be03.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c20b2298-018c-4ca4-8e27-761cbdd6be03.png)'
- en: 'Let''s take a look at the vocabulary that was created:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看创建的词汇表：
- en: '[PRE3]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We get the following output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/b7b02501-31a1-4008-a878-46fddfdfed4d.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b7b02501-31a1-4008-a878-46fddfdfed4d.png)'
- en: 'Now, we have to encode it, as follows:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须按照以下方式对其进行编码：
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s get a summary of the vector and find out the term matrix:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们总结一下向量的内容，并找出项矩阵：
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We get the following output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/4ecef6c4-0f67-42a4-ad3d-11ebebe2c372.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4ecef6c4-0f67-42a4-ad3d-11ebebe2c372.png)'
- en: Executing TF-IDF in Python
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中执行TF-IDF
- en: 'The following are the steps for executing TF-IDF in Python:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中执行TF-IDF的步骤如下：
- en: 'Import the library, as follows:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式导入库：
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s make a corpus by adding four documents, as follows:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过添加四个文档来创建语料库，如下所示：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s set up the vectorizer:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们设置向量器：
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We extract the features out of the text as follows:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们按照以下方式从文本中提取特征：
- en: '[PRE9]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/3c8fc2b7-1107-4330-8b93-ec961fd601d2.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3c8fc2b7-1107-4330-8b93-ec961fd601d2.png)'
- en: 'Here comes the document term matrix; every list indicates a document:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在出现了文档-词矩阵；每个列表表示一个文档：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We get the following output:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/123a887f-48b6-4607-8985-7aec3b856655.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/123a887f-48b6-4607-8985-7aec3b856655.png)'
- en: Sentiment analysis
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: Sentiment analysis is one of the application areas of natural language processing.
    It is widely in use across industries and domains, and there is a big need for
    it in the industry. Every organization is aiming to focus customers and their
    needs. Hence, to understand voice and sentiment, the customer turns out to be
    the prime goal, as knowing the pulse of the customers leads to revenue generation.
    Nowadays, customers voice their sentiments through Twitter, Facebook, or blogs.
    It takes some work to refine that textual data and make it consumable. Let's look
    at how to do it in Python.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是自然语言处理的应用领域之一。它在各个行业和领域中广泛应用，并且在行业中有着巨大的需求。每个组织都致力于关注客户及其需求。因此，为了理解声音和情感，客户成为首要目标，因为了解客户的脉搏可以带来收入。如今，客户通过Twitter、Facebook或博客表达他们的情感。需要做一些工作来提炼这些文本数据，使其可消费。让我们看看如何在Python中实现这一点。
- en: Here, verbatims of cinegoers have been taken from IMDB. This is shared on GitHub,
    too.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，电影观众的评论来自IMDB。这些评论也分享在GitHub上。
- en: 'We will launch the libraries , as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将启动库，如下所示：
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will load the dataset, as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下方式加载数据集：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, let''s explore the data and its dimensions:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索数据和其维度：
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We get the following output:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/dc6c039b-041d-4f4e-8f57-1d31af5f0253.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dc6c039b-041d-4f4e-8f57-1d31af5f0253.png)'
- en: 'We only need two variables, `review` and `label`, to build the model. We will
    just keep both of them in the data. A new dataframe has been created , as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要两个变量，`review`和`label`，来构建模型。我们将保留数据中的这两个变量。已经创建了一个新的数据框，如下所示：
- en: '[PRE14]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/7ede2db9-69bd-4f52-a7a3-e6b5a060f5a6.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7ede2db9-69bd-4f52-a7a3-e6b5a060f5a6.png)'
- en: 'Now, this is the step where we need to check how many categories are in `label`,
    as we are only interested in keeping the positive and negative ones:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这是我们需要检查`label`中有多少类别的步骤，因为我们只对保留正面和负面类别感兴趣：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/b6923b05-aaf6-4e0f-85bf-ef5b502faf68.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b6923b05-aaf6-4e0f-85bf-ef5b502faf68.png)'
- en: 'Now, it''s clear that there are three categories and we will get rid of `unsup`,
    as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，很明显有三个类别，我们将删除`unsup`，如下所示：
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We get the following output:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下输出：
- en: '![](img/2ae43214-b171-45f9-ab0b-10879fc26aef.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2ae43214-b171-45f9-ab0b-10879fc26aef.png)'
- en: 'Our data has now been set up. However, since we got rid of a few rows, we will
    reset the index of the data, as it sometimes causes some issues:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据现在已经设置好了。然而，由于我们删除了一些行，我们将重置数据的索引，因为有时这会引起一些问题：
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/2ae43214-b171-45f9-ab0b-10879fc26aef.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2ae43214-b171-45f9-ab0b-10879fc26aef.png)'
- en: 'We are done with it. Now, we will encode the `label` variable in order to make
    it usable for machine learning models. We have to use `LabelEncode` for that,
    as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了这个步骤。现在，我们将对`label`变量进行编码，以便使其可用于机器学习模型。我们必须使用`LabelEncode`来做到这一点，如下所示：
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We have to work on cleansing part of the data, in order to make it clean and
    standard, as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须对数据的一部分进行清洗，以便使其干净和标准化，如下所示：
- en: '[PRE19]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/5381500b-d813-4742-9ba4-8ac1167130dd.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5381500b-d813-4742-9ba4-8ac1167130dd.png)'
- en: 'Here, we are trying to get rid of the words that are less than `3` in length
    as the idea is that most of the words that are less than `3` in length don''t
    have much of an impact on the meaning:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们试图去除长度小于`3`的单词，因为大多数长度小于`3`的单词对意义的影响不大：
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/e0614fa7-5f4b-408c-97e8-d513ad32aff4.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e0614fa7-5f4b-408c-97e8-d513ad32aff4.png)'
- en: 'The tokenization of the data can now take place, as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的标记化现在可以进行，如下所示：
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/2b38f515-be3b-449b-9060-73f9e57224c0.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2b38f515-be3b-449b-9060-73f9e57224c0.png)'
- en: 'We are making use of stemming, in order to get rid of different variations
    of the same words. For example, we will look at satisfying, satisfy, and satisfied,
    as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用词干提取，以便去除相同单词的不同变体。例如，我们将查看satisfying、satisfy和satisfied，如下所示：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/b1ae0d82-9419-440d-a8b2-69cdab829bce.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b1ae0d82-9419-440d-a8b2-69cdab829bce.png)'
- en: 'After stemming, we have to join the data back, as we are heading towards producing
    a word cloud:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在词干提取后，我们必须将数据重新连接起来，因为我们正在朝着生成文字云的方向前进：
- en: '[PRE23]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We get the following output:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下输出：
- en: '![](img/a2ef8e36-ba0f-4e13-a0c2-aa610dabf9e0.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a2ef8e36-ba0f-4e13-a0c2-aa610dabf9e0.png)'
- en: 'Here, the tokenized data has been combined with the old `Newdata` dataframe:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们将标记化的数据与旧的`Newdata`数据框合并：
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following is the output for the preceding code:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对前面代码的输出：
- en: '![](img/f56c6aec-532d-47e5-934e-1653cd4ebd63.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f56c6aec-532d-47e5-934e-1653cd4ebd63.png)'
- en: 'A word cloud combining all of the words together has been produced:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 已经生成了一个将所有单词组合在一起的文字云：
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output can be seen as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/f99639e7-85e6-44a0-be40-80d09d359a4a.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f99639e7-85e6-44a0-be40-80d09d359a4a.png)'
- en: 'Now, we will make a word cloud for negative and positive sentiments separately,
    as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将分别对正面和负面情感制作文字云，如下所示：
- en: 'For `Negative` sentiments, we will use the following:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`负面`情感，我们将使用以下方法：
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following output shows a word cloud for `Negative` sentiments:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了`负面`情感的文字云：
- en: '![](img/65d91099-624b-46f6-a53d-8f582e57862a.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/65d91099-624b-46f6-a53d-8f582e57862a.png)'
- en: 'We will use the following for `Positive` sentiments:'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用以下内容用于`正面`情感：
- en: '[PRE27]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following output shows a word cloud for `Positive` sentiments:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了`正面`情感的文字云：
- en: '![](img/2f4b243f-f440-4218-8d86-eb80fadd5631.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2f4b243f-f440-4218-8d86-eb80fadd5631.png)'
- en: Sentiment classification
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分类
- en: 'We will take two approaches to sentiment classification (positive and negative),
    as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用两种方法来进行情感分类（正面和负面），如下所示：
- en: TF-IDF
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Count vectorization
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计数向量化
- en: Let's see which one gives us the better result.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看哪一个能给出更好的结果。
- en: TF-IDF feature extraction
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF特征提取
- en: 'The following code will provide us with the TF-IDF feature extraction:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将提供TF-IDF特征提取：
- en: '[PRE28]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We will get the following output:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '![](img/2ca570fe-5e99-4234-85b2-42105f2ef38b.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2ca570fe-5e99-4234-85b2-42105f2ef38b.png)'
- en: Count vectorizer bag of words feature extraction
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计数向量器单词袋特征提取
- en: 'The following code will show the count vectorizer for a bag of words:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将展示单词袋的计数向量器：
- en: '[PRE29]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Model building count vectorization
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型构建计数向量
- en: 'For building count vectorization we can split the data into train and test
    dataset as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建计数向量，我们可以将数据分为训练集和测试集，如下所示：
- en: '[PRE30]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We get the following output:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '![](img/91c1681e-3872-4632-a10b-20d98e6e2fb8.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/91c1681e-3872-4632-a10b-20d98e6e2fb8.png)'
- en: 'Here, we attain an accuracy of 84%. Let''s see how the TF-IDF approach fares:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们达到了84%的准确率。让我们看看TF-IDF方法的表现如何：
- en: '[PRE31]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/b27c588c-06aa-44ec-b149-2012a0477b3d.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b27c588c-06aa-44ec-b149-2012a0477b3d.png)'
- en: Here, the accuracy turns out to be 83.8% (a little less than the count vectorizer).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，准确率达到了83.8%（略低于计数向量器）。
- en: This completes building a model for sentiment classification.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了情感分类模型的构建。
- en: Topic modeling
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模
- en: Modeling is a methodology that's used to identify a topic and derive hidden
    patterns exhibited by a text corpus. Topic modeling resembles clustering, as we
    provide the number of topics as a hyperparameter (similar to the one used in clustering),
    which happens to be the number of clusters (k-means). Through this, we try to
    extract the number of topics or texts having some weights assigned to them.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是一种用于识别主题并推导出文本语料库所展现的隐藏模式的方法。主题建模类似于聚类，因为我们提供主题的数量作为超参数（类似于聚类中使用的超参数），这恰好是簇的数量（k-means）。通过这种方式，我们试图提取具有某些权重分配的主题数量或文本。
- en: The application of modeling lies in the area of document clustering, dimensionality
    reduction, information retrieval, and feature selection.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的应用领域包括文档聚类、降维、信息检索和特征选择。
- en: 'There are multiple ways to perform this, as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标有多种方法，如下所示：
- en: '**Latent dirichlet allocation** (**LDA**): It''s based on probabilistic graphical
    models'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）：它基于概率图模型'
- en: '**Latent semantic analysis** (**LSA**): It works on linear algebra (singular
    value decomposition)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在语义分析**（**LSA**）：它基于线性代数（奇异值分解）'
- en: '**Non-negative matrix factorization**: It''s based on linear algebra'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非负矩阵分解**：它基于线性代数'
- en: We will primarily discuss LDA, which is considered the most popular of all.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要讨论LDA，它被认为是所有中最受欢迎的。
- en: LDA is a matrix factorization technique that works on an assumption that documents
    are formed out of a number of topics, and, in turn, topics are formed out of words.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一种矩阵分解技术，它基于这样一个假设：文档是由多个主题组成的，而主题是由单词组成的。
- en: 'Having read the previous sections, you should be aware that any corpus can
    be represented as a document-term matrix. The following matrix shows a corpus
    of **M** documents and a vocabulary size of **N** words that makes an **M x N
    matrix**. All of the cells in this matrix have the frequency of the words in that
    particular document:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读了前面的章节之后，你应该知道任何语料库都可以表示为一个文档-词矩阵。以下矩阵显示了一个包含**M**个文档和**N**个单词的词汇量，形成一个**M
    x N**矩阵。这个矩阵中的所有单元格都有该特定文档中单词的频率：
- en: '![](img/a70489d5-5907-4d88-a0a1-4f5cbe7d3e8f.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a70489d5-5907-4d88-a0a1-4f5cbe7d3e8f.png)'
- en: 'This **M x N matrix of Document & Words** gets translated into two matrices
    by LDA: **M x X matrix of Documents & Topics** and **X x N matrix of Topics &
    Words**.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这个**文档与词的M x N矩阵**通过LDA转换为两个矩阵：**文档与主题的M x X矩阵**和**主题与词的X x N矩阵**。
- en: LDA architecture
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LDA架构
- en: In the LDA architecture, there are M number of documents having an N number
    of words, that get processed through the black strip called **LDA**. It delivers
    **X Topics** with **Cluster of words**. Each topic has psi distribution of words
    out of topics. Finally, it also comes up with a distribution of topics out of
    documents, which is denoted by phi.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在LDA架构中，有M个文档，包含N个单词，这些单词通过被称为**LDA**的黑色条带进行处理。它提供了**X个主题**和**单词簇**。每个主题都有来自主题的单词的psi分布。最后，它还提供了一个文档中主题的分布，用phi表示。
- en: 'The following diagram illustrates LDA:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了LDA：
- en: '![](img/b9500217-80f6-44c9-b757-32062bde9987.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b9500217-80f6-44c9-b757-32062bde9987.png)'
- en: 'With regard to the **Alpha** and **Beta** hyperparameters: alpha represents
    document-topic concentration and beta represents topic-word concentration. The
    higher the value of alpha, the more topics we get out of documents. On the other
    hand, the higher the value of beta, the more words there are in a topic. These
    can be tweaked based on the domain knowledge.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 关于**Alpha**和**Beta**超参数：alpha代表文档-主题浓度，beta代表主题-单词浓度。alpha的值越高，我们从文档中得到的主题就越多。另一方面，beta的值越高，一个主题中的单词就越多。这些可以根据领域知识进行调整。
- en: 'LDA iterates through each word of every document and assigns and adjusts a
    topic for it. A new topic *X* is assigned to it, on the basis of the product of
    two probabilities: *p1= (topic t/document d),* which means the proportion of the
    words of a document assigned to topic t, and *p2=(word w/topic t),* which refers
    to the proportion of assignments to topic *t* spread over all the documents, which
    has the word w associated with it.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: LDA遍历每篇文档中的每个单词，并为它分配和调整一个主题。基于两个概率的乘积，将一个新的主题*X*分配给它：*p1= (topic t/document
    d)*，这意味着分配给主题t的文档中单词的比例，以及*p2=(word w/topic t)*，这指的是分配给主题*t*的分配在整个文档中，其中与单词w相关联。
- en: With the number of passes, a good distribution of topic-word and topic-documents
    is attained.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遍历次数，我们得到了一个良好的主题-单词和主题-文档的分布。
- en: 'Let''s look at how it''s executed in Python:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它在Python中的执行方式：
- en: 'In this step, we are loading `dataset = fetch_20newsgroups`, which comes from
    `sklearn`:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们正在加载`dataset = fetch_20newsgroups`，它来自`sklearn`：
- en: '[PRE32]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In this step, we will clean the dataset. In order to do that, the `stopwords`
    and `WordNetLemmatizer` functions are required. Hence, the relevant libraries
    are must be loaded, as follows:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将清理数据集。为了做到这一点，需要`stopwords`和`WordNetLemmatizer`函数。因此，必须加载相关的库，如下所示：
- en: '[PRE33]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Ensure that you have downloaded the following dictionaries:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您已下载以下字典：
- en: '[PRE34]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here, a `clean` function is created to put the words in lowercase. Remove the
    `stopwords` and pick the words that have a length greater than `3`. Also, it makes
    it punctuation-free. Finally, lemmatize it , as follows:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里，创建了一个`clean`函数，将单词转换为小写。移除`stopwords`并选择长度大于`3`的单词。它还使其无标点符号。最后，进行词形还原，如下所示：
- en: '[PRE35]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, we have to make the document term matrix with the help of the `gensim`
    library. This library will also enable us to carry out LDA:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须在`gensim`库的帮助下创建文档-术语矩阵。这个库还将使我们能够执行LDA：
- en: '[PRE36]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'A document term matrix based on a bag of words is created here:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里创建了一个基于词袋的文档-术语矩阵：
- en: '[PRE37]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Here, a similar matrix is being created with the help of TF-IDF:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里，正在使用TF-IDF帮助创建一个类似的矩阵：
- en: '[PRE38]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s set up the model with a TF-IDF matrix. The number of topics has been
    given as `10`:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用TF-IDF矩阵设置模型。主题的数量已经给出为`10`：
- en: '[PRE39]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s take a look at the topic with words:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看包含单词的主题：
- en: '[PRE40]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/62a149b1-0e00-455a-9feb-f329a77498d1.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62a149b1-0e00-455a-9feb-f329a77498d1.png)'
- en: 'A similar exercise will be done for the bag of words; later, we will compare
    it:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于词袋模型，我们也将进行类似的练习；稍后，我们将进行比较：
- en: '[PRE41]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We get the following output:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下输出：
- en: '![](img/ae422b0d-be41-4c91-8387-ba7e26aa1c15.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae422b0d-be41-4c91-8387-ba7e26aa1c15.png)'
- en: Evaluating the model
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Log perplexity is a measure of how good an LDA model is. The lower the value
    of the perplexity, the better the model is:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对数困惑度是衡量LDA模型好坏的一个指标。困惑度值越低，模型越好：
- en: '[PRE42]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output for the log perplexity is as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 对数困惑度的输出如下：
- en: '![](img/522d8ec6-6359-444e-aed4-e2b7c52528cd.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/522d8ec6-6359-444e-aed4-e2b7c52528cd.png)'
- en: Visualizing the LDA
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化LDA
- en: 'In order to visualize the data, we can use the following code:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化数据，我们可以使用以下代码：
- en: '[PRE43]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output will be as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下：
- en: '![](img/06c44009-bf6c-47e8-9b71-83eeeab8da61.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06c44009-bf6c-47e8-9b71-83eeeab8da61.png)'
- en: 'We can enable the notebook here, as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在此启用笔记本，如下所示：
- en: '[PRE44]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/6afc488d-22f8-41f0-82b1-b6b7accbb05a.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6afc488d-22f8-41f0-82b1-b6b7accbb05a.png)'
- en: Let's try to interpret this.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试解释这个。
- en: 'On the left hand side, we have the topics, and on the right, we have the terms/words:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，我们有主题，在右侧，我们有术语/单词：
- en: A bigger circle size means more frequent topics.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 圆圈的大小越大，主题出现的频率就越高。
- en: Topics that are overlapping or closer to one another are similar.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相互重叠或彼此更接近的主题是相似的。
- en: Upon selecting a topic, the most representative words for the selected topic
    can be seen. This reflects how frequent the word is. One can toggle the weight
    of each property by using the slider.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个主题后，可以看到该主题的最具代表性的单词。这反映了单词的频率。可以通过使用滑块来调整每个属性的权重。
- en: Hovering over a topic will provide the contribution of words to the topic on
    the right and upon clicking on the word, we will see the circle size changing,
    which reflects how frequent that term is in that topic.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将鼠标悬停在主题上，将在右侧提供单词对该主题的贡献。点击单词后，我们将看到圆圈大小变化，这反映了该术语在该主题中的频率。
- en: The Naive Bayes technique in text classification
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类中的朴素贝叶斯技术
- en: Naive Bayes is a supervised classification algorithm that is based on Bayes
    theorem. It is a probabilistic algorithm. But, you might be wondering why it is
    called **Naive**. It is so because this algorithm works on an assumption that
    all the features are independent of each other. However, we are cognizant of the
    fact that independence of features might not be there in a real-world scenario.
    For example, if we are trying to detect whether an email is spam or not, all we
    look for are the keywords associated with spams such as Lottery, Award, and so
    on. Based on these, we extract those relevant features from the email and say
    that if given spam-related features, the email will be classified as spam.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一种基于贝叶斯定理的监督分类算法。它是一个概率算法。但是，你可能想知道为什么它被称为**朴素**。这是因为该算法基于一个假设，即所有特征都是相互独立的。然而，我们意识到在现实世界中特征之间的独立性可能不存在。例如，如果我们试图检测一封电子邮件是否为垃圾邮件，我们只寻找与垃圾邮件相关的关键词，如彩票、奖项等。基于这些，我们从电子邮件中提取相关特征，并说如果给定与垃圾邮件相关的特征，该电子邮件将被分类为垃圾邮件。
- en: The Bayes theorem
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: 'The Bayes theorem helps us in finding posterior probability, given a certain
    condition:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理帮助我们找到给定一定条件下的后验概率：
- en: '*P(A|B)= P(B|A) * P(A)/P(B)*'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(A|B)= P(B|A) * P(A)/P(B)*'
- en: '*A* and *B* can be deemed as the target and features, respectively.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* 和 *B* 可以被认为是目标变量和特征，分别。'
- en: 'Where, *P(A|B)*: posterior probability, which implies the probability of event
    *A*, given that *B* has taken place:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在哪里，*P(A|B)*：后验概率，表示在事件 *B* 发生的情况下事件 *A* 的概率：
- en: '*P(B|A)*: The likelihood that implies the probability of feature *B*, given
    the target *A*'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(B|A)*：在目标 *A* 给定的情况下，特征 *B* 的似然概率'
- en: '*P(A)*: The prior probability of target *A*'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(A)*：目标 *A* 的先验概率'
- en: '*P(B)*: The prior probability of feature *B*'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(B)*：特征 *B* 的先验概率'
- en: How the Naive Bayes classifier works
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器是如何工作的
- en: 'We will try to understand all of this by looking at the example of the Titanic.
    While the Titanic was sinking, a few of the categories had priority over others,
    in terms of being saved. We have the following dataset (it is a Kaggle dataset):'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过查看泰坦尼克号的例子来理解所有这些。当泰坦尼克号沉没时，一些类别在获救方面比其他类别有优先权。我们有以下数据集（这是一个Kaggle数据集）：
- en: '| **Person category** | **Survival chance** |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| **人员类别** | **生存机会** |'
- en: '| Woman | Yes |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 女性 | 是 |'
- en: '| Kid | Yes |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 儿童 | 是 |'
- en: '| Kid | Yes |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 儿童 | 是 |'
- en: '| Man | No |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 否 |'
- en: '| Woman | Yes |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 女性 | 是 |'
- en: '| Woman | Yes |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 女性 | 是 |'
- en: '| Man | No |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 否 |'
- en: '| Man | Yes |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 是 |'
- en: '| Kid | Yes |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 儿童 | 是 |'
- en: '| Woman | No |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 女性 | 否 |'
- en: '| Kid | No |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 儿童 | 否 |'
- en: '| Woman | No |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 女性 | 否 |'
- en: '| Man | Yes |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 是 |'
- en: '| Man | No |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 否 |'
- en: '| Woman | Yes |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 女性 | 是 |'
- en: 'Now, let''s prepare a likelihood table for the preceding information:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为前面的信息准备一个似然表：
- en: '|  |  | **Survival chance** |  |  |  |  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  |  | **生存机会** |  |  |  |  |'
- en: '|  |  | No | Yes | Grand Total |  |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 否 | 是 | 总计 |  |  |'
- en: '| Category | Kid | 1 | 3 | 4 | 4/15= | 0.27 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 儿童 | 1 | 3 | 4 | 4/15= | 0.27 |'
- en: '| Man | 3 | 2 | 5 | 5/15= | 0.33 |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 3 | 2 | 5 | 5/15= | 0.33 |  |'
- en: '| Woman | 2 | 4 | 6 | 6/15= | 0.40 |  |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 女性 | 2 | 4 | 6 | 6/15= | 0.40 |  |'
- en: '|  | Grand Total | 6 | 9 | 15 |  |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  | 总计 | 6 | 9 | 15 |  |  |'
- en: '|  |  | 6/15 | 9/15 |  |  |  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 6/15 | 9/15 |  |  |  |'
- en: '|  |  | 0.40 | 0.6 |  |  |  |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 0.40 | 0.6 |  |  |  |'
- en: 'Let''s find out which category of people had the maximum chance of survival:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找出哪个类别的人有最大的生存机会：
- en: '*Kid - P(Yes|Kid)= P(Kid|Yes) * P(Yes)/P(Kid)*'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '*儿童 - P(Yes|Kid)= P(Kid|Yes) * P(Yes)/P(Kid)*'
- en: '*P(Kid|Yes) = 3/9= 0.3*'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Kid|Yes) = 3/9= 0.3*'
- en: '*P(Yes) = 9/15 =0.6*'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Yes) = 9/15 =0.6*'
- en: '*P(Kid)= 4/15 =0.27*'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Kid)= 4/15 =0.27*'
- en: '*P(Yes|kid) = 0.33 *0.6/0.27=0.73*'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Yes|kid) = 0.33 *0.6/0.27=0.73*'
- en: '*Woman - P(Yes|Woman)= P(Woman|Yes) * P(Yes)/P(Woman)*'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '*女性 - P(Yes|Woman)= P(Woman|Yes) * P(Yes)/P(Woman)*'
- en: '*P(Woman|Yes) = 4/9= 0.44*'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Woman|Yes) = 4/9= 0.44*'
- en: '*P(Yes) = 9/15 =0.6*'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Yes) = 9/15 =0.6*'
- en: '* P(Woman)= 6/15 =0.4*'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Woman)= 6/15 =0.4*'
- en: '*P(Yes|Woman) = 0.44 *0.6/0.4=0.66*'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Yes|Woman) = 0.44 *0.6/0.4=0.66*'
- en: '*Man - P(Yes|Man)= P(Man|Yes) * P(Yes)/P(Man)*'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '*人 - P(Yes|人) = P(人|Yes) * P(Yes)/P(人)*'
- en: '*P(Man|Yes) = 2/9= 0.22*'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(人|Yes) = 2/9= 0.22*'
- en: '* P(Yes) = 9/15 =0.6*'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '* P(Yes) = 9/15 =0.6*'
- en: '* P(Man)= 6/15 =0.33*'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '* P(人)= 6/15 =0.33*'
- en: '*P(Yes|Man) = 0.22 *0.6/0.33=0.4*'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Yes|人) = 0.22 *0.6/0.33=0.4*'
- en: So, we can see that a child had the maximum chance of survival and a man the
    least chance.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到，孩子有最大的生存机会，而男人有最小的生存机会。
- en: 'Let''s perform the sentiment classification with the help of Naive Bayes, and
    see whether the result is better or worse:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们借助朴素贝叶斯进行情感分类，看看结果是否更好或更差：
- en: '[PRE45]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output is as follows:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/395e9ce6-4752-4128-9ecd-410866ff1142.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/395e9ce6-4752-4128-9ecd-410866ff1142.png)'
- en: Here, we can see that our previous results were better than the Naive Bayes
    results.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们之前的结果比朴素贝叶斯的结果要好。
- en: Summary
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we studied corpus building techniques that consists of sentences
    and words, which includes a bag of words to make the texts usable for the algorithms.
    You also learned about TF-IDF and how important a term is with respect to a document
    and the entire corpus. We went over sentiment analysis, along with classification
    and TF-IDF feature extraction.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了语料库构建技术，这些技术包括句子和单词，其中包括词袋模型，以便使文本可用于算法。你还了解了TF-IDF以及一个术语相对于文档和整个语料库的重要性。我们讨论了情感分析，以及分类和TF-IDF特征提取。
- en: You were also introduced to topic modeling and evaluating models, which includes
    visualizing LDA. We covered the Bayes theorem and working with the Naive Bayes
    classifier. In the next chapter, you will learn about temporal and sequential
    pattern discovery.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 你还介绍了主题建模和评估模型，包括可视化LDA。我们涵盖了贝叶斯定理以及与朴素贝叶斯分类器一起工作。在下一章中，你将学习关于时序和序列模式发现的内容。
