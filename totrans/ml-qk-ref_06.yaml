- en: Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How fast has the world been changing? Well, technology and data have been changing
    just as quickly. With the advent of the internet and social media, our entire
    outlook on data has changed. Initially, the scope of most data analytics revolved
    around structured data. However, due to so much unstructured data being pumped
    in through the internet and social media, the spectrum of analytics has broadened.
    Large amounts of text data, images, sound, and video data are being generated
    every second. They contain lots of information that needs to be synthesized for
    business. Natural language processing is a technique through which we enable a
    machine to understand text or speech. Although unstructured data has a wide range,
    the scope of this chapter will be to expose you to text analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Structured data is typically made up of fixed observations and fixed columns
    set up in relational databases or in a spreadsheet, whereas unstructured data
    doesn't have any structure, and it can't be set up in a relational database; rather,
    it needs a NoSQL database, example, video, text, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The document term matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different approaches to looking at text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Bayesian technique
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A text corpus is text data that forms out of a single document or group of documents
    and can come from any language, such as English, German, Hindi, and so on. In
    today's world, most of the textual data flows from social media, such as Facebook,
    Twitter, blogging sites, and other platforms. Mobile applications have now been
    added to the list of such sources. The larger size of a corpus, which is called
    **corpora,** makes the analytics more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Sentences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A corpus can be broken into units, which are called **sentences**. Sentences
    hold the meaning and context of the corpus, once we combine them together. Sentence
    formation takes place with the help of parts of speech. Every sentence is separated
    from other sentences by a delimiter, such as a period, which we can make use of
    to break it up further. This is called **sentence tokenization**.
  prefs: []
  type: TYPE_NORMAL
- en: Words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Words are the smallest unit of corpuses and take the shape of sentences when
    we put them in order by following the parts of speech. When we break down the
    sentences into words, it is called **word tokenization**.
  prefs: []
  type: TYPE_NORMAL
- en: Bags of words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we have text as input data, we can't go ahead and work with raw text. Hence,
    it's imperative for that text input data to get converted into numbers or vectors
    of numbers, in order to make it usable for a number of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: A bag of words model is one of the ways to make the text usable for the algorithms.
    Essentially, it is a representation of text that works on the occurrence of words
    in the document. It has nothing to do with the structure, order, and location;
    this model only looks for the count of the words as a feature.
  prefs: []
  type: TYPE_NORMAL
- en: The thought process behind this model is that having similar content means having
    a similar document.
  prefs: []
  type: TYPE_NORMAL
- en: 'The different steps to be taken in the bag of words model are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Building the corpus**: In this step, the documents are collected and combined
    together to form a corpus. For example, the famous song from the TV series Friends
    has been used here as a corpus:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*I will be there for you'
  prefs: []
  type: TYPE_NORMAL
- en: When the rain starts to pour
  prefs: []
  type: TYPE_NORMAL
- en: I will be there for you
  prefs: []
  type: TYPE_NORMAL
- en: Like I have been there before
  prefs: []
  type: TYPE_NORMAL
- en: I will be there for you*
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider each line of this song as a separate document.
  prefs: []
  type: TYPE_NORMAL
- en: '**Vocabulary building**: In this step, we figure out the unique words in the
    corpus and create a list of them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: will
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: be
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: there
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: for
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: you
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: when
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: the
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: rain
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: starts
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: to
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: pour
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: like
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: have
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: been
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: before
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document vector creation**: Now, it''s time to convert each document of text
    into a vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The simple way to do this is through a Boolean route. This means that raw text
    will be transformed into a document vector, with the help of the presence/absence
    of that text in the respective document.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if the first line of the song is turned into a document containing
    *I will be there for you*, then the document vector will turn out as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Document vector** |'
  prefs: []
  type: TYPE_TB
- en: '| I | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| will | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| be | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| there | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| for | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| you | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| when | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| the | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| rain | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| starts | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| to | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| pour | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| like | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| have | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| been | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| before | 0 |'
  prefs: []
  type: TYPE_TB
- en: All the words that are present in the document are marked as 1, and the rest
    are marked as 0.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the document vector for the first sentence is *[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0]*.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the document vector for the second sentence is *[0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the size of the corpus continues to increase, the number of zeros in the
    document vector will rise, as well. As a result of that, it induces sparsity in
    the vector and it becomes a sparse vector. Computing a sparse vector becomes really
    challenging for various algorithms. Data cleansing is one of the ways to counter
    it, to some extent:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cleansing the text**: This would involve transforming all of the corpus into
    a single case (either upper (preferably) or lower). The punctuation must be taken
    out of the corpus. Stemming, which means finding the root words of the text, can
    be incorporated, and will be able to reduce the unique words in the corpus. Also,
    removal of stop words, such as *is* and *of*, might be able to abate the pain
    of sparsity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Count vector**: There is another way to create the document vector, with
    the help of the frequency of the words appearing in the document. Let''s suppose
    that there is a corpus comprised of N documents and T tokens (words) have been
    extracted. These T tokens will form our dictionary. Hence, the dimension of the
    count vector matrix will turn out to be N X T. Every row contains the frequency
    of tokens (words) in that respective document comprising the dictionary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, let''s suppose that we have three documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '**N1**: Count vector has got count in it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**N2**: Is count vector better than the Boolean way of creating feature vector?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**N3**: Creation of feature vector is very important'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After removing `stopwords`, the count vector matrix turns out like the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **count** | **vector** | **got** | **it** | **better** | **than** | **Boolean**
    | **way** | **creating** | **feature** | **creation** | **important** |'
  prefs: []
  type: TYPE_TB
- en: '| N1 | 2 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| N2 | 1 | 2 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| N3 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Now, take a look at the matrix dimension carefully; since *N=3* and *T=12*,
    that makes this a matrix of 3 x 12.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at how the matrix formation has taken place. For document N1, the
    number of times the count has occurred in it is 2, the number of times the vector
    has come is 1, and so on. Taking these frequencies, we enter these values. A similar
    process has been completed for the other two documents, as well.
  prefs: []
  type: TYPE_NORMAL
- en: However, this has a drawback. A highly frequent word might start to dominate
    the document, and the corpus, too, which will result in having limited information
    extracted out of the features. To counter this, **term frequency inverse-document
    frequency** (**TF-IDF**) has been introduced.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we understood the limitation of count vectorization that a highly frequent
    word might spoil the party. Hence, the idea is to penalize the frequent words
    occurring in most of the documents by assigning them a lower weight and increasing
    the weight of the words that appear in a subset of documents. This is the principle upon
    which TF-IDF works.
  prefs: []
  type: TYPE_NORMAL
- en: 'TF-IDF is a measure of how important a term is with respect to a document and
    the entire corpus (collection of documents):'
  prefs: []
  type: TYPE_NORMAL
- en: '*TF-IDF(term) = TF(term)* IDF(term)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Term frequency** (**TF**) is the frequency of the word appearing in the document
    out of all the words in the same document. For example, if there are 1,000 words
    in a document and we have to find out the *TF* of a word *NLP* that has appeared
    50 times in that very document, we use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TF(NLP)= 50/1000=0.05*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we can conclude the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TF(term) = Number of times the term appears in the document/total number of
    terms in the document*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example , comprised of three documents, *N1*, *N2*, and *N3*,
    if the *TF* of the term *count* in the document *N1* needs to be found, it will
    turn out to be like the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TF(count) N1= 2/ (2+1+1+1) = 2/5 = 0.4*'
  prefs: []
  type: TYPE_NORMAL
- en: It indicates the contribution of words to the document.
  prefs: []
  type: TYPE_NORMAL
- en: 'However,  IDF is an indicator of how significant this term is for the entire
    corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '*IDF("count") = log(Total number of documents/Number of documents containing
    the term "count")*'
  prefs: []
  type: TYPE_NORMAL
- en: '*IDF("count") = log(3/2)= 0.17*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s calculate the IDF for the term *vector*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*IDF("vector")=log(3/3)= 0*'
  prefs: []
  type: TYPE_NORMAL
- en: How do we interpret this? It implies that if the same word has appeared in all
    of the documents, then it is not relevant to a particular document. But, if the
    word appears only in a subset of documents, this means that it holds some relevance
    to those documents in which it exists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s calculate the TF-IDF for *count* and *vector*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TF-IDF(count) for Document N1= TF(count)*IDF(count)= 0.4 * 0.17 = 0.068*'
  prefs: []
  type: TYPE_NORMAL
- en: '*TF-IDF(vector) for Document N1 = TF(vector)* IDF(vector)= (1/5)*0 = 0*'
  prefs: []
  type: TYPE_NORMAL
- en: It is quite evident that, since it assigns more weight to the *count* in *N1*,
    it is more important than the *vector*. The higher the weight value, the rarer
    the term. The smaller the weight, the more common the term. Search engines makes
    use of TF-IDF to retrieve the relevant documents pertaining to a query.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will look at how to execute the count vectorizer and TF-IDF vectorizer
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Executing the count vectorizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the steps for executing the `CountVectorizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the library required for the count vectorizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a list of the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Tokenize the list of the text and build the vocabulary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c20b2298-018c-4ca4-8e27-761cbdd6be03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at the vocabulary that was created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7b02501-31a1-4008-a878-46fddfdfed4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we have to encode it, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get a summary of the vector and find out the term matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ecef6c4-0f67-42a4-ad3d-11ebebe2c372.png)'
  prefs: []
  type: TYPE_IMG
- en: Executing TF-IDF in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the steps for executing TF-IDF in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the library, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s make a corpus by adding four documents, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s set up the vectorizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We extract the features out of the text as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c8fc2b7-1107-4330-8b93-ec961fd601d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here comes the document term matrix; every list indicates a document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/123a887f-48b6-4607-8985-7aec3b856655.png)'
  prefs: []
  type: TYPE_IMG
- en: Sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentiment analysis is one of the application areas of natural language processing.
    It is widely in use across industries and domains, and there is a big need for
    it in the industry. Every organization is aiming to focus customers and their
    needs. Hence, to understand voice and sentiment, the customer turns out to be
    the prime goal, as knowing the pulse of the customers leads to revenue generation.
    Nowadays, customers voice their sentiments through Twitter, Facebook, or blogs.
    It takes some work to refine that textual data and make it consumable. Let's look
    at how to do it in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Here, verbatims of cinegoers have been taken from IMDB. This is shared on GitHub,
    too.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will launch the libraries , as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will load the dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s explore the data and its dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc6c039b-041d-4f4e-8f57-1d31af5f0253.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We only need two variables, `review` and `label`, to build the model. We will
    just keep both of them in the data. A new dataframe has been created , as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/7ede2db9-69bd-4f52-a7a3-e6b5a060f5a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, this is the step where we need to check how many categories are in `label`,
    as we are only interested in keeping the positive and negative ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6923b05-aaf6-4e0f-85bf-ef5b502faf68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, it''s clear that there are three categories and we will get rid of `unsup`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ae43214-b171-45f9-ab0b-10879fc26aef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our data has now been set up. However, since we got rid of a few rows, we will
    reset the index of the data, as it sometimes causes some issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ae43214-b171-45f9-ab0b-10879fc26aef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are done with it. Now, we will encode the `label` variable in order to make
    it usable for machine learning models. We have to use `LabelEncode` for that,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We have to work on cleansing part of the data, in order to make it clean and
    standard, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5381500b-d813-4742-9ba4-8ac1167130dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we are trying to get rid of the words that are less than `3` in length
    as the idea is that most of the words that are less than `3` in length don''t
    have much of an impact on the meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e0614fa7-5f4b-408c-97e8-d513ad32aff4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The tokenization of the data can now take place, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2b38f515-be3b-449b-9060-73f9e57224c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are making use of stemming, in order to get rid of different variations
    of the same words. For example, we will look at satisfying, satisfy, and satisfied,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1ae0d82-9419-440d-a8b2-69cdab829bce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After stemming, we have to join the data back, as we are heading towards producing
    a word cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2ef8e36-ba0f-4e13-a0c2-aa610dabf9e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the tokenized data has been combined with the old `Newdata` dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output for the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f56c6aec-532d-47e5-934e-1653cd4ebd63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A word cloud combining all of the words together has been produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f99639e7-85e6-44a0-be40-80d09d359a4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will make a word cloud for negative and positive sentiments separately,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For `Negative` sentiments, we will use the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows a word cloud for `Negative` sentiments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65d91099-624b-46f6-a53d-8f582e57862a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will use the following for `Positive` sentiments:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows a word cloud for `Positive` sentiments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f4b243f-f440-4218-8d86-eb80fadd5631.png)'
  prefs: []
  type: TYPE_IMG
- en: Sentiment classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will take two approaches to sentiment classification (positive and negative),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Count vectorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see which one gives us the better result.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code will provide us with the TF-IDF feature extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ca570fe-5e99-4234-85b2-42105f2ef38b.png)'
  prefs: []
  type: TYPE_IMG
- en: Count vectorizer bag of words feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code will show the count vectorizer for a bag of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Model building count vectorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For building count vectorization we can split the data into train and test
    dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91c1681e-3872-4632-a10b-20d98e6e2fb8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we attain an accuracy of 84%. Let''s see how the TF-IDF approach fares:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b27c588c-06aa-44ec-b149-2012a0477b3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the accuracy turns out to be 83.8% (a little less than the count vectorizer).
  prefs: []
  type: TYPE_NORMAL
- en: This completes building a model for sentiment classification.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modeling is a methodology that's used to identify a topic and derive hidden
    patterns exhibited by a text corpus. Topic modeling resembles clustering, as we
    provide the number of topics as a hyperparameter (similar to the one used in clustering),
    which happens to be the number of clusters (k-means). Through this, we try to
    extract the number of topics or texts having some weights assigned to them.
  prefs: []
  type: TYPE_NORMAL
- en: The application of modeling lies in the area of document clustering, dimensionality
    reduction, information retrieval, and feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple ways to perform this, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latent dirichlet allocation** (**LDA**): It''s based on probabilistic graphical
    models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latent semantic analysis** (**LSA**): It works on linear algebra (singular
    value decomposition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-negative matrix factorization**: It''s based on linear algebra'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will primarily discuss LDA, which is considered the most popular of all.
  prefs: []
  type: TYPE_NORMAL
- en: LDA is a matrix factorization technique that works on an assumption that documents
    are formed out of a number of topics, and, in turn, topics are formed out of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having read the previous sections, you should be aware that any corpus can
    be represented as a document-term matrix. The following matrix shows a corpus
    of **M** documents and a vocabulary size of **N** words that makes an **M x N
    matrix**. All of the cells in this matrix have the frequency of the words in that
    particular document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a70489d5-5907-4d88-a0a1-4f5cbe7d3e8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This **M x N matrix of Document & Words** gets translated into two matrices
    by LDA: **M x X matrix of Documents & Topics** and **X x N matrix of Topics &
    Words**.'
  prefs: []
  type: TYPE_NORMAL
- en: LDA architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the LDA architecture, there are M number of documents having an N number
    of words, that get processed through the black strip called **LDA**. It delivers
    **X Topics** with **Cluster of words**. Each topic has psi distribution of words
    out of topics. Finally, it also comes up with a distribution of topics out of
    documents, which is denoted by phi.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates LDA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9500217-80f6-44c9-b757-32062bde9987.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With regard to the **Alpha** and **Beta** hyperparameters: alpha represents
    document-topic concentration and beta represents topic-word concentration. The
    higher the value of alpha, the more topics we get out of documents. On the other
    hand, the higher the value of beta, the more words there are in a topic. These
    can be tweaked based on the domain knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LDA iterates through each word of every document and assigns and adjusts a
    topic for it. A new topic *X* is assigned to it, on the basis of the product of
    two probabilities: *p1= (topic t/document d),* which means the proportion of the
    words of a document assigned to topic t, and *p2=(word w/topic t),* which refers
    to the proportion of assignments to topic *t* spread over all the documents, which
    has the word w associated with it.'
  prefs: []
  type: TYPE_NORMAL
- en: With the number of passes, a good distribution of topic-word and topic-documents
    is attained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at how it''s executed in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step, we are loading `dataset = fetch_20newsgroups`, which comes from
    `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we will clean the dataset. In order to do that, the `stopwords`
    and `WordNetLemmatizer` functions are required. Hence, the relevant libraries
    are must be loaded, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Ensure that you have downloaded the following dictionaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, a `clean` function is created to put the words in lowercase. Remove the
    `stopwords` and pick the words that have a length greater than `3`. Also, it makes
    it punctuation-free. Finally, lemmatize it , as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to make the document term matrix with the help of the `gensim`
    library. This library will also enable us to carry out LDA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'A document term matrix based on a bag of words is created here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, a similar matrix is being created with the help of TF-IDF:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s set up the model with a TF-IDF matrix. The number of topics has been
    given as `10`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the topic with words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62a149b1-0e00-455a-9feb-f329a77498d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A similar exercise will be done for the bag of words; later, we will compare
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae422b0d-be41-4c91-8387-ba7e26aa1c15.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Log perplexity is a measure of how good an LDA model is. The lower the value
    of the perplexity, the better the model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the log perplexity is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/522d8ec6-6359-444e-aed4-e2b7c52528cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing the LDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to visualize the data, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06c44009-bf6c-47e8-9b71-83eeeab8da61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can enable the notebook here, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6afc488d-22f8-41f0-82b1-b6b7accbb05a.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's try to interpret this.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the left hand side, we have the topics, and on the right, we have the terms/words:'
  prefs: []
  type: TYPE_NORMAL
- en: A bigger circle size means more frequent topics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topics that are overlapping or closer to one another are similar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upon selecting a topic, the most representative words for the selected topic
    can be seen. This reflects how frequent the word is. One can toggle the weight
    of each property by using the slider.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hovering over a topic will provide the contribution of words to the topic on
    the right and upon clicking on the word, we will see the circle size changing,
    which reflects how frequent that term is in that topic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Naive Bayes technique in text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naive Bayes is a supervised classification algorithm that is based on Bayes
    theorem. It is a probabilistic algorithm. But, you might be wondering why it is
    called **Naive**. It is so because this algorithm works on an assumption that
    all the features are independent of each other. However, we are cognizant of the
    fact that independence of features might not be there in a real-world scenario.
    For example, if we are trying to detect whether an email is spam or not, all we
    look for are the keywords associated with spams such as Lottery, Award, and so
    on. Based on these, we extract those relevant features from the email and say
    that if given spam-related features, the email will be classified as spam.
  prefs: []
  type: TYPE_NORMAL
- en: The Bayes theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Bayes theorem helps us in finding posterior probability, given a certain
    condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(A|B)= P(B|A) * P(A)/P(B)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*A* and *B* can be deemed as the target and features, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where, *P(A|B)*: posterior probability, which implies the probability of event
    *A*, given that *B* has taken place:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(B|A)*: The likelihood that implies the probability of feature *B*, given
    the target *A*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(A)*: The prior probability of target *A*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(B)*: The prior probability of feature *B*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the Naive Bayes classifier works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will try to understand all of this by looking at the example of the Titanic.
    While the Titanic was sinking, a few of the categories had priority over others,
    in terms of being saved. We have the following dataset (it is a Kaggle dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Person category** | **Survival chance** |'
  prefs: []
  type: TYPE_TB
- en: '| Woman | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Kid | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Kid | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Man | No |'
  prefs: []
  type: TYPE_TB
- en: '| Woman | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Woman | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Man | No |'
  prefs: []
  type: TYPE_TB
- en: '| Man | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Kid | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Woman | No |'
  prefs: []
  type: TYPE_TB
- en: '| Kid | No |'
  prefs: []
  type: TYPE_TB
- en: '| Woman | No |'
  prefs: []
  type: TYPE_TB
- en: '| Man | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Man | No |'
  prefs: []
  type: TYPE_TB
- en: '| Woman | Yes |'
  prefs: []
  type: TYPE_TB
- en: 'Now, let''s prepare a likelihood table for the preceding information:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | **Survival chance** |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | No | Yes | Grand Total |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Category | Kid | 1 | 3 | 4 | 4/15= | 0.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Man | 3 | 2 | 5 | 5/15= | 0.33 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Woman | 2 | 4 | 6 | 6/15= | 0.40 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Grand Total | 6 | 9 | 15 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 6/15 | 9/15 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 0.40 | 0.6 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s find out which category of people had the maximum chance of survival:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Kid - P(Yes|Kid)= P(Kid|Yes) * P(Yes)/P(Kid)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Kid|Yes) = 3/9= 0.3*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Yes) = 9/15 =0.6*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Kid)= 4/15 =0.27*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Yes|kid) = 0.33 *0.6/0.27=0.73*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Woman - P(Yes|Woman)= P(Woman|Yes) * P(Yes)/P(Woman)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Woman|Yes) = 4/9= 0.44*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Yes) = 9/15 =0.6*'
  prefs: []
  type: TYPE_NORMAL
- en: '* P(Woman)= 6/15 =0.4*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Yes|Woman) = 0.44 *0.6/0.4=0.66*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Man - P(Yes|Man)= P(Man|Yes) * P(Yes)/P(Man)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Man|Yes) = 2/9= 0.22*'
  prefs: []
  type: TYPE_NORMAL
- en: '* P(Yes) = 9/15 =0.6*'
  prefs: []
  type: TYPE_NORMAL
- en: '* P(Man)= 6/15 =0.33*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Yes|Man) = 0.22 *0.6/0.33=0.4*'
  prefs: []
  type: TYPE_NORMAL
- en: So, we can see that a child had the maximum chance of survival and a man the
    least chance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the sentiment classification with the help of Naive Bayes, and
    see whether the result is better or worse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/395e9ce6-4752-4128-9ecd-410866ff1142.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that our previous results were better than the Naive Bayes
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we studied corpus building techniques that consists of sentences
    and words, which includes a bag of words to make the texts usable for the algorithms.
    You also learned about TF-IDF and how important a term is with respect to a document
    and the entire corpus. We went over sentiment analysis, along with classification
    and TF-IDF feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: You were also introduced to topic modeling and evaluating models, which includes
    visualizing LDA. We covered the Bayes theorem and working with the Naive Bayes
    classifier. In the next chapter, you will learn about temporal and sequential
    pattern discovery.
  prefs: []
  type: TYPE_NORMAL
