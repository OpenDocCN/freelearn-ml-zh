- en: Feature Selection and Feature Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择和特征工程
- en: Feature engineering is the first step in a machine learning pipeline and involves
    all the techniques adopted to clean existing datasets, increase their signal-noise
    ratio, and reduce their dimensionality. Most algorithms have strong assumptions
    about the input data, and their performances can be negatively affected when raw
    datasets are used. Moreover, the data is seldom isotropic; there are often features
    that determine the general behavior of a sample, while others that are correlated
    don't provide any additional pieces of information. So, it's important to have
    a clear view of a dataset and know the most common algorithms used to reduce the
    number of features or select only the best ones.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是机器学习流程中的第一步，涉及所有用于清理现有数据集、增加其信噪比和减少其维度的技术。大多数算法对输入数据都有强烈的假设，当使用原始数据集时，它们的性能可能会受到负面影响。此外，数据很少是各向同性的；通常有一些特征决定了样本的一般行为，而其他相关的特征并不提供任何额外的信息。因此，了解数据集并知道用于减少特征数量或仅选择最佳特征的最常用算法是很重要的。
- en: scikit-learn toy datasets
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: scikit-learn 玩具数据集
- en: 'scikit-learn provides some built-in datasets that can be used for testing purposes.
    They''re all available in the package `sklearn.datasets` and have a common structure:
    the data instance variable contains the whole input set `X` while target contains
    the labels for classification or target values for regression. For example, considering
    the Boston house pricing dataset (used for regression), we have:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了一些内置数据集，可用于测试目的。它们都包含在 `sklearn.datasets` 包中，并具有一个共同的格式：数据实例变量包含整个输入集
    `X`，而目标包含分类的标签或回归的目标值。例如，考虑波士顿房价数据集（用于回归），我们有：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this case, we have 506 samples with 13 features and a single target value.
    In this book, we''re going to use it for regressions and the MNIST handwritten
    digit dataset (`load_digits()`) for classification tasks. scikit-learn also provides
    functions for creating dummy datasets from scratch: `make_classification()`, `make_regression()`,
    and `make_blobs()` (particularly useful for testing cluster algorithms). They''re
    very easy to use and in many cases, it''s the best choice to test a model without
    loading more complex datasets.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有 506 个样本，13 个特征和一个单一的目标值。在这本书中，我们将用它来进行回归，以及 MNIST 手写数字数据集（`load_digits()`）用于分类任务。scikit-learn
    还提供了从零开始创建虚拟数据集的函数：`make_classification()`、`make_regression()` 和 `make_blobs()`（特别适用于测试聚类算法）。它们非常易于使用，在许多情况下，这是测试模型而不加载更复杂数据集的最佳选择。
- en: Visit [http://scikit-learn.org/stable/datasets/](http://scikit-learn.org/stable/datasets/)
    for further information.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 [http://scikit-learn.org/stable/datasets/](http://scikit-learn.org/stable/datasets/)
    获取更多信息。
- en: The MNIST dataset provided by scikit-learn is limited for obvious reasons. If
    you want to experiment with the original one, refer to the website managed by
    Y. LeCun, C. Cortes, C. Burges: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
    Here you can download a full version made up of 70,000 handwritten digits already
    split into training and test sets.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供的 MNIST 数据集由于明显的原因而有限。如果您想实验原始版本，请参考由 Y. LeCun、C. Cortes 和 C.
    Burges 管理的网站：[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)。在这里，您可以下载一个包含
    70,000 个已拆分为训练集和测试集的手写数字的完整版本。
- en: Creating training and test sets
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建训练集和测试集
- en: 'When a dataset is large enough, it''s a good practice to split it into training
    and test sets; the former to be used for training the model and the latter to
    test its performances. In the following figure, there''s a schematic representation
    of this process:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集足够大时，将其拆分为训练集和测试集是一个好习惯；前者用于训练模型，后者用于测试其性能。在以下图中，有这个过程的示意图：
- en: '![](img/4face3c7-67a7-4b70-b26d-ed1f256b89b4.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4face3c7-67a7-4b70-b26d-ed1f256b89b4.png)'
- en: 'There are two main rules in performing such an operation:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此类操作有两个主要规则：
- en: Both datasets must reflect the original distribution
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个数据集都必须反映原始分布
- en: The original dataset must be randomly shuffled before the split phase in order
    to avoid a correlation between consequent elements
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在拆分阶段之前，原始数据集必须随机打乱，以避免后续元素之间的相关性。
- en: 'With scikit-learn, this can be achieved using the `train_test_split()` function:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn，这可以通过 `train_test_split()` 函数实现：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The parameter `test_size` (as well as `training_size`) allows specifying the
    percentage of elements to put into the test/training set. In this case, the ratio
    is 75 percent for training and 25 percent for the test phase. Another important
    parameter is `random_state` which can accept a NumPy `RandomState` generator or
    an integer seed. In many cases, it''s important to provide reproducibility for
    the experiments, so it''s also necessary to avoid using different seeds and, consequently,
    different random splits:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`test_size`（以及`training_size`）允许指定放入测试/训练集的元素百分比。在这种情况下，训练比为75%，测试比为25%。另一个重要参数是`random_state`，它可以接受NumPy的`RandomState`生成器或一个整数种子。在许多情况下，提供实验的可重复性很重要，因此也需要避免使用不同的种子，从而避免不同的随机分割：
- en: My suggestion is to always use the same number (it can also be 0 or completely
    omitted), or define a global `RandomState` which can be passed to all requiring
    functions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是始终使用相同的数字（也可以是0或完全省略），或者定义一个全局`RandomState`，它可以传递给所有需要的函数。
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this way, if the seed is kept equal, all experiments have to lead to the
    same results and can be easily reproduced in different environments by other scientists.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，如果种子保持相等，所有实验都必须得出相同的结果，并且可以由其他科学家在不同的环境中轻松复制。
- en: For further information about NumPy random number generation, visit [https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.RandomState.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.RandomState.html).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 想要了解更多关于NumPy随机数生成的信息，请访问[https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.RandomState.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.RandomState.html)。
- en: Managing categorical data
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理分类数据
- en: 'In many classification problems, the target dataset is made up of categorical
    labels which cannot immediately be processed by any algorithm. An encoding is
    needed and scikit-learn offers at least two valid options. Let''s consider a very
    small dataset made of 10 categorical samples with two features each:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多分类问题中，目标数据集由无法立即由任何算法处理的分类标签组成。需要编码，scikit-learn提供了至少两种有效选项。让我们考虑一个由10个具有两个特征的分类样本组成的小数据集：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The first option is to use the `LabelEncoder` class, which adopts a dictionary-oriented
    approach, associating to each category label a progressive integer number, that
    is an index of an instance array called `classes_`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种选择是使用`LabelEncoder`类，它采用字典导向的方法，将每个类别标签与一个递增的整数号码关联，即实例数组`classes_`的索引：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The inverse transformation can be obtained in this simple way:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用这种方式获得逆变换：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This approach is simple and works well in many cases, but it has a drawback:
    all labels are turned into sequential numbers. A classifier which works with real
    values will then consider similar numbers according to their distance, without
    any concern for semantics. For this reason, it''s often preferable to use so-called
    **one-hot** **encod****ing**, which binarizes the data. For labels, it can be
    achieved using the `LabelBinarizer` class:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法简单且在许多情况下效果良好，但它有一个缺点：所有标签都被转换成顺序数字。然后，使用实值工作的分类器将根据它们的距离考虑相似数字，而不考虑语义。因此，通常更倾向于使用所谓的**独热编码**，它将数据二进制化。对于标签，可以使用`LabelBinarizer`类实现：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this case, each categorical label is first turned into a positive integer
    and then transformed into a vector where only one feature is 1 while all the others
    are 0\. It means, for example, that using a softmax distribution with a peak corresponding
    to the main class can be easily turned into a discrete vector where the only non-null
    element corresponds to the right class. For example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，每个分类标签首先被转换成一个正整数，然后转换成一个向量，其中只有一个特征是1，而其他所有特征都是0。这意味着，例如，使用具有对应主要类别的峰值的softmax分布可以很容易地转换成一个离散向量，其中唯一的非空元素对应于正确的类别。例如：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Another approach to categorical features can be adopted when they''re structured
    like a list of dictionaries (not necessarily dense, they can have values only
    for a few features). For example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当分类特征的结构类似于字典列表（不一定是密集的，它们可能只为少数几个特征有值）时，可以采用另一种方法。例如：
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this case, scikit-learn offers the classes `DictVectorizer` and `FeatureHasher`;
    they both produce sparse matrices of real numbers that can be fed into any machine
    learning model. The latter has a limited memory consumption and adopts **MurmurHash
    3** (read [https://en.wikipedia.org/wiki/MurmurHash](https://en.wikipedia.org/wiki/MurmurHash),
    for further information). The code for these two methods is shown as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，scikit-learn 提供了 `DictVectorizer` 和 `FeatureHasher` 类；它们都生成可以输入到任何机器学习模型中的稀疏矩阵。后者具有有限的内存消耗，并采用
    **MurmurHash 3**（阅读[https://en.wikipedia.org/wiki/MurmurHash](https://en.wikipedia.org/wiki/MurmurHash)，获取更多信息）。以下是对这两种方法的代码示例：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In both cases, I suggest you read the original scikit-learn documentation to
    know all possible options and parameters.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我建议您阅读原始 scikit-learn 文档，以了解所有可能的选项和参数。
- en: 'When working with categorical features (normally converted into positive integers
    through `LabelEncoder`), it''s also possible to filter the dataset in order to
    apply one-hot encoding using the `OneHotEncoder` class. In the following example,
    the first feature is a binary index which indicates `''Male''` or `''Female''`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理分类特征（通常通过 `LabelEncoder` 转换为正整数）时，也可以使用 `OneHotEncoder` 类来过滤数据集，以便应用独热编码。在以下示例中，第一个特征是一个二进制索引，表示
    `'Male'` 或 `'Female'`：
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Considering that these approaches increase the number of values (also exponentially
    with binary versions), all the classes adopt sparse matrices based on SciPy implementation.
    See [https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html) 
    for further information.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些方法会增加值的数量（也随着二进制版本呈指数增长），所有类都采用基于 SciPy 实现的稀疏矩阵。有关更多信息，请参阅[https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html)。
- en: Managing missing features
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理缺失特征
- en: 'Sometimes a dataset can contain missing features, so there are a few options
    that can be taken into account:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有时数据集可能包含缺失特征，因此可以考虑以下几种选择：
- en: Removing the whole line
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除整行
- en: Creating sub-model to predict those features
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建子模型来预测这些特征
- en: Using an automatic strategy to input them according to the other known values
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动策略根据其他已知值输入它们
- en: The first option is the most drastic one and should be considered only when
    the dataset is quite large, the number of missing features is high, and any prediction
    could be risky. The second option is much more difficult because it's necessary
    to determine a supervised strategy to train a model for each feature and, finally,
    to predict their value. Considering all pros and cons, the third option is likely
    to be the best choice. scikit-learn offers the class `Imputer`, which is responsible
    for filling the holes using a strategy based on the mean (default choice), median,
    or frequency (the most frequent entry will be used for all the missing ones).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种选择是最激进的，只有在数据集相当大、缺失特征数量高，任何预测都可能存在风险时才应考虑。第二种选择要困难得多，因为需要确定一个监督策略来为每个特征训练一个模型，最后预测它们的值。综合考虑所有优缺点，第三种选择可能是最佳选择。scikit-learn
    提供了 `Imputer` 类，该类负责使用基于均值（默认选择）、中位数或频率（将使用最频繁的条目来填充所有缺失值）的策略来填充空缺。
- en: 'The following snippet shows an example using the three approaches (the default
    value for a missing feature entry is `NaN`. However, it''s possible to use a different
    placeholder through the parameter `missing_values`):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了使用三种方法（缺失特征条目的默认值是 `NaN`。然而，可以通过 `missing_values` 参数使用不同的占位符）的示例：
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Data scaling and normalization
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据缩放和归一化
- en: 'A generic dataset (we assume here that it is always numerical) is made up of
    different values which can be drawn from different distributions, having different
    scales and, sometimes, there are also outliers. A machine learning algorithm isn''t
    naturally able to distinguish among these various situations, and therefore, it''s
    always preferable to standardize datasets before processing them. A very common
    problem derives from having a non-zero mean and a variance greater than one. In
    the following figure, there''s a comparison between a raw dataset and the same
    dataset scaled and centered:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的数据集（我们假设它总是数值的）由不同的值组成，这些值可以来自不同的分布，具有不同的尺度，有时也存在异常值。机器学习算法本身无法区分这些不同的情况，因此，在处理之前始终最好对数据集进行标准化。一个非常常见的问题来自于非零均值和大于一的方差。在以下图中，比较了原始数据集和相同数据集缩放和居中的情况：
- en: '![](img/f03ba67d-26af-48a0-bc77-ea25d79ced2e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f03ba67d-26af-48a0-bc77-ea25d79ced2e.png)'
- en: 'This result can be achieved using the `StandardScaler` class:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`StandardScaler`类来实现这一结果：
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'It''s possible to specify if the scaling process must include both mean and
    standard deviation using the parameters `with_mean=True/False` and `with_std=True/False`
    (by default they''re both active). If you need a more powerful scaling feature,
    with a superior control on outliers and the possibility to select a quantile range,
    there''s also the class `RobustScaler`. Here are some examples with different
    quantiles:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过参数`with_mean=True/False`和`with_std=True/False`（默认情况下两者都激活）来指定缩放过程是否必须包括均值和标准差。如果您需要一个更强大的缩放功能，具有对异常值的高级控制以及选择分位数范围的可能性，还有`RobustScaler`类。以下是一些不同分位数的示例：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The results are shown in the following figures:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在下述图中：
- en: '![](img/a924212e-b15b-43dc-a763-148643c16c73.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a924212e-b15b-43dc-a763-148643c16c73.png)'
- en: Other options include `MinMaxScaler` and `MaxAbsScaler`, which scale data by
    removing elements that don't belong to a given range (the former) or by considering
    a maximum absolute value (the latter).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其他选项包括`MinMaxScaler`和`MaxAbsScaler`，它们通过移除不属于给定范围（前者）或考虑最大绝对值（后者）来缩放数据。
- en: 'scikit-learn also provides a class for per-sample normalization, `Normalizer.`
    It can apply `max`, `l1` and `l2` norms to each element of a dataset. In a Euclidean
    space, they are defined in the following way:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn还提供了一个用于样本归一化的类，`Normalizer`。它可以对数据集的每个元素应用`max`、`l1`和`l2`范数。在欧几里得空间中，它们被定义为以下方式：
- en: '![](img/da7716fe-b6c8-454f-8ed5-68db71930fee.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da7716fe-b6c8-454f-8ed5-68db71930fee.png)'
- en: 'An example of every normalization is shown next:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下文展示了每种归一化的示例：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Feature selection and filtering
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择和过滤
- en: 'An unnormalized dataset with many features contains information proportional
    to the independence of all features and their variance. Let''s consider a small
    dataset with three features, generated with random Gaussian distributions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有许多特征的非归一化数据集包含的信息与所有特征及其方差的相关性成比例。让我们考虑一个具有三个特征的小数据集，这些特征是通过随机高斯分布生成的：
- en: '![](img/cc419853-87e5-48aa-951e-75685ec5bc2b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc419853-87e5-48aa-951e-75685ec5bc2b.png)'
- en: 'Even without further analysis, it''s obvious that the central line (with the
    lowest variance) is almost constant and doesn''t provide any useful information. If
    you remember the previous chapter, the entropy H(X) is quite small, while the
    other two variables carry more information. A variance threshold is, therefore,
    a useful approach to remove all those elements whose contribution (in terms of
    variability and so, information) is under a predefined level. scikit-learn provides
    the class `VarianceThreshold` that can easily solve this problem. By applying
    it on the previous dataset, we get the following result:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有进一步的分析，很明显，中心线（具有最低方差）几乎是恒定的，并且不提供任何有用的信息。如果您还记得上一章，熵H(X)相当小，而其他两个变量携带更多的信息。因此，方差阈值是一个有用的方法来移除所有那些贡献（在变异性和信息方面）低于预定义水平的元素。scikit-learn提供了`VarianceThreshold`类，可以轻松解决这个问题。通过将其应用于前一个数据集，我们得到以下结果：
- en: '[PRE15]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The third feature has been completely removed because its variance is under
    the selected threshold (1.5 in this case).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个特征已被完全移除，因为它的方差低于所选阈值（在本例中为1.5）。
- en: There are also many univariate methods that can be used in order to select the
    best features according to specific criteria based on F-tests and p-values, such
    as chi-square or ANOVA. However, their discussion is beyond the scope of this
    book and the reader can find further information in Freedman D., Pisani R., Purves
    R., *Statistics*, Norton & Company.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多单变量方法可以根据基于F检验和p值的特定标准来选择最佳特征，例如卡方检验或方差分析。然而，它们的讨论超出了本书的范围，读者可以在Freedman
    D.、Pisani R.、Purves R.的《统计学》一书中找到更多信息。
- en: 'Two examples of feature selection that use the classes `SelectKBest` (which
    selects the best *K* high-score features) and `SelectPercentile` (which selects
    only a subset of features belonging to a certain percentile) are shown next. It''s
    possible to apply them both to regression and classification datasets, being careful
    to select appropriate score functions:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下文展示了两个特征选择的例子，分别使用了`SelectKBest`类（它选择最佳*K*高分数特征）和`SelectPercentile`类（它仅选择属于特定百分比的子集特征）。它们都可以应用于回归和分类数据集，但需要注意选择合适的评分函数：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: For further details about all scikit-learn score functions and their usage,
    visit [http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 关于所有 scikit-learn 分数函数及其使用的更多详细信息，请访问 [http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)。
- en: Principal component analysis
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: 'In many cases, the dimensionality of the input dataset *X* is high and so is
    the complexity of every related machine learning algorithm. Moreover, the information
    is seldom spread uniformly across all the features and, as discussed in the previous
    chapter, there will be high entropy features together with low entropy ones, which,
    of course, don''t contribute dramatically to the final outcome. In general, if
    we consider a Euclidean space, we have:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，输入数据集 *X* 的维度很高，与之相关的每个机器学习算法的复杂性也高。此外，信息很少均匀地分布在所有特征上，正如前一章所讨论的，将会有高熵特征和低熵特征，当然，它们不会对最终结果产生显著贡献。一般来说，如果我们考虑欧几里得空间，我们有：
- en: '![](img/ecde11b5-c38a-4021-ae35-78ef52874626.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ecde11b5-c38a-4021-ae35-78ef52874626.png)'
- en: 'So each point is expressed using an orthonormal basis made of *m* linearly
    independent vectors. Now, considering a dataset *X*, a natural question arises:
    is it possible to reduce *m* without a drastic loss of information? Let''s consider
    the following figure (without any particular interpretation):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个点都是用由 *m* 个线性无关向量组成的正交基来表示的。现在，考虑一个数据集 *X*，一个自然的问题出现了：在不造成信息量急剧损失的情况下，是否可以减少
    *m*？让我们考虑以下图（没有任何特定的解释）：
- en: '![](img/fdb11a61-2f30-4d27-887b-2fa888fda1e4.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fdb11a61-2f30-4d27-887b-2fa888fda1e4.png)'
- en: It doesn't matter which distributions generated *X=(x,y)*, however, the variance
    of the horizontal component is clearly higher than the vertical one. As discussed,
    it means that the amount of information provided by the first component is higher
    and, for example, if the *x* axis is stretched horizontally keeping the vertical
    one fixed, the distribution becomes similar to a segment where the depth has lower
    and lower importance.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种分布生成 *X=(x,y)*，然而，水平分量的方差明显大于垂直分量。正如所讨论的，这意味着第一个组件提供的信息量更高，例如，如果 *x* 轴在保持垂直轴不变的情况下水平拉伸，分布就变成了一个深度越来越不重要的段。
- en: 'In order to assess how much information is brought by each component, and the
    correlation among them, a useful tool is the covariance matrix (if the dataset
    has zero mean, we can use the correlation matrix):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估每个组件带来的信息量以及它们之间的相关性，一个有用的工具是协方差矩阵（如果数据集的均值为零，我们可以使用相关矩阵）：
- en: '![](img/a6ab6c65-7f4f-4798-88b5-bf0cf16ac842.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a6ab6c65-7f4f-4798-88b5-bf0cf16ac842.png)'
- en: '*C* is symmetric and positive semidefinite, so all the eigenvalues are non-negative,
    but what''s the meaning of each value? The covariance matrix for the previous
    example is:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*C* 是对称和正半定的，所以所有特征值都是非负的，但每个值的含义是什么？前一个示例的协方差矩阵是：'
- en: '![](img/6f4f0527-9563-462c-8671-a4f8a9c8c7ac.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6f4f0527-9563-462c-8671-a4f8a9c8c7ac.png)'
- en: As expected, the horizontal variance is quite a bit higher than the vertical
    one. Moreover, the other values are close to zero. If you remember the definition
    and, for simplicity, remove the mean term, they represent the cross-correlation
    between couples of components. It's obvious that in our example, *X* and *Y* are
    uncorrelated (they're orthogonal), but in real-life examples, there could be features
    which present a residual cross-correlation. In terms of information theory, it
    means that knowing *Y* gives us some information about *X* (which we already know),
    so they share information which is indeed doubled. So our goal is also to decorrelate
    *X* while trying to reduce its dimensionality.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，水平方差明显高于垂直方差。此外，其他值都接近于零。如果你记得定义，并且为了简单起见，移除均值项，它们代表了一对组件之间的互相关。很明显，在我们的例子中，*X*
    和 *Y* 是不相关的（它们是正交的），但在现实生活中的例子中，可能会有一些特征表现出残留的互相关性。从信息论的角度来看，这意味着知道 *Y* 给我们提供了一些关于
    *X*（我们已知）的信息，因此它们共享的信息实际上是翻倍了。所以我们的目标也是在尝试降低其维度的同时去相关 *X*。
- en: 'This can be achieved considering the sorted eigenvalues of *C* and selecting *g
    < m* values:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过考虑 *C* 的排序特征值并选择 *g < m* 值来实现：
- en: '![](img/6ac2fc1e-606d-4aa8-8d71-69865eec2f78.png)![](img/c9b9db73-0c6c-4e46-99fd-b40580ae4835.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6ac2fc1e-606d-4aa8-8d71-69865eec2f78.png)![图片](img/c9b9db73-0c6c-4e46-99fd-b40580ae4835.png)'
- en: 'So, it''s possible to project the original feature vectors into this new (sub-)space,
    where each component carries a portion of total variance and where the new covariance
    matrix is decorrelated to reduce useless information sharing (in terms of correlation)
    among different features. In scikit-learn, there''s the `PCA` class which can
    do all this in a very smooth way:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以将原始特征向量投影到这个新的（子）空间中，其中每个成分携带总方差的一部分，并且新的协方差矩阵被去相关以减少不同特征之间无用的信息共享（就相关性而言）。在scikit-learn中，有一个`PCA`类可以非常顺畅地完成所有这些操作：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'A figure with a few random MNIST handwritten digits is shown as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了几个随机的MNIST手写数字：
- en: '![](img/6015040f-b0ca-4736-aede-87f9a9b6b00f.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图像](img/6015040f-b0ca-4736-aede-87f9a9b6b00f.png)'
- en: 'Each image is a vector of 64 unsigned int (8 bit) numbers (0, 255), so the
    initial number of components is indeed 64\. However, the total amount of black
    pixels is often predominant and the basic signs needed to write 10 digits are
    similar, so it''s reasonable to assume both high cross-correlation and a low variance
    on several components. Trying with 36 principal components, we get:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像都是一个64位无符号整数（8位）数字的向量（0, 255），因此初始的成分数确实是64。然而，黑色像素的总数通常占主导地位，而书写10个数字所需的基本符号是相似的，因此有理由假设在几个成分上存在高交叉相关和低方差。尝试使用36个主成分，我们得到：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In order to improve performance, all integer values are normalized into the
    range [0, 1] and, through the parameter `whiten=True`, the variance of each component
    is scaled to one. As also the official scikit-learn documentation says, this process
    is particularly useful when an isotropic distribution is needed for many algorithms
    to perform efficiently. It''s possible to access the explained variance ratio
    through the instance variable `explained_variance_ratio_`***, ***which shows which
    part of the total variance is carried by each single component:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能，所有整数值都被归一化到[0, 1]的范围内，并且通过参数`whiten=True`，每个成分的方差被缩放到1。正如官方scikit-learn文档所说，这个过程在需要各向同性分布以使许多算法高效运行时特别有用。可以通过实例变量`explained_variance_ratio_`访问解释方差比***，***它显示了每个单独成分携带的总方差的部分：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'A plot for the example of MNIST digits is shown next. The left graph represents
    the variance ratio while the right one is the cumulative variance. It can be immediately
    seen how the first components are normally the most important ones in terms of
    information, while the following ones provide details that a classifier could
    also discard:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了MNIST数字示例的图表。左图表示方差比，右图表示累积方差。可以立即看出，第一成分通常是最重要的信息成分，而后续的成分则提供了分类器可能丢弃的细节：
- en: '![](img/97361a4e-2c85-49b2-870c-09ee33bbae45.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图像](img/97361a4e-2c85-49b2-870c-09ee33bbae45.png)'
- en: 'As expected, the contribution to the total variance decreases dramatically
    starting from the fifth component, so it''s possible to reduce the original dimensionality
    without an unacceptable loss of information, which could drive an algorithm to
    learn wrong classes. In the preceding graph, there are the same handwritten digits
    rebuilt using the first 36 components with whitening and normalization between
    0 and 1\. To obtain the original images, we need to inverse-transform all new
    vectors and project them into the original space:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，从第五个成分开始，对总方差的贡献急剧减少，因此可以在不造成不可接受的信息损失的情况下降低原始维度，这可能导致算法学习错误的类别。在前面的图表中，有使用前36个成分重建的手写数字，这些成分在0到1之间进行了白化和归一化。为了获得原始图像，我们需要对所有新向量进行逆变换，并将它们投影到原始空间中：
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The result is shown in the following figure:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '![](img/5fbbc7f9-9929-471d-b02e-276aab96b146.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图像](img/5fbbc7f9-9929-471d-b02e-276aab96b146.png)'
- en: This process can also partially denoise the original images by removing residual
    variance, which is often associated with noise or unwanted contributions (almost
    every calligraphy distorts some of the structural elements which are used for
    recognition).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程还可以通过去除残留方差来部分去噪原始图像，这种方差通常与噪声或不需要的贡献相关（几乎每种书法都会扭曲一些用于识别的结构元素）。
- en: 'I suggest the reader try different numbers of components (using the explained
    variance data) and also `n_components=''mle''`, which implements an automatic
    selection of the best dimensionality (Minka T.P, *Automatic Choice of Dimensionality
    for PCA*, NIPS 2000: 598-604).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '我建议读者尝试不同的成分数量（使用解释方差数据），以及`n_components=''mle''`，它实现了最佳维度的自动选择（Minka T.P,
    *自动选择PCA的维度*, NIPS 2000: 598-604）。'
- en: scikit-learn solves the PCA problem with **SVD** (**Singular Value Decomposition**),
    which can be studied in detail in Poole D., *Linear Algebra*, Brooks Cole. It's
    possible to control the algorithm through the parameter `svd_solver`, whose values
    are `'auto', 'full', 'arpack', 'randomized'`. Arpack implements a truncated SVD.
    Randomized is based on an approximate algorithm which drops many singular vectors
    and can achieve very good performances also with high-dimensional datasets where
    the actual number of components is sensibly smaller.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn使用**奇异值分解**（**SVD**）解决PCA问题，这可以在Poole D.的《线性代数》，Brooks Cole中详细研究。可以通过参数`svd_solver`控制算法，其值有`'auto',
    'full', 'arpack', 'randomized'`。Arpack实现了截断SVD。随机化基于一个近似算法，它丢弃了许多奇异向量，并且在高维数据集（实际组件数量明显较小）中也能实现非常好的性能。
- en: Non-negative matrix factorization
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非负矩阵分解
- en: 'When the dataset is made up of non-negative elements, it''s possible to use **non-negative
    matrix factorization** (**NNMF**) instead of standard PCA. The algorithm optimizes
    a loss function (alternatively on *W* and *H*) based on the Frobenius norm:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集由非负元素组成时，可以使用**非负矩阵分解**（**NNMF**）而不是标准的主成分分析（PCA）。该算法基于Frobenius范数优化一个损失函数（在*W*和*H*上交替进行）：
- en: '![](img/3b3a598d-3702-4f47-b40f-d74ad844945f.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b3a598d-3702-4f47-b40f-d74ad844945f.png)'
- en: If *dim(X) = n x m*, then *dim(W) = n x p* and *dim(H) = p x m* with *p* equal
    to the number of requested components (the `n_components` parameter), which is
    normally smaller than the original dimensions *n* and *m*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*dim(X) = n x m*，则*dim(W) = n x p*和*dim(H) = p x m*，其中*p*等于请求的组件数量（`n_components`参数），这通常小于原始维度*n*和*m*。
- en: 'The final reconstruction is purely additive and it has been shown that it''s
    particularly efficient for images or text where there are normally no non-negative
    elements. In the following snippet, there''s an example using the Iris dataset
    (which is non-negative). The `init` parameter can assume different values (see
    the documentation) which determine how the data matrix is initially processed.
    A random choice is for non-negative matrices which are only scaled (no SVD is
    performed):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的重构完全是加性的，并且已经证明它对于通常没有非负元素的图像或文本特别有效。在下面的代码片段中，有一个使用Iris数据集（它是非负的）的示例。`init`参数可以假设不同的值（请参阅文档），这些值决定了数据矩阵的初始处理方式。对于非负矩阵，随机选择仅进行缩放（不执行奇异值分解）：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: NNMF, together with other factorization methods, will be very useful for more
    advanced techniques, such as recommendation systems and topic modeling.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: NNMF与其他分解方法一起，对于更高级的技术，如推荐系统和主题建模，将非常有用。
- en: NNMF is very sensitive to its parameters (in particular, initialization and
    regularization), so I suggest reading the original documentation for further information: [http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html.](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: NNMF对其参数（特别是初始化和正则化）非常敏感，因此建议阅读原始文档以获取更多信息：[http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html.](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)
- en: Sparse PCA
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏PCA
- en: 'scikit-learn provides different PCA variants that can solve particular problems.
    I do suggest reading the original documentation. However, I''d like to mention
    `SparsePCA`, which allows exploiting the natural sparsity of data while extracting
    principal components. If you think about the handwritten digits or other images
    that must be classified, their initial dimensionality can be quite high (a 10x10
    image has 100 features). However, applying a standard PCA selects only the average
    most important features, assuming that every sample can be rebuilt using the same
    components. Simplifying, this is equivalent to:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn提供了不同的PCA变体，可以解决特定问题。我确实建议阅读原始文档。然而，我想提到`SparsePCA`，它允许在提取主成分的同时利用数据的自然稀疏性。如果你考虑手写数字或其他必须被分类的图像，它们的初始维度可以相当高（一个10x10的图像有100个特征）。然而，应用标准PCA只选择平均最重要的特征，假设每个样本可以使用相同的组件重建。简化来说，这相当于：
- en: '![](img/c71979ac-0808-4743-9c14-aa7e08345cf1.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c71979ac-0808-4743-9c14-aa7e08345cf1.png)'
- en: 'On the other hand, we can always use a limited number of components, but without
    the limitation given by a dense projection matrix. This can be achieved by using
    sparse matrices (or vectors), where the number of non-zero elements is quite low.
    In this way, each element can be rebuilt using its specific components (in most
    cases, they will be always the most important), which can include elements normally
    discarded by a dense PCA. The previous expression now becomes:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们总是可以使用有限数量的组件，但不需要由密集投影矩阵给出的限制。这可以通过使用稀疏矩阵（或向量）来实现，其中非零元素的数量相当低。这样，每个元素都可以使用其特定的组件重建（在大多数情况下，它们将是始终最重要的），这可以包括通常由密集PCA丢弃的元素。前面的表达式现在变为：
- en: '![](img/06c91861-72a2-4819-81ee-7fb087b0ba77.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/06c91861-72a2-4819-81ee-7fb087b0ba77.png)'
- en: Here the non-null components have been put into the first block (they don't
    have the same order as the previous expression), while all the other zero terms
    have been separated. In terms of linear algebra, the vectorial space now has the
    original dimensions. However, using the power of sparse matrices (provided by
    `scipy.sparse`), scikit-learn can solve this problem much more efficiently than
    a classical PCA.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，非零分量已经被放入了第一个块（它们的顺序与之前的表达式不同），而所有其他的零项已经被分开。从线性代数的角度来看，现在的向量空间具有原始的维度。然而，利用稀疏矩阵的强大功能（由`scipy.sparse`提供），scikit-learn可以比传统的PCA更高效地解决这个问题。
- en: 'The following snippet shows a sparse PCA with 60 components. In this context,
    they''re usually called atoms and the amount of sparsity can be controlled via
    *L1*-norm regularization (higher `alpha` parameter values lead to more sparse
    results). This approach is very common in classification algorithms and will be
    discussed in the next chapters:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段显示了一个具有60个组件的稀疏PCA。在这种情况下，它们通常被称为原子，稀疏度可以通过*L1*范数正则化来控制（更高的`alpha`参数值导致更稀疏的结果）。这种方法在分类算法中非常常见，将在下一章中讨论：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: For further information about SciPy sparse matrices, visit [https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如需有关SciPy稀疏矩阵的更多信息，请访问[https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html)。
- en: Kernel PCA
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核PCA
- en: We're going to discuss kernel methods in [Chapter 7](82926f62-2446-4d69-b575-b6f614bb5b0d.xhtml),
    *Support Vector Machines*, however, it's useful to mention the class `KernelPCA`,
    which performs a PCA with non-linearly separable data sets. Just to understand
    the logic of this approach (the mathematical formulation isn't very simple), it's
    useful to consider a projection of each sample into a particular space where the
    dataset becomes linearly separable. The components of this space correspond to
    the first, second, ... principal components and a kernel PCA algorithm, therefore,
    computes the projection of our samples onto each of them.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第7章](82926f62-2446-4d69-b575-b6f614bb5b0d.xhtml)“支持向量机”中讨论核方法，然而，提及`KernelPCA`类是有用的，它对非线性可分的数据集执行PCA。为了理解这种方法（数学公式并不简单），考虑将每个样本投影到一个特定的空间是有用的，在这个空间中，数据集变得线性可分。这个空间的部分对应于第一个、第二个、...主成分，因此，核PCA算法计算了我们的样本到每个部分的投影。
- en: 'Let''s consider a dataset made up of a circle with a blob inside:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个由一个圆和一个内部的blob组成的集合：
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The graphical representation is shown in the following picture. In this case,
    a classic PCA approach isn''t able to capture the non-linear dependency of existing
    components (the reader can verify that the projection is equivalent to the original
    dataset). However, looking at the samples and using polar coordinates (therefore,
    a space where it''s possible to project all the points), it''s easy to separate
    the two sets, only considering the radius:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图形表示如下。在这种情况下，经典的PCA方法无法捕捉现有组件的非线性依赖关系（读者可以验证投影与原始数据集等价）。然而，通过观察样本并使用极坐标（因此，一个可以投影所有点的空间），很容易将两个集合分开，只需考虑半径：
- en: '![](img/27ca52b7-07f6-499b-9a5d-7ee31b4c3eca.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/27ca52b7-07f6-499b-9a5d-7ee31b4c3eca.png)'
- en: 'Considering the structure of the dataset, it''s possible to investigate the
    behavior of a PCA with a radial basis function kernel. As the default value for
    `gamma` is 1.0/number of features (for now, consider this parameter as inversely
    proportional to the variance of a Gaussian), we need to increase it to capture
    the external circle. A value of 1.0 is enough:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到数据集的结构，我们可以研究具有径向基函数核的主成分分析（PCA）的行为。由于 `gamma` 的默认值是特征数量的 1.0/（目前，将此参数视为与高斯方差成反比），我们需要将其增加以捕捉外部圆环。1.0
    的值就足够了：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The instance variable `X_transformed_fit_` will contain the projection of our
    dataset into the new space. Plotting it, we get:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 实例变量 `X_transformed_fit_` 将包含我们的数据集在新空间中的投影。绘制它，我们得到：
- en: '![](img/eac317ef-e108-4f14-9964-378f07f6278b.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eac317ef-e108-4f14-9964-378f07f6278b.png)'
- en: The plot shows a separation just like expected, and it's also possible to see
    that the points belonging to the central blob have a curve distribution because
    they are more sensitive to the distance from the center.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了一个预期的分离，同时也可以看到属于中心块点的数据点具有曲线分布，因为它们对中心距离更敏感。
- en: Kernel PCA is a powerful instrument when we think of our dataset as made up
    of elements that can be a function of components (in particular, radial-basis
    or polynomials) but we aren't able to determine a linear relationship among them.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将数据集视为由可以构成成分（特别是径向基或多项式）的元素组成时，核主成分分析（Kernel PCA）是一种强大的工具，但我们无法确定它们之间的线性关系。
- en: For more information about the different kernels supported by scikit-learn,
    visit [http://scikit-learn.org/stable/modules/metrics.html#linear-kernel](http://scikit-learn.org/stable/modules/metrics.html#linear-kernel).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于 scikit-learn 支持的不同核的信息，请访问 [http://scikit-learn.org/stable/modules/metrics.html#linear-kernel](http://scikit-learn.org/stable/modules/metrics.html#linear-kernel)。
- en: Atom extraction and dictionary learning
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原子提取和字典学习
- en: 'Dictionary learning is a technique which allows rebuilding a sample starting
    from a sparse dictionary of atoms (similar to principal components). In Mairal
    J., Bach F., Ponce J., Sapiro G., *Online Dictionary Learning for Sparse Coding*,
    Proceedings of the 29th International Conference on Machine Learning, 2009 there''s
    a description of the same online strategy adopted by scikit-learn, which can be
    summarized as a double optimization problem where:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 字典学习是一种技术，它允许从稀疏原子字典（类似于主成分）开始重建样本。在 Mairal J., Bach F., Ponce J., Sapiro G.
    的《在线字典学习用于稀疏编码》（2009年29届国际机器学习会议论文集）中，描述了 scikit-learn 采取的相同在线策略，可以概括为一个双优化问题，其中：
- en: '![](img/02c0fcbb-b224-4945-8488-0831319c3ac8.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02c0fcbb-b224-4945-8488-0831319c3ac8.png)'
- en: 'Is an input dataset and the target is to find both a dictionary *D* and a set
    of weights for each sample:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据集和目标是找到字典 *D* 以及每个样本的一组权重：
- en: '![](img/dec8e494-e572-4cf2-88b1-7ca9aceafa56.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dec8e494-e572-4cf2-88b1-7ca9aceafa56.png)'
- en: 'After the training process, an input vector can be computed as:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程之后，可以计算输入向量如下：
- en: '![](img/2b5629e1-50c6-46fa-98fe-b2f15b75e006.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b5629e1-50c6-46fa-98fe-b2f15b75e006.png)'
- en: 'The optimization problem (which involves both *D* and alpha vectors) can be
    expressed as the minimization of the following loss function:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 优化问题（涉及 *D* 和 alpha 向量）可以表示为以下损失函数的最小化：
- en: '![](img/f0856f12-d499-4825-9430-8477d3b5feb9.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0856f12-d499-4825-9430-8477d3b5feb9.png)'
- en: Here the parameter *c* controls the level of sparsity (which is proportional
    to the strength of *L1* normalization). This problem can be solved by alternating
    the least square variable until a stable point is reached.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，参数 *c* 控制稀疏度水平（与 *L1* 归一化的强度成正比）。这个问题可以通过交替最小二乘变量直到达到稳定点来解决。
- en: 'In scikit-learn, we can implement such an algorithm with the class `DictionaryLearning`(using
    the usual MNIST datasets), where `n_components`, as usual, determines the number
    of atoms:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，我们可以使用 `DictionaryLearning` 类（使用常用的 MNIST 数据集）实现这样的算法，其中 `n_components`
    如常，决定了原子的数量：
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'A plot of each atom (component) is shown in the following figure:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 每个原子（成分）的图表如下所示：
- en: '![](img/af17080b-d4f5-4d0f-a7d3-f9cd63335a63.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af17080b-d4f5-4d0f-a7d3-f9cd63335a63.png)'
- en: This process can be very long on low-end machines. In such a case, I suggest
    limiting the number of samples to 20 or 30.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在低端机器上，这个过程可能非常耗时。在这种情况下，我建议将样本数量限制在 20 或 30 个。
- en: References
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Freedman D., Pisani R., Purves R., *Statistics,* Norton & Company
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freedman D., Pisani R., Purves R., *统计学，* 诺顿出版社
- en: 'Gareth J., Witten D., Hastie T., Tibshirani R., *An Introduction to Statistical
    Learning: With Application in R*, Springer'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gareth J., Witten D., Hastie T., Tibshirani R., *统计学习引论：R语言应用*, Springer
- en: Poole D., *Linear Algebra*, Brooks Cole
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poole D., *线性代数*, Brooks Cole
- en: 'Minka T.P, *Automatic Choice of Dimensionality for PCA*, NIPS 2000: 598-604'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Minka T.P, *PCA的自动维度选择*, NIPS 2000: 598-604'
- en: Mairal J., Bach F., Ponce J., Sapiro G., *Online Dictionary Learning for Sparse
    Coding*, Proceedings of the 29th International Conference on Machine Learning,
    2009
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mairal J., Bach F., Ponce J., Sapiro G., *在线字典学习用于稀疏编码*, 第29届国际机器学习会议论文集，2009年
- en: Summary
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Feature selection is the first (and sometimes the most important) step in a
    machine learning pipeline. Not all the features are useful for our purposes and
    some of them are expressed using different notations, so it's often necessary
    to preprocess our dataset before any further operations.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择是机器学习流程中的第一步（有时也是最重要的一步）。并非所有特征对我们都有用，有些特征使用不同的符号表示，因此通常在执行任何进一步操作之前，需要对我们的数据集进行预处理。
- en: We saw how to split the data into training and test sets using a random shuffle
    and how to manage missing elements. Another very important section covered the
    techniques used to manage categorical data or labels, which are very common when
    a certain feature assumes only a discrete set of values.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了如何使用随机洗牌将数据分为训练集和测试集，以及如何管理缺失元素。另一个非常重要的部分涵盖了用于管理分类数据或标签的技术，这在某些特征只假设离散值集合时非常常见。
- en: Then we analyzed the problem of dimensionality. Some datasets contain many features
    which are correlated with each other, so they don't provide any new information
    but increase the computational complexity and reduce the overall performances.
    Principal component analysis is a method to select only a subset of features which
    contain the largest amount of total variance. This approach, together with its
    variants, allows to decorrelate the features and reduce the dimensionality without
    a drastic loss in terms of accuracy. Dictionary learning is another technique
    used to extract a limited number of building blocks from a dataset, together with
    the information needed to rebuild each sample. This approach is particularly useful
    when the dataset is made up of different versions of similar elements (such as
    images, letters, or digits).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们分析了维度问题。一些数据集包含许多相互关联的特征，它们不提供任何新信息，但增加了计算复杂度并降低了整体性能。主成分分析是一种选择只包含最大总方差子集特征的方法。这种方法及其变体允许解耦特征并降低维度，而不会在准确性方面造成剧烈损失。字典学习是另一种从数据集中提取有限数量的构建块的技术，同时提取重建每个样本所需的信息。当数据集由相似元素的多个版本组成（如图像、字母或数字）时，这种方法特别有用。
- en: In the next chapter, we're going to discuss linear regression, which is the
    most diffused and simplest supervised approach to predict continuous values. We'll
    also analyze how to overcome some limitations and how to solve non-linear problems
    using the same algorithms.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论线性回归，这是预测连续值的最常见和最简单的监督方法。我们还将分析如何克服一些局限性，以及如何使用相同的算法解决非线性问题。
