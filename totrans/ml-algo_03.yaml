- en: Feature Selection and Feature Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature engineering is the first step in a machine learning pipeline and involves
    all the techniques adopted to clean existing datasets, increase their signal-noise
    ratio, and reduce their dimensionality. Most algorithms have strong assumptions
    about the input data, and their performances can be negatively affected when raw
    datasets are used. Moreover, the data is seldom isotropic; there are often features
    that determine the general behavior of a sample, while others that are correlated
    don't provide any additional pieces of information. So, it's important to have
    a clear view of a dataset and know the most common algorithms used to reduce the
    number of features or select only the best ones.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn toy datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'scikit-learn provides some built-in datasets that can be used for testing purposes.
    They''re all available in the package `sklearn.datasets` and have a common structure:
    the data instance variable contains the whole input set `X` while target contains
    the labels for classification or target values for regression. For example, considering
    the Boston house pricing dataset (used for regression), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we have 506 samples with 13 features and a single target value.
    In this book, we''re going to use it for regressions and the MNIST handwritten
    digit dataset (`load_digits()`) for classification tasks. scikit-learn also provides
    functions for creating dummy datasets from scratch: `make_classification()`, `make_regression()`,
    and `make_blobs()` (particularly useful for testing cluster algorithms). They''re
    very easy to use and in many cases, it''s the best choice to test a model without
    loading more complex datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Visit [http://scikit-learn.org/stable/datasets/](http://scikit-learn.org/stable/datasets/)
    for further information.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST dataset provided by scikit-learn is limited for obvious reasons. If
    you want to experiment with the original one, refer to the website managed by
    Y. LeCun, C. Cortes, C. Burges: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
    Here you can download a full version made up of 70,000 handwritten digits already
    split into training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Creating training and test sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When a dataset is large enough, it''s a good practice to split it into training
    and test sets; the former to be used for training the model and the latter to
    test its performances. In the following figure, there''s a schematic representation
    of this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4face3c7-67a7-4b70-b26d-ed1f256b89b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two main rules in performing such an operation:'
  prefs: []
  type: TYPE_NORMAL
- en: Both datasets must reflect the original distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original dataset must be randomly shuffled before the split phase in order
    to avoid a correlation between consequent elements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With scikit-learn, this can be achieved using the `train_test_split()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameter `test_size` (as well as `training_size`) allows specifying the
    percentage of elements to put into the test/training set. In this case, the ratio
    is 75 percent for training and 25 percent for the test phase. Another important
    parameter is `random_state` which can accept a NumPy `RandomState` generator or
    an integer seed. In many cases, it''s important to provide reproducibility for
    the experiments, so it''s also necessary to avoid using different seeds and, consequently,
    different random splits:'
  prefs: []
  type: TYPE_NORMAL
- en: My suggestion is to always use the same number (it can also be 0 or completely
    omitted), or define a global `RandomState` which can be passed to all requiring
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this way, if the seed is kept equal, all experiments have to lead to the
    same results and can be easily reproduced in different environments by other scientists.
  prefs: []
  type: TYPE_NORMAL
- en: For further information about NumPy random number generation, visit [https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.RandomState.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.RandomState.html).
  prefs: []
  type: TYPE_NORMAL
- en: Managing categorical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In many classification problems, the target dataset is made up of categorical
    labels which cannot immediately be processed by any algorithm. An encoding is
    needed and scikit-learn offers at least two valid options. Let''s consider a very
    small dataset made of 10 categorical samples with two features each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The first option is to use the `LabelEncoder` class, which adopts a dictionary-oriented
    approach, associating to each category label a progressive integer number, that
    is an index of an instance array called `classes_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The inverse transformation can be obtained in this simple way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This approach is simple and works well in many cases, but it has a drawback:
    all labels are turned into sequential numbers. A classifier which works with real
    values will then consider similar numbers according to their distance, without
    any concern for semantics. For this reason, it''s often preferable to use so-called
    **one-hot** **encod****ing**, which binarizes the data. For labels, it can be
    achieved using the `LabelBinarizer` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, each categorical label is first turned into a positive integer
    and then transformed into a vector where only one feature is 1 while all the others
    are 0\. It means, for example, that using a softmax distribution with a peak corresponding
    to the main class can be easily turned into a discrete vector where the only non-null
    element corresponds to the right class. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Another approach to categorical features can be adopted when they''re structured
    like a list of dictionaries (not necessarily dense, they can have values only
    for a few features). For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, scikit-learn offers the classes `DictVectorizer` and `FeatureHasher`;
    they both produce sparse matrices of real numbers that can be fed into any machine
    learning model. The latter has a limited memory consumption and adopts **MurmurHash
    3** (read [https://en.wikipedia.org/wiki/MurmurHash](https://en.wikipedia.org/wiki/MurmurHash),
    for further information). The code for these two methods is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In both cases, I suggest you read the original scikit-learn documentation to
    know all possible options and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with categorical features (normally converted into positive integers
    through `LabelEncoder`), it''s also possible to filter the dataset in order to
    apply one-hot encoding using the `OneHotEncoder` class. In the following example,
    the first feature is a binary index which indicates `''Male''` or `''Female''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Considering that these approaches increase the number of values (also exponentially
    with binary versions), all the classes adopt sparse matrices based on SciPy implementation.
    See [https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html) 
    for further information.
  prefs: []
  type: TYPE_NORMAL
- en: Managing missing features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes a dataset can contain missing features, so there are a few options
    that can be taken into account:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing the whole line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating sub-model to predict those features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using an automatic strategy to input them according to the other known values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first option is the most drastic one and should be considered only when
    the dataset is quite large, the number of missing features is high, and any prediction
    could be risky. The second option is much more difficult because it's necessary
    to determine a supervised strategy to train a model for each feature and, finally,
    to predict their value. Considering all pros and cons, the third option is likely
    to be the best choice. scikit-learn offers the class `Imputer`, which is responsible
    for filling the holes using a strategy based on the mean (default choice), median,
    or frequency (the most frequent entry will be used for all the missing ones).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet shows an example using the three approaches (the default
    value for a missing feature entry is `NaN`. However, it''s possible to use a different
    placeholder through the parameter `missing_values`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Data scaling and normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A generic dataset (we assume here that it is always numerical) is made up of
    different values which can be drawn from different distributions, having different
    scales and, sometimes, there are also outliers. A machine learning algorithm isn''t
    naturally able to distinguish among these various situations, and therefore, it''s
    always preferable to standardize datasets before processing them. A very common
    problem derives from having a non-zero mean and a variance greater than one. In
    the following figure, there''s a comparison between a raw dataset and the same
    dataset scaled and centered:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f03ba67d-26af-48a0-bc77-ea25d79ced2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This result can be achieved using the `StandardScaler` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s possible to specify if the scaling process must include both mean and
    standard deviation using the parameters `with_mean=True/False` and `with_std=True/False`
    (by default they''re both active). If you need a more powerful scaling feature,
    with a superior control on outliers and the possibility to select a quantile range,
    there''s also the class `RobustScaler`. Here are some examples with different
    quantiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown in the following figures:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a924212e-b15b-43dc-a763-148643c16c73.png)'
  prefs: []
  type: TYPE_IMG
- en: Other options include `MinMaxScaler` and `MaxAbsScaler`, which scale data by
    removing elements that don't belong to a given range (the former) or by considering
    a maximum absolute value (the latter).
  prefs: []
  type: TYPE_NORMAL
- en: 'scikit-learn also provides a class for per-sample normalization, `Normalizer.`
    It can apply `max`, `l1` and `l2` norms to each element of a dataset. In a Euclidean
    space, they are defined in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da7716fe-b6c8-454f-8ed5-68db71930fee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An example of every normalization is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Feature selection and filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An unnormalized dataset with many features contains information proportional
    to the independence of all features and their variance. Let''s consider a small
    dataset with three features, generated with random Gaussian distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc419853-87e5-48aa-951e-75685ec5bc2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Even without further analysis, it''s obvious that the central line (with the
    lowest variance) is almost constant and doesn''t provide any useful information. If
    you remember the previous chapter, the entropy H(X) is quite small, while the
    other two variables carry more information. A variance threshold is, therefore,
    a useful approach to remove all those elements whose contribution (in terms of
    variability and so, information) is under a predefined level. scikit-learn provides
    the class `VarianceThreshold` that can easily solve this problem. By applying
    it on the previous dataset, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The third feature has been completely removed because its variance is under
    the selected threshold (1.5 in this case).
  prefs: []
  type: TYPE_NORMAL
- en: There are also many univariate methods that can be used in order to select the
    best features according to specific criteria based on F-tests and p-values, such
    as chi-square or ANOVA. However, their discussion is beyond the scope of this
    book and the reader can find further information in Freedman D., Pisani R., Purves
    R., *Statistics*, Norton & Company.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two examples of feature selection that use the classes `SelectKBest` (which
    selects the best *K* high-score features) and `SelectPercentile` (which selects
    only a subset of features belonging to a certain percentile) are shown next. It''s
    possible to apply them both to regression and classification datasets, being careful
    to select appropriate score functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: For further details about all scikit-learn score functions and their usage,
    visit [http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection).
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In many cases, the dimensionality of the input dataset *X* is high and so is
    the complexity of every related machine learning algorithm. Moreover, the information
    is seldom spread uniformly across all the features and, as discussed in the previous
    chapter, there will be high entropy features together with low entropy ones, which,
    of course, don''t contribute dramatically to the final outcome. In general, if
    we consider a Euclidean space, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecde11b5-c38a-4021-ae35-78ef52874626.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So each point is expressed using an orthonormal basis made of *m* linearly
    independent vectors. Now, considering a dataset *X*, a natural question arises:
    is it possible to reduce *m* without a drastic loss of information? Let''s consider
    the following figure (without any particular interpretation):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fdb11a61-2f30-4d27-887b-2fa888fda1e4.png)'
  prefs: []
  type: TYPE_IMG
- en: It doesn't matter which distributions generated *X=(x,y)*, however, the variance
    of the horizontal component is clearly higher than the vertical one. As discussed,
    it means that the amount of information provided by the first component is higher
    and, for example, if the *x* axis is stretched horizontally keeping the vertical
    one fixed, the distribution becomes similar to a segment where the depth has lower
    and lower importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to assess how much information is brought by each component, and the
    correlation among them, a useful tool is the covariance matrix (if the dataset
    has zero mean, we can use the correlation matrix):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6ab6c65-7f4f-4798-88b5-bf0cf16ac842.png)'
  prefs: []
  type: TYPE_IMG
- en: '*C* is symmetric and positive semidefinite, so all the eigenvalues are non-negative,
    but what''s the meaning of each value? The covariance matrix for the previous
    example is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f4f0527-9563-462c-8671-a4f8a9c8c7ac.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the horizontal variance is quite a bit higher than the vertical
    one. Moreover, the other values are close to zero. If you remember the definition
    and, for simplicity, remove the mean term, they represent the cross-correlation
    between couples of components. It's obvious that in our example, *X* and *Y* are
    uncorrelated (they're orthogonal), but in real-life examples, there could be features
    which present a residual cross-correlation. In terms of information theory, it
    means that knowing *Y* gives us some information about *X* (which we already know),
    so they share information which is indeed doubled. So our goal is also to decorrelate
    *X* while trying to reduce its dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be achieved considering the sorted eigenvalues of *C* and selecting *g
    < m* values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ac2fc1e-606d-4aa8-8d71-69865eec2f78.png)![](img/c9b9db73-0c6c-4e46-99fd-b40580ae4835.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, it''s possible to project the original feature vectors into this new (sub-)space,
    where each component carries a portion of total variance and where the new covariance
    matrix is decorrelated to reduce useless information sharing (in terms of correlation)
    among different features. In scikit-learn, there''s the `PCA` class which can
    do all this in a very smooth way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'A figure with a few random MNIST handwritten digits is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6015040f-b0ca-4736-aede-87f9a9b6b00f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each image is a vector of 64 unsigned int (8 bit) numbers (0, 255), so the
    initial number of components is indeed 64\. However, the total amount of black
    pixels is often predominant and the basic signs needed to write 10 digits are
    similar, so it''s reasonable to assume both high cross-correlation and a low variance
    on several components. Trying with 36 principal components, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to improve performance, all integer values are normalized into the
    range [0, 1] and, through the parameter `whiten=True`, the variance of each component
    is scaled to one. As also the official scikit-learn documentation says, this process
    is particularly useful when an isotropic distribution is needed for many algorithms
    to perform efficiently. It''s possible to access the explained variance ratio
    through the instance variable `explained_variance_ratio_`***, ***which shows which
    part of the total variance is carried by each single component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'A plot for the example of MNIST digits is shown next. The left graph represents
    the variance ratio while the right one is the cumulative variance. It can be immediately
    seen how the first components are normally the most important ones in terms of
    information, while the following ones provide details that a classifier could
    also discard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97361a4e-2c85-49b2-870c-09ee33bbae45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As expected, the contribution to the total variance decreases dramatically
    starting from the fifth component, so it''s possible to reduce the original dimensionality
    without an unacceptable loss of information, which could drive an algorithm to
    learn wrong classes. In the preceding graph, there are the same handwritten digits
    rebuilt using the first 36 components with whitening and normalization between
    0 and 1\. To obtain the original images, we need to inverse-transform all new
    vectors and project them into the original space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5fbbc7f9-9929-471d-b02e-276aab96b146.png)'
  prefs: []
  type: TYPE_IMG
- en: This process can also partially denoise the original images by removing residual
    variance, which is often associated with noise or unwanted contributions (almost
    every calligraphy distorts some of the structural elements which are used for
    recognition).
  prefs: []
  type: TYPE_NORMAL
- en: 'I suggest the reader try different numbers of components (using the explained
    variance data) and also `n_components=''mle''`, which implements an automatic
    selection of the best dimensionality (Minka T.P, *Automatic Choice of Dimensionality
    for PCA*, NIPS 2000: 598-604).'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn solves the PCA problem with **SVD** (**Singular Value Decomposition**),
    which can be studied in detail in Poole D., *Linear Algebra*, Brooks Cole. It's
    possible to control the algorithm through the parameter `svd_solver`, whose values
    are `'auto', 'full', 'arpack', 'randomized'`. Arpack implements a truncated SVD.
    Randomized is based on an approximate algorithm which drops many singular vectors
    and can achieve very good performances also with high-dimensional datasets where
    the actual number of components is sensibly smaller.
  prefs: []
  type: TYPE_NORMAL
- en: Non-negative matrix factorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When the dataset is made up of non-negative elements, it''s possible to use **non-negative
    matrix factorization** (**NNMF**) instead of standard PCA. The algorithm optimizes
    a loss function (alternatively on *W* and *H*) based on the Frobenius norm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b3a598d-3702-4f47-b40f-d74ad844945f.png)'
  prefs: []
  type: TYPE_IMG
- en: If *dim(X) = n x m*, then *dim(W) = n x p* and *dim(H) = p x m* with *p* equal
    to the number of requested components (the `n_components` parameter), which is
    normally smaller than the original dimensions *n* and *m*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final reconstruction is purely additive and it has been shown that it''s
    particularly efficient for images or text where there are normally no non-negative
    elements. In the following snippet, there''s an example using the Iris dataset
    (which is non-negative). The `init` parameter can assume different values (see
    the documentation) which determine how the data matrix is initially processed.
    A random choice is for non-negative matrices which are only scaled (no SVD is
    performed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: NNMF, together with other factorization methods, will be very useful for more
    advanced techniques, such as recommendation systems and topic modeling.
  prefs: []
  type: TYPE_NORMAL
- en: NNMF is very sensitive to its parameters (in particular, initialization and
    regularization), so I suggest reading the original documentation for further information: [http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html.](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)
  prefs: []
  type: TYPE_NORMAL
- en: Sparse PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'scikit-learn provides different PCA variants that can solve particular problems.
    I do suggest reading the original documentation. However, I''d like to mention
    `SparsePCA`, which allows exploiting the natural sparsity of data while extracting
    principal components. If you think about the handwritten digits or other images
    that must be classified, their initial dimensionality can be quite high (a 10x10
    image has 100 features). However, applying a standard PCA selects only the average
    most important features, assuming that every sample can be rebuilt using the same
    components. Simplifying, this is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c71979ac-0808-4743-9c14-aa7e08345cf1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the other hand, we can always use a limited number of components, but without
    the limitation given by a dense projection matrix. This can be achieved by using
    sparse matrices (or vectors), where the number of non-zero elements is quite low.
    In this way, each element can be rebuilt using its specific components (in most
    cases, they will be always the most important), which can include elements normally
    discarded by a dense PCA. The previous expression now becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06c91861-72a2-4819-81ee-7fb087b0ba77.png)'
  prefs: []
  type: TYPE_IMG
- en: Here the non-null components have been put into the first block (they don't
    have the same order as the previous expression), while all the other zero terms
    have been separated. In terms of linear algebra, the vectorial space now has the
    original dimensions. However, using the power of sparse matrices (provided by
    `scipy.sparse`), scikit-learn can solve this problem much more efficiently than
    a classical PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet shows a sparse PCA with 60 components. In this context,
    they''re usually called atoms and the amount of sparsity can be controlled via
    *L1*-norm regularization (higher `alpha` parameter values lead to more sparse
    results). This approach is very common in classification algorithms and will be
    discussed in the next chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: For further information about SciPy sparse matrices, visit [https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html).
  prefs: []
  type: TYPE_NORMAL
- en: Kernel PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're going to discuss kernel methods in [Chapter 7](82926f62-2446-4d69-b575-b6f614bb5b0d.xhtml),
    *Support Vector Machines*, however, it's useful to mention the class `KernelPCA`,
    which performs a PCA with non-linearly separable data sets. Just to understand
    the logic of this approach (the mathematical formulation isn't very simple), it's
    useful to consider a projection of each sample into a particular space where the
    dataset becomes linearly separable. The components of this space correspond to
    the first, second, ... principal components and a kernel PCA algorithm, therefore,
    computes the projection of our samples onto each of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a dataset made up of a circle with a blob inside:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The graphical representation is shown in the following picture. In this case,
    a classic PCA approach isn''t able to capture the non-linear dependency of existing
    components (the reader can verify that the projection is equivalent to the original
    dataset). However, looking at the samples and using polar coordinates (therefore,
    a space where it''s possible to project all the points), it''s easy to separate
    the two sets, only considering the radius:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27ca52b7-07f6-499b-9a5d-7ee31b4c3eca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering the structure of the dataset, it''s possible to investigate the
    behavior of a PCA with a radial basis function kernel. As the default value for
    `gamma` is 1.0/number of features (for now, consider this parameter as inversely
    proportional to the variance of a Gaussian), we need to increase it to capture
    the external circle. A value of 1.0 is enough:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The instance variable `X_transformed_fit_` will contain the projection of our
    dataset into the new space. Plotting it, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eac317ef-e108-4f14-9964-378f07f6278b.png)'
  prefs: []
  type: TYPE_IMG
- en: The plot shows a separation just like expected, and it's also possible to see
    that the points belonging to the central blob have a curve distribution because
    they are more sensitive to the distance from the center.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel PCA is a powerful instrument when we think of our dataset as made up
    of elements that can be a function of components (in particular, radial-basis
    or polynomials) but we aren't able to determine a linear relationship among them.
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the different kernels supported by scikit-learn,
    visit [http://scikit-learn.org/stable/modules/metrics.html#linear-kernel](http://scikit-learn.org/stable/modules/metrics.html#linear-kernel).
  prefs: []
  type: TYPE_NORMAL
- en: Atom extraction and dictionary learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dictionary learning is a technique which allows rebuilding a sample starting
    from a sparse dictionary of atoms (similar to principal components). In Mairal
    J., Bach F., Ponce J., Sapiro G., *Online Dictionary Learning for Sparse Coding*,
    Proceedings of the 29th International Conference on Machine Learning, 2009 there''s
    a description of the same online strategy adopted by scikit-learn, which can be
    summarized as a double optimization problem where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02c0fcbb-b224-4945-8488-0831319c3ac8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Is an input dataset and the target is to find both a dictionary *D* and a set
    of weights for each sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dec8e494-e572-4cf2-88b1-7ca9aceafa56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After the training process, an input vector can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b5629e1-50c6-46fa-98fe-b2f15b75e006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The optimization problem (which involves both *D* and alpha vectors) can be
    expressed as the minimization of the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0856f12-d499-4825-9430-8477d3b5feb9.png)'
  prefs: []
  type: TYPE_IMG
- en: Here the parameter *c* controls the level of sparsity (which is proportional
    to the strength of *L1* normalization). This problem can be solved by alternating
    the least square variable until a stable point is reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, we can implement such an algorithm with the class `DictionaryLearning`(using
    the usual MNIST datasets), where `n_components`, as usual, determines the number
    of atoms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'A plot of each atom (component) is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af17080b-d4f5-4d0f-a7d3-f9cd63335a63.png)'
  prefs: []
  type: TYPE_IMG
- en: This process can be very long on low-end machines. In such a case, I suggest
    limiting the number of samples to 20 or 30.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Freedman D., Pisani R., Purves R., *Statistics,* Norton & Company
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gareth J., Witten D., Hastie T., Tibshirani R., *An Introduction to Statistical
    Learning: With Application in R*, Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poole D., *Linear Algebra*, Brooks Cole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minka T.P, *Automatic Choice of Dimensionality for PCA*, NIPS 2000: 598-604'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mairal J., Bach F., Ponce J., Sapiro G., *Online Dictionary Learning for Sparse
    Coding*, Proceedings of the 29th International Conference on Machine Learning,
    2009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature selection is the first (and sometimes the most important) step in a
    machine learning pipeline. Not all the features are useful for our purposes and
    some of them are expressed using different notations, so it's often necessary
    to preprocess our dataset before any further operations.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to split the data into training and test sets using a random shuffle
    and how to manage missing elements. Another very important section covered the
    techniques used to manage categorical data or labels, which are very common when
    a certain feature assumes only a discrete set of values.
  prefs: []
  type: TYPE_NORMAL
- en: Then we analyzed the problem of dimensionality. Some datasets contain many features
    which are correlated with each other, so they don't provide any new information
    but increase the computational complexity and reduce the overall performances.
    Principal component analysis is a method to select only a subset of features which
    contain the largest amount of total variance. This approach, together with its
    variants, allows to decorrelate the features and reduce the dimensionality without
    a drastic loss in terms of accuracy. Dictionary learning is another technique
    used to extract a limited number of building blocks from a dataset, together with
    the information needed to rebuild each sample. This approach is particularly useful
    when the dataset is made up of different versions of similar elements (such as
    images, letters, or digits).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to discuss linear regression, which is the
    most diffused and simplest supervised approach to predict continuous values. We'll
    also analyze how to overcome some limitations and how to solve non-linear problems
    using the same algorithms.
  prefs: []
  type: TYPE_NORMAL
