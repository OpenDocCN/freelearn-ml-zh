<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer003">
<h1 id="_idParaDest-5"><a id="_idTextAnchor004"/>Preface</h1>
<p>The work that researchers do to prepare data for analysis – extraction, transformation, cleaning, and exploration – has not changed fundamentally with the increased popularity of machine learning tools. When we prepared data for multivariate analyses 30 years ago, we were every bit as concerned with missing values, outliers, the shape of the distribution of our variables, and how variables correlate, as we are when we use machine learning algorithms now. Although it is true that widespread use of the same libraries for machine learning (scikit-learn, TensorFlow, PyTorch, and others) does encourage greater uniformity in approach, good data cleaning and exploration practices are largely unchanged.</p>
<p>How we talk about machine learning is still very much algorithm-focused; just choose the right model and organization-changing insights will follow. But we have to make room for the same kind of learning from data that we have been engaged in over the last few decades, where the predictions we make from data, our modeling of relationships in the data, and our cleaning and exploration of that data are very much part of the conversation. Getting our models right has as much to do with gleaning as much information as we can from a histogram or a confusion matrix as from carefully tuning hyperparameters.</p>
<p>Similarly, the work that data analysts and scientists do does not progress neatly from cleaning, to exploration, to preprocessing, to modeling, to evaluation. We have potential models in mind at each step of the process, regularly updating our previous models. For example, we may initially think that we will be using logistic regression to model a particular binary target but then recognize when we see the distribution of features that we might need to at least try using random forest classification. We will discuss implications for modeling throughout this text, even when explaining relatively routine data cleaning tasks. We will also explore the use of machine learning tools early in the process to help us identify anomalies, impute values, and select features.</p>
<p>This points to another change in the workflow of data analysts and scientists over the last decade – less emphasis on <em class="italic">the one model</em> and greater acceptance of model building as an iterative process. A project might require multiple machine learning algorithms – for example, principal component analysis to reduce dimensions (the number of features) and then logistic regression for classification.</p>
<p>That being said, there is one key difference in our approach to data cleaning, exploration, and modeling as machine learning tools guide more of our work – an increased emphasis on prediction over an understanding of the underlying data. We are more concerned with how well our features (also known as independent variables, inputs, or predictors) predict our targets (dependent variables, outputs, responses) than with the relationships between features and the underlying structure of our data. I point out throughout the first two sections of this book how that alters our focus somewhat, even when we are cleaning and exploring our data.</p>
<h1 id="_idParaDest-6"><a id="_idTextAnchor005"/>Who this book is for</h1>
<p>I had multiple audiences in mind as I wrote this book, but I most consistently thought about a dear friend of mine who bought a Transact-SQL book 30 years ago and instantly developed great confidence in her database work, ultimately building a career around those skills. I would love it if someone just starting their career as a data scientist or analyst worked through this book and had a similar experience as my friend. More than anything else, I want you to feel good and excited about what you can do as a result of reading this book.</p>
<p>I also hope this book will be a useful reference for folks who have been doing this kind of work for a while. Here, I imagine someone opening the book and wondering to themselves, <em class="italic">what are good values to use in my grid search for my logistic regression model?</em></p>
<p>In keeping with the hands-on nature of this text, every bit of output is reproducible with code in this book. I also stuck to a rule throughout, even when it was challenging. Every section, except for the conceptual sections, starts with raw data largely unchanged from the original downloaded file. You go from data file to model in each section. If you have forgotten how a particular object was created, all you will ever need to do is turn back a page or two to see.</p>
<p>Readers who have some knowledge of pandas and NumPy will have an easier time with some code blocks, as will folks with some knowledge of Python and scikit-learn. None of that is essential though. There are just some sections you might want to pause over longer. If you need additional instruction on doing data work with Python, my <em class="italic">Python Data Cleaning Cookbook</em> is a good companion book I think.</p>
<h1 id="_idParaDest-7"><a id="_idTextAnchor006"/>What this book covers</h1>
<p><a href="B17978_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Examining the Distribution of Features and Targets</em>, explores using common NumPy and pandas techniques to get a better sense of the attributes of our data. We will generate summary statistics, such as <strong class="source-inline">mean</strong>, <strong class="source-inline">min</strong>, and <strong class="source-inline">max</strong>, and standard deviation, and count the number of missings. We will also create visualizations of key features, including histograms and boxplots, to give us a better sense of the distribution of each feature than we can get by just looking at summary statistics. We will hint at the implications of feature distribution for data transformation, encoding and scaling, and the modeling that we will be doing in subsequent chapters with the same data.</p>
<p><a href="B17978_02_ePub.xhtml#_idTextAnchor025"><em class="italic">Chapter 2</em></a>, <em class="italic">Examining Bivariate and Multivariate Relationships between Features and Targets</em>, focuses on the correlation between possible features and target variables. We will use pandas methods for bivariate analysis, and Matplotlib for visualizations. We will discuss the implications of what we find for feature engineering and modeling. We also use multivariate techniques in this chapter to understand the relationship between features.</p>
<p><a href="B17978_03_ePub.xhtml#_idTextAnchor034"><em class="italic">Chapter 3</em></a>, <em class="italic">Identifying and Fixing Missing Values</em>, goes over techniques for identifying missing values for each feature or target, and for identifying observations where values for a large number of the features are absent. We will explore strategies for imputing values, such as setting values to the overall mean, to the mean for a given category, or forward filling. We will also examine multivariate techniques for imputing values for missings and discuss when they are appropriate.</p>
<p><a href="B17978_04_ePub.xhtml#_idTextAnchor043"><em class="italic">Chapter 4</em></a>, <em class="italic">Encoding, Transforming, and Scaling Features</em>, covers a range of feature engineering techniques. We will use tools to drop redundant or highly correlated features. We will explore the most common kinds of encoding – one-hot, ordinal, and hashing encoding. We will also use transformations to improve the distribution of our features. Finally, we will use common binning and scaling approaches to address skew, kurtosis, and outliers, and to adjust for features with widely different ranges.</p>
<p><a href="B17978_05_ePub.xhtml#_idTextAnchor058"><em class="italic">Chapter 5</em></a>, <em class="italic">Feature Selection</em> will go over a number of feature selection methods, from filter, to wrapper, to embedded methods. We will explore how they work with categorical and continuous targets. For wrapper and embedded methods, we consider how well they work with different algorithms.</p>
<p><a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for Model Evaluation</em>, will see us build our first full-fledged pipeline, separating our data into testing and training datasets, and learning how to do preprocessing without data leakage. We will implement cross-validation with k-fold and look more closely into assessing the performance of our models.</p>
<p><a href="B17978_07_ePub.xhtml#_idTextAnchor091"><em class="italic">Chapter 7</em></a>, <em class="italic">Linear Regression Models</em>, is the first of several chapters on building regression models with an old favorite of many data scientists, linear regression. We will run a classical linear model while also examining the qualities of a feature space that make it a good candidate for a linear model. We will explore how to improve linear models, when necessary, with regularization and transformations. We will look into stochastic gradient descent as an alternative to <strong class="bold">ordinary least square</strong> (<strong class="bold">OLS</strong>) optimization. We will also learn how to do hyperparameter tuning with grid searches.</p>
<p><a href="B17978_08_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 8</em></a>, <em class="italic">Support Vector Regression</em>, discusses key support vector machine concepts and how they can be applied to regression problems. In particular, we will examine how concepts such as epsilon-insensitive tubes and soft margins can give us the flexibility to get the best fit possible, given our data and domain-related challenges. We will also explore, for the first time but definitely not the last, the very handy kernel trick, which allows us to model nonlinear relationships without transformations or increasing the number of features.</p>
<p><a href="B17978_09_ePub.xhtml#_idTextAnchor113"><em class="italic">Chapter 9</em></a>, <em class="italic">K-Nearest Neighbors, Decision Tree, Random Forest, and Gradient Boosted Regression</em>, explores some of the most popular non-parametric regression algorithms. We will discuss the advantages of each algorithm, when you might want to choose one over the other, and possible modeling challenges. These challenges include how to avoid underfitting and overfitting with careful adjusting of hyperparameters.</p>
<p><a href="B17978_10_ePub.xhtml#_idTextAnchor126"><em class="italic">Chapter 10</em></a>, <em class="italic">Logistic Regression</em>, is the first of several chapters on building classification models with logistic regression, an efficient algorithm with low bias. We will carefully examine the assumptions of logistic regression and discuss the attributes of a dataset and a modeling problem that make logistic regression a good choice. We will use regularization to address high variance or when we have a number of highly correlated predictors. We will extend the algorithm to multiclass problems with multinomial logistic regression. We will also discuss how to handle class imbalance for the first, but not the last, time.</p>
<p><a href="B17978_11_ePub.xhtml#_idTextAnchor135"><em class="italic">Chapter 11</em></a>, <em class="italic">Decision Trees and Random Forest Classification</em>, returns to the decision tree and random forest algorithms that were introduced in <a href="B17978_09_ePub.xhtml#_idTextAnchor113"><em class="italic">Chapter 9</em></a>, <em class="italic">K-Nearest Neighbors, Decision Tree, Random Forest, and Gradient Boosted Regression</em>, this time dealing with classification problems. This gives us another opportunity to learn how to construct and interpret decision trees. We will adjust key hyperparameters, including the depth of trees, to avoid overfitting. We will then explore random forest and gradient boosted decision trees as good, lower variance alternatives to decision trees.</p>
<p><a href="B17978_12_ePub.xhtml#_idTextAnchor144"><em class="italic">Chapter 12</em></a>, <em class="italic">K-Nearest Neighbors for Classification</em>, returns to <strong class="bold">k-nearest neighbors</strong> (<strong class="bold">KNNs</strong>) to handle both binary and multiclass modeling problems. We will discuss and demonstrate the advantages of KNN – how easy it is to build a no-frills model and the limited number of hyperparameters to adjust. By the end of the chapter, we will know both – how to do KNN and when we should consider it for our modeling.</p>
<p><a href="B17978_13_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 13</em></a>, <em class="italic">Support Vector Machine Classification</em>, explores different strategies for implementing <strong class="bold">support vector classification</strong> (<strong class="bold">SVC</strong>). We will use linear SVC, which can perform very well when our classes are linearly separable. We will then examine how to use the kernel trick to extend SVC to cases where the classes are not linearly separable. Finally, we will use one-versus-one and one-versus-rest classification to handle targets with more than two values.</p>
<p><a href="B17978_14_ePub.xhtml#_idTextAnchor162"><em class="italic">Chapter 14</em></a>, <em class="italic">Naïve Bayes Classification</em>, discusses the fundamental assumptions of naïve Bayes in this chapter and how the algorithm is used to tackle some of the modeling challenges we have already explored, as well as some new ones, such as text classification. We will consider when naïve Bayes is a good option and when it is not. We will also examine the interpretation of naïve Bayes models.</p>
<p><a href="B17978_15_ePub.xhtml#_idTextAnchor170"><em class="italic">Chapter 15</em></a>, <em class="italic">Principal Component Analysis</em>, examines <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>), including how it works and when we might want to use it. We will learn how to interpret the components created from PCA, including how each feature contributes to each component and how much of the variance is explained. We will learn how to visualize components and how to use components in subsequent analyses. We will also examine how to use kernels for PCA and when that might give us better results.</p>
<p><a href="B17978_16_ePub.xhtml#_idTextAnchor177"><em class="italic">Chapter 16</em></a>, <em class="italic">K-Means and DBSCAN Clustering</em>, explores two popular clustering techniques, k-means and <strong class="bold">Density-based spatial clustering of applications with noise</strong> (<strong class="bold">DBSCAN</strong>). We will discuss the strengths of each approach and develop a sense of when to choose one clustering algorithm over the other. We will also learn how to evaluate our clusters and how to change hyperparameters to improve our model.</p>
<h1 id="_idParaDest-8"><a id="_idTextAnchor007"/>To get the most out of this book</h1>
<p>To run the code in this book, you will need to have installed a scientific distribution of Python, such as Anaconda. All code was tested with scikit-learn versions 0.24.2 and 1.0.2.</p>
<h1 id="_idParaDest-9"><a id="_idTextAnchor008"/>Download the example code files</h1>
<p>You can download the example code files for this book from GitHub at <a href="https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning">https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning</a>. If there’s an update to the code, it will be updated in the GitHub repository.</p>
<p>We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a>. Check them out!</p>
<h1 id="_idParaDest-10"><a id="_idTextAnchor009"/>Download the color images</h1>
<p>We also provide a PDF file that has color images of the screenshots and diagrams used in this book. You can download it here: <a href="https://packt.link/aLE6J">https://packt.link/aLE6J</a>.</p>
<h1 id="_idParaDest-11"><a id="_idTextAnchor010"/>Conventions used</h1>
<p>There are a number of text conventions used throughout this book.</p>
<p><strong class="source-inline">Code in text</strong>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: “For learning purposes, we have provided two example <strong class="source-inline">mlruns</strong> artifacts and the <strong class="source-inline">huggingface</strong> cache folder in the GitHub repository under the <strong class="source-inline">chapter08</strong> folder.”</p>
<p>A block of code is set as follows:</p>
<pre class="source-code">client = boto3.client('sagemaker-runtime') </pre>
<pre class="source-code">response = client.invoke_endpoint(</pre>
<pre class="source-code">        EndpointName=app_name, </pre>
<pre class="source-code">        ContentType=content_type,</pre>
<pre class="source-code">        Accept=accept,</pre>
<pre class="source-code">        Body=payload</pre>
<pre class="source-code">        )</pre>
<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
<pre class="source-code">loaded_model = <strong class="bold">mlflow.pyfunc.spark_udf</strong>(</pre>
<pre class="source-code">    spark,</pre>
<pre class="source-code">    model_uri=logged_model, </pre>
<pre class="source-code">    result_type=StringType())</pre>
<p>Any command-line input or output is written as follows:</p>
<p class="source-code">mlflow models serve -m models:/inference_pipeline_model/6</p>
<p><strong class="bold">Bold</strong>: Indicates a new term, an important word, or words that you see onscreen. For instance, words in menus or dialog boxes appear in <strong class="bold">bold</strong>. Here is an example: “To execute the code in this cell, you can just click on <strong class="bold">Run Cell</strong> in the top-right drop-down menu.”</p>
<p class="callout-heading">Tips or Important Notes	</p>
<p class="callout">Appear like this.</p>
<h1 id="_idParaDest-12"><a id="_idTextAnchor011"/>Get in touch</h1>
<p>Feedback from our readers is always welcome.</p>
<p><strong class="bold">General feedback</strong>: If you have questions about any aspect of this book, email us at <strong class="source-inline">customercare@packtpub.com</strong> and mention the book title in the subject of your message.</p>
<p><strong class="bold">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packtpub.com/support/errata">www.packtpub.com/support/errata</a> and fill in the form.</p>
<p><strong class="bold">Piracy</strong>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <strong class="source-inline">copyright@packt.com</strong> with a link to the material.</p>
<p><strong class="bold">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com">authors.packtpub.com</a>.</p>
<h1 id="_idParaDest-13"><a id="_idTextAnchor012"/>Share Your Thoughts</h1>
<p>Once you’ve read <em class="italic">Data Cleaning and Exploration with Machine Learning</em>, we’d love to hear your thoughts! Please <a href="">click here to go straight to the Amazon review page</a> for this book and share your feedback.</p>
<p>Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.</p>
</div>
</div>
</body></html>