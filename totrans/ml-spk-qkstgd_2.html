<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Setting Up a Local Development Environment</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will install, configure, and deploy a local analytical development environment by provisioning a self-contained single-node cluster that will allow us to do the following:</p>
<ul>
<li>Prototype and develop machine learning models and pipelines in Python</li>
<li>Demonstrate the functionality and usage of Apache Spark's machine learning library, <kbd>MLlib</kbd>, via the Spark Python API (PySpark)</li>
<li>Develop and test machine learning models on a single-node cluster using small sample datasets, and thereafter scale up to multi-node clusters processing much larger datasets with little or no code changes required</li>
</ul>
<p class="mce-root">Our single-node cluster will host the following technologies:</p>
<ul>
<li><strong>Operating system</strong>: CentOS Linux 7<br/>
<a href="https://www.centos.org/download/">https://www.centos.org/download/</a></li>
<li><strong>General Purpose Programming Languages</strong>:<br/>
<ul>
<li>Java SE Development Kit (JDK) 8 (8u181)<br/>
<a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html">https://www.oracle.com/technetwork/java/javase/downloads/index.html</a></li>
<li>Scala 2.11.x (2.11.12)<a href="https://www.scala-lang.org/download/all.html"><br/>
https://www.scala-lang.org/download/all.html</a></li>
<li>Python 3.x (3.6) via Anaconda 5.x (5.2) Python Distribution<br/>
<a href="https://www.anaconda.com/download/">https://www.anaconda.com/download/</a></li>
</ul>
</li>
<li><strong>General purpose distributed processing engine</strong>: Apache Spark 2.3.x (2.3.2)<br/>
<a href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a></li>
<li><strong>Distributed streaming platform</strong>: Apache Kafka 2 (2.0.0)<br/>
<a href="https://kafka.apache.org/downloads">https://kafka.apache.org/downloads</a></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">CentOS Linux 7 virtual machine</h1>
                </header>
            
            <article>
                
<p>First of all, we will assume that you have access to a physical or virtual machine provisioned with the CentOS 7 operating system. CentOS 7 is a free Linux distribution derived from <strong>Red Hat Enterprise Linux</strong> (<strong>RHEL</strong>). It is commonly used, along with its licensed upstream parent, RHEL, as the operating system of choice for Linux-based servers, since it is stable and backed by a large active community with detailed documentation. All the commands that we will use to install the various technologies listed previously will be Linux shell commands to be executed on a single CentOS 7 (or RHEL) machine, whether physical or virtual. If you do not have access to a CentOS 7 machine, then there are quite a few options available to provision a CentOS 7 virtual machine:</p>
<ul>
<li>Cloud computing platforms such as <strong>Amazon Web Services</strong> (<strong>AWS</strong>), <strong>Microsoft Azure</strong>, and the <strong>Google Cloud Platform</strong> (<strong>GCP</strong>) all allow you to stand up virtual machines using a <strong>Pay-As-You-Go</strong> (<strong>PAYG</strong>) pricing model. Often, the major cloud computing platforms also provide a free tier for new users with a small amount of free capacity in order to trial their services. To learn more about these major cloud computing platforms, please visit the following websites:
<ul>
<li>AWS: <a href="https://aws.amazon.com/">https://aws.amazon.com/</a></li>
<li>Microsoft Azure: <a href="https://azure.microsoft.com">https://azure.microsoft.com</a></li>
<li>GCP: <a href="https://cloud.google.com/">https://cloud.google.com/</a></li>
</ul>
</li>
</ul>
<ul>
<li><strong>Virtual Private Server</strong> (<strong>VPS</strong>) hosting companies, such as <strong>Linode</strong> and <strong>Digital Ocean</strong>, also provide the ability to provision low-cost CentOS virtual machines. These VPS providers often employ a much simpler pricing model consisting only of virtual machines of various specifications. To learn more about these major VPS providers, please visit the following websites:
<ul>
<li>Linode: <a href="https://www.linode.com/">https://www.linode.com/</a></li>
<li>Digital Ocean: <a href="https://www.digitalocean.com/">https://www.digitalocean.com/<br/></a></li>
</ul>
</li>
<li>A common and free option, particularly for local development environments used for prototyping and testing, is to provision your own virtual machine hosted by your personal physical desktop or laptop. Virtualization software such as <strong>Oracle VirtualBox</strong> (open source) and <strong>VMWare Workstation Player</strong> (free for personal use) allow you to set up and run virtual machines on your own personal physical devices. To learn more about these virtualization software services, please visit the following websites:
<ul>
<li>Oracle VirtualBox: <a href="https://www.virtualbox.org/">https://www.virtualbox.org/</a></li>
<li>VMWare Workstation Player: <a href="https://www.vmware.com/uk/products/workstation-player.html">https://www.vmware.com/<br/></a></li>
</ul>
</li>
</ul>
<p>For the remainder of this chapter, we will assume that you have provisioned a 64-bit CentOS 7 machine and that you have either direct desktop access to it, or network access to it via both HTTP and SSH protocols. Though the specifications of your virtual machine may differ, we would recommend the following minimum virtual hardware requirements in order to efficiently run the examples in the remainder of this book:</p>
<ul>
<li>Operating System: CentOS 7 (minimum installation)</li>
<li>Virtual CPUs: 4</li>
<li>Memory: 8 GB</li>
<li>Storage: 20 GB</li>
</ul>
<p>In our case, our virtual machine has the following network properties and will be referenced hereafter as such. These will be different for your virtual machine:</p>
<ul>
<li>Static IP address: <kbd>192.168.56.10</kbd></li>
<li>Netmask: <kbd>255.255.255.0</kbd></li>
<li><strong>Fully qualified domain name</strong> (<strong>FQDN</strong>): <kbd>packt.dev.keisan.io</kbd></li>
</ul>
<div class="packt_infobox">Note that the security of your virtual machine, and the subsequent software services that it hosts, including big data technologies, is beyond the scope of this book. Should you wish to learn more about how to harden your base operating system and common software services in order to protect against external attacks, we recommend visiting <a href="https://www.cisecurity.org/cis-benchmarks/">https://www.cisecurity.org/cis-benchmarks/</a> as well as the individual software service websites themselves, such as <a href="https://spark.apache.org/docs/latest/security.html">https://spark.apache.org/docs/latest/security.html</a> for Apache Spark security.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Java SE Development Kit 8</h1>
                </header>
            
            <article>
                
<p>Java is a general purpose programming language often used for <strong>object-oriented programming</strong> (<strong>OOP</strong>). Many of the distributed technologies that we discussed in <a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem</em>, are originally written in Java. As such, th<strong>e Java Development Kit</strong> (<strong>JDK</strong>) is required to be installed in our local development environment in order to run those software services within <strong>Java Virtual Machines</strong> (<strong>JVM</strong>). Apache Spark 2.3.x requires Java 8+ in order to run. To install Oracle Java SE Development Kit 8, please execute the following shell commands as the Linux <em>root</em> user or another user with elevated privileges:</p>
<pre><strong>&gt; rpm -ivh jdk-8u181-linux-x64.rpm</strong><br/><strong>&gt; vi /etc/profile.d/java.sh</strong><br/> <br/>      $ export PATH=/usr/java/default/bin:$PATH<br/>      $ export JAVA_HOME=/usr/java/default<br/><br/><strong>&gt; source /etc/profile.d/java.sh</strong><br/><strong>&gt; echo $PATH</strong><br/><strong>&gt; echo $JAVA_HOME</strong></pre>
<p><span>These commands will install JDK 8 and, thereafter, add the location of the Java binaries to the global <kbd>PATH</kbd> variable, allowing any local Linux user to run Java-based programs. To check that Java 8 has been installed successfully, the following command should return the version of Java installed, demonstrated as follows:</span></p>
<pre><strong>&gt; java –version</strong><br/><strong>      $ java version "1.8.0_181"</strong><br/><strong>      $ Java(TM) SE Runtime Environment (build 1.8.0_181-b13)</strong><br/><strong>      $ Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode)</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scala 2.11</h1>
                </header>
            
            <article>
                
<p>Scala is a general purpose programming language used for both object-oriented programming and functional programming. Apache Spark is, in fact, written in the Scala programming language. However, as described in <a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem</em>, Spark applications can be written in a variety of languages, including Java, Scala, Python, and R. Though the pros and cons of Scala versus Python is beyond the scope of this book, Scala is generally faster than Python within the context of data analysis and naturally more tightly integrated with Spark. Python, however, currently offers a more comprehensive library of advanced third-party data science tools and frameworks and is arguably easier to learn and use. The code examples provided for this book have been written in Python 3. However, this sub-section describes the steps required in order to install Scala should you wish to develop Scala-based applications.</p>
<p>Referring to Scala specifically, Apache Spark 2.3.2 requires Scala 2.11.x in order to run Scala-based Spark applications. In order to install Scala 2.11.12, please execute the following shell commands as the Linux root user or another user with elevated privileges:</p>
<pre><strong>&gt; <span>rpm -ivh scala-2.11.12.rpm</span></strong></pre>
<p><span>These commands will install Scala 2.11.12 and place its binaries in a globally accessible location, allowing any local Linux user to run Scala applications, whether Spark-based or not. To check that Scala 2.11.12 has been installed successfully, the following command should return the version of Scala installed:</span></p>
<pre><strong>&gt; scala –version</strong><br/>     <strong> $ Scala code runner version 2.11.12</strong></pre>
<p><span>You can also access the Scala shell and execute interactive Scala commands as follows:</span></p>
<pre><strong>&gt; scala</strong><br/><strong>&gt;&gt; 1+2</strong><br/><strong>      $ res0: Int = 3</strong><br/><strong>&gt;&gt; :q</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Anaconda 5 with Python 3</h1>
                </header>
            
            <article>
                
<p>Anaconda is a distribution of the Python general purpose programming language. Not only does it contain the Python interpreter, but it also comes bundled with a wide range of commonly used Python data science packages out of the box and a Python package management system called <strong>conda</strong>, making it quick and easy to provision a Python-based data science platform. In fact, we will be taking advantage of some of the pre-bundled Python packages in later chapters.</p>
<p>Anaconda 5.2 comes bundled with Python 3.6. Apache Spark 2.3.x supports both branches of Python, namely Python 2 and Python 3. Specifically, it supports Python 2.7+ and Python 3.4+. As described earlier, the code examples provided for this book have been written in Python 3.</p>
<p>In order to install Anaconda 5.2, please execute the following shell commands. You may or may not choose to execute these shell commands as the Linux root user. If you do not, Anaconda will be installed for the local Linux user running these commands and does not require administrator privileges:</p>
<pre><strong>&gt; bash Anaconda3-5.2.0-Linux-x86_64.sh</strong><br/><strong>&gt; Do you accept the license terms? [yes|no]</strong><br/><strong>&gt;&gt;&gt; yes</strong><br/><strong>&gt; Do you wish the installer to prepend the Anaconda3 install location to PATH in your .bashrc ? [yes|no]</strong><br/><strong>&gt;&gt;&gt; yes<span><br/> The last command will add the location of the Anaconda, and hence Python, binaries to your local PATH variable, allowing your local Linux user to run both Python-based programs (overriding any existing Python interpreters already installed on the operating system) and conda commands. Note that you may need to open a new Linux shell in order for the local PATH updates to take effect.</span></strong></pre>
<p>To check that Anaconda 5.2 and, hence, Python 3.6 have been installed successfully, the following command should return the version of conda installed:</p>
<pre><strong>&gt; conda --version</strong><br/><strong>      $ conda 4.5.4</strong></pre>
<p><span>You can also access the Python shell and execute interactive Python commands as follows:</span></p>
<pre><strong>&gt; python</strong><br/><strong>      $ Python 3.6.5 | Anaconda, Inc.</strong><br/><strong>&gt;&gt;&gt; import sys</strong><br/><strong>&gt;&gt;&gt; sys.path</strong><br/><strong>&gt;&gt;&gt; quit()</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic conda commands</h1>
                </header>
            
            <article>
                
<p>In this sub-section, we will provide some basic conda commands for your reference. These commands assume that your virtual machine can access either the internet or a local Python repository.</p>
<p>In order to upgrade the version of conda and/or Anaconda as a whole, you can execute the following commands:</p>
<pre><strong>&gt; conda update conda</strong><br/><strong>&gt; conda update anaconda</strong></pre>
<p><span>To install or update individual Python packages, you can execute the following commands:</span></p>
<pre><strong>&gt; conda install &lt;name of Python package&gt;</strong><br/><strong>&gt; conda update &lt;name of Python package&gt;</strong></pre>
<p><span>Finally, in order to list the current Python packages and versions installed in your Anaconda distribution, you can execute the following command:</span></p>
<pre><strong>&gt; conda list</strong></pre>
<p><span>To learn more about the conda package management system, please visit</span> <a href="https://conda.io/docs/index.html">https://conda.io/docs/index.html</a><span>.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Additional Python packages</h1>
                </header>
            
            <article>
                
<p>The following Python packages, which are not already contained within the default Anaconda distribution, are required for our local development environment. Please execute the following shell commands to install these prerequisite Python packages:</p>
<pre><strong>&gt; conda install -c conda-forge findspark</strong><br/><strong>&gt; conda install -c conda-forge pykafka</strong><br/><strong>&gt; conda install -c conda-forge tweepy</strong><br/><strong>&gt; conda install -c conda-forge tensorflow</strong><br/><strong>&gt; conda install -c conda-forge keras</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Jupyter Notebook</h1>
                </header>
            
            <article>
                
<p>Jupyter Notebook is an open source, web-based application designed for <em>interactive</em> analytics that comes bundled with the Anaconda distribution. Since it is designed for interactive analytics, it is best suited for ad hoc queries, live simulations, prototyping, and a means to visualize your data and to look for any trends and patterns prior to developing production-ready data science models. <strong>Apache Zeppelin</strong> is another example of an open source, web-based notebook used for similar purposes. Notebooks such as Jupyter Notebook and Apache Zeppelin tend to support multiple kernels, meaning that you can use various general purpose programming languages including Python and Scala.</p>
<p>One of the core advantages of notebooks is that they persist both your input code and any output data structures and visualizations that your code generates, including graphs, charts, and tables. However, they are <em>not</em> fully-fledged <strong>integrated development environments</strong> (<strong>IDE</strong>). This means that, in general, they should not be used for the development of code intended for production-grade data engineering or analytical pipelines. This is because they are difficult (but not impossible) to manage in version control systems such as Git, as they persist both input code and intermediate output structures. As such, they are also difficult to build code artifacts from and to deploy automatically using typical DevOps pipelines. Therefore, notebooks remain ideally suited for interactive analytics, ad hoc queries, and for prototyping.</p>
<p>The majority of the code files provided for this book are, in fact, Jupyter Notebook files (<kbd>.ipynb</kbd>), using the Python 3 kernel, so that readers may see the output of our models immediately. Should you, in the future, wish to write data science code that will eventually be deployed to production-grade systems, we strongly recommend writing your code in a proper IDE such as the following:</p>
<ul>
<li>Eclipse: <a href="https://www.eclipse.org/ide/">https://www.eclipse.org/ide/</a></li>
<li>IntelliJ IDEA: <a href="https://www.jetbrains.com/idea/">https://www.jetbrains.com/idea/</a></li>
<li>PyCharm: <a href="https://www.jetbrains.com/pycharm/">https://www.jetbrains.com/pycharm/</a></li>
<li>Microsoft <strong>Visual Studio Code</strong> (<strong>VS Code</strong>): <a href="https://code.visualstudio.com/">https://code.visualstudio.com/</a></li>
</ul>
<p>As mentioned earlier, Jupyter Notebook is already bundled with the Anaconda distribution. However, a few configuration steps are recommended in order to access it. Please execute the following shell commands as your local Linux user in order to generate a per-user Jupyter Notebook configuration file that you can then edit based on per-user preferences:</p>
<pre><strong>&gt; jupyter notebook --generate-config</strong><br/><strong>      $ Writing default config to: /home/packt/.jupyter/jupyter_notebook_config.py</strong><br/><strong>&gt; vi /home/packt/.jupyter/jupyter_notebook_config.py</strong><br/><strong>      Line 174: c.NotebookApp.ip = '192.168.56.10'</strong><br/><strong>      Line 214: c.NotebookApp.notebook_dir = '/data/workspaces/packt/jupyter/notebooks/'</strong><br/><strong>      Line 240: c.NotebookApp.port = 8888</strong></pre>
<p><span>These commands will configure a per-user Jupyter Notebook instance to listen on a dedicated IP address (in our case, <kbd>192.168.56.10</kbd>) using a designated port (in our case <kbd>8888</kbd>), and to work from a pre-defined base directory in which to persist Jupyter Notebook code files (in our case</span> <kbd>/data/workspaces/packt/jupyter/notebooks</kbd><span>). Note that you should amend these properties based on your specific environment.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Starting Jupyter Notebook</h1>
                </header>
            
            <article>
                
<p>If you have desktop-based access to your CentOS virtual machine, the easiest way to instantiate a new per-user Jupyter Notebook instance is to execute the following shell command as your local Linux user:</p>
<pre><strong>&gt; jupyter notebook</strong></pre>
<p><span>However, should you only have SSH or command-line access with no GUI, then you should use the following command instead:</span></p>
<pre><strong>&gt; jupyter notebook --no-browser</strong></pre>
<p><span>The latter command will stop Jupyter from automatically opening a local browser session. In either case, the resultant logs will state the full URL (including the security token by default) that can be used to access your instance of Jupyter Notebook. The URL should look similar to the following:</span></p>
<p><kbd>http://192.168.56.10:8888/?token=6ebb5f6a321b478162802a97b8e463a1a053df12fcf9d99c</kbd></p>
<p>Please copy and paste this URL into an internet browser supported by Jupyter Notebook (Google Chrome, Mozilla Firefox, or Apple Safari). If successful, a screen similar to the screenshot illustrated in <em>Figure 2.1</em> should be returned:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-545 image-border" src="Images/883be21b-b3bb-4c6b-88d3-16e3768932ae.png" style="width:52.42em;height:14.42em;" width="763" height="210"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 2.1: Jupyter Notebook web session</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Troubleshooting Jupyter Notebook</h1>
                </header>
            
            <article>
                
<p>Since Jupyter Notebook is a web-based application, it is accessible via the HTTP protocol at the designated port number. If you are accessing the generated URL via a remote internet browser and it cannot connect, then please check your firewall settings (and SELinux in the case of CentOS and RHEL) on your virtual machine to ensure that access to the designated port number is provisioned from your location. For example, the following shell commands executed by the Linux <em>root</em> user, or another user with elevated privileges, will open port 8888 in the CentOS 7 firewall via its public zone:</p>
<pre><strong>&gt; firewall-cmd --get-active-zones</strong><br/><strong>&gt; firewall-cmd --zone=public --add-port=8888/tcp --permanent</strong><br/><strong>&gt; firewall-cmd --reload</strong><br/><strong>&gt; firewall-cmd --list-all</strong></pre>
<p><span>Please contact your system administrator or refer to your cloud platform documentation for further network-related information and troubleshooting.</span></p>
<p>To learn more about Jupyter Notebook, its configuration, and common troubleshooting, please visit <a href="https://jupyter-notebook.readthedocs.io/en/stable/index.html">https://jupyter-notebook.readthedocs.io/en/stable/index.html</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Apache Spark 2.3</h1>
                </header>
            
            <article>
                
<p>As described in <a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem</em>, Apache Spark is a general purpose distributed processing engine that is capable of performing data transformations, advanced analytics, machine learning, and graph analytics at scale over petabytes of data. Apache Spark can be deployed either in standalone mode (meaning that we utilize its in-built cluster manager) or integrated with other third-party cluster managers including Apache YARN and Apache Mesos.</p>
<p>In the case of our single-node development cluster, we will deploy Apache Spark in standalone mode where our single-node will host both the Apache Spark Standalone Master server and a single worker node instance. Since Spark software services are designed to run in a JVM, it is perfectly acceptable to co-locate both standalone master and worker processes on a single node, though in practice, in real-world implementations of Apache Spark, clusters can be much larger with multiple worker nodes provisioned. Our single-node Apache Spark cluster will still allow us to prototype and develop Spark applications and machine learning models that can still take advantage of the parallelism offered by a multi-core single machine, and thereafter are capable of being deployed to larger clusters and datasets with ease.</p>
<p>Also note that we will be installing Apache Spark 2.3.2 direct from its pre-built binaries available on the official Apache Spark website at <a href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a>. Nowadays, it is common for distributed technologies such as Spark, Kafka, and Hadoop components to be installed together at the same time via consolidated big data platforms such as those offered by <strong>Hortonworks Data Platform</strong> (<strong>HDP</strong>), <strong>Cloudera</strong>, and <strong>MapR</strong>. The benefits of using consolidated platforms such as these include the deployment of individual component versions that have been fully tested together and guaranteed to fully integrate with one another, as well as web-based installation, monitoring, administration, and support.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Spark binaries</h1>
                </header>
            
            <article>
                
<p>Please execute the following shell commands as a local Linux user to extract the Apache Spark binaries. In our case, we will be installing the Spark binaries into <kbd>/opt</kbd>:</p>
<pre><strong>&gt; tar -xzf spark-2.3.2-bin-hadoop2.7.tgz -C /opt</strong></pre>
<p><span>The resultant Spark parent directory will have the following structure:</span></p>
<ul>
<li><kbd>bin</kbd>: Shell scripts for local Spark services, such as <kbd>spark-submit</kbd></li>
<li><kbd>sbin</kbd>: Shell scripts, including starting and stopping Spark services</li>
<li><kbd>conf</kbd>: Spark configuration files</li>
<li><kbd>jars</kbd>: Spark library dependencies</li>
<li><kbd>python</kbd>: Spark's Python API, called PySpark</li>
<li><kbd>R</kbd>: Spark's R API, called SparkR</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Local working directories</h1>
                </header>
            
            <article>
                
<p>Each node in a Spark cluster (in our case, just the single node) will generate log files as well as local working files, such as when shuffling and serializing RDD data. The following commands will create defined local directories in which to store these local working outputs, the paths of which you can edit as per your preferences and which will be used in later configuration files:</p>
<pre><strong>&gt; mkdir -p /data/spark/local/data</strong><br/><strong>&gt; mkdir -p /data/spark/local/logs</strong><br/><strong>&gt; mkdir -p /data/spark/local/pid</strong><br/><strong>&gt; mkdir -p /data/spark/local/worker</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Spark configuration</h1>
                </header>
            
            <article>
                
<p class="mce-root">Configuration can be applied to Spark in the following ways:</p>
<ul>
<li><strong>Spark properties</strong> control application-level settings, including execution behavior, memory management, dynamic allocation, scheduling, and security, which can be defined in the following order of precedence:
<ul>
<li>Via a Spark configuration programmatic object called <kbd>SparkConf</kbd> defined in your driver program</li>
<li>Via command-line arguments passed to <kbd>spark-submit</kbd> or <kbd>spark-shell</kbd></li>
<li>Via default options set in <kbd>conf/spark-defaults.conf</kbd></li>
</ul>
</li>
<li><strong>Environmental variables</strong> control per-machine settings, such as the local IP address of the local worker node, and which can be defined in <kbd>conf/spark-env.sh</kbd>.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Spark properties</h1>
                </header>
            
            <article>
                
<p>In our case, we will set some basic default Spark properties via <kbd>conf/spark-defaults.conf</kbd>, freeing us to concentrate on the data science content in future chapters. This can be achieved by executing the following shell commands (edit the values as per your environment):</p>
<pre><strong>&gt; cp conf/spark-defaults.conf.template conf/spark-defaults.conf</strong><br/><strong>&gt; vi conf/spark-defaults.conf</strong><br/><strong>      $ spark.master spark://192.168.56.10:7077</strong><br/><strong>      $ spark.driver.cores 1</strong><br/><strong>      $ spark.driver.maxResultSize 0</strong><br/><strong>      $ spark.driver.memory 2g</strong><br/><strong>      $ spark.executor.memory 2g</strong><br/><strong>      $ spark.executor.cores 2</strong><br/><strong>      $ spark.serializer org.apache.spark.serializer.KryoSerializer</strong><br/><strong>      $ spark.rdd.compress true</strong><br/><strong>      $ spark.kryoserializer.buffer.max 128m</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Environmental variables</h1>
                </header>
            
            <article>
                
<p>We will also set basic environmental variables via <kbd>conf/spark-env.sh</kbd> as follows (edit the values as per your environment):</p>
<pre><strong>&gt; cp conf/spark-env.sh.template conf/spark-env.sh</strong><br/><strong>&gt; vi conf/spark-env.sh</strong><br/><strong>      $ export SPARK_LOCAL_IP=192.168.56.10</strong><br/><strong>      $ export SPARK_LOCAL_DIRS=/data/spark/local/data</strong><br/><strong>      $ export SPARK_MASTER_HOST=192.168.56.10</strong><br/><strong>      $ export SPARK_WORKER_DIR=/data/spark/local/worker</strong><br/><strong>      $ export SPARK_CONF_DIR=/opt/spark-2.3.2-bin-hadoop2.7/conf</strong><br/><strong>      $ export SPARK_LOG_DIR=/data/spark/local/logs</strong><br/><strong>      $ export SPARK_PID_DIR=/data/spark/local/pid</strong></pre>
<div class="packt_infobox"><span>To learn more about the various Spark configuration options available, including an exhaustive list of Spark properties and environmental variables, please visit</span> <a href="https://spark.apache.org/docs/latest/configuration.html">https://spark.apache.org/docs/latest/configuration.html</a><span>.</span></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Standalone master server</h1>
                </header>
            
            <article>
                
<p>We are now ready to start the Spark standalone master server, as follows:</p>
<pre><strong>&gt; sbin/start-master.sh</strong></pre>
<p><span>To check whether this was successful, you can examine the Spark logs as written to</span> <kbd>SPARK_LOG_DIR</kbd><span>. Spark applications can be submitted to the standalone master server at <kbd>spark://&lt;Master IP Address&gt;:7077</kbd> (port <kbd>7077</kbd> by default) or <kbd>spark://&lt;Master IP Address&gt;:6066</kbd> using its REST URL in cluster mode (port <kbd>6066</kbd> by default).</span></p>
<p>The Spark Master server also provides an out-of-the-box master web <strong>User Interface</strong> (<strong>UI</strong>) in which running Spark applications and workers can be monitored and performance diagnosed. By default, this master web UI is accessible via HTTP on port <kbd>8080</kbd>, in other words, <kbd>http://&lt;Master IP Address&gt;:8080</kbd>, the interface of which is illustrated in <em>Figure 2.2</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-546 image-border" src="Images/295c2c65-f9a7-4fa9-bf7a-7e6ba03d5cc5.png" style="width:66.25em;height:24.92em;" width="1329" height="501"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 2.2: Spark standalone master server web UI</div>
<p>Again, in the event that you cannot access this URL via a remote internet browser, you may need to open up port <kbd>8080</kbd> (by default) in your firewall and/or SELinux settings.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Spark worker node</h1>
                </header>
            
            <article>
                
<p>We are now ready to start our Spark Worker node, as follows:</p>
<pre><strong>&gt; sbin/start-slave.sh spark://192.168.56.10:7077</strong></pre>
<p><span>Again, to check whether this was successful, you can examine the Spark logs as written to</span> <kbd>SPARK_LOG_DIR</kbd><span>. You can also access the Spark Master web UI to confirm that the worker has been registered successfully, as illustrated in <em>Figure 2.3</em>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-547 image-border" src="Images/d3a310da-b0c6-493f-9897-8de1c32ee451.png" style="width:66.83em;height:19.00em;" width="1221" height="348"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 2.3: Spark worker successfully registered</div>
<p>Note that Spark workers also expose a <strong>Worker UI</strong> via HTTP on port 8081 by default, in other words, <kbd>http://&lt;Worker IP Address&gt;:8081</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">PySpark and Jupyter Notebook</h1>
                </header>
            
            <article>
                
<p>Let's now integrate Jupyter Notebook with PySpark so that we can write our first Spark applications in Python! In the case of our local development environment, the easiest way to integrate Jupyter Notebook with PySpark is to set a global <kbd>SPARK_HOME</kbd> environmental variable that points to the directory containing the Spark binaries. Thereafter, we can employ the <kbd>findspark</kbd> Python package, as installed earlier, that will append the location of <kbd>SPARK_HOME</kbd>, and hence the PySpark API, to <kbd>sys.path</kbd> at runtime. Note that <kbd>findspark</kbd> should not be used for production-grade code development—instead, Spark applications should be deployed as code artifacts submitted via <kbd>spark-submit</kbd>.</p>
<p>Please execute the following shell commands as the Linux root user, or another user with elevated privileges, in order to define a global environmental variable called <kbd>SPARK_HOME</kbd> (or, alternatively, add it to your local Linux user's <kbd>.bashrc</kbd> file, which requires no administrative privileges):</p>
<pre><strong>&gt; cd /etc/profile.d</strong><br/><strong>&gt; vi spark.sh</strong><br/><strong>      $ export SPARK_HOME=/opt/spark-2.3.2-bin-hadoop2.7</strong><br/><strong>&gt; source spark.sh</strong></pre>
<div class="packt_infobox">You will need to restart any running Jupyter Notebook instances, and the underlying Terminal sessions from which they were spawned, in order for the SPARK_HOME environmental variable to be successfully recognized and registered by findspark.</div>
<p>We are now ready to write our first Spark application in Python! Instantiate a Jupyter Notebook instance, access it via your internet browser, and create a new Python 3 notebook containing the following code (it may be easier to split the following code over separate notebook cells for future ease of reference):</p>
<pre># (1) Import required Python dependencies<br/>import findspark<br/>findspark.init()<br/>from pyspark import SparkContext, SparkConf<br/>import random<br/><br/># (2) Instantiate the Spark Context<br/>conf = SparkConf()<br/>   .setMaster("spark://192.168.56.10:7077")<br/>   .setAppName("Calculate Pi")<br/>sc = SparkContext(conf=conf)<br/><br/># (3) Calculate the value of Pi i.e. 3.14...<br/>def inside(p):<br/>    x, y = random.random(), random.random()<br/>    return x*x + y*y &lt; 1<br/><br/>num_samples = 100<br/>count = sc.parallelize(range(0, num_samples)).filter(inside).count()<br/>pi = 4 * count / num_samples<br/><br/># (4) Print the value of Pi<br/>print(pi)<br/><br/># (5) Stop the Spark Context<br/>sc.stop()</pre>
<p><span>This PySpark application, at a high level, works as follows:</span></p>
<ol>
<li>Import the required Python dependencies, including <kbd>findspark</kbd> and <kbd>pyspark</kbd></li>
<li>Create a Spark context, which tells the Spark application how to connect to the Spark cluster, by instantiating it with a <kbd>SparkConf</kbd> object that provides application-level settings at a higher level of precedence</li>
<li>Calculate the mathematical value of Pi π</li>
<li>Print the value of Pi and display it in Jupyter Notebook as a cell output</li>
<li>Stop the Spark context that terminates the Spark application</li>
</ol>
<p>If you access the Spark Master web UI before executing <kbd>sc.stop()</kbd>, the Spark application will be listed under <span class="packt_screen">Running Applications</span>, at which time you may view its underlying worker and executor log files. If you access the Spark Master web UI following execution of <kbd>sc.stop()</kbd>, the Spark application will be listed under <span class="packt_screen">Completed Applications</span>.</p>
<div class="packt_infobox">Note that this notebook can be downloaded from the GitHub repository accompanying this book and is called <kbd>chp02-test-jupyter-notebook-with-pyspark.ipynb</kbd>.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Apache Kafka 2.0</h1>
                </header>
            
            <article>
                
<p>To finish off our local development environment, we will install Apache Kafka. As described in <a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem</em>, Apache Kafka is a distributed streaming platform. We will use Apache Kafka in <a href="cad17bf3-6d9d-4486-a405-3d5103b072c5.xhtml" target="_blank">Chapter 8</a>, <em>Real-Time Machine Learning Using Apache Spark</em>, to develop a real-time analytical model by combining it with Spark Streaming and <kbd>MLlib</kbd>.</p>
<p>Again, for the purposes of our single-node development cluster, Apache Kafka will be deployed on the same single node as the Apache Spark software services. We will also be installing the version of Apache Kafka 2.0.0 that has been built for Scala 2.11.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Kafka binaries</h1>
                </header>
            
            <article>
                
<p>After downloading the Kafka release, the first thing we need to do is to extract and install the pre-compiled binaries on our single-node cluster. In our case, we will be installing the Kafka binaries into <kbd>/opt</kbd>. Please execute the following shell commands as a local Linux user to extract the Apache Kafka binaries:</p>
<pre><strong>&gt; tar -xzf kafka_2.11-2.0.0.tgz -C /opt</strong><br/><strong>&gt; cd /opt/kafka_2.11-2.0.0</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Local working directories</h1>
                </header>
            
            <article>
                
<p>As with Apache Spark processes, Apache Kafka processes also require their own local working directories to persist local data and log files. <span>The following commands will create defined local directories in which to store these local working outputs, the paths of which you can edit as per your preferences:</span></p>
<pre><strong>&gt; mkdir -p /data/zookeeper/local/data</strong><br/><strong>&gt; mkdir -p /data/kafka/local/logs</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Kafka configuration</h1>
                </header>
            
            <article>
                
<p>We will also set some basic configuration as follows (edit the values as per your environment):</p>
<pre><strong>&gt; vi config/zookeeper.properties</strong><br/><strong>      $ dataDir=/data/zookeeper/local/data</strong><br/><strong>&gt; vi config/server.properties</strong><br/><strong>      $ listeners=PLAINTEXT://192.168.56.10:9092</strong><br/><strong>      $ log.dirs=/data/kafka/local/logs</strong><br/><strong>      $ zookeeper.connect=192.168.56.10:2181</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Start the Kafka server</h1>
                </header>
            
            <article>
                
<p>We are now ready to start Apache Kafka as follows:</p>
<pre><strong>&gt; bin/zookeeper-server-start.sh -daemon config/zookeeper.properties</strong><br/><strong>&gt; bin/kafka-server-start.sh -daemon config/server.properties</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Testing Kafka</h1>
                </header>
            
            <article>
                
<p>Finally, we can test our Kafka installation by creating a test topic as follows:</p>
<pre><strong>&gt; bin/kafka-topics.sh --create --zookeeper 192.168.56.10:2181 --replication-factor 1 --partitions 1 --topic our-first-topic</strong><br/><strong>      $ Created topic "our-first-topic".</strong><br/><strong>&gt; bin/kafka-topics.sh --list --zookeeper 192.168.56.10:2181</strong><br/><strong>      $ our-first-topic</strong></pre>
<p><span>Once we have created our test topic, let's start a command-line producer application and send some test messages to this topic as follows:</span></p>
<pre><strong>&gt; bin/kafka-console-producer.sh --broker-list 192.168.56.10:9092 --topic our-first-topic</strong><br/><strong> &gt; This is my 1st test message</strong><br/><strong> &gt; This is my 2nd test message</strong><br/><strong> &gt; This is my 3rd test message</strong></pre>
<p><span>Finally, let's start a command-line consumer application (in another Terminal session) to consume these test messages and print them to the console, as follows:</span></p>
<pre><strong>&gt; bin/kafka-console-consumer.sh --bootstrap-server 192.168.56.10:9092 --topic our-first-topic --from-beginning</strong><br/><strong>      $ This is my 1st test message</strong><br/><strong>      $ This is my 2nd test message</strong><br/><strong>      $ This is my 3rd test message</strong></pre>
<p><span>In fact, if you keep typing new messages in the Terminal running the producer application, you will see them immediately being consumed by the consumer application and printed to the console in its Terminal!</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have installed, configured, and deployed a local analytical development environment consisting of a single-node Apache Spark 2.3.2 and Apache Kafka 2.0.0 cluster that will also allow us to interactively develop Spark applications using Python 3.6 via Jupyter Notebook.</p>
<p>In the next chapter, we will discuss some of the high-level concepts behind common artificial intelligence and machine learning algorithms, as well as introducing Apache Spark's machine learning library, <kbd>MLlib</kbd>!</p>


            </article>

            
        </section>
    </div>



  </body></html>