<html><head></head><body>
<div id="_idContainer058">
<h1 class="chapter-number" id="_idParaDest-102"><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-103"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.2.1">Introducing Artificial Neural Network Modeling</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.3.1">Artificial neural networks</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.5.1">ANNs</span></strong><span class="koboSpan" id="kobo.6.1">) include data structures and algorithms for learning and classifying data. </span><span class="koboSpan" id="kobo.6.2">Through neural network techniques, a program can learn through </span><a id="_idIndexMarker468"/><span class="koboSpan" id="kobo.7.1">examples and create an internal structure of rules to classify different inputs. </span><span class="koboSpan" id="kobo.7.2">MATLAB provides algorithms, pre-trained models, and apps to create, train, visualize, and simulate ANNs. </span><span class="koboSpan" id="kobo.7.3">In this chapter, we will see how to use MATLAB to build an ANN-based model to predict values and </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">classify data.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.11.1">Getting started </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">with ANNs</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Training and testing an ANN model </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">in MATLAB</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Understanding data fitting </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">with ANNs</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Discovering pattern recognition </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">using ANNs</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Building a clustering application with </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">an ANN</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">Exploring advanced </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">optimization techniques</span></span></li>
</ul>
<h1 id="_idParaDest-104"><a id="_idTextAnchor107"/><span class="koboSpan" id="kobo.23.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.24.1">In this chapter, we will introduce basic concepts relating to machine learning. </span><span class="koboSpan" id="kobo.24.2">To understand these topics, a basic knowledge of algebra and mathematical modeling is needed. </span><span class="koboSpan" id="kobo.24.3">You will also need a working knowledge of the </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">MATLAB environment.</span></span></p>
<p><span class="koboSpan" id="kobo.26.1">To work with the MATLAB code in this chapter, you need the following files (available on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">at </span></span><a href="https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition"><span class="No-Break"><span class="koboSpan" id="kobo.28.1">https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.29.1">):</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.30.1">ANNFitting.m</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.31.1">ANNPatReg.m</span></strong></span></li>
</ul>
<h1 id="_idParaDest-105"><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.32.1">Getting started with ANNs</span></h1>
<p><span class="koboSpan" id="kobo.33.1">Serial computers and their programs exhibit remarkable capabilities in executing tasks that involve repetitive, well-defined operations, prioritizing accuracy, reliability, and speed. </span><span class="koboSpan" id="kobo.33.2">While these </span><a id="_idIndexMarker469"/><span class="koboSpan" id="kobo.34.1">information-processing systems are highly valuable, they lack true intelligence. </span><span class="koboSpan" id="kobo.34.2">The sole element of intelligence lies with the </span><a id="_idIndexMarker470"/><span class="koboSpan" id="kobo.35.1">programmer who comprehends the task and formulates the program. </span><span class="koboSpan" id="kobo.35.2">To achieve true </span><strong class="bold"><span class="koboSpan" id="kobo.36.1">artificial intelligence</span></strong><span class="koboSpan" id="kobo.37.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.38.1">AI</span></strong><span class="koboSpan" id="kobo.39.1">), a system must possess the ability to solve problems that humans consider simple, trivial, </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">and intuitive.</span></span></p>
<h2 id="_idParaDest-106"><a id="_idTextAnchor109"/><span class="koboSpan" id="kobo.41.1">Basic concepts relating to ANNs</span></h2>
<p><span class="koboSpan" id="kobo.42.1">ANNs are designed to emulate the intricate workings of biological nervous systems, comprising a </span><a id="_idIndexMarker471"/><span class="koboSpan" id="kobo.43.1">vast array of nerve cells or neurons interconnected in a complex network. </span><span class="koboSpan" id="kobo.43.2">Typically, each neuron establishes connections with tens of thousands of other neurons, resulting in hundreds of billions of connections. </span><span class="koboSpan" id="kobo.43.3">The emergence of intelligent behavior stems from the myriad interactions among these </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">interconnected units.</span></span></p>
<p><span class="koboSpan" id="kobo.45.1">In this network, specific units serve distinct functions. </span><span class="koboSpan" id="kobo.45.2">Some units act as receivers of information from the environment, while others respond to stimuli in the environment. </span><span class="koboSpan" id="kobo.45.3">Certain units, known as hidden units, solely communicate within the network, concealed from direct interaction with the external environment. </span><span class="koboSpan" id="kobo.45.4">Overall, the neural network’s structure involves input units, output units, and hidden units working in concert to process information and exhibit </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">intelligent behavior.</span></span></p>
<p><span class="koboSpan" id="kobo.47.1">Each unit in the neural network executes a simple operation: it activates when the cumulative received signal surpasses an activation threshold. </span><span class="koboSpan" id="kobo.47.2">Once active, the unit transmits a signal to other connected units through communication channels. </span><span class="koboSpan" id="kobo.47.3">These connections act as filters, transforming the messages into excitatory or inhibitory signals, and adjusting their intensity based on individual characteristics. </span><span class="koboSpan" id="kobo.47.4">Remarkably, the network’s input-output link, or transfer function, is not explicitly programmed but rather acquired through a learning process using empirical data, which can be supervised, unsupervised, or </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">reinforcement learning.</span></span></p>
<p><span class="koboSpan" id="kobo.49.1">ANNs are a type of AI that can learn from experience. </span><span class="koboSpan" id="kobo.49.2">This learning is possible thanks to their structure, which is similar to that of the human brain. </span><span class="koboSpan" id="kobo.49.3">Like the human brain, ANNs are composed of a large number of nodes, which are connected to each other by connections. </span><span class="koboSpan" id="kobo.49.4">Nodes process information, and connections determine how information is transmitted </span><a id="_idIndexMarker472"/><span class="koboSpan" id="kobo.50.1">from one node to another. </span><span class="koboSpan" id="kobo.50.2">ANNs learn from experience through a process called supervised learning. </span><span class="koboSpan" id="kobo.50.3">In this process, ANNs are provided with a dataset of desired input and output examples. </span><span class="koboSpan" id="kobo.50.4">ANNs then use this data to learn to map inputs to desired outputs. </span><span class="koboSpan" id="kobo.50.5">For example, an ANN that needs to learn to recognize images of dogs is provided with a dataset of images of dogs and images of other animals. </span><span class="koboSpan" id="kobo.50.6">The ANN uses these images to learn to distinguish between dogs and </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">other animals.</span></span></p>
<p><span class="koboSpan" id="kobo.52.1">Neural networks operate in parallel, enabling them to handle multiple data simultaneously, in contrast to serial computers, which process data individually and sequentially. </span><span class="koboSpan" id="kobo.52.2">Although individual neurons may be relatively slow, the parallel nature of neural networks accounts for the brain’s higher processing speed when tackling tasks requiring the simultaneous handling of numerous data points, such as visual object recognition. </span><span class="koboSpan" id="kobo.52.3">This remarkable system exhibits robust noise immunity, akin to a sophisticated statistical model. </span><span class="koboSpan" id="kobo.52.4">Even in the event of some unit malfunctions, the overall network performance may experience reductions, but a complete system shutdown is unlikely </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">to occur.</span></span></p>
<p><span class="koboSpan" id="kobo.54.1">Nevertheless, the latest generation of neural network software demands a solid understanding of statistics. </span><span class="koboSpan" id="kobo.54.2">Despite their seemingly immediate usability, users must not be deceived, as they can quickly make predictions or classifications within certain limitations. </span><span class="koboSpan" id="kobo.54.3">From an industrial perspective, these networks prove effective when historical data is available for processing with neural algorithms. </span><span class="koboSpan" id="kobo.54.4">This capability is particularly valuable in production environments as it facilitates data extraction and </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">model creation.</span></span></p>
<p><span class="koboSpan" id="kobo.56.1">It’s essential to note that while models generated by neural networks are highly efficient, they lack explanations in human symbolic language. </span><span class="koboSpan" id="kobo.56.2">The outcomes must be accepted as they are, leading to the characterization of neural networks as </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">black boxes.</span></span></p>
<p><span class="koboSpan" id="kobo.58.1">Like any modeling algorithm, the efficiency of neural networks relies heavily on the careful selection of predictive variables. </span><span class="koboSpan" id="kobo.58.2">These networks require a training phase to establish individual neuron weights, which can be time-consuming when dealing with many records and variables. </span><span class="koboSpan" id="kobo.58.3">Unlike conventional models, neural networks lack theorems or definitive guidelines, making the success of a network heavily dependent on the </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">creator’s experience.</span></span></p>
<p><span class="koboSpan" id="kobo.60.1">Neural networks find their utility in scenarios where data may be partially inaccurate or where analytical models are unavailable for problem-solving. </span><span class="koboSpan" id="kobo.60.2">They are commonly used in Optical Character Recognition (OCR) software, facial recognition systems, and other applications that handle error-prone or noisy data. </span><span class="koboSpan" id="kobo.60.3">Moreover, they are widely employed in data mining analysis and serve as tools </span><a id="_idIndexMarker473"/><span class="koboSpan" id="kobo.61.1">for forecasting in financial and meteorological domains. </span><span class="koboSpan" id="kobo.61.2">In recent years, their significance has substantially increased in the field of bioinformatics, where they are instrumental in identifying functional and structural patterns in nucleic acids and proteins. </span><span class="koboSpan" id="kobo.61.3">By providing a comprehensive set of input data, the network can produce the most </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">probable output.</span></span></p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor110"/><span class="koboSpan" id="kobo.63.1">Understanding how perceptrons work</span></h2>
<p><span class="koboSpan" id="kobo.64.1">The fundamental unit of a neural network is the </span><strong class="bold"><span class="koboSpan" id="kobo.65.1">perceptron</span></strong><span class="koboSpan" id="kobo.66.1">, which emulates the essential functions </span><a id="_idIndexMarker474"/><span class="koboSpan" id="kobo.67.1">of a biological neuron. </span><span class="koboSpan" id="kobo.67.2">It evaluates the strength of each input, accumulates these inputs, and then compares the </span><a id="_idIndexMarker475"/><span class="koboSpan" id="kobo.68.1">sum to a specific threshold. </span><span class="koboSpan" id="kobo.68.2">Based on this comparison, the perceptron determines the output value. </span><span class="koboSpan" id="kobo.68.3">The neuron’s basic structure is well understood, and researchers have identified the primary biochemical reactions that govern its activity. </span><span class="koboSpan" id="kobo.68.4">As such, a neuron can be regarded as the elemental computational unit of the brain. </span><span class="koboSpan" id="kobo.68.5">Within the human brain, approximately 100 distinct classes of neurons have been identified, each contributing to the intricate neural network responsible for our cognitive processes </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">and abilities.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer045">
<span class="koboSpan" id="kobo.70.1"><img alt="Figure 5.1 – Understanding the perceptron scheme" src="image/B21156_05_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.71.1">Figure 5.1 – Understanding the perceptron scheme</span></p>
<p><span class="koboSpan" id="kobo.72.1">The primary function of a biological neuron is to generate an electrical potential that travels along its </span><strong class="bold"><span class="koboSpan" id="kobo.73.1">axon</span></strong><span class="koboSpan" id="kobo.74.1"> (neuron output) when the electrical activity at the neuron’s cell body surpasses a specific threshold. </span><span class="koboSpan" id="kobo.74.2">The neuron’s input is received through a set of fibers called </span><strong class="bold"><span class="koboSpan" id="kobo.75.1">dendrites</span></strong><span class="koboSpan" id="kobo.76.1">, which </span><a id="_idIndexMarker476"/><span class="koboSpan" id="kobo.77.1">contact the axons of other neurons, transmitting electrical potentials from them. </span><span class="koboSpan" id="kobo.77.2">The point of connection between an axon of one neuron and the dendrite of another neuron is referred to as </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">a </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.79.1">synapse</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.81.1">The synapse </span><a id="_idIndexMarker477"/><span class="koboSpan" id="kobo.82.1">possesses the ability to regulate the electrical pulse </span><a id="_idIndexMarker478"/><span class="koboSpan" id="kobo.83.1">emanating from the axon. </span><span class="koboSpan" id="kobo.83.2">The electrical potential generated by a neuron is essentially binary: an on/off state. </span><span class="koboSpan" id="kobo.83.3">If the neuron’s electrical activity surpasses a specific threshold, an impulse is generated; otherwise, no </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">impulse occurs.</span></span></p>
<p><span class="koboSpan" id="kobo.85.1">Notably, the intensity of the generated pulse remains consistent across different neurons. </span><span class="koboSpan" id="kobo.85.2">As the potential propagates along the axon and reaches the synapse connected to another neuron’s dendrite, the post-synaptic potential relies on the biochemical characteristics of the synapse. </span><span class="koboSpan" id="kobo.85.3">Despite having the same pre-synaptic potential, two distinct synapses may generate varying post-synaptic potentials. </span><span class="koboSpan" id="kobo.85.4">In other words, the synapse modulates and weighs the input potential </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">before transmission.</span></span></p>
<p><span class="koboSpan" id="kobo.87.1">Post-synaptic potentials continue through the neuron’s dendrites and accumulate at the soma level. </span><span class="koboSpan" id="kobo.87.2">Only when the sum of these potentials surpasses a certain threshold does the neuron trigger the potential to propagate through its axon. </span><span class="koboSpan" id="kobo.87.3">Both biological neurons and artificial neurons receive multiple inputs through dendrites. </span><span class="koboSpan" id="kobo.87.4">The artificial neuron aggregates these various input values and computes </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">the result.</span></span></p>
<p><span class="koboSpan" id="kobo.89.1">If the computed value exceeds a particular threshold, the artificial neuron produces an output signal or potential; otherwise, it </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">remains inactive.</span></span></p>
<p><span class="koboSpan" id="kobo.91.1">The initial function implemented by the artificial neuron is the algebraic sum of its inputs, which serves </span><a id="_idIndexMarker479"/><span class="koboSpan" id="kobo.92.1">to construct the system’s response. </span><span class="koboSpan" id="kobo.92.2">When simulating a phenomenon, the system may encounter errors, necessitating appropriate corrections. </span><span class="koboSpan" id="kobo.92.3">To achieve this, each input is assigned a weight, a numerical value that </span><a id="_idIndexMarker480"/><span class="koboSpan" id="kobo.93.1">modulates its impact on the total sum, determining the </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">neuron’s potential.</span></span></p>
<p><span class="koboSpan" id="kobo.95.1">In other words, each input contributes differently to the determination of the threshold value and potential triggering, akin to the characteristic of biological neurons involving synapses between the axon of one neuron and the dendrite </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">of another.</span></span></p>
<p><span class="koboSpan" id="kobo.97.1">From the perspective of the post-synaptic neuron, the inputs consist of potentials from other neurons whose axons synapse with their dendrites, and these inputs are precisely modulated by the synapses. </span><span class="koboSpan" id="kobo.97.2">Some inputs may exert a stronger influence on the total sum, while others could even be inhibitory, reducing the overall sum and thereby lowering the probability of exceeding the threshold and triggering </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">a potential.</span></span></p>
<p><span class="koboSpan" id="kobo.99.1">This essential property of biological systems is mathematically modeled in connection systems using the concept of weights. </span><span class="koboSpan" id="kobo.99.2">Each connection is assigned a numeric value as its weight, which is multiplied by the input value. </span><span class="koboSpan" id="kobo.99.3">Consequently, the input’s effect on the total sum is determined by the magnitude of </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">its weight.</span></span></p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor111"/><span class="koboSpan" id="kobo.101.1">Activation function to introduce non-linearity</span></h2>
<p><span class="koboSpan" id="kobo.102.1">In our previous discussion, we explored the weighted sum function introduced by incorporating </span><a id="_idIndexMarker481"/><span class="koboSpan" id="kobo.103.1">the concept of weights. </span><span class="koboSpan" id="kobo.103.2">Now, let’s delve into another property of the artificial neuron, once again inspired by </span><a id="_idIndexMarker482"/><span class="koboSpan" id="kobo.104.1">the behavior of biological neurons. </span><span class="koboSpan" id="kobo.104.2">As mentioned earlier, the biological neuron sums up the post-synaptic potentials of its dendrites at the soma level. </span><span class="koboSpan" id="kobo.104.3">However, this summation is not a simple algebraic addition of these potentials. </span><span class="koboSpan" id="kobo.104.4">Various factors, such as the passive resistance of the neuron membrane, come into play, making the actual summation a function that is </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">typically non-linear.</span></span></p>
<p><span class="koboSpan" id="kobo.106.1">Similarly, artificial neurons compute the weighted inputs and then modify the result using a specific function. </span><span class="koboSpan" id="kobo.106.2">This function is referred to as the activation function, which is applied to the output of the neuron to determine its true potential. </span><span class="koboSpan" id="kobo.106.3">The activation function plays a crucial role in shaping the behavior of the neuron and ultimately influences the outcome of the neural </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">network’s computation.</span></span></p>
<p><span class="koboSpan" id="kobo.108.1">The activation </span><a id="_idIndexMarker483"/><span class="koboSpan" id="kobo.109.1">function takes the weighted sum of inputs and an additional </span><a id="_idIndexMarker484"/><span class="koboSpan" id="kobo.110.1">bias term and applies a specific mathematical operation to produce the neuron’s output. </span><span class="koboSpan" id="kobo.110.2">This output, often referred to as the activation or the post-activation value, is then used as input to the subsequent layers in the </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">neural network.</span></span></p>
<p><span class="koboSpan" id="kobo.112.1">Different types of activation functions are utilized in neural networks, each with its unique characteristics and applications. </span><span class="koboSpan" id="kobo.112.2">Common activation functions include </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.114.1">Step function</span></strong><span class="koboSpan" id="kobo.115.1">: A simple </span><a id="_idIndexMarker485"/><span class="koboSpan" id="kobo.116.1">binary function </span><a id="_idIndexMarker486"/><span class="koboSpan" id="kobo.117.1">that outputs </span><strong class="source-inline"><span class="koboSpan" id="kobo.118.1">1</span></strong><span class="koboSpan" id="kobo.119.1"> if the input is greater than or equal to a threshold, and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.120.1">0</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.121.1"> otherwise:</span></span></li>
</ul>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.122.1">f</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.123.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.124.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.125.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.126.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.127.1">0</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.128.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.129.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.130.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.131.1">&lt;</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.132.1">0</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.133.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.134.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.135.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.136.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.137.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.138.1">≥</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.139.1">0</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.140.1">Sigmoid function</span></strong><span class="koboSpan" id="kobo.141.1">: This </span><a id="_idIndexMarker487"/><span class="koboSpan" id="kobo.142.1">generates a smooth </span><a id="_idIndexMarker488"/><span class="koboSpan" id="kobo.143.1">S-shaped curve, mapping the input to an output between </span><strong class="source-inline"><span class="koboSpan" id="kobo.144.1">0</span></strong><span class="koboSpan" id="kobo.145.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.146.1">1</span></strong><span class="koboSpan" id="kobo.147.1">. </span><span class="koboSpan" id="kobo.147.2">It was commonly used in the past but has fallen out of favor in deeper networks due to the vanishing </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">gradient problem:</span></span></li>
</ul>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.149.1">f</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.150.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.151.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.152.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.153.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.154.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.155.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.156.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.157.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.158.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.159.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.160.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.161.1">e</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.162.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.163.1">−</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.164.1">x</span></span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.165.1">Rectified linear unit (ReLU)</span></strong><span class="koboSpan" id="kobo.166.1">: One of the most popular activation functions, ReLU </span><a id="_idIndexMarker489"/><span class="koboSpan" id="kobo.167.1">outputs the input </span><a id="_idIndexMarker490"/><span class="koboSpan" id="kobo.168.1">value if it is positive; otherwise, it outputs </span><strong class="source-inline"><span class="koboSpan" id="kobo.169.1">0</span></strong><span class="koboSpan" id="kobo.170.1">. </span><span class="koboSpan" id="kobo.170.2">It helps mitigate the vanishing gradient problem and accelerates convergence in </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">deep networks:</span></span></li>
</ul>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.172.1">f</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.173.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.174.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.175.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.176.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.177.1">0</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.178.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.179.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.180.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.181.1">&lt;</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.182.1">0</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.183.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.184.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.185.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.186.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.187.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.188.1">≥</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.189.1">0</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.190.1">Hyperbolic tangent (Tanh)</span></strong><span class="koboSpan" id="kobo.191.1">: Similar to the sigmoid function, Tanh produces an S-shaped </span><a id="_idIndexMarker491"/><span class="koboSpan" id="kobo.192.1">curve, but it </span><a id="_idIndexMarker492"/><span class="koboSpan" id="kobo.193.1">maps the input to an output between </span><strong class="source-inline"><span class="koboSpan" id="kobo.194.1">-1</span></strong><span class="koboSpan" id="kobo.195.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.196.1">1</span></strong><span class="koboSpan" id="kobo.197.1">. </span><span class="koboSpan" id="kobo.197.2">It is symmetric around the origin, providing a better range for the gradient compared to the </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">sigmoid function:</span></span></li>
</ul>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.199.1">f</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.200.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.201.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.202.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.203.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Function_v-normal"><span class="koboSpan" id="kobo.204.1">tanh</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.205.1">(</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.206.1">x</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.207.1">)</span></span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.208.1">Softmax function</span></strong><span class="koboSpan" id="kobo.209.1">: Primarily </span><a id="_idIndexMarker493"/><span class="koboSpan" id="kobo.210.1">used in the </span><a id="_idIndexMarker494"/><span class="koboSpan" id="kobo.211.1">output layer for multi-class classification problems, the softmax function converts a vector of real numbers into a probability distribution, ensuring the sum of probabilities </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">equals </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.213.1">1</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">:</span></span></li>
</ul>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.215.1">f</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.216.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.217.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.218.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.219.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.220.1">i</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.221.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.222.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.223.1">e</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.224.1"> </span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.225.1">−</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.226.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.227.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.228.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.229.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.230.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.231.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.232.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.233.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.234.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.235.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.236.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.237.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.238.1">k</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.239.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.240.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.241.1">e</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.242.1"> </span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.243.1">−</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.244.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.245.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.246.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.247.1"> </span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.248.1">Exponential linear unit (ELU)</span></strong><span class="koboSpan" id="kobo.249.1">: ELU is an activation function used in ANNs. </span><span class="koboSpan" id="kobo.249.2">It is a </span><a id="_idIndexMarker495"/><span class="koboSpan" id="kobo.250.1">smooth, non-saturating </span><a id="_idIndexMarker496"/><span class="koboSpan" id="kobo.251.1">function that can handle both positive and negative inputs. </span><span class="koboSpan" id="kobo.251.2">ELU is a more recent activation function than ReLU, which is also </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">widely used:</span></span></li>
</ul>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.253.1">f</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.254.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.255.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.256.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.257.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.258.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.259.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.260.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.261.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.262.1">&gt;</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.263.1">0</span></span></p>
<p class="list-inset"><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.264.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.265.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.266.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.267.1">h</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.268.1">a</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.269.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.270.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.271.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.272.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.273.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.274.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.275.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.276.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.277.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.278.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.279.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.280.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.281.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.282.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.283.1">&lt;</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.284.1">0</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.285.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.286.1">x</span></em><span class="koboSpan" id="kobo.287.1"> is the input to the ELU function and </span><em class="italic"><span class="koboSpan" id="kobo.288.1">α</span></em><span class="koboSpan" id="kobo.289.1"> is a hyperparameter that controls the steepness of the negative slope. </span><span class="koboSpan" id="kobo.289.2">The default value of </span><em class="italic"><span class="koboSpan" id="kobo.290.1">α</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.291.1">is </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.292.1">1.0</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.294.1">The ELU function is a smooth, non-saturating function that can handle both positive and negative inputs. </span><span class="koboSpan" id="kobo.294.2">This makes it a good choice for tasks that involve both types of inputs, such as image recognition and natural </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">language processing.</span></span></p>
<p><span class="koboSpan" id="kobo.296.1">The choice </span><a id="_idIndexMarker497"/><span class="koboSpan" id="kobo.297.1">of the activation function influences the neural network’s </span><a id="_idIndexMarker498"/><span class="koboSpan" id="kobo.298.1">performance, training speed, and ability to capture complex relationships in the data. </span><span class="koboSpan" id="kobo.298.2">Different activation functions may be used in different layers of the network, depending on the specific problem and </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">architectural considerations.</span></span></p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor112"/><span class="koboSpan" id="kobo.300.1">ANN’s architecture explained</span></h2>
<p><span class="koboSpan" id="kobo.301.1">Having discussed the properties of an artificial neuron, we will now delve into the architecture </span><a id="_idIndexMarker499"/><span class="koboSpan" id="kobo.302.1">of a neural network. </span><span class="koboSpan" id="kobo.302.2">This entails both a physical illustration of the network’s structure and a determination of the role each neuron plays within this framework. </span><span class="koboSpan" id="kobo.302.3">Consider a scenario with multiple inputs and nodes, such that each input is connected to every node. </span><span class="koboSpan" id="kobo.302.4">Similarly, each output node receives connections from all inputs. </span><span class="koboSpan" id="kobo.302.5">Each output node possesses the characteristics described earlier and carries out its computations in conjunction with the other nodes. </span><span class="koboSpan" id="kobo.302.6">Upon introducing an input pattern, the output values are influenced either by the input values themselves or by the network’s weights. </span><span class="koboSpan" id="kobo.302.7">The weights play a crucial role in the network, determining the extent to which a particular input influences a specific node. </span><span class="koboSpan" id="kobo.302.8">The collection of nodes in the structure is commonly referred to as </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">a layer.</span></span></p>
<p><span class="koboSpan" id="kobo.304.1">Neural networks are organized into layers, each serving a specific purpose in information processing. </span><span class="koboSpan" id="kobo.304.2">The primary types of layers in a neural network include </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.306.1">Input layer</span></strong><span class="koboSpan" id="kobo.307.1">: The initial </span><a id="_idIndexMarker500"/><span class="koboSpan" id="kobo.308.1">layer that receives the input data and passes it on to the </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">subsequent layers.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.310.1">Hidden layers</span></strong><span class="koboSpan" id="kobo.311.1">: Intermediate layers between the input and output layers. </span><span class="koboSpan" id="kobo.311.2">These </span><a id="_idIndexMarker501"/><span class="koboSpan" id="kobo.312.1">layers process the data and extract relevant features through </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">complex transformations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.314.1">Output layer</span></strong><span class="koboSpan" id="kobo.315.1">: The </span><a id="_idIndexMarker502"/><span class="koboSpan" id="kobo.316.1">final layer that produces the network’s output or predictions based on the processed information from the </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">hidden layers.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.318.1">The number </span><a id="_idIndexMarker503"/><span class="koboSpan" id="kobo.319.1">of hidden layers and nodes within them, along with the choice of activation functions and weights, constitute the architecture of the neural network. </span><span class="koboSpan" id="kobo.319.2">The architecture (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.320.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.321.1">.2</span></em><span class="koboSpan" id="kobo.322.1">) significantly influences the network’s ability to learn from data, generalize to new examples, and perform specific </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">tasks efficiently.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer046">
<span class="koboSpan" id="kobo.324.1"><img alt="Figure 5.2 – ANN architecture with weights and activation function" src="image/B21156_05_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.325.1">Figure 5.2 – ANN architecture with weights and activation function</span></p>
<p><span class="koboSpan" id="kobo.326.1">Neural networks can take the form of multiple layers, and each layer added enhances the network’s computational capacity. </span><span class="koboSpan" id="kobo.326.2">Inputs are numerical values that undergo evaluation through the weights of connections with the first layer of nodes, known as the hidden layer. </span><span class="koboSpan" id="kobo.326.3">In this hidden layer, each node conducts computations as described earlier, leading to the generation of a potential that then propagates to the nodes of the output layer. </span><span class="koboSpan" id="kobo.326.4">The potentials produced by the output nodes collectively represent the final output calculated by the </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">neural network.</span></span></p>
<p><span class="koboSpan" id="kobo.328.1">The architecture of a neural network refers to the specific way in which the nodes are interconnected. </span><span class="koboSpan" id="kobo.328.2">In the case of feedforward neural networks, which are characterized by the architecture shown in the previous figure, the activation of input nodes propagates forward through the hidden layer and further to the output layer. </span><span class="koboSpan" id="kobo.328.3">Changing the connections </span><a id="_idIndexMarker504"/><span class="koboSpan" id="kobo.329.1">between nodes alters the network’s architecture. </span><span class="koboSpan" id="kobo.329.2">This not only yields practical consequences in terms of the network’s computational capacity but also carries significant theoretical implications related to the concept of learning. </span><span class="koboSpan" id="kobo.329.3">The arrangement of nodes in the network affects its ability to learn from data and perform specific tasks, making architecture design a crucial aspect of neural </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">network development.</span></span></p>
<p><span class="koboSpan" id="kobo.331.1">After analyzing the basic concepts of ANNs, we now need to pay attention to how these algorithms </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">are trained.</span></span></p>
<h1 id="_idParaDest-110"><a id="_idTextAnchor113"/><span class="koboSpan" id="kobo.333.1">Training and testing an ANN model in MATLAB</span></h1>
<p><span class="koboSpan" id="kobo.334.1">In the previous section, we saw the architecture of an ANN. </span><span class="koboSpan" id="kobo.334.2">It imposes two layers, input and output, which cannot be altered. </span><span class="koboSpan" id="kobo.334.3">Consequently, the critical factor lies in the number of hidden </span><a id="_idIndexMarker505"/><span class="koboSpan" id="kobo.335.1">layers we consider. </span><span class="koboSpan" id="kobo.335.2">The </span><a id="_idIndexMarker506"/><span class="koboSpan" id="kobo.336.1">size of a neural network is defined by the number of hidden neurons. </span><span class="koboSpan" id="kobo.336.2">Determining the optimal size </span><a id="_idIndexMarker507"/><span class="koboSpan" id="kobo.337.1">of the network </span><a id="_idIndexMarker508"/><span class="koboSpan" id="kobo.338.1">remains an ongoing challenge, as no analytical solution has been discovered to date. </span><span class="koboSpan" id="kobo.338.2">One approach to tackle this problem is to employ a heuristic method: creating various networks with increasing complexity, using a subset of the training data, and monitoring the error on a validation subset simultaneously. </span><span class="koboSpan" id="kobo.338.3">After completing the training process, the network with the lowest validation error is chosen as the </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">preferred one.</span></span></p>
<h2 id="_idParaDest-111"><a id="_idTextAnchor114"/><span class="koboSpan" id="kobo.340.1">How to train an ANN</span></h2>
<p><span class="koboSpan" id="kobo.341.1">Let’s discuss the process of choosing the number of layers. </span><span class="koboSpan" id="kobo.341.2">The number of input nodes is fixed </span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.342.1">based on the number of features in the input data, while the number of output nodes is determined by the number of outcomes to be modeled or the class levels in the outcome. </span><span class="koboSpan" id="kobo.342.2">The real challenge lies in determining the appropriate number of neurons for the hidden layer. </span><span class="koboSpan" id="kobo.342.3">Unfortunately, there is no analytical method to accomplish this task. </span><span class="koboSpan" id="kobo.342.4">The optimal number of neurons depends on various factors, such as the number of input nodes, the volume of training data, and the complexity of the learning algorithm, </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">among others.</span></span></p>
<p><span class="koboSpan" id="kobo.344.1">Having more neurons in the hidden layer will lead to a model that better fits the training data, but it comes with the risk of overfitting, potentially resulting in poor generalization on future data. </span><span class="koboSpan" id="kobo.344.2">Additionally, neural networks with many nodes can be computationally expensive and slow </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">to train.</span></span></p>
<p><span class="koboSpan" id="kobo.346.1">To address this, a heuristic approach can be adopted, where different configurations are experimented with to find an optimal balance. </span><span class="koboSpan" id="kobo.346.2">This trial-and-error method allows us to strike a balance between model complexity, accuracy, and </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">computational efficiency.</span></span></p>
<p><span class="koboSpan" id="kobo.348.1">ANNs consist of simple elements that operate in parallel. </span><span class="koboSpan" id="kobo.348.2">The connections between these elements </span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.349.1">play a vital role as they dictate the network’s functionalities. </span><span class="koboSpan" id="kobo.349.2">These connections influence the output through their respective weights, which are adjusted during the neural network’s </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">training phase.</span></span></p>
<p><span class="koboSpan" id="kobo.351.1">During training, the network is fine-tuned by modifying the connection weights, enabling specific inputs to yield desired outputs. </span><span class="koboSpan" id="kobo.351.2">For instance, the network can be calibrated by comparing its practical output with the target output we want to achieve. </span><span class="koboSpan" id="kobo.351.3">This iterative process continues until the network’s output aligns with the </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">desired target.</span></span></p>
<p><span class="koboSpan" id="kobo.353.1">To obtain dependable results, a substantial number of input/target pairs are required to shape the network appropriately. </span><span class="koboSpan" id="kobo.353.2">This training process ensures that the neural network can accurately produce the desired outputs for a variety of inputs, making it a reliable tool for </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">various tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.355.1">The adjustment of these weights is determined by the specific algorithm we choose to adopt. </span><span class="koboSpan" id="kobo.355.2">In the following practical examples, we will discuss and refer to various algorithms that govern the process of </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">weight adjustment.</span></span></p>
<h2 id="_idParaDest-112"><a id="_idTextAnchor115"/><span class="koboSpan" id="kobo.357.1">Introducing the MATLAB Neural Network Toolbox</span></h2>
<p><span class="koboSpan" id="kobo.358.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.359.1">Neural Network Toolbox</span></strong><span class="koboSpan" id="kobo.360.1"> offers a range of algorithms, pre-trained models, and apps that </span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.361.1">enable users to create, train, visualize, and simulate neural networks. </span><span class="koboSpan" id="kobo.361.2">It supports both shallow neural networks (with one hidden layer) and deep neural networks (with multiple hidden layers). </span><span class="koboSpan" id="kobo.361.3">With these tools, various tasks, such as classification, regression, clustering, dimensionality reduction, time-series forecasting, and dynamic system modeling and control, can </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">be performed.</span></span></p>
<p><span class="koboSpan" id="kobo.363.1">There are four primary ways to utilize the Neural </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">Network Toolbox:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.365.1">MATLAB graphical user interfaces</span></strong><span class="koboSpan" id="kobo.366.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.367.1">GUI</span></strong><span class="koboSpan" id="kobo.368.1">): The most user-friendly approach involves </span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.369.1">using MATLAB GUIs. </span><span class="koboSpan" id="kobo.369.2">The main window can be launched using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.370.1">nnstart</span></strong><span class="koboSpan" id="kobo.371.1"> command, granting access to automatic tasks such as function fitting (</span><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">nftool</span></strong><span class="koboSpan" id="kobo.373.1">), pattern recognition (</span><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">nprtool</span></strong><span class="koboSpan" id="kobo.375.1">), data clustering (</span><strong class="source-inline"><span class="koboSpan" id="kobo.376.1">nctool</span></strong><span class="koboSpan" id="kobo.377.1">), and time-series </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">analysis (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.379.1">ntstool</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.381.1">Basic command-line operations</span></strong><span class="koboSpan" id="kobo.382.1">: For greater flexibility, users can utilize command-line operations. </span><span class="koboSpan" id="kobo.382.2">While more knowledge is required, this method allows </span><a id="_idIndexMarker513"/><span class="koboSpan" id="kobo.383.1">users to have complete control over the process, without relying on menus and icons typically found in </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">the GUI.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.385.1">Customizing the toolbox</span></strong><span class="koboSpan" id="kobo.386.1">: Users can customize the toolbox by creating their own </span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.387.1">neural networks with arbitrary connections. </span><span class="koboSpan" id="kobo.387.2">Existing toolbox training features in the GUI can be used to continue training these </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">custom networks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.389.1">Modifying toolbox functions</span></strong><span class="koboSpan" id="kobo.390.1">: All computational components in the toolbox are written </span><a id="_idIndexMarker515"/><span class="koboSpan" id="kobo.391.1">in MATLAB code and are fully accessible, allowing users to modify and tailor them to </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">specific needs.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.393.1">This toolbox caters to users of all levels, from beginners to experts. </span><span class="koboSpan" id="kobo.393.2">It offers simple tools that guide </span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.394.1">new users through specific applications and more complex tools that enable experts to customize networks and experiment with </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">new architectures.</span></span></p>
<p><span class="koboSpan" id="kobo.396.1">Regardless of the approach chosen, a proper analysis using neural networks should encompass the </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">following steps:</span></span></p>
<ol>
<li><span class="No-Break"><span class="koboSpan" id="kobo.398.1">Data collection</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.399.1">Network creation</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.400.1">Network configuration</span></span></li>
<li><span class="koboSpan" id="kobo.401.1">Weight and </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">bias initialization</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.403.1">Network training</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.404.1">Network validation</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.405.1">Network testing</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.406.1">By following </span><a id="_idIndexMarker517"/><span class="koboSpan" id="kobo.407.1">these steps, users can effectively apply neural networks to various problems and tasks. </span><span class="koboSpan" id="kobo.407.2">They are explained in more </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">detail here:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.409.1">The first step in the process involves collecting the data to be analyzed, which is typically done outside the MATLAB environment. </span><span class="koboSpan" id="kobo.409.2">This data collection phase is crucial, as the quality of the data will significantly impact the final results and the ability to extract </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">meaningful insights.</span></span></li>
<li><span class="koboSpan" id="kobo.411.1">Next, we proceed to create the neural network using various functions available in the toolbox. </span><span class="koboSpan" id="kobo.411.2">These functions allow us to build the network through the chosen algorithm, resulting in the creation of a neural network object. </span><span class="koboSpan" id="kobo.411.3">This object stores all the necessary information defining the neural network’s properties, such as its architecture, subobject structures, functions, and weight and </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">bias values.</span></span></li>
<li><span class="koboSpan" id="kobo.413.1">The third step is network configuration, where we examine input and output data, set the dimensions of the network to fit the data, and choose appropriate input and output processing settings to enhance network performance. </span><span class="koboSpan" id="kobo.413.2">This configuration step is usually performed automatically when the training function is called but can also be done manually using the </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">configuration function.</span></span></li>
<li><span class="koboSpan" id="kobo.415.1">After configuration, the fourth step involves initializing the weights and biases. </span><span class="koboSpan" id="kobo.415.2">We set initial values from which the network will begin its training process. </span><span class="koboSpan" id="kobo.415.3">This initialization is usually done automatically based on the chosen training algorithm, but users can also set custom values </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">if needed.</span></span></li>
<li><span class="koboSpan" id="kobo.417.1">The fifth step is network training, which is a critical phase of the process. </span><span class="koboSpan" id="kobo.417.2">During training, the weights and biases are fine-tuned to optimize the network’s performance. </span><span class="koboSpan" id="kobo.417.3">This stage is crucial for the network’s ability to generalize well with new, unseen data. </span><span class="koboSpan" id="kobo.417.4">A portion of the collected data (typically around 70% of available cases) is used </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">for training.</span></span></li>
<li><span class="koboSpan" id="kobo.419.1">Next, in the sixth step, network validation takes place. </span><span class="koboSpan" id="kobo.419.2">Here, a fraction of randomly selected data (usually around 15% of available cases) is passed through the network to estimate how well the model has been trained. </span><span class="koboSpan" id="kobo.419.3">The results obtained during this phase help determine whether the chosen model adequately reflects the initial expectations or whether adjustments </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">are needed.</span></span></li>
<li><span class="koboSpan" id="kobo.421.1">Finally, in the last step, we utilize the trained network. </span><span class="koboSpan" id="kobo.421.2">A portion of the collected data (approximately 15% of available cases) is used to test the network’s performance. </span><span class="koboSpan" id="kobo.421.3">The trained neural network object can then be saved and employed </span><a id="_idIndexMarker518"/><span class="koboSpan" id="kobo.422.1">multiple times with new data as needed. </span><span class="koboSpan" id="kobo.422.2">This allows for the reuse of the network to make predictions or analyze </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">various datasets.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.424.1">The </span><a id="_idIndexMarker519"/><span class="koboSpan" id="kobo.425.1">workflow for </span><strong class="bold"><span class="koboSpan" id="kobo.426.1">neural network design</span></strong><span class="koboSpan" id="kobo.427.1"> involves breaking down the collected data into three sets: the </span><strong class="bold"><span class="koboSpan" id="kobo.428.1">training set</span></strong><span class="koboSpan" id="kobo.429.1">, the </span><strong class="bold"><span class="koboSpan" id="kobo.430.1">validation set</span></strong><span class="koboSpan" id="kobo.431.1">, and the </span><strong class="bold"><span class="koboSpan" id="kobo.432.1">test set</span></strong><span class="koboSpan" id="kobo.433.1">. </span><span class="koboSpan" id="kobo.433.2">Let’s describe each of them </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">in detail:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.435.1">Training set (usually 70% of the available cases)</span></strong><span class="koboSpan" id="kobo.436.1">: The training set is a collection of examples used to train the neural network and adjust its parameters. </span><span class="koboSpan" id="kobo.436.2">During the training process, the network learns from the input-output pairs </span><a id="_idIndexMarker520"/><span class="koboSpan" id="kobo.437.1">in the training set to optimize its internal weights and biases. </span><span class="koboSpan" id="kobo.437.2">The goal is to find the optimal set of parameters that best captures the underlying patterns and relationships in the data. </span><span class="koboSpan" id="kobo.437.3">The neural network improves its performance through iterative adjustments during the </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">training phase.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.439.1">Validation set (usually 15% of the available cases)</span></strong><span class="koboSpan" id="kobo.440.1">: The validation set is a separate </span><a id="_idIndexMarker521"/><span class="koboSpan" id="kobo.441.1">set of examples used to fine-tune the network’s parameters and assess its performance during training. </span><span class="koboSpan" id="kobo.441.2">It serves as a validation mechanism to prevent overfitting, a condition where the network performs well on the training data but poorly on new, unseen data. </span><span class="koboSpan" id="kobo.441.3">By monitoring the network’s performance on the validation set, we can make decisions about the model’s complexity, such as determining the optimal number of </span><a id="_idIndexMarker522"/><span class="koboSpan" id="kobo.442.1">hidden units or identifying a suitable stopping point for the training algorithm. </span><span class="koboSpan" id="kobo.442.2">Adjustments based on the validation set help ensure the network generalizes well to </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">new data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.444.1">Test set (usually 15% of the available cases)</span></strong><span class="koboSpan" id="kobo.445.1">: The test set is a separate and independent collection of examples used solely to evaluate the performance </span><a id="_idIndexMarker523"/><span class="koboSpan" id="kobo.446.1">of a fully trained neural network. </span><span class="koboSpan" id="kobo.446.2">After the training and validation stages, the final model is assessed using the test set to estimate its error rate and validate its generalization capabilities. </span><span class="koboSpan" id="kobo.446.3">It is crucial to refrain from making any further adjustments to the model based on the test set evaluation to avoid bias or overfitting. </span><span class="koboSpan" id="kobo.446.4">The test set provides an unbiased measure of how well the neural network is likely to perform on new, </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">real-world data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.448.1">By dividing </span><a id="_idIndexMarker524"/><span class="koboSpan" id="kobo.449.1">the data into these three distinct sets, the neural network design workflow ensures that the model is trained, validated, and tested in a controlled and reliable manner, allowing for accurate assessments of its performance and </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">generalization capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.451.1">With a clear understanding of this process, we are now ready to proceed with our work on analyzing a practical example of an ANN implementation </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">in MATLAB.</span></span></p>
<h1 id="_idParaDest-113"><a id="_idTextAnchor116"/><span class="koboSpan" id="kobo.453.1">Understanding data fitting with ANNs</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.454.1">Data fitting</span></strong><span class="koboSpan" id="kobo.455.1"> is the </span><a id="_idIndexMarker525"/><span class="koboSpan" id="kobo.456.1">process of constructing a curve or mathematical function that best matches a given set of collected data points. </span><span class="koboSpan" id="kobo.456.2">This curve fitting can involve either </span><a id="_idIndexMarker526"/><span class="koboSpan" id="kobo.457.1">interpolations, where exact data points are fitted, or smoothing, where a smooth function approximates the data. </span><span class="koboSpan" id="kobo.457.2">In the context of regression </span><a id="_idIndexMarker527"/><span class="koboSpan" id="kobo.458.1">analysis, curve fitting is closely related to statistical inference, considering uncertainties arising from random errors in </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">observed data.</span></span></p>
<p><span class="koboSpan" id="kobo.460.1">The approximate curves obtained through data fitting have multiple applications. </span><span class="koboSpan" id="kobo.460.2">They can be used to visualize and display the data, predict function values in regions with no available data, and summarize the relationships between multiple variables. </span><span class="koboSpan" id="kobo.460.3">This process is valuable for understanding and interpreting complex datasets, making predictions, and gaining insights from the </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">collected information.</span></span></p>
<p><span class="koboSpan" id="kobo.462.1">Predicting the trend of a particular distribution using mathematical formulas can be challenging, and these formulas may not always accurately represent all the data or cover the </span><a id="_idIndexMarker528"/><span class="koboSpan" id="kobo.463.1">entire range of existence. </span><span class="koboSpan" id="kobo.463.2">To address such cases, machine learning algorithms come to the rescue. </span><span class="koboSpan" id="kobo.463.3">These algorithms can build models without relying on complex </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">mathematical formulas.</span></span></p>
<p><span class="koboSpan" id="kobo.465.1">ANNs </span><a id="_idIndexMarker529"/><span class="koboSpan" id="kobo.466.1">are well suited for data-fitting tasks and trend prediction. </span><span class="koboSpan" id="kobo.466.2">They can adapt and learn from a given set of inputs and </span><a id="_idIndexMarker530"/><span class="koboSpan" id="kobo.467.1">associated target outputs. </span><strong class="bold"><span class="koboSpan" id="kobo.468.1">Function fitting</span></strong><span class="koboSpan" id="kobo.469.1"> is the process of training a neural network with such input-output pairs, enabling it to form a generalization of the underlying input-output relationship. </span><span class="koboSpan" id="kobo.469.2">Once trained, the neural network can generate outputs for inputs it has not encountered </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">during training.</span></span></p>
<p><span class="koboSpan" id="kobo.471.1">The flexibility of neural networks allows them to capture complex patterns and relationships in the data, making them powerful tools for data-fitting and prediction tasks. </span><span class="koboSpan" id="kobo.471.2">By utilizing machine learning algorithms such as neural networks, we can overcome the limitations of traditional mathematical formulas and achieve accurate predictions across various datasets </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">and scenarios.</span></span></p>
<p><span class="koboSpan" id="kobo.473.1">The performance of algorithms based on machine learning strongly depends on the quality of the data being worked on. </span><span class="koboSpan" id="kobo.473.2">The data collection process typically occurs outside the MATLAB environment, which means that you need to have a properly collected data file ready to initiate an analysis in MATLAB. </span><span class="koboSpan" id="kobo.473.3">However, if you don’t have access to the data yet and are here to learn, there’s no need to worry because MATLAB has </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">a solution.</span></span></p>
<p><span class="koboSpan" id="kobo.475.1">The Neural Network Toolbox software provides several sample datasets that you can use to experiment with the functionality of the toolbox. </span><span class="koboSpan" id="kobo.475.2">These sample datasets are readily available and can serve as a starting point for your analysis. </span><span class="koboSpan" id="kobo.475.3">To explore the available datasets, you can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">following command:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.477.1">
help nndatasets</span></pre> <p><span class="koboSpan" id="kobo.478.1">A list of datasets sorted by application type will be returned. </span><span class="koboSpan" id="kobo.478.2">With these sample datasets at your disposal, you can begin your learning journey and gain hands-on experience with the Neural Network Toolbox in MATLAB. </span><span class="koboSpan" id="kobo.478.3">Now we will work on an example of data fitting </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">in MATLAB:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.480.1">Let’s focus </span><a id="_idIndexMarker531"/><span class="koboSpan" id="kobo.481.1">our attention on the dataset for data fitting, specifically, </span><strong class="source-inline"><span class="koboSpan" id="kobo.482.1">abalone_dataset</span></strong><span class="koboSpan" id="kobo.483.1">, which contains the </span><em class="italic"><span class="koboSpan" id="kobo.484.1">abalone shell rings dataset</span></em><span class="koboSpan" id="kobo.485.1">. </span><span class="koboSpan" id="kobo.485.2">To load the dataset into the MATLAB workspace, let’s use the </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.487.1">
load abalone_dataset</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.488.1">Executing this command will load the data from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.489.1">abaloneInputs</span></strong><span class="koboSpan" id="kobo.490.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.491.1">abaloneTargets</span></strong><span class="koboSpan" id="kobo.492.1"> datasets into the MATLAB workspace. </span><span class="koboSpan" id="kobo.492.2">If you prefer to load the input and target arrays with different variable names, you can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">following command:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.494.1">[Input,Target] = abalone_dataset;</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.495.1">Executing this command will load the data into the arrays named </span><strong class="source-inline"><span class="koboSpan" id="kobo.496.1">Input</span></strong><span class="koboSpan" id="kobo.497.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.498.1">Target</span></strong><span class="koboSpan" id="kobo.499.1">. </span><span class="koboSpan" id="kobo.499.2">The aim of the model is to estimate the age of an abalone, utilizing physical measurements to achieve </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">this prediction.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.501.1">To obtain a description of a specific dataset, you can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">following command:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.503.1">help abalone_dataset</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.504.1">A comprehensive description of the dataset is provided, including details such as the number of attributes, the total number of items, and a list of variables. </span><span class="koboSpan" id="kobo.504.2">Additionally, the description offers valuable insights into potential use cases for </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">the dataset.</span></span></p></li> <li><span class="koboSpan" id="kobo.506.1">Now that we have the data, we have to choose the training algorithm and set the architecture network. </span><span class="koboSpan" id="kobo.506.2">In MATLAB different functions are available for training. </span><span class="koboSpan" id="kobo.506.3">To get a list of training algorithms available, we can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.508.1">
help nntrain</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.509.1">For </span><a id="_idIndexMarker532"/><span class="koboSpan" id="kobo.510.1">our training, we will use the </span><strong class="bold"><span class="koboSpan" id="kobo.511.1">Levenberg-Marquardt</span></strong><span class="koboSpan" id="kobo.512.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.513.1">LM</span></strong><span class="koboSpan" id="kobo.514.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.515.1">backpropagation</span></strong><span class="koboSpan" id="kobo.516.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">as follows:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.518.1">TFunc = 'trainlm';</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.519.1">LM backpropagation is an optimization algorithm used in training ANNs. </span><span class="koboSpan" id="kobo.519.2">It is an extension of the standard backpropagation algorithm that improves convergence and robustness, especially for nonlinear and ill-conditioned problems. </span><span class="koboSpan" id="kobo.519.3">In standard backpropagation, the algorithm adjusts the weights of the neural network using the gradient of the error with respect to the weights. </span><span class="koboSpan" id="kobo.519.4">However, in some cases, this process can be slow, and the algorithm might get stuck in local minima. </span><span class="koboSpan" id="kobo.519.5">LM backpropagation addresses this issue by incorporating the LM optimization method, which is commonly used in nonlinear least squares fitting problems. </span><span class="koboSpan" id="kobo.519.6">The LM algorithm combines the ideas of both gradient descent and </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">Gauss-Newton methods.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.521.1">Here’s </span><a id="_idIndexMarker533"/><span class="koboSpan" id="kobo.522.1">a basic outline of how LM </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">backpropagation works:</span></span></p><ol><li class="upper-roman"><span class="koboSpan" id="kobo.524.1">Calculate the gradient of the error function with respect to the weights using the standard </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">backpropagation algorithm</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.526.1">Calculate the Hessian matrix, which represents the curvature of the error surface with respect to </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">the weights</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.528.1">Adjust the weights using a combination of gradient descent and the LM </span><span class="No-Break"><span class="koboSpan" id="kobo.529.1">optimization method</span></span></li></ol><p class="list-inset"><span class="koboSpan" id="kobo.530.1">The LM backpropagation algorithm adapts the learning rate during training. </span><span class="koboSpan" id="kobo.530.2">When the error surface is steep, it behaves more like gradient descent, which helps in avoiding overshooting. </span><span class="koboSpan" id="kobo.530.3">When the error surface is relatively flat, it behaves more like the Gauss-Newton method, which speeds up convergence. </span><span class="koboSpan" id="kobo.530.4">This combination of techniques makes LM backpropagation an efficient and effective algorithm for training neural networks, particularly in cases where standard backpropagation might face convergence issues or slow learning rates. </span><span class="koboSpan" id="kobo.530.5">It is commonly used in various applications, including pattern recognition, function approximation, and nonlinear </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">regression tasks.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.532.1">After that, we have to set the number of nodes in the </span><span class="No-Break"><span class="koboSpan" id="kobo.533.1">hidden layer:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.534.1">HLNodesNum =10;</span></pre></li> <li><span class="koboSpan" id="kobo.535.1">Finally, we have to create our ANN using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.536.1">fitnet()</span></strong><span class="koboSpan" id="kobo.537.1"> function </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.539.1">
AbaFitNet = fitnet(HLNodesNum, TFunc);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.540.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.541.1">fitnet()</span></strong><span class="koboSpan" id="kobo.542.1"> function is a built-in MATLAB function used to create and train a </span><a id="_idIndexMarker534"/><span class="koboSpan" id="kobo.543.1">feedforward neural network with a single hidden layer for function fitting, pattern recognition, and regression tasks. </span><span class="koboSpan" id="kobo.543.2">The following parameters </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">are passed:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.545.1">LnodesNum</span></strong><span class="koboSpan" id="kobo.546.1">: This is a vector that specifies the number of neurons in each </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">hidden layer.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.548.1">Tfunc</span></strong><span class="koboSpan" id="kobo.549.1">: This parameter specifies the training function to be used for training the neural network. </span><span class="koboSpan" id="kobo.549.2">It represents the optimization algorithm that updates the network weights during the training process. </span><span class="koboSpan" id="kobo.549.3">Some commonly used training functions include </span><strong class="source-inline"><span class="koboSpan" id="kobo.550.1">trainlm</span></strong><span class="koboSpan" id="kobo.551.1"> (LM), </span><strong class="source-inline"><span class="koboSpan" id="kobo.552.1">trainbfg</span></strong><span class="koboSpan" id="kobo.553.1"> (BFGS Quasi-Newton), and </span><strong class="source-inline"><span class="koboSpan" id="kobo.554.1">traingd</span></strong><span class="koboSpan" id="kobo.555.1"> (</span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">gradient descent).</span></span></li></ul></li> <li><span class="koboSpan" id="kobo.557.1">Training the algorithm is necessary to preprocess the data, as we showed in the </span><em class="italic"><span class="koboSpan" id="kobo.558.1">Exploring data wrangling</span></em><span class="koboSpan" id="kobo.559.1"> section, in the </span><em class="italic"><span class="koboSpan" id="kobo.560.1">Exploring MATLAB for Machine Learning</span></em><span class="koboSpan" id="kobo.561.1"> chapter. </span><span class="koboSpan" id="kobo.561.2">In this case, we can use the neural network processing functions, available in the Neural Network Toolbox. </span><span class="koboSpan" id="kobo.561.3">To print all the general data preprocessing functions available, we can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.563.1">
help nnprocess</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.564.1">The following functions </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">are listed:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.566.1">removerows</span></strong><span class="koboSpan" id="kobo.567.1">: Eliminate rows from the matrix based on </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">specified indices</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.569.1">mapminmax</span></strong><span class="koboSpan" id="kobo.570.1">: Map the minimum and maximum values of each matrix row to the range [-</span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">1, 1]</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.572.1">processpca</span></strong><span class="koboSpan" id="kobo.573.1">: Perform principal component analysis on the </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">matrix rows</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.575.1">mapstd</span></strong><span class="koboSpan" id="kobo.576.1">: Map the row means and deviations of the matrix to </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">standard values</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.578.1">fixunknowns</span></strong><span class="koboSpan" id="kobo.579.1">: Handle matrix rows with unknown values using a </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">specific procedure</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.581.1">The </span><a id="_idIndexMarker535"/><span class="koboSpan" id="kobo.582.1">first two represent the default functions applied to feedforward multilayer networks and are therefore the ones we will apply to </span><span class="No-Break"><span class="koboSpan" id="kobo.583.1">our case:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.584.1">AbaFitNet.input.processFcns =
                   {'removeconstantrows','mapminmax'};
AbaFitNet.output.processFcns =
                   {'removeconstantrows','mapminmax'};</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.585.1">As already mentioned, these are the default processing functions applied to both input and output. </span><span class="koboSpan" id="kobo.585.2">The first removes the constant records, as they do not bring any content for data adaptation, and the second instead maps the elements of a matrix or vector from their original range to a specified </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">target range.</span></span></p></li> <li><span class="koboSpan" id="kobo.587.1">Then the preprocessing of data is necessary to operate </span><strong class="bold"><span class="koboSpan" id="kobo.588.1">data splitting</span></strong><span class="koboSpan" id="kobo.589.1">. </span><span class="koboSpan" id="kobo.589.2">This is a common technique used in machine learning and data analysis to divide a dataset into separate subsets for different purposes. </span><span class="koboSpan" id="kobo.589.3">The main reason for data splitting is to have distinct portions of data for training, validation, and testing, which allows for the evaluation and improvement of machine learning models. </span><span class="koboSpan" id="kobo.589.4">There are several techniques to split the data; in this case, the dataset is split into three parts: a training set, a validation set, and a testing set. </span><span class="koboSpan" id="kobo.589.5">The training set is used for model training, the validation set is used to tune hyperparameters and optimize the model, and the testing set is used for </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">final evaluation:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.591.1">
AbaFitNet.divideFcn = 'dividerand';</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.592.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.593.1">dividerand</span></strong><span class="koboSpan" id="kobo.594.1"> property randomly assigns each sample to one of the three subsets based on the specified ratios. </span><span class="koboSpan" id="kobo.594.2">It returns three index vectors: </span><strong class="source-inline"><span class="koboSpan" id="kobo.595.1">trainInd</span></strong><span class="koboSpan" id="kobo.596.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.597.1">valInd</span></strong><span class="koboSpan" id="kobo.598.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.599.1">testInd</span></strong><span class="koboSpan" id="kobo.600.1">, which indicate the samples’ positions in the original dataset for the training, validation, and testing sets, respectively. </span><span class="koboSpan" id="kobo.600.2">This attribute specifies the data splitting method employed during the training of the </span><a id="_idIndexMarker536"/><span class="koboSpan" id="kobo.601.1">network with a supervised algorithm such as backpropagation. </span><span class="koboSpan" id="kobo.601.2">You can assign the name of the desired data division function to this attribute. </span><span class="koboSpan" id="kobo.601.3">When this attribute is modified, the network’s adaptation parameters are automatically updated to include the parameters and default values associated with the newly </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">selected function:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.603.1">AbaFitNet.divideMode = 'sample';</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.604.1">This attribute determines the dimensions of the target data to be partitioned when invoking the data division function. </span><span class="koboSpan" id="kobo.604.2">The default value is </span><strong class="source-inline"><span class="koboSpan" id="kobo.605.1">sample</span></strong><span class="koboSpan" id="kobo.606.1"> for static networks and </span><strong class="source-inline"><span class="koboSpan" id="kobo.607.1">time</span></strong><span class="koboSpan" id="kobo.608.1"> for dynamic networks. </span><span class="koboSpan" id="kobo.608.2">Alternatively, it can be set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.609.1">sampletime</span></strong><span class="koboSpan" id="kobo.610.1"> to divide targets by both sample and timestep, </span><strong class="source-inline"><span class="koboSpan" id="kobo.611.1">all</span></strong><span class="koboSpan" id="kobo.612.1"> to partition targets at each scalar value, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.613.1">none</span></strong><span class="koboSpan" id="kobo.614.1"> to keep the data undivided (meaning all data is used for training and none for validation </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">or testing).</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.616.1">Let’s now set the percentage of data to use for the </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">different phases:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.618.1">AbaFitNet.divideParam.trainRatio = 70/100;
AbaFitNet.divideParam.valRatio = 15/100;
AbaFitNet.divideParam.testRatio = 15/100;</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.619.1">This attribute contains the parameters and corresponding values of the currently employed data division function. </span><span class="koboSpan" id="kobo.619.2">The choice of data splitting method depends on the specific problem, dataset size, and available resources. </span><span class="koboSpan" id="kobo.619.3">To find out the splitting functions available, we can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">following command:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.621.1">help nndivision</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.622.1">It is essential to perform data splitting carefully to avoid issues such as data leakage and ensure reliable </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">model evaluation.</span></span></p></li> <li><span class="koboSpan" id="kobo.624.1">Before training the ANN, it is necessary to choose something. </span><span class="koboSpan" id="kobo.624.2">To start with, we must choose the evaluation metrics to check the performance of the model that we are setting. </span><span class="koboSpan" id="kobo.624.3">Evaluation metrics are quantitative measures used to assess the performance </span><a id="_idIndexMarker537"/><span class="koboSpan" id="kobo.625.1">and effectiveness of a model, algorithm, system, or process. </span><span class="koboSpan" id="kobo.625.2">In various fields, such as machine learning, data science, and information retrieval, evaluation metrics are essential for comparing different methods, tuning parameters, and understanding the strengths and weaknesses of a particular solution. </span><span class="koboSpan" id="kobo.625.3">The choice of evaluation metric depends on the specific problem being addressed and the goals of the analysis. </span><span class="koboSpan" id="kobo.625.4">The selection of appropriate metrics depends on the nature of the problem and the objectives of the analysis. </span><span class="koboSpan" id="kobo.625.5">It’s essential to choose evaluation metrics that align with the specific goals and requirements of the task at hand. </span><span class="koboSpan" id="kobo.625.6">We can use a command such </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">as this:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.627.1">
AbaFitNet.performFcn = 'mse';</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.628.1">In MATLAB, there are some commonly used evaluation metrics available. </span><span class="koboSpan" id="kobo.628.2">In this case, we have </span><a id="_idIndexMarker538"/><span class="koboSpan" id="kobo.629.1">used the </span><strong class="bold"><span class="koboSpan" id="kobo.630.1">mean squared error</span></strong><span class="koboSpan" id="kobo.631.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.632.1">MSE</span></strong><span class="koboSpan" id="kobo.633.1">), which measures the average squared difference between predicted values and actual values. </span><span class="koboSpan" id="kobo.633.2">In other words, it measures how well a model’s predictions align with the actual data points. </span><span class="koboSpan" id="kobo.633.3">Larger errors result in a higher MSE, while smaller errors result in a lower MSE. </span><span class="koboSpan" id="kobo.633.4">The MSE quantifies the deviation between a regression line and a set of data points. </span><span class="koboSpan" id="kobo.633.5">It serves as a risk metric that aligns with the expected value of the squared error loss. </span><span class="koboSpan" id="kobo.633.6">The MSE is calculated by averaging the squared differences between the observed and predicted values over the </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">entire dataset.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.635.1">To obtain a comprehensive list of performance functions, please enter the following command in the </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">command line:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.637.1">help nnperformance</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.638.1">A list of the evaluation metrics with a short summary will </span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">be printed.</span></span></p></li> <li><span class="koboSpan" id="kobo.640.1">Finally, we must choose the plot functions to get a visual representation of the results. </span><span class="koboSpan" id="kobo.640.2">To do that, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.641.1">plotFcns()</span></strong><span class="koboSpan" id="kobo.642.1"> function, which contains a one-dimensional cell array of strings that defines the plot functions associated with a </span><a id="_idIndexMarker539"/><span class="koboSpan" id="kobo.643.1">network. </span><span class="koboSpan" id="kobo.643.2">The neural network training window, accessible through the </span><strong class="source-inline"><span class="koboSpan" id="kobo.644.1">train()</span></strong><span class="koboSpan" id="kobo.645.1"> function, displays a button for each plot function. </span><span class="koboSpan" id="kobo.645.2">Simply click on the respective button during or after the training process to open the </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">desired plot:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.647.1">
AbaFitNet.plotFcns = {'plottrainstate','plotperform',
                   'ploterrhist',   'plotregression'};</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.648.1">Four plots </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">were selected:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.650.1">plottrainstate</span></strong><span class="koboSpan" id="kobo.651.1">: Reports the training </span><span class="No-Break"><span class="koboSpan" id="kobo.652.1">state values</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.653.1">plotperform</span></strong><span class="koboSpan" id="kobo.654.1">: Reports the </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">network performance</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.656.1">ploterrhist</span></strong><span class="koboSpan" id="kobo.657.1">: Reports the </span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">error histogram</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.659.1">plotregression</span></strong><span class="koboSpan" id="kobo.660.1">: Reports the </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">linear regression</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.662.1">To obtain a comprehensive list of plot functions, please enter the following command in the </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">command line:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.664.1">help nnplot</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.665.1">A list of the plots available for an ANN with a short summary will </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">be returned.</span></span></p></li> <li><span class="koboSpan" id="kobo.667.1">Now we can train the ANN already set; for that, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.668.1">train()</span></strong><span class="koboSpan" id="kobo.669.1"> function </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.671.1">
[AbaFitNet,Trs] = train(AbaFitNet,Input,Target);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.672.1">This function trains a shallow neural network, and three arguments </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">are passed:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.674.1">AbaFitNet</span></strong><span class="koboSpan" id="kobo.675.1">: The neural network </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">model object.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.677.1">Input</span></strong><span class="koboSpan" id="kobo.678.1">: The input data used for training the neural network. </span><span class="koboSpan" id="kobo.678.2">It should be a matrix where each row represents a single input pattern. </span><span class="koboSpan" id="kobo.678.3">The abalone dataset is a collection of features describing the physical dimensions of an abalone. </span><span class="koboSpan" id="kobo.678.4">This dataset consists of eight features: sex, length, diameter, height, whole weight, shucked weight, viscera weight, and </span><span class="No-Break"><span class="koboSpan" id="kobo.679.1">shell weight.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.680.1">Target</span></strong><span class="koboSpan" id="kobo.681.1">: The target data corresponding to the input patterns. </span><span class="koboSpan" id="kobo.681.2">It should be a matrix with the same number of rows as the input matrix, where each row contains the corresponding target values. </span><span class="koboSpan" id="kobo.681.3">The target is the age of an abalone, measured in the form of the number </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">of rings.</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.683.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.684.1">train()</span></strong><span class="koboSpan" id="kobo.685.1"> function performs the training process using the specified input data and </span><a id="_idIndexMarker540"/><span class="koboSpan" id="kobo.686.1">target data. </span><span class="koboSpan" id="kobo.686.2">The type of training algorithm used depends on the specific neural network model and its settings. </span><span class="koboSpan" id="kobo.686.3">After training, the object returned will contain the trained neural network, which can be used for making predictions on new data using the tasks related to the </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">trained network.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.688.1">During the training phase, a new window will be opened (as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.689.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.690.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">).</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer047">
<span class="koboSpan" id="kobo.692.1"><img alt="Figure 5.3 – Training results window" src="image/B21156_05_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.693.1">Figure 5.3 – Training results window</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.694.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.695.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.696.1">.3</span></em><span class="koboSpan" id="kobo.697.1">, we can check the training progress, which shows information such as </span><strong class="bold"><span class="koboSpan" id="kobo.698.1">Epoch</span></strong><span class="koboSpan" id="kobo.699.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.700.1">Elapsed Time</span></strong><span class="koboSpan" id="kobo.701.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.702.1">Performance</span></strong><span class="koboSpan" id="kobo.703.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.704.1">Gradient</span></strong><span class="koboSpan" id="kobo.705.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.706.1">Mu</span></strong><span class="koboSpan" id="kobo.707.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.708.1">Validation</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.709.1">Checks</span></strong><span class="koboSpan" id="kobo.710.1">. </span><span class="koboSpan" id="kobo.710.2">Furthermore, the properties set in the previous steps are summarized. </span><span class="koboSpan" id="kobo.710.3">Finally, several buttons related to the plot set </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">are available.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.712.1">By clicking </span><a id="_idIndexMarker541"/><span class="koboSpan" id="kobo.713.1">on the buttons at the bottom of this window, we can draw the </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">specific plot.</span></span></p>
<ol>
<li value="9"><span class="koboSpan" id="kobo.715.1">After the ANN is trained, it is time to test the network. </span><span class="koboSpan" id="kobo.715.2">Testing a neural network involves evaluating its performance on a separate test dataset that the network has not seen during the training phase. </span><span class="koboSpan" id="kobo.715.3">This step helps assess the generalization ability of the trained network and ensures it can make accurate predictions on unseen data. </span><span class="koboSpan" id="kobo.715.4">To test the ANN that we trained, we can apply the network to the unseen data, </span><span class="No-Break"><span class="koboSpan" id="kobo.716.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.717.1">
SimTarget = AbaFitNet(Input);
Diff = gsubtract(Target, SimTarget);
Performance = perform(AbaFitNet, Target, SimTarget)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.718.1">In this piece of code, we start evaluating the network on all the data, obtaining a first </span><a id="_idIndexMarker542"/><span class="koboSpan" id="kobo.719.1">value of performance to compare with </span><span class="No-Break"><span class="koboSpan" id="kobo.720.1">other ones.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.721.1">The following performance </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">is returned:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.723.1">Performance =
    4.1539</span></pre></li> <li><span class="koboSpan" id="kobo.724.1">Now, we have to recalculate training, validation, and testing performance </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.726.1">
trainTargets =Target.* Trs.trainMask{1};
valTargets =Target.* Trs.valMask{1};
testTargets =TargetB.* Trs.testMask{1};
trainPerformance = perform(AbaFitNet,trainTargets, SimTarget)
valPerformance = perform(AbaFitNet,valTargets, SimTarget)
testPerformance = perform(AbaFitNet,testTargets, SimTarget)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.727.1">The following performance values </span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">are returned:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.729.1">trainPerformance =     3.9470
valPerformance =     5.1255
testPerformance =     4.1468</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.730.1">To show the network architecture, we can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">following command:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.732.1">view(AbaFitNet)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.733.1">Finally, to show the plots, we can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">following commands:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.735.1">figure,plotperform(Trs)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.736.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.737.1">plotperform()</span></strong><span class="koboSpan" id="kobo.738.1"> function shows the network performance by using the training record returned by the specific </span><span class="No-Break"><span class="koboSpan" id="kobo.739.1">training function.</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.740.1">figure,plottrainstate(Trs)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.741.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.742.1">plottrainstate()</span></strong><span class="koboSpan" id="kobo.743.1"> function plots the training states returned by the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.744.1">train()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.745.1"> function.</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.746.1">figure,ploterrhist(Diff)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.747.1">A good </span><a id="_idIndexMarker543"/><span class="koboSpan" id="kobo.748.1">way to evaluate the error made in data simulation </span><a id="_idIndexMarker544"/><span class="koboSpan" id="kobo.749.1">is to represent it using a histogram. </span><span class="koboSpan" id="kobo.749.2">To do this, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.750.1">ploterrhist()</span></strong><span class="koboSpan" id="kobo.751.1"> function, which plots the error histogram. </span><span class="koboSpan" id="kobo.751.2">An error histogram is a graphical representation that illustrates the distribution of errors in a dataset. </span><span class="koboSpan" id="kobo.751.3">Errors, in this context, typically refer to the differences between predicted values and actual (observed) values. </span><span class="koboSpan" id="kobo.751.4">In practice, examining an error histogram can provide valuable insights into the performance of a predictive model. </span><span class="koboSpan" id="kobo.751.5">It can help identify patterns in the errors, assess the model’s bias, and detect the presence of outliers or other issues that may need attention during model development </span><span class="No-Break"><span class="koboSpan" id="kobo.752.1">and evaluation.</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.753.1">figure,plotregression(Target, SimTarget)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.754.1">Finally, to evaluate the network’s ability to estimate the model target, we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.755.1">plotregression()</span></strong><span class="koboSpan" id="kobo.756.1"> function. </span><span class="koboSpan" id="kobo.756.2">This function plots the linear regression of targets relative </span><span class="No-Break"><span class="koboSpan" id="kobo.757.1">to outputs.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.758.1">The following plots will </span><span class="No-Break"><span class="koboSpan" id="kobo.759.1">be printed:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer048">
<span class="koboSpan" id="kobo.760.1"><img alt="Figure 5.4 – ANN plots" src="image/B21156_05_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.761.1">Figure 5.4 – ANN plots</span></p>
<p><span class="koboSpan" id="kobo.762.1">In this way, we will be able to evaluate the trend of the training parameters throughout the </span><a id="_idIndexMarker545"/><span class="koboSpan" id="kobo.763.1">process period by period. </span><span class="koboSpan" id="kobo.763.2">Furthermore, we will have a statistic </span><a id="_idIndexMarker546"/><span class="koboSpan" id="kobo.764.1">of the distribution of errors and, finally, an indication of the position of the forecasts compared to the real values through the </span><span class="No-Break"><span class="koboSpan" id="kobo.765.1">regression graph.</span></span></p>
<p><span class="koboSpan" id="kobo.766.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.767.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.768.1">.4</span></em><span class="koboSpan" id="kobo.769.1">, we can analyze the regression line of the model. </span><span class="koboSpan" id="kobo.769.2">The regression line is a key element in statistical data analysis, providing valuable information about a model’s ability to predict the data. </span><span class="koboSpan" id="kobo.769.3">This straight line represents the mathematical relationship between the independent and dependent variables, trying to minimize the difference between the observed values and those predicted by the model. </span><span class="koboSpan" id="kobo.769.4">The slope of the line reflects the average change in the dependent variable for each unit change in the independent variable. </span><span class="koboSpan" id="kobo.769.5">If the slope is close to </span><strong class="source-inline"><span class="koboSpan" id="kobo.770.1">1</span></strong><span class="koboSpan" id="kobo.771.1">, the model accurately predicts the data. </span><span class="koboSpan" id="kobo.771.2">Additionally, the intercept indicates the value of the dependent variable when the independent variable </span><span class="No-Break"><span class="koboSpan" id="kobo.772.1">is 0.</span></span></p>
<p><span class="koboSpan" id="kobo.773.1">The goodness of fit of the model is measured through the correlation coefficient </span><em class="italic"><span class="koboSpan" id="kobo.774.1">R</span></em><span class="koboSpan" id="kobo.775.1">, which measures the strength of the association between two variables. </span><span class="koboSpan" id="kobo.775.2">A high </span><em class="italic"><span class="koboSpan" id="kobo.776.1">R</span></em><span class="koboSpan" id="kobo.777.1"> indicates </span><a id="_idIndexMarker547"/><span class="koboSpan" id="kobo.778.1">good predictive ability. </span><span class="koboSpan" id="kobo.778.2">It is </span><a id="_idIndexMarker548"/><span class="koboSpan" id="kobo.779.1">important to note that the regression line may have limitations in its predictive ability if the data has complex or nonlinear patterns. </span><span class="koboSpan" id="kobo.779.2">In such cases, more advanced models may </span><span class="No-Break"><span class="koboSpan" id="kobo.780.1">be necessary.</span></span></p>
<p><span class="koboSpan" id="kobo.781.1">After having analyzed an example of data fitting in detail, we will now see how to tackle a </span><span class="No-Break"><span class="koboSpan" id="kobo.782.1">classification problem.</span></span></p>
<h1 id="_idParaDest-114"><a id="_idTextAnchor117"/><span class="koboSpan" id="kobo.783.1">Discovering pattern recognition using ANNs</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.784.1">Pattern recognition</span></strong><span class="koboSpan" id="kobo.785.1"> is a branch </span><a id="_idIndexMarker549"/><span class="koboSpan" id="kobo.786.1">of machine learning and AI that focuses on the identification of patterns or regularities in data. </span><span class="koboSpan" id="kobo.786.2">It involves the automatic </span><a id="_idIndexMarker550"/><span class="koboSpan" id="kobo.787.1">discovery and extraction of meaningful information from datasets, with the aim of categorizing or classifying data into different classes or groups. </span><span class="koboSpan" id="kobo.787.2">Overall, pattern recognition plays a crucial role in automating the process of identifying patterns and making decisions based on data, making </span><a id="_idIndexMarker551"/><span class="koboSpan" id="kobo.788.1">it a fundamental component of many modern </span><span class="No-Break"><span class="koboSpan" id="kobo.789.1">AI systems.</span></span></p>
<p><span class="koboSpan" id="kobo.790.1">Pattern recognition offers numerous benefits in automating decision-making and handling complex data. </span><span class="koboSpan" id="kobo.790.2">However, it also poses challenges related to data quality, interpretability, and computational requirements. </span><span class="koboSpan" id="kobo.790.3">To leverage its advantages effectively, practitioners need to carefully design and train models while being aware of potential limitations and biases in the data </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">and algorithms.</span></span></p>
<p><span class="koboSpan" id="kobo.792.1">The classical approach to pattern recognition uses data collected by sensors as input to a classification system. </span><span class="koboSpan" id="kobo.792.2">This data generally represents typical measurements such as kinematic and kinetic data from a motion analysis system. </span><span class="koboSpan" id="kobo.792.3">The measurements are usually subjected to a preprocessing phase. </span><span class="koboSpan" id="kobo.792.4">This is done to improve the signal properties. </span><span class="koboSpan" id="kobo.792.5">Subsequent feature extraction provides a feature vector for subsequent classification. </span><span class="koboSpan" id="kobo.792.6">This vector describes the input measurements in feature space. </span><span class="koboSpan" id="kobo.792.7">In supervised classification, labeled feature vectors are presented to a classifier for training. </span><span class="koboSpan" id="kobo.792.8">The vectors used to train the classifier form the training set. </span><span class="koboSpan" id="kobo.792.9">These labels assign a feature vector to one of several possible classes. </span><span class="koboSpan" id="kobo.792.10">In the recognition phase, the trained classifier uses this decision rule and automatically assigns a feature vector to a class. </span><span class="koboSpan" id="kobo.792.11">Different classifiers can be used, using different </span><span class="No-Break"><span class="koboSpan" id="kobo.793.1">learning strategies.</span></span></p>
<p><span class="koboSpan" id="kobo.794.1">Let’s now analyze a practical example of pattern recognition using MATLAB. </span><span class="koboSpan" id="kobo.794.2">In this section, our objective is to develop a classification model that can categorize thyroid disease based on various patient data. </span><span class="koboSpan" id="kobo.794.3">The steps involved are </span><span class="No-Break"><span class="koboSpan" id="kobo.795.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.796.1">To initiate the process, we acquire the data for analysis. </span><span class="koboSpan" id="kobo.796.2">For this study, we will utilize an existing dataset that comes pre-packaged with MATLAB. </span><span class="koboSpan" id="kobo.796.3">As mentioned earlier, MATLAB offers several readily available databases that can be easily imported </span><a id="_idIndexMarker552"/><span class="koboSpan" id="kobo.797.1">into the workspace using </span><a id="_idIndexMarker553"/><span class="koboSpan" id="kobo.798.1">the </span><strong class="source-inline"><span class="koboSpan" id="kobo.799.1">load</span></strong><span class="koboSpan" id="kobo.800.1"> command, followed by the specific database name. </span><span class="koboSpan" id="kobo.800.2">In this instance, we will work with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.801.1">thyroid_dataset</span></strong><span class="koboSpan" id="kobo.802.1"> as our </span><span class="No-Break"><span class="koboSpan" id="kobo.803.1">chosen dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.804.1">
[Input,Target] = thyroid_dataset;</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.805.1">In the MATLAB workspace, we now have </span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">two variables:</span></span></p><ul><li><strong class="bold"><span class="koboSpan" id="kobo.807.1">Input</span></strong><span class="koboSpan" id="kobo.808.1">: This is a 21x7,200 matrix representing data from 7,200 patients. </span><span class="koboSpan" id="kobo.808.2">Each patient is characterized by 15 binary attributes and 6 </span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">continuous attributes.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.810.1">Target</span></strong><span class="koboSpan" id="kobo.811.1">: This is a 3x7,200 matrix containing associated class vectors for each input. </span><span class="koboSpan" id="kobo.811.2">It indicates which of the three classes each patient belongs to. </span><span class="koboSpan" id="kobo.811.3">In the </span><strong class="source-inline"><span class="koboSpan" id="kobo.812.1">Target</span></strong><span class="koboSpan" id="kobo.813.1"> variable, classes are represented by a </span><strong class="source-inline"><span class="koboSpan" id="kobo.814.1">1</span></strong><span class="koboSpan" id="kobo.815.1"> in the first, second, or third row. </span><span class="koboSpan" id="kobo.815.2">A </span><strong class="source-inline"><span class="koboSpan" id="kobo.816.1">1</span></strong><span class="koboSpan" id="kobo.817.1"> in the first row indicates the patient is classified as normal (not hyperthyroid), a </span><strong class="source-inline"><span class="koboSpan" id="kobo.818.1">1</span></strong><span class="koboSpan" id="kobo.819.1"> in the second row indicates hyperfunction (hyperthyroidism), and a </span><strong class="source-inline"><span class="koboSpan" id="kobo.820.1">1</span></strong><span class="koboSpan" id="kobo.821.1"> in the third row indicates subnormal </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">functioning (hypothyroidism).</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.823.1">The specific problem at hand is to determine whether a patient referred to the clinic is hypothyroid. </span><span class="koboSpan" id="kobo.823.2">It is important to note that due to many patients not being hyperthyroid (92%), a successful classifier must achieve a significantly higher accuracy than 92% to be </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">considered effective.</span></span></p></li> <li><span class="koboSpan" id="kobo.825.1">Now, it is time to select the appropriate training function for the neural network. </span><span class="koboSpan" id="kobo.825.2">To set a specific training algorithm for the network, you can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.826.1">trainFcn</span></strong><span class="koboSpan" id="kobo.827.1"> property and assign the name of the desired function. </span><span class="koboSpan" id="kobo.827.2">Among several </span><a id="_idIndexMarker554"/><span class="koboSpan" id="kobo.828.1">available algorithms, we will opt for the </span><strong class="bold"><span class="koboSpan" id="kobo.829.1">scaled conjugate gradient</span></strong><span class="koboSpan" id="kobo.830.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.831.1">SCG</span></strong><span class="koboSpan" id="kobo.832.1">) backpropagation method. </span><span class="koboSpan" id="kobo.832.2">This method is an optimization algorithm commonly used in training ANNs, particularly for solving the problem of weight updates during the training process. </span><span class="koboSpan" id="kobo.832.3">It is </span><a id="_idIndexMarker555"/><span class="koboSpan" id="kobo.833.1">an alternative to other optimization techniques, such as gradient descent, </span><strong class="bold"><span class="koboSpan" id="kobo.834.1">stochastic gradient descent</span></strong><span class="koboSpan" id="kobo.835.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.836.1">SGD</span></strong><span class="koboSpan" id="kobo.837.1">), and various flavors of backpropagation. </span><span class="koboSpan" id="kobo.837.2">SCG is known for its efficiency and speed in converging to a minimum of the loss function. </span><span class="koboSpan" id="kobo.837.3">The advantages of the SCG algorithm include its ability to converge quickly and efficiently in many cases, making it a good choice for training neural networks with relatively small to moderate-sized datasets. </span><span class="koboSpan" id="kobo.837.4">However, it may not always outperform other optimization methods on large datasets or in more complex network architectures. </span><span class="koboSpan" id="kobo.837.5">The choice of optimization algorithm often depends on the specific problem, dataset size, and computational </span><span class="No-Break"><span class="koboSpan" id="kobo.838.1">resources available:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.839.1">
trainFcn = 'trainscg'</span></pre></li> <li><span class="koboSpan" id="kobo.840.1">After </span><a id="_idIndexMarker556"/><span class="koboSpan" id="kobo.841.1">selecting the training algorithm, the next step is to construct the neural network. </span><span class="koboSpan" id="kobo.841.2">To achieve this, we need </span><a id="_idIndexMarker557"/><span class="koboSpan" id="kobo.842.1">to determine the number of nodes in the hidden layer. </span><span class="koboSpan" id="kobo.842.2">In this case, we have decided to build a function-fitting neural network with one hidden layer consisting of </span><span class="No-Break"><span class="koboSpan" id="kobo.843.1">10 nodes:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.844.1">
hiddenLayerSize = 10;</span></pre></li> <li><span class="koboSpan" id="kobo.845.1">We can construct a pattern recognition network using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.846.1">patternnet()</span></strong><span class="koboSpan" id="kobo.847.1"> function. </span><span class="koboSpan" id="kobo.847.2">Pattern recognition networks are feedforward networks designed for classifying inputs based on target classes. </span><span class="koboSpan" id="kobo.847.3">The target data for these networks should consist of vectors with all </span><strong class="source-inline"><span class="koboSpan" id="kobo.848.1">0</span></strong><span class="koboSpan" id="kobo.849.1"> values except for a </span><strong class="source-inline"><span class="koboSpan" id="kobo.850.1">1</span></strong><span class="koboSpan" id="kobo.851.1"> in the element corresponding to the class </span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">they represent.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.853.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.854.1">patternnet()</span></strong><span class="koboSpan" id="kobo.855.1"> function accepts the </span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">following arguments:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.857.1">hiddenSizes</span></strong><span class="koboSpan" id="kobo.858.1">: A row vector specifying one or more hidden layer sizes. </span><span class="koboSpan" id="kobo.858.2">The default is </span><strong class="source-inline"><span class="koboSpan" id="kobo.859.1">10</span></strong><span class="koboSpan" id="kobo.860.1"> if a value is </span><span class="No-Break"><span class="koboSpan" id="kobo.861.1">not provided.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.862.1">trainFcn</span></strong><span class="koboSpan" id="kobo.863.1">: The training function to be used. </span><span class="koboSpan" id="kobo.863.2">The default is </span><strong class="source-inline"><span class="koboSpan" id="kobo.864.1">trainscg</span></strong><span class="koboSpan" id="kobo.865.1"> (</span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">SCG backpropagation).</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.867.1">performFcn</span></strong><span class="koboSpan" id="kobo.868.1">: The performance function used during training. </span><span class="koboSpan" id="kobo.868.2">The default </span><span class="No-Break"><span class="koboSpan" id="kobo.869.1">is </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.870.1">crossentropy</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.871.1">.</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.872.1">The function returns a pattern recognition neural network with the specified architecture </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">and settings:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.874.1">
TyroidPatNet = patternnet(hiddenLayerSize, trainFcn);</span></pre></li> <li><span class="koboSpan" id="kobo.875.1">Once </span><a id="_idIndexMarker558"/><span class="koboSpan" id="kobo.876.1">the network has been constructed </span><a id="_idIndexMarker559"/><span class="koboSpan" id="kobo.877.1">and the data has been preprocessed, the next step is to partition the data into separate sets for training, validation, </span><span class="No-Break"><span class="koboSpan" id="kobo.878.1">and testing.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.879.1">To achieve this, we typically divide the available dataset into three </span><span class="No-Break"><span class="koboSpan" id="kobo.880.1">different subsets:</span></span></p><ul><li><strong class="bold"><span class="koboSpan" id="kobo.881.1">Training set</span></strong><span class="koboSpan" id="kobo.882.1">: This subset is used to train the neural network. </span><span class="koboSpan" id="kobo.882.2">The network learns from the input data and corresponding target outputs during the </span><span class="No-Break"><span class="koboSpan" id="kobo.883.1">training process.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.884.1">Validation set</span></strong><span class="koboSpan" id="kobo.885.1">: The validation set is used to fine-tune the network’s hyperparameters and prevent overfitting. </span><span class="koboSpan" id="kobo.885.2">It helps in optimizing the network’s performance by evaluating its performance on data it has not seen </span><span class="No-Break"><span class="koboSpan" id="kobo.886.1">during training.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.887.1">Testing set</span></strong><span class="koboSpan" id="kobo.888.1">: This set is used to evaluate the final performance of the trained network. </span><span class="koboSpan" id="kobo.888.2">It provides an unbiased estimate of the network’s generalization ability on </span><span class="No-Break"><span class="koboSpan" id="kobo.889.1">unseen data.</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.890.1">The division of data can be achieved using various techniques, such as random sampling, stratified sampling, or time-based splitting, depending on the nature of the data and the </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">specific problem:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.892.1">
TyroidPatNet.divideFcn = 'dividerand';
TyroidPatNet.divideMode = 'sample';
TyroidPatNet.divideParam.trainRatio = 70/100;
TyroidPatNet.divideParam.valRatio = 15/100;
TyroidPatNet.divideParam.testRatio = 15/100;</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.893.1">The </span><a id="_idIndexMarker560"/><span class="koboSpan" id="kobo.894.1">significance of operations and the types of functions used in the network construction and data </span><a id="_idIndexMarker561"/><span class="koboSpan" id="kobo.895.1">preprocessing steps was extensively covered previously. </span><span class="koboSpan" id="kobo.895.2">In case of any uncertainties, it is recommended that you review that paragraph for a more </span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">detailed understanding.</span></span></p></li> <li><span class="koboSpan" id="kobo.897.1">Regarding </span><a id="_idIndexMarker562"/><span class="koboSpan" id="kobo.898.1">measuring network performance, we will select the </span><strong class="bold"><span class="koboSpan" id="kobo.899.1">cross-entropy performance</span></strong><span class="koboSpan" id="kobo.900.1"> function. </span><span class="koboSpan" id="kobo.900.2">This specific performance function is well suited for classification and pattern recognition tasks. </span><span class="koboSpan" id="kobo.900.3">It quantifies the network’s performance by calculating the cross-entropy, which measures the difference between estimated and actual </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">class memberships.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.902.1">By utilizing the cross-entropy performance function, we can effectively evaluate how well the neural network performs in classifying inputs and make informed decisions about its effectiveness for the given pattern </span><span class="No-Break"><span class="koboSpan" id="kobo.903.1">recognition problem.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.904.1">By dividing the data into these separate sets, we can ensure that the neural network’s performance is not only measured on the training data but also validated on unseen data, making the evaluation more reliable and indicative of the network’s </span><span class="No-Break"><span class="koboSpan" id="kobo.905.1">true capabilities:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.906.1">
TyroidPatNet.performFcn = 'crossentropy';</span></pre></li> <li><span class="koboSpan" id="kobo.907.1">Finally, we can now set plot functions to visualize the results of </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">the simulation:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.909.1">
TyroidPatNet.plotFcns = {'plotperform','plottrainstate','ploterrhist', 'plotconfusion', 'plotroc'};</span></pre></li> <li><span class="koboSpan" id="kobo.910.1">Now, we are ready to initiate the training process for the network using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.911.1">train()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.912.1"> function:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.913.1">
[TyroidPatNet,Trs] = train(TyroidPatNet,Input,Target);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.914.1">While </span><a id="_idIndexMarker563"/><span class="koboSpan" id="kobo.915.1">training the neural network, the </span><strong class="bold"><span class="koboSpan" id="kobo.916.1">Neural Network Training</span></strong><span class="koboSpan" id="kobo.917.1"> window will be displayed. </span><span class="koboSpan" id="kobo.917.2">This window </span><a id="_idIndexMarker564"/><span class="koboSpan" id="kobo.918.1">comprises four sections, each offering valuable information throughout the </span><span class="No-Break"><span class="koboSpan" id="kobo.919.1">training process:</span></span></p><ul><li><strong class="bold"><span class="koboSpan" id="kobo.920.1">Neural network</span></strong><span class="koboSpan" id="kobo.921.1">: This section provides a summary of the neural network’s architecture </span><a id="_idIndexMarker565"/><span class="koboSpan" id="kobo.922.1">and configuration, including the number of layers, nodes in each layer, and the chosen </span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">training algorithm.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.924.1">Algorithms</span></strong><span class="koboSpan" id="kobo.925.1">: In this area, details about the training algorithm being used, such </span><a id="_idIndexMarker566"/><span class="koboSpan" id="kobo.926.1">as the specific optimization technique and convergence criteria, </span><span class="No-Break"><span class="koboSpan" id="kobo.927.1">are presented.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.928.1">Progress</span></strong><span class="koboSpan" id="kobo.929.1">: The progress section shows real-time updates on the training process, such </span><a id="_idIndexMarker567"/><span class="koboSpan" id="kobo.930.1">as the current epoch, training error, and validation performance. </span><span class="koboSpan" id="kobo.930.2">It allows monitoring of the network’s performance as it improves over </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">successive epochs.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.932.1">Plots</span></strong><span class="koboSpan" id="kobo.933.1">: This </span><a id="_idIndexMarker568"/><span class="koboSpan" id="kobo.934.1">section displays various plots, such as training and validation errors over epochs, enabling a visual assessment of the network’s learning progress and </span><span class="No-Break"><span class="koboSpan" id="kobo.935.1">potential overfitting.</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.936.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.937.1">Neural Network Training</span></strong><span class="koboSpan" id="kobo.938.1"> window (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.939.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.940.1">.5</span></em><span class="koboSpan" id="kobo.941.1">) offers a comprehensive view of the training procedure, providing insights into the network’s behavior and performance at different stages. </span><span class="koboSpan" id="kobo.941.2">These insights help fine-tune the network and make informed decisions to optimize its performance for the pattern recognition task </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">at hand.</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer049">
<span class="koboSpan" id="kobo.943.1"><img alt="Figure 5.5 – Neural Network Training window for pattern recognition" src="image/B21156_05_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.944.1">Figure 5.5 – Neural Network Training window for pattern recognition</span></p>
<ol>
<li value="9"><span class="koboSpan" id="kobo.945.1">Once </span><a id="_idIndexMarker569"/><span class="koboSpan" id="kobo.946.1">the network training is complete, we can employ the trained model </span><a id="_idIndexMarker570"/><span class="koboSpan" id="kobo.947.1">to test its performance on the same input data used during the training phase. </span><span class="koboSpan" id="kobo.947.2">By doing so, we can obtain the results and utilize them for </span><span class="No-Break"><span class="koboSpan" id="kobo.948.1">evaluation purposes.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.949.1">Testing the network on the same data helps us assess how well the model generalizes to familiar inputs and provides insights into its effectiveness in handling real-world scenarios. </span><span class="koboSpan" id="kobo.949.2">The evaluation results obtained from this process aid in understanding the network’s accuracy, precision, recall, and other relevant metrics, allowing us to make informed decisions about its overall performance and suitability for the intended pattern </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">recognition task:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.951.1">
SimData = TyroidPatNet(Input);
Diff = gsubtract(Target, SimData);
performance = perform(TyroidPatNet, Target, SimData);
TargetInd = vec2ind(Target);
SimDataInd = vec2ind(SimData);
percentErrors = sum(TargetInd ~=
                      SimDataInd)/numel(TargetInd);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.952.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.953.1">vec2ind()</span></strong><span class="koboSpan" id="kobo.954.1"> function is used to convert vectors to indices. </span><span class="koboSpan" id="kobo.954.2">It allows indices to be </span><a id="_idIndexMarker571"/><span class="koboSpan" id="kobo.955.1">represented either directly as themselves or as vectors with a </span><strong class="source-inline"><span class="koboSpan" id="kobo.956.1">1</span></strong><span class="koboSpan" id="kobo.957.1"> in the </span><a id="_idIndexMarker572"/><span class="koboSpan" id="kobo.958.1">row corresponding to the index they represent. </span><span class="koboSpan" id="kobo.958.2">In the context of this problem, </span><strong class="source-inline"><span class="koboSpan" id="kobo.959.1">TargetInd</span></strong><span class="koboSpan" id="kobo.960.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.961.1">SimDataInd</span></strong><span class="koboSpan" id="kobo.962.1"> are vectors containing values </span><strong class="source-inline"><span class="koboSpan" id="kobo.963.1">1</span></strong><span class="koboSpan" id="kobo.964.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.965.1">2</span></strong><span class="koboSpan" id="kobo.966.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.967.1">3</span></strong><span class="koboSpan" id="kobo.968.1">, representing the classes to which the targets and outputs belong. </span><span class="koboSpan" id="kobo.968.2">The last row of these vectors contains the percentage of </span><span class="No-Break"><span class="koboSpan" id="kobo.969.1">error occurrences.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.970.1">The following value </span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">is obtained:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.972.1">percentErrors = 0.0742</span></pre></li> <li><span class="koboSpan" id="kobo.973.1">Now, let’s proceed with the evaluation of the network. </span><span class="koboSpan" id="kobo.973.2">The following commands extract the outputs and targets that pertain to the training, validation, and test subsets. </span><span class="koboSpan" id="kobo.973.3">This extracted data will be used in the subsequent step to construct the confusion matrix, which will aid in further assessing the performance of the network on each of </span><span class="No-Break"><span class="koboSpan" id="kobo.974.1">these subsets:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.975.1">
trOut = SimData(:,Trs.trainInd);
vOut = SimData (:,Trs.valInd);
tsOut = SimData (:,Trs.testInd);
trTarg = Target(:,Trs.trainInd);
vTarg = Target (:,Trs.valInd);
tsTarg = Target (:,Trs.testInd);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.976.1">The subsequent command generates separate plots for the confusion matrix during </span><a id="_idIndexMarker573"/><span class="koboSpan" id="kobo.977.1">each phase of the evaluation (training, validation, and testing) and an </span><a id="_idIndexMarker574"/><span class="koboSpan" id="kobo.978.1">additional plot representing the overall confusion matrix for the entire evaluation process. </span><span class="koboSpan" id="kobo.978.2">These plots will provide a visual representation of the network’s performance in classifying the data across different phases and help in understanding the overall classification accuracy and </span><span class="No-Break"><span class="koboSpan" id="kobo.979.1">potential misclassifications:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.980.1">figure, plotconfusion(trTarg, trOut, 'Train', vTarg, vOut, 'Validation', tsTarg, tsOut, 'Testing', Target,SimData,'All')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.981.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.982.1">confusion matrix</span></strong><span class="koboSpan" id="kobo.983.1"> is a valuable tool that allows us to compare the classification </span><a id="_idIndexMarker575"/><span class="koboSpan" id="kobo.984.1">results of our model to the real data. </span><span class="koboSpan" id="kobo.984.2">It provides insights into the nature and quantity of classification errors. </span><span class="koboSpan" id="kobo.984.3">The matrix consists of cells where the diagonal elements represent the number of cases that were correctly classified, while the off-diagonal elements show the </span><span class="No-Break"><span class="koboSpan" id="kobo.985.1">misclassified cases.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.986.1">In an ideal situation, a machine learning algorithm should perfectly discriminate between two populations (such as healthy and diseased) that are not overlapping (mutually exclusive). </span><span class="koboSpan" id="kobo.986.2">However, in real-world scenarios, the two populations often overlap to some extent, leading to the algorithm making some false positive and false </span><span class="No-Break"><span class="koboSpan" id="kobo.987.1">negative predictions.</span></span></p><p class="list-inset"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.988.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.989.1">.6</span></em><span class="koboSpan" id="kobo.990.1"> displays the confusion matrix for the training, testing, and validation phases, as well as a combined matrix that considers all three sets of </span><span class="No-Break"><span class="koboSpan" id="kobo.991.1">data together.</span></span></p><div class="IMG---Figure" id="_idContainer050"><span class="koboSpan" id="kobo.992.1"><img alt="" role="presentation" src="image/B21156_05_06.jpg"/></span></div></li> </ol>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.993.1">Figure 5.6 – The confusion matrix for training, testing, and validation, and the three kinds of data combined</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.994.1">This </span><a id="_idIndexMarker576"/><span class="koboSpan" id="kobo.995.1">visual representation will </span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.996.1">help us understand how well the model performed across different phases and its overall classification performance considering all data subsets. </span><span class="koboSpan" id="kobo.996.2">The confusion matrix has been computed for the training, testing, and validation phases, as well as for the combination of all three data subsets. </span><span class="koboSpan" id="kobo.996.3">This comprehensive matrix provides a detailed overview of the model’s classification performance across various evaluation stages and gives a complete picture of its effectiveness in handling </span><span class="No-Break"><span class="koboSpan" id="kobo.997.1">different datasets.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.998.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.999.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1000.1">.6</span></em><span class="koboSpan" id="kobo.1001.1">, the blue cell located in the bottom-right corner represents the total percentage of correctly classified cases, which are depicted by green cells positioned diagonally. </span><span class="koboSpan" id="kobo.1001.2">The red cells in the matrix signify the total percentage of misclassified cases located in other cells. </span><span class="koboSpan" id="kobo.1001.3">The confusion matrix organizes data with </span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.1002.1">rows representing the actual values and columns representing the predicted </span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.1003.1">values. </span><span class="koboSpan" id="kobo.1003.2">For instance, in the top-left plot of the figure, the first row indicates that 85 cases are correctly classified as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1004.1">1</span></strong><span class="koboSpan" id="kobo.1005.1"> (normal), 5 cases are incorrectly classified as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1006.1">2</span></strong><span class="koboSpan" id="kobo.1007.1"> (hyperfunction), and 8 cases are incorrectly classified as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1008.1">3</span></strong><span class="koboSpan" id="kobo.1009.1"> (subnormal). </span><span class="koboSpan" id="kobo.1009.2">By observing the blue cell in the bottom right of each plot in the confusion matrix, we can deduce that the classification accuracy is consistently high, exceeding 92%. </span><span class="koboSpan" id="kobo.1009.3">These results demonstrate excellent recognition capabilities. </span><span class="koboSpan" id="kobo.1009.4">If higher accuracy is required, retraining the data could be considered. </span><span class="koboSpan" id="kobo.1009.5">It should be noted that the starting dataset is not correctly balanced across all classes. </span><span class="koboSpan" id="kobo.1009.6">This can result in a high-performance value for one class and a low-performance value for another. </span><span class="koboSpan" id="kobo.1009.7">It is therefore advisable to always work on datasets that are correctly balanced across </span><span class="No-Break"><span class="koboSpan" id="kobo.1010.1">all classes.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1011.1">Another </span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.1012.1">method of evaluating network performance is through the </span><strong class="bold"><span class="koboSpan" id="kobo.1013.1">receiver operating characteristic</span></strong><span class="koboSpan" id="kobo.1014.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1015.1">ROC</span></strong><span class="koboSpan" id="kobo.1016.1">). </span><span class="koboSpan" id="kobo.1016.2">The ROC curve is a valuable tool for assessing the model’s performance in terms of sensitivity and specificity across various classification thresholds. </span><span class="koboSpan" id="kobo.1016.3">The subsequent command generates plots for the ROC during each evaluation phase and for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1017.1">entire process:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1018.1">
figure, plotroc(trTarg, trOut, 'Train', vTarg, vOut, 'Validation', tsTarg, tsOut, 'Testing', Target,SimData,'All')</span></pre> <p class="list-inset"><span class="koboSpan" id="kobo.1019.1">The ROC is a metric used to evaluate the performance of classifiers. </span><span class="koboSpan" id="kobo.1019.2">It assesses the quality of a classifier for each class by applying threshold values across the interval </span><strong class="source-inline"><span class="koboSpan" id="kobo.1020.1">[0, 1]</span></strong><span class="koboSpan" id="kobo.1021.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.1022.1">its outputs.</span></span></p>
<p><span class="koboSpan" id="kobo.1023.1">In </span><a id="_idIndexMarker581"/><span class="koboSpan" id="kobo.1024.1">the following figure, ROC plots are displayed for the training, testing, and validation phases, as </span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.1025.1">well as for the combined data from all three subsets. </span><span class="koboSpan" id="kobo.1025.2">These ROC curves allow us to analyze the classifier’s sensitivity and specificity across different classification thresholds for each class, providing valuable insights into its discrimination capabilities and </span><span class="No-Break"><span class="koboSpan" id="kobo.1026.1">overall performance.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<span class="koboSpan" id="kobo.1027.1"><img alt="Figure 5.7 – ROC curves for training, testing, and validation, and the three kinds of data combined" src="image/B21156_05_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1028.1">Figure 5.7 – ROC curves for training, testing, and validation, and the three kinds of data combined</span></p>
<p><span class="koboSpan" id="kobo.1029.1">In the graph, the colored lines on each axis represent the ROC curves. </span><span class="koboSpan" id="kobo.1029.2">The ROC curve illustrates the relationship between the </span><strong class="bold"><span class="koboSpan" id="kobo.1030.1">true positive rate</span></strong><span class="koboSpan" id="kobo.1031.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1032.1">TPR, or sensitivity</span></strong><span class="koboSpan" id="kobo.1033.1">) and the </span><strong class="bold"><span class="koboSpan" id="kobo.1034.1">false positive rate</span></strong><span class="koboSpan" id="kobo.1035.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1036.1">FPR</span></strong><span class="koboSpan" id="kobo.1037.1">) as the classification threshold is varied. </span><span class="koboSpan" id="kobo.1037.2">TPR measures the proportion of actual positive cases that are correctly identified as positive by </span><span class="No-Break"><span class="koboSpan" id="kobo.1038.1">the classifier.</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1039.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1040.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1041.1">R</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1042.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1043.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1044.1">P</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1045.1">/</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1046.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1047.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1048.1">P</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1049.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1050.1">F</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1051.1">N</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1052.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.1053.1">Here, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.1054.1">the following:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.1055.1">TP</span></em><span class="koboSpan" id="kobo.1056.1"> = </span><span class="No-Break"><span class="koboSpan" id="kobo.1057.1">True positives</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.1058.1">FN</span></em><span class="koboSpan" id="kobo.1059.1"> = </span><span class="No-Break"><span class="koboSpan" id="kobo.1060.1">False negatives</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1061.1">In </span><a id="_idIndexMarker583"/><span class="koboSpan" id="kobo.1062.1">other words, it calculates the ability of the model to detect all positive instances in the dataset. </span><span class="koboSpan" id="kobo.1062.2">FPR measures the proportion of actual negative cases that are incorrectly classified </span><a id="_idIndexMarker584"/><span class="koboSpan" id="kobo.1063.1">as positive by the classifier. </span><span class="koboSpan" id="kobo.1063.2">It quantifies the rate of false alarms or false positives made by </span><span class="No-Break"><span class="koboSpan" id="kobo.1064.1">the model.</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1065.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1066.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1067.1">R</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1068.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1069.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1070.1">P</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1071.1">/</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1072.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1073.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1074.1">P</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1075.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1076.1">T</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1077.1">N</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1078.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.1079.1">Here, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.1080.1">the following:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.1081.1">FP</span></em><span class="koboSpan" id="kobo.1082.1"> is the number of false positives, which are the number of negative examples that are incorrectly classified </span><span class="No-Break"><span class="koboSpan" id="kobo.1083.1">as positive</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.1084.1">TN</span></em><span class="koboSpan" id="kobo.1085.1"> is the number of true negatives, which are the number of negative examples that are correctly classified </span><span class="No-Break"><span class="koboSpan" id="kobo.1086.1">as negative</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1087.1">An ideal test would exhibit points located in the upper-left corner, indicating 100% sensitivity and 100% specificity. </span><span class="koboSpan" id="kobo.1087.2">The closer the lines approach the upper-left corner, the better the network’s performance, indicating its ability to achieve higher sensitivity while keeping the FPR low, leading to improved </span><span class="No-Break"><span class="koboSpan" id="kobo.1088.1">classification accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.1089.1">After tackling a pattern recognition problem using the tools available in MATLAB, in the next practical example, we will see how to tackle a clustering problem with the help of the Neural </span><span class="No-Break"><span class="koboSpan" id="kobo.1090.1">Network tool.</span></span></p>
<h1 id="_idParaDest-115"><a id="_idTextAnchor118"/><span class="koboSpan" id="kobo.1091.1">Building a clustering application with an ANN</span></h1>
<p><span class="koboSpan" id="kobo.1092.1">Clustering </span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.1093.1">is a popular unsupervised machine </span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.1094.1">learning technique used for grouping similar data points together in a dataset. </span><span class="koboSpan" id="kobo.1094.2">The goal of clustering is to partition data into clusters in such a way that data points within the same cluster are more like each other than those in other clusters. </span><span class="koboSpan" id="kobo.1094.3">We examined this topic in depth in </span><a href="B21156_04.xhtml#_idTextAnchor084"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1095.1">Chapter 4</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.1096.1">, Clustering Analysis and </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1097.1">Dimensionality Reduction</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1098.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1099.1">In this section, we will see how to address a clustering problem using an ANN in the MATLAB environment. </span><span class="koboSpan" id="kobo.1099.2">So far, to train a neural network in the MATLAB environment, we have </span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.1100.1">used the commands available from </span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.1101.1">the command line, or much more simply to be implemented in a script with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1102.1">.m</span></strong><span class="koboSpan" id="kobo.1103.1"> extension to reproduce the algorithm whenever we like. </span><span class="koboSpan" id="kobo.1103.2">But MATLAB has out-of-the-box apps that let us use a wizard to train an ANN. </span><span class="koboSpan" id="kobo.1103.3">We will do this to address a clustering problem </span><span class="No-Break"><span class="koboSpan" id="kobo.1104.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1105.1">To get an overview of the apps made available by the Neural Network tool, we can type the following on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1106.1">command line:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1107.1">
nnstart</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1108.1">The following window </span><span class="No-Break"><span class="koboSpan" id="kobo.1109.1">will open:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer052">
<span class="koboSpan" id="kobo.1110.1"><img alt="Figure 5.8 – Neural Network start window" src="image/B21156_05_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1111.1">Figure 5.8 – Neural Network start window</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1112.1">Four apps are available: </span><strong class="bold"><span class="koboSpan" id="kobo.1113.1">Fitting</span></strong><span class="koboSpan" id="kobo.1114.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.1115.1">Pattern Recognition</span></strong><span class="koboSpan" id="kobo.1116.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.1117.1">Clustering</span></strong><span class="koboSpan" id="kobo.1118.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.1119.1">Time Series</span></strong><span class="koboSpan" id="kobo.1120.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1121.1">Nonlinear time series prediction and modeling (ntstool)</span></strong><span class="koboSpan" id="kobo.1122.1">). </span><span class="koboSpan" id="kobo.1122.2">The first two tasks were already addressed in this chapter in the </span><em class="italic"><span class="koboSpan" id="kobo.1123.1">Discovering pattern recognition using ANNs</span></em><span class="koboSpan" id="kobo.1124.1"> section. </span><span class="koboSpan" id="kobo.1124.2">To do this, we used the command line, but we can also use the app available in the MATLAB environment. </span><span class="koboSpan" id="kobo.1124.3">Specifically, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1125.1">nftool</span></strong><span class="koboSpan" id="kobo.1126.1"> command to start the </span><strong class="bold"><span class="koboSpan" id="kobo.1127.1">Fitting</span></strong><span class="koboSpan" id="kobo.1128.1"> app, and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1129.1">nprtool</span></strong><span class="koboSpan" id="kobo.1130.1"> command to open the </span><strong class="bold"><span class="koboSpan" id="kobo.1131.1">Pattern </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1132.1">Recognition</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1133.1"> app.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1134.1">In this example, we will use the </span><strong class="bold"><span class="koboSpan" id="kobo.1135.1">Clustering</span></strong><span class="koboSpan" id="kobo.1136.1"> app, which is started using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1137.1">following command:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1138.1">
nctool</span></pre> <p class="list-inset"><span class="koboSpan" id="kobo.1139.1">The following window </span><span class="No-Break"><span class="koboSpan" id="kobo.1140.1">will open:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer053">
<span class="koboSpan" id="kobo.1141.1"><img alt="Figure 5.9 – MATLAB app for clustering using ANN" src="image/B21156_05_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1142.1">Figure 5.9 – MATLAB app for clustering using ANN</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1143.1">In </span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.1144.1">this app, we can use a wizard </span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.1145.1">for the training of the ANN. </span><span class="koboSpan" id="kobo.1145.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1146.1">see how.</span></span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.1147.1">To start, we have to import the data. </span><span class="koboSpan" id="kobo.1147.2">For this example, we will use a dataset already available in the MATLAB environment to explain the clustering application using an ANN. </span><span class="koboSpan" id="kobo.1147.3">To import that dataset in the MATLAB workspace, we use the </span><span class="No-Break"><span class="koboSpan" id="kobo.1148.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1149.1">
 [Input,Target] = simplecluster_dataset;</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1150.1">Now, we have two matrices in the MATLAB workspace (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1151.1">Input</span></strong><span class="koboSpan" id="kobo.1152.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1153.1">Target</span></strong><span class="koboSpan" id="kobo.1154.1">). </span><span class="koboSpan" id="kobo.1154.2">We will use only the first (wx1,000 double) for a clustering problem, which is the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1155.1">Input</span></strong><span class="koboSpan" id="kobo.1156.1"> matrix with two variables and 1,000 records. </span><span class="koboSpan" id="kobo.1156.2">By examining the structure of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1157.1">Target</span></strong><span class="koboSpan" id="kobo.1158.1"> (4x1,000 double) matrix, we realize that four classes are available, so the data has </span><strong class="source-inline"><span class="koboSpan" id="kobo.1159.1">4</span></strong><span class="koboSpan" id="kobo.1160.1"> groups of data, which will be useful in justifying </span><span class="No-Break"><span class="koboSpan" id="kobo.1161.1">the results.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1162.1">Now, we </span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.1163.1">have to import the data </span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.1164.1">in the app. </span><span class="koboSpan" id="kobo.1164.2">To do this, we can click on the </span><strong class="bold"><span class="koboSpan" id="kobo.1165.1">Import</span></strong><span class="koboSpan" id="kobo.1166.1"> button in the window shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1167.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1168.1">.9</span></em><span class="koboSpan" id="kobo.1169.1">. </span><span class="koboSpan" id="kobo.1169.2">The window shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1170.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1171.1">.10</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.1172.1">will open:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer054">
<span class="koboSpan" id="kobo.1173.1"><img alt="Figure 5.10 – Import data in Clustering app" src="image/B21156_05_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1174.1">Figure 5.10 – Import data in Clustering app</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1175.1">The right matrix was already detected for </span><strong class="bold"><span class="koboSpan" id="kobo.1176.1">Predictors</span></strong><span class="koboSpan" id="kobo.1177.1">; if you want to change the data, you can select the right one using the browse button next to  the </span><strong class="bold"><span class="koboSpan" id="kobo.1178.1">Predictors</span></strong><span class="koboSpan" id="kobo.1179.1"> field. </span><span class="koboSpan" id="kobo.1179.2">In this case, we have to set the observations in columns, so we will have 1,000 observations with 2 features. </span><span class="koboSpan" id="kobo.1179.3">Let’s just click on the </span><strong class="bold"><span class="koboSpan" id="kobo.1180.1">OK</span></strong><span class="koboSpan" id="kobo.1181.1"> button to import </span><span class="No-Break"><span class="koboSpan" id="kobo.1182.1">the data.</span></span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.1183.1">We have the ANN architecture already set in the app, so we can identify the number of inputs and the topology of the network. </span><span class="koboSpan" id="kobo.1183.2">The output will be a network with 10x10 nodes (100 nodes). </span><span class="koboSpan" id="kobo.1183.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1184.1">nctool</span></strong><span class="koboSpan" id="kobo.1185.1"> guides you in solving clustering problems </span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.1186.1">by employing a </span><strong class="bold"><span class="koboSpan" id="kobo.1187.1">self-organizing map</span></strong><span class="koboSpan" id="kobo.1188.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1189.1">SOM</span></strong><span class="koboSpan" id="kobo.1190.1">). </span><span class="koboSpan" id="kobo.1190.2">This map creates a condensed depiction of the input space, capturing both the density patterns of input vectors in that space and a compressed, two-dimensional representation of the input </span><span class="No-Break"><span class="koboSpan" id="kobo.1191.1">space’s topology.</span></span><p class="list-inset"><strong class="bold"><span class="koboSpan" id="kobo.1192.1">SOMs</span></strong><span class="koboSpan" id="kobo.1193.1">, also known as </span><strong class="bold"><span class="koboSpan" id="kobo.1194.1">Kohonen maps</span></strong><span class="koboSpan" id="kobo.1195.1">, are a type of ANN that belongs to the family of </span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.1196.1">unsupervised learning algorithms. </span><span class="koboSpan" id="kobo.1196.2">They were introduced by the Finnish professor Teuvo Kohonen in the 1980s. </span><span class="koboSpan" id="kobo.1196.3">SOMs are used for tasks such as dimensionality reduction, data visualization, clustering, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1197.1">feature extraction.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1198.1">The </span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.1199.1">primary idea behind SOMs is to map high-dimensional input data onto a lower-dimensional grid or lattice in such </span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.1200.1">a way that similar input data points are mapped to nearby grid cells. </span><span class="koboSpan" id="kobo.1200.2">This results in a topological representation of the input data, where similar data points are located close to each other on the map, allowing for easier visualization </span><span class="No-Break"><span class="koboSpan" id="kobo.1201.1">and interpretation.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1202.1">SOMs consist of two essential layers: the input layer and the output layer, often referred to as the feature map. </span><span class="koboSpan" id="kobo.1202.2">The input layer serves as the initial stage in a SOM. </span><span class="koboSpan" id="kobo.1202.3">Each data point from the dataset competes for representation to identify its own characteristics. </span><span class="koboSpan" id="kobo.1202.4">The process begins with weight vector initialization, kickstarting the mapping process of </span><span class="No-Break"><span class="koboSpan" id="kobo.1203.1">the SOM.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1204.1">The mapped vectors are subsequently scrutinized to identify the weight vector that best represents the chosen sample using a randomly selected sample vector. </span><span class="koboSpan" id="kobo.1204.2">Nearby weights to each weighted vector are considered, and the chosen weight evolves into a vector for the random sample, fostering the map’s growth and the emergence of new patterns. </span><span class="koboSpan" id="kobo.1204.3">In a two-dimensional feature space, these patterns often take on hexagonal or square shapes. </span><span class="koboSpan" id="kobo.1204.4">This entire process is repeated over </span><span class="No-Break"><span class="koboSpan" id="kobo.1205.1">1,000 times.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1206.1">In essence, learning occurs </span><span class="No-Break"><span class="koboSpan" id="kobo.1207.1">as follows:</span></span></p><ol><li class="upper-roman"><span class="koboSpan" id="kobo.1208.1">Each </span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.1209.1">node is analyzed to determine whether its weights are similar to the input vector. </span><span class="koboSpan" id="kobo.1209.2">The node that best matches the input vector is termed the </span><strong class="bold"><span class="koboSpan" id="kobo.1210.1">best matching </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1211.1">unit</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1212.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1213.1">BMU</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1214.1">).</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.1215.1">The neighborhood value of the BMU is then established, and over time, the number of neighbors tends </span><span class="No-Break"><span class="koboSpan" id="kobo.1216.1">to decrease.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.1217.1">The BMU’s weight vector further adapts to resemble the sample vector, leading to similar changes in the surrounding areas. </span><span class="koboSpan" id="kobo.1217.2">The weight of a node changes more as it gets closer to the BMU and less as it moves away from </span><span class="No-Break"><span class="koboSpan" id="kobo.1218.1">its neighbors.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.1219.1">Repeat step 2 for </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1220.1">N</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1221.1"> iterations.</span></span></li></ol><p class="list-inset"><span class="koboSpan" id="kobo.1222.1">This </span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.1223.1">iterative process allows the </span><a id="_idIndexMarker599"/><span class="koboSpan" id="kobo.1224.1">SOM to refine its representation of the input data, ultimately leading to a more organized and compressed representation that captures the underlying structure of </span><span class="No-Break"><span class="koboSpan" id="kobo.1225.1">the data.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1226.1">Now, we can train the network just by clicking on the </span><strong class="bold"><span class="koboSpan" id="kobo.1227.1">train</span></strong><span class="koboSpan" id="kobo.1228.1"> button of the app. </span><span class="koboSpan" id="kobo.1228.2">After a few seconds, the ANN will be trained and ready </span><span class="No-Break"><span class="koboSpan" id="kobo.1229.1">for use.</span></span></p></li>
<li><span class="koboSpan" id="kobo.1230.1">Finally, we can view the results using the plots available at the top of the </span><strong class="bold"><span class="koboSpan" id="kobo.1231.1">App</span></strong><span class="koboSpan" id="kobo.1232.1"> window. </span><span class="koboSpan" id="kobo.1232.2">We will first use the neighbor distances, and the following plot will </span><span class="No-Break"><span class="koboSpan" id="kobo.1233.1">be drawn:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer055">
<span class="koboSpan" id="kobo.1234.1"><img alt="Figure 5.11 – SOM neighbor weight distances" src="image/B21156_05_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1235.1">Figure 5.11 – SOM neighbor weight distances</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1236.1">This is a graphical representation that illustrates the distances between weight vectors </span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.1237.1">of neighboring nodes in a SOM. </span><span class="koboSpan" id="kobo.1237.2">This plot helps visualize how the weights of neighboring nodes change during </span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.1238.1">the learning process, providing insights into the topology and organization of the SOM. </span><span class="koboSpan" id="kobo.1238.2">We can identify more colored cells that separate the nodes in four areas using the class available in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1239.1">Target</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1240.1"> matrix.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1241.1">The blue hexagons represent the neurons. </span><span class="koboSpan" id="kobo.1241.2">The red lines connect neighboring neurons. </span><span class="koboSpan" id="kobo.1241.3">The colors in the regions containing the red lines indicate the distances between neurons. </span><span class="koboSpan" id="kobo.1241.4">The darker colors represent larger distances, and the lighter colors represent </span><span class="No-Break"><span class="koboSpan" id="kobo.1242.1">smaller distances.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1243.1">One visualization tool for the SOM is the weight distance matrix (also called the U-matrix). </span><span class="koboSpan" id="kobo.1243.2">To view the U-matrix, click </span><strong class="bold"><span class="koboSpan" id="kobo.1244.1">SOM Neighbor Distances</span></strong><span class="koboSpan" id="kobo.1245.1"> in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1246.1">training window.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1247.1">This shows us that there are four potential clusters in the data correctly detected by the algorithm. </span><span class="koboSpan" id="kobo.1247.2">To have a confirmation of this indication, we can plot the weight </span><span class="No-Break"><span class="koboSpan" id="kobo.1248.1">position plot:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<span class="koboSpan" id="kobo.1249.1"><img alt="Figure 5.12 – SOM weight positions" src="image/B21156_05_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1250.1">Figure 5.12 – SOM weight positions</span></p>
<p><span class="koboSpan" id="kobo.1251.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1252.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1253.1">.12</span></em><span class="koboSpan" id="kobo.1254.1">, the four clusters can be easily identified. </span><span class="koboSpan" id="kobo.1254.2">In this diagram, the input vectors </span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.1255.1">are depicted as green dots, and </span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.1256.1">it illustrates the SOM’s classification of the input space by displaying blue-gray dots representing each neuron’s weight vector. </span><span class="koboSpan" id="kobo.1256.2">Additionally, neighboring neurons relate to red lines, offering a visual representation of the SOM’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1257.1">organizational structure.</span></span></p>
<p><span class="koboSpan" id="kobo.1258.1">After having also analyzed a case of clustering with ANNs, to complete the topic, we will see how to optimize the </span><span class="No-Break"><span class="koboSpan" id="kobo.1259.1">results obtained.</span></span></p>
<h1 id="_idParaDest-116"><a id="_idTextAnchor119"/><span class="koboSpan" id="kobo.1260.1">Exploring advanced optimization techniques</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.1261.1">Advanced optimization techniques</span></strong><span class="koboSpan" id="kobo.1262.1"> are powerful methods used to enhance the efficiency </span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.1263.1">and effectiveness of optimization algorithms. </span><span class="koboSpan" id="kobo.1263.2">These techniques aim to overcome the limitations of traditional optimization </span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.1264.1">approaches, particularly in complex, high-dimensional, or non-convex </span><span class="No-Break"><span class="koboSpan" id="kobo.1265.1">optimization problems.</span></span></p>
<p><span class="koboSpan" id="kobo.1266.1">In machine learning, advanced optimization techniques are essential for training complex models effectively, improving convergence, avoiding overfitting, and handling high-dimensional data. </span><span class="koboSpan" id="kobo.1266.2">In the following subsection, some advanced optimization techniques commonly used in machine learning </span><span class="No-Break"><span class="koboSpan" id="kobo.1267.1">are listed.</span></span></p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor120"/><span class="koboSpan" id="kobo.1268.1">Understanding SGD</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.1269.1">SGD</span></strong><span class="koboSpan" id="kobo.1270.1"> is a popular and fundamental optimization algorithm used in machine learning for training models, especially in large-scale and complex settings. </span><span class="koboSpan" id="kobo.1270.2">It’s a variant of the traditional </span><a id="_idIndexMarker606"/><span class="koboSpan" id="kobo.1271.1">gradient descent method designed to address efficiency and convergence issues when dealing with large datasets. </span><span class="koboSpan" id="kobo.1271.2">The stochastic </span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.1272.1">aspect of SGD comes from the fact that it uses a random mini-batch for each iteration, making the optimization process more stochastic (randomized) compared to the deterministic nature of regular gradient descent, which uses the entire dataset for each update. </span><span class="koboSpan" id="kobo.1272.2">This stochasticity introduces noise into the gradient estimates, which can help the algorithm escape local minima, converge faster, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1273.1">generalize better.</span></span></p>
<p><span class="koboSpan" id="kobo.1274.1">The primary objective of any optimization algorithm is to minimize the discrepancy between the predicted values of the model and the actual values observed in the data. </span><span class="koboSpan" id="kobo.1274.2">The smaller the error between the observed and predicted values, the more effective the algorithm is at simulating the real-world scenario. </span><span class="koboSpan" id="kobo.1274.3">Minimizing this discrepancy is equivalent to optimizing an objective function on which the model construction </span><span class="No-Break"><span class="koboSpan" id="kobo.1275.1">is based.</span></span></p>
<p><span class="koboSpan" id="kobo.1276.1">Descent methods are iterative techniques that, commencing from an initial point </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1277.1">x</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1278.1">0</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.1279.1">∈</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1280.1">R</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1281.1">n</span></span><span class="koboSpan" id="kobo.1282.1">, produce a sequence of points </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1283.1">{</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1284.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1285.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1286.1">}</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.1287.1">∈</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1288.1">N</span></span><span class="koboSpan" id="kobo.1289.1"> based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1290.1">subsequent equation:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1291.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1292.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1293.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1294.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1295.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1296.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1297.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1298.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1299.1">n</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1300.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1301.1">γ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1302.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1303.1">n</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.1304.1">*</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1305.1">g</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1306.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1307.1">n</span></span></span></p>
<p><span class="koboSpan" id="kobo.1308.1">In a descent method, the vector </span><em class="italic"><span class="koboSpan" id="kobo.1309.1">g</span></em><em class="italic"><span class="koboSpan" id="kobo.1310.1">n</span></em><span class="koboSpan" id="kobo.1311.1"> represents the search direction, and the scalar </span><em class="italic"><span class="koboSpan" id="kobo.1312.1">γ</span></em><em class="italic"><span class="koboSpan" id="kobo.1313.1">n</span></em><span class="koboSpan" id="kobo.1314.1"> serves as a positive parameter known as the step length, determining the distance of movement in the </span><em class="italic"><span class="koboSpan" id="kobo.1315.1">g</span></em><em class="italic"><span class="koboSpan" id="kobo.1316.1">n</span></em><span class="koboSpan" id="kobo.1317.1"> direction. </span><span class="koboSpan" id="kobo.1317.2">These choices of </span><em class="italic"><span class="koboSpan" id="kobo.1318.1">g</span></em><em class="italic"><span class="koboSpan" id="kobo.1319.1">n</span></em><span class="koboSpan" id="kobo.1320.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1321.1">γ</span></em><em class="italic"><span class="koboSpan" id="kobo.1322.1">n</span></em><span class="koboSpan" id="kobo.1323.1"> are made to ensure the reduction of the objective function </span><em class="italic"><span class="koboSpan" id="kobo.1324.1">f</span></em><span class="koboSpan" id="kobo.1325.1"> in each iteration, following </span><span class="No-Break"><span class="koboSpan" id="kobo.1326.1">the principle:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1327.1">f</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1328.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1329.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1330.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1331.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1332.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1333.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1334.1">&lt;</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1335.1">f</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1336.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1337.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1338.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1339.1">n</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.1340.1">∀</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1341.1">n</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1342.1">≥</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1343.1">0</span></span></p>
<p><span class="koboSpan" id="kobo.1344.1">We select the vector </span><em class="italic"><span class="koboSpan" id="kobo.1345.1">g</span></em><em class="italic"><span class="koboSpan" id="kobo.1346.1">n</span></em><span class="koboSpan" id="kobo.1347.1"> as a descent direction, ensuring that the line </span><em class="italic"><span class="koboSpan" id="kobo.1348.1">x = x</span></em><em class="italic"><span class="koboSpan" id="kobo.1349.1">n</span></em><em class="italic"><span class="koboSpan" id="kobo.1350.1"> + γ</span></em><em class="italic"><span class="koboSpan" id="kobo.1351.1">n</span></em><em class="italic"><span class="koboSpan" id="kobo.1352.1"> * g</span></em><em class="italic"><span class="koboSpan" id="kobo.1353.1">n</span></em><span class="koboSpan" id="kobo.1354.1"> creates </span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.1355.1">an obtuse angle with the gradient vector </span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.1356.1">∇</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1357.1">f</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1358.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1359.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1360.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1361.1">)</span></span><span class="koboSpan" id="kobo.1362.1">. </span><span class="koboSpan" id="kobo.1362.2">This guarantees the decrease of the objective function </span><em class="italic"><span class="koboSpan" id="kobo.1363.1">f</span></em><span class="koboSpan" id="kobo.1364.1">, as long as the value of </span><em class="italic"><span class="koboSpan" id="kobo.1365.1">γ</span></em><em class="italic"><span class="koboSpan" id="kobo.1366.1">n</span></em><span class="koboSpan" id="kobo.1367.1"> is sufficiently small. </span><span class="koboSpan" id="kobo.1367.2">This approach allows for various descent methods, contingent on the specific choice </span><span class="No-Break"><span class="koboSpan" id="kobo.1368.1">of </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1369.1">g</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1370.1">n</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1371.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1372.1">A gradient is a function that produces a vector, representing the slope of the tangent to the </span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.1373.1">graph of a function. </span><span class="koboSpan" id="kobo.1373.2">It points in the direction of the greatest increase in the function. </span><span class="koboSpan" id="kobo.1373.3">Let’s examine the convex function illustrated in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1374.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<span class="koboSpan" id="kobo.1375.1"><img alt="Figure 5.13 – How the gradient descent algorithm searches the global optimum" src="image/B21156_05_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1376.1">Figure 5.13 – How the gradient descent algorithm searches the global optimum</span></p>
<p><span class="koboSpan" id="kobo.1377.1">The primary objective of the gradient descent algorithm is to locate the function’s nadir, or lowest point. </span><span class="koboSpan" id="kobo.1377.2">In more precise terms, the gradient functions as a derivative, indicating the incline or steepness of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1378.1">objective function.</span></span></p>
<p><span class="koboSpan" id="kobo.1379.1">To provide a clearer analogy, let’s imagine we find ourselves lost in the mountains at night </span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.1380.1">with limited visibility. </span><span class="koboSpan" id="kobo.1380.2">Our perception is restricted to sensing the slope of the terrain beneath our feet. </span><span class="koboSpan" id="kobo.1380.3">The aim is to reach the lowest point of the mountain. </span><span class="koboSpan" id="kobo.1380.4">Achieving this goal involves </span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.1381.1">taking successive steps in the direction of the steepest slope. </span><span class="koboSpan" id="kobo.1381.2">We proceed iteratively, advancing step by step, until we finally arrive at the valley of </span><span class="No-Break"><span class="koboSpan" id="kobo.1382.1">the mountain.</span></span></p>
<p><span class="koboSpan" id="kobo.1383.1">We’ll examine a two-variable function, denoted as </span><em class="italic"><span class="koboSpan" id="kobo.1384.1">f(x, y)</span></em><span class="koboSpan" id="kobo.1385.1">, with its gradient represented as a vector encompassing the partial derivatives of </span><em class="italic"><span class="koboSpan" id="kobo.1386.1">f</span></em><span class="koboSpan" id="kobo.1387.1">. </span><span class="koboSpan" id="kobo.1387.2">The first derivative pertains to </span><em class="italic"><span class="koboSpan" id="kobo.1388.1">x</span></em><span class="koboSpan" id="kobo.1389.1">, while the second derivative pertains to </span><em class="italic"><span class="koboSpan" id="kobo.1390.1">y</span></em><span class="koboSpan" id="kobo.1391.1">. </span><span class="koboSpan" id="kobo.1391.2">Upon computation of these partial derivatives, the results are </span><span class="No-Break"><span class="koboSpan" id="kobo.1392.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1393.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1394.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1395.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1396.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1397.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1398.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1399.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1400.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1401.1"> </span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1402.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1403.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1404.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1405.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1406.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1407.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1408.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1409.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1410.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1411.1"> </span></span><span class="_-----MathTools-_Math_Space"> </span></p>
<p><span class="koboSpan" id="kobo.1412.1">The initial expression corresponds to the partial derivative concerning </span><em class="italic"><span class="koboSpan" id="kobo.1413.1">x</span></em><span class="koboSpan" id="kobo.1414.1">, whereas the subsequent expression pertains to the partial derivative regarding </span><em class="italic"><span class="koboSpan" id="kobo.1415.1">y</span></em><span class="koboSpan" id="kobo.1416.1">. </span><span class="koboSpan" id="kobo.1416.2">The gradient is represented by the </span><span class="No-Break"><span class="koboSpan" id="kobo.1417.1">ensuing vector:</span></span></p>
<p><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.1418.1">∇</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1419.1">f</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1420.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1421.1">x</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1422.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1423.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1424.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1425.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1426.1">⎡</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1427.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1428.1">⎢</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1429.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1430.1">⎣</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1431.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1432.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1433.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1434.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1435.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1436.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1437.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1438.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1439.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1440.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1441.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1442.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1443.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1444.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1445.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1446.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1447.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1448.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1449.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1450.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1451.1">⎤</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1452.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1453.1">⎥</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1454.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1455.1">⎦</span></span></p>
<p><span class="koboSpan" id="kobo.1456.1">The given equation represents a function in a two-dimensional space, effectively forming a two-dimensional vector. </span><span class="koboSpan" id="kobo.1456.2">Each component of this vector signifies the steepest ascent direction for the respective function variable. </span><span class="koboSpan" id="kobo.1456.3">Consequently, the gradient points toward the direction where the function exhibits the most </span><span class="No-Break"><span class="koboSpan" id="kobo.1457.1">significant increase.</span></span></p>
<p><span class="koboSpan" id="kobo.1458.1">Likewise, if we consider a function with five variables, the resulting gradient vector will encompass five partial derivatives. </span><span class="koboSpan" id="kobo.1458.2">In general, a function with </span><em class="italic"><span class="koboSpan" id="kobo.1459.1">n</span></em><span class="koboSpan" id="kobo.1460.1"> variables gives rise to an </span><em class="italic"><span class="koboSpan" id="kobo.1461.1">n</span></em><span class="koboSpan" id="kobo.1462.1">-dimensional gradient vector, exemplified </span><span class="No-Break"><span class="koboSpan" id="kobo.1463.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.1464.1">∇</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1465.1">f</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1466.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1467.1">x</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1468.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1469.1">y</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1470.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1471.1">…</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1472.1">.</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1473.1">z</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1474.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1475.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1476.1">⎡</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1477.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1478.1">⎢</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1479.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1480.1">⎣</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1481.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1482.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1483.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1484.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1485.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1486.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1487.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1488.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1489.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1490.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1491.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1492.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1493.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1494.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1495.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1496.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1497.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1498.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1499.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1500.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1501.1"> </span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1502.1">…</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1503.1"> </span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1504.1">…</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1505.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1506.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1507.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1508.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1509.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1510.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1511.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1512.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1513.1">z</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1514.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1515.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1516.1">⎤</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1517.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1518.1">⎥</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1519.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1520.1">⎦</span></span></p>
<p><span class="koboSpan" id="kobo.1521.1">When </span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.1522.1">utilizing gradient descent, our objective isn’t to maximize </span><em class="italic"><span class="koboSpan" id="kobo.1523.1">f</span></em><span class="koboSpan" id="kobo.1524.1"> as rapidly as possible; instead, we aim to minimize it—specifically, to locate </span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.1525.1">the smallest point that minimizes </span><span class="No-Break"><span class="koboSpan" id="kobo.1526.1">the function.</span></span></p>
<p><span class="koboSpan" id="kobo.1527.1">Consider a function </span><em class="italic"><span class="koboSpan" id="kobo.1528.1">y = f(x)</span></em><span class="koboSpan" id="kobo.1529.1">. </span><span class="koboSpan" id="kobo.1529.2">The foundation of gradient descent relies on the observation that when the function </span><em class="italic"><span class="koboSpan" id="kobo.1530.1">f</span></em><span class="koboSpan" id="kobo.1531.1"> is well defined and differentiable within a vicinity of </span><em class="italic"><span class="koboSpan" id="kobo.1532.1">x</span></em><span class="koboSpan" id="kobo.1533.1">, it experiences a faster decrease as we proceed in the direction opposite to the negative gradient. </span><span class="koboSpan" id="kobo.1533.2">Commencing from an initial value of </span><em class="italic"><span class="koboSpan" id="kobo.1534.1">x</span></em><span class="koboSpan" id="kobo.1535.1">, we can express this </span><span class="No-Break"><span class="koboSpan" id="kobo.1536.1">as follows:</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.1537.1">x</span></em><em class="italic"><span class="koboSpan" id="kobo.1538.1">n</span></em><em class="italic"><span class="koboSpan" id="kobo.1539.1"> + 1 = </span></em><em class="italic"><span class="koboSpan" id="kobo.1540.1">x</span></em><em class="italic"><span class="koboSpan" id="kobo.1541.1">n</span></em><span class="koboSpan" id="kobo.1542.1"> - </span><em class="italic"><span class="koboSpan" id="kobo.1543.1">γ</span></em> <em class="italic"><span class="koboSpan" id="kobo.1544.1">*</span></em><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.1545.1">∇</span></span> <span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1546.1">f(</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1547.1">x</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1548.1">n</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1549.1">)</span></em></span></p>
<p><span class="koboSpan" id="kobo.1550.1">where gamma (</span><em class="italic"><span class="koboSpan" id="kobo.1551.1">γ</span></em><span class="koboSpan" id="kobo.1552.1">) is learning rate and delta (</span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.1553.1">∇</span></span><span class="koboSpan" id="kobo.1554.1">) is </span><span class="No-Break"><span class="koboSpan" id="kobo.1555.1">the gradient.</span></span></p>
<p><span class="koboSpan" id="kobo.1556.1">In this context, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.1557.1">the following:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.1558.1">γ</span></em><span class="koboSpan" id="kobo.1559.1"> represents the </span><span class="No-Break"><span class="koboSpan" id="kobo.1560.1">learning rate</span></span></li>
<li><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.1561.1">∇</span></span><span class="koboSpan" id="kobo.1562.1"> signifies </span><span class="No-Break"><span class="koboSpan" id="kobo.1563.1">the gradient</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1564.1">When using sufficiently small values of </span><em class="italic"><span class="koboSpan" id="kobo.1565.1">γ</span></em><span class="koboSpan" id="kobo.1566.1">, the algorithm converges to the minimum value of the function </span><em class="italic"><span class="koboSpan" id="kobo.1567.1">f</span></em><span class="koboSpan" id="kobo.1568.1"> within a finite number of iterations. </span><span class="koboSpan" id="kobo.1568.2">In essence, when the gradient is negative, it indicates a decreasing objective function at that point, implying that the parameter should shift toward larger values to approach a minimum point. </span><span class="koboSpan" id="kobo.1568.3">Conversely, when the gradient is positive, the parameters should move toward smaller values to attain lower values of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1569.1">objective function.</span></span></p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor121"/><span class="koboSpan" id="kobo.1570.1">Exploring Adam optimization</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.1571.1">Adaptive moment estimation</span></strong><span class="koboSpan" id="kobo.1572.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1573.1">Adam</span></strong><span class="koboSpan" id="kobo.1574.1">) is an advanced optimization algorithm commonly </span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.1575.1">used for training deep learning models, particularly in neural networks. </span><span class="koboSpan" id="kobo.1575.2">It is an extension of the SGD optimization method that adapts the learning rate for each </span><span class="No-Break"><span class="koboSpan" id="kobo.1576.1">parameter individually.</span></span></p>
<p><span class="koboSpan" id="kobo.1577.1">This </span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.1578.1">gradient descent </span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.1579.1">algorithm aims to locate the objective function’s minimum using an iterative approach. </span><span class="koboSpan" id="kobo.1579.2">In each step, an approximation of the gradient is computed to guide the descent in the direction that effectively reduces the objective function. </span><span class="koboSpan" id="kobo.1579.3">Within this process, the selection of the learning rate parameter holds significant importance. </span><span class="koboSpan" id="kobo.1579.4">This parameter governs the speed at which we approach the optimal objective </span><span class="No-Break"><span class="koboSpan" id="kobo.1580.1">function values:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1581.1">If the learning rate is excessively small, a substantial number of iterations are required to converge toward the </span><span class="No-Break"><span class="koboSpan" id="kobo.1582.1">optimal values</span></span></li>
<li><span class="koboSpan" id="kobo.1583.1">Conversely, an overly high learning rate might cause us to overlook the </span><span class="No-Break"><span class="koboSpan" id="kobo.1584.1">optimal solution</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1585.1">Adam adjusts the learning rates for each parameter based on the first-order moment (mean) and second-order moment (uncentered variance) of the gradients. </span><span class="koboSpan" id="kobo.1585.2">This adaptability helps the algorithm converge more efficiently, especially in high-dimensional spaces with varying </span><span class="No-Break"><span class="koboSpan" id="kobo.1586.1">gradient magnitudes.</span></span></p>
<p><span class="koboSpan" id="kobo.1587.1">This method addresses the issue of initial bias in the moments by correcting them. </span><span class="koboSpan" id="kobo.1587.2">In the early stages of training, the moments’ estimates may be biased toward 0, and Adam compensates for this bias, particularly when the learning rate </span><span class="No-Break"><span class="koboSpan" id="kobo.1588.1">is low.</span></span></p>
<p><span class="koboSpan" id="kobo.1589.1">Adam combines the concepts of momentum (accumulating a fraction of the past gradients to enhance convergence) and </span><strong class="bold"><span class="koboSpan" id="kobo.1590.1">RMSProp</span></strong><span class="koboSpan" id="kobo.1591.1"> (scaling the learning rates based on the magnitudes of recent gradients) to perform well on a wide range of </span><span class="No-Break"><span class="koboSpan" id="kobo.1592.1">optimization problems.</span></span></p>
<p><span class="koboSpan" id="kobo.1593.1">The Adam optimization algorithm involves the following calculations during </span><span class="No-Break"><span class="koboSpan" id="kobo.1594.1">each iteration:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1595.1">Compute the gradient of the loss with respect to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1596.1">model’s parameters</span></span></li>
<li><span class="koboSpan" id="kobo.1597.1">Update the first and second moments (mean and uncentered variance) of the gradients using a </span><span class="No-Break"><span class="koboSpan" id="kobo.1598.1">moving average</span></span></li>
<li><span class="koboSpan" id="kobo.1599.1">Correct the bias in the moments (especially in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1600.1">early iterations)</span></span></li>
<li><span class="koboSpan" id="kobo.1601.1">Update the parameters using the corrected moments and the </span><span class="No-Break"><span class="koboSpan" id="kobo.1602.1">learning rate</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1603.1">Overall, Adam </span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.1604.1">is effective in many deep learning scenarios, often providing faster convergence than standard SGD with manually tuned learning </span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.1605.1">rates. </span><span class="koboSpan" id="kobo.1605.2">It’s a popular choice for training neural networks and has become a standard optimization algorithm in many deep learning frameworks due to its adaptive properties and </span><span class="No-Break"><span class="koboSpan" id="kobo.1606.1">strong performance.</span></span></p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor122"/><span class="koboSpan" id="kobo.1607.1">Introducing second-order methods</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.1608.1">Second-order optimization methods</span></strong><span class="koboSpan" id="kobo.1609.1">, also </span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.1610.1">known as </span><strong class="bold"><span class="koboSpan" id="kobo.1611.1">Newton-like methods</span></strong><span class="koboSpan" id="kobo.1612.1">, are advanced techniques used for solving optimization problems. </span><span class="koboSpan" id="kobo.1612.2">Unlike </span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.1613.1">first-order methods (such as gradient descent) that primarily rely on gradients, second-order methods take advantage </span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.1614.1">of both gradients </span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.1615.1">and second-order derivatives (</span><strong class="bold"><span class="koboSpan" id="kobo.1616.1">Hessian matrix</span></strong><span class="koboSpan" id="kobo.1617.1">) of the objective function. </span><span class="koboSpan" id="kobo.1617.2">This additional information can lead to faster convergence and more accurate solutions, especially in complex and non-convex </span><span class="No-Break"><span class="koboSpan" id="kobo.1618.1">optimization landscapes.</span></span></p>
<p><span class="koboSpan" id="kobo.1619.1">Second-order methods consider not only the first-order derivative (gradient) of the function but also its second-order derivative (Hessian matrix). </span><span class="koboSpan" id="kobo.1619.2">The Hessian matrix captures curvature information and provides insights into the shape of the objective function’s surface. </span><span class="koboSpan" id="kobo.1619.3">These methods often use quadratic approximations of the objective function around the current point. </span><span class="koboSpan" id="kobo.1619.4">These approximations consider the first and second derivatives and provide a more accurate representation of the local behavior of </span><span class="No-Break"><span class="koboSpan" id="kobo.1620.1">the function.</span></span></p>
<p><span class="koboSpan" id="kobo.1621.1">The use of curvature information from the Hessian matrix can lead to faster convergence rates compared to first-order methods, especially when the objective function is well behaved and smooth. </span><span class="koboSpan" id="kobo.1621.2">Second-order methods can be more robust against the choice of learning rates or step sizes, as they inherently adjust the step size based on the curvature of </span><span class="No-Break"><span class="koboSpan" id="kobo.1622.1">the function.</span></span></p>
<p><span class="koboSpan" id="kobo.1623.1">Second-order methods are valuable tools for optimization, particularly when computational resources allow for the calculation of second-order derivatives. </span><span class="koboSpan" id="kobo.1623.2">However, their application may depend on the specific characteristics of the problem </span><span class="No-Break"><span class="koboSpan" id="kobo.1624.1">being solved.</span></span></p>
<p><span class="koboSpan" id="kobo.1625.1">An example of a second-order method is Newton’s method. </span><span class="koboSpan" id="kobo.1625.2">This method directly minimizes a quadratic approximation of the objective function using the Hessian matrix. </span><span class="koboSpan" id="kobo.1625.3">Newton’s method is a classic optimization algorithm used to find the minimum or maximum </span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.1626.1">of a function. </span><span class="koboSpan" id="kobo.1626.2">Named after Sir Isaac Newton, this method utilizes both the gradient (first derivative) and the second derivative (Hessian matrix) of the function to iteratively approach the </span><span class="No-Break"><span class="koboSpan" id="kobo.1627.1">optimal solution.</span></span></p>
<p><span class="koboSpan" id="kobo.1628.1">This </span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.1629.1">approach adopts the structure of Newton’s method, which is commonly used to locate the roots of a function, but in this case, it’s applied to the derivative of function </span><em class="italic"><span class="koboSpan" id="kobo.1630.1">f</span></em><span class="koboSpan" id="kobo.1631.1">. </span><span class="koboSpan" id="kobo.1631.2">The rationale behind this is that identifying the minimum point of function f  is equivalent to finding the root of its first derivative </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1632.1">f</span></span><span class="_-----MathTools-_Math_Operator_Extended_v-normal"><span class="koboSpan" id="kobo.1633.1">′</span></span><span class="koboSpan" id="kobo.1634.1">. </span><span class="koboSpan" id="kobo.1634.2">In this scenario, the updated formula can be expressed </span><span class="No-Break"><span class="koboSpan" id="kobo.1635.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1636.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1637.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1638.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1639.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1640.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1641.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1642.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1643.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1644.1">n</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1645.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1646.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1647.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1648.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1649.1">′</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1650.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1651.1">(</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1652.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1653.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1654.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1655.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1656.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1657.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1658.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1659.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1660.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1661.1">″</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1662.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1663.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1664.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1665.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1666.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1667.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1668.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.1669.1">In the given equation, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.1670.1">the following:</span></span></p>
<ul>
<li><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1671.1">f</span></span><span class="_-----MathTools-_Math_Operator_Extended_v-normal"><span class="koboSpan" id="kobo.1672.1">′</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1673.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1674.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1675.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1676.1">)</span></span><span class="koboSpan" id="kobo.1677.1"> represents the first derivative of </span><span class="No-Break"><span class="koboSpan" id="kobo.1678.1">function </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1679.1">f</span></em></span></li>
<li><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1680.1">f</span></span><span class="_-----MathTools-_Math_Symbol_Extended_v-normal"><span class="koboSpan" id="kobo.1681.1">″</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1682.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1683.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1684.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1685.1">)</span></span><span class="koboSpan" id="kobo.1686.1"> signifies the second derivative of </span><span class="No-Break"><span class="koboSpan" id="kobo.1687.1">function </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1688.1">f</span></em></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1689.1">Second-order methods are typically favored over gradient descent due to their faster convergence rate, provided that the analytical expressions for both first and second derivatives are available. </span><span class="koboSpan" id="kobo.1689.2">However, these methods converge regardless of whether they’re approaching minima or maxima. </span><span class="koboSpan" id="kobo.1689.3">Variations of this method exist that ensure global convergence and reduce computational costs by sidestepping the need to solve the system for determining the search direction using </span><span class="No-Break"><span class="koboSpan" id="kobo.1690.1">direct methods.</span></span></p>
<h1 id="_idParaDest-120"><a id="_idTextAnchor123"/><span class="koboSpan" id="kobo.1691.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1692.1">In this chapter, we gained insight into simulating typical human brain activities using ANNs. </span><span class="koboSpan" id="kobo.1692.2">We grasped the fundamental concepts behind ANNs, delving into the creation of a basic neural network architecture. </span><span class="koboSpan" id="kobo.1692.3">This exploration encompassed elements such as input, hidden, and output layers, connection weights, and activation functions. </span><span class="koboSpan" id="kobo.1692.4">Our understanding extended to crucial decisions regarding hidden layer count, node quantity within each layer, and network </span><span class="No-Break"><span class="koboSpan" id="kobo.1693.1">training algorithms.</span></span></p>
<p><span class="koboSpan" id="kobo.1694.1">Then we focused on data fitting and pattern recognition using neural networks. </span><span class="koboSpan" id="kobo.1694.2">We engaged in script analysis to master the utilization of neural network functions via the command line. </span><span class="koboSpan" id="kobo.1694.3">We then ventured into the Neural Network Toolbox, featuring algorithms, pre-trained models, and apps for crafting, training, visualizing, and simulating shallow and deep neural networks. </span><span class="koboSpan" id="kobo.1694.4">The Neural Network Toolbox offers an accessible interface—the </span><strong class="bold"><span class="koboSpan" id="kobo.1695.1">Neural Network</span></strong><span class="koboSpan" id="kobo.1696.1"> getting started GUI—which serves as the launchpad for tasks such as neural network fitting, pattern recognition, clustering, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1697.1">time-series analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.1698.1">Finally, we introduced some of the commonly used advanced optimization techniques. </span><span class="koboSpan" id="kobo.1698.2">These methods are potent techniques employed to amplify the efficiency and efficacy of optimization algorithms. </span><span class="koboSpan" id="kobo.1698.3">Their purpose is to surmount the constraints posed by conventional optimization methodologies, especially in intricate, high-dimensional, or non-convex </span><span class="No-Break"><span class="koboSpan" id="kobo.1699.1">optimization scenarios.</span></span></p>
<p><span class="koboSpan" id="kobo.1700.1">In the next chapter, we will understand the basic concepts of deep learning. </span><span class="koboSpan" id="kobo.1700.2">We also will learn about the different types of deep learning and understand </span><strong class="bold"><span class="koboSpan" id="kobo.1701.1">convolutional</span></strong><strong class="bold"> </strong><strong class="bold"><span class="koboSpan" id="kobo.1702.1">neural networks</span></strong><span class="koboSpan" id="kobo.1703.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1704.1">CNNs</span></strong><span class="koboSpan" id="kobo.1705.1">). </span><span class="koboSpan" id="kobo.1705.2">Additionally, we will learn how to build a CNN using MATLAB and understand recurrent neural networks, long short-term memory networks, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1706.1">transformer models.</span></span></p>
</div>
</body></html>