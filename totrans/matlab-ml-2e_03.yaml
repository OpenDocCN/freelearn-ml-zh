- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prediction Using Classification and Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification algorithms return accurate predictions based on our observations.
    Starting from a set of predefined class labels, the classifier assigns each piece
    of input data a class label according to the training model. Classification algorithms
    learn linear or non-linear associations between independent and categorical dependent
    variables. For example, a classification algorithm may learn to predict the weather
    as clear sky, gentle showers or heavy rain, and so on. Regression relates a set
    of independent variables to a dependent variable, numeric or continuous, for example,
    predicting rainfall in units of millimeters. Through this technique, it is possible
    to understand how the value of the dependent variable changes as the independent
    variable varies. This chapter shows us how to classify an object using nearest
    neighbors and how to perform an accurate regression analysis in a MATLAB environment.
    The aim of this chapter is to provide you with an introduction, background information,
    and a basic knowledge of classification and regression techniques and how to perform
    them in MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing classification methods using MATLAB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an effective and accurate classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring different types of regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making predictions with regression analysis in MATLAB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using advanced techniques for model evaluation and selection in MATLAB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce basic concepts relating to machine learning.
    To understand these topics, a basic knowledge of algebra and mathematical modeling
    is needed. A working knowledge of the MATLAB environment is also required.
  prefs: []
  type: TYPE_NORMAL
- en: 'To work with the MATLAB code in this chapter, you need the following files
    (available on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)):'
  prefs: []
  type: TYPE_NORMAL
- en: '`datatraining.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VehiclesItaly.xlsx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Employees.xlsx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AirfoilSelfNoise.xlsx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing classification methods using MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Classification** methods are an essential component of machine learning and
    data analysis. These methods allow us to categorize data into predefined classes
    or groups based on specific characteristics or attributes. By utilizing classification
    algorithms, we can train models to make predictions or assign labels to new, unseen
    data points. Classification plays a vital role in various domains, including image
    recognition, spam filtering, sentiment analysis, fraud detection, and medical
    diagnosis. It enables us to make informed decisions, identify patterns, and gain
    insights from data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are numerous classification algorithms available, each with its own strengths,
    assumptions, and applications. Some common classification methods include decision
    trees, **support vector machines** (**SVMs**), random forests, logistic regression,
    and naive Bayes classifiers. SVM has two variations: SVC for classification and
    SVR for regression. To effectively employ classification methods, we need to understand
    the underlying concepts, techniques, and evaluation metrics associated with them.
    Additionally, data preparation and feature engineering play crucial roles in ensuring
    accurate and reliable classification results.'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout our learning journey, we will explore various classification methods,
    learn how to implement them in the MATLAB environment, and understand the necessary
    steps for data preparation and model evaluation. By the end, you will be equipped
    with the knowledge and skills to apply classification techniques to your own datasets
    and extract valuable insights. Let’s start by exploring decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees for decision-making
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **decision tree** serves as a visual representation of a choice or decision
    being made. It captures the complexity of decision-making processes where the
    most interesting option may not always be the most useful, and situations may
    not always offer clear-cut choices. Often, decisions are determined by a series
    of conditional factors that need to be evaluated. Representing this concept solely
    through tables and numbers can be challenging, as the justification behind the
    decision is not immediately apparent.
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree structure helps us convey the same information with enhanced
    readability by emphasizing the specific branches that lead to the decision or
    evaluation. The technology of decision trees is valuable in identifying strategies
    or achieving goals by creating models with probable outcomes. The graphical representation
    of a decision tree immediately guides readers to comprehend the result. A visual
    representation is much more effective than a table filled with numbers. The human
    mind prefers to see the solution first and then trace back to understand the reasoning
    behind it, rather than being presented with a series of algebraic descriptions,
    percentages, and data to describe a result.
  prefs: []
  type: TYPE_NORMAL
- en: 'A decision tree is composed of the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nodes**: These contain the names of independent variables or factors involved
    in the decision-making process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Branches**: These are labeled with the possible values of the independent
    variables, representing different pathways or choices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leaf nodes**: These represent the classes or outcomes, where observations
    are grouped based on the values of an independent variable. Leaf nodes are connected
    to the nodes through the branches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using these tools, we assign labels to data and classes to represent the confidence
    level of the classification itself. The decision tree provides the probability
    or likelihood of belonging to a specific class, reflecting the level of confidence
    in the classification.
  prefs: []
  type: TYPE_NORMAL
- en: The process of classification begins with a set of pre-classified data known
    as the training set. The goal is to define rules that characterize different classes
    based on this data. Once the model is built, it is tested using a separate test
    set. The resulting descriptions or classes are then generalized through inference
    or induction. These generalized descriptions are then used to classify records
    whose membership class is unknown.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees offer a straightforward approach to classifying objects into
    a finite number of classes. They are constructed by recursively dividing the records
    into homogeneous subsets based on a target attribute, which must be categorical.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of classification rules: univariate and multivariate. **Univariate
    rules** consider a single predictor or target attribute at a time, while **multivariate
    algorithms** represent the predictor as a linear combination of variables. The
    process of subdivision results in a hierarchical tree structure, where subsets
    are referred to as nodes, and the final nodes are called leaf nodes. Nodes are
    labeled with the attribute name, branches are labeled with possible attribute
    values, and leaf nodes are labeled with different values representing the membership
    classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To classify an object using a decision tree, we use the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Begin at the root of the tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the instance attribute associated with the current node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Follow the branch corresponding to the attribute value assigned to the instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a leaf node is reached, return the label associated with that leaf. Otherwise,
    repeat from *step 2*, starting from the current node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By traversing the decision tree, the path taken represents the classification
    rules or production rules. The branches represent the values assumed by different
    attributes, and the leaves represent the classification outcomes. Each rule is
    written along the tree, from the node to the corresponding leaf. All possible
    paths in the tree represent the different classification rules.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, when classifying an instance using a decision tree, the process
    involves starting from the root, selecting the appropriate attribute at each node,
    following the corresponding branch based on the attribute value, and repeating
    these steps until a leaf node is reached, and then returning the associated label.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring decision trees in MATLAB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the MATLAB environment, the Statistics and Machine Learning Toolbox provides
    the necessary tools for constructing a classification tree from raw data. In this
    section, we will delve into a well-known example featured in many machine learning
    books—the iris flower dataset. Let’s explore how to handle this within the MATLAB
    environment. We already used this dataset in [*Chapter 2*](B21156_02.xhtml#_idTextAnchor040),
    *Working with Data* *in MATLAB.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Within this dataset, there are 50 samples belonging to each of three iris species:
    Iris setosa, Iris virginica, and Iris versicolor. Each sample consists of four
    measured features—sepal length, sepal width, petal length, and petal width—all
    measured in centimeters. The dataset includes the following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: Sepal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class: `setosa`, `versicolour`, `virginica`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our goal is to create a classification tree that can accurately classify flower
    species based on the size of their sepals and petals. Fortunately, there is no
    need to connect to the previously mentioned external archive to upload data to
    MATLAB’s workspace. MATLAB already includes a file containing the necessary data
    within its software distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To import this data, simply execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In MATLAB, two variables, namely `meas` and `species`, have been successfully
    imported. `meas` contains the measurements for sepal and petal length and width,
    forming a 150x4 double matrix. On the other hand, `species` represents the classification
    information and is structured as a 150x1 cell array. Now, let’s examine the distribution
    of the three species within the `species` variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After careful analysis, it has been determined that the sample is evenly distributed
    among the three species. To gain an overview of the listed floral species’ features,
    we can utilize a scatter plot matrix. This showcases the scatter plots of the
    species’ features in a convenient matrix format.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To effectively display the characteristics of the species in pairs and distinguish
    observations within their respective groups, we can leverage the `gplotmatrix()`
    function. This function is specifically designed to create a matrix of scatter
    plots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The variables we want to compare in pairs are contained in the `meas` variable,
    while the groups are contained in the species variable. Each set of axes in the
    resulting figure contains a scatter plot of a column of `meas` against a column
    of `meas`. All graphs are grouped to describe bivariate relationships between
    combinations of variables.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Figure 3**.1* depicts the scatter plot of the floral features for the three
    iris species.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Matrix of scatter plots grouped by species](img/B21156_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Matrix of scatter plots grouped by species
  prefs: []
  type: TYPE_NORMAL
- en: Upon a preliminary analysis of *Figure 3**.1*, it becomes evident that the `setosa`
    species' values are distinctly separated from the other two species, because the
    markers are located at various distances from the others. Conversely, the values
    of the other two species exhibit overlapping patterns across all plots.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s focus on investigating the variations in petal measurements across
    different species. To achieve this, we can utilize the two columns containing
    petal measurements, specifically the third and fourth columns. To visually represent
    the distribution of these measurements by species, we can employ a modified version
    of the scatter plot that we have used previously. This can be achieved using the
    `gscatter()` function, which generates a scatter plot grouped by a specified grouping
    variable. The function requires two vectors of the same size as arguments. The
    grouping variable must be provided in the form of a categorical variable, vector,
    character array, or cell array of character vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 3**.2* distinctly illustrates the distribution of the three floral
    species, each occupying distinct regions within the plot.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Scatter plot grouped by species](img/B21156_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Scatter plot grouped by species
  prefs: []
  type: TYPE_NORMAL
- en: These observations indicate that it is feasible to perform classification based
    on petal characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a classification tree, we can utilize the `fitctree()` function.
    This function generates a fitted binary decision tree by considering the input
    and output variables provided as inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Figure 3**.3* presents a graphical representation of the tree, depicting its
    branches and leaf nodes. Each node includes the conditions that must be met to
    traverse a particular branch.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Graphical description of the tree](img/B21156_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Graphical description of the tree
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3**.3* offers valuable insights into the classification of the three
    floral species, providing immediate information. In many cases, decision tree
    construction is primarily focused on predicting class labels or responses. Once
    a tree is constructed, it becomes straightforward to predict responses for new
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a scenario where a new combination of four data points, representing
    the length and width of the sepal and petal for a specific class of floral species,
    has been identified. We can use this data to predict the iris species.
  prefs: []
  type: TYPE_NORMAL
- en: 'To predict the classification for new data based on the previously created
    and trained decision tree named `ClassTree`, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `predict()` function returns a vector comprising the predicted class labels
    for the predictor data, which can be in the form of a table or matrix. These predictions
    are based on the trained classification tree. In the given case, only one prediction
    is made because the input variable contains a single record. If a data matrix
    with multiple observations were used, the function would produce a series of results
    equivalent to the number of rows in the data matrix.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Having built a classification tree from our data, the next step is to assess
    the model’s performance in predicting new observations. Various tools are available
    to measure the quality of the tree. A commonly used method is **resubstitution
    error**, which calculates the difference between the predicted responses and the
    actual responses in the training data. It serves as an initial estimate of the
    model’s performance but only provides insight in one direction. A high resubstitution
    error suggests that the tree’s predictions may not be accurate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To calculate the resubstitution error, you can use the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The low value obtained indicates that the classification tree accurately classifies
    a significant portion of the data. However, to enhance the assessment of the tree’s
    predictive accuracy, we can perform `crossval()` and `kfoldLoss()` for performing
    cross-validation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During cross-validation, the training data is divided randomly by default into
    10 parts. Subsequently, 10 new trees are trained, with each tree utilizing 9 parts
    of the data for training. The predictive accuracy of each new tree is then evaluated
    on the remaining data that was not used for training that particular tree. Unlike
    the resubstitution error, this approach provides a reliable estimate of the predictive
    accuracy of the resulting tree since it tests the new trees on fresh, unseen data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Performing cross-validation helps gauge the tree’s generalizability and offers
    a more robust measure of its predictive performance:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We used the `crossval()` function, which performs a loss estimate using cross-validation.
    A cross-validated classification model is returned. A number of properties are
    now available in MATLAB’s workspace.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To calculate the cross-validation loss, you can utilize the `kfoldLoss()` function
    in the following manner:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we calculated the classification loss for observations not used for training
    by using the `kfoldLoss()` function. The low calculated value confirms that the
    model works well.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `kfoldLoss()` function calculates the average loss across all folds during
    cross-validation. It provides a measure of the predictive accuracy of the classification
    tree on unseen data. By default, `kfoldLoss()` employs 10-fold cross-validation,
    where the data is randomly divided into 10 parts. Each part is then used as a
    testing set while the remaining data is used for training the tree.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using `kfoldLoss()` enables us to obtain a more reliable estimate of the tree’s
    predictive accuracy as it assesses the model’s performance on new and unseen data,
    rather than the training data itself.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At the start of this section, we listed several classification methods. In the
    following section, we will address SVM techniques and discriminant analysis for
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Building an effective and accurate classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification in machine learning is a supervised learning task that involves
    categorizing or classifying data into predefined classes or categories. It is
    one of the fundamental and widely used techniques in machine learning and data
    mining. The goal of classification is to develop a model or classifier that can
    accurately assign new, unseen instances to the correct class based on their features
    or attributes. The classifier learns patterns and relationships from a labeled
    training dataset, where each instance is associated with a known class label.
  prefs: []
  type: TYPE_NORMAL
- en: We will first discuss **SVMs**.
  prefs: []
  type: TYPE_NORMAL
- en: SVMs explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SVMs are powerful supervised machine learning algorithms used for classification
    and regression tasks. They are particularly effective in solving complex problems
    with a clear margin of separation between classes. SVMs can handle both linearly
    separable and non-linearly separable data by transforming the input space into
    a higher-dimensional feature space. The main idea behind SVMs is to find the best
    possible decision boundary, known as the *hyperplane*, that maximally separates
    the classes while minimizing classification errors. The hyperplane is determined
    by a subset of training examples called support vectors, which are the closest
    points to the decision boundary. SVMs employ a kernel function to map the input
    data into a higher-dimensional space, where linear separation is more feasible.
    This allows SVMs to solve non-linear problems by finding non-linear decision boundaries
    in the transformed feature space.
  prefs: []
  type: TYPE_NORMAL
- en: SVMs excel in tackling intricate problems where there exists a discernible gap
    between different categories. They possess the capacity to handle datasets that
    can be separated both linearly and non-linearly through a process involving the
    transformation of the initial input space into a higher-dimensional feature space.
    The fundamental concept underpinning SVMs revolves around the quest for the optimal
    decision boundary, known as the hyperplane. SVMs leverage a kernel function to
    project the input data into this higher-dimensional space, where achieving linear
    separation becomes more attainable. This distinctive capability empowers SVMs
    to address non-linear problems by identifying non-linear decision boundaries within
    the transformed feature space.
  prefs: []
  type: TYPE_NORMAL
- en: The training of an SVM involves optimizing a cost function that penalizes misclassified
    examples and maximizes the margin between the support vectors and the decision
    boundary. There are different variations of SVMs, such as **C-support vector machine**(**C-SVM**)
    for classification tasks and **epsilon-SVM** for regression tasks. SVMs are widely
    used in various domains, including image classification, text categorization,
    bioinformatics, and finance. They are known for their ability to handle high-dimensional
    data, handle outliers well, and generalize effectively to unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'An SVM-based classification technique enables the classification of both linear
    and non-linear datasets. In SVMs, the training data instances are represented
    on a plane with dimensions equal to the number of attributes in each instance.
    For instance, a three-dimensional plane is used to represent instances with three
    attributes. The three main components of an SVM classifier are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Lines or hyperplanes, which act as boundaries to classify instances into different
    classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Margins, which are the distances between the closest instances of different
    classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vectors, which are the instances within the hyperplane boundaries that
    are challenging to classify
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVMs separate data by finding an optimal hyperplane in a higher-dimensional
    feature space that maximizes the margin between different classes.
  prefs: []
  type: TYPE_NORMAL
- en: SVMs can handle linearly separable datasets, where instances can be separated
    by straight lines on the plane, as well as non-linearly separable datasets. In
    the case of linearly separable instances, the goal is to find the lines or hyperplanes
    that maximize the margin value. This selection minimizes the classification error.
  prefs: []
  type: TYPE_NORMAL
- en: For non-linearly separable datasets, the classification process is more complex.
    It involves two phases that build upon the previous approach.
  prefs: []
  type: TYPE_NORMAL
- en: In the first phase, instances are mapped to a higher-dimensional space to achieve
    linear separability. In the second phase, the previous approach is used to find
    a line or hyperplane that maximizes the margin, taking advantage of the now linearly
    separable instances.
  prefs: []
  type: TYPE_NORMAL
- en: To handle datasets that require non-linear functions for separation, **feature
    spaces** are used in SVMs. This technique involves mapping the initial data to
    a higher-dimensional space. If the initial data has *m* dimensions and the feature
    space has *n* dimensions, with *m > n*, a mapping function is applied. Suppose
    we have an input denoted with *x* and *y*. The concept of kernel spaces is particularly
    useful for algorithms that rely on training data solely through scalar products.
    In this case, instead of explicitly finding *f(x)* and *f(y)* in the *m*-dimensional
    space, we only need to calculate their scalar product, denoted as *f(x) ·* *f(y)*.
  prefs: []
  type: TYPE_NORMAL
- en: The function *f* is employed to map the input from the original *n*-dimensional
    space to a higher-dimensional *m*-space. To simplify this computation, especially
    in large spaces, a kernel function is utilized. The kernel function directly provides
    the scalar product of the images, eliminating the need for explicit mapping and
    making the calculation more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: K(x, y) = f(x) · f(y)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x* and *y* are input vectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f* is a transformation function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dot product operation, denoted by *f(x) · f(y)*, calculates the dot product
    of the transformed input vectors. It allows us to efficiently compute the scalar
    product of the mapped input vectors without explicitly calculating the mapping
    function, *f(x)* and *f(y)*. This dot product operation is an essential component
    in various algorithms that utilize kernel methods for classification and regression
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the **kernel function** is to transform the input data into a
    suitable form, particularly when it is not feasible to determine a linearly separable
    hyperplane, which is often the case. There are several commonly used kernels,
    including **linear**, **polynomial**, **radial basis function** (**RBF**), and
    **sigmoid**.
  prefs: []
  type: TYPE_NORMAL
- en: In this methodology, the training phase and the subsequent error evaluation
    activity play a crucial role. To accomplish this, the data is split into two subsets,
    named training and testing. The **training set** is utilized for algorithm training
    and comprises labeled inputs and outputs for supervised learning. Typically, it
    consists of approximately 80% of the total data. The **testing set**, on the other
    hand, is used to evaluate the accuracy of the SVM. It contains 20% of the remaining
    data that was not used in training, and it helps measure the prediction error.
    This phase also serves to test the algorithm in real-world scenarios, simulating
    the actual usage of the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: One notable advantage of SVM-based classifiers is their capability to handle
    complex and non-linear classification problems while ensuring a high level of
    accuracy. However, for simpler problems, the accuracy may be comparable to that
    of decision tree-based classification techniques. Some drawbacks of SVMs include
    the relatively longer time required for model creation, although it is still faster
    compared to neural networks. The other drawback is the lack of interpretability
    of the model’s inner workings.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised classification using SVM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supervised classification is a machine learning technique used to classify or
    categorize data into predefined classes or categories. Supervised classification
    relies on labeled training data, in which data points are linked to a specific
    class. The objective of supervised classification is to construct a model capable
    of making precise predictions regarding the class of previously unseen or fresh
    data points, utilizing their inherent features as guidance.
  prefs: []
  type: TYPE_NORMAL
- en: To understand how to implement the SVM algorithm in MATLAB, we will use a dataset
    for binary classification. **Binary classification** is a specific type of supervised
    classification where the goal is to classify data into one of two mutually exclusive
    classes or categories. The two classes are often referred to as the positive class
    and the negative class, or class 1 and class 0\. In binary classification, the
    labeled training data consists of examples where each data point is associated
    with one of the two classes. The machine learning algorithm is then trained on
    this labeled data to learn a model that can accurately predict the class of unseen
    data points.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset consists of features such as `Temperature`, `Humidity`, `Light`,
    and `CO2`, and the target variable is room occupancy. The ground-truth occupancy
    information was obtained by capturing time-stamped pictures every minute.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this experiment is to train a binary classification model that can
    predict whether the room is occupied or unoccupied based on the given sensor data.
    The features (`Temperature`, `Humidity`, `Light`, `CO2`, and `Humidity Ratio`)
    are used as input to the model, and the corresponding occupancy status (`occupied`
    or `unoccupied`) serves as the target variable for training and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'To download the dataset and a short summary of the variables contained within
    it, please refer to UCI Machine Learning Repository at the following link: [https://doi.org/10.24432/C5X01N](https://doi.org/10.24432/C5X01N).'
  prefs: []
  type: TYPE_NORMAL
- en: By analyzing the relationship between the sensor data and the ground-truth occupancy,
    the model aims to learn patterns and make accurate predictions on unseen data.
    The training process involves feeding the labeled data into a binary classification
    algorithm, using SVM. The algorithm learns from the features and their corresponding
    occupancy labels to create a model that can classify new instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the data into the MATLAB environment. To do that, we
    will use the `readtable()` function, as we learned in [*Chapter 2*](B21156_02.xhtml#_idTextAnchor040),
    *Working with Data in MATLAB*, in the *Importing data into MATLAB* section. We
    need to use the `datatraining.txt` file, containing eight variables (`Num`, `date`,
    `Temperature`, `Humidity`, `Light`, `CO2`, `HumidityRatio`, and `Occupancy`).
    But for our purpose, only six variables are needed, that is, the last six. So,
    we have to set some options to use in the function call, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Only the last six variables have been selected.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we are ready to import the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see, in the MATLAB workspace, a new variable named `DataMatrix` as a
    table of size 814x6\. This table contains the five predictors (`Temperature`,
    `Humidity`, `Light`, `CO2`, and `HumidityRatio`) and one dichotomic response (`Occupancy`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To train an SVM classifier for binary classification problems, we can use the
    `fitcsvm()` function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `DataMatrix(:,1:5)` parameter represents the predictor data, which is an
    8,143-by-5 matrix, where 8,143 is the matrix’s dimension consisting of the number
    of observations, denoting data points, and 5 signifies the count of predictor
    variables, or features. Each row within this matrix represents a distinct observation,
    while each column pertains to an individual predictor variable. The `DataMatrix(:,6)`
    parameter represents the response data, which is an 8,143-by-1 vector. For binary
    classification, this vector should be a binary vector of 0s and 1s, where 0 represents
    the negative class and 1 represents the positive class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The function returns an `SVMClassifier` object, which represents the trained
    SVM classifier. This object contains information about the trained model, such
    as support vectors, coefficients, and kernel parameters. We can print some of
    this information simply by typing the name of the model into the MATLAB command
    prompt:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `fitcsvm()` function applies the linear kernel by default. It’s possible
    to set a different kernel using the `KernelFunction` parameter. The supported
    kernels are the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Linear**: This is a commonly used kernel function. It assumes that the data
    can be separated by a linear decision boundary or hyperplane. The linear kernel
    computes the dot product between the input feature vectors, which effectively
    measures the similarity between the samples in the original feature space.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial**: This can be used to handle non-linearly separable data. It
    maps the input feature vectors into a higher-dimensional space using polynomial
    functions, which allows for the learning of non-linear decision boundaries. This
    kernel requires setting the degree of the polynomial and the scale factor.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaussian or RBF**: This can handle non-linearly separable data by mapping
    the input feature vectors into an infinite-dimensional feature space. The RBF
    kernel is particularly effective when the decision boundary is complex or not
    well defined. This kernel requires setting the kernel scale or gamma parameter,
    which determines the influence of each training example.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calculating the resubstitution error helps us gauge the performance of the
    method employed. As mentioned earlier, this error is determined by measuring the
    disparity between the predicted and actual responses in the training data. While
    it offers an initial estimate of the model’s performance, it only provides insights
    in one direction. A high resubstitution error implies that the predictions made
    by the tree model may not be accurate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on our analysis, we have encountered an error of at least 15%. This suggests
    that there is room for improvement in the performance of the classifier. It is
    likely that the data cannot be effectively separated using a linear plane, indicating
    the need to modify the kernel function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To improve the performance of the model, we can choose an SVM classifier based
    on the RBF kernel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We initially had an error rate of 15%, but we have successfully reduced it to
    0.61%. This significant improvement indicates that the model is robust in handling
    non-linearity in the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have introduced classification, it is time to explore the world
    of regression, which allows us to work with continuous numerical values.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring different types of regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Regression analysis** is a statistical method used to examine the connection
    between a group of independent variables (also known as explanatory variables)
    and a dependent variable (referred to as the response variable). By employing
    this technique, it becomes possible to comprehend how the value of the response
    variable fluctuates when the explanatory variable is altered.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regression analysis serves a dual purpose: explanatory and predictive. The
    **explanatory** role helps us understand and assess the impact of independent
    variables on the dependent variable based on a specific theoretical model. It
    allows us to quantify the relationship and determine the magnitude and significance
    of the effects. In the **predictive** role, regression analysis aims to identify
    the optimal linear combination of independent variables to predict the value of
    the dependent variable accurately. By utilizing this technique, we can make predictions
    based on the observed relationships between variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To characterize the relationship between variables, we can employ a mathematical
    function that captures the observed behavior, interpolates the data, and represents
    its underlying trend while retaining its key information. Linear regression is
    a method that specifically aims to identify a line that can effectively represent
    the distribution of points in a two-dimensional plane. When the observed points
    closely align with the line, it indicates that the chosen model accurately describes
    the relationship between the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'While there are theoretically infinite lines that can interpolate the observations,
    in practice, only one mathematical model can optimize the data representation.
    In the case of a linear mathematical relationship, the observations of the variable
    *y* can be derived from a linear function of the observations of the variable
    *x*. For each observation, this relationship can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y = α * x+ β
  prefs: []
  type: TYPE_NORMAL
- en: In the given formula, *x* represents the explanatory variable, and *y* represents
    the response variable. The parameters *α* and *β* correspond to the slope of the
    line and the intercept with the *y* axis, respectively. These parameters need
    to be estimated using the collected observations for both variables included in
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Of particular interest is the slope *α*, which signifies the change in the average
    response for each unit increase in the explanatory variable. What happens when
    this coefficient changes? If the slope is positive, the regression line ascends
    from left to right, indicating that the response increases as the explanatory
    variable increases. Conversely, if the slope is negative, the line descends from
    left to right, suggesting that the response decreases with an increase in the
    explanatory variable. When the slope is 0, it implies that the explanatory variable
    has no effect on the response value.
  prefs: []
  type: TYPE_NORMAL
- en: However, the significance of the relationship between the variables is not solely
    determined by the sign of *α*; its value is also crucial. In the case of a positive
    slope, the average response is higher when the explanatory variable is higher.
    Conversely, for a negative slope, the average response is lower when the explanatory
    variable is higher. The magnitude of the slope plays a vital role in understanding
    the strength and nature of the relationship between the variables.
  prefs: []
  type: TYPE_NORMAL
- en: In MATLAB, you can perform simple linear regression using the built-in functions
    and tools available in the Statistics and Machine Learning Toolbox. Another approach
    to perform simple linear regression in MATLAB is by using the `polyfit()` and
    `polyval()` functions. These functions allow you to fit data to a pattern that
    is linear in the coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression model in MATLAB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A convenient method to construct a linear regression model in MATLAB is by employing
    the `fitlm()` function, which is a part of the Statistics and Machine Learning
    Toolbox. This function offers a straightforward approach to creating and analyzing
    linear regression models.
  prefs: []
  type: TYPE_NORMAL
- en: The `fitlm()` function generates a `LinearModel` object in MATLAB. This object
    contains various properties that can be easily examined by simply selecting it.
    Additionally, the `LinearModel` object offers several methods, such as `plot`,
    `plotResiduals`, and `plotDiagnostics`, which facilitate creating plots and conducting
    diagnostic analysis.
  prefs: []
  type: TYPE_NORMAL
- en: A `LinearModel` object encapsulates training data, model description, diagnostic
    information, and fitted coefficients for a linear regression. By default, when
    using the `fitlm()` function, the last variable in a table or dataset array is
    considered the response variable. Alternatively, you can specify the predictors
    and response variables explicitly using a formula. Moreover, you have the flexibility
    to designate a specific column as the response variable by utilizing the `ResponseVar`
    name-value pair argument. To define a set of columns as predictors, you can employ
    the `PredictorVars` name-value pair argument. The predictor variables can be numeric
    or of any grouping variable type, such as logical or categorical. However, the
    response variable must be numeric or logical in nature.
  prefs: []
  type: TYPE_NORMAL
- en: 'To gain insight into the functionality of the `fitlm()` function, let’s consider
    a dataset that includes information on the number of vehicles registered in Italy
    and the population of various regions. The dataset includes the following fields:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Names of Italian regions (`Region`)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Vehicle registrations for each region (`Registrations`)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Resident population in each region (`Population`)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s start by importing the data into a table:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To fit a linear regression model with the `Registrations` variable as the response
    and `Population` as the explanatory variable (predictor), we can use the `fitlm()`
    function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following information is printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results of the linear regression model include the model formula and estimated
    coefficients. Each term in the model is represented by a row, and the following
    columns provide additional information:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Estimate`: The estimated coefficient value for each corresponding term in
    the model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SE`: The standard error of the estimate.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tStat`: The t-statistic for each coefficient, which tests the null hypothesis
    that the coefficient is 0 against the alternative that it is different from 0,
    given the other predictors in the model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pValue`: The p-value for the F-statistic of the hypothesis test that the coefficient
    is equal to 0\. In our example, a p-value lower than 0.05 indicates that the term
    is significant at the 5% significance level, given the other terms in the model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretation of the intercept adds more meaning to the problem statement,
    as an average 70,549 registrations are happening in the city.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also explain the slope (0.59212) as with every 1,000-person increase
    in population, registrations increase by 592, with approximately 6 out of every
    10 people having vehicles.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Registrations` response variable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Adjusted R-squared**: A modified version of R-squared that takes into account
    the number of predictors in the model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**F-statistic versus constant model**: The test statistic for the F-test on
    the regression model, which tests for a significant linear regression relationship
    between the response variable and the predictor variables.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**p-value**: The p-value for the F-test on the model. In our example, the model
    is significant with a very low p-value of 1.03e-21.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s examine the results of the last MATLAB command. Two values stand out
    among the others: R-squared and p-value. The calculated R-squared value is very
    high, equal to 0.994\. This indicates that a substantial variation in the response
    variable can be explained by the predictor variable. On the other hand, the p-value
    is very small, but understanding its significance requires further exploration.
    During a statistical significance test, we start by assuming the **null hypothesis**,
    which states that there is no difference between the groups with respect to the
    parameters being considered. Under the null hypothesis, any observed differences
    are attributed to chance.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we must make a decision, whether to accept or reject the null hypothesis.
    To make this determination, we analyze our data using a significance test. If
    the test *suggests* rejecting the null hypothesis, we declare the observed difference
    as statistically significant. Conversely, if the test *advises* accepting the
    null hypothesis, the difference is deemed statistically not significant. The results
    of a statistical test always come with a certain level of uncertainty and probability.
    Thus, a decision to reject the null hypothesis is likely correct but may also
    be incorrect. Assessing the risk of making an error is known as the **significance
    level** of the test.
  prefs: []
  type: TYPE_NORMAL
- en: This significance level, also known as the p-value, provides a quantitative
    estimate of the probability that the observed differences are due to chance. The
    **p-value** is a probability and can only assume values between 0 and 1\. A p-value
    approaching 0 indicates a low probability that the observed difference is due
    to chance. Researchers typically choose a significance level of 0.05 (5%) or 0.01
    (1%). In our case, we calculated a p-value of 1.03e-21, which is far below the
    chosen significance level. This suggests that the observed difference is statistically
    significant and not likely due to chance.
  prefs: []
  type: TYPE_NORMAL
- en: After having seen the first example of linear regression that deals with continuous
    predictors, it is necessary to see how to deal with the case in which the type
    of predictors is different.
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions with regression analysis in MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having explored numerous instances of linear regression, we can confidently
    assert that we comprehend the underlying mechanisms of this statistical method.
    Non-linear regression is used to model the relationship between a dependent variable
    and one or more independent variables when the relationship is not linear. In
    contrast to linear regression, where the relationship is assumed to be a straight
    line, non-linear regression allows for more complex and flexible relationships
    between variables.
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we have exclusively employed continuous variables as predictors.
    However, what transpires when the predictors are categorical variables? No need
    to fret, as the fundamental principles of regression techniques remain unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple linear regression with categorical predictor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Categorical variables differ from numerical ones as they do not stem from measurement
    operations but rather from classification and comparison operations. Categorical
    variables can be categorized into nominal, dichotomous, or ordinal groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s delve into a real-life scenario. Within a company, we have diligently
    gathered information on employee salaries, which are determined by their years
    of experience. Our objective is to construct a model that enables us to track
    the progression of an employee’s salary over time. We have categorized the employees
    into three types: Management, Technical Staff, and General Staff.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see practically how to process the MATLAB code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin our analysis, we will import the relevant data from an Excel worksheet
    named `employees.xlsx` into MATLAB:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can check the type of the variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So, the variable is corrected and recognized by MATLAB.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, it is time to create the regression model by applying the `fitlm()` function
    using `Salary` as the dependent variable, and `YearsExperience` and `LevelOfEmployee`
    as the independent variables. Given that `LevelOfEmployee` is a categorical variable
    with three levels (`Management`, `TechnicalStaff`, and `GeneralStaff`), it is
    represented in the model as two indicator variables. In MATLAB, categorical predictors
    are typically included as dummy indicator variables. An indicator variable takes
    on values of 0 or 1\. For a categorical variable with *n* categories, it can be
    represented by *n – 1* indicator variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To account for the distinctions between the different employee types, we can
    incorporate interaction terms between `YearsExperience` and `LevelOfEmployee`.
    This allows us to capture the interaction effects between years of experience
    and the specific employee category:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s see the characteristics of the model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Regression model summary](img/B21156_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Regression model summary
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the results, the model equation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In this equation, the term `LevelOfEmployee(GeneralStaff)` is not included because
    the first level, by default, serves as the reference group. However, the first-order
    terms for `YearsExperience` and `LevelOfEmployee`, along with all the interactions,
    are present.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is apparent that a single equation for the entire system is insufficient
    to obtain accurate wage estimates. To address this, we need to differentiate between
    the three categories of employees and create separate models for each. Consequently,
    we obtain the following three equations to capture the wage dynamics for each
    employee category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: From a simple comparison between the three equations we have just seen, we can
    see that the intercept progressively increases with the category of employees
    (`20.2;20.2 + 10.4; 20.2 + 30.2`); this shifts the regression line upward. Similarly,
    the slope also progressively increases with the employee category (`0.25 ; 0.25
    + 0.24; 0.25 + 0.49`), leading to a greater increase in salary as the number of
    years of experience increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enhance our understanding of the progress made thus far, let’s incorporate
    the following lines into the scatter plot of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The results are shown in *Figure 3**.5*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the previously suggested code, we first created a linearly spaced vector
    between the minimum and maximum values of the `YearsExperience` variable using
    the `linspace()` function. Subsequently, we generated a scatter plot depicting
    the relationship between employee salaries (`Salary`) and years of experience
    (`YearsExperience`), with the data points grouped by the level of employee (`LevelOfEmployee`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Scatter plot with the three straight lines that fit three data
    groups](img/B21156_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Scatter plot with the three straight lines that fit three data
    groups
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we added the three lines representing the respective trends for each
    employee category. This was accomplished by utilizing the `feval()` function to
    evaluate the model at the specified points in the `Xvalues` variable. It is now
    evident that the three straight lines, corresponding to the three equations, are
    distinguished by both their intercepts and slopes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After having analyzed two regression examples in detail, we can see how to evaluate
    the performance of the models created so far.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Model performance** refers to how well a model fits the given data and accurately
    predicts outcomes. It is important to evaluate model performance to assess its
    reliability and effectiveness in making predictions or in capturing the underlying
    patterns in the data. One commonly used metric to evaluate model performance is
    the R-squared value, also known as the coefficient of determination. R-squared
    measures the proportion of the variance in the dependent variable that can be
    explained by the independent variables in the model. A higher R-squared value
    indicates a better fit, as it means a larger proportion of the variability in
    the data is accounted for by the model.'
  prefs: []
  type: TYPE_NORMAL
- en: However, R-squared alone may not provide a complete picture of model performance.
    Other metrics, such as **mean squared error** (**MSE**) or **mean absolute error**
    (**MAE**), can be used to assess the average prediction error of the model. Lower
    values of MSE or MAE indicate better predictive performance. Furthermore, it is
    important to consider the context and specific requirements of the problem at
    hand. For example, if the residuals (the differences between the predicted and
    actual values) exhibit certain patterns or non-random behavior, it may indicate
    that the model is missing important factors or violating assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, cross-validation techniques can be used to further evaluate the
    model’s performance by assessing its ability to generalize to new, unseen data.
    This helps to ensure that the model is not overfitting the training data and can
    make accurate predictions on new observations. In summary, model performance is
    assessed through various metrics, including R-squared, MSE, MAE, and consideration
    of the residuals and cross-validation results. Evaluating these aspects provides
    a comprehensive understanding of how well the model performs and its suitability
    for the given task.
  prefs: []
  type: TYPE_NORMAL
- en: In the next example, we will exploit the evaluation metrics to improve the performance
    of the model by identifying the outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing outlier effects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, the `fitlm()` function generates a `LinearModel` object
    that contains valuable information about the linear regression, including training
    data, model description, diagnostic details, and estimated coefficients. Now,
    we can utilize some of these properties to gain additional insights from the model.
    The least-squares method is commonly employed when we have sufficient knowledge
    about the model’s shape and aim to determine its parameters. It is also useful
    for model exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this method requires manual inspection of the data to identify and
    handle outliers. Therefore, we will now examine whether there are any outliers
    in the data that should be excluded from the fitting process. Residual plots can
    aid us in this analysis. The most used plots include the default histogram plot,
    which displays the range and frequencies of the residuals, and the probability
    plot, which compares the distribution of the residuals to a normal distribution
    with a similar variance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start as usual by importing the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we can work on the evaluation of the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To start, we extract some different evaluation metrics of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: MSE quantifies the mean squared disparity between the model’s predicted values
    and the actual observed values. The MSE is calculated by taking the average of
    the squared differences between the predicted values and the true values over
    the entire dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The R-squared and MSE values obtained will be used to compare the performance
    of the different models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To show how residual plots are useful for identifying outliers, we will refer
    to the example used in the *Linear regression model in MATLAB* section. We will
    use the same model (`LRModel`). To visualize the residuals, we will utilize a
    specific property of the `LinearModel` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Upon analyzing the histogram obtained, a distinct asymmetry is evident in the
    negative values. Specifically, the observations near `-2 *105` appear to be potential
    outliers. To achieve a more accurate fit, we will construct a probability plot.
    As previously mentioned, the plot in *Figure 3**.6* demonstrates the comparison
    between the distribution of residuals and a normal distribution with similar variance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The probability plot of residuals also reveals potential outliers, particularly
    in the lower left region. We can observe three values that significantly deviate
    from the dotted line. On the other hand, the probability plot appears reasonably
    linear for the remaining residual values, indicating a reasonable fit to normally
    distributed residuals.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Histogram of residuals and probability plot of residuals for
    the linear regression model](img/B21156_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Histogram of residuals and probability plot of residuals for the
    linear regression model
  prefs: []
  type: TYPE_NORMAL
- en: 'We can identify these outliers and proceed to remove them from the dataset.
    To locate them, we can utilize the `find()` function, examining values to the
    left of the abscissa equal to `-1.5*105`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Currently, we have the outliers detected in the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, at this stage, we can proceed to create the model once again, this time
    excluding the aforementioned outlier values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following values are printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model’s performance has noticeably improved, as indicated by the increased
    R-squared value of `0.997` compared to the previous model’s R-squared value of
    `0.994`. This improvement suggests that the new model provides a better fit to
    the data. Now we will extract the MSE:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From a comparison, we can see an improvement; the MSE decreased from `1.3407e+10`
    to `6.8047e+09`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To further evaluate the model’s performance, we can assess other metrics, such
    as the MSE or MAE. Additionally, conducting cross-validation or assessing the
    residuals’ distribution and patterns can provide further insights into the model’s
    effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Having discussed the utilization of evaluation metrics to enhance model performance,
    it is now opportune to delve into these methodologies and strive toward constructing
    increasingly effective models. By employing these techniques, we aim to improve
    the overall performance and predictive capabilities of our models.
  prefs: []
  type: TYPE_NORMAL
- en: Using advanced techniques for model evaluation and selection in MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model evaluation and selection are crucial steps in machine learning to ensure
    the chosen model performs well on unseen data and generalizes effectively. When
    it comes to advanced techniques for model evaluation and selection in MATLAB,
    there are several approaches you can consider.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent sub-section, we will take a look at the most important techniques
    for model evaluation and selection.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding k-fold cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**K-fold cross-validation** is a widely used technique for model evaluation
    and selection. It involves partitioning the dataset into *k* equally sized subsets
    or folds. The model undergoes training and assessment in *k* iterations, with
    each iteration employing a distinct fold as the validation set while using the
    remaining folds as the training set. The outcomes of each iteration are then averaged
    to derive a comprehensive performance estimation. This is the essence of how k-fold
    cross-validation operates:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Splitting the dataset**: Partition the dataset into *k* non-overlapping folds.
    Generally, the value of *k* falls within the range of 5 to 10, although it can
    be adjusted based on factors such as dataset size and complexity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training and evaluating the model**: This entails a repetitive process repeated
    *k* times. Within each iteration, the model is trained using *k – 1* folds and
    subsequently assessed for its performance on the remaining fold.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Performance metric aggregation**: Calculate the performance metric of interest
    for each iteration. The metrics from all iterations are then averaged to obtain
    a robust estimation of the model’s performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model selection**: Use the average performance metric to compare and select
    the best model among different algorithms or hyperparameter configurations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K-fold cross-validation helps address the limitations of single train-test splits
    by providing a more reliable estimate of a model’s performance. It helps assess
    how well the model generalizes to unseen data and reduces the risk of overfitting
    or underfitting. MATLAB provides built-in functions, such as `crossval()` and
    `cvpartition()`, to facilitate k-fold cross-validation. These functions automate
    the process and allow for easy implementation and evaluation of models using k-fold
    cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how we can practically approach a k-fold cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the use of cross-validation, we employ a NASA dataset derived
    from a sequence of aerodynamic and acoustic experiments conducted in an anechoic
    wind tunnel. The dataset comprises several fields, including the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Frequency, in Hertz (named `FreqH`).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Angle of attack, in degrees (named `AngleD`).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Chord length, in meters (named `ChLenM`).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Free-stream velocity, in meters per second (named `FStVelMs`).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Suction side displacement thickness, in meters (named `SucSDTM`).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaled sound pressure level, in decibels (named `SPLdB`).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As usual, we start importing the dataset into the MATLAB environment:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So, data is now available in the MATLAB workspace. To apply cross-validation
    in our model, we can use the apps available in the MATLAB environment, for example,
    the Regression Learner app. This app facilitates a streamlined and efficient process
    for conducting step-by-step regression analysis. With this app, importing and
    exploring data, selecting features, defining validation schemes, training models,
    and evaluating results becomes remarkably straightforward and swift. The app offers
    automated training functionality, enabling the search for the optimal regression
    model type. It includes options such as linear regression models, regression trees,
    Gaussian process regression models, support vector regression, and an ensemble
    of regression trees. Additionally, the trained model can be exported to the workspace
    for reuse with new data or to generate MATLAB code for programmatic regression.
    By leveraging the Regression Learner app, users can save time and effort in performing
    regression analysis, benefiting from its intuitive interface and powerful features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To open the app, select the **APPS** tab on the MATLAB toolstrip, and click
    on the **Regression Learner** icon. The Regression Learner app window will open,
    as shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Regression Learner app window](img/B21156_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Regression Learner app window
  prefs: []
  type: TYPE_NORMAL
- en: 'To import data from the MATLAB workspace into the Regression Learner app, follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Regression Learner app by clicking on the **New Session** button located
    in the **File** section of the **Regression** **Learner** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **New Session** dialog box will appear, containing three sections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Step 1*: Select a table or matrix. In this section, choose the source of your
    data. You can select a table or a matrix containing the data you want to analyze.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Step 2*: Select predictors and a response. In this section, specify the variables
    that will serve as predictors and the variable that will be the response variable.
    This allows you to define the type of variables involved in the analysis. In this
    case, we can select **SPLdB** as the response and the other variables as predictors.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Step 3*: Define a validation method. In this section, you can choose the type
    of validation method to evaluate the predictive accuracy of the fitted models.
    Different validation methods, such as cross-validation or holdout validation,
    can be selected to estimate model performance on new data.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Validation methods play a crucial role in assessing how well the fitted models
    can make accurate predictions on unseen data. The Regression Learner app provides
    tools to evaluate and compare models based on their estimated performance, enabling
    the selection of the best model for the given data.
  prefs: []
  type: TYPE_NORMAL
- en: We will choose `5` by default), then we can press the **Start** **Session**
    button.
  prefs: []
  type: TYPE_NORMAL
- en: 'To select the desired model type in the Regression Learner app, follow these
    steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expand the **Model Type** section by clicking on the arrow icon present in the
    app.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A list of available regression models will be displayed, including the following:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Linear** **regression models**'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression trees**'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SVMs**'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaussian process** **regression models**'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensembles** **of trees**'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To get started, select the **All Quick-To-Train** option. This option allows
    you to select all the models available for this type of problem, so it will be
    possible to train the models using default settings quickly.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Train** icon to initiate the training process. The app will start
    training the selected models, and a selection of model types will appear in the
    **History** section.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the models finish training, the best RMSE score will be highlighted in
    a box. This score indicates the model’s performance.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To further improve the model’s performance, you can try training with all available
    algorithms. Click on **All** and then click **Train**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: By comparing the results in the **History** section, you can observe the RMSE
    scores for each model. The lower RMSE score represents better performance. In
    the example provided, the Gaussian process regression model achieved the *lowest
    RMSE score (RMSE = 1.49)*, while the boosted trees model obtained the *highest
    RMSE score (RMSE =* *6.18)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To assess the improvements obtained, you can compare the predicted versus actual
    plots for the extreme models. In the model with the lowest RMSE, the data points
    should closely align with the reference line, indicating a close match between
    the predicted and actual data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Overall, the Regression Learner app provides a visual representation of the
    results, allowing you to easily compare and analyze the performance of different
    models. Let’s see another way to select the data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring leave-one-out cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Leave-one-out cross-validation** (**LOOCV**) is a specific type of cross-validation
    technique used for model evaluation. In LOOCV, each data point is sequentially
    held out as the validation set while the rest of the data is used for training
    the model. This process is repeated for all data points, and the performance of
    the model is evaluated by averaging the results across all iterations. The LOOCV
    technique works in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each data point in the dataset, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the data point from the training set
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the model using the remaining data points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the trained model to predict the removed data point
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the performance of the model by comparing the predicted value with
    the actual value of the removed data point
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the performance metric (such as MSE or accuracy) for each iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the average performance metric across all iterations to obtain an overall
    estimation of the model’s performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LOOCV is particularly useful when the dataset is small or when there is a limited
    amount of data available. Since each data point serves as a validation set once,
    LOOCV provides a more reliable estimate of the model’s performance compared to
    other cross-validation techniques. It is worth mentioning that this technique
    is good for small datasets, as it is difficult to judge its variance error as
    the data points are almost the same for each fold; it just means one new record
    in each fold.
  prefs: []
  type: TYPE_NORMAL
- en: In MATLAB, you can use the `cvpartition()` function with the `LeaveOut` option
    to generate indices for performing LOOCV. These indices can then be used to partition
    the data for training and validation purposes in a loop.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will introduce the bootstrap method.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the bootstrap method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The bootstrap method is a resampling technique used for estimating the sampling
    distribution of a statistic. This process entails generating multiple bootstrap
    samples by randomly selecting data points from the original dataset with replacement,
    from which estimates of the statistic can be obtained. This method allows us to
    assess the variability and uncertainty associated with the statistic of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'The bootstrap algorithm provides the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with an original dataset of size *N*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample *N* data points from the original dataset with replacement to
    create a bootstrap sample. This means that each data point in the bootstrap sample
    is selected independently, and it is possible to select the same data point multiple
    times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the desired statistic on the bootstrap sample. This could be the mean,
    median, standard deviation, or any other statistic of interest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* and *3* a number of times equal to *B*, where *B* is the number
    of bootstrap iterations desired.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect the computed statistics from each bootstrap sample to create the bootstrap
    distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the desired confidence intervals or standard errors based on the bootstrap
    distribution to quantify the uncertainty around the statistic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The bootstrap method provides a robust approach for estimating the sampling
    distribution when the underlying distribution of the data is unknown or when analytical
    methods are not readily available. It can be used for various purposes, such as
    hypothesis testing, constructing confidence intervals, and assessing model stability.
  prefs: []
  type: TYPE_NORMAL
- en: The bootstrap method holds significance as it enables the estimation of a model’s
    performance attributes, encompassing measures such as central tendency and uncertainty,
    all without requiring stringent assumptions about the underlying data distribution.
    This method enhances the reliability and robustness of assessing a model’s performance,
    particularly in scenarios where data is scarce or the data distribution remains
    poorly defined.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have gained valuable insights into performing accurate classification
    tasks within the MATLAB environment. We began by delving into the realm of decision
    tree methods, where we familiarized ourselves with key concepts such as nodes,
    branches, and leaf nodes. By repeatedly dividing records into homogeneous subsets
    based on the target attribute, we learned how to classify objects into distinct
    classes effectively. Moreover, we explored the prediction aspect of SVMs, which
    are particularly effective in solving complex problems with a clear margin of
    separation between classes. SVMs can handle both linearly separable and non-linearly
    separable data by transforming the input space into a higher-dimensional feature
    space.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent section, our focus shifted toward conducting precise regression
    analysis within the MATLAB environment. We commenced by delving into simple linear
    regression, gaining an understanding of its definition and the process of obtaining
    ordinary least squares estimation. Additionally, we explored multiple techniques
    for quantifying the intercept and slope of the linear relationship.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, we unearthed the linear regression model builder, a valuable tool
    for constructing an object encompassing training data, model description, diagnostic
    information, and fitted coefficients necessary for linear regression. Furthermore,
    we familiarized ourselves with the correct interpretation of simulation results
    and grasped the techniques to mitigate the influence of outliers through robust
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we understood how to use the tools available in MATLAB to perform an accurate
    evaluation of the model trained. Finally, we discovered the cross-validation methods
    to increase the performance of the model, finding the best-performing model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the clustering methodology to find hidden
    patterns or groupings in a dataset.
  prefs: []
  type: TYPE_NORMAL
