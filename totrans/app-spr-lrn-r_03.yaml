- en: '*Chapter 3:*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain supervised learning and machine learning workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use and explore the Beijing PM2.5 dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the difference between continuous and categorical dependent variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the basic regression and classification algorithms in R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the key differences between supervised learning and other types of
    machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with the evaluation metrics of supervised learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform model diagnostics for avoiding biased coefficient estimates and large
    standard errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will introduce supervised learning and demonstrate the workflow
    of building machine learning models with real-world examples.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapters, we explored some of packages of R, such as the `dplyr`,
    `plyr`, `lubridate`, and `ggplot2`, where we discussed the basics of storing and
    processing data in R. Later, the same ideas were used in Exploratory Data Analysis
    (EDA) to understand the ways to break data into smaller parts, extract insights
    from data, and explore other ways to understand the data better, before venturing
    into advanced modeling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will take one step further toward introducing machine learning
    ideas. While broadly laying the foundation for thinking about various algorithms
    in machine learning, we will discuss supervised learning at length.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning is based on data that is well labeled by domain experts.
    For classifying cats and dogs from images, an algorithm first needs to see the
    images labeled as cats and dogs and then learn the features based on the label.
    Most enterprises with a good volume of historical data are the biggest beneficiaries
    of the wealth of knowledge they can extract from such data. If the data is clean
    and annotated well, supervised learning can result in a high accuracy of prediction,
    unlike other machine learning algorithms, which generally produce large errors
    in the beginning. In the absence of the right labels, it becomes difficult to
    derive any meaning out of data, other than just being able to do exploratory analysis
    and clustering.
  prefs: []
  type: TYPE_NORMAL
- en: The standard component in solving real-world problems like predicting loan default
    (yes/no), failure of manufacturing machines in factories (yes/no), object detection
    in driverless cars (road, car, signal), predicting stock market prices (numeric)
    is a set of inputs (features) and a given output (label), which is usually obtained
    from historical data. When we predict the quantitative output, we call it **regression**,
    and when we predict the qualitative output, we call it **classification**.
  prefs: []
  type: TYPE_NORMAL
- en: Summary of the Beijing PM2.5 Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the urban and rural parts of many nations, the primary pollutant, fine particulate
    matter, is the cause of many health risks in humans and also affects climate change.
    In particular, PM2.5, defined as an airborne particle with an aerodynamic diameter
    of less than 2.5 µm, is the major category of atmospheric particulate matter.
    Various studies have linked PM2.5 with serious health problems such as heart attack
    and lung morbidity. The table in this section shows the types of atmospheric particulate
    matter and their size distribution in micrometers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this and the remaining chapters, we will use the dataset published by the
    authors of the research paper, *Assessing Beijing''s PM2.5 pollution: severity,
    weather impact, APEC and winter heating*, where they use hourly PM2.5 readings
    taken at the US Embassy in Beijing located at 116.47 E, 39.95 N in conjunction
    with hourly meteorological measurements at **Beijing Capital International Airport**
    (**BCIA**), obtained from weather.nocrew.org. Their study claims to be the first
    to combine PM2.5 and meteorological data for an extended period in China''s PM2.5
    pollution. The following table describes the attributes in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Attributes in Beijing''s PM2.5 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.1: Attributes in Beijing''s PM2.5 dataset.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 40: Exploring the Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will learn the structure of the data with sample values
    for each attribute and use the `summary` function. We will see the five number
    summary statistics for numeric variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, use the following command to read the Beijing PM2.5 dataset into the
    PM25 DataFrame object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, print the structure of data with sample values using the `str` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the previous command is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Observe that the dataset contains `43824` observations and 13 attributes. Observe
    that the dataset contains data from 2010 to 2014\. The values of pm2.5, temperature,
    pressure, combined wind direction, cumulated wind speed, cumulated hours of snow,
    and cumulated hours of rain are aggregated at every hour of the day.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s show the summary statistics of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following image is a graphical representation of the size distribution
    (in micrometers) of atmospheric particulate matter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Types and size distribution (in micrometers) of atmospheric particulate
    matter.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.2: Types and size distribution (in micrometers) of atmospheric particulate
    matter.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Source: https://en.wikipedia.org/wiki/File:Airborne-particulate-size-chart.svg'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The authors of the article "The impact of PM2.5 on the human respiratory system"
    published in the **Journal of Thoracic Disease** (**JTD**) discuss the association
    of air pollution with respiratory system diseases. They offer a comprehensive
    data-driven approach for explaining the factors causing such respiratory diseases.
    Special attention is given to Beijing, where the adverse effect of rising PM2.5
    has been studied extensively by researchers and has become a mainstream discussion
    point in the various climate change forums around the world. One can find more
    detail in the article at https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4740125/.
  prefs: []
  type: TYPE_NORMAL
- en: Regression and Classification Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We see classification and regression problems all around us in our daily life.
    The chances of rain from https://weather.com, our emails getting filtered into
    the spam mailbox and inbox, our personal and home loans getting accepted or rejected,
    deciding to pick our next holiday destination, exploring the options for buying
    a new house, investment decisions to gain short- and long-term benefits, purchasing
    the next book from Amazon; the list goes on and on. The world around us today
    is increasingly being run by algorithms that help us with our choices (which is
    not always a good thing).
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in *Chapter 2*, *Exploratory Analysis of Data*, we will use the
    **Minto Pyramid** principle called **Situation–Complication–Question** (**SCQ**)
    to define our problem statement. The following table shows the SCQ approach for
    Beijing''s PM2.5 problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3: Applying SCQ on Beijing''s PM2.5 problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.3: Applying SCQ on Beijing''s PM2.5 problem.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, in the SCQ construct described in the previous table, we can do a simple
    correlation analysis to establish the factors affecting the PM2.5 levels or create
    a predictive problem (prediction means finding an approximate function that maps
    from input variables to an output) that estimates the PM2.5 levels using all the
    factors. For the clarity of terminology, we will refer to factors as input variables.
    Then, PM2.5 becomes the dependent variable (often referred to as output variable).
    The dependent variable could be either categorical or continuous.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the email classification into **SPAM**/**NOT SPAM** problem,
    the dependent variable is categorical. The following table highlights some critical
    differences between regression and classification problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4: Difference between regression and classification problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.4: Difference between regression and classification problems.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Machine Learning Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to demonstrate the end-to-end process of building a predictive model
    (machine learning or supervised learning), we have created an easy-to-comprehend
    workflow. The first step is to design the problem, then source and prepare the
    data, which leads to coding the model for training and evaluation, and, finally,
    deploying the model. In the scope of this chapter, we will keep the model explanation
    to a bare minimum, as it will be covered again in detail in chapters 4 and 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure describes the workflow required to build a predictive
    model starting from preparing the data to deploying the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5: Machine learning workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.5: Machine learning workflow.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Design the Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we identify the domain of work, brainstorming on the designing of the problem
    is carried out. The idea is to first define the problem as a regression or classification
    problem. Once that is done, we choose the right target variable, along with identifying
    the features. The target variable is important because it decides how the training
    will take place. A supervised learning algorithm keeps the target variable at
    the center, while it tries to find a pattern from the given set of features.
  prefs: []
  type: TYPE_NORMAL
- en: Source and Prepare Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data gathering and preparation is a painstaking job, mainly when the data sources
    are diverse and many. With each data source, the challenges are different and
    hence the time taken to process it varies. Data sources with tabular data are
    the easiest to process provided they do not contain a lot of garbage information,
    whereas textual data is the hardest to clean because of its free-flowing nature.
  prefs: []
  type: TYPE_NORMAL
- en: Code the Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the data is prepared and ready, we take up the task of choosing the right
    model. Most often, the experts first go with one baseline model to gauge the predictability
    power of the algorithm using input features and the target variable. Then, one
    can either directly try the state-of-the-art algorithms or decide to go with a
    trial-and-error method (of trying to use all the possible models). One must understand
    that there is no right or wrong model, and everything depends on the data. In
    coding, the data is randomly divided into training and testing. The code is written
    to train the model on the training dataset, and evaluation happens on the testing
    data. This ensures that the model does not underperform when it is deployed in
    the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Train and Evaluate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model evaluation is the important part of the model, where its usability in
    practice is decided. Based on a given set of model evaluation metrics, we need
    to decide, after all the trial and error, the best model. In each iteration, metrics
    such as the R-squared value, accuracy, precision, and F-score are computed. Usually,
    the entire data is divided into training and testing data (with a third split
    for validation set also often included). The model is trained on the training
    data and tested on the testing data. This separation ensures that the model is
    not doing any rote learning. In more technical terms, the model is not overfitting
    (more on this in the *Evaluation Metrics* section in this chapter). Usually, at
    this stage of the workflow, one could decide to go back and include more variables,
    train the model, and redeploy. The process is repeated until the accuracy (or
    the other metrics of importance) of the model reaches a plateau.
  prefs: []
  type: TYPE_NORMAL
- en: We use a random number generator function like `sample()` in R for splitting
    the data randomly into different parts as done in the next exercise 2, step 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 41: Creating a Train-and-Test Dataset Randomly Generated by the Beijing
    PM2.5 Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will create a randomly generated train-and-test dataset
    from the Beijing PM2.5 dataset. We will reuse the `PM25` object created in the
    earlier exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `num_index` variable and set it to a value equal to the number of
    observations in the Beijing''s PM2.5 dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the `sample()` function, randomly select 70% of the `num_index` values,
    and store them in `train_index`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `train_index` to select a random subset of rows from the Beijing PM2.5
    dataset and store them in a DataFrame named `PM25_Train`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Store the remaining observation into a DataFrame named `PM25_Test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The exercise shows a simple example for creating the train-and-test set. A randomly
    selected set for training and testing ensures that the model has no bias and learns
    well from all the possible examples before being used in the real world on unseen
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy the Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the best model is selected, the next step is to enable the model output
    to be used by a business application. The model is hosted as a **REpresentational
    State Transfer** (**REST**) API. These APIs are a way to host a web application
    as an endpoint that listens to any request for a model call and usually returns
    a JSON object as a response.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment of the model is becoming an essential part of all machine learning
    projects in the industry. A model that is not deployable is no good for a company,
    and perhaps, merely serves the purpose of R&D. An increasing number of professionals
    are specializing in model deployment, which is sometimes a tedious and complicated
    process. In order to give the model deployment its due importance, we have given
    it a dedicated chapter, that is *Chapter 8*, *Model Deployment*.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have seen the machine learning workflow, we will take two widely
    used types of machine learning algorithms: regression and classification; both
    employ supervised learning to train the models. The entire theme of this book
    revolves around these two types of algorithms. The Beijing PM2.5 dataset will
    be used extensively in demonstrating both these types. The dataset will help in
    understanding how one can convert a regression problem into a classification problem
    and vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: Simple and Multiple Linear Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regression is one of the most useful and essential tools in analytics and econometrics
    (the branch of economics concerned with the use of mathematical methods, especially
    statistics, in describing economic systems). In many ways, modern machine learning
    has its roots in statistics, and one can attribute that mostly to Sir Francis
    Galton's work. Galton was an English Victorian-era statistician and polymath with
    deep interest and expertise in fields such as genetics, psychology, and anthropology.
    He was the first to apply statistical methods to study human behavior and intelligence.
    Notably, his publication, *Regression Towards Mediocrity in Hereditary Stature*,
    had many insightful findings based on regression.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will briefly analyze the various factors that affect the
    PM2.5 levels using the Beijing dataset. In particular, the effect of variables
    such as dew point, temperature, wind speed, and pressure on PM2.5 will be explored.
  prefs: []
  type: TYPE_NORMAL
- en: Assumptions in Linear Regression Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As regression borrows many of its concepts from applied statistics to model
    the data, it comes with many assumptions. One should not apply regression algorithms
    to any dataset or problem. Let's examine the assumptions for linear regression
    before we build any model.
  prefs: []
  type: TYPE_NORMAL
- en: The following table shows the assumptions and how we can statistically test
    whether the linear regression model follows the assumption or not. The table also
    shows some corrective actions if the assumption is violated. We will take up an
    elaborate discussion on these assumptions and perform diagnostic analysis to identify
    the violation in much detail in *Chapter 4*, *Regression.*
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6: Various assumptions in a linear regression model (Part 1).'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.6: Various assumptions in a linear regression model (Part 1).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 3.7: Various assumptions in a linear regression model (Part 2).'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.7: Various assumptions in a linear regression model (Part 2).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Exploratory Data Analysis (EDA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building regression models requires an in-depth analysis of the patterns and
    relationship between target and input variables. The Beijing dataset provides
    a magnitude of different environmental factors that may affect the PM2.5 levels
    in the atmosphere.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 42: Exploring the Time Series Views of PM2.5, DEWP, TEMP, and PRES
    variables of the Beijing PM2.5 Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will visualize the `pm2.5`, `DEWP`, `TEMP`, and `PRES`
    variables in a time series plot and observe any patterns that may emerge over
    the years in these variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the required libraries in the system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, transform year, month, and hour into datetime using the `lubridate` package
    function named `ymd_h`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the PM2.5, TEMP, DEWP, and PRES for all the years using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use the following command to plot the graphs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.8: Scatterplot showing the trend and seasonality of environmental
    factors like temperature, dew point, and pressure, along with PM2.5 levels in
    Beijing from 2010 to 2014 end.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.8: Scatterplot showing the trend and seasonality of environmental
    factors like temperature, dew point, and pressure, along with PM2.5 levels in
    Beijing from 2010 to 2014 end.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this exercise, we first show a time series view of the `PM2.5`, `DEWP`, `TEMP`,
    and `PRES` variables from the dataset and observe the pattern. As shown in *Figure
    3.8*, a distinct seasonality is observed `DEWP`, `TEMP`, and `PRES` show seasonality
    (the same pattern repeating every 12 months), PM2.5 seems to have a random pattern.
    This is an early indication that it's highly unlikely that we will see any effect
    of the three variables on PM2.5\. However, let's probe further to ascertain this
    hypothesis using a correlation plot and observe if there exits any relationship
    between the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 43: Undertaking Correlation Analysis'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will undertake a correlation analysis to study the strength
    of the relationship between the various factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `corrplot` package into the system using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, create a new object and store the required values from `PM25` into it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `corrplot` package to display the graphical representation of a correlation
    matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.9: Correlation between all the pairs of variables in the Beijing
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.9: Correlation between all the pairs of variables in the Beijing dataset.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: First, we compute the correlation between all the variables. The resulting correlation
    plot shows that there appear to be no strong correlations between PM2.5 and the
    other variables. However, `PM2.5` and `DEWP`, `TEMP`, and `Iws` show some mild
    correlation, which indicates some relationship. This should not come as a surprise,
    because we saw in *Figure 3.8*, that while three variables follow a seasonality
    trend, PM2.5 seems more random. Note here that we have not done any processing
    or transformation to the dataset; these findings come directly from our first
    level of analysis. We will go into much detail later, in *Chapter 4*, *Regression*.
    Now, let's also visualize the relationship between the variables using a scatterplot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 44: Drawing a Scatterplot to Explore the Relationship between PM2.5
    Levels and Other Factors'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will use a scatterplot to explore the relationship between
    `pm2.5` levels and other factors. We will like to see whether there emerge any
    interesting patterns or relationships. A scatterplot is a simple and effective
    visualization for exploratory analysis on the relationships between variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `ggplot2` package into your system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the scatterplot between `DEWP` and `PM2.5`, with the `month` variable
    used for color:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The scatterplot is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.10: Scatterplot showing the relationship between DEWP and PM2.5
    levels.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_03_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.10: Scatterplot showing the relationship between DEWP and PM2.5 levels.'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the scatterplot between `TEMP` and `PM2.5`, with the `month` variable
    used for color:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The scatterplot is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11: Scatterplot showing the relationship between TEMP and PM2.5
    levels.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_03_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.11: Scatterplot showing the relationship between TEMP and PM2.5 levels.'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Create a scatterplot between `DEWP` and `PM2.5`, with an hour of the day used
    for color and separate views for months of the year:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The scatterplot is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12: Scatterplot showing the relationship between DEWP and PM2.5
    split by month of the year.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.12: Scatterplot showing the relationship between DEWP and PM2.5 split
    by month of the year.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In order to gauge some relationship between variables, we used a scatterplot
    between `PM2.5` and `DEWP` with a line of fit. Observe that in the code, we have
    passed an argument to `geom_smooth()`, that is, `method = "auto"`, which automatically
    decides, based on the data, which model to use to fit a line. As shown in *Figure
    3.10*, the line is not linear. The `geom_smooth` method chooses `TEMP` and `PM2.5`
    plot, as shown in *Figure 3.11*. However, we could go one step further and split
    the scatterplot month-wise, as shown in *Figure 3.12*. This shows that a linear
    relationship exists, but it is highly season-dependent. For example, in April
    (represented by the integer `4`), the `DEWP` and `PM2.5` have a near-to-perfect
    straight line fit. We will extend this discussion into further details in *Chapter
    4*, *Regression*.
  prefs: []
  type: TYPE_NORMAL
- en: So, we have seen some violation of assumption and the lack of a strong correlation
    between the environmental factors and PM2.5\. However, there seems to be some
    scope for further scrutiny. In this introductory chapter on supervised learning,
    we will only focus on the approach based on our machine learning workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To know more about GAM, review this document: https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch13.pdf.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 5: Draw a Scatterplot between PRES and PM2.5 Split by Months'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will create a scatterplot between `DWEP` and `PM2.5`. Through
    this activity, we will learn to use the `facet_wrap()` function to create a layer
    on top of `ggplot()` for splitting the visualization of scatterplot into each
    month, thus helping to observe any seasonality pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: In `ggplot`, assign the component of the `a()` method with the `PRES` variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next layer of the `geom_smooth()` method, set `colour = "blue"` to differentiate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, in the `facet_wrap()` layer, use the `month` variable to draw a separate
    segregation for each month.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The plot is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.13: Scatterplot showing the relationship between PRES and PM2.5.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.13: Scatterplot showing the relationship between PRES and PM2.5.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 445.
  prefs: []
  type: TYPE_NORMAL
- en: Model Building
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have briefly explored the relationship between `PM2.5` and a few factors
    such as `TEMP` and `DEWP`. The same analysis could be followed for other variables
    such as `PRES`, `Iwd`, and more. In this section, let's create a linear model.
    (We never hesitate to run a model even if we know the choice of model isn't the
    best. A trial-and-error approach in machine learning is always the best way to
    establish facts.)
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, a linear regression models the linear relationship between an input
    variable (independent variable) and a target variable (dependent variable or explanatory
    variable). If we have one explanatory variable, it is called **simple linear regression**,
    and where there is more than one explanatory variable, it''s called **multiple
    linear regression**. The following equation is the mathematical representation
    of linear regression or a linear predictor function with *p* explanatory variables
    and *n* observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, each ![A picture containing furniture, table, seat, stool
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_03_21.png) is a vector of column
    values (explanatory variable) for ![](img/C12624_03_22.png), and ![A picture containing
    furniture
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_03_23.png) is the unknown parameter
    or coefficient. ![A picture containing furniture, seat
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_03_24.png) makes this equation
    suitable for simple linear regression. There are many algorithms to fit this function
    onto the data. The most popular one is **ordinary least square** (**OLS**). We
    will discuss OLS in detail in our next chapter on regression.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to think of ![A picture containing furniture
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_03_25.png) is that it's a linear
    predictor function that fits the observations in the ![](img/C12624_03_26.png)—dimension
    space as closely as possible, minimizing the residual sum of squares (the difference
    in the actual value of the target value from the predicted value).
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we will skip the split of the dataset into train
    and test, as we are still in the exploration stage and have not decided to formally
    approach the modeling exercise. (We will touch on that in the next chapter.) We
    will use the `lm()` method in R for building a linear model. Again, more details
    on that in the next chapter. At this point, it is suffice to note that `lm()`
    fits a target variable to a straight line using either one or more input variables.
    In a simple linear regression, we use only one variable to fit the line, and in
    multiple linear regression, we can use more than one variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 45: Exploring Simple and Multiple Regression Models'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will explore simple and multiple regression models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries and packages into R-Studio.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, create a DataFrame object named `simple_PM25_linear_model` and use the
    `lm()` method to build a linear model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the summary of the object using the summary method, as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create another DataFrame object and use the `lm()` method to build a
    linear model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the summary of the model object using the `summary` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Model Interpretation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, based on the previous output of both the simple and multiple linear regression
    models, let's try to understand what each part of the output means. At this juncture
    of the book, it's sufficient to know what each part means; we will discuss the
    results in *Chapter 4*, *Regression*.
  prefs: []
  type: TYPE_NORMAL
- en: Part `lm()` method with the dependent and independent variables, represented
    like a formula using the `~` symbol. This resembles our linear predictor function.
    In a simple regression model, there is only one variable—`DEWP`—and in a multiple
    model, there are `DEWP`, `TEMP`, and `Iws`. You also see the five summary statistics
    of residuals (min, first quartile, median, third quartile, and max). This indicates
    how far the predicted values are from the actual value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part `X_j` into our prediction equation, we will get the predictions. The column
    named `Std`. Error is the standard error of the estimate. t-value is obtained
    by taking the ratio of `Estimate` and `Std`. Error, and p-value highlights the
    statistical significance of the estimate. The visual clues, that is, the `*` and
    . symbols are based on the p-value. A value less than 0.001 gets a three star
    versus a value between 0.1 and 0.05, which gets a `.` (dot). Three stars means
    the best case and that the estimates corresponding to the independent variable
    are significant and useful in predicting (or explaining) the dependent variable.
    In other words, p-value helps in determining the significance of a regression
    model over a null model (just the mean of the dependent variable).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part **C**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This part is the one that shows the efficacy of the model. The most important
    values to observe are the R-squared and adjusted R-squared values, which are statistical
    measures that signify the percentage of variation for a dependent variable that's
    explained by independent variable(s) in a regression model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go through the section on evaluation metrics in this chapter to see the interpretation
    on how well the model has done on R-squared and adjusted R-squared metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the regression algorithm, classification also learns from the dependent
    or target variables and uses all the predictor or independent variables to find
    the right pattern. The major difference comes from the idea that in classification,
    the target variable is categorical, whereas in regression, it is numeric. In this
    section, we will introduce logistic regression to demonstrate the concept using
    the Beijing PM2.5 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Logistic regression** is the most favorable white-box model used for binary
    classification. White-box models are defined as models providing visibility into
    the entire reasoning done for the prediction. For each prediction made, we can
    leverage the model''s mathematical equation and decode the reasons for the prediction
    made. There are also a set of classification models that are entirely black-box,
    that is, by no means can we understand the reasoning for the prediction leveraged
    by the model. In situations where we want to focus only on the end outcome, we
    should prefer black-box models as they are more powerful.'
  prefs: []
  type: TYPE_NORMAL
- en: A Brief Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Though the name ends with **regression**, logistic regression is a technique
    used to predict binary categorical outcomes and is hence a good choice for classification
    problems. As discussed in the previous section, we need a different approach to
    model for a categorical outcome. This can be done by transforming the outcome
    into the log of the odds ratio or the probability of the event to happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s distill this approach into simpler constructs. Assume that the probability
    of success for an event is 0.7\. Then, the probability of failure for the same
    event would be defined as *1 – 0.7 = 0.3*. The odds of success are defined as
    the ratio of the probability of success to the probability of failure. The odds
    of success would then be *0.7/0.3 = 2.33*, that is, the odds of success are 2
    to 1\. If the probability of success is 0.5, that is, a 50-50 chance, the odds
    of success are 1 to 1\. The logistic regression model can be mathematically represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![A drawing of a person
  prefs: []
  type: TYPE_NORMAL
- en: 'Description automatically generated](img/C12624_03_28.png) is the log of the
    odds ratio, which is also called the **logit** function. Solving the math further,
    we can deduce the probability of the outcome as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Discussing the mathematical background and derivation of the equations is beyond
    the scope of this chapter. However, to summarize, the logit function, which is
    the link function (or logic function), helps logistic regression reframe the problem
    (predicted outcome) intuitively as the log of the odds ratio. This, when solved,
    helps us to predict the probability of a binary dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: Mechanics of Logistic Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just like linear regression, where the beta coefficients for the variables are
    estimated using the OLS method, the logistic regression model leverages the **maximum
    likelihood estimation** (**MLE**) method. The MLE function estimates the best
    set of values of the model parameters or beta coefficients such that it maximizes
    the likelihood function, that is, the probability estimates. It can also be defined
    as the *agreement* of the selected model with the observed data). When the best
    set of parameter values is estimated, plugging these values/beta coefficients
    into the model equation, as defined earlier, helps in estimating the probability
    of the outcome for a given sample. Akin to OLS, MLE is an iterative process.
  prefs: []
  type: TYPE_NORMAL
- en: Model Building
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like linear regression for building a logistic regression model in R, we have
    the `glm()` generalized linear model method to fit the data and logit function
    to score the observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The syntax of using the glm() function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Here, Y is our dependent variable and X1, X2 and X3 are the independent variables.
    The argument data will take the training dataset. The family argument is set to
    binomial(link='logit'), which fits a logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 46: Storing the Rolling 3-Hour Average in the Beijing PM2.5 Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will create a new variable that stores the rolling 3-hour
    average of the PM2.5 variable in the Beijing PM2.5 dataset. The rolling average
    will smoothen any noise from a reading of PM2.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the `rollapply` method from the `zoo` package to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Combine the `year`, `month`, `day`, and `hour` into a new variable called `datetime`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remove the NAs and look at the top 6 values of the `pm2.5` variable in the
    PM2.5 dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Store the `PM25_subset` into a `zoo` object of ordered observation with datetime
    as its index, and print the top 6 values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `rollapply` function to create a 3-hour rolling average of the `pm2.5`
    variable, and print the top 6 values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Observe that the `145.33` value is the average of three hours of the `pm2.5`
    variable, as shown in step 3 (`129`, `148`, and `159`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6: Transforming Variables and Deriving New Variables to Build a Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will perform a series of transformations and derive new
    variables before building the model. We need to convert the `pm2.5` variable into
    a categorical variable to apply a logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps need to be performed before we can build a logistic regression
    classification model:'
  prefs: []
  type: TYPE_NORMAL
- en: Combine the year, month, day, and hour into a new variable called `datetime`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the datetime variable, calculate the average of the `pm2.5` values with
    a 3-hour window. Name this new variable `PM25_three_hour_pm25_avg`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a binary variable called `pollution_level`. It gets a value `1` if `PM25_three_hour_pm25_avg`
    is greater than `35`, else `0`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `pollution_level` as the dependent variable, build a logistic regression
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the summary of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The final output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 446.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Interpreting a Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A large part of the `glm()` output looks similar to the `lm()` method but with
    a few new values, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Null deviance**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Residual deviance**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Akaike Information Criterion** (**AIC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fisher scoring**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to avoid scoring, all the above measures will be described in detail
    in *Chapter 5*, *Classification*.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to0 the next section on *Evaluation Metrics* (the *Confusion Matrix Based
    Metrics* section) in this chapter to find an interpretation of how well the model
    has done on R-squared and adjusted R-squared metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will go through all the evaluation measures for assessing
    the quality of the machine learning model predictions. Based on the dependent
    variable, we have several choices for the evaluation measures. In the train and
    evaluate step of our Machine Learning Workflow, we mentioned that until we get
    the desired results, we keep iterating the training model by adding new variables
    or changing the parameters. In each iteration, we try to optimize for any one
    or two evaluation metrics. The following table summarizes the various types of
    metrics used for regression, classification, and recommender systems. Given the
    scope of this book, we will delve into more details on regression and classification
    algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14: Metrics for various types of machine learning algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.14: Metrics for various types of machine learning algorithms.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Mean Absolute Error (MAE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Absolute error is direction-agnostic, which means that it is does not matter
    whether the predicted value of the dependent variable by the model on the test
    dataset is less than or greater than the actual value. So, in our example of the
    Beijing PM2.5 dataset, MAE will give us the average absolute error (difference
    in the predicted and actual values of the dependent variable) in PM2.5 prediction
    indifferent to the direction of error (positive or negative):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/C12624_03_31.png) is the value of the ith observation of the dependent
    variable, and ![](img/C12624_03_32.png) is the predicted or expected value.
  prefs: []
  type: TYPE_NORMAL
- en: Root Mean Squared Error (RMSE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to MAE, root mean square error also computes the average prediction
    error. However, it is based on a quadratic scoring, where the square root of the
    average squared error is computed. Moreover, unlike MAE, which takes the absolute
    difference between the predicted and actual values, RMSE takes the square, which
    adds more weight to the high error values before taking the square root:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/C12624_03_34.png) represents the difference between the actual
    and estimated values of the dependent variable for the ith observation.
  prefs: []
  type: TYPE_NORMAL
- en: R-squared
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: R-squared measures the percentage (value between 0 and 1 or from 0% to 100%)
    of the variance in the response variable explained by the linear model. In other
    words, it measures the variance explained by the input features. 0% R-squared
    means the model's input feature explains nothing about the response variable.
    Closer to 100% means that the model is a good predictor of the response variable.
    For example, if we want to predict the price of a house in a locality, features
    such as the number of bedrooms, area in sq. ft, and proximity to a school and
    market decides the value of a property. However, R-squared alone cannot be used
    for assessing the goodness of the model. Various diagnostic checks on residual,
    normality, and heteroscedasticity are also required. We will discuss this in detail
    in *Chapter 4*, *Regression*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/C12624_03_36.png) is the sum of the square difference between
    the actual and estimated values of the dependent variable, while ![](img/C12624_03_37.png)
    represents the sum of the square difference between the actual and mean of the
    dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Adjusted R-square
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we add new variables in the regression model, the R-squared value of the
    model improves as the contribution of the newer variables in explaining the variation
    of the dependent variable increases. (A counter-argument arises if the newer variables
    are poorly designed and are not relevant for explaining the dependent variable.)
    So, for the evaluation metric to be agnostic to the number of variables, we penalize
    the R-squared value by incorporating *n* and *q* (number of observations and number
    of variables, respectively) in the calculation. This is called adjusted R-squared,
    adjusted for both the number of observations and variables. It is a good practice
    to look at the adjusted R-squared when dealing with multiple linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: '**MSE** (**mean squared error**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n* is the number of observations, and *q* is the number of coefficients
    in the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**MST** (**mean squared total**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Mean Reciprocal Rank (MRR)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MRR is popularly used to evaluate algorithms in search engines, recommender
    algorithms, and many other information retrieval algorithms in the digital space.
    MRR is easy to interpret. In general, it could be used to evaluate algorithms
    that produce a list of responses for an input. Examples are the search results
    you see in Google for your query and the product recommendations you see on Amazon.
    The following table shows an example of computing the reciprocal rank. MRR ranges
    from 0 to 1; a value closer to 1 indicates that the algorithm is giving relevant
    results at the top of the list.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_41.jpg)![Figure 3.15: Example of computing reciprocal rank.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.15: Example of computing reciprocal rank.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 47: Finding Evaluation Metrics'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will find the MAE, RMSE, R-squared, Adjusted R-squared,
    and MRR.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries and packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a variable named `y_predicted` and assign the value from the `multiple_PM25_linear_model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following command to assign values from the `PM25` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the MAE using the mean function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, calculate the RMSE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, calculate the R-squared value using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, find the adjusted R-squared using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, use the following command to find the MRR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Observe that MAE gives a value of `59.82` and RMSE is `82.09,` which shows a
    high variance in the errors. In other words, the observations have a high error
    (which increases the variance of the frequency distribution of error magnitudes)
    in prediction; MAE fails to identify the error, whereas RMSE amplifies it well.
    If the MAE and RMSE are almost equal, we could infer that the variance in the
    frequency distribution of error magnitudes is low and that the model is doing
    well with all the observations.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrix-Based Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Confusion matrix-based metrics are used in classification algorithms. There
    are a series of metrics one could derive from the confusion matrix (also called
    the `A` and `B`. Otherwise, there is nothing negative or positive about the target
    variable. The contingency table could also be NxN, where *N* is the number of
    classes or categories in the response variable. For example, if we want to classify
    the 26 handwritten characters of the English alphabet in a given image, we need
    a 26x26 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16: Elements of the confusion matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.16: Elements of the confusion matrix.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If we arrange the **TP**, **TN**, **FP**, and **FN** in a 2x2 contingency matrix,
    we obtain the confusion matrix, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17: Confusion matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_03_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.17: Confusion matrix.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Accuracy measures the correct overall classifications by the model for both
    positive and negative examples. The sum of the diagonal elements in the matrix
    (TP and TN) divided by the total number of positive and negative observations
    gives the accuracy. Accuracy is not always a reliable metric in real-world scenarios.
    Consider that we would like to distinguish cancer CT scans from benign CT scans.
    Clearly, we may have many negative scans and few positive scans. This leads to
    what we call the **unbalanced dataset**. If the model mostly predicts benign scans
    accurately but produces a significant error in predicting cancer CT scans, the
    accuracy may still be high, but the model is not so useful.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Sensitivity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to tackle the issue we discussed with *accuracy*, we could use a combination
    of sensitivity, also known as recall, hit rate, or **true positive rate** (**TPR**),
    and specificity (discussed in the next section). Sensitivity gives the predictive
    power of the model with respect to the positive cases (detecting cancer in a CT
    scan). We obtain sensitivity from the ratio of all **true positive** (**TP**)
    cases to the number of **positive** (**P**) cases.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Specificity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Specificity provides the quantitative assessment of correct predictions of negative
    examples (for example, detecting benign CT scans). We obtain sensitivity from
    the ratio of a number of true negative cases to the number of negative cases.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: High sensitivity and specificity values signify a superior model. In most cases,
    we try to balance the two metrics to get the best model.
  prefs: []
  type: TYPE_NORMAL
- en: F1 Score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: F1 score combines precision and sensitivity by taking the harmonic mean (appropriate
    for taking averages of two or more rates) of both, as described by the following
    formulas. **Positive predictive value** (**PPV** or precision) measures the number
    of true predictions over the sum of a number of true and false positives, that
    is, how many of all the predictions of positive cases were correct.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_03_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: F1 score is more robust than accuracy but still suffers in the case of unbalanced
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: There is no good or bad metric for evaluating the goodness of a classification
    model. Machine learning practitioners usually look at a combination of many metrics
    to conclude the goodness of a model. That is why it becomes important to know
    how to interpret each of the above discussed metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 48: Working with Model Evaluation on Training Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will work with model evaluation on training data using
    the `confusionMatrix` function from the `caret` package. The function prints metrics
    such as accuracy, sensitivity, specificity, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries and packages into the system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a variable name `predicated` and assign the value, as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create another variable named `actual`, as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the caret library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, use the `confusionMatrix` method to describe the performance of the
    classification model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Many of the metrics shown in the results of the `confusionMatric()` output are
    described in this section. However, here's a quick summary before you read the
    details. The accuracy of this logistic regression model is 80%, which is good
    as per the standard. This indicates that we can predict the normal and above normal
    PM2.5 values using other environmental factors with 80% accuracy. However, note
    that the accuracy is on the entire training dataset. We have not split the data
    into two parts for checking the overfitting scenarios, a condition in which the
    model performs really good when tested on training data but shows inferior results
    on testing (or unseen) data.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity and specificity are 46% and 93%, respectively. This means the model
    is doing good for negative cases (1-Above normal PM2.5). Generally, there must
    be a tradeoff between these two metrics. However, in this case, the priority for
    the model is to be able to predict as many **Above Normal** states as possible.
    Hence, high specificity is desirable once we have the confusion matrix; it's possible
    to calculate all the metrics from it.
  prefs: []
  type: TYPE_NORMAL
- en: Receiver Operating Characteristic (ROC) Curve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the context of classification models, the output of a prediction is obtained
    as a quantitative estimate, usually a probability measure. In a binary logistic
    regression, the usual choice of the threshold to classify one observation from
    the other (for example, spam versus non-spam) is 0.5\. This means that if the
    probability is greater than 0.5, classify it as spam and if not, non-spam. Now,
    depending on the threshold, you will get different values of TP, TN, FP, and FN
    in the confusion matrix we discussed earlier. While it is a standard practice
    to look at the confusion matrix at a given threshold (usually 0.5), it might not
    give us the complete view of whether the model will perform well in the real world,
    which is why the choice of threshold is essential.
  prefs: []
  type: TYPE_NORMAL
- en: The ROC curve is an elegant visualization showing the variation between the
    true positive rate (often referenced by sensitivity) and the true negative rate
    (often referenced by specificity) at every possible threshold. It helps us identify
    the right threshold for classification. Also, the area under the ROC curve (referred
    to as AUC), which varies between 0 and 1, tells us how good the model is. Closer
    to 1 means that the model is successfully able to classify between positive and
    negative classes for most of the observation.
  prefs: []
  type: TYPE_NORMAL
- en: Using the ROCR package in R, we will obtain the ROC curve for the PM2.5 prediction
    using logistic regression. Also, we will observe the AUC in the next exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 49: Creating an ROC Curve'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will use the ROCR package to obtain the ROC curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the ROCR package into the system using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define the pred1 and pref1 objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, find the AUC using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the graph using the plot command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.18: ROC curve between true positive rate (sensitivity) and false
    positive rate (specificity).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_03_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.18: ROC curve between true positive rate (sensitivity) and false positive
    rate (specificity).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we started out with laying the process for building a machine
    learning workflow, starting from designing the problem and moving to deploying
    the model. We briefly discussed simple and multiple and logistic regressions along
    with all the evaluation metrics needed to interpret and judge the performance
    of the model. These two algorithms demonstrate the supervised learning for regression
    and classification problems, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the chapter, we used the Beijing PM2.5 dataset to build the models.
    In the process, we also converted a regression problem to a classification problem
    by simply re-engineering the dependent variable. Such re-engineering is often
    taken up on real-world problems to suit a particular use case.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into the details of regression algorithms
    and will elaborate the various types of regression algorithms beyond linear regression
    and discuss when to use which one.
  prefs: []
  type: TYPE_NORMAL
