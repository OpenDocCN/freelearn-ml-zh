<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Cyber Attack Detection</h1>
                
            
            <article>
                
<p class="calibre2">So far, we have been mainly developing <strong class="calibre4">machine learning</strong> (<strong class="calibre4">ML</strong>) models with well-balanced sample sets, where the target classes are distributed equally or almost equally across the sample records in the dataset. However, there are cases where a dataset has imbalanced class distributions. Class imbalance is especially common in anomaly and fraud detections. These kinds of class imbalance problems causes issues when training ML models, as most ML algorithms work best when the target classes are roughly equally distributed. In order to tackle this imbalanced class problem, we cannot approach it the same way we have been developing models for various classification and regression problems. We will need to approach it differently.</p>
<p class="calibre2">In this chapter, we are going to discuss how we can build an anomaly detection model. We will be using a network intrusion dataset, <strong class="calibre4">KDD Cup 1999 Data</strong>, which has a large amount of network connection data where some of the connections are normal network connections, and some others are cyber attacks. We will first look at the structure of the data, types of cyber attacks present in the dataset, and distributions of various network features. Then, we will apply some of the feature-engineering techniques we have discussed in previous chapters, as the feature set contains both categorical and continuous variables. We are also going to apply the dimensionality reduction technique, <strong class="calibre4">Principal Component Analysis</strong> (<strong class="calibre4">PCA</strong>), that we discussed in the previous chapter. In addition to what we covered about PCA in the previous chapter, we are going to use PCA to build models for anomaly detection. With the models built using PCA, we are going to further discuss some of the ways to evaluate anomaly detection models, and what will work best for the cyber attack detection project.</p>
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre10">
<li class="calibre11">Problem definition for the cyber attack detection project</li>
<li class="calibre11">Data analysis for the internet traffic dataset</li>
<li class="calibre11">Feature engineering and PCA</li>
<li class="calibre11">Principal component classifier for anomaly detection</li>
<li class="calibre11">Evaluating anomaly detection models</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Problem definition</h1>
                
            
            <article>
                
<p class="calibre2">Datasets with imbalanced class distributions cause problems for most ML algorithms, as they typically perform well for well-balanced datasets. There are various ways to handle class imbalance problems in ML. Resampling the dataset to balance the target classes is one way. You can upsample the positive training samples, where you randomly select and duplicate the positive training samples, so that roughly 50% of the dataset belongs to a positive class. You can also downsample the negative training samples so that the number of negative training examples matches with the number of positive training examples. In cases of extreme class imbalance, you can approach it as an anomaly detection problem, where the positive events are considered anomalies or outliers. Anomaly detection techniques have many applications in real-world problems. They are often used for network intrusion detection, credit card fraud detection, or even medical diagnosis.</p>
<p class="calibre2">In this chapter, we are going to work on building an anomaly detection model for cyber attacks. In order to build a cyber attack detection model, we are going to use the <strong class="calibre4">KDD Cup 1999 Data</strong>, which has a large amount of artificial and hand-injected cyber attack data, along with normal network connection data. This data can be found at the following link: <a href="http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html" class="calibre9">http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html</a>. With this data, we are going to first look at the distributions of the cyber attack types and then the distributions of the network features. Since this is a simulated and artificial dataset, the majority of this dataset is made up of cyber attacks, which are abnormal and unrealistic in the real world. In order to simulate real-world examples of cyber attacks we are going to randomly sub-select the cyber attack events from the sample set and build a new training set that contains more normal network connections than malicious connections. With this sub-sampled dataset, we are going to build an anomaly detection model using PCA. Then, we are going to evaluate this model by looking at the cyber attack detection rate at various target false alarm rates.</p>
<p class="calibre2"><span class="calibre5">To summarize our problem definition for the cyber attack detection project:</span></p>
<ul class="calibre10">
<li class="calibre11">What is the problem? We need a cyber attack detection model that can identify potential malicious connections from large amounts of network connections so that we can avoid cyber attacks.</li>
<li class="calibre11">Why is it a problem? T<span><span>he number of cyber attacks increases every year and without being properly prepared for such attacks, our systems will become more vulnerable from various cyber attacks. With a cyber attack detection model, we can avoid becoming the victims of cyber attacks.</span></span></li>
<li class="calibre11">What are some of the approaches to solving this problem? We are going to use publicly available data that has a large amount of artificial and simulated cyber attack data. We are going to sub-sample this data to replicate a real-life situation where there are more normal network connections than abnormal and malicious connections. Then, we are going to use PCA and its principal components to detect anomalies.</li>
<li class="calibre11">What are the success criteria? <span>We want a high cyber attack detection rate, even if we need to sacrifice it for higher false alarm rate. This is because we are more concerned about allowing cyber attacks than false positive alerts.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data analysis for internet traffic data</h1>
                
            
            <article>
                
<p class="calibre2">Let's start by taking a look into the internet traffic data. As mentioned previously, we are going to use the KDD Cup 1999 Data, which you can download from the following link: <a href="http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html" class="calibre9">http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html</a>. We will be using the <kbd class="calibre12">kddcup.data_10_percent.gz</kbd> data for this cyber attack detection project.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data clean-up</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">The first thing we need to do is clean up the data for future steps. If you open the data that you just downloaded, you will notice that there is no header in the dataset. However, for future data analysis and model building, it is always beneficial to have headers associated with each column. Based on the column description that can be found at <a href="http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names" target="_blank" class="calibre9">http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names</a>, we are going to attach headers to the raw dataset. The code to attach column names to the data frame looks as follows:</span></p>
<pre class="calibre19">// Read in the Cyber Attack dataset<br class="title-page-name"/>// TODO: change the path to point to your data directory<br class="title-page-name"/>string dataDirPath = @"&lt;path-to-data-dir&gt;";<br class="title-page-name"/><br class="title-page-name"/>// Load the data into a data frame<br class="title-page-name"/>string dataPath = Path.Combine(dataDirPath, "kddcup.data_10_percent");<br class="title-page-name"/>Console.WriteLine("Loading {0}\n\n", dataPath);<br class="title-page-name"/>var featuresDF = Frame.ReadCsv(<br class="title-page-name"/>    dataPath,<br class="title-page-name"/>    hasHeaders: false,<br class="title-page-name"/>    inferTypes: true<br class="title-page-name"/>);<br class="title-page-name"/><br class="title-page-name"/>string[] colnames =<br class="title-page-name"/>{<br class="title-page-name"/>    "duration", "protocol_type", "service", "flag", "src_bytes",<br class="title-page-name"/>    "dst_bytes", "land", "wrong_fragment", "urgent", "hot",<br class="title-page-name"/>    "num_failed_logins", "logged_in", "num_compromised", "root_shell",<br class="title-page-name"/>    "su_attempted", "num_root", "num_file_creations", "num_shells",<br class="title-page-name"/>    "num_access_files", "num_outbound_cmds", "is_host_login", "is_guest_login",<br class="title-page-name"/>    "count", "srv_count", "serror_rate", "srv_serror_rate", "rerror_rate",<br class="title-page-name"/>    "srv_rerror_rate", "same_srv_rate", "diff_srv_rate", "srv_diff_host_rate",<br class="title-page-name"/>    "dst_host_count", "dst_host_srv_count", "dst_host_same_srv_rate",<br class="title-page-name"/>    "dst_host_diff_srv_rate", "dst_host_same_src_port_rate",<br class="title-page-name"/>    "dst_host_srv_diff_host_rate", "dst_host_serror_rate",<br class="title-page-name"/>    "dst_host_srv_serror_rate", "dst_host_rerror_rate", "dst_host_srv_rerror_rate",<br class="title-page-name"/>    "attack_type"<br class="title-page-name"/>};<br class="title-page-name"/>featuresDF.RenameColumns(colnames);</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code, we are loading this raw dataset without headers, by supplying the <kbd class="calibre12">hasHeaders: false</kbd> flag to the <kbd class="calibre12">ReadCsv</kbd> method of Deedle's data frame. By supplying this flag, we are telling Deedle not to take the first row of the dataset as the header. Once this data is loaded into a data frame, we are using the <kbd class="calibre12">RenameColumns</kbd> method to attach the names of the columns to the data frame.</span></p>
<p class="calibre2">The next clean-up task we are going to take is to group the cyber attack types together by corresponding categories. You can find the mapping between the attack type and the category at the following link: <a href="http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types" class="calibre9">http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types</a>. Using this mapping, we are going to create a new column in the data frame that contains information about the attack category. Let's look at the code first:</p>
<pre class="calibre19">// keeping "normal" for now for plotting purposes<br class="title-page-name"/>IDictionary&lt;string, string&gt; attackCategories = new Dictionary&lt;string, string&gt;<br class="title-page-name"/>{<br class="title-page-name"/>    {"back", "dos"},<br class="title-page-name"/>    {"land", "dos"},<br class="title-page-name"/>    {"neptune", "dos"},<br class="title-page-name"/>    {"pod", "dos"},<br class="title-page-name"/>    {"smurf", "dos"},<br class="title-page-name"/>    {"teardrop", "dos"},<br class="title-page-name"/>    {"ipsweep", "probe"},<br class="title-page-name"/>    {"nmap", "probe"},<br class="title-page-name"/>    {"portsweep", "probe"},<br class="title-page-name"/>    {"satan", "probe"},<br class="title-page-name"/>    {"ftp_write", "r2l"},<br class="title-page-name"/>    {"guess_passwd", "r2l"},<br class="title-page-name"/>    {"imap", "r2l"},<br class="title-page-name"/>    {"multihop", "r2l"},<br class="title-page-name"/>    {"phf", "r2l"},<br class="title-page-name"/>    {"spy", "r2l"},<br class="title-page-name"/>    {"warezclient", "r2l"},<br class="title-page-name"/>    {"warezmaster", "r2l"},<br class="title-page-name"/>    {"buffer_overflow", "u2r"},<br class="title-page-name"/>    {"loadmodule", "u2r"},<br class="title-page-name"/>    {"perl", "u2r"},<br class="title-page-name"/>    {"rootkit", "u2r"},<br class="title-page-name"/>    {"normal", "normal"}<br class="title-page-name"/>};<br class="title-page-name"/><br class="title-page-name"/>featuresDF.AddColumn(<br class="title-page-name"/>    "attack_category",<br class="title-page-name"/>    featuresDF.GetColumn&lt;string&gt;("attack_type")<br class="title-page-name"/>        .Select(x =&gt; attackCategories[x.Value.Replace(".", "")])<br class="title-page-name"/>);</pre>
<p class="calibre2"><span class="calibre5">If you look closely at this code, we created a <kbd class="calibre12">Dictionary</kbd> object that has mapping between an attack type and its category. For example, the attack type, <kbd class="calibre12">"back"</kbd>, is one of the <strong class="calibre4">Denial-of-Service</strong> (<strong class="calibre4">DOS</strong>) attacks and the attack type, <kbd class="calibre12">"rootkit"</kbd>, is one of the <strong class="calibre4">User-to-Root</strong> (<strong class="calibre4">U2R</strong>) attacks. Using this mapping, we created a new column, <kbd class="calibre12">"attack_category"</kbd>, and added it to the <kbd class="calibre12">featuresDF</kbd>. Now that we have cleaned the raw dataset with column names and attack categories, we need to export it and store it into our local drive for future use. You can use the following code to export this data:</span></p>
<pre class="calibre19">featuresDF.SaveCsv(Path.Combine(dataDirPath, "data.csv"));</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Target variable distribution</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">Now that we have clean data to work with, we will start digging into the data. Let's first look at the distributions of cyber attack categories. The code to get the distribution of target variables looks as follows:</span></p>
<pre class="calibre19">// 1. Target Variable Distribution<br class="title-page-name"/>Console.WriteLine("\n\n-- Counts by Attack Category --\n");<br class="title-page-name"/>var attackCount = featuresDF.AggregateRowsBy&lt;string, int&gt;(<br class="title-page-name"/>    new string[] { "attack_category" },<br class="title-page-name"/>    new string[] { "duration" },<br class="title-page-name"/>    x =&gt; x.ValueCount<br class="title-page-name"/>).SortRows("duration");<br class="title-page-name"/>attackCount.RenameColumns(new string[] { "attack_category", "count" });<br class="title-page-name"/><br class="title-page-name"/>attackCount.Print();<br class="title-page-name"/><br class="title-page-name"/>DataBarBox.Show(<br class="title-page-name"/>    attackCount.GetColumn&lt;string&gt;("attack_category").Values.ToArray(),<br class="title-page-name"/>    attackCount["count"].Values.ToArray()<br class="title-page-name"/>).SetTitle(<br class="title-page-name"/>    "Counts by Attack Category"<br class="title-page-name"/>);</pre>
<p class="calibre2"><span class="calibre5">Similar to the previous chapters, we are using the <kbd class="calibre12">AggregateRowsBy</kbd> method in Deedle's data frame to group by the target variable, <kbd class="calibre12">attack_category</kbd>, and count the number of occurrences per category in the dataset. Then, we use the <kbd class="calibre12">DataBarBox</kbd> class to display a bar chart of this distribution. Once you run this code, the following bar chart will be displayed:</span></p>
<div class="mce-root"><img src="../images/00141.jpeg" class="calibre76"/></div>
<p class="calibre2"><span class="calibre5">And the output that shows us the number of occurrences of each cyber attack category looks as follows:</span></p>
<div class="mce-root"><img src="../images/00142.jpeg" class="calibre97"/></div>
<p class="calibre2"><span class="calibre5">There is one thing that is noticeable here. There are more DOS attack samples than normal samples in the dataset. As mentioned previously, the</span> KDD Cup 1999 dataset <span class="calibre5">that we are using for this project is artificial and simulated data, and thus, it does not reflect a real-life situation, where the number of normal internet connections will outnumber the number of all the other cyber attacks combined. We will have to keep this in mind when building models in the following sections.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Categorical variable distribution</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">The features that we have in this dataset are a mixture of categorical and continuous variables. For example, the feature named <kbd class="calibre12">duration</kbd>, which represents the length of the connection, is a continuous variable. However, the feature, named <kbd class="calibre12">protocol_type</kbd>, which represents the type of the protocol, such as <kbd class="calibre12">tcp</kbd>, <kbd class="calibre12">udp</kbd>, and so forth, is a categorical variable. For a complete set of feature descriptions, you can go to this link: <a href="http://kdd.ics.uci.edu/databases/kddcup99/task.html" class="calibre9">http://kdd.ics.uci.edu/databases/kddcup99/task.html</a>.</span></p>
<p class="calibre2">In this section, we are going to take a look at the distribution differences in the categorical variables between the normal connections and the malicious connections. The following code shows how we separate the sample set into two subgroups, one for normal connections and another for abnormal connections:</p>
<pre class="calibre19">var attackSubset = featuresDF.Rows[<br class="title-page-name"/>    featuresDF.GetColumn&lt;string&gt;("attack_category").Where(<br class="title-page-name"/>        x =&gt; !x.Value.Equals("normal")<br class="title-page-name"/>    ).Keys<br class="title-page-name"/>];<br class="title-page-name"/>var normalSubset = featuresDF.Rows[<br class="title-page-name"/>    featuresDF.GetColumn&lt;string&gt;("attack_category").Where(<br class="title-page-name"/>        x =&gt; x.Value.Equals("normal")<br class="title-page-name"/>    ).Keys<br class="title-page-name"/>];</pre>
<p class="calibre2">Now that we have these two subsets, let's start comparing the distributions of categorical variables between normal and malicious connections. Let's first take a look at the code:</p>
<pre class="calibre19">// 2. Categorical Variable Distribution<br class="title-page-name"/>string[] categoricalVars =<br class="title-page-name"/>{<br class="title-page-name"/>    "protocol_type", "service", "flag", "land"<br class="title-page-name"/>};<br class="title-page-name"/>foreach (string variable in categoricalVars)<br class="title-page-name"/>{<br class="title-page-name"/>    Console.WriteLine("\n\n-- Counts by {0} --\n", variable);<br class="title-page-name"/>    Console.WriteLine("* Attack:");<br class="title-page-name"/>    var attackCountDF = attackSubset.AggregateRowsBy&lt;string, int&gt;(<br class="title-page-name"/>        new string[] { variable },<br class="title-page-name"/>        new string[] { "duration" },<br class="title-page-name"/>        x =&gt; x.ValueCount<br class="title-page-name"/>    );<br class="title-page-name"/>    attackCountDF.RenameColumns(new string[] { variable, "count" });<br class="title-page-name"/><br class="title-page-name"/>    attackCountDF.SortRows("count").Print();<br class="title-page-name"/><br class="title-page-name"/>    Console.WriteLine("* Normal:");<br class="title-page-name"/>    var countDF = normalSubset.AggregateRowsBy&lt;string, int&gt;(<br class="title-page-name"/>        new string[] { variable },<br class="title-page-name"/>        new string[] { "duration" },<br class="title-page-name"/>        x =&gt; x.ValueCount<br class="title-page-name"/>    );<br class="title-page-name"/>    countDF.RenameColumns(new string[] { variable, "count" });<br class="title-page-name"/><br class="title-page-name"/>    countDF.SortRows("count").Print();<br class="title-page-name"/><br class="title-page-name"/>    DataBarBox.Show(<br class="title-page-name"/>        countDF.GetColumn&lt;string&gt;(variable).Values.ToArray(),<br class="title-page-name"/>        new double[][] <br class="title-page-name"/>        {<br class="title-page-name"/>            attackCountDF["count"].Values.ToArray(),<br class="title-page-name"/>            countDF["count"].Values.ToArray()<br class="title-page-name"/>        }<br class="title-page-name"/>    ).SetTitle(<br class="title-page-name"/>        String.Format("Counts by {0} (0 - Attack, 1 - Normal)", variable)<br class="title-page-name"/>    );<br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">In this code, we are iterating through an array of categorical variables: <kbd class="calibre12">protocol_type</kbd>, <kbd class="calibre12">service</kbd>, <kbd class="calibre12">flag</kbd>, and <kbd class="calibre12">land</kbd>. We will defer the feature descriptions to the description page that can be found at the following link: <a href="http://kdd.ics.uci.edu/databases/kddcup99/task.html" target="_blank" class="calibre9">http://kdd.ics.uci.edu/databases/kddcup99/task.html</a>. For each categorical variable, we used the <kbd class="calibre12">AggregateRowsBy</kbd> method to group by each type of the variable and count the number of occurrences for each type. We do this aggregation once for the normal group and then once more for the attack group. Then, we use the <kbd class="calibre12">DataBarBox</kbd> class to display bar charts to visually show the differences in the distributions. Let's take a look at a few plots and outputs.</span></p>
<p class="calibre2">The following bar chart is for the <kbd class="calibre12">protocol_type</kbd> feature:</p>
<div class="mce-root"><img src="../images/00143.jpeg" class="calibre98"/></div>
<p class="calibre2">The actual counts per type between the two groups look as follows:</p>
<div class="mce-root"><img src="../images/00144.jpeg" class="calibre99"/></div>
<p class="calibre2"><span class="calibre5">As you can see from these outputs, there are some noticeable distinctions between the distributions of normal and cyber attack groups. For example, the majority of attacks happen on <kbd class="calibre12">icmp</kbd> and <kbd class="calibre12">tcp</kbd> protocols, while the majority of normal connections are on <kbd class="calibre12">tcp</kbd> and <kbd class="calibre12">udp</kbd>.</span></p>
<p class="calibre2">The following bar chart is for the <kbd class="calibre12">land</kbd> feature:</p>
<div class="mce-root"><img src="../images/00145.jpeg" class="calibre100"/></div>
<p class="calibre2">The actual counts for each type in this feature look as follows:</p>
<div class="mce-root"><img src="../images/00146.jpeg" class="calibre101"/></div>
<p class="calibre2"><span class="calibre5">It is quite hard to tell if we can deduce any meaningful insights from these outputs. Almost all samples in the dataset have a value of <kbd class="calibre12">0</kbd> for both the attack and normal groups. Let's take a look at one more feature.</span></p>
<p class="calibre2">The following bar chart shows the distributions of the feature <kbd class="calibre12">flag</kbd> in attack and normal groups:</p>
<div class="mce-root"><img src="../images/00147.jpeg" class="calibre102"/></div>
<p class="calibre2"><span class="calibre5">And the actual counts look as follows:</span></p>
<div class="mce-root"><img src="../images/00148.jpeg" class="calibre103"/></div>
<p class="calibre2"><span class="calibre5">There are some noticeable distinctions in this feature, even though the most frequently appearing flag type for both attack and normal groups is <kbd class="calibre12">SF</kbd>. It seems the flag types <kbd class="calibre12">SF</kbd> and <kbd class="calibre12">REJ</kbd> take up the majority of the normal group. On the other hand, the flag types <kbd class="calibre12">SF</kbd>, <kbd class="calibre12">S0</kbd>, and <kbd class="calibre12">REJ</kbd> take up the majority of the attack group. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Continuous variable distribution</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">So far, we have looked at the distributions of categorical variables. Let's now look at the distributions of continuous variables in our feature set. Similar to the previous chapters, we are going to look at the quartiles for each continuous variable. The code to compute quartiles for each continuous feature looks as follows:</span></p>
<pre class="calibre19">foreach (string variable in continuousVars)<br class="title-page-name"/>{<br class="title-page-name"/>    Console.WriteLine(String.Format("\n\n-- {0} Distribution (Attack) -- ", variable));<br class="title-page-name"/>    double[] attachQuartiles = Accord.Statistics.Measures.Quantiles(<br class="title-page-name"/>        attackSubset[variable].DropMissing().ValuesAll.ToArray(),<br class="title-page-name"/>        new double[] { 0, 0.25, 0.5, 0.75, 1.0 }<br class="title-page-name"/>    );<br class="title-page-name"/>    Console.WriteLine(<br class="title-page-name"/>        "Min: \t\t\t{0:0.00}\nQ1 (25% Percentile): \t{1:0.00}\nQ2 (Median): \t\t{2:0.00}\nQ3 (75% Percentile): \t{3:0.00}\nMax: \t\t\t{4:0.00}",<br class="title-page-name"/>        attachQuartiles[0], attachQuartiles[1], attachQuartiles[2], attachQuartiles[3], attachQuartiles[4]<br class="title-page-name"/>    );<br class="title-page-name"/><br class="title-page-name"/>    Console.WriteLine(String.Format("\n\n-- {0} Distribution (Normal) -- ", variable));<br class="title-page-name"/>    double[] normalQuantiles = Accord.Statistics.Measures.Quantiles(<br class="title-page-name"/>        normalSubset[variable].DropMissing().ValuesAll.ToArray(),<br class="title-page-name"/>        new double[] { 0, 0.25, 0.5, 0.75, 1.0 }<br class="title-page-name"/>    );<br class="title-page-name"/>    Console.WriteLine(<br class="title-page-name"/>        "Min: \t\t\t{0:0.00}\nQ1 (25% Percentile): \t{1:0.00}\nQ2 (Median): \t\t{2:0.00}\nQ3 (75% Percentile): \t{3:0.00}\nMax: \t\t\t{4:0.00}",<br class="title-page-name"/>        normalQuantiles[0], normalQuantiles[1], normalQuantiles[2], normalQuantiles[3], normalQuantiles[4]<br class="title-page-name"/>    );<br class="title-page-name"/>}</pre>
<p class="calibre2">And the variable, <kbd class="calibre12">continuousVars</kbd>, is defined as the following array of strings:</p>
<pre class="calibre19">// 3. Continuous Variable Distribution<br class="title-page-name"/>string[] continuousVars =<br class="title-page-name"/>{<br class="title-page-name"/>    "duration", "src_bytes", "dst_bytes", "wrong_fragment", "urgent", "hot",<br class="title-page-name"/>    "num_failed_logins", "num_compromised", "root_shell", "su_attempted",<br class="title-page-name"/>    "num_root", "num_file_creations", "num_shells", "num_access_files",<br class="title-page-name"/>    "num_outbound_cmds", "count", "srv_count", "serror_rate", "srv_serror_rate",<br class="title-page-name"/>    "rerror_rate", "srv_rerror_rate", "same_srv_rate", "diff_srv_rate",<br class="title-page-name"/>    "srv_diff_host_rate", "dst_host_count", "dst_host_srv_count",<br class="title-page-name"/>    "dst_host_same_srv_rate", "dst_host_diff_srv_rate", "dst_host_same_src_port_rate",<br class="title-page-name"/>    "dst_host_srv_diff_host_rate", "dst_host_serror_rate", "dst_host_srv_serror_rate",<br class="title-page-name"/>    "dst_host_rerror_rate", "dst_host_srv_rerror_rate"<br class="title-page-name"/>};</pre>
<p class="calibre2"><span class="calibre5">Similar to what we did for categorical variable analysis, we start iterating through the continuous variables in the preceding code. The string array, <kbd class="calibre12">continuousVars</kbd>, contains a list of all the continuous features we have in our dataset, and we iterate through this array to start computing the quartiles of each distribution. As in the previous chapters, we are using the <kbd class="calibre12">Accord.Statistics.Measures.Quantiles</kbd> method to compute quartiles, which are min, 25% percentile, median, 75% percentile, and max numbers. We do this twice, once for the attack group and another time for the normal group, so that we can see if there are any noticeable differences in the distributions. Let's take a look at a few features and their distributions.</span></p>
<p class="calibre2">First, the following output is for the distribution of a feature called <kbd class="calibre12">duration</kbd>:</p>
<div class="mce-root"><img src="../images/00149.jpeg" class="calibre104"/></div>
<p class="calibre2"><span class="calibre5">From this output, we can see that the majority of the values for this feature are <kbd class="calibre12">0</kbd> for both attack and normal groups. As there is not so much variance in this variable, our model might not learn much information from this feature. Let's take a look at another feature.</span></p>
<p class="calibre2">The following output is for the distribution of a feature called <kbd class="calibre12">dst_bytes</kbd>, which represents the number of data bytes from destination to source:</p>
<div class="mce-root"><img src="../images/00150.jpeg" class="calibre105"/></div>
<p class="calibre2"><span class="calibre5">Here, we see some noticeable distinctions in the distributions between the attack and normal groups. Almost all the cyber attacks have a value of 0, while the values are distributed across a wide range for the normal network connections.</span></p>
<p class="calibre2">Lastly, the following output is for a feature called <kbd class="calibre12">wrong_fragment</kbd>:</p>
<div class="mce-root"><img src="../images/00151.jpeg" class="calibre106"/></div>
<p class="calibre2"><span class="calibre5">Similar to the case of the <kbd class="calibre12">duration</kbd> feature, the majority of the values are <kbd class="calibre12">0</kbd> for both the attack and normal group, which suggests that our model might not learn many insights from this feature. You can run the previous code to look at the distribution differences between the two groups for all the other features.</span></p>
<p class="calibre2">The full code to run this data analysis step can be found at this link: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.9/DataAnalyzer.cs" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.9/DataAnalyzer.cs</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Feature engineering and PCA</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">So far, we have analyzed the distributions of the target variable <kbd class="calibre12">attack_category</kbd>, as well as the categorical and continuous variables in the cyber attack dataset. In this section, we are going to focus on encoding the target variable and categorical features, and creating PCA features for our future model-building step.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Target and categorical variables encoding</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">First, we will have to encode different classes in the target variable, <kbd class="calibre12">attack_category</kbd>. If you recall from the previous data analysis step, there are five different categories: normal, <kbd class="calibre12">dos</kbd>, <kbd class="calibre12">probe</kbd>, <kbd class="calibre12">r2l</kbd>, and <kbd class="calibre12">u2r</kbd>. We are going to encode each of these string values with a corresponding integer representation. Then, we are going to encode each of the categorical variables with one-hot encoding, where we encode with 1 if the given value appears in the example, and 0 if not. Let's first load the cleaned-up data that we created in the previous data analysis step, using the following code:</span></p>
<pre class="calibre19">// Read in the Cyber Attack dataset<br class="title-page-name"/>// TODO: change the path to point to your data directory<br class="title-page-name"/>string dataDirPath = @"&lt;path-to-data-dir&gt;";<br class="title-page-name"/><br class="title-page-name"/>// Load the data into a data frame<br class="title-page-name"/>string dataPath = Path.Combine(dataDirPath, "data.csv");<br class="title-page-name"/>Console.WriteLine("Loading {0}\n\n", dataPath);<br class="title-page-name"/>var rawDF = Frame.ReadCsv(<br class="title-page-name"/>    dataPath,<br class="title-page-name"/>    hasHeaders: true,<br class="title-page-name"/>    inferTypes: true<br class="title-page-name"/>);</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code, we set <kbd class="calibre12">hasHeaders: true</kbd>, as the cleaned-up data now has correct headers associated with each of the columns. The following code shows how we went about encoding the target and categorical variables:</span></p>
<pre class="calibre19">// Encode Categorical Variables<br class="title-page-name"/>string[] categoricalVars =<br class="title-page-name"/>{<br class="title-page-name"/>    "protocol_type", "service", "flag", "land"<br class="title-page-name"/>};<br class="title-page-name"/>// Encode Target Variables<br class="title-page-name"/>IDictionary&lt;string, int&gt; targetVarEncoding = new Dictionary&lt;string, int&gt;<br class="title-page-name"/>{<br class="title-page-name"/>    {"normal", 0},<br class="title-page-name"/>    {"dos", 1},<br class="title-page-name"/>    {"probe", 2},<br class="title-page-name"/>    {"r2l", 3},<br class="title-page-name"/>    {"u2r", 4}<br class="title-page-name"/>};<br class="title-page-name"/><br class="title-page-name"/>var featuresDF = Frame.CreateEmpty&lt;int, string&gt;();<br class="title-page-name"/><br class="title-page-name"/>foreach (string col in rawDF.ColumnKeys)<br class="title-page-name"/>{<br class="title-page-name"/>    if(col.Equals("attack_type"))<br class="title-page-name"/>    {<br class="title-page-name"/>        continue;<br class="title-page-name"/>    }<br class="title-page-name"/>    else if (col.Equals("attack_category"))<br class="title-page-name"/>    {<br class="title-page-name"/>        featuresDF.AddColumn(<br class="title-page-name"/>            col, <br class="title-page-name"/>            rawDF.GetColumn&lt;string&gt;(col).Select(x =&gt; targetVarEncoding[x.Value])<br class="title-page-name"/>        );<br class="title-page-name"/>    }<br class="title-page-name"/>    else if (categoricalVars.Contains(col))<br class="title-page-name"/>    {<br class="title-page-name"/>        var categoryDF = EncodeOneHot(rawDF.GetColumn&lt;string&gt;(col), col);<br class="title-page-name"/><br class="title-page-name"/>        foreach (string newCol in categoryDF.ColumnKeys)<br class="title-page-name"/>        {<br class="title-page-name"/>            featuresDF.AddColumn(newCol, categoryDF.GetColumn&lt;int&gt;(newCol));<br class="title-page-name"/>        }<br class="title-page-name"/>    }<br class="title-page-name"/>    else<br class="title-page-name"/>    {<br class="title-page-name"/>        featuresDF.AddColumn(<br class="title-page-name"/>            col, <br class="title-page-name"/>            rawDF[col].Select((x, i) =&gt; double.IsNaN(x.Value) ? 0.0 : x.Value)<br class="title-page-name"/>        );<br class="title-page-name"/>    }<br class="title-page-name"/>}</pre>
<p class="calibre2">Let's take a deeper look at this code. We first created a string array variable, <kbd class="calibre12">categoricalVars</kbd>, which contains the column names of all the categorical variables, and a dictionary variable, <kbd class="calibre12">targetVarEncoding</kbd>, which maps each target class to an integer value. For example, we are encoding the <kbd class="calibre12">normal</kbd> class as <kbd class="calibre12">0</kbd>, the <kbd class="calibre12">dos</kbd> attack class as <kbd class="calibre12">1</kbd>, and so forth. Then, we iterate through all the columns in the <kbd class="calibre12">rawDF</kbd> data frame and start adding encoded data to the new and empty <kbd class="calibre12">featuresDF</kbd>. One thing to note here is that we use a helper function, <kbd class="calibre12">EncodeOneHot</kbd>, for encoding each of the categorical variables. Let's take a look at the following code:</p>
<pre class="calibre19">private static Frame&lt;int, string&gt; EncodeOneHot(Series&lt;int, string&gt; rows, string originalColName)<br class="title-page-name"/>{<br class="title-page-name"/><br class="title-page-name"/>    var categoriesByRows = rows.GetAllValues().Select((x, i) =&gt;<br class="title-page-name"/>    {<br class="title-page-name"/>        // Encode the categories appeared in each row with 1<br class="title-page-name"/>        var sb = new SeriesBuilder&lt;string, int&gt;();<br class="title-page-name"/>        sb.Add(String.Format("{0}_{1}", originalColName, x.Value), 1);<br class="title-page-name"/><br class="title-page-name"/>        return KeyValue.Create(i, sb.Series);<br class="title-page-name"/>    });<br class="title-page-name"/><br class="title-page-name"/>    // Create a data frame from the rows we just created<br class="title-page-name"/>    // And encode missing values with 0<br class="title-page-name"/>    var categoriesDF = Frame.FromRows(categoriesByRows).FillMissing(0);<br class="title-page-name"/><br class="title-page-name"/>    return categoriesDF;<br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">If you recall <a target="_blank" href="part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 2</a>, <em class="calibre13">Spam Email Filtering</em> and <a target="_blank" href="part0036.html#12AK80-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 3</a>, <em class="calibre13">Twitter Sentiment Analysis</em>, this code should look familiar. In this code, we iterate through each row, create a new variable that is a combination of the original column name and the value, and finally create a new Deedle data frame, <kbd class="calibre12">categoriesDF</kbd>. Once this step is done, this data frame output gets appended to the <kbd class="calibre12">featuresDF</kbd> in the previous code.</span></p>
<p class="calibre2">Now that we are done with encoding the target and categorical variables, we will need to export and store this new data frame, <kbd class="calibre12">featuresDF</kbd>. We are using the following code to store this data:</p>
<pre class="calibre19">Console.WriteLine("* Exporting feature set...");<br class="title-page-name"/>featuresDF.SaveCsv(Path.Combine(dataDirPath, "features.csv"));</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Fitting PCA</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">With the encoded data we just created in the previous section, let's start building PCA features that we are going to use for the anomaly detection in the following model-building step.</span></p>
<p class="calibre2">The first thing we need to do is separate our sample set into two separate sets—one with normal connection data and another with malicious connections. While we create these subsets, we need to create more realistic distributions between the two groups. If you recall from the previous data analysis step, we noticed that there are more malicious connections than normal connections, which is unrealistic, due to the fact that the KDD CUP 1999 Dataset is an artificial and hand-injected dataset. Aside from the purpose of creating a dataset with a more realistic number of normal and malicious connections, we need to create the two subsets so that we can apply PCA to the normal group only, and then apply it to the abnormal group.</p>
<p class="calibre2">This is because we want to learn and build principal components only from the normal connections group, and be able to flag any outliers as potential cyber attacks. We will discuss more in detail about how we are going to build an anomaly detection model using principal components.</p>
<p class="calibre2">Let's take a look at the following code for splitting our sample set into two groups—one for the normal group and another for the cyber attack group:</p>
<pre class="calibre19">// Build PCA with only normal data<br class="title-page-name"/>var rnd = new Random();<br class="title-page-name"/><br class="title-page-name"/>int[] normalIdx = featuresDF["attack_category"]<br class="title-page-name"/>    .Where(x =&gt; x.Value == 0)<br class="title-page-name"/>    .Keys<br class="title-page-name"/>    .OrderBy(x =&gt; rnd.Next())<br class="title-page-name"/>    .Take(90000).ToArray();<br class="title-page-name"/>int[] attackIdx = featuresDF["attack_category"]<br class="title-page-name"/>    .Where(x =&gt; x.Value &gt; 0)<br class="title-page-name"/>    .Keys<br class="title-page-name"/>    .OrderBy(x =&gt; rnd.Next())<br class="title-page-name"/>    .Take(10000).ToArray();<br class="title-page-name"/>int[] totalIdx = normalIdx.Concat(attackIdx).ToArray();</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code, we are building arrays of indexes for the normal and cyber attack groups by filtering for whether the <kbd class="calibre12">attack_category</kbd> is <kbd class="calibre12">0</kbd> (normal) or greater than 0 (cyber attacks). Then, we randomly select 90,000 samples from the normal connections and 10,000 samples from the malicious connections. Now that we have the indexes for the normal and abnormal groups, we are going to use the following code to build the actual data for fitting PCA:</span></p>
<pre class="calibre19">var normalSet = featuresDF.Rows[normalIdx];<br class="title-page-name"/><br class="title-page-name"/>string[] nonZeroValueCols = normalSet.ColumnKeys.Where(<br class="title-page-name"/>    x =&gt; !x.Equals("attack_category") &amp;&amp; normalSet[x].Max() != normalSet[x].Min()<br class="title-page-name"/>).ToArray();<br class="title-page-name"/><br class="title-page-name"/>double[][] normalData = BuildJaggedArray(<br class="title-page-name"/>    normalSet.Columns[nonZeroValueCols].ToArray2D&lt;double&gt;(), <br class="title-page-name"/>    normalSet.RowCount, <br class="title-page-name"/>    nonZeroValueCols.Length<br class="title-page-name"/>);<br class="title-page-name"/>double[][] wholeData = BuildJaggedArray(<br class="title-page-name"/>    featuresDF.Rows[totalIdx].Columns[nonZeroValueCols].ToArray2D&lt;double&gt;(),<br class="title-page-name"/>    totalIdx.Length,<br class="title-page-name"/>    nonZeroValueCols.Length<br class="title-page-name"/>);<br class="title-page-name"/>int[] labels = featuresDF<br class="title-page-name"/>    .Rows[totalIdx]<br class="title-page-name"/>    .GetColumn&lt;int&gt;("attack_category")<br class="title-page-name"/>    .ValuesAll.ToArray();</pre>
<p class="calibre2">As you can see from this code, the <kbd class="calibre12">normalData</kbd> variable contains all the normal connection samples and the <kbd class="calibre12">wholeData</kbd> variable contains both the normal and cyber attack connection samples. We will be using <kbd class="calibre12">normalData</kbd> to fit PCA, and then apply this learned PCA to the <kbd class="calibre12">wholeData</kbd>, as you can see from the following code:</p>
<pre class="calibre19">var pca = new PrincipalComponentAnalysis(<br class="title-page-name"/>    PrincipalComponentMethod.Standardize<br class="title-page-name"/>);<br class="title-page-name"/>pca.Learn(normalData);<br class="title-page-name"/><br class="title-page-name"/>double[][] transformed = pca.Transform(wholeData);</pre>
<p class="calibre2"><span class="calibre5">As in <a target="_blank" href="part0097.html#2SG6I0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 8</a>, <em class="calibre13">Handwritten Digit Recognition</em></span>, <span class="calibre5">we are using the <kbd class="calibre12">PrincipalComponentAnalysis</kbd> class in the Accord.NET framework to fit PCA. Once we have trained PCA with the normal connections data, we apply it to the <kbd class="calibre12">wholeData</kbd> that contains both normal and cyber attack connections by using the <kbd class="calibre12">Transform</kbd> method of the <kbd class="calibre12">pca</kbd> object.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">PCA features</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">We have now built principal components using just the normal connections group. Let's briefly inspect how well our target classes are separated on different combinations of principal components. Take a look at the following code:</span></p>
<pre class="calibre19">double[][] first2Components = transformed.Select(<br class="title-page-name"/>    x =&gt; x.Where((y, i) =&gt; i &lt; 2).ToArray()<br class="title-page-name"/>).ToArray();<br class="title-page-name"/>ScatterplotBox.Show("Component #1 vs. Component #2", first2Components, labels);<br class="title-page-name"/><br class="title-page-name"/>double[][] next2Components = transformed.Select(<br class="title-page-name"/>    x =&gt; x.Where((y, i) =&gt; i &lt; 3 &amp;&amp; i &gt;= 1).ToArray()<br class="title-page-name"/>).ToArray();<br class="title-page-name"/>ScatterplotBox.Show("Component #2 vs. Component #3", next2Components, labels);<br class="title-page-name"/><br class="title-page-name"/>next2Components = transformed.Select(<br class="title-page-name"/>    x =&gt; x.Where((y, i) =&gt; i &lt; 4 &amp;&amp; i &gt;= 2).ToArray()<br class="title-page-name"/>).ToArray();<br class="title-page-name"/>ScatterplotBox.Show("Component #3 vs. Component #4", next2Components, labels);<br class="title-page-name"/><br class="title-page-name"/>next2Components = transformed.Select(<br class="title-page-name"/>    x =&gt; x.Where((y, i) =&gt; i &lt; 5 &amp;&amp; i &gt;= 3).ToArray()<br class="title-page-name"/>).ToArray();<br class="title-page-name"/>ScatterplotBox.Show("Component #4 vs. Component #5", next2Components, labels);<br class="title-page-name"/><br class="title-page-name"/>next2Components = transformed.Select(<br class="title-page-name"/>    x =&gt; x.Where((y, i) =&gt; i &lt; 6 &amp;&amp; i &gt;= 4).ToArray()<br class="title-page-name"/>).ToArray();<br class="title-page-name"/>ScatterplotBox.Show("Component #5 vs. Component #6", next2Components, labels);</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code, we are building scatter plots between two principal components at a time, for the first six components. When you run this code, you will see plots similar to the following ones.</span></p>
<p class="calibre2">The first plot is between the first and second principal components, and looks as follows:</p>
<div class="mce-root"><img src="../images/00152.jpeg" class="calibre107"/></div>
<p class="calibre2">The blue dots represent the normal connections, and the other dots with different colors represent the cyber attacks. We can see some distinctions in the distributions among different classes, but the pattern does not seem so strong.</p>
<p class="calibre2">The following plot is between the second and third components:</p>
<div class="mce-root"><img src="../images/00153.jpeg" class="calibre108"/></div>
<p class="calibre2">Lastly, the following plot is between the third and fourth components:</p>
<div class="mce-root"><img src="../images/00154.jpeg" class="calibre109"/></div>
<p class="calibre2"><span class="calibre5">We cannot really see many distinctions among different classes in the last plot. Although the pattern does not seem very strong, previous scatter plots show some differences in the distributions. It is especially more difficult to visually see the distinctions on two-dimensional plots. If we take this to higher-dimensional space, which our anomaly detection model is going to be looking at, the differences in the patterns will become more noticeable.</span></p>
<p class="calibre2">Let's now look at the amount of variance explained from the principal components. The following code shows how we can get the cumulative proportion of variances explained, and display it in a line chart:</p>
<pre class="calibre19">double[] explainedVariance = pca.Components<br class="title-page-name"/>    .Select(x =&gt; x.CumulativeProportion)<br class="title-page-name"/>    .Where(x =&gt; x &lt; 1)<br class="title-page-name"/>    .ToArray();<br class="title-page-name"/><br class="title-page-name"/>DataSeriesBox.Show(<br class="title-page-name"/>    explainedVariance.Select((x, i) =&gt; (double)i),<br class="title-page-name"/>    explainedVariance<br class="title-page-name"/>).SetTitle("Explained Variance");<br class="title-page-name"/>System.IO.File.WriteAllLines(<br class="title-page-name"/>    Path.Combine(dataDirPath, "explained-variance.csv"),<br class="title-page-name"/>    explainedVariance.Select((x, i) =&gt; String.Format("{0},{1:0.0000}", i, x))<br class="title-page-name"/>);</pre>
<p class="calibre2"><span class="calibre5">If you look at this code more closely, the <kbd class="calibre12">Components</kbd> property in the <kbd class="calibre12">pca</kbd> object contains information about the proportion of variance explained. We can iterate through each component and get the cumulative proportion by using the <kbd class="calibre12">CumulativeProportion</kbd> property. Once we have extracted these values, we then use the <kbd class="calibre12">DataSeriesBox</kbd> class to display a line chart that shows the cumulative proportion of variance explained. The output looks like the following:</span></p>
<div class="mce-root"><img src="../images/00155.jpeg" class="calibre110"/></div>
<p class="calibre2"><span class="calibre5">Now, we have successfully created PCA features and have full PCA-transformed data. You can use the following code to export this data:</span></p>
<pre class="calibre19">Console.WriteLine("* Exporting pca-transformed feature set...");<br class="title-page-name"/>System.IO.File.WriteAllLines(<br class="title-page-name"/>    Path.Combine(<br class="title-page-name"/>        dataDirPath,<br class="title-page-name"/>        "pca-transformed-features.csv"<br class="title-page-name"/>    ),<br class="title-page-name"/>    transformed.Select(x =&gt; String.Join(",", x))<br class="title-page-name"/>);<br class="title-page-name"/>System.IO.File.WriteAllLines(<br class="title-page-name"/>    Path.Combine(<br class="title-page-name"/>        dataDirPath,<br class="title-page-name"/>        "pca-transformed-labels.csv"<br class="title-page-name"/>    ),<br class="title-page-name"/>    labels.Select(x =&gt; x.ToString())<br class="title-page-name"/>);</pre>
<p class="calibre2">The full code for the feature engineering step can be found at this link: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.9/FeatureEngineering.cs" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.9/FeatureEngineering.cs</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Principal component classifier for anomaly detection</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">We have compiled everything and are now ready to start building an anomaly detection model for the cyber attack detection project. As mentioned previously, we are going to use the data of the distributions of principal components from the normal connections group, and take it as the normal ranges of principal components. For any records that deviate from these normal ranges of the principal component values, we are going to flag them as abnormal and potential cyber attacks.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Preparation for training</h1>
                
            
            <article>
                
<p class="calibre2">First, let's load the features data that we created from the feature engineering step. You can use the following code to load the PCA-transformed data and the labels data:</p>
<pre class="calibre19">// Read in the Cyber Attack dataset<br class="title-page-name"/>// TODO: change the path to point to your data directory<br class="title-page-name"/>string dataDirPath = @"&lt;path-to-dir&gt;";<br class="title-page-name"/><br class="title-page-name"/>// Load the data into a data frame<br class="title-page-name"/>string dataPath = Path.Combine(dataDirPath, "pca-transformed-features.csv");<br class="title-page-name"/>Console.WriteLine("Loading {0}\n\n", dataPath);<br class="title-page-name"/>var featuresDF = Frame.ReadCsv(<br class="title-page-name"/>    dataPath,<br class="title-page-name"/>    hasHeaders: false,<br class="title-page-name"/>    inferTypes: true<br class="title-page-name"/>);<br class="title-page-name"/>featuresDF.RenameColumns(<br class="title-page-name"/>    featuresDF.ColumnKeys.Select((x, i) =&gt; String.Format("component-{0}", i + 1))<br class="title-page-name"/>);<br class="title-page-name"/><br class="title-page-name"/>int[] labels = File.ReadLines(<br class="title-page-name"/>    Path.Combine(dataDirPath, "pca-transformed-labels.csv")<br class="title-page-name"/>).Select(x =&gt; int.Parse(x)).ToArray();<br class="title-page-name"/>featuresDF.AddColumn("attack_category", labels);</pre>
<p class="calibre2">Let's quickly look at the distributions of our target classes. The code to count by each target class is as follows:</p>
<pre class="calibre19">var count = featuresDF.AggregateRowsBy&lt;string, int&gt;(<br class="title-page-name"/>    new string[] { "attack_category" },<br class="title-page-name"/>    new string[] { "component-1" },<br class="title-page-name"/>    x =&gt; x.ValueCount<br class="title-page-name"/>).SortRows("component-1");<br class="title-page-name"/>count.RenameColumns(new string[] { "attack_category", "count" });<br class="title-page-name"/>count.Print();</pre>
<p class="calibre2"><span class="calibre5">Once you run this code, you will see the following output:</span></p>
<div class="mce-root"><img src="../images/00156.jpeg" class="calibre26"/></div>
<p class="calibre2"><span class="calibre5">As expected, the majority of the samples belong to the 0 class, which is the normal group, and the rest combined are the minority (about 10%) in our sample set. This is a more realistic view of cyber attacks. Cyber attacks happen way less frequently than normal connections.</span></p>
<p class="calibre2">For illustration purposes, we are going to use the first <kbd class="calibre12">27</kbd> principal components that explain about 70% of the overall variance in the dataset. You can experiment with different numbers of principal components and see how model performances change. The following code shows how we created a training set using the first <kbd class="calibre12">27</kbd> principal components:</p>
<pre class="calibre19">// First 13 components explain about 50% of the variance<br class="title-page-name"/>// First 19 components explain about 60% of the variance<br class="title-page-name"/>// First 27 components explain about 70% of the variance<br class="title-page-name"/>// First 34 components explain about 80% of the variance<br class="title-page-name"/>int numComponents = 27;<br class="title-page-name"/>string[] cols = featuresDF.ColumnKeys.Where((x, i) =&gt; i &lt; numComponents).ToArray();<br class="title-page-name"/><br class="title-page-name"/>// First, compute distances from the center/mean among normal events<br class="title-page-name"/>var normalDF = featuresDF.Rows[<br class="title-page-name"/>    featuresDF["attack_category"].Where(x =&gt; x.Value == 0).Keys<br class="title-page-name"/>].Columns[cols];<br class="title-page-name"/><br class="title-page-name"/>double[][] normalData = BuildJaggedArray(<br class="title-page-name"/>    normalDF.ToArray2D&lt;double&gt;(), normalDF.RowCount, cols.Length<br class="title-page-name"/>);</pre>
<p class="calibre2"><span class="calibre5">If you look at this code closely, you will notice that we are creating <kbd class="calibre12">normalDF</kbd> and <kbd class="calibre12">normalData</kbd> variables with normal connection samples only. As mentioned previously, we want to learn only from the normal data, so that we can flag any outliers and extreme deviations from the normal ranges of principal components. We are going to use these variables to build a principal component classifier for the cyber attack detection in the following section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Building a principal component classifier</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">In order to build a principal component classifier, which will flag those events that deviate from the normal connections, we need to calculate the distance between a record and the distributions of normal connections. We are going to use a distance metric, the <strong class="calibre4">Mahalanobis distance</strong>, which measures the distance between a point and a distribution. For the standardized principal components, like those here, the equation to compute the <strong class="calibre4">Mahalanobis distance</strong> is as follows:</span></p>
<div class="mce-root"><img class="fm-editor-equation2" src="../images/00157.jpeg"/></div>
<p class="calibre2"><em class="calibre13">C<sub class="calibre111">i</sub></em> in this equation represents the value of each principal component, and <em class="calibre13">var<sub class="calibre111">i</sub></em> represents the variance of each principal component. Let's take a look at the following example:</p>
<div class="mce-root"><img src="../images/00158.jpeg" class="calibre112"/></div>
<p class="calibre2">Assume you have 5 principal components with values as shown in this image and assume the variance for each principal is 1 for simplicity and demonstration purposes, then you can compute the <strong class="calibre4">Mahalanobis distance</strong> as the following:</p>
<div class="mce-root5"><img src="../images/00159.jpeg" class="calibre113"/></div>
<p class="calibre2">And the computed <strong class="calibre4">Mahalanobis distance</strong> for this example is 0.64. For a more detailed description of this distance metric, it is recommended that you review the following Wikipedia page: <a href="https://en.wikipedia.org/wiki/Mahalanobis_distance" target="_blank" class="calibre9">https://en.wikipedia.org/wiki/Mahalanobis_distance</a>, or the following research paper: <a href="https://users.cs.fiu.edu/~chens/PDF/ICDM03_WS.pdf" class="calibre9">https://users.cs.fiu.edu/~chens/PDF/ICDM03_WS.pdf</a>.</p>
<p class="calibre2">We implemented the Mahalanobis distance equation as a helper function, <kbd class="calibre12">ComputeDistances</kbd>, and it looks as follows:</p>
<pre class="calibre19">private static double[] ComputeDistances(double[][] data, double[] componentVariances)<br class="title-page-name"/>{<br class="title-page-name"/><br class="title-page-name"/>    double[] distances = data.Select(<br class="title-page-name"/>        (row, i) =&gt; Math.Sqrt(<br class="title-page-name"/>            row.Select(<br class="title-page-name"/>                (x, j) =&gt; Math.Pow(x, 2) / componentVariances[j]<br class="title-page-name"/>            ).Sum()<br class="title-page-name"/>        )<br class="title-page-name"/>    ).ToArray();<br class="title-page-name"/><br class="title-page-name"/>    return distances;<br class="title-page-name"/>}</pre>
<p class="calibre2">As you can see from this code snippet, the <kbd class="calibre12">ComputeDistances</kbd> method takes in two arguments—<kbd class="calibre12">data</kbd> and <kbd class="calibre12">componentVariances</kbd>. The variable <kbd class="calibre12">data</kbd> is a two-dimensional array that we want to compute distances for, and the <kbd class="calibre12">componentVariances</kbd> variable is the variance of the principal components that are learned from the normal network connections data. In order to compute the variances of the principal components, we use the following helper function:</p>
<pre class="calibre19">private static double[] ComputeVariances(double[][] data)<br class="title-page-name"/>{<br class="title-page-name"/>    double[] componentVariances = new double[data[0].Length];<br class="title-page-name"/><br class="title-page-name"/>    for (int j = 0; j &lt; data[0].Length; j++)<br class="title-page-name"/>    {<br class="title-page-name"/>        componentVariances[j] = data<br class="title-page-name"/>            .Select((x, i) =&gt; Math.Pow(data[i][j], 2))<br class="title-page-name"/>            .Sum() / data.Length;<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    return componentVariances;<br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code snippet, it is computing the variances of individual columns, where each column of this two-dimensional array represents each principal component. In order to compute the distances of individual records from the distribution of normal network connections data, we can simply pass the two-dimensional array to the helper function <kbd class="calibre12">ComputeDistances</kbd>, as follows:</span></p>
<pre class="calibre19">double[] distances = ComputeDistances(normalData);</pre>
<p class="calibre2">Now that we have computed the distances of individual records, let's analyze how the ranges for the normal connections look. We used the following code to calculate the mean and standard deviation of the distances, and a histogram to visualize the overall distance distributions:</p>
<pre class="calibre19">double meanDistance = distances.Average();<br class="title-page-name"/>double stdDistance = Math.Sqrt(<br class="title-page-name"/>    distances<br class="title-page-name"/>    .Select(x =&gt; Math.Pow(x - meanDistance, 2))<br class="title-page-name"/>    .Sum() / distances.Length<br class="title-page-name"/>);<br class="title-page-name"/><br class="title-page-name"/>Console.WriteLine(<br class="title-page-name"/>    "* Normal - mean: {0:0.0000}, std: {1:0.0000}",<br class="title-page-name"/>    meanDistance, stdDistance<br class="title-page-name"/>);<br class="title-page-name"/><br class="title-page-name"/>HistogramBox.Show(<br class="title-page-name"/>    distances,<br class="title-page-name"/>    title: "Distances"<br class="title-page-name"/>)<br class="title-page-name"/>.SetNumberOfBins(50);</pre>
<p class="calibre2"><span class="calibre5">When you run this code, you will see the following output for the mean and standard deviation of the distance metrics for normal connections:</span></p>
<div class="mce-root"><img src="../images/00160.jpeg" class="calibre26"/></div>
<p class="calibre2"><span class="calibre5">And the histogram looks as follows:</span></p>
<div class="mce-root"><img src="../images/00161.jpeg" class="calibre114"/></div>
<p class="calibre2"><span class="calibre5">As you can see from these outputs, the majority of the distances are very small, which suggests that the non-attack and normal connections are typically clustered together closely. With this information about the distance distributions within the normal connections group, let's start looking to see if we can build a detection model by flagging certain network connections that go beyond the normal range of distances.</span></p>
<p class="calibre2">The following code shows how we computed the distances of cyber attack connections from the distribution of normal network connections:</p>
<pre class="calibre19">// Detection<br class="title-page-name"/>var attackDF = featuresDF.Rows[<br class="title-page-name"/>    featuresDF["attack_category"].Where(x =&gt; x.Value &gt; 0).Keys<br class="title-page-name"/>].Columns[cols];<br class="title-page-name"/><br class="title-page-name"/>double[][] attackData = BuildJaggedArray(<br class="title-page-name"/>    attackDF.ToArray2D&lt;double&gt;(), attackDF.RowCount, cols.Length<br class="title-page-name"/>);<br class="title-page-name"/><br class="title-page-name"/>double[] attackDistances = ComputeDistances(attackData, normalVariances);</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code, we first created a variable, called <kbd class="calibre12">attackData</kbd>, which contains all the cyber attack connections from our training set. Then, we used the <kbd class="calibre12">ComputeDistances</kbd> method to calculate the distances of individual records in the cyber attack connections group.</span></p>
<p class="calibre2">Now, we are ready to start flagging suspicious network connections based on the distance metrics that we just calculated. Let's take a look at the following code first:</p>
<pre class="calibre19">// 5-10% false alarm rate<br class="title-page-name"/>for (int i = 4; i &lt; 10; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    double targetFalseAlarmRate = 0.01 * (i + 1);<br class="title-page-name"/>    double threshold = Accord.Statistics.Measures.Quantile(<br class="title-page-name"/>        distances,<br class="title-page-name"/>        1 - targetFalseAlarmRate<br class="title-page-name"/>    );<br class="title-page-name"/><br class="title-page-name"/>    int[] detected = attackDistances.Select(x =&gt; x &gt; threshold ? 1 : 0).ToArray();<br class="title-page-name"/><br class="title-page-name"/>    EvaluateResults(attackLabels, detected, targetFalseAlarmRate);<br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code, we decide the threshold based on the distribution of distances within the normal connections group. For example, if our target is to have a 5% false alarm rate, we flag all the connections that have distances from the normal range greater than the 95% percentile of the distribution of distances within the normal connections group. More specifically, the 95% percentile of the normal connections' distance distribution in our case was 5.45. So, in this case, we will flag all the connections that have distances from the normal range greater than 5.45 as cyber attacks. We repeat this process for the false alarm rates from 5% to 10%. We will discuss the performance of this anomaly detection model in more detail in the following model-evaluation step.</span></p>
<p class="calibre2">The full code for the model-building step can be found at this link: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.9/Modeling.cs" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.9/Modeling.cs</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Evaluating anomaly detection models</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">We built an anomaly detection model for cyber attacks in the previous model-building step. In the previous code, you might have noticed that we are using a function named <kbd class="calibre12">EvaluateResults</kbd>. It is a helper function that we wrote for evaluating model performances. Let's take a look at the following code:</span></p>
<pre class="calibre19">private static void EvaluateResults(int[] attackLabels, int[] detected, double targetFalseAlarmRate)<br class="title-page-name"/>{<br class="title-page-name"/>    double overallRecall = (double)detected.Sum() / attackLabels.Length;<br class="title-page-name"/><br class="title-page-name"/>    double[] truePositives = new double[4];<br class="title-page-name"/>    double[] actualClassCounts = new double[4];<br class="title-page-name"/><br class="title-page-name"/>    for (int i = 0; i &lt; attackLabels.Length; i++)<br class="title-page-name"/>    {<br class="title-page-name"/>        actualClassCounts[attackLabels[i] - 1] += 1.0;<br class="title-page-name"/><br class="title-page-name"/>        if (detected[i] &gt; 0)<br class="title-page-name"/>        {<br class="title-page-name"/>            truePositives[attackLabels[i] - 1] += 1.0;<br class="title-page-name"/>        }<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    double[] recalls = truePositives.Select((x, i) =&gt; x / actualClassCounts[i]).ToArray();<br class="title-page-name"/><br class="title-page-name"/>    Console.WriteLine("\n\n---- {0:0.0}% False Alarm Rate ----", targetFalseAlarmRate * 100.0);<br class="title-page-name"/>    Console.WriteLine("* Overall Attack Detection: {0:0.00}%", overallRecall * 100.0);<br class="title-page-name"/>    Console.WriteLine(<br class="title-page-name"/>        "* Detection by Attack Type:\n\t{0}",<br class="title-page-name"/>        String.Join("\n\t", recalls.Select(<br class="title-page-name"/>            (x, i) =&gt; String.Format("Class {0}: {1:0.00}%", (i + 1), x * 100.0))<br class="title-page-name"/>        )<br class="title-page-name"/>    );<br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code, we are interested in two metrics: overall cyber attack detection rate and per-class detection rate. The evaluation results look as follows:</span></p>
<div class="mce-root"><img src="../images/00162.jpeg" class="calibre115"/></div>
<p class="calibre2">The overall results look good with over 99% detection rates. At 5% false alarm rate, about 99.1% of the cyber attacks are detected. However, if we look closer at the per-class detection rates, we can see their weaknesses and strengths. At 5% false alarm rate, our model does very well for detecting classes 1 and 2, which are <kbd class="calibre12">dos</kbd> and <kbd class="calibre12">probe</kbd> attacks. On the other hand, our model does poorly in detecting classes 3 and 4, which are <kbd class="calibre12">r2l</kbd> and <kbd class="calibre12">u2r</kbd> attacks. As you can see from this output, as we increase the target false alarm rates, the overall and per-class detection rates increase as well. In a real-world situation, you will have to evaluate the trade-offs between a higher detection rate and a higher false alarm rate, and make a decision on the target false alarm rate that meets your business requirements.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we built our very first anomaly detection model that can detect cyber attacks. At the beginning of this chapter, we discussed how this type of anomaly detection model can be used and applied to real-life situations, and how developing an anomaly detection model is different from other ML models that we have built so far. Then, we started analyzing the distributions of target classes and various features to understand the dataset better. While we were analyzing this dataset, we also noticed how there are more cyber attack samples than normal connection samples, which is unrealistic in real life. In order to simulate real-life situations, where abnormal malicious connections occur much less frequently than normal connections, we randomly sub-selected the normal and malicious connection samples so that 90% of the training set were normal connections and only 10% were cyber attack examples.</p>
<p class="calibre2">With this sub-selected training set, we applied PCA to the normal connections data to find out the normal ranges of principal components. Using the <strong class="calibre4">Mahalanobis distance</strong> metric, we computed the distances between individual records from the distributions of normal connections. During the model-building step, we experimented with different thresholds based on the target false alarm rates. Using 5% to 10% false alarm rates, we built cyber attack detection models and evaluated their performance. In our model-evaluation step, we noticed that the overall detection rates were over 99%, while a closer look at per-attack detection rates exposed the weaknesses and strengths of the models. We also noticed that as we sacrifice and increase the false alarm rates, the overall cyber attack detection rates improved. When applying this anomaly detection technique, it becomes necessary to understand this tradeoff between the false alarm rate and detection rate, and make a decision based on pertinent business requirements.</p>
<p class="calibre2">In the next chapter, we are going to expand our knowledge and experience in building anomaly detection models. We are going to work on a credit card fraud detection project with a credit card dataset. On top of the PCA-based anomaly detection model, we are going to discuss how to use a one-class support vector machine for anomaly detection.</p>


            </article>

            
        </section>
    </body></html>