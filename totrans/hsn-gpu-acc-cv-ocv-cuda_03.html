<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Threads, Synchronization, and Memory</h1>
                </header>
            
            <article>
                
<p>In the last chapter, we saw how to write CUDA programs that leverage the processing capabilities of a GPU by executing multiple threads and blocks in parallel. In all programs, until the last chapter, all threads were independent of each other and there was no communication between multiple threads. Most of the real-life applications need communication between intermediate threads. So, in this chapter, we will look in detail at how communication between different threads can be done, and explain the synchronization between multiple threads working on the same data. We will examine the hierarchical memory architecture of a CUDA and how different memories can be used to accelerate CUDA programs. The last part of this chapter explains a very useful application of a CUDA in the dot product of vectors and matrix multiplication, using all the concepts we have covered earlier.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Thread calls</li>
<li>CUDA memory architecture</li>
<li>Global, local, and cache memory</li>
<li>Shared memory and thread synchronization</li>
<li>Atomic operations</li>
<li>Constant and texture memory</li>
<li>Dot product and a matrix multiplication example</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>This chapter requires familiarity with the basic C or C++ programming language and the codes that were explained in the previous chapters.</span><span> </span><span>All the code used in this chapter can be downloaded from the following </span>GitHub<span> link: <a href="https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA">https://GitHub.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA</a></span><span>. The code can be executed on any operating system, though it has only been tested on Windows 10. </span></p>
<p>Check out the following video to see the code in action:<br/>
<a href="http://bit.ly/2prnGAD">http://bit.ly/2prnGAD</a></p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Threads</h1>
                </header>
            
            <article>
                
<p>The CUDA has a hierarchical architecture in terms of parallel execution. The kernel execution can be done in parallel with multiple blocks. Each block is further divided into multiple threads. In the last chapter, we saw that CUDA runtime can carry out parallel operations by launching the same copies of the kernel multiple times. We saw that it can be done in two ways: either by launching multiple blocks in parallel, with one thread per block, or by launching a single block, with many threads in parallel. So, two questions you might ask are, which method should I use in my code? And, is there any limitation on the number of blocks and threads that can be launched in parallel?</p>
<p>The answers to these questions are pivotal. As we will see later on in this chapter, threads in the same blocks can communicate with each other via shared memory. So, there is an advantage to launching one block with many threads in parallel so that they can communicate with each other. In the last chapter, we also saw the <kbd>maxThreadPerBlock</kbd> property that limits the number of threads that can be launched per block. Its value is 512 or 1,024 for the latest GPUs. Similarly, in the second method, the maximum number of blocks that can be launched in parallel is limited to 65,535.</p>
<p>Ideally, instead of launching multiple threads per single block or multiple blocks with a single thread, we launch multiple blocks with each having multiple threads (which can be equal to <kbd>maxThreadPerBlock</kbd>) in parallel. So, suppose you want to launch N = 50,000 threads in parallel in the vector add example, which we saw in the last chapter. The kernel call would be as follows:</p>
<p> <kbd>gpuAdd&lt;&lt; &lt;((N +511)/512),512 &gt; &gt;&gt;(d_a,d_b,d_c)</kbd>  </p>
<p>The maximum threads per block are 512, and hence the total number of blocks is calculated by dividing the total number of threads (N) by 512. But if N is not an exact multiple of 512, then N divided by 512 may give a wrong number of blocks, which is one less than the actual count. So, to get the next highest integer value for the number of blocks, 511 is added to N and then it is divided by 512. It is basically the <strong>ceil</strong> operation on division.</p>
<p>Now, the question is, will this work for all values of N? The answer, sadly, is no. From the preceding discussion, the total number of blocks can't go beyond 65,535. So, in the afore as-mentioned kernel call, if <kbd>(N+511)/512</kbd> is above  65,535, then again the code will fail. To overcome this, a small constant number of blocks and threads are launched with some modification in the kernel code, which we will see further by rewriting the kernel for our vector addition program, as seen in <a href="bf5e2281-2978-4e37-89d8-8c4b781a34cd.xhtml">Chapter 2</a>, <em>Parallel Programming using Cuda C</em>:</p>
<pre>#include "stdio.h"<br/>#include&lt;iostream&gt;<br/>#include &lt;cuda.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/>//Defining number of elements in array<br/>#define N 50000<br/>__global__ void gpuAdd(int *d_a, int *d_b, int *d_c)<br/>{<br/>    //Getting index of current kernel<br/>  int tid = threadIdx.x + blockIdx.x * blockDim.x; <br/>    <br/>  while (tid &lt; N)<br/>    {<br/>       d_c[tid] = d_a[tid] + d_b[tid];<br/>       tid += blockDim.x * gridDim.x;<br/>    }<br/>}</pre>
<p>This kernel code is similar to what we wrote in the last chapter. It has two modifications. One modification is in the calculation of thread ID and the second is the inclusion of the <kbd>while</kbd> loop in the kernel function. The change in thread ID calculation is due to the launching of multiple threads and blocks in parallel. This calculation can be understood by considering blocks and threads as a two-dimensional matrix with the number of blocks equal to the number of rows, and the number of columns equal to the number of threads per block. We will take an example of three blocks and three threads/blocks, as shown in the following table:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-200 image-border" src="assets/af773f86-ee8e-4395-af5f-b6139b230bac.png" style="" width="305" height="77"/></div>
<p>We can get the ID of each block by using <kbd>blockIdx.x</kbd> and the ID of each thread in the current block by the <kbd>threadIdx.x</kbd> command. So, for the thread shown in green, the block ID will be 2 and the thread ID will be 1. But what if we want a unique index for this thread among all the threads? This can be calculated by multiplying its block ID with the total number of threads per block, which is given by <kbd>blockDim.x</kbd>, and then summing it with its thread ID. This can be represented mathematically as follows:</p>
<pre>tid = threadIdx.x + blockIdx.x * blockDim.x; </pre>
<p>For example, in green, <kbd>threadIdx.x = 1</kbd>, <kbd>blockIdx.x = 2</kbd> , and <kbd>blockDim.x = 3</kbd> equals <kbd>tid = 7</kbd>. This calculation is very important to learn as it will be used widely in your code.</p>
<p>The <kbd>while</kbd> loop is included in the code because when N is very large, the total number of threads can't be equal to N because of the limitation described earlier. So, one thread has to do multiple operations separated by the total number of threads launched. This value can be calculated by multiplying <kbd>blockDim.x</kbd>  by  <kbd>gridDim.x</kbd>, which gives block and grid dimensions, respectively. Inside the <kbd>while</kbd> loop, the thread ID is incremented by this offset value. Now, this code will work for any value of N. To complete the program, we will write the main function for this code as follows: </p>
<pre class="mce-root">int main(void) <br/>{<br/>    //Declare host and device arrays<br/>  int h_a[N], h_b[N], h_c[N];<br/>  int *d_a, *d_b, *d_c;<br/><br/>    //Allocate Memory on Device<br/>  cudaMalloc((void**)&amp;d_a, N * sizeof(int));<br/>  cudaMalloc((void**)&amp;d_b, N * sizeof(int));<br/>  cudaMalloc((void**)&amp;d_c, N * sizeof(int));<br/>    //Initialize host array<br/>  for (int i = 0; i &lt; N; i++) <br/>  {<br/>    h_a[i] = 2 * i*i;<br/>    h_b[i] = i;<br/>  }<br/><br/>  cudaMemcpy(d_a, h_a, N * sizeof(int), cudaMemcpyHostToDevice);<br/>  cudaMemcpy(d_b, h_b, N * sizeof(int), cudaMemcpyHostToDevice);<br/>    //Kernel Call<br/>  gpuAdd &lt;&lt; &lt;512, 512 &gt;&gt; &gt;(d_a, d_b, d_c);<br/><br/>  cudaMemcpy(h_c, d_c, N * sizeof(int), cudaMemcpyDeviceToHost);<br/>    //This ensures that kernel execution is finishes before going forward<br/>  cudaDeviceSynchronize();<br/>  int Correct = 1;<br/>  printf("Vector addition on GPU \n");<br/>  for (int i = 0; i &lt; N; i++) <br/>  {<br/>    if ((h_a[i] + h_b[i] != h_c[i]))<br/>      { Correct = 0; }<br/>  }<br/>  if (Correct == 1)<br/>  { <br/>    printf("GPU has computed Sum Correctly\n"); <br/>  }<br/>  else<br/>  { <br/>    printf("There is an Error in GPU Computation\n");<br/>  }<br/>    //Free up memory<br/>  cudaFree(d_a);<br/>  cudaFree(d_b);<br/>   cudaFree(d_c);<br/>  return 0;<br/>}<br/><br/></pre>
<p>Again, the main function is very similar to what we wrote last time. The only changes are in terms of how we launch the kernel function. The kernel is launched with 512 blocks, each having 512 threads in parallel. This will solve the problem for large values of N. Instead of printing the addition of a very long vector, only one print statement, which indicates whether the calculated answer is right or wrong, is printed. The output of the code will be seen as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-201 image-border" src="assets/e869db0f-e17a-4b05-84b4-9d28f024085d.png" style="" width="282" height="84"/></div>
<p>This section explained the hierarchical execution concept in a CUDA. The next section will take this concept further by explaining a hierarchical memory architecture. </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Memory architecture</h1>
                </header>
            
            <article>
                
<p>The execution of code on a GPU is divided among streaming multiprocessors, blocks, and threads. The GPU has several different memory spaces, with each having particular features and uses and different speeds and scopes. This memory space is hierarchically divided into different chunks, like global memory, shared memory, local memory, constant memory, and texture memory, and each of them can be accessed from different points in the program. This memory architecture is shown in preceding diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-202 image-border" src="assets/ec8e0d7d-05a0-4eca-9c5a-648df0f49a97.png" style="" width="622" height="384"/></div>
<p>As shown in the diagram, each thread has its own local memory and a register file. Unlike processors, GPU cores have lots of registers to store local data. When the data of a thread does not fit in the register file, the local memory is used. Both of them are unique to each thread. The register file is the fastest memory. Threads in the same blocks have shared memory that can be accessed by all threads in that block. It is used for communication between threads. There is a global memory that can be accessed by all blocks and all threads. Global memory <span><span>has a large memory access latency</span></span>. There is a concept of caching to speed up this operation. L1 and L2 caches are available, as shown in the following table. There is a read-only constant memory that is used to store constants and kernel parameters. Finally, there is a texture memory that can take advantage of different two-dimensional or three-dimensional access patterns.</p>
<p>The features of all memories are summarized in the following table:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Memory</strong></td>
<td><strong>Access Pattern</strong></td>
<td><strong>Speed</strong></td>
<td><strong>Cached?</strong></td>
<td><strong>Scope</strong></td>
<td><strong>Lifetime</strong></td>
</tr>
<tr>
<td>Global</td>
<td>Read and Write</td>
<td>Slow</td>
<td>Yes</td>
<td>Host and All Threads</td>
<td><span>Entire Program</span></td>
</tr>
<tr>
<td>Local</td>
<td>Read and Write</td>
<td>Slow</td>
<td>Yes</td>
<td>Each Thread</td>
<td>Thread</td>
</tr>
<tr>
<td>Registers</td>
<td>Read and Write</td>
<td>Fast</td>
<td>-</td>
<td>Each Thread</td>
<td>Thread</td>
</tr>
<tr>
<td>Shared</td>
<td>Read and Write</td>
<td>Fast</td>
<td>No</td>
<td>Each Block</td>
<td class="CDPAlignLeft CDPAlign">Block</td>
</tr>
<tr>
<td>Constant</td>
<td>Read only</td>
<td>Slow</td>
<td>Yes</td>
<td class="CDPAlignLeft CDPAlign">Host and All Threads</td>
<td>Entire Program</td>
</tr>
<tr>
<td>Texture</td>
<td>Read only</td>
<td>Slow</td>
<td>Yes</td>
<td>Host and All Threads </td>
<td>Entire Program</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The preceding table describes important features of all memories. The scope defines the part of the program that can use this memory, and lifetime defines the time for which data in that memory will be visible to the program. Apart from this, L1 and L2 caches are also available for GPU programs for faster memory access.</p>
<p>To summarize, all threads have a register file, which is the fastest. Multiple threads in the same blocks have shared memory that is faster than global memory. All blocks can access global memory, which will be the slowest. Constant and texture memory are used for a special purpose, which will be discussed in the next section. Memory access is the biggest bottleneck in the fast execution of the program.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Global memory</h1>
                </header>
            
            <article>
                
<p>All blocks have read and write access to global memory. This memory is slow but can be accessed from anywhere in your device code. The concept of caching is used to speed up access to a global memory. All memories allocated using <kbd>cudaMalloc</kbd> will be a global memory. The following simple example demonstrates how you can use a global memory from your program:</p>
<pre>#include &lt;stdio.h&gt;<br/>#define N 5<br/><br/>__global__ void gpu_global_memory(int *d_a)<br/>{<br/>  d_a[threadIdx.x] = threadIdx.x;<br/>}<br/><br/>int main(int argc, char **argv)<br/>{<br/>  int h_a[N]; <br/>  int *d_a; <br/>  <br/>  cudaMalloc((void **)&amp;d_a, sizeof(int) *N);<br/>  cudaMemcpy((void *)d_a, (void *)h_a, sizeof(int) *N, cudaMemcpyHostToDevice);<br/>  <br/>  gpu_global_memory &lt;&lt; &lt;1, N &gt;&gt; &gt;(d_a); <br/>  cudaMemcpy((void *)h_a, (void *)d_a, sizeof(int) *N, cudaMemcpyDeviceToHost);<br/>  <br/>  printf("Array in Global Memory is: \n");<br/>  for (int i = 0; i &lt; N; i++) <br/>  {<br/>    printf("At Index: %d --&gt; %d \n", i, h_a[i]);<br/>  }<br/>  return 0;<br/>}</pre>
<p>This code demonstrates how you can write in global memory from your device code. The memory is allocated using <kbd>cudaMalloc</kbd> from the host code and a pointer to this array is passed as a parameter to the kernel function. The kernel function populates this memory chunk with values of the thread ID. This is copied back to host memory for printing. The result is shown as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-203 image-border" src="assets/a67ad366-a6c8-4cc6-bd8d-3b8e7327152a.png" style="" width="284" height="149"/></div>
<p>As we are using global memory, this operation will be slow. There are advanced concepts to speed up this operation which will be explained later on. In the next section, we will explain local memory and registers that are unique to all threads.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Local memory and registers</h1>
                </header>
            
            <article>
                
<p>Local memory and register files are unique to each thread. Register files are the fastest memory available for each thread. When variables of the kernel do not fit in register files, they use local memory. This is called <strong>register spilling</strong>. Basically, local memory is a part of global memory that is unique for each thread. Access to local memory will be slow compared to register files. <span>Though local memory is cached in L1 and L2 caches, register spilling might not affect your program adversely</span>. </p>
<p>A simple program to understand how to use local memory is shown as follows:</p>
<pre>#include &lt;stdio.h&gt;<br/>#define N 5<br/><br/>__global__ void gpu_local_memory(int d_in)<br/>{<br/>  int t_local; <br/>  t_local = d_in * threadIdx.x; <br/>  printf("Value of Local variable in current thread is: %d \n", t_local);<br/>}<br/>int main(int argc, char **argv)<br/>{<br/>  printf("Use of Local Memory on GPU:\n");<br/>  gpu_local_memory &lt;&lt; &lt;1, N &gt;&gt; &gt;(5); <br/>  cudaDeviceSynchronize();<br/>  return 0;<br/>}</pre>
<p>The  <kbd>t_local</kbd> variable will be local to each thread and stored in a register file. When this variable is used for computation in the kernel function, the computation will be the fastest. The output of the preceding code is shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-204 image-border" src="assets/01037099-35aa-46bd-bf0b-3ffc7eb92540.png" style="" width="397" height="145"/></div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cache memory</h1>
                </header>
            
            <article>
                
<p>On the latest GPUs, there is an L1 cache per multiprocessor and an L2 cache, which is shared between all multiprocessors. Both global and local memories use these caches. As L1 is near to thread execution, it is very fast. As shown in the diagram for memory architecture earlier, the L1 cache and shared memory use the same 64 KB. Both can be configured for how many bytes they will use out of the 64 KB. All global memory access goes through an L2 cache. Texture memory and constant memory have their separate caches.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Thread synchronization</h1>
                </header>
            
            <article>
                
<p>Up until now, whatever examples we have seen in this book had all threads independent of each other. But rarely, in real life, do you find examples where threads operate on data and terminate without passing results to any other threads. So there has to be some mechanism for threads to communicate with each other, and that is why the concept of shared memory is explained in this section. When many threads work in parallel and operate on the same data or read and write from the same memory location, there has to be synchronization between all threads. Thus, thread synchronization is also explained in this section. The last part of this section explains atomic operations, which are very useful in read-modified write conditions.   </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Shared memory</h1>
                </header>
            
            <article>
                
<p>Shared memory is available on-chip, and hence it is much faster than global memory. Shared memory latency is roughly 100 times lower than uncached global memory latency. <span>All the threads from the same block can access shared memory. This is very useful in many applications where threads need to share their results with other threads. However, it can also create chaos or false results if it is not synchronized. If one thread reads data from memory before the other thread has written to it, it can lead to false results. So, the memory access should be controlled or managed properly.  This is done by the <kbd>__syncthreads()</kbd> directive, which ensures that all the <kbd>write</kbd> operations to memory are completed before moving ahead in the programs. This is also called a <strong>barrier</strong>. The meaning of barrier is that all threads will reach this line and wait for other threads to finish. After all threads have reached this barrier, they can move further. To demonstrate the use of shared memory and thread synchronization, an example of a moving average is taken. The kernel function for that is shown as follows:</span></p>
<pre>#include &lt;stdio.h&gt;<br/>__global__ void gpu_shared_memory(float *d_a)<br/>{<br/>  int i, index = threadIdx.x;<br/>  float average, sum = 0.0f;<br/>  //Defining shared memory<br/>  __shared__ float sh_arr[10];<br/>      <br/>  sh_arr[index] = d_a[index];<br/> // This directive ensure all the writes to shared memory have completed<br/> <br/>  __syncthreads();  <br/>  for (i = 0; i&lt;= index; i++) <br/>  { <br/>    sum += sh_arr[i]; <br/>  }<br/>  average = sum / (index + 1.0f);<br/>  d_a[index] = average;<br/>      <br/>    //This statement is redundant and will have no effect on overall code execution  <br/>  sh_arr[index] = average;<br/>}</pre>
<p>The moving average operation is nothing but finding an average of all elements in an array up to the current element. Many threads will need the same data of an array for their calculation. This is an ideal case of using shared memory, and it will provide faster data than global memory. This will reduce the number of global memory accesses per thread, which in turn will reduce the latency of the program. The shared memory location is defined using the <kbd>__shared__ </kbd> directive. In this example, the shared memory of ten float elements is defined. Normally, the size of shared memory should be equal to the number of threads per block. Here, we are working on an array of 10, and hence we have taken the shared memory of this size.</p>
<p>The next step is to copy data from global memory to this shared memory. All the threads copy the element indexed by its thread ID to the shared array. Now, this is a shared memory write operation and, in the next line, we will read from this shared array. So, before proceeding, we should ensure that all shared memory write operations are completed. Therefore, let's introduce the  <kbd>__synchronizethreads()</kbd> barrier.</p>
<p>Next, the <kbd>for</kbd> loop calculates the average of all elements up to the current elements using the values in shared memory and stores the answer in global memory which is indexed by the current thread ID. The last line copies the calculated value in shared memory also. This line will have no effect on the overall execution of the code because shared memory has a lifetime up until the end of the current block execution, and this is the last line after which block execution is complete. It is just used to demonstrate this concept about shared memory. Now, we will try to write the main function for this code as follows:</p>
<div>
<pre>int main(int argc, char **argv)<br/>{<br/>   float h_a[10]; <br/>   float *d_a; <br/>  <br/>      //Initialize host Array<br/>   for (int i = 0; i &lt; 10; i++) <br/>   {<br/>     h_a[i] = i;<br/>   }<br/>  <br/>    // allocate global memory on the device<br/>    cudaMalloc((void **)&amp;d_a, sizeof(float) * 10);<br/>      <br/>    // copy data from host memory  to device memory <br/>    cudaMemcpy((void *)d_a, (void *)h_a, sizeof(float) * 10,         cudaMemcpyHostToDevice);<br/>    gpu_shared_memory &lt;&lt; &lt;1, 10 &gt;&gt; &gt;(d_a);<br/>  <br/>    // copy the modified array back to the host<br/>    cudaMemcpy((void *)h_a, (void *)d_a, sizeof(float) * 10, cudaMemcpyDeviceToHost);<br/>    printf("Use of Shared Memory on GPU: \n");<br/>  <br/>    for (int i = 0; i &lt; 10; i++) <br/>    {<br/>      printf("The running average after %d element is %f \n", i, h_a[i]);<br/>    }<br/>    return 0;<br/>}</pre></div>
<p>In the <kbd>main</kbd> function, after allocating memory for host and device arrays, host array is populated with values from zero to nine. This is copied to device memory where the moving average is calculated and the result is stored. The result from device memory is copied back to host memory and then printed on the console. The output on the console is shown as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-206 image-border" src="assets/d25f4dfd-77ed-4a21-8a9b-3ec1b3a7ca9c.png" style="" width="394" height="231"/></div>
<p>This section demonstrated the use of shared memory when multiple threads use data from the same memory location. The next section demonstrates the use of the <kbd>atomic</kbd> operations, which are very important in read-modified write operations. </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Atomic operations</h1>
                </header>
            
            <article>
                
<p>Consider a situation in which a large number of threads try to modify a small portion of memory. This is a frequently occurring phenomenon. It creates more problems when we try to perform a read-modify-write operation. The example of this operation is <kbd>d_out[i] ++,</kbd> where the first <kbd>d_out[i]</kbd> is read from memory, then incremented and then written back to the memory. However, when multiple threads are doing this operation on the same memory location, it can give a wrong output.</p>
<p>Suppose one memory location has an initial value of six, and threads p and q are trying to increment this memory location, then the final answer should be eight. But at the time of execution, it may happen that both the p and q threads read this value simultaneously, then both will get the value six. They increment it to seven and both will store this seven in the memory. So instead of eight, our final answer is seven, which is wrong.  How this can be dangerous is understood by taking an example of ATM cash withdrawal. Suppose you have a balance of Rs 5,000 in your account. You have two ATM cards of the same account. You and your friend go to two different ATMs simultaneously to withdraw Rs 4,000. Both of you swipe your card simultaneously; so, when the ATM checks for the balance, both will show a balance of Rs 5,000. When both of you withdraw Rs 4,000, then both machines will look at the initial balance, which was Rs 5,000. The amount to withdraw is less than the balance, and hence both machines will give Rs 4,000. Even though your balance was Rs 5,000, you got Rs 8,000, which is dangerous. To demonstrate this phenomenon, one example of large threads trying to access a small array is taken. The kernel function for this example is shown as follows:   </p>
<pre>include &lt;stdio.h&gt;<br/><br/>#define NUM_THREADS 10000<br/>#define SIZE 10<br/><br/>#define BLOCK_WIDTH 100<br/><br/>__global__ void gpu_increment_without_atomic(int *d_a)<br/>{<br/>  int tid = blockIdx.x * blockDim.x + threadIdx.x;<br/><br/>  // Each thread increment elements which wraps at SIZE<br/>  tid = tid % SIZE;<br/>  d_a[tid] += 1;<br/>}<br/><br/><br/></pre>
<p class="mce-root CDPAlignLeft CDPAlign">The kernel function is just incrementing memory location in the  <kbd>d_a[tid] +=1</kbd> line. The issue is how many times this memory location is incremented. The total number of threads is 10,000 and the array is only of size 10. We are indexing an array by taking the <kbd>modulo</kbd> operation between the thread ID and the size of the array. So, 1,000 threads will try to increment the same memory location. Ideally, every location in the array should be incremented 1,000 times. But as we will see in the output, this is not the case. Before seeing the output, we will try to write the <kbd>main</kbd> function:</p>
<div>
<pre>int main(int argc, char **argv)<br/>{<br/>  printf("%d total threads in %d blocks writing into %d array elements\n",<br/>  NUM_THREADS, NUM_THREADS / BLOCK_WIDTH, SIZE);<br/>  <br/>  // declare and allocate host memory<br/>  int h_a[SIZE];<br/>  const int ARRAY_BYTES = SIZE * sizeof(int);<br/>  // declare and allocate GPU memory<br/>  int * d_a;<br/>  cudaMalloc((void **)&amp;d_a, ARRAY_BYTES);<br/>  <br/>  // Initialize GPU memory with zero value.<br/>  cudaMemset((void *)d_a, 0, ARRAY_BYTES);<br/>  gpu_increment_without_atomic &lt;&lt; &lt;NUM_THREADS / BLOCK_WIDTH, BLOCK_WIDTH &gt;&gt; &gt;(d_a);<br/>  <br/>  // copy back the array of sums from GPU and print<br/>  cudaMemcpy(h_a, d_a, ARRAY_BYTES, cudaMemcpyDeviceToHost);<br/>  <br/>  printf("Number of times a particular Array index has been incremented without atomic add is: \n");<br/>  for (int i = 0; i &lt; SIZE; i++)<br/>  {<br/>    printf("index: %d --&gt; %d times\n ", i, h_a[i]);<br/>  }<br/>  cudaFree(d_a);<br/>  return 0;<br/>}</pre></div>
<p>In the <kbd>main</kbd> function, the device array is declared and initialized to zero. Here, a special  <kbd>cudaMemSet</kbd> function is used to initialize memory on the device. This is passed as a parameter to the kernel, which increments these 10 memory locations. Here, a total of 10,000 threads are launched as 1,000 blocks and 100 threads per block. The answer stored on the device after the kernel's execution is copied back to the host, and the value of each memory location is displayed on the console.</p>
<p>The output is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-207 image-border" src="assets/220c69ea-0945-4d74-9c6c-58b48f265382.png" style="" width="685" height="246"/></div>
<p>As discussed previously, ideally, each memory location should have been incremented 1,000 times, but most of the memory locations have values of 16 and 17. This is because many threads read the same locations simultaneously and hence increment the same value and store it in memory. As the timing of the thread's execution is beyond the control of the programmer, how many times simultaneous memory access will happen is not known. If you run your program a second time, then will your output be same as the first time? Your output might look like the following:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-208 image-border" src="assets/4492405e-802e-4611-8571-0b5e79f7c4e4.png" style="" width="686" height="245"/></div>
<p>As you might have guessed, every time you run your program, the memory locations may have different values. This happens because of the random execution of all threads on the device.</p>
<p>To solve this problem, CUDA provides an API called <kbd>atomicAdd</kbd> operations. It is a <kbd>blocking</kbd> operation, which means that when multiple threads are trying to access the same memory location, only one thread can access the memory location at a time. Other threads have to wait for this thread to finish and <kbd>write</kbd> its answer on memory.  The kernel function to use the <kbd>atomicAdd</kbd> operation is shown as follows:</p>
<pre>#include &lt;stdio.h&gt;<br/>#define NUM_THREADS 10000<br/>#define SIZE 10<br/>#define BLOCK_WIDTH 100<br/><br/>__global__ void gpu_increment_atomic(int *d_a)<br/>{<br/>  // Calculate thread index <br/>  int tid = blockIdx.x * blockDim.x + threadIdx.x;<br/><br/>  // Each thread increments elements which wraps at SIZE<br/>  tid = tid % SIZE;<br/>  atomicAdd(&amp;d_a[tid], 1);<br/>}<br/><br/></pre>
<p>The <kbd>kernel</kbd> function is quite similar to what we saw earlier. Instead of incrementing memory location using the <kbd>+=</kbd> operator, the <kbd>atomicAdd</kbd> function is used. It takes two arguments. The first is the memory location we want to increment, and the second is the value by which this location has to be incremented. In this code,  1,000 threads will again try to access the same location; so when one thread is using this location, the other 999 threads have to wait. This will increase the cost in terms of execution time. The <kbd>main</kbd> function of increment using <kbd>atomic</kbd> operations is shown as follows:</p>
<div>
<pre>int main(int argc, char **argv)<br/>{<br/>  printf("%d total threads in %d blocks writing into %d array elements\n",NUM_THREADS, NUM_THREADS / BLOCK_WIDTH, SIZE);<br/><br/>  // declare and allocate host memory<br/>  int h_a[SIZE];<br/>  const int ARRAY_BYTES = SIZE * sizeof(int);<br/><br/>  // declare and allocate GPU memory<br/>  int * d_a;<br/>  cudaMalloc((void **)&amp;d_a, ARRAY_BYTES);<br/>  <br/>   // Initialize GPU memory withzero value<br/>  cudaMemset((void *)d_a, 0, ARRAY_BYTES);<br/>  <br/>  gpu_increment_atomic &lt;&lt; &lt;NUM_THREADS / BLOCK_WIDTH, BLOCK_WIDTH &gt;&gt; &gt;(d_a);<br/>    // copy back the array from GPU and print<br/>  cudaMemcpy(h_a, d_a, ARRAY_BYTES, cudaMemcpyDeviceToHost);<br/>    <br/>  printf("Number of times a particular Array index has been incremented is: \n");<br/>  for (int i = 0; i &lt; SIZE; i++) <br/>  { <br/>     printf("index: %d --&gt; %d times\n ", i, h_a[i]); <br/>  }<br/>  <br/>  cudaFree(d_a);<br/>  return 0;<br/>}</pre></div>
<p>In the <kbd>main</kbd> function, the array with 10 elements is initialized with a zero value and passed to the kernel. But now, the kernel will do the <kbd>atomic add</kbd> operation. So, the output of this program should be accurate. Each element in the array should be incremented 1,000 times. The following will be the output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-209 image-border" src="assets/29bea1ee-443f-46cb-9742-0841c6ff3b47.png" style="" width="533" height="242"/></div>
<p>If you measure the execution time of the program with atomic operations, it may take a longer time than that taken by simple programs using global memory. This is because many threads are waiting for memory access in the atomic operation. Use of shared memory can help to speed up operations. Also, if the same number of threads are accessing more memory locations, then the atomic operation will incur less time overhead as a smaller number of threads having to wait for memory access.   </p>
<p>In this section, we have seen that atomic operations help in avoiding race conditions in memory operations and make the code simpler to write and understand. In the next section, we will explain two special types of memories, constant and texture, which help in accelerating certain types of code.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Constant memory</h1>
                </header>
            
            <article>
                
<p>The CUDA language makes another type of memory available to the programmer, which is known as <strong>constant</strong> memory. NVIDIA hardware provides 64 KB of this constant memory, which is used to store data that remains constant throughout the execution of the kernel. This constant memory is cached on-chip so that the use of constant memory instead of global memory can speed up execution. The use of constant memory will also reduce memory bandwidth to the device's global memory. In this section, we will see how to use constant memory in CUDA programs. A simple program that performs a simple math operation, <kbd>a*x + b</kbd>, where <kbd>a</kbd> and <kbd>b</kbd> are constants, is taken as an example. The <kbd>kernel</kbd> function code for this program is shown as follows:</p>
<pre>#include "stdio.h"<br/>#include&lt;iostream&gt;<br/>#include &lt;cuda.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/><br/>//Defining two constants<br/>__constant__ int constant_f;<br/>__constant__ int constant_g;<br/>#define N 5<br/><br/>//Kernel function for using constant memory <br/>__global__ void gpu_constant_memory(float *d_in, float *d_out) <br/>{<br/>  //Getting thread index for current kernel<br/>  int tid = threadIdx.x; <br/>  d_out[tid] = constant_f*d_in[tid] + constant_g;<br/>}<br/><br/></pre>
<p>Constant memory variables are defined using the <kbd>__constant__</kbd>  keyword. In the preceding code, two float variables, <kbd>constant_f</kbd> and <kbd>constant_g</kbd>, are defined as constants that will not change throughout the kernel's execution. The second thing to note is that once variables are defined as constants, they should not be defined again in the kernel function. The kernel function computes a simple mathematical operation using these two constants. There is a special way in which constant variables are copied to memory from the <kbd>main</kbd> function. This is shown in the following code: </p>
<div>
<pre>int main(void) <br/>{<br/>  //Defining Arrays for host<br/>  float h_in[N], h_out[N];<br/>  //Defining Pointers for device<br/>  float *d_in, *d_out;<br/>  int h_f = 2;<br/>  int h_g = 20;<br/>  <br/>  // allocate the memory on the cpu<br/>  cudaMalloc((void**)&amp;d_in, N * sizeof(float));<br/>  cudaMalloc((void**)&amp;d_out, N * sizeof(float));<br/>  <br/>  //Initializing Array<br/>  for (int i = 0; i &lt; N; i++) <br/>  {<br/>    h_in[i] = i;<br/>  }<br/>  <br/>  //Copy Array from host to device<br/>  cudaMemcpy(d_in, h_in, N * sizeof(float), cudaMemcpyHostToDevice);<br/>  //Copy constants to constant memory<br/>  cudaMemcpyToSymbol(constant_f, &amp;h_f, sizeof(int),0,cudaMemcpyHostToDevice);<br/>  cudaMemcpyToSymbol(constant_g, &amp;h_g, sizeof(int));<br/><br/>  //Calling kernel with one block and N threads per block<br/>  gpu_constant_memory &lt;&lt; &lt;1, N &gt;&gt; &gt;(d_in, d_out);<br/>  <br/>  //Coping result back to host from device memory<br/>  cudaMemcpy(h_out, d_out, N * sizeof(float), cudaMemcpyDeviceToHost);<br/>  <br/>  //Printing result on console<br/>  printf("Use of Constant memory on GPU \n");<br/>  for (int i = 0; i &lt; N; i++) <br/>  {<br/>    printf("The expression for index %f is %f\n", h_in[i], h_out[i]);<br/>  }<br/>  <br/>  cudaFree(d_in);<br/>  cudaFree(d_out);<br/>  return 0;<br/>}</pre></div>
<p>In the <kbd>main</kbd> function, the <kbd>h_f</kbd> and <kbd>h_g</kbd> constants are defined and initialized on the host, which will be copied to constant memory.  The <kbd>cudaMemcpyToSymbol</kbd> instruction is used to copy these constants onto constant memory for kernel execution. It has five arguments. First is the destination, which is defined using the <kbd>__constant__</kbd> keyword. Second is the host address, third is the size of the transfer, fourth is memory offset, which is taken as zero, and fifth is the direction of data transfer, which is taken as the host to the device. The last two arguments are optional, and hence they are omitted in the second call to the <kbd>cudaMemcpyToSymbol</kbd> instruction.</p>
<p>The output of the code is shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-210 image-border" src="assets/e604f1e3-6ad4-4b42-a22e-55ad71959484.png" style="" width="380" height="143"/></div>
<p>One thing to note is that constant memory is a <kbd>Read-only</kbd> memory. This example is used just to explain the use of the constant memory from the CUDA program. It is not the optimal use of constant memory. As discussed earlier, constant memory helps in conserving memory bandwidth to global memory. To understand this, you have to understand the concept of warp. One warp is a collection of 32 threads woven together and executed in lockstep. A single read from constant memory can be broadcast to half warp, which can reduce up to 15 memory transactions. Also, constant memory is cached so that memory access to a nearby location will not incur an additional memory transaction. When each half warp, which contains 16 threads, operates on the same memory locations, the use of constant memory saves a lot of execution time. It should also be noted that if half-warp threads use completely different memory locations, then the use of constant memory may increase the execution time. So, the constant memory should be used with proper care.  </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Texture memory</h1>
                </header>
            
            <article>
                
<p><strong>Texture</strong> memory is another read-only memory that can accelerate the program and reduce memory bandwidth when data is read in a certain pattern. Like constant memory, it is also cached on a chip. This memory was originally designed for rendering graphics, but it can also be used for general purpose computing applications. It is very effective when applications have memory access that exhibits a great deal of spatial locality. The meaning of spatial locality is that each thread is likely to read from the nearby location what other nearby threads read. This is great in image processing applications where we work on 4-point connectivity and 8-point connectivity. A two-dimensional spatial locality for accessing memory location by threads may look something like this:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr style="height: 32px">
<td style="height: 32px">Thread 0</td>
<td style="height: 32px">Thread 2</td>
</tr>
<tr style="height: 17.5938px">
<td style="height: 17.5938px">Thread 1</td>
<td style="height: 17.5938px">
<p class="mce-root">Thread 3</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root">General global memory cache will not be able to capture this spatial locality and will result in lots of memory traffic to global memory. Texture memory is designed for this kind of access pattern so that it will only read from memory once, and then it will be cached so that execution will be much faster. Texture memory supports one and two-dimensional <kbd>fetch</kbd> operations. Using texture memory in your CUDA program is not trivial, especially for those who are not programming experts. In this section, a simple example of how to copy array values using texture memory is explained. The <kbd>kernel</kbd> function for using texture memory is explained as follows:</p>
<pre>#include "stdio.h"<br/>#include&lt;iostream&gt;<br/>#include &lt;cuda.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/><br/>#define NUM_THREADS 10<br/>#define N 10<br/><br/>//Define texture reference for 1-d access<br/>texture &lt;float, 1, cudaReadModeElementType&gt; textureRef;<br/><br/>__global__ void gpu_texture_memory(int n, float *d_out)<br/>{<br/>    int idx = blockIdx.x*blockDim.x + threadIdx.x;<br/>    if (idx &lt; n) {<br/>      float temp = tex1D(textureRef, float(idx));<br/>      d_out[idx] = temp;<br/>    }<br/>}</pre>
<p>The part of texture memory that should be fetched is defined by texture reference. In code, it is defined using the texture API. It has three arguments. The first argument indicates the data type of texture elements. In this example, it is a <kbd>float</kbd>. The second argument indicates the type of texture reference, which can be one-dimensional, two-dimensional, and so on. Here, it is a one-dimensional reference. The third argument specifies the read mode and it is an optional argument. Please make sure that this texture reference is declared as a static global variable, and it should not be passed as parameters to any function. In the kernel function, data stored at the thread ID is read from this texture reference and copied to the <kbd>d_out</kbd> global memory pointer. Here, we are not using any spatial locality as this example is only taken to show you how to use texture memory from CUDA programs. The spatial locality will be explained in the next chapter when we see some image processing applications with CUDA. The <kbd>main</kbd> function for this example is shown as follows: </p>
<pre>int main()<br/>{<br/>  //Calculate number of blocks to launch<br/>  int num_blocks = N / NUM_THREADS + ((N % NUM_THREADS) ? 1 : 0);<br/>  float *d_out;<br/>  // allocate space on the device for the results<br/>  cudaMalloc((void**)&amp;d_out, sizeof(float) * N);<br/>  // allocate space on the host for the results<br/>  float *h_out = (float*)malloc(sizeof(float)*N);<br/>  float h_in[N];<br/>  for (int i = 0; i &lt; N; i++) <br/>  {<br/>    h_in[i] = float(i);<br/>  }<br/>  //Define CUDA Array<br/>  cudaArray *cu_Array;<br/>  cudaMallocArray(&amp;cu_Array, &amp;textureRef.channelDesc, N, 1);<br/>  <br/>  cudaMemcpyToArray(cu_Array, 0, 0, h_in, sizeof(float)*N, cudaMemcpyHostToDevice);<br/>  <br/>  // bind a texture to the CUDA array<br/>  cudaBindTextureToArray(textureRef, cu_Array);<br/>  <br/>  <br/>  gpu_texture_memory &lt;&lt; &lt;num_blocks, NUM_THREADS &gt;&gt; &gt;(N, d_out);<br/>  <br/>  // copy result to host<br/>  cudaMemcpy(h_out, d_out, sizeof(float)*N, cudaMemcpyDeviceToHost);<br/>  printf("Use of Texture memory on GPU: \n");<br/>  // Print the result<br/>  for (int i = 0; i &lt; N; i++) <br/>  {<br/>    printf("Average between two nearest element is : %f\n", h_out[i]);<br/>  }<br/>  free(h_out);<br/>  cudaFree(d_out);<br/>  cudaFreeArray(cu_Array);<br/>  cudaUnbindTexture(textureRef);<br/>}</pre>
<p>In the <kbd>main</kbd> function, after declaring and allocating memory for host and device arrays, the host array is initialized with values from zero to nine. In this example, you will see the first use of CUDA arrays. They are similar to normal arrays, but they are dedicated to textures. They are read-only to kernel functions and can be written to device memory from the host by using the <kbd>cudaMemcpyToArray</kbd> function, as shown in the preceding code. The second and third arguments in that function are width and height offset that are taken as 0, 0, meaning that we are starting from the top left corner. They are opaque memory layouts optimized for texture memory fetches. </p>
<p>The <kbd>cudaBindTextureToArray</kbd> functions bind texture reference to this CUDA array. This means, it copies this array to a texture reference starting from the top left corner. After binding the texture reference, the kernel is called, which uses this texture reference and computes the array to be stored on device memory. After the kernel finishes, the output array is copied back to the host for displaying on the console. When using texture memory, we have to unbind the texture from our code. This is done by using the <kbd>cudaUnbindTexture</kbd> function. The <kbd>cudaFreeArray</kbd> function is used to free up memory used by the CUDA array. The output of the program displayed on the console is shown as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-211 image-border" src="assets/047e2343-482f-480a-aa49-834805450ca6.png" style="" width="286" height="227"/></div>
<p>This section finishes our discussion on memory architecture in CUDA. When the memories available in CUDA are used judiciously according to your application, it improves the performance of the program drastically. You need to look carefully at the memory access pattern of all threads in your application and then select which memory you should use for your application. The last section of this chapter briefly <span>describes</span> the complex CUDA program, which uses all the concepts we have used up until this point. </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dot product and matrix multiplication example</h1>
                </header>
            
            <article>
                
<p>Up to this point, we have learned almost all the important concepts related to basic parallel programming using CUDA. In this section, we will show you how to write CUDA programs for important mathematical operations like dot product and matrix multiplication, which are used in almost all applications. This will make use of all the concepts we saw earlier and help you in writing code for your applications.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dot product</h1>
                </header>
            
            <article>
                
<p>The dot product between two vectors is an important mathematical operation. It will also explain one important concept in CUDA programming that is called <strong>reduction</strong> operation. The dot product between two vectors can be defined as follows:</p>
<pre>(x1,x1,x3) . (y1,y2,y3) = x1y1 + x2y2 +x3y3</pre>
<p>Now, if you see this operation, it is very similar to an element-wise addition operation on vectors. Instead of addition, you have to perform element-wise multiplication. All threads also have to keep running the sum of multiplication they have performed because all individual multiplications need to be summed up to get a final answer of the dot product. The answer of the dot product will be a single number. This operation where the final answer is the reduced version of the original two arrays is called a <strong>reduce</strong> operation in CUDA. It is useful in many applications. To perform this operation in CUDA, we will start by writing a kernel function for it, as follows:</p>
<pre>#include &lt;stdio.h&gt;<br/>#include&lt;iostream&gt;<br/>#include &lt;cuda.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/>#define N 1024<br/>#define threadsPerBlock 512<br/><br/><br/>__global__ void gpu_dot(float *d_a, float *d_b, float *d_c) <br/>{<br/>  //Define Shared Memory<br/>  __shared__ float partial_sum[threadsPerBlock];<br/>  int tid = threadIdx.x + blockIdx.x * blockDim.x;<br/>  int index = threadIdx.x;<br/><br/>  float sum = 0;<br/>  while (tid &lt; N) <br/>  {<br/>    sum += d_a[tid] * d_b[tid];<br/>    tid += blockDim.x * gridDim.x;<br/>  }<br/><br/>  // set the partial sum in shared memory<br/>  partial_sum[index] = sum;<br/><br/>  // synchronize threads in this block<br/>  __syncthreads();<br/><br/>  //Calculate Patial sum for a current block using data in shared memory<br/>  int i = blockDim.x / 2;<br/>  while (i != 0) {<br/>    if (index &lt; i)<br/>      {partial_sum[index] += partial_sum[index + i];}<br/>    __syncthreads();<br/>    i /= 2;<br/>  }<br/>  //Store result of partial sum for a block in global memory<br/>  if (index == 0)<br/>    d_c[blockIdx.x] = partial_sum[0];<br/>}<br/><br/><br/></pre>
<p>The <kbd>kernel</kbd> function takes two input arrays as input and stores the final partial sum in the third array. Shared memory is defined to store intermediate answers of the partial answer. The size of the shared memory is equal to the number of threads per block, as all separate blocks will have the separate copy of this shared memory. After that, two indexes are calculated; the first one, which calculates the unique thread ID, is similar to what we have done in the vector addition example. The second index is used to store the partial product answer on shared memory. Again, every block has a separate copy of shared memory, so only the thread ID used to index the shared memory is of a given block.</p>
<p>The <kbd>while</kbd> loop will perform element-wise multiplication of elements indexed by the thread ID. It will also do multiplication of elements that is offset by the total threads to the current thread ID. The partial sum of this element is stored in the shared memory. We are going to use these results from the shared memory to calculate the partial sum for a single block. So, before reading this shared memory block, we must ensure that all threads have finished writing to this shared memory. This is ensured by using the <kbd>__syncthreads()</kbd> directive.</p>
<p>Now, one method to get an answer for the dot product is that one thread iterates over all these partial sums to get a final answer. One thread can perform the reduce operation. This will take N operations to complete, where N is the number of partial sums to be added (equal to the number of threads per block) to get a final answer.</p>
<p>The question is, can we do this reduce operation in parallel? The answer is yes. The idea is that every thread will add two elements of the partial sum and store the answer in the location of the first element. Since each thread combines two entries in one, the operation can be completed in half entries. Now, we will repeat this operation for the remaining half until we get the final answer that calculates the partial sum for this entire block. The complexity of this operation is <kbd>log<sub>2</sub>(N)</kbd> , which is far better than the complexity of N when one thread performs the reduce operation.</p>
<p>The operation explained is calculated by the block starting with <kbd>while (i != 0)</kbd> . The block sums the partial answer of the current thread and the thread offset by <kbd>blockdim/2</kbd>. It continues this addition until we get a final single answer, which is a sum of all partial products in a given block. The final answer is stored in the global memory. Each block will have a separate answer to be stored in the global memory so that it is indexed by the block ID, which is unique for each block. Still, we have not got the final answer. This can be performed in the <kbd>device</kbd> function or the <kbd>main</kbd> function.  </p>
<p>Normally, the last few additions in the reduce operation need very little resources. Much of the GPU resource remains idle, and that is not the optimal use of the GPU. So, the final addition operation of all partial sums for an individual block is done in the <kbd>main</kbd> function. The <kbd>main</kbd> function is as follows:</p>
<div>
<pre>int main(void) <br/>{<br/>  float *h_a, *h_b, h_c, *partial_sum;<br/>  float *d_a, *d_b, *d_partial_sum;<br/><br/>  //Calculate number of blocks and number of threads<br/>  int block_calc = (N + threadsPerBlock - 1) / threadsPerBlock;<br/>  int blocksPerGrid = (32 &lt; block_calc ? 32 : block_calc);<br/>  // allocate memory on the cpu side<br/>  h_a = (float*)malloc(N * sizeof(float));<br/>  h_b = (float*)malloc(N * sizeof(float));<br/>  partial_sum = (float*)malloc(blocksPerGrid * sizeof(float));<br/><br/>  // allocate the memory on the gpu<br/>  cudaMalloc((void**)&amp;d_a, N * sizeof(float));<br/>  cudaMalloc((void**)&amp;d_b, N * sizeof(float));<br/>  cudaMalloc((void**)&amp;d_partial_sum, blocksPerGrid * sizeof(float));<br/><br/>  // fill in the host mempory with data<br/>  for (int i = 0; i&lt;N; i++) {<br/>    h_a[i] = i;<br/>    h_b[i] = 2;<br/>  }<br/><br/><br/>  // copy the arrays to the device<br/>  cudaMemcpy(d_a, h_a, N * sizeof(float), cudaMemcpyHostToDevice);<br/>  cudaMemcpy(d_b, h_b, N * sizeof(float), cudaMemcpyHostToDevice);<br/><br/>  gpu_dot &lt;&lt; &lt;blocksPerGrid, threadsPerBlock &gt;&gt; &gt;(d_a, d_b, d_partial_sum);<br/><br/>  // copy the array back to the host<br/>  cudaMemcpy(partial_sum, d_partial_sum, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost);<br/><br/>  // Calculate final dot prodcut<br/>  h_c = 0;<br/>  for (int i = 0; i&lt;blocksPerGrid; i++) <br/> {<br/>    h_c += partial_sum[i];<br/>  }<br/>  <br/>}</pre></div>
<p>Three arrays are defined and memory is allocated for both host and device to store inputs and output. The two host arrays are initialized inside a <kbd>for</kbd> loop. One array is initialized with <kbd>0</kbd> to <kbd>N</kbd> and the second is initialized with a constant value <kbd>2</kbd>. The calculation of the number of blocks in a grid and number of threads in a block is also done. It is similar to what we did at the start of this chapter. Bear in mind, you can also keep these value as constants, like we did in the first program of this chapter, to avoid complexity.</p>
<p>These arrays are copied to device memory and passed as parameters to the <kbd>kernel</kbd> function. The <kbd>kernel</kbd> function will return an array, which has answers of the partial products of individual blocks indexed by their block ID. This array is copied back to the host in the <kbd>partial_sum</kbd> array. The final answer of the dot product is calculated by iterating over this <kbd>partial_sum</kbd> array, using the <kbd>for</kbd> loop starting from zero to the number of blocks per grid. The final dot product is stored in  <kbd>h_c</kbd>. To check whether the calculated dot product is correct or not, the following code can be added to the <kbd>main</kbd> function:</p>
<pre>printf("The computed dot product is: %f\n", h_c);<br/>#define cpu_sum(x) (x*(x+1))<br/>  if (h_c == cpu_sum((float)(N - 1)))<br/>  {<br/>    printf("The dot product computed by GPU is correct\n");<br/>  }<br/>  else<br/>  {<br/>    printf("Error in dot product computation");<br/>  }<br/>  // free memory on the gpu side<br/>  cudaFree(d_a);<br/>  cudaFree(d_b);<br/>  cudaFree(d_partial_sum);<br/>  // free memory on the cpu side<br/>  free(h_a);<br/>  free(h_b);<br/>  free(partial_sum);</pre>
<p>The answer is verified with the answer calculated mathematically. In two input arrays, if one array has values from <kbd>0</kbd> to <kbd>N-1</kbd> and the second array has a constant value of 2, then the dot product will be <kbd>N*(N+1)</kbd>. We print the answer of the dot product calculated mathematically, along with whether it has been calculated correctly or not. The host and device memory is freed up in the end. The output of the program is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-212 image-border" src="assets/ec2ca87b-9356-443f-83ee-d4d81cdcc920.png" style="" width="355" height="83"/></div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Matrix multiplication</h1>
                </header>
            
            <article>
                
<p>The second most important mathematical operation performed on a GPU using CUDA is matrix multiplication. It is a very complicated mathematical operation when the sizes of the matrices are very large. It should be kept in mind that for matrix multiplication, the number of columns in the first matrix should be equal to the number of rows in the second matrix. Matrix multiplication is not a cumulative operation. To avoid complexity, in this example, we are taking a square matrix of the same size. If you are familiar with the mathematics of matrix multiplication, then you may recall that a row in the first matrix will be multiplied with all the columns in the second matrix. This is repeated for all rows in the first matrix. It is shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-213 image-border" src="assets/64671ce5-f20d-4b13-a9d2-b0ac3767f5c6.png" style="" width="505" height="78"/></div>
<p>Same data is reused many times, so this is an ideal case of using shared memory. In this section, we will make two separate kernel functions, with and without using shared memory. You can compare the execution of two kernels to get an idea of how shared memory improves the performance of the program. We will first start by writing a <kbd>kernel</kbd> function without using shared memory:</p>
<pre><br/>#include &lt;stdio.h&gt;<br/>#include&lt;iostream&gt;<br/>#include &lt;cuda.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/>#include &lt;math.h&gt;<br/><br/>//This defines size of a small square box or thread dimensions in one block<br/>#define TILE_SIZE 2<br/><br/>//Matrix multiplication using non shared kernel<br/>__global__ void gpu_Matrix_Mul_nonshared(float *d_a, float *d_b, float *d_c, const int size)<br/>{<br/>  int row, col;<br/>  col = TILE_SIZE * blockIdx.x + threadIdx.x;<br/>  row = TILE_SIZE * blockIdx.y + threadIdx.y;<br/><br/>  for (int k = 0; k&lt; size; k++)<br/>  {<br/>    d_c[row*size + col] += d_a[row * size + k] * d_b[k * size + col];<br/>  }<br/>}<br/><br/></pre>
<p>Matrix multiplication is performed using two-dimensional threads. If we launch two-dimensional threads with each thread performing a single element of the output matrix, then up to 16 x 16 matrices can be multiplied. If the size is greater than this, then it will need more than 512 threads for computation, which is not possible on most GPUs. So, we need to launch multiple blocks with each containing less than 512 threads. To accomplish this, the output matrix is divided into small square blocks having dimensions of <kbd>TILE_SIZE</kbd> in both directions. Each thread in a block will calculate elements of this square block. The total number of blocks for matrix multiplication will be calculated by dividing the size of the matrix by the size of this small square defined by <kbd>TILE_SIZE</kbd>.</p>
<p>If you understand this, then calculating the row and column index for the output will be very easy. It is similar to what we have done up till now, with <kbd>blockdim.x</kbd>  being equal to <kbd>TILE_SIZE</kbd>. Now, every element in the output will be the dot product of one row in the first matrix and one column in the second matrix. Both the matrices have the same size so the dot product has to be performed for a number of elements equal to the size variable. So the <kbd>for</kbd> loop in the <kbd>kernel</kbd> function is running from <kbd>0</kbd> to <kbd>size</kbd>.</p>
<p>To calculate the individual index of both matrices, consider that this matrix is stored as a linear array in system memory in row-major fashion. Its meaning is that all elements in the first row are placed in a consecutive memory location and then rows are placed one after the other, as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-214 image-border" src="assets/5f13d0c1-02da-4efa-8f76-3da8da6f4735.png" style="" width="278" height="43"/></div>
<p class="mce-root CDPAlignLeft CDPAlign"><span>The index of a linear array can be calculated by its row ID multiplied by the size of </span>the matrix <span>plus its column</span> ID. <span>So, the index for</span> <em>M<sub>1,0</sub></em> <span>will be 2 as its row ID is 1, the matrix size is 2 and the column ID is zero.  This method is used to calculate the element index in both the matrices.</span></p>
<p class="mce-root CDPAlignLeft CDPAlign"><span>To calculate the element at <kbd>[row, col]</kbd> in the resultant matrix, the index in the first matrix will be equal to <kbd>row*size + k</kbd> , and for the second matrix, it will be <kbd>k*size + col</kbd>. This is a very simple <kbd>kernel</kbd> function. There is a large amount of data reuse in matrix multiplication. This function is not utilizing the advantage of shared memory. So, we will try to modify the kernel function that makes use of shared memory. The modified <kbd>kernel</kbd> function is shown as follows:</span></p>
<pre>// shared<br/>__global__ void gpu_Matrix_Mul_shared(float *d_a, float *d_b, float *d_c, const int size)<br/>{<br/>  int row, col;<br/> <br/>  __shared__ float shared_a[TILE_SIZE][TILE_SIZE];<br/><br/>  __shared__ float shared_b[TILE_SIZE][TILE_SIZE];<br/><br/>  // calculate thread id<br/>  col = TILE_SIZE * blockIdx.x + threadIdx.x;<br/>  row = TILE_SIZE * blockIdx.y + threadIdx.y;<br/><br/>  for (int i = 0; i&lt; size / TILE_SIZE; i++) <br/>  {<br/>    shared_a[threadIdx.y][threadIdx.x] = d_a[row* size + (i*TILE_SIZE + threadIdx.x)];<br/>    shared_b[threadIdx.y][threadIdx.x] = d_b[(i*TILE_SIZE + threadIdx.y) * size + col];<br/>    }<br/>    __syncthreads(); <br/><br/>             <br/>    for (int j = 0; j&lt;TILE_SIZE; j++)<br/>      d_c[row*size + col] += shared_a[threadIdx.x][j] * shared_b[j][threadIdx.y];<br/>    __syncthreads(); // for synchronizing the threads<br/><br/>  }<br/>}</pre>
<p class="mce-root">A two-shared memory with size equal to the size of a small square block, which is <kbd>TILE_SIZE</kbd>, is defined for storing data for reuse. Row and column indexes are calculated in the same way as seen earlier. First, this shared memory is filled up in the first <kbd>for</kbd> loop. After that, <kbd>__syncthreads()</kbd> is included so that memory read from shared memory only happens when all threads have finished writing to it. The last <kbd>for</kbd> loop again calculates the dot product. As this is done by only using shared memory, this considerably reduces memory traffic to a global memory, which in turn improves the performance of the program for larger matrix dimensions. The <kbd>main</kbd> function of this program is shown as follows:</p>
<pre>int main()<br/>{<br/>   //Define size of the matrix<br/>  const int size = 4;<br/>   //Define host and device arrays<br/>  float h_a[size][size], h_b[size][size],h_result[size][size];<br/>  float *d_a, *d_b, *d_result; // device array<br/>  //input in host array<br/>  for (int i = 0; i&lt;size; i++)<br/>  {<br/>    for (int j = 0; j&lt;size; j++)<br/>    {<br/>      h_a[i][j] = i;<br/>      h_b[i][j] = j;<br/>    }<br/>  }<br/> <br/><br/>  cudaMalloc((void **)&amp;d_a, size*size*sizeof(int));<br/>  cudaMalloc((void **)&amp;d_b, size*size * sizeof(int));<br/>  cudaMalloc((void **)&amp;d_result, size*size* sizeof(int));<br/>  //copy host array to device array<br/>  cudaMemcpy(d_a, h_a, size*size* sizeof(int), cudaMemcpyHostToDevice);<br/>  cudaMemcpy(d_b, h_b, size*size* sizeof(int), cudaMemcpyHostToDevice);<br/>  //calling kernel<br/>  dim3 dimGrid(size / TILE_SIZE, size / TILE_SIZE, 1);<br/>  dim3 dimBlock(TILE_SIZE, TILE_SIZE, 1);<br/><br/>  gpu_Matrix_Mul_nonshared &lt;&lt; &lt;dimGrid, dimBlock &gt;&gt; &gt; (d_a, d_b, d_result, size);<br/>  //gpu_Matrix_Mul_shared &lt;&lt; &lt;dimGrid, dimBlock &gt;&gt; &gt; (d_a, d_b, d_result, size);<br/><br/>  cudaMemcpy(h_result, d_result, size*size * sizeof(int), cudaMemcpyDeviceToHost);<br/>  <br/>  return 0;<br/>}</pre>
<p>After defining and allocating memory for host and device arrays, the host array is filled with some random values. These arrays are copied to device memory so that it can be passed to the <kbd>kernel</kbd> functions. The number of grid blocks and the number of block threads is defined using the <kbd>dim3</kbd> structure, with the dimensions equal to that calculated earlier. You can call any of the kernels. The calculated answer is copied back to the host memory. To display the output on the console, the following code is added to the <kbd>main</kbd> function:</p>
<pre>printf("The result of Matrix multiplication is: \n");<br/>  <br/>  for (int i = 0; i&lt; size; i++)<br/>  {<br/>    for (int j = 0; j &lt; size; j++)<br/>    {<br/>      printf("%f ", h_result[i][j]);<br/>    }<br/>    printf("\n");<br/>  }<br/>cudaFree(d_a)<br/>cudaFree(d_b)<br/>cudaFree(d_result)</pre>
<p>The memory used to store matrices on device memory is also freed up. The output on the console is as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-215 image-border" src="assets/b86bd951-bfd0-4b27-96b2-a20663044300.png" style="" width="580" height="164"/></div>
<p>This section demonstrated CUDA programs for two important mathematical operations used in a wide range of applications. It also explained the use of shared memory and multidimensional threads.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter explained the launch of multiple blocks, with each having multiple threads from the kernel function. It showed the method for choosing the two parameters for a large value of threads. It also explained the hierarchical memory architecture that can be used by CUDA programs. The memory nearest to the thread being executed is fast, and as we move away from it, memories get slower. When multiple threads want to communicate with each other, then CUDA provides the flexibility of using shared memory, by which threads from the same blocks can communicate with each other. When multiple threads use the same memory location, then there should be synchronization between the memory access; otherwise, the final result will not be as expected. We also saw the use of an atomic operation to accomplish this synchronization. <span>If some </span>parameters<span> remain constant throughout the kernel's execution, then it can be stored in constant memory for speed up. </span>When CUDA programs exhibit a certain communication pattern like spatial locality, then texture memory should be used to improve the performance of the program. To summarize, to improve the performance of CUDA programs, we should reduce memory traffic to slow memories. If this is done efficiently, drastic improvement in the performance of the program can be achieved.</p>
<p>In the next chapter, the concept of CUDA streams will be discussed, which is similar to multitasking in CPU programs. How we can measure the performance of CUDA programs will also be discussed. It will also show the use of CUDA in simple image processing applications.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Suppose you want to launch 100,000 threads in parallel. What is the best choice of the number of blocks in a grid and number of threads in a block and why?</li>
<li>Write a CUDA program to find out the cube of every element in an array when the number of elements in the array is 100,000.</li>
<li> State whether the following statement is true or false and give a reason: An assignment operator between local variables will be faster than an assignment operator between global variables.</li>
<li>What is register spilling? How it can harm the performance of your CUDA program?</li>
<li>State whether the following line of code will give the required output or not: <kbd>d_out[i]</kbd> = <kbd>d_out[i-1]</kbd>.</li>
</ol>
<ol start="6">
<li><span>State whether the following statement is true or false and give a reason: </span>Atomic operations increase the execution time for CUDA programs.</li>
<li>Which kinds of communication patterns are ideal for using texture memory in your CUDA programs?</li>
<li>What will be the effect of using the <kbd>__syncthreads</kbd> directive inside an if statement?</li>
</ol>


            </article>

            
        </section>
    </div></div></body></html>