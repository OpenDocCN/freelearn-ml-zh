- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applying Machine Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned about understanding data and visualization.
    It is now time to move on to the modeling phase and study machine learning algorithms!
    In the earlier chapters, you learned that building machine learning models requires
    a lot of knowledge about AWS services, data engineering, data exploration, data
    architecture, and much more. This time, you will delve deeper into the algorithms
    that have been introduced and more.
  prefs: []
  type: TYPE_NORMAL
- en: Having a good sense of the different types of algorithms and machine learning
    approaches will put you in a very good position to make decisions during your
    projects. Of course, this type of knowledge is also crucial to the AWS Certified
    Machine Learning Specialty exam.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that there are thousands of algorithms out there. You can even
    propose your own algorithm for a particular problem. In this chapter, you will
    learn about the most relevant ones and, hopefully, the ones that you will probably
    face in the exam.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics of this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Storing the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A word about ensemble models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Supervised learning:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object2Vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unsupervised learning:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP Insights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Textual analysis (natural language processing)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alright, grab a coffee and rock it!
  prefs: []
  type: TYPE_NORMAL
- en: Introducing this chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During this chapter, you will read about several algorithms, modeling concepts,
    and learning strategies. All these topics are beneficial for you to know for the
    exam and throughout your career as a data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has been structured in such a way that it not only covers the necessary
    topics of the exam but also gives you a good sense of the most important learning
    strategies out there. For example, the exam will check your knowledge regarding
    the basic concepts of K-Means. However, this chapter will cover it on a much deeper
    level, since this is an important topic for your career as a data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter will follow this approach of looking deeper into the algorithms’
    logic for some types of models that every data scientist should master. Furthermore,
    keep this in mind: sometimes you may go deeper than what is expected of you in
    the exam, but that will be extremely important for you in your career.'
  prefs: []
  type: TYPE_NORMAL
- en: Many times during this chapter, you will see the term **built-in algorithms**.
    This term will be used to refer to the list of algorithms implemented by AWS on
    their SageMaker SDK.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a concrete example: you can use scikit-learn’s **K-nearest neighbors**
    algorithm, or KNN for short (if you don’t remember what scikit-learn is, refresh
    your memory by going back to [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*,
    Machine Learning Fundamentals*) to create a classification model and deploy it
    to SageMaker. However, AWS also offers its own implementation of the KNN algorithm
    on its SDK, which is optimized to run in the AWS environment. Here, KNN is an
    example of a built-in algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The possibilities on AWS are endless because you can either take advantage
    of built-in algorithms or bring in your own algorithm to create models on SageMaker.
    Finally, just to make this very clear, here is an example of how to import a built-in
    algorithm from the AWS SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You will learn how to create models on SageMaker in [*Chapter 9*](B21197_09.xhtml#_idTextAnchor1224)*,
    Amazon SageMaker Modeling*. For now, just understand that AWS has its own set
    of libraries where those built-in algorithms are implemented.
  prefs: []
  type: TYPE_NORMAL
- en: To train and evaluate a model, you need training and testing data. After instantiating
    your estimator, you should then feed it with those datasets. Not to spoil [*Chapter
    9*](B21197_09.xhtml#_idTextAnchor1224)*, Amazon SageMaker Modeling*, but you should
    know about the concept of **data channels** in advance.
  prefs: []
  type: TYPE_NORMAL
- en: Data channels are configurations related to input data that you can pass to
    SageMaker when you are creating a training job. You should set these configurations
    just to inform SageMaker of how your input data is formatted.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 9*](B21197_09.xhtml#_idTextAnchor1224)*, Amazon SageMaker Modeling*,
    you will learn how to create training jobs and how to set data channels. As of
    now, you should know that while configuring data channels, you can set a `ContentType`)
    and an `TrainingInputMode`). You will now take a closer look at how and where
    the training data should be stored so that it can be integrated properly with
    AWS’s built-in algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Storing the training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, you can use multiple AWS services to prepare data for machine
    learning, such as **Elastic MapReduce (EMR),** Redshift, Glue, and so on. After
    preprocessing the training data, you should store it in S3, in a format expected
    by the algorithm you are using. *Table 6.1* shows the list of acceptable data
    formats per algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data format** | **Algorithm** |'
  prefs: []
  type: TYPE_TB
- en: '| `Application/x-image` | Object detection algorithm, semantic segmentation
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Application/x-recordio` | Object detection algorithm |'
  prefs: []
  type: TYPE_TB
- en: '| `Application/x-recordio-protobuf` | Factorization machines, K-Means, KNN,
    latent Dirichlet allocation, linear learner, NTM, PCA, RCF, sequence-to-sequence
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Application/jsonlines` | BlazingText, DeepAR |'
  prefs: []
  type: TYPE_TB
- en: '| `Image/.jpeg` | Object detection algorithm, semantic segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| `Image/.png` | Object detection algorithm, semantic segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| `Text/.csv` | IP Insights, K-Means, KNN, latent Dirichlet allocation, linear
    learner, NTM, PCA, RCF, XGBoost |'
  prefs: []
  type: TYPE_TB
- en: '| `Text/.libsvm` | XGBoost |'
  prefs: []
  type: TYPE_TB
- en: Table 6.1 – Data formats that are acceptable per AWS algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, many algorithms accept `Text/.csv` format. You should follow
    these rules if you want to use that format:'
  prefs: []
  type: TYPE_NORMAL
- en: Your CSV file *cannot* have a header record.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For supervised learning, the target variable must be in the first column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While configuring the training pipeline, set the input data channel as `content_type`
    equal to `text/csv`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For unsupervised learning, set `label_size` within `content_type`, as follows:
    `''content_type=text/csv;label_size=0''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although `text/.csv` format is fine for many use cases, most of the time, AWS’s
    built-in algorithms work better with `recordIO-protobuf`. This is an optimized
    data format that is used to train AWS’s built-in algorithms, where SageMaker converts
    each observation in the dataset into a binary representation that is a set of
    4-byte floats.
  prefs: []
  type: TYPE_NORMAL
- en: 'RecordIO-protobuf accepts two types of input modes: pipe mode and file mode.
    In pipe mode, the data will be streamed directly from S3, which helps optimize
    storage. In file mode, the data is copied from S3 to the training instance’s store
    volume.'
  prefs: []
  type: TYPE_NORMAL
- en: You are almost ready! Now you can take a quick look at some modeling definitions
    that will help you understand some more advanced algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: A word about ensemble models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before you start diving into the algorithms, there is an important modeling
    concept that you should be aware of – **ensemble**. The term ensemble is used
    to describe methods that use multiple algorithms to create a model.
  prefs: []
  type: TYPE_NORMAL
- en: A regular algorithm that *does not* implement ensemble methods will rely on
    a single model to train and predict the target variable. That is what happens
    when you create a decision tree or regression model. On the other hand, algorithms
    that *do* implement ensemble methods will rely on multiple models to predict the
    target variable. In that case, since each of these models might come up with a
    different prediction for the target variable, ensemble algorithms implement either
    a voting (for classification models) or averaging (for regression models) system
    to output the final results. *Table 6.2* illustrates a very simple voting system
    for an ensemble algorithm composed of three models.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Transaction** | **Model A** | **Model B** | **Model C** | **Prediction**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Fraud | Fraud | Not Fraud | Fraud |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Not Fraud | Not Fraud | Not Fraud | Not Fraud |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Fraud | Fraud | Fraud | Fraud |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Not Fraud | Not Fraud | Fraud | Not Fraud |'
  prefs: []
  type: TYPE_TB
- en: Table 6.2 – An example of a voting system on ensemble methods
  prefs: []
  type: TYPE_NORMAL
- en: As described before, the same approach works for regression problems, where
    instead of voting, it could average the results of each model and use that as
    the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Voting and averaging are just two examples of ensemble approaches. Other powerful
    techniques include blending and stacking, where you can create multiple models
    and use the outcome of each model as a feature for a main model. Looking back
    at *Table 6.2*, columns *Model A*, *Model B*, and *Model C* could be used as features
    to predict the final outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that many machine learning algorithms use ensemble methods while
    training, in an embedded way. These algorithms can be classified into two main
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrapping aggregation** or **bagging**: With this approach, several models
    are trained on top of different samples of data. Predictions are then made through
    the voting or averaging system. The most popular algorithm from this category
    is known as **Random Forest**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting**: With this approach, several models are trained on top of different
    samples of the data. One model then tries to correct the error of the next model
    by penalizing incorrect predictions. The most popular algorithms from this category
    are **stochastic gradient boosting** and **AdaBoost**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you know what ensemble models are, you can look at some machine learning
    algorithms that are likely to be present in your exam. Not all of them use ensemble
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next few sections are split based on AWS algorithm categories, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Textual analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, you will have an overview of reinforcement learning on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AWS provides supervised learning algorithms for general purposes (regression
    and classification tasks) and more specific purposes (forecasting and vectorization).
    The list of built-in algorithms that can be found in these sub-categories is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear learner algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factorization machines algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object2Vec algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepAR forecasting algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will start by learning about regression models and the linear learner algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Working with regression models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Looking at **linear regression** models is a nice way to understand what is
    going on inside **regression models** in general (linear and non-linear regression
    models). This is mandatory knowledge for every data scientist and can help you
    solve real challenges as well. You will now take a closer look at this in the
    following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing regression algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear regression models aim to predict a numeric value (*y*) according to one
    or more variables (*x*). Mathematically, such a relationship can be defined as
    *y = f(x)*, where *y* is known as the **dependent variable** and *x* is known
    as the **independent variable**.
  prefs: []
  type: TYPE_NORMAL
- en: With regression models, the component that you want to predict (*y*) is always
    a continuous number – for example, the price of houses or the number of transactions.
    You saw this in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*, Machine* *Learning
    Fundamentals*, in *Figure 1**.2*, when you were learning about the right type
    of supervised learning algorithm, given the target variable. Please feel free
    to go back and review it.
  prefs: []
  type: TYPE_NORMAL
- en: When you use *just one variable to predict y*, this problem is referred to as
    **simple linear regression**. On the other hand, when you use *more than one variable
    to predict y*, you have a **multiple linear** **regression** problem.
  prefs: []
  type: TYPE_NORMAL
- en: There is also another class of regression models, known as **non-linear regression**.
    However, let us put that aside for a moment and understand what simple linear
    regression means.
  prefs: []
  type: TYPE_NORMAL
- en: Regression models belong to the supervised side of machine learning (the other
    side is non-supervised) because algorithms try to predict values according to
    existing correlations between independent and dependent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what does *f* mean in *y=f(x)*? Here, *f* is the regression function responsible
    for predicting *y* based on *x*. In other words, this is the function that you
    want to find. When talking about simple linear regression, pay attention to the
    next three questions and answers:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the shape of *f* in linear regression?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear, of course!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How can you represent a linear relationship?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a *straight* line (you will understand why in a few minutes).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So what is the function that defines a line?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ax + b* (just check any *mathematics* book).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: That is it! Linear regression models are given by *y = ax + b*. When you are
    trying to predict *y* given *x*, you just need to find out the values of *a* and
    *b*. You can adopt the same logic to figure out what is going on inside other
    kinds of regression.
  prefs: []
  type: TYPE_NORMAL
- en: Finding out the values of *a* and *b* is the only thing you are going to do.
    It is nice to know that *a* is also known as the **alpha coefficient**, or **slope**,
    and represents the line’s inclination, while *b* is also known as the **beta coefficient**,
    or **y intercept**, and represents the place where the line crosses the *y* axis
    (into a two-dimensional plane consisting of *x* and *y*). You will learn about
    these two terms in a later subsection.
  prefs: []
  type: TYPE_NORMAL
- en: It is also nice to know that there is a bias (*e*) associated with every predictor
    that you do not have control over. That being said, the formal definition of simple
    linear regression is given by *y = ax + b +* *e*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, you will learn how to find alpha and beta to solve a
    simple linear regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: Least squares method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are different ways to find the slope and *y* intercept of a line, but
    the most used method is known as the **least squares method**. The principle behind
    this method is simple: you have to find the *best line that reduces the sum of*
    *squared error*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 6**.1*, you can see a Cartesian plane with multiple points and lines
    in it. *Line a* represents the best fit for this data – in other words, that would
    be the best linear regression function for those points. But how can you know
    that? It is simple: if you compute the error associated with each point, you will
    realize that *Line a* contains the least sum of squared errors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Visualizing the principle of the least squares method](img/B21197_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Visualizing the principle of the least squares method
  prefs: []
  type: TYPE_NORMAL
- en: It is worth understanding linear regression from scratch not only for the certification
    exam but also for your career as a data scientist. To provide you with a complete
    example, a spreadsheet containing all the calculations that you are going to see
    has been developed! You are encouraged to jump on this support material and perform
    some simulations. In any case, you will see these calculations in action in the
    next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a linear regression model from scratch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You are going to use a very simple dataset, with only two variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x*: Represents the person’s number of years of work experience'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*: Represents the person’s average salary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to understand the relationship between *x* and *y* and, if possible,
    predict the salary (*y*) based on years of experience (*x*). Real problems very
    often have far more independent variables and are not necessarily linear. However,
    this example will give you the baseline knowledge to master more complex algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: To find out what the alpha and beta coefficients are (or slope and *y* intercept
    if you prefer), you need to find some statistics related to the dataset. In *Table
    6.3*, you have the data and these auxiliary statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '| **X (****INDEPENDENT)** | **Y (****DEPENDENT)** | **X MEAN** | **Y MEAN**
    | **COVARIANCE  (****X,Y)** | **X VARIANCE** | **Y VARIANCE** |'
  prefs: []
  type: TYPE_TB
- en: '| *1* | *1.000* |  |  | 21.015 | 20 | 21.808.900 |'
  prefs: []
  type: TYPE_TB
- en: '| *2* | *1.500* |  |  | 14.595 | 12 | 17.388.900 |'
  prefs: []
  type: TYPE_TB
- en: '| *3* | *3.700* |  |  | 4.925 | 6 | 3.880.900 |'
  prefs: []
  type: TYPE_TB
- en: '| *4* | *5.000* |  |  | 1.005 | 2 | 448.900 |'
  prefs: []
  type: TYPE_TB
- en: '| *5* | *4.000* |  |  | 835 | 0 | 2.788.900 |'
  prefs: []
  type: TYPE_TB
- en: '| *6* | *6.500* |  |  | 415 | 0 | 688.900 |'
  prefs: []
  type: TYPE_TB
- en: '| *7* | *7.000* |  |  | 1.995 | 2 | 1.768.900 |'
  prefs: []
  type: TYPE_TB
- en: '| *8* | *9.000* |  |  | 8.325 | 6 | 11.088.900 |'
  prefs: []
  type: TYPE_TB
- en: '| *9* | *9.000* |  |  | 11.655 | 12 | 11.088.900 |'
  prefs: []
  type: TYPE_TB
- en: '| *10* | *10.000* |  |  | 19.485 | 20 | 18.748.900 |'
  prefs: []
  type: TYPE_TB
- en: '| COUNT | 10 | **5,50** | **5.670,00** | **8.425,00** | **8,25** | **8.970.100,00**
    |'
  prefs: []
  type: TYPE_TB
- en: Table 6.3 – Dataset to predict average salary based on the amount of work experience
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, there is an almost perfect linear relationship between *x*
    and *y*. As the amount of work experience increases, so does the salary. In addition
    to *x* and *y*, you need to compute the following statistics: the number of records,
    the mean of *x*, the mean of *y*, the covariance of *x* and *y*, the variance
    of *x*, and the variance of *y*. *Figure 6**.2 * depicts formulas that provide
    a mathematical representation of variance and covariance (respectively), where
    *x bar*, *y bar*, and *n* represent the mean of *x*, the mean of *y*, and the
    number of records, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Mathematical representation of variance and covariance respectively](img/B21197_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Mathematical representation of variance and covariance respectively
  prefs: []
  type: TYPE_NORMAL
- en: If you want to check the calculation details of the formulas for each of those
    auxiliary statistics in *Table 6.2*, please refer to the support material provided
    along with this book. There, you will find these formulas already implemented
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: These statistics are important because they will be used to compute the alpha
    and beta coefficients. *Figure 6**.3* explains how you can compute both coefficients,
    along with the correlation coefficients R and R squared. These last two metrics
    will give you an idea about the quality of the model, where the closer they are
    to 1, the better the model is.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Equations to calculate coefficients for simple linear regression](img/B21197_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Equations to calculate coefficients for simple linear regression
  prefs: []
  type: TYPE_NORMAL
- en: 'After applying these formulas, you will come up with the results shown in *Table
    6.4*. It already contains all the information that you need to make predictions,
    on top of the new data. If you replace the coefficients in the original equation,
    *y = ax + b + e*, you will find the regression formula to be as follows: *y =
    1021.212 * x +* *53.3*.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Coefficient** | **Description** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| Alpha | Line inclination | 1,021,212,121 |'
  prefs: []
  type: TYPE_TB
- en: '| Beta | Interceptor | 53 |'
  prefs: []
  type: TYPE_TB
- en: '| R | Correlation | 0,979,364,354 |'
  prefs: []
  type: TYPE_TB
- en: '| R^2 | Determination | 0,959,154,538 |'
  prefs: []
  type: TYPE_TB
- en: Table 6.4 – Finding regression coefficients
  prefs: []
  type: TYPE_NORMAL
- en: From this point on, to make predictions, all you have to do is replace *x* with
    the number of years of experience. As a result, you will find *y*, which is the
    projected salary. You can see the model fit in *Figure 6**.4* and some model predictions
    in *Table 6.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Fitting data in the regression equation](img/B21197_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Fitting data in the regression equation
  prefs: []
  type: TYPE_NORMAL
- en: '| **INPUT** | **PREDICTION** | **ERROR** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.075 | 75 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2.096 | 596 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 3.117 | - 583 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 4.138 | - 862 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 5.159 | 1.159 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 6.181 | - 319 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 7.202 | 202 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 8.223 | - 777 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 9.244 | 244 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 10.265 | 265 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 11.287 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 12.308 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 13.329 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 14.350 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 15.372 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 16.393 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | 17.414 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | 18.435 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | 19.456 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 20.478 |  |'
  prefs: []
  type: TYPE_TB
- en: Table 6.5 – Model predictions
  prefs: []
  type: TYPE_NORMAL
- en: While you are analyzing regression models, you should be able to know whether
    your model is of good quality or not. You read about many modeling issues (such
    as overfitting) in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*, Machine Learning
    Fundamentals*, and you already know that you always have to check model performance.
  prefs: []
  type: TYPE_NORMAL
- en: A good approach to regression models is performing what is called residual analysis.
    This is where you plot the errors of the model in a scatter plot and check whether
    they are randomly distributed (as expected) or not. If the errors are *not* randomly
    distributed, this means that your model was unable to generalize the data. *Figure
    6**.5* shows a residual analysis based on the data from *Table 6.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Residual analysis](img/B21197_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Residual analysis
  prefs: []
  type: TYPE_NORMAL
- en: The takeaway here is that the errors are randomly distributed. Such evidence,
    along with a high R squared rating, can be used as arguments to support the use
    of this model.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B21197_07.xhtml#_idTextAnchor970)*, Evaluating and Optimizing
    Models*, you will learn about evaluation metrics. For instance, you will learn
    that each type of model may have its own set of evaluation metrics. Regression
    models are commonly evaluated with **Mean Squared Error (MSE)** and **Root Mean
    Squared Error (RMSE)**. In other words, apart from R, R squared, and residual
    analysis, ideally, you will execute your model on test sets to extract other performance
    metrics. You can even use a cross-validation system to check model performance,
    as you learned in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*, Machine* *Learning
    Fundamentals*.
  prefs: []
  type: TYPE_NORMAL
- en: Very often, when the model residuals *do* present a pattern and are *not* randomly
    distributed, it is because the existing relationship in the data is not linear,
    but non-linear, so another modeling technique must be applied. In the next subsection,
    you will learn how you can interpret regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting regression models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is also good to know how to interpret a linear regression model. Sometimes,
    you use linear regression not necessarily to create a predictive model but to
    do a regression analysis. You can then use regression analysis to understand the
    relationship between the independent and dependent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking back at the regression equation (*y = 1021.212 * x + 53.30*), you can
    see the two terms: alpha or slope (*1021.20*) and beta or *y* intercept (*53.3*).
    You can interpret this model as follows: *for each additional year of working
    experience, you will increase your salary by $1,021.20*. Also, note that when
    “years of experience” is equal to 0, the expected salary is going to be $53.30
    (this is the point where the straight line crosses the *y* axis).'
  prefs: []
  type: TYPE_NORMAL
- en: 'From a broad perspective, your regression analysis should answer the following
    question: for each extra unit that is added to the independent variable (slope),
    what is the average change in the dependent variable?'
  prefs: []
  type: TYPE_NORMAL
- en: Checking adjusted R squared
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, you have a much better idea of regression models! There is just
    one other very important topic that you should be aware of, regardless of whether
    it will come up in the exam or not, which is the parsimony aspect of your model.
  prefs: []
  type: TYPE_NORMAL
- en: You have already heard about parsimony in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*,
    Machine Learning Fundamentals*. This is the ability to prioritize simple models
    over complex ones. Looking into regression models, you might have to use more
    than one feature to predict your outcome. This is also known as a multiple regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: When that is the case, the R and R squared coefficients tend to reward more
    complex models with more features. In other words, if you keep adding new features
    to a multiple regression model, you will come up with higher R and R squared coefficients.
    That is why you *cannot* anchor your decisions *only* based on those two metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Another additional metric that you could use (apart from R, R squared, MSE,
    and RMSE) is known as **adjusted R squared**. This metric is penalized when you
    add extra features to the model that do not bring any real value. In *Table 6.6*,
    you can see when a model is starting to lose parsimony.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Number** **of features** | **R squared** | **Adjusted** **R squared** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 81 | 79 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 83 | 82 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 88 | 87 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 90 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 92 | 85 |'
  prefs: []
  type: TYPE_TB
- en: Table 6.6 – Comparing R squared and adjusted R squared
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can conclude that maintaining three variables in the model is better
    than maintaining four or five. Adding four or five variables to the model will
    increase the R squared (as expected), but decrease the adjusted R squared.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have a very good understanding of regression models.
    Now, let us check what AWS offers in terms of built-in algorithms for this class
    of models.
  prefs: []
  type: TYPE_NORMAL
- en: Regression modeling on AWS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS has a built-in algorithm known as **linear learner**, where you can implement
    linear regression models. The built-in linear learner uses **Stochastic Gradient
    Descent (SGD)** to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You will learn more about SGD when neural networks are discussed. For now, you
    can look at SGD as an alternative to the popular least squares error method that
    was just discussed.
  prefs: []
  type: TYPE_NORMAL
- en: The linear learner built-in algorithm provides a hyperparameter that can apply
    normalization to the data, prior to the training process. The name of this hyperparameter
    is `normalize_data`. This is very helpful since linear models are sensitive to
    the scale of the data and usually take advantage of data normalization.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Data normalization was discussed in [*Chapter 4*](B21197_04.xhtml#_idTextAnchor451)*,
    Data Preparation and Transformation*. Please review that chapter if you need to.
  prefs: []
  type: TYPE_NORMAL
- en: Some other important hyperparameters of the linear learner algorithm are **L1**
    and **wd**, which play the roles of **L1 regularization** and **L2** **regularization**,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: L1 and L2 regularization help the linear learner (or any other regression algorithm
    implementation) to avoid overfitting. Conventionally, regression models that implement
    L1 regularization are called **lasso regression** models, while regression models
    with L2 regularization are called **ridge** **regression** models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it might sound complex, it is not! The regression model equation is
    still the same: *y = ax + b + e*. The change is in the loss function, which is
    used to find the coefficients that best minimize the error. If you look back at
    *Figure 6**.1*, you will see that the error function is defined as *e = (ŷ -
    y)^2*, where *ŷ* is the regression function value and *y* is the real value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'L1 and L2 regularization add a penalty term to the loss function, as shown
    in the formulas in *Figure 6**.6* (note that you are replacing *ŷ* with *ax +*
    *b*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – L1 and L2 regularization](img/B21197_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – L1 and L2 regularization
  prefs: []
  type: TYPE_NORMAL
- en: The λ (lambda) parameter must be greater than 0 and manually tuned. A very high
    lambda value may result in an underfitting issue, while a very low lambda may
    not result in expressive changes in the end results (if your model is overfitted,
    it will stay overfitted).
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, the main difference between L1 and L2 regularization is
    that L1 will shrink the less important coefficients to 0, which will force the
    feature to be dropped (acting as a feature selector). In other words, if your
    model is overfitting because of the high number of features, L1 regularization
    should help you solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: During your exam, remember the basis of L1 and L2 regularization, especially
    the key difference between them, where L1 works well as a feature selector.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, many built-in algorithms can serve multiple modeling purposes. The
    linear learner algorithm can be used for regression, binary classification, and
    multi-classification. Make sure you remember this during your exam (it is *not
    just* about regression models).
  prefs: []
  type: TYPE_NORMAL
- en: AWS has other built-in algorithms that work for regression and classification
    problems –that is**, factorization machines, KNN,** and the **XGBoost** algorithm.
    Since these algorithms can also be used for classification purposes, these will
    be covered in the section about classification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ve just been given a very important tip to remember during the exam: linear
    learner, factorization machines, KNN, and XGBoost are suitable for both regression
    and classification problems. These algorithms are often known as algorithms for
    general purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, you have reached the end of this section about regression models.
    Remember to check out the supporting material before you take the exam. You can
    also use the reference material when you are working on your daily activities!
    Now, let us move on to another classical example of a machine learning problem:
    classification models.'
  prefs: []
  type: TYPE_NORMAL
- en: Working with classification models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have been learning what classification models are throughout this book.
    However, now, you are going to look at some algorithms that are suitable for classification
    problems. Keep in mind that there are hundreds of classification algorithms out
    there, but since you are preparing for the AWS Certified Machine Learning Specialty
    exam, the ones that have been pre-built by AWS will be covered.
  prefs: []
  type: TYPE_NORMAL
- en: You will start with **factorization machines**. Factorization machines is considered
    an extension of the linear learner algorithm, optimized to find the relationship
    between features within high-dimensional sparse datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A very traditional use case for factorization machines is *recommendation systems*,
    where you usually have a high level of sparsity in the data. During the exam,
    if you are faced with a general-purpose problem (either a regression or binary
    classification task) where the underlying datasets are sparse, then factorization
    machines is probably the best answer from an algorithm perspective.
  prefs: []
  type: TYPE_NORMAL
- en: When you use factorization machines in a regression model, the RMSE will be
    used to evaluate the model. On the other hand, in the binary classification mode,
    the algorithm will use log loss, accuracy, and F1 score to evaluate results. A
    deeper discussion about evaluation metrics will be provided in [*Chapter 7*](B21197_07.xhtml#_idTextAnchor970)*,
    Evaluating and* *Optimizing Models*.
  prefs: []
  type: TYPE_NORMAL
- en: You should be aware that factorization machines only accepts input data in the
    `text/.csv` format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next built-in algorithm suitable for classification problems is known as
    K-nearest neighbors, or KNN for short. As the name suggests, this algorithm will
    try to find the *K* closest points to the input data and return either of the
    following predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: The most repeated class of the *K* closest points, if it is a classification
    task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average value of the label of the *K* closest points, if it is a regression
    task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN is an **index-based algorithm** because it computes distances between points,
    assigns indexes for these points, and then stores the sorted distances and their
    indexes. With that type of data structure, KNN can easily select the top *K* closest
    points to make the final prediction. Note that *K* is a hyperparameter of KNN
    and should be optimized during the modeling process.
  prefs: []
  type: TYPE_NORMAL
- en: The other AWS built-in algorithm available for general purposes, including classification,
    is known as **eXtreme Gradient Boosting**, or **XGBoost** for short. This is an
    ensemble, decision tree-based model.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost uses a set of **weaker** models (decision trees) to predict the target
    variable, which can be a regression task, binary class, or multi-class. This is
    a very popular algorithm and has been used in machine learning competitions by
    the top performers.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost uses a boosting learning strategy, in which one model tries to correct
    the error of the prior model. It carries the name “gradient” because it uses the
    gradient descent algorithm to minimize the loss when adding new trees.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The term *weaker* is used in this context to describe very simple decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Although XGBoost is much more robust than a single decision tree, it is important
    to go into the exam with a clear understanding of what decision trees are and
    their main configurations. By the way, they are the base model of many ensemble
    algorithms, such as AdaBoost, Random Forest, gradient boost, and XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are rule-based algorithms that organize decisions in the form
    of a tree, as shown in *Figure 6**.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Example of what a decision tree model looks like](img/B21197_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Example of what a decision tree model looks like
  prefs: []
  type: TYPE_NORMAL
- en: They are formed by a root node (at the very top of the tree), intermediary or
    decision nodes (in the middle of the tree), and leaf nodes (bottom nodes with
    no splits). The depth of the tree is given by the difference between the root
    node and the very last leaf node. For example, in *Figure 6**.7*, the depth of
    the tree is 3.
  prefs: []
  type: TYPE_NORMAL
- en: The depth of the tree is one of the most important hyperparameters of this type
    of model and it is often known as the **max depth**. In other words, the max depth
    controls the maximum depth that a decision tree can reach.
  prefs: []
  type: TYPE_NORMAL
- en: Another very important hyperparameter of decision tree models is the minimum
    number of samples/observations in the leaf nodes. It is also used to control the
    growth of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees have many other types of hyperparameters, but these two are especially
    important for controlling how the model overfits. Decision trees with a high depth
    or a very small number of observations in the leaf nodes are likely to face issues
    during extrapolation/prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason for this is simple: decision trees use data from the leaf nodes
    to make predictions, based on the proportion (for classification tasks) or average
    value (for regression tasks) of each observation/target variable that belongs
    to that node. Thus, the node should have enough data to make good predictions
    outside the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter the term **CART** during the exam, you should know that it
    stands for **Classification and Regression Trees**, since decision trees can be
    used for classification and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: To select the best variables to split the data in the tree, the model will choose
    the ones that maximize the separation of the target variables across the nodes.
    This task can be performed by different methods, such as **Gini** and **information
    gain**.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Time series refers to data points that are collected on a regular basis with
    a sequence dependency. Time series have a measure, a fact, and a time unit, as
    shown in *Figure 6**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Time series statement](img/B21197_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Time series statement
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, time series can be classified as **univariate** or **multivariate**.
    A univariate time series contains just one variable connected across a period
    of time, while a multivariate time series contains two or more variables connected
    across a period. *Figure 6**.9* shows the univariate time series.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Time series example](img/B21197_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Time series example
  prefs: []
  type: TYPE_NORMAL
- en: 'Time series can be decomposed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Observed** or **level**: The average values of the series'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trend**: Increasing, decreasing pattern (sometimes, there is no trend)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seasonality**: Regular peaks at specific periods of time (sometimes, there
    is no seasonality)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise**: Something that cannot be explained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, you can also find isolated peaks in the series that cannot be captured
    in a forecasting model. In such cases, you might want to consider those peaks
    as outliers. *Figure 6**.10* is a decomposition of the time series shown in *Figure
    6**.9*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Time series decomposition](img/B21197_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Time series decomposition
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth highlighting that you can use **additive** or **multiplicative**
    approaches to decompose time series. Additive models suggest that your time series
    *adds* each component to explain the target variable – that is, *y(t) = level
    + trend + seasonality +* *noise*.
  prefs: []
  type: TYPE_NORMAL
- en: Multiplicative models, on the other hand, suggest that your time series *multiplies*
    each component to explain the target variable – that is, *y(t) = level * trend
    * seasonality ** *noise*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will take a closer look at time series components.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the stationarity of time series
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decomposing time series and understanding how their components interact with
    additive and multiplicative models is a great achievement! However, the more you
    learn, the more you want to go deeper into the problem. Maybe you have realized
    that time series without trend and seasonality are easier to predict than the
    ones with all those components!
  prefs: []
  type: TYPE_NORMAL
- en: That is naturally right. If you do not have to understand trend and seasonality,
    and if you do not have control over the noise, all you have to do is explore the
    observed values and find their regression relationship.
  prefs: []
  type: TYPE_NORMAL
- en: A time series with constant mean and variance across a time period is known
    as **stationary**. In general, time series *with* trend and seasonality are *not*
    stationary. It is possible to apply data transformations to the series to transform
    it into a stationary time series so that the modeling task tends to be easier.
    This type of transformation is known as **differentiation**.
  prefs: []
  type: TYPE_NORMAL
- en: While you are exploring a time series, you can check stationarity by applying
    hypothesis tests, such as **Dickey-Fuller**, **KPSS**, and **Phillips-Perron**,
    just to mention a few. If you find it non-stationary, then you can apply differentiation
    to make it a stationary time series. Some algorithms already have that capability
    embedded.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring, exploring, and exploring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, it is important to remember that exploration tasks happen all
    the time in data science. Nothing is different here. While you are building time
    series models, you might want to take a look at the data and check whether it
    is suitable for this type of modeling.
  prefs: []
  type: TYPE_NORMAL
- en: '**Autocorrelation plots** are one of the tools that you can use for time series
    analysis. Autocorrelation plots allow you to check the correlations between lags
    in the time series. *Figure 6**.11* shows an example of this type of visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Autocorrelation plot](img/B21197_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Autocorrelation plot
  prefs: []
  type: TYPE_NORMAL
- en: Remember, if you are playing with univariate time series, your time series just
    contains one variable. Therefore, finding autocorrelation across the lags of your
    unique variable is crucial to understanding whether you can build a good model
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: And yes, it turns out that, sometimes, it might happen that you do not have
    a time series in front of you. Furthermore, no matter your efforts, you will not
    be able to model this data as a time series. This type of data is often known
    as **white** **noise**.
  prefs: []
  type: TYPE_NORMAL
- en: Another type of series that you cannot predict is known as a **random** **walk**.
    Random walks are random by nature, but they have a dependency on the previous
    time step. For example, the next point of a random walk could be a random number
    between 0 and 1, and also the last point of the series.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Be careful if you come across those terms in the exam and remember to relate
    them to randomness in time series.
  prefs: []
  type: TYPE_NORMAL
- en: With that, you have covered the main theory behind time series modeling. You
    should also be aware that the most popular algorithms out there for working with
    time series are known as **Auto-Regressive Integrated Moving Average (ARIMA)**
    and **Exponential Smoothing (ETS)**. This book will not go into the details of
    these two models. Instead, you will see what AWS can offer in terms of time series
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding DeepAR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **DeepAR** forecasting algorithm is a built-in SageMaker algorithm that
    is used to forecast a one-dimensional time series using a **Recurrent Neural**
    **Network (RNN)**.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional time series algorithms, such as ARIMA and ETS, are designed to fit
    one model per time series. For example, if you want to forecast sales per region,
    you might have to create one model per region, since each region may have its
    own sales behaviors. DeepAR, on the other hand, allows you to operate more than
    one time series in a single model, which seems to be a huge advantage for more
    complex use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input data for DeepAR, as expected, is *one or more* time series. Each
    of these time series can be associated with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A vector of static (time-independent) categorical features, controlled by the
    `cat` field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A vector of dynamic (time-dependent) time series, controlled by `dynamic_feat`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Note that the ability to train and make predictions on top of multiple time
    series is strictly related to the vector of static categorical features. While
    defining the time series that DeepAR will train on, you can set categorical variables
    to specify which group each time series belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: Two of the main hyperparameters of DeepAR are `context_length`, which is used
    to control how far in the past the model can see during the training process,
    and `prediction_length`, which is used to control how far in the future the model
    will output predictions.
  prefs: []
  type: TYPE_NORMAL
- en: DeepAR can also handle missing values, which, in this case, refers to existing
    gaps in the time series. A very interesting functionality of DeepAR is its ability
    to create derived features from time series. These derived features, which are
    created from basic time frequencies, help the algorithm learn time-dependent patterns.
    *Table 6.7* shows all the derived features created by DeepAR, according to each
    type of time series that it is trained on.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Frequency of the** **time series** | **Derived feature** |'
  prefs: []
  type: TYPE_TB
- en: '| Minute | Minute of hour, hour of day, day of week, day of month, day of year
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hour | Hour of day, day of week, day of month, day of year |'
  prefs: []
  type: TYPE_TB
- en: '| Day | Day of week, day of month, day of year |'
  prefs: []
  type: TYPE_TB
- en: '| Week | Day of month, week of year |'
  prefs: []
  type: TYPE_TB
- en: '| Month | Month of year |'
  prefs: []
  type: TYPE_TB
- en: Table 6.7 – DeepAR derived features per frequency of time series
  prefs: []
  type: TYPE_NORMAL
- en: You have now completed this section about forecasting models. Next, you will
    take a look at the last algorithm regarding supervised learning – that is, the
    **Object2Vec** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Object2Vec
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object2Vec is a built-in SageMaker algorithm that generalizes the well-known
    **Word2Vec** algorithm. Object2Vec is used to create **embedding spaces** for
    high dimensional objects. These embedding spaces are, per definition, compressed
    representations of the original object and can be used for multiple purposes,
    such as feature engineering or object comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – A visual example of an embedding space](img/B21197_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – A visual example of an embedding space
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.12* illustrates what is meant by an embedding space. The first
    and last layers of the neural network model just map the input data with itself
    (represented by the same vector size).'
  prefs: []
  type: TYPE_NORMAL
- en: As you move on to the internal layers of the model, the data is compressed more
    and more until it hits the layer in the middle of this architecture, known as
    the embedding layer. On that particular layer, you have a smaller vector, which
    aims to be an accurate and compressed representation of the high-dimensional original
    vector from the first layer.
  prefs: []
  type: TYPE_NORMAL
- en: With this, you just completed the first section about machine learning algorithms
    in AWS. Coming up next, you will take a look at some unsupervised algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AWS provides several unsupervised learning algorithms for the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering: K-Means algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dimension reduction: **Principal Component** **Analysis (PCA)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pattern recognition: IP Insights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anomaly detection: The **Random Cut Forest (****RCF)** algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us start by talking about clustering and how the most popular clustering
    algorithm works: K-Means.'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering algorithms are very popular in data science. Basically, they aim
    to identify similar groups in a given dataset, also known as *clusters*. Clustering
    algorithms belong to the field of non-supervised learning, which means that they
    do not need a label or response variable to be trained.
  prefs: []
  type: TYPE_NORMAL
- en: This is just fantastic since labeled data is very scarce! However, it comes
    with some limitations. The main one is that clustering algorithms provide clusters
    for you, but not the meaning of each cluster. Thus, someone, as a subject matter
    expert, has to analyze the properties of each cluster to define their meanings.
  prefs: []
  type: TYPE_NORMAL
- en: There are many types of clustering approaches, such as hierarchical clustering
    and partitional clustering. Inside each approach, you will find several algorithms.
    However, K-Means is probably the most popular clustering algorithm, and you are
    likely to come across it in your exam.
  prefs: []
  type: TYPE_NORMAL
- en: When you are playing with K-Means, somehow, you have to specify the number of
    clusters that you want to create. Then, you have to allocate the data points across
    each cluster, so that each data point will belong to a single cluster. This is
    exactly what you should expect as a result at the end of the clustering process!
  prefs: []
  type: TYPE_NORMAL
- en: You need to specify the number of clusters that you want to create and pass
    this number to the K-Means algorithm. Then, the algorithm will randomly initiate
    the central point of each cluster (this is known as **centroid initialization**).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have the centroids of each cluster, all you need to do is assign a
    cluster to each data point. To do that, you have to use a proximity or distance
    metric! This book will use the term *distance metric*.
  prefs: []
  type: TYPE_NORMAL
- en: The **distance metric** is responsible for calculating the distance between
    data points and centroids. The data point will belong to the closer cluster centroid,
    according to the distance metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular distance metric is called **Euclidean distance** and the math
    behind it is simple; imagine that the points of your dataset are composed of two
    dimensions, *x* and *y*. So, you could consider points *a* and *b* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a (**x=1, y=1)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b (**x=2, y=5)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Euclidean distance between points *a* and *b* is given by the following
    formula, where *x*1 and *y*1 refer to the values of point *a*, and *x*2 and *y*2
    refer to the values of point *b*: ![](img/B21197_06_12a.png). The same function
    can be generalized by the following equation: ![](img/B21197_06_12b.png). Once
    you have completed this process and assigned a cluster for each data point, you
    ![](img/B21197_06_12c.png) methods, such as **single link, average link**, and
    **complete link**.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to this centroid refreshment, you will have to keep checking the closest
    cluster for each data point and keep refreshing the centroids, iteratively, until
    the cluster centroids converge and no cluster reassignment is needed, or the maximum
    number of allowed iterations is reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alright, the following is a summarization of the components and steps that
    compose the K-Means method:'
  prefs: []
  type: TYPE_NORMAL
- en: Centroid initialization, cluster assignment, centroid refreshment, and then
    redo the last two steps until it converges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A distance metric to assign data points to each cluster (in this case, Euclidian
    distance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A linkage method to recalculate the cluster centroids (for the sake of our demonstration,
    you will learn about the average linkage)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these definitions, you are now ready to walk through the following real
    example, step by step (some support material is also available for your reference).
  prefs: []
  type: TYPE_NORMAL
- en: Computing K-Means step by step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this example, you will simulate K-Means in a very small dataset, with only
    two columns (*x* and *y*) and six data points (*A*, *B*, *C*, *D*, *E*, and *F*),
    as defined in *Table 6.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Point** | **x** | **y** |'
  prefs: []
  type: TYPE_TB
- en: '| A | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 5 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| D | 5 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| E | 1 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| F | 2 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cluster 1** | **1** | **1** |'
  prefs: []
  type: TYPE_TB
- en: '| **Cluster 2** | **2** | **2** |'
  prefs: []
  type: TYPE_TB
- en: '| **Cluster 3** | **5** | **5** |'
  prefs: []
  type: TYPE_TB
- en: Table 6.8 – Iteration input data for K-Means
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 6.8* contains three clusters with the following centroids: *(1,1), (2,2),
    (5,5).* The number of clusters (3) was defined *a priori* and the centroid for
    each cluster was randomly defined. *Figure 6**.13* shows the stage of the algorithm
    that you are at right now.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Plotting the K-Means results before completing the first iteration](img/B21197_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Plotting the K-Means results before completing the first iteration
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can’t see points *A*, *B*, and *C* since they overlap with cluster
    centroids, but don’t worry – they will appear soon. Next, you have to compute
    the distance of each data point to each cluster centroid, and then, you need to
    choose the cluster that is the closest to each point.
  prefs: []
  type: TYPE_NORMAL
- en: '| **xc1** | **yc1** | **xc2** | **yc2** | **xc3** | **yc3** | **distance-c1**
    | **distance-c2** | **distance-c3** | **Cluster** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 0,0 | 1,4 | 5,7 | Cluster 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 1,4 | 0,0 | 4,2 | Cluster 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 5,7 | 4,2 | 0,0 | Cluster 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 6,4 | 5,0 | 1,0 | Cluster 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 4,0 | 3,2 | 4,0 | Cluster 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 5,1 | 4,0 | 3,2 | Cluster 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Legendxc1 = x value of cluster 1yc1 = y value of cluster 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 6.9 – Processing iteration 1
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 6.9* contains the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: Each row represents a data point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first six columns represent the centroid axis (*x* and *y*) of each cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next three columns represent the distance of each data point to each cluster
    centroid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last column represents the clusters that are the closest to each data point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at data point *A* (first row), you can see that it was assigned to cluster
    1 because the distance from data point *A* to cluster 1 is 0 (do you remember
    that they were overlapping?). The same calculation happens to all other data points
    to define a cluster for each data point.
  prefs: []
  type: TYPE_NORMAL
- en: Before you move on, you might want to see how those Euclidian distances between
    the clusters and the data points were computed. For demonstration purposes, the
    following simulation will consider the distance from data point *A* to cluster
    3 (the first row in *Table 6.9*, column `distance-c3`, value *5,7*).
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, the following formula was used to calculate the Euclidian distance:
    ![](img/B21197_06_13a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x*1 = *x* of data point *A* = 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*1 = *y* of data point *A* = 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*2 = *x* of cluster 3 = 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*2 = *y* of cluster 3 = 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 6**.14* applies the formula, step by step.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Computing the Euclidian distance step by step](img/B21197_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Computing the Euclidian distance step by step
  prefs: []
  type: TYPE_NORMAL
- en: 'That is just fantastic, isn’t it? You have almost completed the first iteration
    of K-Means. In the very last step of iteration 1, you have to refresh the cluster
    centroids. Remember: initially, they were randomly defined, but now, you have
    just assigned some data points to each cluster, which means you should be able
    to identify where the central point of the cluster is.'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the **linkage** method will be used to refresh the cluster
    centroids. This is a very simple step, and the results are presented in *Table
    6.10*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Point** | **x** | **y** |'
  prefs: []
  type: TYPE_TB
- en: '| A | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 5 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| D | 5 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| E | 1 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| F | 2 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cluster 1** | **1** | **1** |'
  prefs: []
  type: TYPE_TB
- en: '| **Cluster 2** | **1,5** | **3,5** |'
  prefs: []
  type: TYPE_TB
- en: '| **Cluster 3** | **4** | **5,7** |'
  prefs: []
  type: TYPE_TB
- en: Table 6.10 – K-Means results after iteration 1
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 6.10* shows the same data points (*A* to *F*) that you are dealing with
    (by the way, they will never change), and the centroids of clusters 1, 2, and
    3\. Those centroids are quite different from what they were initially, as shown
    in *Table 6.8*. This is because they were refreshed using average linkage! The
    method got the average value of all the *x* and *y* values of the data points
    of each cluster. In the next simulation, have a look at how *(1.5, 3.5)* were
    obtained as centroids of cluster 2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at *Table 6.9*, you will see that cluster 2 only has two data points
    assigned to it: *B* and *E*. These are the second and fifth rows in that figure.
    If you take the average values of the *x* axis of each point, then you will have
    *(2 + 1) / 2 = 1.5* and *(2 + 5) / 2 =* *3.5*.'
  prefs: []
  type: TYPE_NORMAL
- en: With that, you are done with iteration 1 of K-Means and you can view the results
    in *Figure 6**.15*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Plotting the K-Means results after the first iteration](img/B21197_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Plotting the K-Means results after the first iteration
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you can see almost all the data points, except for data point A because
    it is still overlapping with the centroid of cluster 1\. Moving on, you have to
    redo the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Recalculate the distance between each data point and each cluster centroid and
    reassign clusters, if needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recalculate the cluster centroids.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You do those two tasks many times until the cluster centroids converge and they
    don’t change anymore, *or* you reach the maximum number of allowed iterations,
    which can be set as a hyperparameter of K-Means. For demonstration purposes, after
    four iterations, your clusters will look like *Figure 6**.16*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Plotting the K-Means results after the fourth iteration](img/B21197_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Plotting the K-Means results after the fourth iteration
  prefs: []
  type: TYPE_NORMAL
- en: On the fourth iteration, all the cluster centroids look pretty consistent, and
    you can clearly see that all data points could be grouped according to their proximity.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In this example, you have only set two dimensions for each data point (dimensions
    *x* and *y*). In real use cases, you can see far more dimensions, and that is
    why clustering algorithms play a very important role in identifying groups in
    the data in a more automated fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, you have enjoyed how to compute K-Means from scratch! This knowledge
    will be beneficial for the exam and for your career as a data scientist. By the
    way, as advised many times, data scientists must be skeptical and curious, so
    you might be wondering why three clusters were defined in this example and not
    two or four. You may also be wondering how you measure the quality of the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: You didn’t think this explanation wouldn’t be provided, did you?
  prefs: []
  type: TYPE_NORMAL
- en: Defining the number of clusters and measuring cluster quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although K-Means is a great algorithm for finding patterns in your data, it
    will not provide the meaning of each cluster, nor the number of clusters you have
    to create to maximize cluster quality.
  prefs: []
  type: TYPE_NORMAL
- en: In clustering, cluster quality means that you want to create groups with a high
    homogeneity among the elements of the same cluster, and a high heterogeneity among
    the elements of different clusters. In other words, the elements of the same clusters
    should be close/similar, whereas the elements of different clusters should be
    well separated.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to compute the cluster’s homogeneity is by using a metric known as
    the **sum of square errors**, or **SSE** for short. This metric will compute the
    sum of squared differences between each data point and its cluster centroid. For
    example, when all the data points are located at the same point where the cluster
    centroid is, then the SSE will be 0\. In other words, you want to minimize the
    SSE. The following equation formally defines the SSE: ![](img/B21197_06_16a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to check the cluster quality, it is easier to understand
    how to define the number of appropriate clusters for a given dataset. All you
    have to do is find the optimal number of clusters to minimize the SSE. A very
    popular method that works around that logic is known as the **elbow method**.
  prefs: []
  type: TYPE_NORMAL
- en: The elbow method proposes executing the clustering algorithm many times. In
    each execution, you will test a different number of clusters, *k*. After each
    execution, you compute the SSE related to that *k* number of clusters. Finally,
    you can plot these results and select the number of *k* where the SSE stops to
    drastically decrease.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Adding more clusters will naturally decrease the SSE. In the elbow method, you
    want to find the point where that change becomes smoother, which means that the
    addition of new clusters will not bring too much value.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, three clusters were created. *Figure 6**.17* shows
    the elbow analysis that supports this decision.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – The elbow method](img/B21197_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – The elbow method
  prefs: []
  type: TYPE_NORMAL
- en: You can conclude that adding more than three or four clusters will add unnecessary
    complexity to the clustering process.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you should always consider the business background while defining
    the number of clusters. For example, if you are creating a customer segmentation
    model and your company has prepared the commercial team and business processes
    to support four segments of customers, there is no harm in setting up four clusters
    instead of three.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you should know that AWS has implemented K-Means as part of its list
    of built-in algorithms. In other words, you don’t have to use external libraries
    or bring your own algorithm to play with K-Means on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'That was a really good accomplishment: you just mastered the basics of clustering
    algorithms and you should now be able to drive your own projects and research
    about this topic! For the exam, remember that clustering belongs to the unsupervised
    field of machine learning, so there is no need to have labeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, make sure that you know how the most popular algorithm of this field works
    – that is, K-Means. Although clustering algorithms do not provide the meaning
    of each group, they are very powerful for finding patterns in the data, either
    to model a particular problem or just to explore the data.
  prefs: []
  type: TYPE_NORMAL
- en: Coming up next, you will keep studying unsupervised algorithms and see how AWS
    has built one of the most powerful algorithms out there for anomaly detection,
    known as **RCF**.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finding anomalies in data is a very common task in modeling and data exploratory
    analysis. Sometimes, you might want to find anomalies in the data just to remove
    them before fitting a regression model, while other times, you might want to create
    a model that identifies anomalies as an end goal – for example, in fraud detection
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, you can use many different methods to find anomalies in the data. With
    some creativity, the possibilities are endless. However, there is a particular
    algorithm that works around this problem that you should definitely be aware of
    for your exam: RCF.'
  prefs: []
  type: TYPE_NORMAL
- en: RCF is an unsupervised decision tree-based algorithm that creates multiple decision
    trees (forests) using random subsamples of the training data. Technically, it
    randomizes the data and then creates samples according to the number of trees.
    Finally, these samples are distributed across each tree.
  prefs: []
  type: TYPE_NORMAL
- en: These sets of trees are used to assign an anomaly score to the data points.
    To calculate the anomaly score for a particular data point, it is passed down
    each tree in the forest. As the data point moves through the tree, the path length
    from the root node to the leaf node is recorded for that specific tree. The anomaly
    score for that data point is then determined by considering the distribution of
    path lengths across all the trees in the forest.
  prefs: []
  type: TYPE_NORMAL
- en: If a data point follows a short path in most trees (i.e., it is close to the
    root node), it is considered a common point and will have a lower anomaly score.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if a data point follows a long path in many trees (i.e.,
    it is far from the root node), it is considered an uncommon point and will have
    a higher anomaly score.
  prefs: []
  type: TYPE_NORMAL
- en: The most important hyperparameters of RCF are `num_trees` and `num_samples_per_tree`,
    which are the number of trees in the forest and the number of samples per tree,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another unsupervised algorithm that was implemented by AWS in its list of built-in
    algorithms is known as principal component analysis, or PCA for short. PCA is
    a technique that’s used to reduce the number of variables/dimensions in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea behind PCA is plotting the data points to another set of coordinates,
    known as **Principal Components (PCs)**, which aims to explain the most variance
    in the data. By definition, the first component will capture more variance than
    the second component, then the second component will capture more variance than
    the third one, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can set up as many PCs as you need, as long as it does not surpass the
    number of variables in your dataset. *Figure 6**.18* shows how these PCs are drawn:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Finding PCs in PCA](img/B21197_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Finding PCs in PCA
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, the first PC will be drawn in such a way that it will
    capture most of the variance in the data. That is why it passes near the majority
    of the data points in *Figure 6**.18*.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the second PC will be perpendicular to the first one, so that it will
    be the second component that explains the variance in the data. If you want to
    create more components (consequentially, capturing more variance), you just have
    to follow the same rule of adding perpendicular components. **Eigenvectors** and
    **eigenvalues** are the linear algebra concepts associated with PCA that compute
    the PCs.
  prefs: []
  type: TYPE_NORMAL
- en: So, what is the story with dimension reduction here? In case it is not clear
    yet, these PCs can be used to replace your original variables. For example, consider
    you have 10 variables in your dataset, and you want to reduce this dataset to
    three variables that best represent the others. A potential solution for that
    would be applying PCA and extracting the first three PCs!
  prefs: []
  type: TYPE_NORMAL
- en: Do these three components explain 100% of your dataset? Probably not, but ideally,
    they will explain most of the variance. Adding more PCs will explain more variance
    but at the cost of adding extra dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Using AWS’s built-in algorithm for PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In AWS, PCA works in two different modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regular**: For datasets with a moderate number of observations and features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Randomized**: For datasets with a large number of observations and features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference is that, in randomized mode, it is used as an approximation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the main hyperparameter of PCA is the number of components that you
    want to extract, known as `num_components`.
  prefs: []
  type: TYPE_NORMAL
- en: IP Insights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IP Insights is an unsupervised algorithm that is used for pattern recognition.
    Essentially, it learns the usage pattern of IPv4 addresses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *modus operandi* of this algorithm is very intuitive: it is trained on
    top of pairs of events in the format of entity and IPv4 address so that it can
    understand the pattern of each entity that it was trained on.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: For instance, you can understand “entity” as user IDs or account numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Then, to make predictions, it receives a pair of events with the same data structure
    (entity, IPv4 address) and returns an anomaly score for that particular IP address,
    according to the input entity.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: This anomaly score that is returned by IP Insights infers how anomalous the
    pattern of the event is.
  prefs: []
  type: TYPE_NORMAL
- en: You might come across many applications with IP Insights. For example, you can
    create an IP Insights model that was trained on top of your application login
    events (this is your entity). You should be able to expose this model through
    an API endpoint to make predictions in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Then, during the authentication process of your application, you could call
    your endpoint and pass the IP address that is trying to log in. If you got a high
    score (meaning this pattern of logging in looks anomalous), you can request extra
    information before authorizing access (even if the password was right).
  prefs: []
  type: TYPE_NORMAL
- en: This is just one of the many applications of IP Insights you could think about.
    Next, you will learn about textual analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Textual analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern applications use **Natural Language Processing (NLP)** for several purposes,
    such as text translation, document classifications, web search, **Named Entity
    Recognition (NER)**, and many others.
  prefs: []
  type: TYPE_NORMAL
- en: AWS offers a suite of algorithms for most NLP use cases. In the next few subsections,
    you will have a look at these built-in algorithms for textual analysis.
  prefs: []
  type: TYPE_NORMAL
- en: BlazingText algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'BlazingText does two different types of tasks: text classification, which is
    a supervised learning approach that extends the **fastText** text classifier,
    and Word2Vec, which is an unsupervised learning algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: BlazingText’s implementations of these two algorithms are optimized to run on
    large datasets. For example, you can train a model on top of billions of words
    in a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This scalability aspect of BlazingText is possible due to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Its ability to use multi-core CPUs and a single GPU to accelerate text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its ability to use multi-core CPUs or GPUs, with custom CUDA kernels for GPU
    acceleration, when playing with the Word2Vec algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Word2Vec option supports **batch_skipgram** mode, which allows BlazingText
    to do distributed training across multiple CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The distributed training that’s performed by BlazingText uses a mini-batching
    approach to convert **level-1 BLAS (Basic Linear Algebra Subprograms)** operations
    into **level-3 BLAS** operations. If you see these terms during your exam, you
    should know that they are related to BlazingText (Word2Vec mode).
  prefs: []
  type: TYPE_NORMAL
- en: Still in Word2Vec mode, BlazingText supports both the **skip-gram** and **Continuous
    Bag of Words (****CBOW)** architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, note the following configurations of BlazingText, since they are likely
    to be present in your exam:'
  prefs: []
  type: TYPE_NORMAL
- en: In Word2Vec mode, only the train channel is available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BlazingText expects a single text file with space-separated tokens. Each line
    of the file must contain a single sentence. This means you usually have to preprocess
    your corpus of data before using BlazingText.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence-to-sequence algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a supervised algorithm that transforms an input sequence into an output
    sequence. This sequence can be a text sentence or even an audio recording.
  prefs: []
  type: TYPE_NORMAL
- en: The most common use cases for sequence-to-sequence are machine translation,
    text summarization, and speech-to-text. Anything that you think is a sequence-to-sequence
    problem can be approached by this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Technically, AWS SageMaker’s Seq2Seq uses two types of neural networks to create
    models: an **RNN** and a **Convolutional Neural Network (CNN)** with an attention
    mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latent Dirichlet allocation**, or **LDA** for short, is used for topic modeling.
    Topic modeling is a textual analysis technique where you can extract a set of
    topics from a corpus of text data. LDA learns these topics based on the probability
    distribution of the words in the corpus of text.'
  prefs: []
  type: TYPE_NORMAL
- en: Since this is an unsupervised algorithm, there is no need to set a target variable.
    Also, the number of topics must be specified up-front, and you will have to analyze
    each topic to find its domain meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Topic Model algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like the LDA algorithm, the **Neural Topic Model (NTM)** also aims to extract
    topics from a corpus of data. However, the difference between LDA and NTM is their
    learning logic. While LDA learns from probability distributions of the words in
    the documents, NTM is built on top of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: The NTM network architecture has a bottleneck layer, which creates an embedding
    representation of the documents. This bottleneck layer contains all the necessary
    information to predict document composition, and its coefficients can be considered
    topics.
  prefs: []
  type: TYPE_NORMAL
- en: With that, you have completed this section on textual analysis. In the next
    section, you will learn about image processing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Image processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Image processing is a very popular topic in machine learning. The idea is pretty
    self-explanatory: creating models that can analyze images and make inferences
    on top of them. By inference, you can understand this as detecting objects in
    an image, classifying images, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: AWS offers a set of built-in algorithms you can use to train image processing
    models. In the next few sections, you will have a look at those algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Image classification algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, the image classification algorithm is used to classify
    images using supervised learning. In other words, it needs a label within each
    image. It supports multi-label classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way it operates is simple: during training, it receives an image and its
    associated labels. During inference, it receives an image and returns all the
    predicted labels. The image classification algorithm uses a CNN (**ResNet**) for
    training. It can either train the model from scratch or take advantage of transfer
    learning to pre-load the first few layers of the neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: According to AWS’s documentation, the `.jpg` and `.png` file formats are supported,
    but the recommended format is **MXNet’s RecordIO**.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The semantic segmentation algorithm provides a pixel-level capability for creating
    computer vision applications. It tags each pixel of the image with a class, which
    is an important feature for complex applications such as self-driving and medical
    image diagnostics.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of its implementation, the semantic segmentation algorithm uses the
    **MXNet Gluon framework** and the **Gluon CV toolkit**. You can choose any of
    the following algorithms to train a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully Convolutional** **Network (FCN)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pyramid Scene** **Parsing (PSP)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepLabV3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these options work as an **encoder-decoder** neural network architecture.
    The output of the network is known as a **segmentation mask**.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just like the image classification algorithm, the main goal of the object detection
    algorithm is also self-explanatory: it detects and classifies objects in images.
    It uses a supervised approach to train a deep neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During the inference process, this algorithm returns the identified objects
    and a score of confidence regarding the prediction. The object detection algorithm
    uses **Single Shot MultiBox Detector (SSD)** and supports two types of network
    architecture: **Visual Geometry Group (VGG)** and **Residual** **Network (ResNet).**'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'That was such a journey! Take a moment to recap what you have just learned.
    This chapter had four main topics: supervised learning, unsupervised learning,
    textual analysis, and image processing. Everything that you have learned fits
    into those subfields of machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of supervised learning algorithms that you have studied includes the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear learner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factorization machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object2Vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepAR forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that you can use linear learner, factorization machines, XGBoost, and
    KNN for multiple purposes, including solving regression and classification problems.
    Linear learner is probably the simplest algorithm out of these four; factorization
    machines extends linear earner and is good for sparse datasets, XGBoost uses an
    ensemble method based on decision trees, and KNN is an index-based algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The other two algorithms, Object2Vec and DeepAR, are used for specific purposes.
    Object2Vec is used to create vector representations of the data, while DeepAR
    is used to create forecast models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of unsupervised learning algorithms that you have studied includes
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: K-Means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP Insights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RCF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Means is a very popular algorithm that is used for clustering. PCA is used
    for dimensionality reduction, IP Insights is used for pattern recognition, and
    RCF is used for anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: You then looked at regression models and K-Means in more detail. You did this
    because, as a data scientist, you should at least master these two very popular
    algorithms so that you can go deeper into other algorithms by yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you moved on to the second half of this chapter, where you learned about
    textual analysis and the following algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: BlazingText
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence-to-sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, you learned about image processing and looked at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Image classification algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic segmentation algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the topics covered in this chapter are very important with regard to the
    AWS Certified Machine Learning Specialty exam, you are highly encouraged to jump
    into the AWS website and search for machine learning algorithms. There, you will
    find the most recent information about the algorithms that you have just learned
    about. Please make sure you do it before taking the exam.
  prefs: []
  type: TYPE_NORMAL
- en: That brings you to the end of this quick refresher and the end of this chapter.
    In the next chapter, you will learn about the existing mechanisms provided by
    AWS that you can use to optimize and evaluate these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Exam Readiness Drill – Chapter Review Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How To Access These Resources
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the link – [https://packt.link/MLSC01E2_CH06](https://packt.link/MLSC01E2_CH06).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, you can scan the following **QR code** (*Figure 6**.19*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.19 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – QR code that opens Chapter Review Questions for logged-in users
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 6**.20*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Chapter Review Questions for Chapter 6](img/B21197_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Chapter Review Questions for Chapter 6
  prefs: []
  type: TYPE_NORMAL
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exam Readiness Drill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first three attempts, don’t worry about the time limit.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  prefs: []
  type: TYPE_NORMAL
- en: Working On Timing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attempt** | **Score** | **Time Taken** |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  prefs: []
  type: TYPE_TB
- en: Table 6.11 – Sample timing practice drills on the online platform
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  prefs: []
  type: TYPE_NORMAL
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  prefs: []
  type: TYPE_NORMAL
