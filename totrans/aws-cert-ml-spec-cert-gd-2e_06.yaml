- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applying Machine Learning Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned about understanding data and visualization.
    It is now time to move on to the modeling phase and study machine learning algorithms!
    In the earlier chapters, you learned that building machine learning models requires
    a lot of knowledge about AWS services, data engineering, data exploration, data
    architecture, and much more. This time, you will delve deeper into the algorithms
    that have been introduced and more.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Having a good sense of the different types of algorithms and machine learning
    approaches will put you in a very good position to make decisions during your
    projects. Of course, this type of knowledge is also crucial to the AWS Certified
    Machine Learning Specialty exam.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that there are thousands of algorithms out there. You can even
    propose your own algorithm for a particular problem. In this chapter, you will
    learn about the most relevant ones and, hopefully, the ones that you will probably
    face in the exam.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics of this chapter are as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Storing the training data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A word about ensemble models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Supervised learning:'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object2Vec
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unsupervised learning:'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP Insights
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Textual analysis (natural language processing)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image processing
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alright, grab a coffee and rock it!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Introducing this chapter
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During this chapter, you will read about several algorithms, modeling concepts,
    and learning strategies. All these topics are beneficial for you to know for the
    exam and throughout your career as a data scientist.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has been structured in such a way that it not only covers the necessary
    topics of the exam but also gives you a good sense of the most important learning
    strategies out there. For example, the exam will check your knowledge regarding
    the basic concepts of K-Means. However, this chapter will cover it on a much deeper
    level, since this is an important topic for your career as a data scientist.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter will follow this approach of looking deeper into the algorithms’
    logic for some types of models that every data scientist should master. Furthermore,
    keep this in mind: sometimes you may go deeper than what is expected of you in
    the exam, but that will be extremely important for you in your career.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Many times during this chapter, you will see the term **built-in algorithms**.
    This term will be used to refer to the list of algorithms implemented by AWS on
    their SageMaker SDK.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a concrete example: you can use scikit-learn’s **K-nearest neighbors**
    algorithm, or KNN for short (if you don’t remember what scikit-learn is, refresh
    your memory by going back to [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*,
    Machine Learning Fundamentals*) to create a classification model and deploy it
    to SageMaker. However, AWS also offers its own implementation of the KNN algorithm
    on its SDK, which is optimized to run in the AWS environment. Here, KNN is an
    example of a built-in algorithm.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'The possibilities on AWS are endless because you can either take advantage
    of built-in algorithms or bring in your own algorithm to create models on SageMaker.
    Finally, just to make this very clear, here is an example of how to import a built-in
    algorithm from the AWS SDK:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You will learn how to create models on SageMaker in [*Chapter 9*](B21197_09.xhtml#_idTextAnchor1224)*,
    Amazon SageMaker Modeling*. For now, just understand that AWS has its own set
    of libraries where those built-in algorithms are implemented.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: To train and evaluate a model, you need training and testing data. After instantiating
    your estimator, you should then feed it with those datasets. Not to spoil [*Chapter
    9*](B21197_09.xhtml#_idTextAnchor1224)*, Amazon SageMaker Modeling*, but you should
    know about the concept of **data channels** in advance.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Data channels are configurations related to input data that you can pass to
    SageMaker when you are creating a training job. You should set these configurations
    just to inform SageMaker of how your input data is formatted.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 9*](B21197_09.xhtml#_idTextAnchor1224)*, Amazon SageMaker Modeling*,
    you will learn how to create training jobs and how to set data channels. As of
    now, you should know that while configuring data channels, you can set a `ContentType`)
    and an `TrainingInputMode`). You will now take a closer look at how and where
    the training data should be stored so that it can be integrated properly with
    AWS’s built-in algorithms.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Storing the training data
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, you can use multiple AWS services to prepare data for machine
    learning, such as **Elastic MapReduce (EMR),** Redshift, Glue, and so on. After
    preprocessing the training data, you should store it in S3, in a format expected
    by the algorithm you are using. *Table 6.1* shows the list of acceptable data
    formats per algorithm.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data format** | **Algorithm** |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| `Application/x-image` | Object detection algorithm, semantic segmentation
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| `Application/x-recordio` | Object detection algorithm |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| `Application/x-recordio-protobuf` | Factorization machines, K-Means, KNN,
    latent Dirichlet allocation, linear learner, NTM, PCA, RCF, sequence-to-sequence
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| `Application/jsonlines` | BlazingText, DeepAR |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '| `Image/.jpeg` | Object detection algorithm, semantic segmentation |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| `Image/.png` | Object detection algorithm, semantic segmentation |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| `Text/.csv` | IP Insights, K-Means, KNN, latent Dirichlet allocation, linear
    learner, NTM, PCA, RCF, XGBoost |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| `Text/.libsvm` | XGBoost |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| `Text/.libsvm` | XGBoost |'
- en: Table 6.1 – Data formats that are acceptable per AWS algorithm
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 – AWS算法可接受的每种数据格式
- en: 'As you can see, many algorithms accept `Text/.csv` format. You should follow
    these rules if you want to use that format:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，许多算法接受`Text/.csv`格式。如果你想使用该格式，你应该遵循以下规则：
- en: Your CSV file *cannot* have a header record.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的CSV文件*不能*有标题记录。
- en: For supervised learning, the target variable must be in the first column.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于监督学习，目标变量必须在第一列。
- en: While configuring the training pipeline, set the input data channel as `content_type`
    equal to `text/csv`.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在配置训练管道时，将输入数据通道设置为`content_type`等于`text/csv`。
- en: 'For unsupervised learning, set `label_size` within `content_type`, as follows:
    `''content_type=text/csv;label_size=0''`.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于无监督学习，在`content_type`中设置`label_size`，如下所示：`'content_type=text/csv;label_size=0'`。
- en: Although `text/.csv` format is fine for many use cases, most of the time, AWS’s
    built-in algorithms work better with `recordIO-protobuf`. This is an optimized
    data format that is used to train AWS’s built-in algorithms, where SageMaker converts
    each observation in the dataset into a binary representation that is a set of
    4-byte floats.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对于许多用例来说，`text/.csv`格式是可行的，但大多数情况下，AWS的内置算法与`recordIO-protobuf`配合得更好。这是一种用于训练AWS内置算法的优化数据格式，其中SageMaker将数据集中的每个观测值转换为二进制表示，即一组4字节的浮点数。
- en: 'RecordIO-protobuf accepts two types of input modes: pipe mode and file mode.
    In pipe mode, the data will be streamed directly from S3, which helps optimize
    storage. In file mode, the data is copied from S3 to the training instance’s store
    volume.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: RecordIO-protobuf接受两种输入模式：管道模式和文件模式。在管道模式下，数据将直接从S3流式传输，这有助于优化存储。在文件模式下，数据将从S3复制到训练实例的存储卷中。
- en: You are almost ready! Now you can take a quick look at some modeling definitions
    that will help you understand some more advanced algorithms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你几乎准备好了！现在你可以快速查看一些建模定义，这将帮助你理解一些更高级的算法。
- en: A word about ensemble models
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于集成模型的一些话
- en: Before you start diving into the algorithms, there is an important modeling
    concept that you should be aware of – **ensemble**. The term ensemble is used
    to describe methods that use multiple algorithms to create a model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始深入研究算法之前，有一个重要的建模概念你应该了解——**集成**。集成这个术语用来描述使用多个算法来创建模型的方法。
- en: A regular algorithm that *does not* implement ensemble methods will rely on
    a single model to train and predict the target variable. That is what happens
    when you create a decision tree or regression model. On the other hand, algorithms
    that *do* implement ensemble methods will rely on multiple models to predict the
    target variable. In that case, since each of these models might come up with a
    different prediction for the target variable, ensemble algorithms implement either
    a voting (for classification models) or averaging (for regression models) system
    to output the final results. *Table 6.2* illustrates a very simple voting system
    for an ensemble algorithm composed of three models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不实现集成方法的常规算法将依赖于单个模型来训练和预测目标变量。这就是当你创建决策树或回归模型时发生的情况。另一方面，实现集成方法的算法将依赖于多个模型来预测目标变量。在这种情况下，由于每个模型可能会对目标变量提出不同的预测，集成算法实现了一个投票系统（用于分类模型）或平均系统（用于回归模型）来输出最终结果。*表6.2*展示了由三个模型组成的集成算法的一个非常简单的投票系统。
- en: '| **Transaction** | **Model A** | **Model B** | **Model C** | **Prediction**
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| **交易** | **模型A** | **模型B** | **模型C** | **预测** |'
- en: '| 1 | Fraud | Fraud | Not Fraud | Fraud |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 欺诈 | 欺诈 | 非欺诈 | 欺诈 |'
- en: '| 2 | Not Fraud | Not Fraud | Not Fraud | Not Fraud |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 非欺诈 | 非欺诈 | 非欺诈 | 非欺诈 |'
- en: '| 3 | Fraud | Fraud | Fraud | Fraud |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 欺诈 | 欺诈 | 欺诈 | 欺诈 |'
- en: '| 4 | Not Fraud | Not Fraud | Fraud | Not Fraud |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 非欺诈 | 非欺诈 | 欺诈 | 非欺诈 |'
- en: Table 6.2 – An example of a voting system on ensemble methods
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2 – 集成方法中投票系统的一个示例
- en: As described before, the same approach works for regression problems, where
    instead of voting, it could average the results of each model and use that as
    the outcome.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，同样的方法也适用于回归问题，其中不是投票，而是可以平均每个模型的预测结果，并使用该结果作为输出。
- en: Voting and averaging are just two examples of ensemble approaches. Other powerful
    techniques include blending and stacking, where you can create multiple models
    and use the outcome of each model as a feature for a main model. Looking back
    at *Table 6.2*, columns *Model A*, *Model B*, and *Model C* could be used as features
    to predict the final outcome.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 投票和平均只是集成方法的两个例子。其他强大的技术包括混合和堆叠，其中你可以创建多个模型，并将每个模型的输出作为主模型的特征。回顾一下 *表 6.2*，*模型
    A*、*模型 B* 和 *模型 C* 的列可以用作预测最终结果的特性。
- en: 'It turns out that many machine learning algorithms use ensemble methods while
    training, in an embedded way. These algorithms can be classified into two main
    categories:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，许多机器学习算法在训练过程中使用集成方法，以嵌入式的方式。这些算法可以分为两大类：
- en: '**Bootstrapping aggregation** or **bagging**: With this approach, several models
    are trained on top of different samples of data. Predictions are then made through
    the voting or averaging system. The most popular algorithm from this category
    is known as **Random Forest**.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自助聚合** 或 **袋装法**：这种方法中，在数据的不同样本上训练多个模型。然后，通过投票或平均系统进行预测。这个类别中最受欢迎的算法被称为 **随机森林**。'
- en: '**Boosting**: With this approach, several models are trained on top of different
    samples of the data. One model then tries to correct the error of the next model
    by penalizing incorrect predictions. The most popular algorithms from this category
    are **stochastic gradient boosting** and **AdaBoost**.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升法**：这种方法中，在数据的不同样本上训练多个模型。然后，一个模型试图通过惩罚错误预测来纠正下一个模型的错误。这个类别中最受欢迎的算法是 **随机梯度提升**
    和 **AdaBoost**。'
- en: Now that you know what ensemble models are, you can look at some machine learning
    algorithms that are likely to be present in your exam. Not all of them use ensemble
    approaches.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了集成模型，你可以看看一些可能出现在你的考试中的机器学习算法。并非所有这些算法都使用集成方法。
- en: 'The next few sections are split based on AWS algorithm categories, as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下几节将根据 AWS 算法类别进行划分，如下所示：
- en: Supervised learning
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Textual analysis
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分析
- en: Image processing
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像处理
- en: Finally, you will have an overview of reinforcement learning on AWS.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将了解 AWS 上的强化学习概述。
- en: Supervised learning
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'AWS provides supervised learning algorithms for general purposes (regression
    and classification tasks) and more specific purposes (forecasting and vectorization).
    The list of built-in algorithms that can be found in these sub-categories is as
    follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 为通用目的（回归和分类任务）和更具体的目的（预测和向量化）提供了监督学习算法。以下是在这些子类别中可以找到的内置算法列表：
- en: Linear learner algorithm
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性学习算法
- en: Factorization machines algorithm
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子分解机算法
- en: XGBoost algorithm
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost 算法
- en: KNN algorithm
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KNN 算法
- en: Object2Vec algorithm
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Object2Vec 算法
- en: DeepAR forecasting algorithm
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepAR 预测算法
- en: You will start by learning about regression models and the linear learner algorithm.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你将首先学习关于回归模型和线性学习算法。
- en: Working with regression models
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与回归模型一起工作
- en: Looking at **linear regression** models is a nice way to understand what is
    going on inside **regression models** in general (linear and non-linear regression
    models). This is mandatory knowledge for every data scientist and can help you
    solve real challenges as well. You will now take a closer look at this in the
    following subsections.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 **线性回归** 模型是理解一般回归模型（线性回归和非线性回归模型）内部情况的好方法。这是每位数据科学家必备的知识，也能帮助你解决实际问题。你将在接下来的小节中对此进行更深入的了解。
- en: Introducing regression algorithms
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍回归算法
- en: Linear regression models aim to predict a numeric value (*y*) according to one
    or more variables (*x*). Mathematically, such a relationship can be defined as
    *y = f(x)*, where *y* is known as the **dependent variable** and *x* is known
    as the **independent variable**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型旨在根据一个或多个变量（*x*）预测一个数值（*y*）。从数学上讲，这种关系可以定义为 *y = f(x)*，其中 *y* 被称为 **因变量**，*x*
    被称为 **自变量**。
- en: With regression models, the component that you want to predict (*y*) is always
    a continuous number – for example, the price of houses or the number of transactions.
    You saw this in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*, Machine* *Learning
    Fundamentals*, in *Figure 1**.2*, when you were learning about the right type
    of supervised learning algorithm, given the target variable. Please feel free
    to go back and review it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归模型中，你想要预测的组成部分（*y*）始终是一个连续的数字——例如，房屋价格或交易数量。你在[*第一章*](B21197_01.xhtml#_idTextAnchor018)*，机器学习基础*，*图1**.2中看到了这一点，当你学习关于给定目标变量的正确监督学习算法时。请随时回去复习。
- en: When you use *just one variable to predict y*, this problem is referred to as
    **simple linear regression**. On the other hand, when you use *more than one variable
    to predict y*, you have a **multiple linear** **regression** problem.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当你只用一个变量来预测*y*时，这个问题被称为**简单线性回归**。另一方面，当你使用多个变量来预测*y*时，你面临的是一个**多元线性****回归**问题。
- en: There is also another class of regression models, known as **non-linear regression**.
    However, let us put that aside for a moment and understand what simple linear
    regression means.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一种被称为**非线性回归**的回归模型类别。然而，让我们暂时将其放在一边，先来理解一下简单线性回归的含义。
- en: Regression models belong to the supervised side of machine learning (the other
    side is non-supervised) because algorithms try to predict values according to
    existing correlations between independent and dependent variables.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型属于机器学习的监督方面（另一方面是非监督），因为算法试图根据独立变量和依赖变量之间的现有相关性来预测值。
- en: 'But what does *f* mean in *y=f(x)*? Here, *f* is the regression function responsible
    for predicting *y* based on *x*. In other words, this is the function that you
    want to find. When talking about simple linear regression, pay attention to the
    next three questions and answers:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但在*y=f(x)*中，*f*代表什么？在这里，*f*是负责根据*x*预测*y*的回归函数。换句话说，这就是你想要找到的函数。在谈论简单线性回归时，请注意以下三个问题和答案：
- en: What is the shape of *f* in linear regression?
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归中*f*的形状是什么？
- en: Linear, of course!
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当然是线性的！
- en: How can you represent a linear relationship?
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何表示线性关系？
- en: Using a *straight* line (you will understand why in a few minutes).
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用一条*直线*（你将在几分钟内理解原因）。
- en: So what is the function that defines a line?
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那么，定义一条线的函数是什么？
- en: '*ax + b* (just check any *mathematics* book).'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ax + b*（只需查看任何一本*数学*书）。'
- en: That is it! Linear regression models are given by *y = ax + b*. When you are
    trying to predict *y* given *x*, you just need to find out the values of *a* and
    *b*. You can adopt the same logic to figure out what is going on inside other
    kinds of regression.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！线性回归模型由*y = ax + b*给出。当你试图根据*x*预测*y*时，你只需要找出*a*和*b*的值。你可以采用相同的逻辑来了解其他类型回归内部的情况。
- en: Finding out the values of *a* and *b* is the only thing you are going to do.
    It is nice to know that *a* is also known as the **alpha coefficient**, or **slope**,
    and represents the line’s inclination, while *b* is also known as the **beta coefficient**,
    or **y intercept**, and represents the place where the line crosses the *y* axis
    (into a two-dimensional plane consisting of *x* and *y*). You will learn about
    these two terms in a later subsection.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 找出*a*和*b*的值是你唯一要做的事情。值得知道的是，*a*也被称为**alpha系数**，或**斜率**，表示线的倾斜度，而*b*也被称为**beta系数**，或**y截距**，表示线与*y*轴交叉的位置（进入由*x*和*y*组成的二维平面）。你将在下一小节中了解这两个术语。
- en: It is also nice to know that there is a bias (*e*) associated with every predictor
    that you do not have control over. That being said, the formal definition of simple
    linear regression is given by *y = ax + b +* *e*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个值得知道的事实，每个你无法控制的预测因子都存在一个偏差（*e*）。换句话说，简单线性回归的正式定义是*y = ax + b +* *e*。
- en: In the next subsection, you will learn how to find alpha and beta to solve a
    simple linear regression problem.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，你将学习如何找到alpha和beta来解决简单线性回归问题。
- en: Least squares method
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小二乘法
- en: 'There are different ways to find the slope and *y* intercept of a line, but
    the most used method is known as the **least squares method**. The principle behind
    this method is simple: you have to find the *best line that reduces the sum of*
    *squared error*.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法来找到直线的斜率和*y*截距，但最常用的方法是**最小二乘法**。这个方法背后的原理很简单：你必须找到一条*最佳线*，以减少*平方误差*的总和。
- en: 'In *Figure 6**.1*, you can see a Cartesian plane with multiple points and lines
    in it. *Line a* represents the best fit for this data – in other words, that would
    be the best linear regression function for those points. But how can you know
    that? It is simple: if you compute the error associated with each point, you will
    realize that *Line a* contains the least sum of squared errors.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6*.1中，你可以看到一个带有多个点和线的笛卡尔平面。*线a*代表这些数据点的最佳拟合线——换句话说，那将是这些点的最佳线性回归函数。但你怎么知道呢？很简单：如果你计算每个点的误差，你就会发现*线a*包含了最小的平方误差总和。
- en: '![Figure 6.1 – Visualizing the principle of the least squares method](img/B21197_06_01.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 最小二乘法原理的可视化](img/B21197_06_01.jpg)'
- en: Figure 6.1 – Visualizing the principle of the least squares method
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 最小二乘法原理的可视化
- en: It is worth understanding linear regression from scratch not only for the certification
    exam but also for your career as a data scientist. To provide you with a complete
    example, a spreadsheet containing all the calculations that you are going to see
    has been developed! You are encouraged to jump on this support material and perform
    some simulations. In any case, you will see these calculations in action in the
    next subsection.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始理解线性回归不仅对认证考试很重要，而且对你的数据科学家职业生涯也至关重要。为了提供一个完整的示例，已经开发了一个包含你将要看到的全部计算的电子表格！鼓励你利用这份辅助材料进行一些模拟。无论如何，你将在下一小节中看到这些计算的实际应用。
- en: Creating a linear regression model from scratch
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从零开始创建线性回归模型
- en: 'You are going to use a very simple dataset, with only two variables:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用一个非常简单的数据集，其中只有两个变量：
- en: '*x*: Represents the person’s number of years of work experience'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*：代表一个人的工作年限'
- en: '*y*: Represents the person’s average salary'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*：代表一个人的平均工资'
- en: You want to understand the relationship between *x* and *y* and, if possible,
    predict the salary (*y*) based on years of experience (*x*). Real problems very
    often have far more independent variables and are not necessarily linear. However,
    this example will give you the baseline knowledge to master more complex algorithms.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要了解*x*和*y*之间的关系，如果可能的话，根据工作经验*x*预测工资(*y*)。现实问题通常有更多的独立变量，并且不一定呈线性。然而，这个例子将为你掌握更复杂的算法提供基础知识。
- en: To find out what the alpha and beta coefficients are (or slope and *y* intercept
    if you prefer), you need to find some statistics related to the dataset. In *Table
    6.3*, you have the data and these auxiliary statistics.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出alpha和beta系数（或者如果你更喜欢，斜率和*y*截距），你需要找到与数据集相关的某些统计信息。在*表6.3*中，你有数据以及这些辅助统计信息。
- en: '| **X (****INDEPENDENT)** | **Y (****DEPENDENT)** | **X MEAN** | **Y MEAN**
    | **COVARIANCE  (****X,Y)** | **X VARIANCE** | **Y VARIANCE** |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **X (****独立变量)** | **Y (****依赖变量)** | **X 平均值** | **Y 平均值** | **协方差  (****X,Y)**
    | **X 方差** | **Y 方差** |'
- en: '| *1* | *1.000* |  |  | 21.015 | 20 | 21.808.900 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| *1* | *1.000* |  |  | 21.015 | 20 | 21.808.900 |'
- en: '| *2* | *1.500* |  |  | 14.595 | 12 | 17.388.900 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| *2* | *1.500* |  |  | 14.595 | 12 | 17.388.900 |'
- en: '| *3* | *3.700* |  |  | 4.925 | 6 | 3.880.900 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| *3* | *3.700* |  |  | 4.925 | 6 | 3.880.900 |'
- en: '| *4* | *5.000* |  |  | 1.005 | 2 | 448.900 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| *4* | *5.000* |  |  | 1.005 | 2 | 448.900 |'
- en: '| *5* | *4.000* |  |  | 835 | 0 | 2.788.900 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| *5* | *4.000* |  |  | 835 | 0 | 2.788.900 |'
- en: '| *6* | *6.500* |  |  | 415 | 0 | 688.900 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| *6* | *6.500* |  |  | 415 | 0 | 688.900 |'
- en: '| *7* | *7.000* |  |  | 1.995 | 2 | 1.768.900 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| *7* | *7.000* |  |  | 1.995 | 2 | 1.768.900 |'
- en: '| *8* | *9.000* |  |  | 8.325 | 6 | 11.088.900 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| *8* | *9.000* |  |  | 8.325 | 6 | 11.088.900 |'
- en: '| *9* | *9.000* |  |  | 11.655 | 12 | 11.088.900 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| *9* | *9.000* |  |  | 11.655 | 12 | 11.088.900 |'
- en: '| *10* | *10.000* |  |  | 19.485 | 20 | 18.748.900 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| *10* | *10.000* |  |  | 19.485 | 20 | 18.748.900 |'
- en: '| COUNT | 10 | **5,50** | **5.670,00** | **8.425,00** | **8,25** | **8.970.100,00**
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| COUNT | 10 | **5,50** | **5.670,00** | **8.425,00** | **8,25** | **8.970.100,00**
    |'
- en: Table 6.3 – Dataset to predict average salary based on the amount of work experience
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3 – 基于工作经验预测平均工资的数据集
- en: 'As you can see, there is an almost perfect linear relationship between *x*
    and *y*. As the amount of work experience increases, so does the salary. In addition
    to *x* and *y*, you need to compute the following statistics: the number of records,
    the mean of *x*, the mean of *y*, the covariance of *x* and *y*, the variance
    of *x*, and the variance of *y*. *Figure 6**.2 * depicts formulas that provide
    a mathematical representation of variance and covariance (respectively), where
    *x bar*, *y bar*, and *n* represent the mean of *x*, the mean of *y*, and the
    number of records, respectively:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Mathematical representation of variance and covariance respectively](img/B21197_06_02.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Mathematical representation of variance and covariance respectively
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: If you want to check the calculation details of the formulas for each of those
    auxiliary statistics in *Table 6.2*, please refer to the support material provided
    along with this book. There, you will find these formulas already implemented
    for you.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: These statistics are important because they will be used to compute the alpha
    and beta coefficients. *Figure 6**.3* explains how you can compute both coefficients,
    along with the correlation coefficients R and R squared. These last two metrics
    will give you an idea about the quality of the model, where the closer they are
    to 1, the better the model is.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Equations to calculate coefficients for simple linear regression](img/B21197_06_03.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Equations to calculate coefficients for simple linear regression
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'After applying these formulas, you will come up with the results shown in *Table
    6.4*. It already contains all the information that you need to make predictions,
    on top of the new data. If you replace the coefficients in the original equation,
    *y = ax + b + e*, you will find the regression formula to be as follows: *y =
    1021.212 * x +* *53.3*.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '| **Coefficient** | **Description** | **Value** |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| Alpha | Line inclination | 1,021,212,121 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| Beta | Interceptor | 53 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| R | Correlation | 0,979,364,354 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| R^2 | Determination | 0,959,154,538 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: Table 6.4 – Finding regression coefficients
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: From this point on, to make predictions, all you have to do is replace *x* with
    the number of years of experience. As a result, you will find *y*, which is the
    projected salary. You can see the model fit in *Figure 6**.4* and some model predictions
    in *Table 6.5*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Fitting data in the regression equation](img/B21197_06_04.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Fitting data in the regression equation
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '| **INPUT** | **PREDICTION** | **ERROR** |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.075 | 75 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2.096 | 596 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| 3 | 3.117 | - 583 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| 4 | 4.138 | - 862 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| 5 | 5.159 | 1.159 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| 6 | 6.181 | - 319 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| 7 | 7.202 | 202 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| 8 | 8.223 | - 777 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| 9 | 9.244 | 244 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| 10 | 10.265 | 265 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| 11 | 11.287 |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| 12 | 12.308 |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| 13 | 13.329 |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| 14 | 14.350 |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| 15 | 15.372 |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| 16 | 16.393 |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| 17 | 17.414 |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| 18 | 18.435 |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| 19 | 19.456 |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| 20 | 20.478 |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 20.478 |  |'
- en: Table 6.5 – Model predictions
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.5 – 模型预测
- en: While you are analyzing regression models, you should be able to know whether
    your model is of good quality or not. You read about many modeling issues (such
    as overfitting) in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*, Machine Learning
    Fundamentals*, and you already know that you always have to check model performance.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当你分析回归模型时，你应该能够知道你的模型是否质量良好。你在[*第一章*](B21197_01.xhtml#_idTextAnchor018)*机器学习基础*中了解到许多建模问题（例如过拟合），并且你已经知道你总是要检查模型性能。
- en: A good approach to regression models is performing what is called residual analysis.
    This is where you plot the errors of the model in a scatter plot and check whether
    they are randomly distributed (as expected) or not. If the errors are *not* randomly
    distributed, this means that your model was unable to generalize the data. *Figure
    6**.5* shows a residual analysis based on the data from *Table 6.5*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型的一个良好方法是进行所谓的残差分析。这就是你在散点图上绘制模型的误差，并检查它们是否随机分布（如预期的那样）或不随机分布。如果误差不是随机分布的，这意味着你的模型无法泛化数据。"图6.5**.5*"显示了基于"表6.5"数据的残差分析。
- en: '![Figure 6.5 – Residual analysis](img/B21197_06_05.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 残差分析](img/B21197_06_05.jpg)'
- en: Figure 6.5 – Residual analysis
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 残差分析
- en: The takeaway here is that the errors are randomly distributed. Such evidence,
    along with a high R squared rating, can be used as arguments to support the use
    of this model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的要点是误差是随机分布的。这样的证据，加上高R平方评分，可以用作支持使用此模型的论据。
- en: Important note
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In [*Chapter 7*](B21197_07.xhtml#_idTextAnchor970)*, Evaluating and Optimizing
    Models*, you will learn about evaluation metrics. For instance, you will learn
    that each type of model may have its own set of evaluation metrics. Regression
    models are commonly evaluated with **Mean Squared Error (MSE)** and **Root Mean
    Squared Error (RMSE)**. In other words, apart from R, R squared, and residual
    analysis, ideally, you will execute your model on test sets to extract other performance
    metrics. You can even use a cross-validation system to check model performance,
    as you learned in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*, Machine* *Learning
    Fundamentals*.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第七章*](B21197_07.xhtml#_idTextAnchor970)*评估和优化模型*中，你将了解评估指标。例如，你将了解到每种类型的模型可能都有自己的评估指标集。回归模型通常使用**均方误差（MSE）**和**均方根误差（RMSE）**进行评估。换句话说，除了R、R平方和残差分析之外，理想情况下，你将在测试集上执行你的模型以提取其他性能指标。你甚至可以使用交叉验证系统来检查模型性能，正如你在[*第一章*](B21197_01.xhtml#_idTextAnchor018)*机器学习基础*中学到的。
- en: Very often, when the model residuals *do* present a pattern and are *not* randomly
    distributed, it is because the existing relationship in the data is not linear,
    but non-linear, so another modeling technique must be applied. In the next subsection,
    you will learn how you can interpret regression models.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 非常常见的是，当模型残差确实呈现模式并且不是随机分布时，这是因为数据中现有的关系不是线性的，而是非线性的，因此必须应用另一种建模技术。在下一小节中，你将学习如何解释回归模型。
- en: Interpreting regression models
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释回归模型
- en: It is also good to know how to interpret a linear regression model. Sometimes,
    you use linear regression not necessarily to create a predictive model but to
    do a regression analysis. You can then use regression analysis to understand the
    relationship between the independent and dependent variables.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 了解如何解释线性回归模型也是很好的。有时，你使用线性回归不一定是为了创建预测模型，而是为了进行回归分析。然后你可以使用回归分析来理解自变量和因变量之间的关系。
- en: 'Looking back at the regression equation (*y = 1021.212 * x + 53.30*), you can
    see the two terms: alpha or slope (*1021.20*) and beta or *y* intercept (*53.3*).
    You can interpret this model as follows: *for each additional year of working
    experience, you will increase your salary by $1,021.20*. Also, note that when
    “years of experience” is equal to 0, the expected salary is going to be $53.30
    (this is the point where the straight line crosses the *y* axis).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾回归方程（*y = 1021.212 * x + 53.30*），你可以看到两个术语：alpha或斜率（*1021.20*）和beta或*y*截距（*53.3*）。你可以这样解释这个模型：*对于每增加一年工作经验，你的薪水将增加1,021.20美元*。此外，请注意，当“工作经验年数”等于0时，预期的薪水将是53.30美元（这是直线与*y*轴相交的点）。
- en: 'From a broad perspective, your regression analysis should answer the following
    question: for each extra unit that is added to the independent variable (slope),
    what is the average change in the dependent variable?'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Checking adjusted R squared
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, you have a much better idea of regression models! There is just
    one other very important topic that you should be aware of, regardless of whether
    it will come up in the exam or not, which is the parsimony aspect of your model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: You have already heard about parsimony in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*,
    Machine Learning Fundamentals*. This is the ability to prioritize simple models
    over complex ones. Looking into regression models, you might have to use more
    than one feature to predict your outcome. This is also known as a multiple regression
    model.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: When that is the case, the R and R squared coefficients tend to reward more
    complex models with more features. In other words, if you keep adding new features
    to a multiple regression model, you will come up with higher R and R squared coefficients.
    That is why you *cannot* anchor your decisions *only* based on those two metrics.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Another additional metric that you could use (apart from R, R squared, MSE,
    and RMSE) is known as **adjusted R squared**. This metric is penalized when you
    add extra features to the model that do not bring any real value. In *Table 6.6*,
    you can see when a model is starting to lose parsimony.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '| **Number** **of features** | **R squared** | **Adjusted** **R squared** |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| 1 | 81 | 79 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| 2 | 83 | 82 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| 3 | 88 | 87 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| 4 | 90 | 86 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| 5 | 92 | 85 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: Table 6.6 – Comparing R squared and adjusted R squared
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can conclude that maintaining three variables in the model is better
    than maintaining four or five. Adding four or five variables to the model will
    increase the R squared (as expected), but decrease the adjusted R squared.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have a very good understanding of regression models.
    Now, let us check what AWS offers in terms of built-in algorithms for this class
    of models.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Regression modeling on AWS
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS has a built-in algorithm known as **linear learner**, where you can implement
    linear regression models. The built-in linear learner uses **Stochastic Gradient
    Descent (SGD)** to train the model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: You will learn more about SGD when neural networks are discussed. For now, you
    can look at SGD as an alternative to the popular least squares error method that
    was just discussed.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The linear learner built-in algorithm provides a hyperparameter that can apply
    normalization to the data, prior to the training process. The name of this hyperparameter
    is `normalize_data`. This is very helpful since linear models are sensitive to
    the scale of the data and usually take advantage of data normalization.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Data normalization was discussed in [*Chapter 4*](B21197_04.xhtml#_idTextAnchor451)*,
    Data Preparation and Transformation*. Please review that chapter if you need to.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Some other important hyperparameters of the linear learner algorithm are **L1**
    and **wd**, which play the roles of **L1 regularization** and **L2** **regularization**,
    respectively.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: L1 and L2 regularization help the linear learner (or any other regression algorithm
    implementation) to avoid overfitting. Conventionally, regression models that implement
    L1 regularization are called **lasso regression** models, while regression models
    with L2 regularization are called **ridge** **regression** models.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it might sound complex, it is not! The regression model equation is
    still the same: *y = ax + b + e*. The change is in the loss function, which is
    used to find the coefficients that best minimize the error. If you look back at
    *Figure 6**.1*, you will see that the error function is defined as *e = (ŷ -
    y)^2*, where *ŷ* is the regression function value and *y* is the real value.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'L1 and L2 regularization add a penalty term to the loss function, as shown
    in the formulas in *Figure 6**.6* (note that you are replacing *ŷ* with *ax +*
    *b*):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – L1 and L2 regularization](img/B21197_06_06.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – L1 and L2 regularization
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: The λ (lambda) parameter must be greater than 0 and manually tuned. A very high
    lambda value may result in an underfitting issue, while a very low lambda may
    not result in expressive changes in the end results (if your model is overfitted,
    it will stay overfitted).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, the main difference between L1 and L2 regularization is
    that L1 will shrink the less important coefficients to 0, which will force the
    feature to be dropped (acting as a feature selector). In other words, if your
    model is overfitting because of the high number of features, L1 regularization
    should help you solve this problem.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: During your exam, remember the basis of L1 and L2 regularization, especially
    the key difference between them, where L1 works well as a feature selector.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Finally, many built-in algorithms can serve multiple modeling purposes. The
    linear learner algorithm can be used for regression, binary classification, and
    multi-classification. Make sure you remember this during your exam (it is *not
    just* about regression models).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: AWS has other built-in algorithms that work for regression and classification
    problems –that is**, factorization machines, KNN,** and the **XGBoost** algorithm.
    Since these algorithms can also be used for classification purposes, these will
    be covered in the section about classification algorithms.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ve just been given a very important tip to remember during the exam: linear
    learner, factorization machines, KNN, and XGBoost are suitable for both regression
    and classification problems. These algorithms are often known as algorithms for
    general purposes.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, you have reached the end of this section about regression models.
    Remember to check out the supporting material before you take the exam. You can
    also use the reference material when you are working on your daily activities!
    Now, let us move on to another classical example of a machine learning problem:
    classification models.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Working with classification models
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have been learning what classification models are throughout this book.
    However, now, you are going to look at some algorithms that are suitable for classification
    problems. Keep in mind that there are hundreds of classification algorithms out
    there, but since you are preparing for the AWS Certified Machine Learning Specialty
    exam, the ones that have been pre-built by AWS will be covered.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: You will start with **factorization machines**. Factorization machines is considered
    an extension of the linear learner algorithm, optimized to find the relationship
    between features within high-dimensional sparse datasets.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: A very traditional use case for factorization machines is *recommendation systems*,
    where you usually have a high level of sparsity in the data. During the exam,
    if you are faced with a general-purpose problem (either a regression or binary
    classification task) where the underlying datasets are sparse, then factorization
    machines is probably the best answer from an algorithm perspective.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: When you use factorization machines in a regression model, the RMSE will be
    used to evaluate the model. On the other hand, in the binary classification mode,
    the algorithm will use log loss, accuracy, and F1 score to evaluate results. A
    deeper discussion about evaluation metrics will be provided in [*Chapter 7*](B21197_07.xhtml#_idTextAnchor970)*,
    Evaluating and* *Optimizing Models*.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: You should be aware that factorization machines only accepts input data in the
    `text/.csv` format.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'The next built-in algorithm suitable for classification problems is known as
    K-nearest neighbors, or KNN for short. As the name suggests, this algorithm will
    try to find the *K* closest points to the input data and return either of the
    following predictions:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The most repeated class of the *K* closest points, if it is a classification
    task
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average value of the label of the *K* closest points, if it is a regression
    task
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN is an **index-based algorithm** because it computes distances between points,
    assigns indexes for these points, and then stores the sorted distances and their
    indexes. With that type of data structure, KNN can easily select the top *K* closest
    points to make the final prediction. Note that *K* is a hyperparameter of KNN
    and should be optimized during the modeling process.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: The other AWS built-in algorithm available for general purposes, including classification,
    is known as **eXtreme Gradient Boosting**, or **XGBoost** for short. This is an
    ensemble, decision tree-based model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost uses a set of **weaker** models (decision trees) to predict the target
    variable, which can be a regression task, binary class, or multi-class. This is
    a very popular algorithm and has been used in machine learning competitions by
    the top performers.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost uses a boosting learning strategy, in which one model tries to correct
    the error of the prior model. It carries the name “gradient” because it uses the
    gradient descent algorithm to minimize the loss when adding new trees.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: The term *weaker* is used in this context to describe very simple decision trees.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Although XGBoost is much more robust than a single decision tree, it is important
    to go into the exam with a clear understanding of what decision trees are and
    their main configurations. By the way, they are the base model of many ensemble
    algorithms, such as AdaBoost, Random Forest, gradient boost, and XGBoost.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are rule-based algorithms that organize decisions in the form
    of a tree, as shown in *Figure 6**.7*.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Example of what a decision tree model looks like](img/B21197_06_07.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Example of what a decision tree model looks like
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: They are formed by a root node (at the very top of the tree), intermediary or
    decision nodes (in the middle of the tree), and leaf nodes (bottom nodes with
    no splits). The depth of the tree is given by the difference between the root
    node and the very last leaf node. For example, in *Figure 6**.7*, the depth of
    the tree is 3.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: The depth of the tree is one of the most important hyperparameters of this type
    of model and it is often known as the **max depth**. In other words, the max depth
    controls the maximum depth that a decision tree can reach.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Another very important hyperparameter of decision tree models is the minimum
    number of samples/observations in the leaf nodes. It is also used to control the
    growth of the tree.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees have many other types of hyperparameters, but these two are especially
    important for controlling how the model overfits. Decision trees with a high depth
    or a very small number of observations in the leaf nodes are likely to face issues
    during extrapolation/prediction.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason for this is simple: decision trees use data from the leaf nodes
    to make predictions, based on the proportion (for classification tasks) or average
    value (for regression tasks) of each observation/target variable that belongs
    to that node. Thus, the node should have enough data to make good predictions
    outside the training set.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter the term **CART** during the exam, you should know that it
    stands for **Classification and Regression Trees**, since decision trees can be
    used for classification and regression tasks.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: To select the best variables to split the data in the tree, the model will choose
    the ones that maximize the separation of the target variables across the nodes.
    This task can be performed by different methods, such as **Gini** and **information
    gain**.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting models
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Time series refers to data points that are collected on a regular basis with
    a sequence dependency. Time series have a measure, a fact, and a time unit, as
    shown in *Figure 6**.8*.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Time series statement](img/B21197_06_08.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Time series statement
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, time series can be classified as **univariate** or **multivariate**.
    A univariate time series contains just one variable connected across a period
    of time, while a multivariate time series contains two or more variables connected
    across a period. *Figure 6**.9* shows the univariate time series.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Time series example](img/B21197_06_09.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Time series example
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Time series can be decomposed as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '**Observed** or **level**: The average values of the series'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trend**: Increasing, decreasing pattern (sometimes, there is no trend)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seasonality**: Regular peaks at specific periods of time (sometimes, there
    is no seasonality)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise**: Something that cannot be explained'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, you can also find isolated peaks in the series that cannot be captured
    in a forecasting model. In such cases, you might want to consider those peaks
    as outliers. *Figure 6**.10* is a decomposition of the time series shown in *Figure
    6**.9*.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Time series decomposition](img/B21197_06_10.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Time series decomposition
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth highlighting that you can use **additive** or **multiplicative**
    approaches to decompose time series. Additive models suggest that your time series
    *adds* each component to explain the target variable – that is, *y(t) = level
    + trend + seasonality +* *noise*.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Multiplicative models, on the other hand, suggest that your time series *multiplies*
    each component to explain the target variable – that is, *y(t) = level * trend
    * seasonality ** *noise*.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will take a closer look at time series components.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Checking the stationarity of time series
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decomposing time series and understanding how their components interact with
    additive and multiplicative models is a great achievement! However, the more you
    learn, the more you want to go deeper into the problem. Maybe you have realized
    that time series without trend and seasonality are easier to predict than the
    ones with all those components!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: That is naturally right. If you do not have to understand trend and seasonality,
    and if you do not have control over the noise, all you have to do is explore the
    observed values and find their regression relationship.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: A time series with constant mean and variance across a time period is known
    as **stationary**. In general, time series *with* trend and seasonality are *not*
    stationary. It is possible to apply data transformations to the series to transform
    it into a stationary time series so that the modeling task tends to be easier.
    This type of transformation is known as **differentiation**.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: While you are exploring a time series, you can check stationarity by applying
    hypothesis tests, such as **Dickey-Fuller**, **KPSS**, and **Phillips-Perron**,
    just to mention a few. If you find it non-stationary, then you can apply differentiation
    to make it a stationary time series. Some algorithms already have that capability
    embedded.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Exploring, exploring, and exploring
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, it is important to remember that exploration tasks happen all
    the time in data science. Nothing is different here. While you are building time
    series models, you might want to take a look at the data and check whether it
    is suitable for this type of modeling.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '**Autocorrelation plots** are one of the tools that you can use for time series
    analysis. Autocorrelation plots allow you to check the correlations between lags
    in the time series. *Figure 6**.11* shows an example of this type of visualization.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Autocorrelation plot](img/B21197_06_11.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Autocorrelation plot
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Remember, if you are playing with univariate time series, your time series just
    contains one variable. Therefore, finding autocorrelation across the lags of your
    unique variable is crucial to understanding whether you can build a good model
    or not.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: And yes, it turns out that, sometimes, it might happen that you do not have
    a time series in front of you. Furthermore, no matter your efforts, you will not
    be able to model this data as a time series. This type of data is often known
    as **white** **noise**.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Another type of series that you cannot predict is known as a **random** **walk**.
    Random walks are random by nature, but they have a dependency on the previous
    time step. For example, the next point of a random walk could be a random number
    between 0 and 1, and also the last point of the series.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Be careful if you come across those terms in the exam and remember to relate
    them to randomness in time series.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: With that, you have covered the main theory behind time series modeling. You
    should also be aware that the most popular algorithms out there for working with
    time series are known as **Auto-Regressive Integrated Moving Average (ARIMA)**
    and **Exponential Smoothing (ETS)**. This book will not go into the details of
    these two models. Instead, you will see what AWS can offer in terms of time series
    modeling.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Understanding DeepAR
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **DeepAR** forecasting algorithm is a built-in SageMaker algorithm that
    is used to forecast a one-dimensional time series using a **Recurrent Neural**
    **Network (RNN)**.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Traditional time series algorithms, such as ARIMA and ETS, are designed to fit
    one model per time series. For example, if you want to forecast sales per region,
    you might have to create one model per region, since each region may have its
    own sales behaviors. DeepAR, on the other hand, allows you to operate more than
    one time series in a single model, which seems to be a huge advantage for more
    complex use cases.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'The input data for DeepAR, as expected, is *one or more* time series. Each
    of these time series can be associated with the following:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: A vector of static (time-independent) categorical features, controlled by the
    `cat` field
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A vector of dynamic (time-dependent) time series, controlled by `dynamic_feat`
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Note that the ability to train and make predictions on top of multiple time
    series is strictly related to the vector of static categorical features. While
    defining the time series that DeepAR will train on, you can set categorical variables
    to specify which group each time series belongs to.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Two of the main hyperparameters of DeepAR are `context_length`, which is used
    to control how far in the past the model can see during the training process,
    and `prediction_length`, which is used to control how far in the future the model
    will output predictions.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: DeepAR can also handle missing values, which, in this case, refers to existing
    gaps in the time series. A very interesting functionality of DeepAR is its ability
    to create derived features from time series. These derived features, which are
    created from basic time frequencies, help the algorithm learn time-dependent patterns.
    *Table 6.7* shows all the derived features created by DeepAR, according to each
    type of time series that it is trained on.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '| **Frequency of the** **time series** | **Derived feature** |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| Minute | Minute of hour, hour of day, day of week, day of month, day of year
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| Hour | Hour of day, day of week, day of month, day of year |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| Day | Day of week, day of month, day of year |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| Week | Day of month, week of year |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| Month | Month of year |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: Table 6.7 – DeepAR derived features per frequency of time series
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: You have now completed this section about forecasting models. Next, you will
    take a look at the last algorithm regarding supervised learning – that is, the
    **Object2Vec** algorithm.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Object2Vec
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object2Vec is a built-in SageMaker algorithm that generalizes the well-known
    **Word2Vec** algorithm. Object2Vec is used to create **embedding spaces** for
    high dimensional objects. These embedding spaces are, per definition, compressed
    representations of the original object and can be used for multiple purposes,
    such as feature engineering or object comparison.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – A visual example of an embedding space](img/B21197_06_12.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – A visual example of an embedding space
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.12* illustrates what is meant by an embedding space. The first
    and last layers of the neural network model just map the input data with itself
    (represented by the same vector size).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: As you move on to the internal layers of the model, the data is compressed more
    and more until it hits the layer in the middle of this architecture, known as
    the embedding layer. On that particular layer, you have a smaller vector, which
    aims to be an accurate and compressed representation of the high-dimensional original
    vector from the first layer.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: With this, you just completed the first section about machine learning algorithms
    in AWS. Coming up next, you will take a look at some unsupervised algorithms.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AWS provides several unsupervised learning algorithms for the following tasks:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering: K-Means algorithm'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dimension reduction: **Principal Component** **Analysis (PCA)**'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pattern recognition: IP Insights'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anomaly detection: The **Random Cut Forest (****RCF)** algorithm'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us start by talking about clustering and how the most popular clustering
    algorithm works: K-Means.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering algorithms are very popular in data science. Basically, they aim
    to identify similar groups in a given dataset, also known as *clusters*. Clustering
    algorithms belong to the field of non-supervised learning, which means that they
    do not need a label or response variable to be trained.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: This is just fantastic since labeled data is very scarce! However, it comes
    with some limitations. The main one is that clustering algorithms provide clusters
    for you, but not the meaning of each cluster. Thus, someone, as a subject matter
    expert, has to analyze the properties of each cluster to define their meanings.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: There are many types of clustering approaches, such as hierarchical clustering
    and partitional clustering. Inside each approach, you will find several algorithms.
    However, K-Means is probably the most popular clustering algorithm, and you are
    likely to come across it in your exam.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: When you are playing with K-Means, somehow, you have to specify the number of
    clusters that you want to create. Then, you have to allocate the data points across
    each cluster, so that each data point will belong to a single cluster. This is
    exactly what you should expect as a result at the end of the clustering process!
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: You need to specify the number of clusters that you want to create and pass
    this number to the K-Means algorithm. Then, the algorithm will randomly initiate
    the central point of each cluster (this is known as **centroid initialization**).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Once you have the centroids of each cluster, all you need to do is assign a
    cluster to each data point. To do that, you have to use a proximity or distance
    metric! This book will use the term *distance metric*.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: The **distance metric** is responsible for calculating the distance between
    data points and centroids. The data point will belong to the closer cluster centroid,
    according to the distance metric.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular distance metric is called **Euclidean distance** and the math
    behind it is simple; imagine that the points of your dataset are composed of two
    dimensions, *x* and *y*. So, you could consider points *a* and *b* as follows:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '*a (**x=1, y=1)*'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b (**x=2, y=5)*'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Euclidean distance between points *a* and *b* is given by the following
    formula, where *x*1 and *y*1 refer to the values of point *a*, and *x*2 and *y*2
    refer to the values of point *b*: ![](img/B21197_06_12a.png). The same function
    can be generalized by the following equation: ![](img/B21197_06_12b.png). Once
    you have completed this process and assigned a cluster for each data point, you
    ![](img/B21197_06_12c.png) methods, such as **single link, average link**, and
    **complete link**.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Due to this centroid refreshment, you will have to keep checking the closest
    cluster for each data point and keep refreshing the centroids, iteratively, until
    the cluster centroids converge and no cluster reassignment is needed, or the maximum
    number of allowed iterations is reached.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'Alright, the following is a summarization of the components and steps that
    compose the K-Means method:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Centroid initialization, cluster assignment, centroid refreshment, and then
    redo the last two steps until it converges
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A distance metric to assign data points to each cluster (in this case, Euclidian
    distance)
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A linkage method to recalculate the cluster centroids (for the sake of our demonstration,
    you will learn about the average linkage)
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these definitions, you are now ready to walk through the following real
    example, step by step (some support material is also available for your reference).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Computing K-Means step by step
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this example, you will simulate K-Means in a very small dataset, with only
    two columns (*x* and *y*) and six data points (*A*, *B*, *C*, *D*, *E*, and *F*),
    as defined in *Table 6.8*.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '| **Point** | **x** | **y** |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| A | 1 | 1 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: '| B | 2 | 2 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '| C | 5 | 5 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '| D | 5 | 6 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: '| E | 1 | 5 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: '| F | 2 | 6 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '| **Cluster 1** | **1** | **1** |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '| **Cluster 2** | **2** | **2** |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
- en: '| **Cluster 3** | **5** | **5** |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
- en: Table 6.8 – Iteration input data for K-Means
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 6.8* contains three clusters with the following centroids: *(1,1), (2,2),
    (5,5).* The number of clusters (3) was defined *a priori* and the centroid for
    each cluster was randomly defined. *Figure 6**.13* shows the stage of the algorithm
    that you are at right now.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Plotting the K-Means results before completing the first iteration](img/B21197_06_13.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Plotting the K-Means results before completing the first iteration
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can’t see points *A*, *B*, and *C* since they overlap with cluster
    centroids, but don’t worry – they will appear soon. Next, you have to compute
    the distance of each data point to each cluster centroid, and then, you need to
    choose the cluster that is the closest to each point.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '| **xc1** | **yc1** | **xc2** | **yc2** | **xc3** | **yc3** | **distance-c1**
    | **distance-c2** | **distance-c3** | **Cluster** |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 0,0 | 1,4 | 5,7 | Cluster 1 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 1,4 | 0,0 | 4,2 | Cluster 2 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 5,7 | 4,2 | 0,0 | Cluster 3 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 6,4 | 5,0 | 1,0 | Cluster 3 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 4,0 | 3,2 | 4,0 | Cluster 2 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 5,1 | 4,0 | 3,2 | Cluster 3 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| Legendxc1 = x value of cluster 1yc1 = y value of cluster 1 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: Table 6.9 – Processing iteration 1
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 6.9* contains the following elements:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Each row represents a data point.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first six columns represent the centroid axis (*x* and *y*) of each cluster.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next three columns represent the distance of each data point to each cluster
    centroid.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last column represents the clusters that are the closest to each data point.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at data point *A* (first row), you can see that it was assigned to cluster
    1 because the distance from data point *A* to cluster 1 is 0 (do you remember
    that they were overlapping?). The same calculation happens to all other data points
    to define a cluster for each data point.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Before you move on, you might want to see how those Euclidian distances between
    the clusters and the data points were computed. For demonstration purposes, the
    following simulation will consider the distance from data point *A* to cluster
    3 (the first row in *Table 6.9*, column `distance-c3`, value *5,7*).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, the following formula was used to calculate the Euclidian distance:
    ![](img/B21197_06_13a.png)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you have the following:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '*x*1 = *x* of data point *A* = 1'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*1 = *y* of data point *A* = 1'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*2 = *x* of cluster 3 = 5'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*2 = *y* of cluster 3 = 5'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 6**.14* applies the formula, step by step.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Computing the Euclidian distance step by step](img/B21197_06_14.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Computing the Euclidian distance step by step
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'That is just fantastic, isn’t it? You have almost completed the first iteration
    of K-Means. In the very last step of iteration 1, you have to refresh the cluster
    centroids. Remember: initially, they were randomly defined, but now, you have
    just assigned some data points to each cluster, which means you should be able
    to identify where the central point of the cluster is.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the **linkage** method will be used to refresh the cluster
    centroids. This is a very simple step, and the results are presented in *Table
    6.10*.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '| **Point** | **x** | **y** |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| A | 1 | 1 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| B | 2 | 2 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| C | 5 | 5 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| D | 5 | 6 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| E | 1 | 5 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| F | 2 | 6 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| **Cluster 1** | **1** | **1** |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| **Cluster 2** | **1,5** | **3,5** |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| **Cluster 3** | **4** | **5,7** |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: Table 6.10 – K-Means results after iteration 1
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 6.10* shows the same data points (*A* to *F*) that you are dealing with
    (by the way, they will never change), and the centroids of clusters 1, 2, and
    3\. Those centroids are quite different from what they were initially, as shown
    in *Table 6.8*. This is because they were refreshed using average linkage! The
    method got the average value of all the *x* and *y* values of the data points
    of each cluster. In the next simulation, have a look at how *(1.5, 3.5)* were
    obtained as centroids of cluster 2.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at *Table 6.9*, you will see that cluster 2 only has two data points
    assigned to it: *B* and *E*. These are the second and fifth rows in that figure.
    If you take the average values of the *x* axis of each point, then you will have
    *(2 + 1) / 2 = 1.5* and *(2 + 5) / 2 =* *3.5*.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: With that, you are done with iteration 1 of K-Means and you can view the results
    in *Figure 6**.15*.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Plotting the K-Means results after the first iteration](img/B21197_06_15.jpg)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Plotting the K-Means results after the first iteration
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you can see almost all the data points, except for data point A because
    it is still overlapping with the centroid of cluster 1\. Moving on, you have to
    redo the following steps:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Recalculate the distance between each data point and each cluster centroid and
    reassign clusters, if needed.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recalculate the cluster centroids.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You do those two tasks many times until the cluster centroids converge and they
    don’t change anymore, *or* you reach the maximum number of allowed iterations,
    which can be set as a hyperparameter of K-Means. For demonstration purposes, after
    four iterations, your clusters will look like *Figure 6**.16*.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Plotting the K-Means results after the fourth iteration](img/B21197_06_16.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Plotting the K-Means results after the fourth iteration
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: On the fourth iteration, all the cluster centroids look pretty consistent, and
    you can clearly see that all data points could be grouped according to their proximity.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: In this example, you have only set two dimensions for each data point (dimensions
    *x* and *y*). In real use cases, you can see far more dimensions, and that is
    why clustering algorithms play a very important role in identifying groups in
    the data in a more automated fashion.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, you have enjoyed how to compute K-Means from scratch! This knowledge
    will be beneficial for the exam and for your career as a data scientist. By the
    way, as advised many times, data scientists must be skeptical and curious, so
    you might be wondering why three clusters were defined in this example and not
    two or four. You may also be wondering how you measure the quality of the clusters.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: You didn’t think this explanation wouldn’t be provided, did you?
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Defining the number of clusters and measuring cluster quality
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although K-Means is a great algorithm for finding patterns in your data, it
    will not provide the meaning of each cluster, nor the number of clusters you have
    to create to maximize cluster quality.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: In clustering, cluster quality means that you want to create groups with a high
    homogeneity among the elements of the same cluster, and a high heterogeneity among
    the elements of different clusters. In other words, the elements of the same clusters
    should be close/similar, whereas the elements of different clusters should be
    well separated.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to compute the cluster’s homogeneity is by using a metric known as
    the **sum of square errors**, or **SSE** for short. This metric will compute the
    sum of squared differences between each data point and its cluster centroid. For
    example, when all the data points are located at the same point where the cluster
    centroid is, then the SSE will be 0\. In other words, you want to minimize the
    SSE. The following equation formally defines the SSE: ![](img/B21197_06_16a.png)'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to check the cluster quality, it is easier to understand
    how to define the number of appropriate clusters for a given dataset. All you
    have to do is find the optimal number of clusters to minimize the SSE. A very
    popular method that works around that logic is known as the **elbow method**.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: The elbow method proposes executing the clustering algorithm many times. In
    each execution, you will test a different number of clusters, *k*. After each
    execution, you compute the SSE related to that *k* number of clusters. Finally,
    you can plot these results and select the number of *k* where the SSE stops to
    drastically decrease.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Adding more clusters will naturally decrease the SSE. In the elbow method, you
    want to find the point where that change becomes smoother, which means that the
    addition of new clusters will not bring too much value.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, three clusters were created. *Figure 6**.17* shows
    the elbow analysis that supports this decision.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – The elbow method](img/B21197_06_17.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – The elbow method
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: You can conclude that adding more than three or four clusters will add unnecessary
    complexity to the clustering process.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you should always consider the business background while defining
    the number of clusters. For example, if you are creating a customer segmentation
    model and your company has prepared the commercial team and business processes
    to support four segments of customers, there is no harm in setting up four clusters
    instead of three.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you should know that AWS has implemented K-Means as part of its list
    of built-in algorithms. In other words, you don’t have to use external libraries
    or bring your own algorithm to play with K-Means on AWS.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'That was a really good accomplishment: you just mastered the basics of clustering
    algorithms and you should now be able to drive your own projects and research
    about this topic! For the exam, remember that clustering belongs to the unsupervised
    field of machine learning, so there is no need to have labeled data.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: Also, make sure that you know how the most popular algorithm of this field works
    – that is, K-Means. Although clustering algorithms do not provide the meaning
    of each group, they are very powerful for finding patterns in the data, either
    to model a particular problem or just to explore the data.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: Coming up next, you will keep studying unsupervised algorithms and see how AWS
    has built one of the most powerful algorithms out there for anomaly detection,
    known as **RCF**.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finding anomalies in data is a very common task in modeling and data exploratory
    analysis. Sometimes, you might want to find anomalies in the data just to remove
    them before fitting a regression model, while other times, you might want to create
    a model that identifies anomalies as an end goal – for example, in fraud detection
    systems.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, you can use many different methods to find anomalies in the data. With
    some creativity, the possibilities are endless. However, there is a particular
    algorithm that works around this problem that you should definitely be aware of
    for your exam: RCF.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: RCF is an unsupervised decision tree-based algorithm that creates multiple decision
    trees (forests) using random subsamples of the training data. Technically, it
    randomizes the data and then creates samples according to the number of trees.
    Finally, these samples are distributed across each tree.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: These sets of trees are used to assign an anomaly score to the data points.
    To calculate the anomaly score for a particular data point, it is passed down
    each tree in the forest. As the data point moves through the tree, the path length
    from the root node to the leaf node is recorded for that specific tree. The anomaly
    score for that data point is then determined by considering the distribution of
    path lengths across all the trees in the forest.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: If a data point follows a short path in most trees (i.e., it is close to the
    root node), it is considered a common point and will have a lower anomaly score.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if a data point follows a long path in many trees (i.e.,
    it is far from the root node), it is considered an uncommon point and will have
    a higher anomaly score.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: The most important hyperparameters of RCF are `num_trees` and `num_samples_per_tree`,
    which are the number of trees in the forest and the number of samples per tree,
    respectively.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another unsupervised algorithm that was implemented by AWS in its list of built-in
    algorithms is known as principal component analysis, or PCA for short. PCA is
    a technique that’s used to reduce the number of variables/dimensions in a dataset.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: The main idea behind PCA is plotting the data points to another set of coordinates,
    known as **Principal Components (PCs)**, which aims to explain the most variance
    in the data. By definition, the first component will capture more variance than
    the second component, then the second component will capture more variance than
    the third one, and so on.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: 'You can set up as many PCs as you need, as long as it does not surpass the
    number of variables in your dataset. *Figure 6**.18* shows how these PCs are drawn:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Finding PCs in PCA](img/B21197_06_18.jpg)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Finding PCs in PCA
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, the first PC will be drawn in such a way that it will
    capture most of the variance in the data. That is why it passes near the majority
    of the data points in *Figure 6**.18*.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Then, the second PC will be perpendicular to the first one, so that it will
    be the second component that explains the variance in the data. If you want to
    create more components (consequentially, capturing more variance), you just have
    to follow the same rule of adding perpendicular components. **Eigenvectors** and
    **eigenvalues** are the linear algebra concepts associated with PCA that compute
    the PCs.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: So, what is the story with dimension reduction here? In case it is not clear
    yet, these PCs can be used to replace your original variables. For example, consider
    you have 10 variables in your dataset, and you want to reduce this dataset to
    three variables that best represent the others. A potential solution for that
    would be applying PCA and extracting the first three PCs!
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: Do these three components explain 100% of your dataset? Probably not, but ideally,
    they will explain most of the variance. Adding more PCs will explain more variance
    but at the cost of adding extra dimensions.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: Using AWS’s built-in algorithm for PCA
  id: totrans-441
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In AWS, PCA works in two different modes:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '**Regular**: For datasets with a moderate number of observations and features'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Randomized**: For datasets with a large number of observations and features'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference is that, in randomized mode, it is used as an approximation algorithm.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the main hyperparameter of PCA is the number of components that you
    want to extract, known as `num_components`.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: IP Insights
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IP Insights is an unsupervised algorithm that is used for pattern recognition.
    Essentially, it learns the usage pattern of IPv4 addresses.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'The *modus operandi* of this algorithm is very intuitive: it is trained on
    top of pairs of events in the format of entity and IPv4 address so that it can
    understand the pattern of each entity that it was trained on.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: For instance, you can understand “entity” as user IDs or account numbers.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: Then, to make predictions, it receives a pair of events with the same data structure
    (entity, IPv4 address) and returns an anomaly score for that particular IP address,
    according to the input entity.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: This anomaly score that is returned by IP Insights infers how anomalous the
    pattern of the event is.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: You might come across many applications with IP Insights. For example, you can
    create an IP Insights model that was trained on top of your application login
    events (this is your entity). You should be able to expose this model through
    an API endpoint to make predictions in real time.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: Then, during the authentication process of your application, you could call
    your endpoint and pass the IP address that is trying to log in. If you got a high
    score (meaning this pattern of logging in looks anomalous), you can request extra
    information before authorizing access (even if the password was right).
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: This is just one of the many applications of IP Insights you could think about.
    Next, you will learn about textual analysis.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Textual analysis
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern applications use **Natural Language Processing (NLP)** for several purposes,
    such as text translation, document classifications, web search, **Named Entity
    Recognition (NER)**, and many others.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: AWS offers a suite of algorithms for most NLP use cases. In the next few subsections,
    you will have a look at these built-in algorithms for textual analysis.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: BlazingText algorithm
  id: totrans-461
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'BlazingText does two different types of tasks: text classification, which is
    a supervised learning approach that extends the **fastText** text classifier,
    and Word2Vec, which is an unsupervised learning algorithm.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: BlazingText’s implementations of these two algorithms are optimized to run on
    large datasets. For example, you can train a model on top of billions of words
    in a few minutes.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: 'This scalability aspect of BlazingText is possible due to the following:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: Its ability to use multi-core CPUs and a single GPU to accelerate text classification
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its ability to use multi-core CPUs or GPUs, with custom CUDA kernels for GPU
    acceleration, when playing with the Word2Vec algorithm
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Word2Vec option supports **batch_skipgram** mode, which allows BlazingText
    to do distributed training across multiple CPUs.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: The distributed training that’s performed by BlazingText uses a mini-batching
    approach to convert **level-1 BLAS (Basic Linear Algebra Subprograms)** operations
    into **level-3 BLAS** operations. If you see these terms during your exam, you
    should know that they are related to BlazingText (Word2Vec mode).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: Still in Word2Vec mode, BlazingText supports both the **skip-gram** and **Continuous
    Bag of Words (****CBOW)** architectures.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, note the following configurations of BlazingText, since they are likely
    to be present in your exam:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: In Word2Vec mode, only the train channel is available.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BlazingText expects a single text file with space-separated tokens. Each line
    of the file must contain a single sentence. This means you usually have to preprocess
    your corpus of data before using BlazingText.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence-to-sequence algorithm
  id: totrans-474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a supervised algorithm that transforms an input sequence into an output
    sequence. This sequence can be a text sentence or even an audio recording.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: The most common use cases for sequence-to-sequence are machine translation,
    text summarization, and speech-to-text. Anything that you think is a sequence-to-sequence
    problem can be approached by this algorithm.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: 'Technically, AWS SageMaker’s Seq2Seq uses two types of neural networks to create
    models: an **RNN** and a **Convolutional Neural Network (CNN)** with an attention
    mechanism.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '**Latent Dirichlet allocation**, or **LDA** for short, is used for topic modeling.
    Topic modeling is a textual analysis technique where you can extract a set of
    topics from a corpus of text data. LDA learns these topics based on the probability
    distribution of the words in the corpus of text.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: Since this is an unsupervised algorithm, there is no need to set a target variable.
    Also, the number of topics must be specified up-front, and you will have to analyze
    each topic to find its domain meaning.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: Neural Topic Model algorithm
  id: totrans-480
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like the LDA algorithm, the **Neural Topic Model (NTM)** also aims to extract
    topics from a corpus of data. However, the difference between LDA and NTM is their
    learning logic. While LDA learns from probability distributions of the words in
    the documents, NTM is built on top of neural networks.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: The NTM network architecture has a bottleneck layer, which creates an embedding
    representation of the documents. This bottleneck layer contains all the necessary
    information to predict document composition, and its coefficients can be considered
    topics.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: With that, you have completed this section on textual analysis. In the next
    section, you will learn about image processing algorithms.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: Image processing
  id: totrans-484
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Image processing is a very popular topic in machine learning. The idea is pretty
    self-explanatory: creating models that can analyze images and make inferences
    on top of them. By inference, you can understand this as detecting objects in
    an image, classifying images, and so on.'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: AWS offers a set of built-in algorithms you can use to train image processing
    models. In the next few sections, you will have a look at those algorithms.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: Image classification algorithm
  id: totrans-487
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, the image classification algorithm is used to classify
    images using supervised learning. In other words, it needs a label within each
    image. It supports multi-label classification.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: 'The way it operates is simple: during training, it receives an image and its
    associated labels. During inference, it receives an image and returns all the
    predicted labels. The image classification algorithm uses a CNN (**ResNet**) for
    training. It can either train the model from scratch or take advantage of transfer
    learning to pre-load the first few layers of the neural network.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: According to AWS’s documentation, the `.jpg` and `.png` file formats are supported,
    but the recommended format is **MXNet’s RecordIO**.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation algorithm
  id: totrans-491
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The semantic segmentation algorithm provides a pixel-level capability for creating
    computer vision applications. It tags each pixel of the image with a class, which
    is an important feature for complex applications such as self-driving and medical
    image diagnostics.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of its implementation, the semantic segmentation algorithm uses the
    **MXNet Gluon framework** and the **Gluon CV toolkit**. You can choose any of
    the following algorithms to train a model:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully Convolutional** **Network (FCN)**'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pyramid Scene** **Parsing (PSP)**'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepLabV3
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these options work as an **encoder-decoder** neural network architecture.
    The output of the network is known as a **segmentation mask**.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: Object detection algorithm
  id: totrans-498
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just like the image classification algorithm, the main goal of the object detection
    algorithm is also self-explanatory: it detects and classifies objects in images.
    It uses a supervised approach to train a deep neural network.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: 'During the inference process, this algorithm returns the identified objects
    and a score of confidence regarding the prediction. The object detection algorithm
    uses **Single Shot MultiBox Detector (SSD)** and supports two types of network
    architecture: **Visual Geometry Group (VGG)** and **Residual** **Network (ResNet).**'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-501
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'That was such a journey! Take a moment to recap what you have just learned.
    This chapter had four main topics: supervised learning, unsupervised learning,
    textual analysis, and image processing. Everything that you have learned fits
    into those subfields of machine learning.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of supervised learning algorithms that you have studied includes the
    following:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: Linear learner
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factorization machines
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object2Vec
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepAR forecasting
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that you can use linear learner, factorization machines, XGBoost, and
    KNN for multiple purposes, including solving regression and classification problems.
    Linear learner is probably the simplest algorithm out of these four; factorization
    machines extends linear earner and is good for sparse datasets, XGBoost uses an
    ensemble method based on decision trees, and KNN is an index-based algorithm.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: The other two algorithms, Object2Vec and DeepAR, are used for specific purposes.
    Object2Vec is used to create vector representations of the data, while DeepAR
    is used to create forecast models.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of unsupervised learning algorithms that you have studied includes
    the following:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: K-Means
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP Insights
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RCF
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Means is a very popular algorithm that is used for clustering. PCA is used
    for dimensionality reduction, IP Insights is used for pattern recognition, and
    RCF is used for anomaly detection.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: You then looked at regression models and K-Means in more detail. You did this
    because, as a data scientist, you should at least master these two very popular
    algorithms so that you can go deeper into other algorithms by yourself.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you moved on to the second half of this chapter, where you learned about
    textual analysis and the following algorithms:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: BlazingText
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence-to-sequence
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDA
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NTM
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, you learned about image processing and looked at the following:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: Image classification algorithm
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic segmentation algorithm
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection algorithm
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the topics covered in this chapter are very important with regard to the
    AWS Certified Machine Learning Specialty exam, you are highly encouraged to jump
    into the AWS website and search for machine learning algorithms. There, you will
    find the most recent information about the algorithms that you have just learned
    about. Please make sure you do it before taking the exam.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: That brings you to the end of this quick refresher and the end of this chapter.
    In the next chapter, you will learn about the existing mechanisms provided by
    AWS that you can use to optimize and evaluate these algorithms.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: Exam Readiness Drill – Chapter Review Questions
  id: totrans-530
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: How To Access These Resources
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: Click the link – [https://packt.link/MLSC01E2_CH06](https://packt.link/MLSC01E2_CH06).
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, you can scan the following **QR code** (*Figure 6**.19*):'
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.19 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_06_19.jpg)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – QR code that opens Chapter Review Questions for logged-in users
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 6**.20*:'
  id: totrans-540
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Chapter Review Questions for Chapter 6](img/B21197_06_20.jpg)'
  id: totrans-541
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Chapter Review Questions for Chapter 6
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exam Readiness Drill
  id: totrans-544
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first three attempts, don’t worry about the time limit.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 1
  id: totrans-546
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 2
  id: totrans-548
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 3
  id: totrans-550
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: Working On Timing
  id: totrans-554
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attempt** | **Score** | **Time Taken** |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
- en: Table 6.11 – Sample timing practice drills on the online platform
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
