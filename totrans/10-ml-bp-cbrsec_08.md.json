["```py\ndef read_dataset(num_authors = 99):\n  X = []\n  y = []\n  data_root_dir = \"../data/corpora/amt/\"\n  authors_to_ignore = []\n  authorCount = 0\n  for author_name in os.listdir(data_root_dir):\n      # Check if the maximum number of authors has been parsed\n      if authorCount > self.numAuthors:\n         break\n      if author_name not in authors_to_ignore:\n         label = author_name\n         documents_path = data_root_dir + author_name + \"/\"\n         authorCount += 1\n         for doc in os.listdir(documents_path):\n            if validate_file(doc):\n              text = open(docPath + doc, errors = \"ignore\").read()\n              X.append(text)\n              y.append(label)\n  return X, y\n```", "```py\ndef validate_file(file_name):\n    filterWords = [\"imitation\", \"demographics\", \"obfuscation\", \"verification\"]\n    for fw in filterWords:\n        if fw in file_name:\n            return False\n    return True\n```", "```py\nimport os\nimport nltk\nimport re\nimport spacy\nfrom sortedcontainers import SortedDict\nfrom keras.preprocessing import text\nimport numpy as np\n```", "```py\ndef CountChars(input):\n    num_chars = len(input)\n    return num_chars\n```", "```py\ndef averageCharacterPerWord(input):\n    text_array = text.text_to_word_sequence(input,\n                                            filters=' !#$%&()*+,-./:;<=>?@[\\\\]^_{|}~\\t\\n\"',\n                                            lower=False, split=\" \")\n    num_words = len(text_array)\n    text_without_spaces = input.replace(\" \", \"\")\n    num_chars = len(text_without_spaces)\n    avgCharPerWord = 1.0 * num_chars / num_words\n    return avgCharPerWord\n```", "```py\ndef frequencyOfLetters(input):\n    input = input.lower()  # because its case sensitive\n    input = input.lower().replace(\" \", \"\")\n    num_chars = len(input)\n    characters = \"abcdefghijklmnopqrstuvwxyz\".split()\n    frequencies = []\n    for each_char in characters:\n      char_count = input.count(each_char)\n      if char_count < 0:\n        frequencies.append(0)\n      else:\n        frequencies.append(char_count/num_chars)\n    return frequencies\n```", "```py\ndef CommonLetterBigramFrequency(input):\n    common_bigrams = ['th','he','in','er','an','re','nd',\n                      'at','on','nt','ha','es','st','en',\n                      'ed','to','it','ou','ea','hi','is',\n                      'or','ti','as','te','et','ng','of',\n                      'al','de','se','le','sa','si','ar',\n                      've','ra','ld','ur']\n    bigramCounter = []\n    input = input.lower().replace(\" \", \"\")\n    for bigram in common_bigrams:\n      bigram_count = input.count(bigram)\n      if bigram_count == -1:\n        bigramCounter.append(0)\n      else:\n        bigramCounter.append(bigram_count)\n    total_bigram_count = np.sum(bigramCounter)\n    bigramCounterNormalized = []\n    for bigram_count in bigramCounter:\n      bigramCounterNormalized.append(bigram_count / total_bigram_count)\n    return bigramCounterNormalized\n```", "```py\ndef CommonLetterTrigramFrequency(input):\n    common_trigrams = [\"the\", \"and\", \"ing\", \"her\", \"hat\",\n                       \"his\", \"tha\", \"ere\", \"for\", \"ent\",\n                       \"ion\", \"ter\", \"was\", \"you\", \"ith\",\n                       \"ver\", \"all\", \"wit\", \"thi\", \"tio\"]\n    trigramCounter = []\n    input = input.lower().replace(\" \", \"\")\n    for trigram in common_trigrams:\n      trigram_count = input.count(trigram)\n      if trigram_count == -1:\n        trigramCounter.append(0)\n      else:\n        trigramCounter.append(trigram_count)\n    total_trigram_count = np.sum(trigramCounter)\n    trigramCounterNormalized = []\n    for trigram_count in trigramCounter:\n      trigramCounterNormalized.append(trigram_count / total_trigram_count)\n    return trigramCounterNormalized\n```", "```py\ndef digitsPercentage(input):\n    num_chars = len(input)\n    num_digits = 0\n    for each_char in input:\n      if each_char.isnumeric():\n        num_digits = num_digits + 1\n    digit_percent = num_digits / num_chars\n    return digit_percent\n```", "```py\ndef charactersPercentage(input):\n    input = input.lower().replace(\" \", \"\")\n    characters = \"abcdefghijklmnopqrstuvwxyz\"\n    total_chars = len(input)\n    char_count = 0\n    for each_char in input:\n      if each_char in characters:\n        char_count = char_count + 1\n    char_percent = char_count / total_chars\n    return char_percent\n```", "```py\ndef frequencyOfDigits(input):\n    input = input.lower().replace(\" \", \"\")\n    num_chars = len(input)\n    digits = \"0123456789\".split()\n    frequencies = []\n    for each_digit in digits:\n      digit_count = input.count(each_digit)\n      if digit_count < 0:\n        frequencies.append(0)\n      else:\n        frequencies.append(digit_count/num_chars)\n    return frequencies\n```", "```py\ndef upperCaseCharactersPercentage(input):\n    input = input.replace(\" \", \"\")\n    upper_characters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    num_chars = len(input)\n    upper_count = 0\n    for each_char in upper_characters:\n      char_count = input.count(each_char)\n      if char_count > 0:\n        upper_count = upper_count + char_count\n    upper_percent = upper_count / num_chars\n    return upper_percent\n```", "```py\ndef frequencyOfSpecialCharacters(input):\n    SPECIAL_CHARS_FILE = \"static_files/writeprints_special_chars.txt\"\n    num_chars = len(input)\n    special_counts = []\n    special_characters = open(SPECIAL_CHARS_FILE , \"r\").readlines()\n    for each_char in special_characters:\n      special = each_char.strip().rstrip()\n      special_count = input.count(special)\n      if special_count < 0:\n        special_counts.append(0)\n      else:\n        special_counts.append(special_count / num_chars)\n    return special_counts\n```", "```py\ndef CountShortWords(input):\n    words = text.text_to_word_sequence(input, filters=\",.?!\\\"'`;:-()&$\", lower=True, split=\" \")\n    short_word_count = 0\n    for word in words:\n        if len(word) <= 3:\n            short_word_count = short_word_count + 1\n    return short_word_count\n```", "```py\ndef CountWords(input):\n    words = text.text_to_word_sequence(input, filters=\",.?!\\\"'`;:-()&$\", lower=True, split=\" \")\n    return len(words)\n```", "```py\ndef averageWordLength(input):\n    words = text.text_to_word_sequence(inputText, filters=\",.?!\\\"'`;:-()&$\", lower=True, split=\" \")\n    lengths = []\n    for word in words:\n        lengths.append(len(word))\n    return np.mean(lengths)\n```", "```py\ndef calculate_features(input):\n  features = []\n  features.extend([CountWords(input)])\n  features.extend([averageWordLength(input)])\n  features.extend([CountShortWords(input)])\n  features.extend([CountChars(input)])\n  features.extend([averageCharacterPerWord(input)])\n  features.extend([frequencyOfLetters(input)])\n  features.extend([CommonLetterBigramFrequency(input)])\n  features.extend([CommonLetterTrigramFrequency(input)])\n  features.extend([digitsPercentage(input)])\n  features.extend([charactersPercentage(input)])\n  features.extend([frequencyOfDigits(input)])\n  features.extend([upperCaseCharactersPercentage(input)])\n  features.extend([frequencyOfSpecialCharacters(input)])\n  features.extend([frequencyOfPunctuationCharacters(input)])\n  features.extend([posTagFrequency(input)])\n```", "```py\nX_original, Y = read_dataset(num_authors = 6)\nX_Features = []\nfor x in X_original:\n  x_features = calculate_features(x)\n  X.append(x_features)\n```", "```py\n# Import Packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n# Training and Test Datasets\nX_train, X_test, Y_train, Y_test = train_test_split(X_Features, Y)\n# Train the model\nmodel = RandomForestClassifier(n_estimators = 100)\nmodel.fit(X_train, Y_train)\n# Plot the confusion matrix\nY_predicted = model.predict(X_test)\nconfusion = confusion_matrix(Y_test, Y_predicted)\nplt.figure(figsize = (10,8))\nsns.heatmap(confusion, annot = True,\n            fmt = 'd', cmap=\"YlGnBu\")\n```", "```py\ndef calculate_accuracy(actual, predicted):\n  total_examples = len(actual)\n  correct_examples = 0\n  for idx in range(total_examples):\n    if actual[i] == predicted[i]:\n      correct_examples = correct_examples + 1\n  accuracy = correct_examples / total_examples\n  return accuracy\n```", "```py\nfrom sklearn.metrics import classification_report\nclassification_report(Y_test, Y_predicted)\n```", "```py\nimport nltk\nimport re\nimport random\nimport pickle\nfrom nltk.wsd import lesk\nfrom nltk.corpus import wordnet as wn\nimport WSD_with_UKB as wsd\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import RegexpTokenizer\n```", "```py\ndef contraction_replacement(sentence):\n    # Read Contractions\n    CONTRACTION_FILE = 'contraction_extraction.pickle'\n    with open(CONTRACTION_FILE, 'rb') as contraction_file:\n        contractions = pickle.load(contraction_file)\n    # Calculate contraction counts\n    all_contractions = contractions.keys()\n    contractions_count = 0\n    for contraction in all_contractions:\n        if contraction.lower() in sentence.lower():\n            contractions_count += 1\n    # Calculate expansion counts\n    all_expansions = contractions.values()\n    expansions_count = 0\n    for expansion in all_expansions:\n        if expansion.lower() in sentence.lower():\n            expansions_count += 1\n    if contractions_count > expansions_count:\n        # There are more contractions than expansions\n        # So we should replace all contractions with their expansions\n        temp_contractions = dict((k.lower(), v) for k, v in contractions.items())\n        for contraction in all_contractions:\n            if contraction.lower() in sentence.lower():\n                case_insensitive = re.compile(re.escape(contraction.lower()), re.IGNORECASE)\n                sentence = case_insensitive.sub(temp_contractions[contraction.lower()], sentence)\n        contractions_applied = True\n    elif expansions_count > contractions_count:\n        # There are more expansions than contractions\n        # So we should replace expansions by contractions\n        inv_map = {v: k for k, v in contractions.items()}\n        temp_contractions = dict((k.lower(), v) for k, v in inv_map.items())\n        for expansion in all_expansions:\n            if expansion.lower() in sentence.lower():\n                case_insensitive = re.compile(re.escape(expansion.lower()), re.IGNORECASE)\n                sentence = case_insensitive.sub(temp_contractions[expansion.lower()], sentence)\n        contractions_applied = True\n    else:\n        # Both expansions and contractions are equal\n        # So do nothing\n        contractions_applied = False\n    return sentence, contractions_applied\n```", "```py\ndef remove_parenthesis(sentence):\n    parantheses = ['(', ')', '{', '}', '[', ']']\n    for paranthesis in parantheses:\n      sentence = sentence.replace(paranthesis, \"\")\n    return sentence\n```", "```py\ndef remove_discourse_markers(sentence):\n    # Read Discourse Markers\n    DISCOURSE_FILE = 'discourse_markers.pkl'\n    with open(DISCOURSE_FILE , 'rb') as discourse_file:\n        discourse_markers = pickle.load(discourse_file)\n    sent_tokens = sentence.lower().split()\n    for marker in discourse_markers:\n        if marker.lower() in sent_tokens:\n            case_insensitive = re.compile(re.escape(marker.lower()), re.IGNORECASE)\n            sentence = case_insensitive.sub('', sentence)\n    return sentence\n```", "```py\ndef remove_appositions(sentence):\n    sentence = re.sub(r\" ?\\,[^)]+\\,\", \"\", sentence)\n    return sentence\n```", "```py\ndef apply_possessive_transformation(text):\n    if re.match(r\"(\\w+) of (\\w+)\", text):\n        rnd = random.choice([False, True, False])\n        if rnd:\n            return re.sub(r\"(\\w+) of (\\w+)\" , r\"\\2's \\1\", text)\n    return text\n```", "```py\ndef apply_equation_transformation(text):\n    words = RegexpTokenizer(r'\\w+').tokenize(text)\n    symbol_to_text =   {\n                '+': ' plus ',\n                '-': ' minus ',\n                '*': ' multiplied by ',\n                '/': ' divided by ',\n                '=': ' equals ',\n                '>': ' greater than ',\n                '<': ' less than ',\n                '<=': ' less than or equal to ',\n                '>=': ' greater than or equal to ',\n            }\n    for n,w in enumerate(words):\n        for symbol in symbol_to_text:\n            if symbol in w:\n                words[n] = words[n].replace(symbol, symbol_to_text[sym])\n    sentence = ''\n    for word in words:\n      sentence = sentence + word + \" \"\n    return sentence\n```", "```py\ndef untokenize(words):\n    text = ' '.join(words)\n    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .', '...')\n    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n        \"can not\", \"cannot\")\n    step6 = step5.replace(\" ` \", \" '\")\n    return step6.strip()\n```", "```py\ndef synonym_substitution(sentence, all_words):\n    new_tokens = []\n    output = wsd.process_text(sentence)\n    for token, synset in output:\n        if synset != None:\n            try:\n                # Get the synset name\n                synset = synset.split('-')\n                offset = int(synset[0])\n                pos = synset[1]\n                synset_name = wn.synset_from_pos_and_offset(pos, offset)\n                # List of Synonyms\n                synonyms = synset_name.lemma_names()\n                for synonym in synonyms:\n                    if synonym.lower() not in all_words:\n                        token = synonym\n                        break\n            except Exception as e:\n                # Some error in the synset naming....\n                continue\n        new_tokens.append(token)\n    final = untokenize(new_tokens)\n    final = final.capitalize()\n    return final\n```", "```py\ndef obfuscate_text(input_text):\n    obfuscated_text = []\n    sentences = sent_tokenize(input_text)\n    tokens = set(nltk.word_tokenize(input_text.lower()))\n    for sentence in sentences:\n        # 1\\. Apply Contractions\n        sentence, contractions_applied = contraction_replacement(sentence, contractions)\n        # 2\\. Remove Parantheses\n        sentence = remove_parenthesis(sentence)\n        # 3\\. Remove Discourse Markers\n        sentence = remove_discourse_markers(sentence, discourse_markers)\n        # 4\\. Remove Appositions\n        sentence = remove_appositions(sentence)\n        # 5\\. Synonym Substitution\n        sentence = synonym_substitution(sentence, tokens)\n        # 6\\. Apply possessive transformation\n        sentence = apply_possessive_transformation(sentence)\n        # 7\\. Apply equation transformation\n        sentence = apply_equation_transformation(sentence)\n        obfuscated_text.append(sentence)\n    obfuscated_text = \" \".join(obfuscated_text)\n    return obfuscated_text\n```", "```py\nfrom sklearn.model_selection import train_test_split\n# Read Data\nX, Y = read_dataset(num_authors = 6)\n# Split it into train and test\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n```", "```py\n# Extract features from training data\nX_train_features = []\nfor x in X_train:\n  x_features = calculate_features(x)\n  X_train_features.append(x_features)\n# Train the model\nmodel = RandomForestClassifier(n_estimators = 100)\nmodel.fit(X_train_features, Y_train)\n```", "```py\nX_test_obfuscated = []\nfor x in X_test:\n  # Obfuscate\n  x_obfuscated = obfuscate_text(x)\n  # Extract features\n  x_obfuscated_features = calculate_features(x_obfuscated)\n  X_test_obfuscated.append(x_obfuscated_features)\n```", "```py\n# Calculate accuracy on original\nY_pred_original = model.predict(X_test)\naccuracy_orig = calculate_accuracy(Y_test, Y_pred_original)\n# Calculate accuracy on obfuscated\nY_pred_obfuscated = model.predict(X_test_obfuscated)\naccuracy_obf = calculate_accuracy(Y_test, Y_pred_obfuscated)\n```"]