<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;7.&#xA0;Gyroscopic Video Stabilization" id="1NA0K1-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07" class="calibre1"/>Chapter 7. Gyroscopic Video Stabilization</h1></div></div></div><p class="calibre8">Video stabilization is<a id="id643" class="calibre1"/> a classic problem in computer vision. The idea is simple – you have a video stream that's shaky, and you're trying to identify the best way to negate the motion of the camera to produce a smooth motion across images. The resulting video is easier to view and has a cinematic look.</p><p class="calibre8">Over the years, there have been a number of approaches being tried to solve this. Videos have traditionally been stabilized by using data available only from images, or using specialized hardware to negate physical motion in the camera. Gyroscopes in mobile devices are the middle ground between these two approaches.</p><p class="calibre8">In this chapter, we'll cover the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">An Android camera application to record media and gyroscope traces</li><li class="listitem">Using the video and gyroscope trace to find mathematical unknowns</li><li class="listitem">Using the physical camera unknowns to compensate for camera motion</li><li class="listitem">Identifying rolling shutter in the camera</li></ul></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note124" class="calibre1"/>Note</h3><p class="calibre8">Rolling shutter on a camera sensor produces unwanted effects. We'll cover this in detail in a later section. Also, refer to <a class="calibre1" title="Chapter 1. Getting the Most out of Your Camera System" href="part0015_split_000.html#E9OE2-940925703e144daa867f510896bffb69">Chapter 1</a>, <span class="strong"><em class="calibre10">Getting the Most out of Your Camera System</em></span>, for a detailed discussion.</p></div><p class="calibre8">Before we get started, let's take a look at some techniques that were used in the past to solve this problem.</p></div>

<div class="book" title="Chapter&#xA0;7.&#xA0;Gyroscopic Video Stabilization" id="1NA0K1-940925703e144daa867f510896bffb69">
<div class="book" title="Stabilization with images"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch07lvl1sec52" class="calibre1"/>Stabilization with images</h1></div></div></div><p class="calibre8">Video<a id="id644" class="calibre1"/> stabilization with images alone seems like the first logical step. Indeed, initial research on stabilizing video captured by cameras was based on using information readily available from the camera sensors to understand how they move image by image.</p><p class="calibre8">The idea is to find keypoints in an image sequence to understand how they move image by image. Keypoints are pixel locations on an image that match these criteria:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Keypoints <a id="id645" class="calibre1"/>should be easily recognizable and distinguishable from each other. Corners of objects are good keypoints while a point on a blank wall is not.</li><li class="listitem">It should be possible to track keypoints across multiple images to calculate motion. You should be able to tell exactly where the keypoint has moved from one frame to another.</li><li class="listitem">For performance, identifying these keypoints should be fast and memory-efficient. This is usually a bottleneck on low memory and low power devices.</li></ul></div><p class="calibre8">Research with these criteria led to several unique approaches like including some famous algorithms such as SIFT, SURF, ORB, FREAK, and so on. These techniques often work well.</p><p class="calibre8">OpenCV <a id="id646" class="calibre1"/>comes with several common keypoint detectors. These include ORB, FAST, BRISK, SURF, and so on. Check the 2D Features Framework documentation page for more information on how to use these at: <a class="calibre1" href="http://docs.opencv.org/3.0-beta/modules/features2d/doc/feature_detection_and_description.html">http://docs.opencv.org/3.0-beta/modules/features2d/doc/feature_detection_and_description.html</a>.</p><p class="calibre8">The keypoint detectors, however, have their own set of drawbacks. Firstly, the results of stabilization are highly dependent on the quality of the images. For example, a low resolution image might not produce the best set of features. Out of focus and blurry images are another concern. This puts a constraint on the types of sequences that can be stabilized. A scene with a clear blue sky and yellow sand might not contain enough features.</p><p class="calibre8">A surface with a repetitive pattern will confuse the algorithm because the same features keep showing up in different positions.</p><div class="mediaobject"><img src="../images/00116.jpeg" alt="Stabilization with images" class="calibre11"/></div><p class="calibre12"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note125" class="calibre1"/>Note</h3><p class="calibre8">The<a id="id647" class="calibre1"/> image above is taken from: <a class="calibre1" href="https://d2v9y0dukr6mq2.cloudfront.net/video/thumbnail/kG-5Wkc/crowd-of-people-walking-crossing-street-at-night-in-times-square-slow-motion-30p_ekqzvese__S0000.jpg">https://d2v9y0dukr6mq2.cloudfront.net/video/thumbnail/kG-5Wkc/crowd-of-people-walking-crossing-street-at-night-in-times-square-slow-motion-30p_ekqzvese__S0000.jpg</a></p></div><p class="calibre8">Secondly, if there is a large amount of motion in the image (such as people walking in the background or <a id="id648" class="calibre1"/>a truck moving across the road), the stabilization will be skewed because of it. The keypoint is tracking the moving object and not the motion of the camera itself. This limits the types of videos that can be successfully stabilized with such an approach. There are ways of getting around such constraints – however, it makes the algorithm more complex.</p></div></div>
<div class="book" title="Stabilization with hardware" id="1O8H61-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec53" class="calibre1"/>Stabilization with hardware</h1></div></div></div><p class="calibre8">Certain<a id="id649" class="calibre1"/> industries, such as the movie industry and the military, expect high quality video stabilization. Using just images in those varied environments would not work. This led industries to create hardware-based image stabilization rigs. For example, a quadcopter with a camera needs to have a high quality video output despite (potentially) bad lighting conditions, wind, and so on.</p><div class="mediaobject"><img src="../images/00117.jpeg" alt="Stabilization with hardware" class="calibre11"/></div><p class="calibre12"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note126" class="calibre1"/>Note</h3><p class="calibre8">The <a id="id650" class="calibre1"/>image above is taken from <a class="calibre1" href="http://g01.a.alicdn.com/kf/HTB1.ogPIpXXXXaXXVXXq6xXFXXXw/GH4-A7S-SUMMER-DYS-3-axis-3-Axis-Gimbal-dslr-camera-Stabilizer-gyro-brushless-gimbal-steadicam.jpg">http://g01.a.alicdn.com/kf/HTB1.ogPIpXXXXaXXVXXq6xXFXXXw/GH4-A7S-SUMMER-DYS-3-axis-3-Axis-Gimbal-dslr-camera-Stabilizer-gyro-brushless-gimbal-steadicam.jpg</a></p></div><p class="calibre8">These <a id="id651" class="calibre1"/>devices use a gyroscope to physically move and rotate the camera so that the image sequence stays stable. The results look excellent since you're actually compensating for the motion of the camera.</p><p class="calibre8">These devices tend to be on the more expensive side and thus unaffordable for the common consumer. They also tend to be quite bulky. The average person would not want to carry a two kilogram rig on his vacation.</p></div>
<div class="book" title="A hybrid of hardware and software" id="1P71O1-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec54" class="calibre1"/>A hybrid of hardware and software</h1></div></div></div><p class="calibre8">This <a id="id652" class="calibre1"/>chapter covers a hybrid solution between the original software-only approach and hardware devices. This became possible only recently, with the advent of the smartphone. People now had a high quality camera and a gyroscope in a small form.</p><p class="calibre8">The idea behind this approach is to use the gyroscope to capture motion and the camera sensor to capture light. These two streams are then fused so that the image is always stable.</p><p class="calibre8">As the sensors' density increases and we head to 4K cameras, selecting a (stable) subregion of the image becomes an increasingly viable option as we can discard more of the image without compromising on the quality.</p></div>

<div id="page" style="height:0pt"/><div class="book" title="The math"><div class="book" id="1Q5IA2-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec55" class="calibre1"/>The math</h1></div></div></div><p class="calibre8">Before <a id="id653" class="calibre1"/>we jump into the code, let's take an overview of the algorithm. There are four key components.</p><div class="book"><ul class="itemizedlist"><li class="listitem">The first is the pinhole camera model. We try and approximate real world positions to pixels using this matrix.</li><li class="listitem">The second is the camera motion estimate. We need to use data from the gyroscope to figure out the orientation of the phone at any given moment.</li><li class="listitem">The third is the rolling shutter computation. We need to specify the direction of the rolling shutter and estimate the duration of the rolling shutter.</li><li class="listitem">The fourth is the image warping expression. Using all the information from the previous<a id="id654" class="calibre1"/> calculations, we need to generate a new image so that it becomes stable.</li></ul></div></div>

<div class="book" title="The math">
<div class="book" title="The camera model"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec69" class="calibre1"/>The camera model</h2></div></div></div><p class="calibre8">We use <a id="id655" class="calibre1"/>the standard pinhole camera model. This model is used in several algorithms and is a good approximation of an actual camera.</p><div class="mediaobject"><img src="../images/00118.jpeg" alt="The camera model" class="calibre11"/></div><p class="calibre12"> </p><div class="mediaobject"><img src="../images/00119.jpeg" alt="The camera model" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">There are three unknowns. The <span class="strong"><em class="calibre10">o</em></span> variables indicate the origin of the camera axis in the image plane (these can be assumed to be 0). The two 1s in the matrix indicate the aspect ratio of the pixels (we're assuming square pixels). The <span class="strong"><em class="calibre10">f</em></span> indicates the focal length of the lens. We're assuming the focal length is the same in both horizontal and vertical directions.</p><p class="calibre8">Using this model, we can see that:</p><div class="mediaobject"><img src="../images/00120.jpeg" alt="The camera model" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Here, X is the point in the real world. There is also an unknown scaling factor, <span class="strong"><em class="calibre10">q</em></span>, present.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note127" class="calibre1"/>Note</h3><p class="calibre8">Estimating this unknown is not possible for monocular vision unless the physical dimensions of an object are known.</p></div><p class="calibre8"><span class="strong"><em class="calibre10">K</em></span> is the intrinsic matrix and <span class="strong"><em class="calibre10">x</em></span> is the point on the image.</p></div></div>

<div class="book" title="The math">
<div class="book" title="The Camera motion"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec70" class="calibre1"/>The Camera motion</h2></div></div></div><p class="calibre8">We can <a id="id656" class="calibre1"/>assume that the world origin is the same as the camera origin. Then, the motion of the camera can be described in terms of the orientation of the camera. Thus, at any given time <span class="strong"><em class="calibre10">t</em></span>:</p><div class="mediaobject"><img src="../images/00121.jpeg" alt="The Camera motion" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The rotation matrix <span class="strong"><em class="calibre10">R</em></span> can be calculated by integrating the angular velocity of the camera (obtained from the gyroscope).</p><div class="mediaobject"><img src="../images/00122.jpeg" alt="The Camera motion" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Here, <span class="strong"><em class="calibre10">ω<sub class="calibre29">d</sub></em></span> is the gyroscope drift and <span class="strong"><em class="calibre10">t<sub class="calibre29">d</sub></em></span> is the delay between the gyroscope and frame timestamps. These are unknowns as well; we need a mechanism to calculate them.</p></div></div>

<div class="book" title="The math">
<div class="book" title="Rolling shutter compensation"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec71" class="calibre1"/>Rolling shutter compensation</h2></div></div></div><p class="calibre8">When <a id="id657" class="calibre1"/>you click a picture, the common assumption is that the entire image is captured in one go. This is indeed the case for images captured with CCD sensors (which were prevalent a while back). With the commercialization of CMOS image sensors, this is no longer the case. Some CMOS sensors support a global shutter too but, in this chapter, we'll assume the sensor has a rolling shutter.</p><p class="calibre8">Images are captured one row at a time—usually the first row is captured first, then the second row, and so on. There's a very slight delay between the consecutive rows of an image.</p><p class="calibre8">This leads to strange effects. This is very visible when we're correcting camera shake (for example if there's a lot of motion in the camera).</p><div class="mediaobject"><img src="../images/00123.jpeg" alt="Rolling shutter compensation" class="calibre11"/><div class="caption"><p class="calibre28">The fan blades are the same size; however due to the fast motion, the rolling shutter causes artifacts in the image recorded by the sensor.</p></div></div><p class="calibre12"> </p><p class="calibre8">To<a id="id658" class="calibre1"/> model the rolling shutter, we need to identify at what time a specific row was captured. This can be done as follows:</p><div class="mediaobject"><img src="../images/00124.jpeg" alt="Rolling shutter compensation" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Here, <span class="strong"><em class="calibre10">t<sub class="calibre29">i</sub></em></span> is the time when the i<sup class="calibre31">th</sup> frame was captured, <span class="strong"><em class="calibre10">h</em></span> is the height of the image frame, and <span class="strong"><em class="calibre10">t<sub class="calibre29">s</sub></em></span> is the duration of the rolling shutter, that is, the time it takes to scan from top to bottom. Assuming each row takes the same time, the y<sup class="calibre31">th</sup> row would take <span class="strong"><em class="calibre10">t<sub class="calibre29">s</sub> * y / h</em></span> additional time to get scanned.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note128" class="calibre1"/>Note</h3><p class="calibre8">This assumes the rolling shutter happens from top to bottom. A rolling shutter from bottom to top can be modeled with a negative value for t<sub class="calibre29">s</sub>. Also, a rolling shutter from left to right can be modeled by replacing <span class="strong"><em class="calibre10">y / h</em></span> with <span class="strong"><em class="calibre10">x / w</em></span> where <span class="strong"><em class="calibre10">w</em></span> is the width of the frame.</p></div></div></div>

<div class="book" title="The math">
<div class="book" title="Image warping"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec72" class="calibre1"/>Image warping</h2></div></div></div><p class="calibre8">So far, we have the estimated camera motion and a model for correcting the rolling shutter. We'll combine both and identify a relationship across multiple frames:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><img src="../images/00125.jpeg" alt="Image warping" class="calibre32"/></span> (for frame i with rotation configuration 1)</li><li class="listitem"><span class="strong"><img src="../images/00126.jpeg" alt="Image warping" class="calibre32"/></span> (for frame j with rotation configuration 2)
</li></ul></div><p class="calibre8">We can<a id="id659" class="calibre1"/> combine these two equations:</p><div class="mediaobject"><img src="../images/00127.jpeg" alt="Image warping" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">From here, we can calculate a warping matrix:</p><div class="mediaobject"><img src="../images/00128.jpeg" alt="Image warping" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Now, the relationship between points x<sub class="calibre29">i</sub> and x<sub class="calibre29">j</sub> can be more succinctly described as:</p><div class="mediaobject"><img src="../images/00129.jpeg" alt="Image warping" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">This warp matrix simultaneously corrects both the video shake and the rolling shutter.</p><p class="calibre8">Now we can map the original video to an artificial camera that has smooth motion and a global shutter (no rolling shutter artifacts).</p><p class="calibre8">This artificial camera can be simulated by low-pass filtering the input camera's motion and setting the rolling shutter duration to zero. A low pass filter removes high frequency noise from the camera orientation. Thus, the artificial camera's motion will appear much smoother.</p><p class="calibre8">Ideally, this matrix can be calculated for each row in the image. However, in practice, subdividing the image into five subsections produces good results as well (with better performance).</p></div></div>
<div class="book" title="Project overview" id="1R42S1-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec56" class="calibre1"/>Project overview</h1></div></div></div><p class="calibre8">Let's take<a id="id660" class="calibre1"/> a moment to understand how the code in this chapter is organized. We have two moving pieces. One is the mobile application and the second is the video stabilizer.</p><p class="calibre8">The mobile app only records video and stores the gyroscope signals during the video. It dumps this data into two files: a <code class="email">.mp4</code> and a <code class="email">.csv</code> file. These two files are the input for the next step. There is no computation on the mobile device. In this chapter, we'll use Android as our platform. Moving to any other platform should be fairly easy—we are doing only basic tasks that any platform should support.</p><p class="calibre8">The video stabilizer runs on a desktop. This is to help you figure out what's happening in the stabilization algorithm much more easily. Debugging, stepping through code and viewing images on a mobile device is relatively slower than iterating on a desktop. We have some really good scientific modules available for free (from the Python community. In this project, we will use Scipy, Numpy, and Matplotlib).</p></div>

<div id="page" style="height:0pt"/><div class="book" title="Capturing data"><div class="book" id="1S2JE2-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec57" class="calibre1"/>Capturing data</h1></div></div></div><p class="calibre8">First, we <a id="id661" class="calibre1"/>need to create an app for a mobile device that can capture both images and gyroscope signals simultaneously. Interestingly, these aren't readily available (at least on Android).</p><p class="calibre8">Once we have a video and gyro stream, we'll look at how to use that data.</p><p class="calibre8">Create a standard Android application (I use Android Studio). We'll start by creating a blank application. The goal is to create a simple app that starts recording video and gyro signals on touching the screen. On touching again, the recording stops and a video file and a text file are saved on the phone. These two files can then be used by OpenCV to compute the best stabilization.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note129" class="calibre1"/>Note</h3><p class="calibre8">Code in this section is available in the GitHub repository for this book: <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_7">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_7</a></p></div></div>

<div class="book" title="Capturing data">
<div class="book" title="Recording video"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec73" class="calibre1"/>Recording video</h2></div></div></div><p class="calibre8">We'll start by <a id="id662" class="calibre1"/>implementing a simple video recording utility. Create a new blank project in Android Studio (I named it GyroRecorder and named the activity Recorder). First, we start by adding permissions to our app. Open <code class="email">AndroidManifest.xml</code> in your project and add these permissions:</p><div class="informalexample"><pre class="programlisting">&lt;manifest ...&gt;
    &lt;uses-permission android:name="android.permission.CAMERA" /&gt;
    &lt;uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" /&gt;
    &lt;application ...&gt;</pre></div><p class="calibre8">This simply lets our app access the camera and write to storage (the gyro file and the video). Next, open<a id="id663" class="calibre1"/> the main activity visual editor and add a TextureView and a Button element inside a vertical LinearLayout.</p><p class="calibre8">Change the names of these elements to <code class="email">texturePreview</code> and <code class="email">btnRecord</code> respectively.</p><div class="mediaobject"><img src="../images/00130.jpeg" alt="Recording video" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Now we start with some code. In the main activity class, add these lines:</p><div class="informalexample"><pre class="programlisting">public class Recorder extends AppCompatActivity {
    private TextureView mPreview;  // For displaying the live camera preview
    private Camera mCamera;        // Object to contact the camera hardware
    private MediaRecorder mMediaRecorder;    // Store the camera's image stream as a video

    private boolean isRecording = false; // Is video being recoded?
    private Button btnRecord;            // Button that triggers recording</pre></div><p class="calibre8">These objects will be used to communicate with Android to indicate when to start recording.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note130" class="calibre1"/>Note</h3><p class="calibre8">Android Studio automatically adds imports to your code as you type. For example, the above piece results in the addition of:</p><div class="informalexample"><pre class="programlisting">import android.hardware.Camera;
import android.media.MediaRecorder;
import android.view.TextureView;</pre></div></div><p class="calibre8">Next, we<a id="id664" class="calibre1"/> need to initialize these objects. We do that in the <code class="email">onCreate</code> event.</p><div class="informalexample"><pre class="programlisting">    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_recorder);

        mPreview = (TextureView)findViewById(R.id.texturePreview);
        btnRecord = (Button)findViewById(R.id.btnRecord);

        btnRecord.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                onCaptureClick(view);
            }
        });</pre></div><p class="calibre8">The <code class="email">onCreate</code> method already contains some methods (<code class="email">super.onCreate</code>, <code class="email">setContentView</code>, and so on; we will add a few lines after that). Now, we need to define what <code class="email">onCaptureClick</code> does.</p><div class="informalexample"><pre class="programlisting">    public void onCaptureClick(View view) {
        if (isRecording) {
            // Already recording? Release camera lock for others
            mMediaRecorder.stop();
            releaseMediaRecorder();
            mCamera.lock();

            isRecording = false;
            releaseCamera();
            mGyroFile.close();
            mGyroFile = null;
            btnRecord.setText("Start");
            mStartTime = -1;
        } else {
            // Not recording – launch new "thread" to initiate!
            new MediaPrepareTask().execute(null, null, null);
        }
    }</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note131" class="calibre1"/>Note</h3><p class="calibre8">If you want to explore <code class="email">strings.xml</code>, refer to <a class="calibre1" title="Chapter 2. Photographing Nature and Wildlife with an Automated Camera" href="part0023_split_000.html#LTSU2-940925703e144daa867f510896bffb69">Chapter 2</a>, <span class="strong"><em class="calibre10">Working with Camera Frames</em></span>, of the PacktPub book <span class="strong"><em class="calibre10">Android Application Programming with OpenCV</em></span>.</p></div><p class="calibre8">Here, we<a id="id665" class="calibre1"/> use the internal <code class="email">isRecording</code> variable to notify the media recorder and camera to start saving the stream. We need to create a new thread because initializing the media recorder and camera usually takes a few milliseconds. This lag would be noticeable on the UI if we did it in the main thread.</p><p class="calibre8">Once we're done recording (the user taps the Stop button and we need to release the media recorder. This happens in the <code class="email">releaseMediaRecorder</code> method:</p><div class="informalexample"><pre class="programlisting">    private void releaseMediaRecorder() {
        if(mMediaRecorder != null) {
            mMediaRecorder.reset();
            mMediaRecorder.release();
            mMediaRecorder = null;
            mCamera.lock();
        }
    }</pre></div><p class="calibre8">Now, we look at creating a new thread. Create this class in your main activity class.</p><div class="informalexample"><pre class="programlisting">    class MediaPrepareTask extends AsyncTask&lt;Void, Void, Boolean&gt;{
        @Override
        protected Boolean doInBackground(Void... voids) {
            if(prepareVideoRecorder()) {
                mMediaRecorder.start();
                isRecording = true;
            } else {
                releaseMediaRecorder();
                return false;
            }
            return true;
        }

        @Override
        protected void onPostExecute(Boolean result) {
            if(!result) {
                Recorder.this.finish();
            }

            btnRecord.setText("Stop");
        }
    }</pre></div><p class="calibre8">This creates an object of the type <code class="email">AsyncTask</code>. Creating a new object of this class automatically creates a new thread and runs <code class="email">doInBackground</code> in that thread. We want to prepare the <a id="id666" class="calibre1"/>media recorder in this thread. Preparing the media recorder involves identifying supported image sizes from the camera, finding the suitable height, setting the bitrate of the video and specifying the destination video file.</p><p class="calibre8">In your main activity class, create a new method called <code class="email">prepareVideoRecorder</code>:</p><div class="informalexample"><pre class="programlisting">    @TargetApi(Build.VERSION_CODES.HONEYCOMB)
    private boolean prepareVideoRecorder() {
        mCamera = Camera.open();

        Camera.Parameters parameters = mCamera.getParameters();
        List&lt;Camera.Size&gt; mSupportedPreviewSizes = parameters.getSupportedPreviewSizes();</pre></div><p class="calibre8">Now that we have supported video sizes, we need to find the optimal image size for the camera. This is done here:</p><div class="informalexample"><pre class="programlisting">        Camera.Size optimalSize = getOptimalPreviewSize(mSupportedPreviewSizes,mPreview.getWidth(),mPreview.getHeight());
        parameters.setPreviewSize(optimalSize.width,optimalSize.height);</pre></div><p class="calibre8">With the optimal size in hand, we can now set up the camera recorder settings:</p><div class="informalexample"><pre class="programlisting">        CamcorderProfile profile = CamcorderProfile.get(CamcorderProfile.QUALITY_HIGH);
        profile.videoFrameWidth = optimalSize.width;
        profile.videoFrameHeight = optimalSize.height;</pre></div><p class="calibre8">Now, we try to contact the camera hardware and set up these parameters:</p><div class="informalexample"><pre class="programlisting">        mCamera.setParameters(parameters);
        try {
               mCamera.setPreviewTexture(mPreview.getSurfaceTexture());
        } catch(IOException e) {
            Log.e(TAG,"Surface texture is unavailable or unsuitable" + e.getMessage());
            return false;
        }</pre></div><p class="calibre8">Here, along with setting the camera parameters, we also specify a preview surface. The preview surface is used to display what the camera sees live.</p><p class="calibre8">With the camera setup done, we can now set up the media recorder:</p><div class="informalexample"><pre class="programlisting">    mMediaRecorder = new MediaRecorder();
        mCamera.unlock();
        mMediaRecorder.setCamera(mCamera);

        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);

        mMediaRecorder.setOutputFormat(profile.fileFormat);
        mMediaRecorder.setVideoFrameRate(profile.videoFrameRate);
        mMediaRecorder.setVideoSize(profile.videoFrameWidth,profile.videoFrameHeight);
        mMediaRecorder.setVideoEncodingBitRate(
                                    profile.videoBitRate);

        mMediaRecorder.setVideoEncoder(profile.videoCodec);
        mMediaRecorder.setOutputFile(getOutputMediaFile().toString());</pre></div><p class="calibre8">This just <a id="id667" class="calibre1"/>sets whatever we already know about the video stream—we're simply passing information from what we've gathered into the media recorder.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note132" class="calibre1"/>Note</h3><p class="calibre8">This configuration does not record audio. In this project, we're not concerned with the audio signals. However, it should be straightforward to configure the media recorder to store audio as well.</p></div><p class="calibre8">With everything in place, we try to start the media recorder:</p><div class="informalexample"><pre class="programlisting">        try {
            mMediaRecorder.prepare();
        } catch (IllegalStateException e) {
            Log.d(TAG, "IllegalStateException preparing MediaRecorder: " + e.getMessage());
            releaseMediaRecorder();
            return false;
        } catch (IOException e) {
            Log.d(TAG, "IOException preparing MediaRecorder: " + e.getMessage());
            releaseMediaRecorder();
            return false;
        }
        return true;
    }</pre></div><p class="calibre8">And that's the end of the <code class="email">prepareVideoRecorder</code> method. We've referenced a bunch of variables and functions that do not exist yet, so we'll define some of them now.</p><p class="calibre8">The first is <code class="email">getOptimalPreviewSize</code>. Define this method in your activity's class:</p><div class="informalexample"><pre class="programlisting">    private Camera.Size getOptimalPreviewSize(List&lt;Camera.Size&gt; sizes, int w, int h) {
        final double ASPECT_TOLERANCE = 0.1;
        double targetRatio = (double)w / h;

        if(sizes == null) {
            return null;
        }

        Camera.Size optimalSize = null;

        double minDiff = Double.MAX_VALUE;

        int targetHeight = h;

        for (Camera.Size size : sizes) {
            double ratio = (double)size.width / size.height;
            double diff = Math.abs(ratio - targetRatio);

            if(Math.abs(ratio - targetRatio) &gt; ASPECT_TOLERANCE)
                continue;

            if(Math.abs(size.height - targetHeight) &lt; minDiff) {
                optimalSize = size;
                minDiff = Math.abs(size.height - targetHeight);
            }
        }

        if(optimalSize == null) {
            minDiff = Double.MAX_VALUE;
            for(Camera.Size size : sizes) {
                if(Math.abs(size.height-targetHeight) &lt; minDiff) {
                    optimalSize = size;
                    minDiff = Math.abs(size.height-targetHeight);
                }
            }
        }

        return optimalSize;
    }</pre></div><p class="calibre8">This<a id="id668" class="calibre1"/> function simply tries to match all possible image sizes against an expected aspect ratio. If it cannot find a close match, it returns the closest match (based on the expected height).</p><p class="calibre8">The second is <code class="email">getOutputMediaFile</code>. This function uses the Android API to find an acceptable location to store our videos. Define this method in the main activity class as well:</p><div class="informalexample"><pre class="programlisting">    private File getOutputMediaFile() {
        if(!Environment.getExternalStorageState().equalsIgnoreCase(Environment.MEDIA_MOUNTED)) {
            return null;
        }

        File mediaStorageDir = new File(Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_PICTURES), "Recorder");

        if(!mediaStorageDir.exists()) {
            if(!mediaStorageDir.mkdirs()) {
                Log.d("Recorder", "Failed to create directory");
                return null;
            }
        }

        String timeStamp = new SimpleDateFormat("yyyyMMdd_HHmmss").format(new Date());
        File mediaFile;
        mediaFile = new File(mediaStorageDir.getPath() + File.separator + "VID_" + timeStamp + ".mp4");

        return mediaFile;
    }</pre></div><p class="calibre8">It finds<a id="id669" class="calibre1"/> the media storage location for pictures and appends a timestamp to the filename.</p><p class="calibre8">Now we have almost everything to start recording videos. Two more method definitions and we'll have a working video recorder.</p><div class="informalexample"><pre class="programlisting">    private void releaseCamera() {
        if(mCamera != null) {
            mCamera.release();
            mCamera = null;
        }
    }
    @Override
    protected void onPause() {
        super.onPause();

        releaseMediaRecorder();
        releaseCamera();
    }</pre></div><p class="calibre8">The <code class="email">onPause</code> method is<a id="id670" class="calibre1"/> called whenever the user switches to another app. It's being a good citizen to release hardware dependencies when you're not using them.</p></div></div>

<div class="book" title="Capturing data">
<div class="book" title="Recording gyro signals"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec74" class="calibre1"/>Recording gyro signals</h2></div></div></div><p class="calibre8">In<a id="id671" class="calibre1"/> the previous section, we only looked at recording video. For this project, we also need to record gyroscope signals. With Android, this is accomplished by using a sensor event listener. We'll modify the main activity class for this. Add this <code class="email">implements</code> clause:</p><div class="informalexample"><pre class="programlisting">public class Recorder extends Activity implements SensorEventListener {
    private TextureView mPreview;
    private Camera mCamera;
    ...</pre></div><p class="calibre8">Now, we need to add a few new objects to our class:</p><div class="informalexample"><pre class="programlisting">    ...
    private Button btnRecord;

    private SensorManager mSensorManager;
    private Sensor mGyro;
    private PrintStream mGyroFile;
    private long mStartTime = -1;

    private static String TAG = "GyroRecorder";

    @Override
    protected void onCreate(Bundle savedInstanceState) {
    ....</pre></div><p class="calibre8">The <code class="email">SensorManager</code> object manages all sensors on the hardware. We're only interested in the gyroscope, so we have a <code class="email">Sensor</code> object for it. <code class="email">PrintStream</code> writes a text file with the gyroscope signals. We now need to initialize these objects. We do that in the <code class="email">onCreate</code> method. Modify the method so that it looks like this:</p><div class="informalexample"><pre class="programlisting">                onCaptureClick(view);
            }
        });

        mSensorManager = (SensorManager)getSystemService(Context.SENSOR_SERVICE);
        mGyro = mSensorManager.getDefaultSensor(Sensor.TYPE_GYROSCOPE);
        mSensorManager.registerListener(this, mGyro, SensorManager.SENSOR_DELAY_FASTEST);
    }</pre></div><p class="calibre8">Here, we're <a id="id672" class="calibre1"/>fetching the gyroscope sensor and registering that this class should receive events (<code class="email">registerListener</code>). We're also mentioning the frequency we want data to flow in.</p><p class="calibre8">Next, we initialize the <code class="email">PrintStream</code> in the <code class="email">prepareVideoRecorder</code> method:</p><div class="informalexample"><pre class="programlisting">    private boolean prepareVideoRecorder() {
        mCamera = Camera.open();
        ...
        mMediaRecorder.setOutputFile(getOutputMediaFile().toString());

        try {
            mGyroFile = new PrintStream(getOutputGyroFile());
            mGyroFile.append("gyro\n");
        } catch(IOException e) {
            Log.d(TAG, "Unable to create acquisition file");
            return false;
        }

        try {
            mMediaRecorder.prepare();
        ...</pre></div><p class="calibre8">This tries to open a new stream to a text file. We fetch the text file name using:</p><div class="informalexample"><pre class="programlisting">    private File getOutputGyroFile() {
        if(!Environment.getExternalStorageState().equalsIgnoreCase(Environment.MEDIA_MOUNTED)) {
            return null;
        }

        File gyroStorageDir = new File(Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_PICTURES), "Recorder");

        if(!gyroStorageDir.exists()) {
            if(!gyroStorageDir.mkdirs()) {
                Log.d("Recorder", "Failed to create directory");
                return null;
            }
        }

        String timeStamp = new SimpleDateFormat("yyyyMMdd_HHmmss").format(new Date());
        File gyroFile;
        gyroFile = new File(gyroStorageDir.getPath() + File.separator + "VID_" + timeStamp + "gyro.csv");

        return gyroFile;
    }</pre></div><p class="calibre8">This is <a id="id673" class="calibre1"/>almost the same code as <code class="email">getOutputMediaFile</code>, except that it returns a <code class="email">.csv</code> file (instead of an <code class="email">.mp4</code>) in the same directory.</p><p class="calibre8">One last thing and we'll be recording gyroscope signals as well. Add this method to the main activity class:</p><div class="informalexample"><pre class="programlisting">    @Override
    public void onAccuracyChanged(Sensor sensor, int accuracy) {
        // Empty on purpose
        // Required because we implement SensorEventListener
    }

    @Override
    public void onSensorChanged(SensorEvent sensorEvent) {
        if(isRecording) {
            if(mStartTime == -1) {
                mStartTime = sensorEvent.timestamp;
            }
            mGyroFile.append(sensorEvent.values[0] + "," +
                             sensorEvent.values[1] + "," +
                             sensorEvent.values[2] + "," +
                             (sensorEvent.timestamp-mStartTime) + "\n");
        }
    }</pre></div><p class="calibre8">The idea is to store values returned by the sensor into the file as soon as possible.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Android specifics"><div class="book" id="1T1402-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec58" class="calibre1"/>Android specifics</h1></div></div></div><p class="calibre8">In this<a id="id674" class="calibre1"/> section, we'll look at some Android-specific tasks: one is rendering an overlay on top of the camera view and the second is reading media files on Android.</p><p class="calibre8">The overlay is helpful for general information and debugging, and looks nice too! Think of it like the heads up display on a consumer camera.</p><p class="calibre8">The reading media files section is something we don't use in this chapter (we read media files using Python). However, if<a id="id675" class="calibre1"/> you decide to write an Android app that processes videos on the device itself, this section should get you started.</p></div>

<div class="book" title="Android specifics">
<div class="book" title="Threaded overlay"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec75" class="calibre1"/>Threaded overlay</h2></div></div></div><p class="calibre8">Now that<a id="id676" class="calibre1"/> we have the camera preview working, we want to render some additional information on top of it. We'll be drawing three things; first, a red circle to indicate whether recording is active, second, the current gyroscope values (angular velocity and estimated theta) just for information, and third, a safety rectangle. When stabilizing, we'll probably be cropping the image a bit. The rectangle will guide your video recording to stay within a relatively safe zone.</p><p class="calibre8">Along with this, we'll also be setting it up so that you can create buttons on this overlay. Simple touch events can be used to execute specific functions.</p><p class="calibre8">You don't need this section for the application to work, but it's a good idea to know how to render on top of an OpenCV camera view while recording.</p><p class="calibre8">Before we start working on the overlay widget, let's define a supporting class, <code class="email">Point3</code>. Create a new class called <code class="email">Point3</code> with three double attributes:</p><div class="informalexample"><pre class="programlisting">public class Point3 {
    public double x;
    public double y;
    public double z;
}</pre></div><p class="calibre8">We start by defining a new class, <code class="email">CameraOverlayWidget</code>.</p><div class="informalexample"><pre class="programlisting">public class CameraOverlayWidget extends SurfaceView implements GestureDetector.OnGestureListener, SurfaceHolder.Callback {
    public static String TAG= "SFOCV::Overlay";
    protected Paint paintSafeExtents;
    protected Button btn;
    protected GestureDetector mGestureDetector;</pre></div><p class="calibre8">We've subclassed this from <code class="email">SurfaceView</code> to be able to render things on it. It also implements the gesture detector class so that we'll be able to monitor touch events on this widget.</p><div class="informalexample"><pre class="programlisting">    private long sizeWidth = 0, sizeHeight = 0;

    // Stuff required to paint the recording sign
    protected boolean mRecording = false;
    protected Paint paintRecordCircle;
    protected Paint paintRecordText;

    // Calibrate button
    private Paint paintCalibrateText;
    private Paint paintCalibrateTextOutline;

    private Paint paintTransparentButton;

    private RenderThread mPainterThread;
    private boolean bStopPainting = false;

    private Point3 omega;
    private Point3 drift;
    private Point3 theta;

    public static final double SAFETY_HORIZONTAL = 0.15;
    public static final double SAFETY_VERTICAL = 0.15;</pre></div><p class="calibre8">We define <a id="id677" class="calibre1"/>a bunch of variables to be used by the class. Some of them are <code class="email">Paint</code> objects – which are used by the <code class="email">SurfaceView</code> to render things. We've created different paints for the safety rectangle, the red recording circle, and the text.</p><p class="calibre8">Next, there are variables that describe the current state of the recorder. These variables answer questions like, is it currently recording? What's the size of the video? What is the latest gyro reading? We'll use these state variables to render the appropriate overlay.</p><p class="calibre8">We also define some safety fractions – the safety rectangle will have a margin of 0.15 on each edge.</p><div class="informalexample"><pre class="programlisting">    protected GestureDetector.OnGestureListener mCustomTouchMethods = null;
    protected OverlayEventListener mOverlayEventListener = null;</pre></div><p class="calibre8">And finally, we add a few event listeners – we'll use these to detect touches in specific areas of the overlay (we won't be using these though).</p><p class="calibre8">Let's look at the constructor for this class:</p><div class="informalexample"><pre class="programlisting">    public CameraOverlayWidget(Context ctx, AttributeSet attrs) {
        super(ctx, attrs);

        // Position at the very top and I'm the event handler
        setZOrderOnTop(true);
        getHolder().addCallback(this);

        // Load all the required objects
        initializePaints();

        // Setup the required handlers/threads
        mPainterThread = new RenderThread();
        mGestureDetector = new GestureDetector(ctx, this);
    }</pre></div><p class="calibre8">Here, we <a id="id678" class="calibre1"/>set up some basics when the object is initialized. We create the paint objects in <code class="email">initializePaints</code>, create a new thread for rendering the overlay and also create a gesture detector.</p><div class="informalexample"><pre class="programlisting">    /**
     * Initializes all paint objects.
     */
    protected void initializePaints() {
        paintSafeExtents = new Paint();
        paintSafeExtents.setColor(Color.WHITE);
        paintSafeExtents.setStyle(Paint.Style.STROKE);
        paintSafeExtents.setStrokeWidth(3);

        paintRecordCircle = new Paint();
        paintRecordCircle.setColor(Color.RED);
        paintRecordCircle.setStyle(Paint.Style.FILL);

        paintRecordText = new Paint();
        paintRecordText.setColor(Color.WHITE);
        paintRecordText.setTextSize(20);

        paintCalibrateText = new Paint();
        paintCalibrateText.setColor(Color.WHITE);
        paintCalibrateText.setTextSize(35);
        paintCalibrateText.setStyle(Paint.Style.FILL);

        paintCalibrateTextOutline = new Paint();
        paintCalibrateTextOutline.setColor(Color.BLACK);
        paintCalibrateTextOutline.setStrokeWidth(2);
        paintCalibrateTextOutline.setTextSize(35);
        paintCalibrateTextOutline.setStyle(Paint.Style.STROKE);

        paintTransparentButton = new Paint();
        paintTransparentButton.setColor(Color.BLACK);
        paintTransparentButton.setAlpha(128);
        paintTransparentButton.setStyle(Paint.Style.FILL);
    }</pre></div><p class="calibre8">As you can see, paints describe the physical attributes of the things to draw. For example, <code class="email">paintRecordCircle</code> is red and fills whatever shape we draw. Similarly, the record text shows up white with a text size of 20.</p><p class="calibre8">Now let's <a id="id679" class="calibre1"/>look at the <code class="email">RenderThread</code> class—the thing that does the actual drawing of the overlay. We start by defining the class itself and defining the <code class="email">run</code> method. The <code class="email">run</code> method is executed when the thread is spawned. On returning from this method, the thread stops.</p><div class="informalexample"><pre class="programlisting">    class RenderThread extends Thread {
        private long start = 0;
        @Override
        public void run() {
            super.run();

            start = SystemClock.uptimeMillis();

            while(!bStopPainting &amp;&amp; !isInterrupted()) {
                long tick = SystemClock.uptimeMillis();
                renderOverlay(tick);
            }
        }</pre></div><p class="calibre8">Now let's add the <code class="email">renderOverlay</code> method to <code class="email">RenderThread</code>. We start by getting a lock on the canvas and drawing a transparent color background. This clears anything that already exists on the overlay.</p><div class="informalexample"><pre class="programlisting">        /**
         * A renderer for the overlay with no state of its own.
         * @returns nothing
         */
        public void renderOverlay(long tick) {
            Canvas canvas = getHolder().lockCanvas();

            long width = canvas.getWidth();
            long height = canvas.getHeight();

            // Clear the canvas
            canvas.drawColor(Color.TRANSPARENT, PorterDuff.Mode.CLEAR);</pre></div><p class="calibre8">Now, we draw the safety bounds of the camera view. While stabilizing the video, we'll inevitably have to crop certain parts of the image. The safe lines mark this boundary. In our case, we take a certain percentage of the view as safe.</p><div class="informalexample"><pre class="programlisting">            // Draw the bounds
            long lSafeW = (long)(width * SAFETY_HORIZONTAL);
            long lSafeH = (long)(height * SAFETY_VERTICAL);
            canvas.drawRect(lSafeW, lSafeH, width-lSafeW, height-lSafeH, paintSafeExtents);</pre></div><p class="calibre8">If we're<a id="id680" class="calibre1"/> recording, we want to blink the red recording circle and the recording text. We do this by taking the current time and the start time.</p><div class="informalexample"><pre class="programlisting">            if(mRecording) {
                // Render this only on alternate 500ms intervals
                if(((tick-start) / 500) % 2 == 1) {
                    canvas.drawCircle(100, 100, 20, paintRecordCircle);
                    final String s = "Recording";
                    canvas.drawText(s, 0, s.length(), 130, 110, paintRecordText);
                }
            }</pre></div><p class="calibre8">Now we draw a button that says "Record" on it.</p><div class="informalexample"><pre class="programlisting">            canvas.drawRect((float)(1-SAFETY_HORIZONTAL)*sizeWidth, (float)(1-SAFETY_VERTICAL)*sizeHeight, sizeWidth , sizeHeight, paintTransparentButton);

            final String strCalibrate = "Calibrate";
            canvas.drawText(strCalibrate, 0, strCalibrate.length(), width-200, height-200, paintCalibrateText);
            canvas.drawText(strCalibrate, 0, strCalibrate.length(), width-200, height-200, paintCalibrateTextOutline);</pre></div><p class="calibre8">While recording the video, we will also display some useful information—the current angular velocity and estimated angle. You can verify if the algorithm is working as expected or not.</p><div class="informalexample"><pre class="programlisting">            if(omega!=null) {
                final String strO = "O: ";
                canvas.drawText(strO, 0, strO.length(), width - 200, 200, paintCalibrateText);
                String strX = Math.toDegrees(omega.x) + "";
                String strY = Math.toDegrees(omega.y) + "";
                String strZ = Math.toDegrees(omega.z) + "";
                canvas.drawText(strX, 0, strX.length(), width - 200, 250, paintCalibrateText);
                canvas.drawText(strY, 0, strY.length(), width - 200, 300, paintCalibrateText);
                canvas.drawText(strZ, 0, strZ.length(), width - 200, 350, paintCalibrateText);
            }

            if(theta!=null) {
                final String strT = "T: ";
                canvas.drawText(strT, 0, strT.length(), width - 200, 500, paintCalibrateText);
                String strX = Math.toDegrees(theta.x) + "";
                String strY = Math.toDegrees(theta.y) + "";
                String strZ = Math.toDegrees(theta.z) + "";
                canvas.drawText(strX, 0, strX.length(), width - 200, 550, paintCalibrateText);
                canvas.drawText(strY, 0, strY.length(), width - 200, 600, paintCalibrateText);
                canvas.drawText(strZ, 0, strZ.length(), width - 200, 650, paintCalibrateText);
            }</pre></div><p class="calibre8">And, with this, the render overlay method is complete!</p><div class="informalexample"><pre class="programlisting">            // Flush out the canvas
            getHolder().unlockCanvasAndPost(canvas);
        }
    }</pre></div><p class="calibre8">This class <a id="id681" class="calibre1"/>can be used to spawn a new thread and this thread simply keeps the overlay updated. We've added a special logic for the recording circle so that it makes the red circle blink.</p><p class="calibre8">Next, let's look at some of the supporting functions in <code class="email">CameraOverlayWidget</code>.</p><div class="informalexample"><pre class="programlisting">    public void setRecording() {
        mRecording = true;
    }

    public void unsetRecording() {
        mRecording = false;
    }</pre></div><p class="calibre8">Two simple set and unset methods enable or disable the red circle.</p><div class="informalexample"><pre class="programlisting">    @Override
    public void onSizeChanged(int w,int h,int oldw,int oldh) {
        super.onSizeChanged(w, h, oldw, oldh);

        sizeWidth = w;
        sizeHeight = h;
    }</pre></div><p class="calibre8">If the size of the widget changes (we'll be setting it fullscreen on the preview pane), we should know about it and capture the size in these variables. This will affect the positioning of the various elements and the safety rectangle.</p><div class="informalexample"><pre class="programlisting">    public void setCustomTouchMethods(GestureDetector.SimpleOnGestureListener c){
        mCustomTouchMethods = c;
    }

    public void setOverlayEventListener(OverlayEventListener listener) {
        mOverlayEventListener = listener;
    }</pre></div><p class="calibre8">We also<a id="id682" class="calibre1"/> have a few set methods that let you change the values to be displayed on the overlay.</p><div class="informalexample"><pre class="programlisting">    public void setOmega(Point3 omega) {
        this.omega = omega;
    }

    public void setDrift(Point3 drift) {
        this.drift = drift;
    }

    public void setTheta(Point3 theta) {
        this.theta = theta;
    }</pre></div><p class="calibre8">There are other functions that can be used to modify the overlay being displayed. These functions set the gyroscope values.</p><p class="calibre8">Now, let's look at some Android-specific lifecycle events such as pause, resume, and so on.</p><div class="informalexample"><pre class="programlisting">    /**
     * This method is called during the activity's onResume. This ensures a wakeup
     * re-instantiates the rendering thread.
     */
    public void resume() {
        bStopPainting = false;
        mPainterThread = new RenderThread();
    }

    /**
     * This method is called during the activity's onPause method. This ensures
     * going to sleep pauses the rendering.
     */
    public void pause() {
        bStopPainting = true;

        try {
            mPainterThread.join();
        }
        catch(InterruptedException e) {
            e.printStackTrace();
        }
        mPainterThread = null;
    }</pre></div><p class="calibre8">These two <a id="id683" class="calibre1"/>methods ensure we're not using processor cycles when the app isn't in the foreground. We simply stop the rendering thread if the app goes to a paused state and resume painting when it's back.</p><div class="informalexample"><pre class="programlisting">    @Override
    public void surfaceCreated(SurfaceHolder surfaceHolder) {
        getHolder().setFormat(PixelFormat.RGBA_8888);

        // We created the thread earlier - but we should start it only when
        // the surface is ready to be drawn on.
        if(mPainterThread != null &amp;&amp; !mPainterThread.isAlive()) {
            mPainterThread.start();
        }
    }

    @Override
    public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
        // Required for implementation
    }
    @Override
    public void surfaceDestroyed(SurfaceHolder holder) {
        // Required for implementation
    }</pre></div><p class="calibre8">When the surface is created, we set up the pixel format (we want it to be transparent, we make the surface of the type RGBA). Also, we should spawn a new thread to get the overlay rendering going.</p><p class="calibre8">With that, we're almost ready with our overlay display. One last thing remains—responding to touch events. Let's do that now:</p><div class="informalexample"><pre class="programlisting">    @Override
    public boolean onTouchEvent(MotionEvent motionEvent) {
        boolean result = mGestureDetector.onTouchEvent(motionEvent);
        return result;
    }

    @Override
    public boolean onDown(MotionEvent motionEvent) {
        MotionEvent.PointerCoords coords =new MotionEvent.PointerCoords();

        motionEvent.getPointerCoords(0, coords);

        // Handle these only if there is an event listener
        if(mOverlayEventListener!=null) {
            if(coords.x &gt;= (1-SAFETY_HORIZONTAL)*sizeWidth &amp;&amp;coords.x&lt;sizeWidth &amp;&amp;
               coords.y &gt;= (1-SAFETY_VERTICAL)*sizeHeight &amp;&amp;coords.y&lt;sizeHeight) {
                return mOverlayEventListener.onCalibrate(motionEvent);
            }
        }

        // Didn't match? Try passing a raw event - just in case
        if(mCustomTouchMethods!=null)
            return mCustomTouchMethods.onDown(motionEvent);

        // Nothing worked - let it bubble up
        return false;
    }

    @Override
    public void onShowPress(MotionEvent motionEvent) {
        if(mCustomTouchMethods!=null)
            mCustomTouchMethods.onShowPress(motionEvent);
    }

    @Override
    public boolean onFling(MotionEvent motionEvent,MotionEvent motionEvent2,float v, float v2) {
        Log.d(TAG, "onFling");

        if(mCustomTouchMethods!=null)
            return mCustomTouchMethods.onFling(motionEvent,motionEvent2,v, v2);

        return false;
    }

    @Override
    public void onLongPress(MotionEvent motionEvent) {
        Log.d(TAG, "onLongPress");

        if(mCustomTouchMethods!=null)
            mCustomTouchMethods.onLongPress(motionEvent);
    }

    @Override
    public boolean onScroll(MotionEvent motionEvent,MotionEvent motionEvent2,float v, float v2) {
        Log.d(TAG, "onScroll");

        if(mCustomTouchMethods!=null)
            return mCustomTouchMethods.onScroll(motionEvent,motionEvent2,v, v2);

        return false;
    }

    @Override
    public boolean onSingleTapUp(MotionEvent motionEvent) {
        Log.d(TAG, "onSingleTapUp");

        if(mCustomTouchMethods!=null)
            return mCustomTouchMethods.onSingleTapUp(motionEvent);

        return false;
    }</pre></div><p class="calibre8">These<a id="id684" class="calibre1"/> functions do nothing but pass on events to the event listener, if there is any. We're responding to the following events: <code class="email">onTouchEvent</code>, <code class="email">onDown</code>, <code class="email">onShowPress</code>, <code class="email">onFlight</code>, <code class="email">onLongPress</code>, <code class="email">onScroll</code>, <code class="email">onSingleTapUp</code>.</p><p class="calibre8">One final piece of code remains for the overlay class. We've used something called <code class="email">OverlayEventListener</code> at certain places in the class but have not yet defined it. Here's what it looks like:</p><div class="informalexample"><pre class="programlisting">    public interface OverlayEventListener {
        public boolean onCalibrate(MotionEvent e);
    }
}</pre></div><p class="calibre8">With this <a id="id685" class="calibre1"/>defined, we will now be able to create event handlers for specific buttons being touched on the overlay (the calibrate and record buttons).</p></div></div>

<div class="book" title="Android specifics">
<div class="book" title="Reading media files"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec76" class="calibre1"/>Reading media files</h2></div></div></div><p class="calibre8">Once <a id="id686" class="calibre1"/>you've written the media file, you need a mechanism to read individual frames from the movie. We can use Android's Media Decoder to extract frames and convert them into OpenCV's native Mat data structure. We'll start by creating a new class called <code class="email">SequentialFrameExtractor</code>.</p><p class="calibre8">Most of this section is based on Andy McFadden's tutorial on using the MediaCodec at bigflake.com.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note133" class="calibre1"/>Note</h3><p class="calibre8">As mentioned earlier, you don't need this class to get through this chapter's project. If you decide to write an Android app that reads media files, this class should get you started. Feel free to skip this if you like!</p><p class="calibre8">We will be using the Android app only to record the video and gyro signals.</p></div><div class="informalexample"><pre class="programlisting">public class SequentialFrameExtractor {
    private String mFilename = null;
    private CodecOutputSurface outputSurface = null;
    private MediaCodec decoder = null;

    private FrameAvailableListener frameListener = null;

    private static final int TIMEOUT_USEC = 10000;
    private long decodeCount = 0;
}</pre></div><p class="calibre8"><code class="email">mFilename</code> is the name of the file that's being read, it should only be set when an object of <code class="email">SequentialFrameExtractor</code> is created. <code class="email">CodecOutputSurface</code> is a construct borrowed <a id="id687" class="calibre1"/>from <a class="calibre1" href="http://bigflake.com">http://bigflake.com</a> that encapsulates logic to render a frame using OpenGL and fetches raw bytes for us to use. It is available on the website and also in the accompanying code. The next is <code class="email">MediaCodec</code>—Android's way of letting you access the decoding pipeline.</p><p class="calibre8"><code class="email">FrameAvailableListener</code> is an interface we'll create in just a moment. It allows us to respond whenever a frame becomes available.</p><p class="calibre8">What is <code class="email">TIMEOUT_USEC</code> and <code class="email">decodeCount</code>?</p><div class="informalexample"><pre class="programlisting">    private long decodeCount = 0;
    public SequentialFrameExtractor(String filename) {
        mFilename = filename;
    }

    public void start() {
        MediaExtractor mediaExtractor = new MediaExtractor();
        try {
            mediaExtractor.setDataSource(mFilename);
        } catch(IOException e) {
            e.printStackTrace();
        }
    }</pre></div><p class="calibre8">We've <a id="id688" class="calibre1"/>created a constructor and a new <code class="email">start</code> method. The <code class="email">start</code> method is when the decoding begins and it will start firing the <code class="email">onFrameAvailable</code> method as new frames become available.</p><div class="informalexample"><pre class="programlisting">            e.printStackTrace();
        }
        MediaFormat format = null;
        int numTracks = mediaExtract.getTrackCount();
        int track = -1;
        for(int i=0;i&lt;numTracks;i++) {
            MediaFormat fmt = mediaExtractor.getTrackFormat(i);
            String mime = fmt.getString(MediaFormat.KEY_MIME);
            if(mime.startswith("video/")) {
                mediaExtractor.selectTrack(i);
                track = i;
                format = fmt;
                break;
            }
        }
        if(track==-1) {
            // Did the user select an audio file?
        }</pre></div><p class="calibre8">Here, we loop over all the tracks available in the given file (audio, video, and so on) and identify a video track to work with. We're assuming this is a mono-video file, so we should be good to select the first video track that shows up.</p><p class="calibre8">With the track selected, we can now start the actual decoding process. Before that, we must set up a decoding surface and some buffers. The way MediaCodec works is that it keeps accumulating data into a buffer. Once it accumulates an entire frame, the data is passed onto a surface to be rendered.</p><div class="informalexample"><pre class="programlisting">        int frameWidth = format.getInteger(MediaFormat.KEY_WIDTH);
        int frameHeight = format.getInteger(MediaFormat.KEY_HEIGHT);
        outputSurface = new CodecOutputSurface(frameWidth,frameHeight);

        String mime = format.getString(MediaFormat.KEY_MIME);
        decoder = MediaCodec.createDecoderByType(mime);
        decoder.configure(format,outputSurface.getSurface(),null,0);
        decoder.start();
        
        ByteBuffer[] decoderInputBuffers =
                                       decoder.getInputBuffers();
        MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
        int inputChunk = 0;
        boolean outputDone = false, inputDone = false;
        long presentationTimeUs = 0;</pre></div><p class="calibre8">With<a id="id689" class="calibre1"/> the initial setup done, we now get into the decoding loop:</p><div class="informalexample"><pre class="programlisting">        while(!outputDone) {
            if(!inputDone) {
                int inputBufIndex =decoder.dequeueInputBuffer(TIMEOUT_USEC);
                if(inputBufIndex &gt;= 0) {
                    ByteBuffer inputBuf =decoderInputBuffers[inputBufIndex];
                    int chunkSize = mediaExtractor.readSampleData(inputBuf, 0);
                    if(chunkSize &lt; 0) {
                        decoder.queueInputBuffer(inputBufIndex,0, 0, 0L,
                               mediaCodec.BUFFER_FLAG_END_OF_STREAM);
                        inputDone = true;
                    } else {
                        if(mediaExtractor.getSampleTrackIndex()!= track) {
                            // We somehow got data that did not
                            // belong to the track we selected
                        }
                        presentationTimeUs =mediaExtractor.getSampleTime();
                        decoder.queueInputBuffer(inputBufIndex,0, chunkSize,                                                                                                                                                           presentationTimeUs, 0);
                        inputChunk++;
                        mediaExtractor.advance();
                    }
                }    
            } else {
                // We shouldn't reach here – inputDone, protect us
            }
        }</pre></div><p class="calibre8">This is<a id="id690" class="calibre1"/> the input half of the media extraction. This loop reads the file and queues chunks for the decoder. As things get decoded, we need to route it to the places we need:</p><div class="informalexample"><pre class="programlisting">            } else {
                // We shouldn't reach here – inputDone, protect us
            }            
            if(!outputDone) {
                int decoderStatus = decoder.dequeueOutputBuffer(
                                              info, TIMEOUT_USEC);
                if(decoderStatus ==
                                MediaCodec.INFO_TRY_AGAIN_LATER) {
                    // Can't do anything here
                } else if(decoderStatus ==
                         MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                    // Not important since we're using a surface
                } else if(decoderStatus ==
                          MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                    MediaFormat newFormat = decoder.getOutputFormat();
                    // Handled automatically for us
                } else if(decoderStatus &lt; 0) {
                    // Something bad has happened
                } else {
                    if((info.flags &amp; MediaCodec.BUFFER_FLAG_END_OF_STREAM) != ) {
                        outputDone = true;
                    }
                }

                boolean doRender = (info.size != 0);
                decoder.releaseOutputBuffer(decoderStatus, doRender);
                if(doRender) {
                    outputSurface.awaitNewImage();
                    outputSurface.drawImage(true);

                    try {
                        Mat img = outputSurface.readFrameAsMat();
                        if(frameListener != null) {
                            Frame frame = new Frame(img, presentationTimeUs,
                                                    new Point3(), new Point());
                            frameListener.onFrameAvailable(frame);
                        }
                    } catch(IOException e) {
                        e.printStackTrace();
                    }
                    decodeCount++;
                    if(frameListener!=null)
                        frameListener.onFrameComplete(decodeCount);
                }
            }
        }
        
        medaiExtractor.release();
        mediaExtractor = null;
    }</pre></div><p class="calibre8">This<a id="id691" class="calibre1"/> completes the output half of the decode loop. Whenever a frame is complete, it converts the raw data into a Mat structure and creates a new <code class="email">Frame</code> object. This is then passed to the <code class="email">onFrameAvailable</code> method.</p><p class="calibre8">Once the decoding is complete, the media extractor is released and we're done!</p><p class="calibre8">The only thing left is to define what <code class="email">FrameAvailableListener</code> is. We shall do that now:</p><div class="informalexample"><pre class="programlisting">    public void setFrameAvailableListener(FrameAvailableListener listener) {
        frameListener = listener;
    }

    public interface FrameAvailableListener {
        public void onFrameAvailable(Frame frame);
        public void onFrameComplete(long frameDone);
    }
}</pre></div><p class="calibre8">This is a common pattern in Java when defining such listeners. The listeners contain methods that are fired on specific events (in our case, when a frame is available, or when the processing<a id="id692" class="calibre1"/> of a frame is complete).</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Calibration"><div class="book" id="1TVKI2-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec59" class="calibre1"/>Calibration</h1></div></div></div><p class="calibre8">In the<a id="id693" class="calibre1"/> section that discusses the mathematical basis, we found several unknown camera parameters. These parameters need to be figured out so we can process each image and stabilize it. As with any calibration process, we need to use a predefined scene. Using this scene and a relative handshake, we will try to estimate the unknown parameters.</p><p class="calibre8">The <a id="id694" class="calibre1"/>unknown parameters are:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Focal length of the lens</li><li class="listitem">Delay between gyroscope and frame timestamps</li><li class="listitem">Bias in the gyroscope</li><li class="listitem">Duration of the rolling shutter</li></ul></div><p class="calibre8">It is often possible to detect the focal length of a phone camera (in terms of millimeters) using the platform API (<code class="email">getFocalLength()</code> for Android). However, we're interested in the camera space focal length. This number is a product of the physical focal length and a conversion ratio that depends on the image resolution and the physical size of the camera sensor, which might differ across cameras. It is also possible to find the conversion ratio by trigonometry if the field of view (<code class="email">getVerticalViewAngle()</code> and <code class="email">getHorizontalViewAngle()</code> for Android) is  known for a sensor and lens setup. However, we'll just leave it as an unknown and let the calibration find it for us.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note134" class="calibre1"/>Note</h3><p class="calibre8">If you're interested in more information on this, refer to <a class="calibre1" title="Chapter 5. Generic Object Detection for Industrial Applications" href="part0043_split_000.html#190862-940925703e144daa867f510896bffb69">Chapter 5</a>, <span class="strong"><em class="calibre10">Combining Image Tracking with 3D Rendering</em></span>, of PacktPub's <span class="strong"><em class="calibre10">Android Application Programming with OpenCV</em></span>.</p></div><p class="calibre8">We need to estimate the delay between gyro and frame timestamps to improve the quality of the output on sharp turns. This also offsets any lag introduced by the phone when recording the video.</p><p class="calibre8">Rolling shutter effects are visible at high speed and the estimated parameter tries to correct these.</p><p class="calibre8">It is possible to calculate these parameters with a short clip that's shaky. We use a feature detector from OpenCV to do this.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note135" class="calibre1"/>Note</h3><p class="calibre8">During this<a id="id695" class="calibre1"/> phase, we'll be using Python. The SciPy library provides us with mathematical functions that we can use out of the box. It is possible to implement these on your own, but that would require a more in-depth explanation of how mathematical optimization works. Along with this, we'll use Matplotlib to generate graphs.</p></div></div>

<div class="book" title="Calibration">
<div class="book" title="Data structures"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec77" class="calibre1"/>Data structures</h2></div></div></div><p class="calibre8">We'll set <a id="id696" class="calibre1"/>up three key data structures: first, the unknown<a id="id697" class="calibre1"/> parameters, second, something to read the gyro data file generated by the Android app, and third, a representation of the video being processed.</p><p class="calibre8">The first structure is to store the estimates from the calibration. It contains four values:</p><div class="book"><ul class="itemizedlist"><li class="listitem">An estimate of the focal length of the camera (in camera units, not physical units)</li><li class="listitem">The delay between the gyroscope timestamps and the frame timestamps</li><li class="listitem">The gyroscope bias</li><li class="listitem">The rolling shutter estimated</li></ul></div><p class="calibre8">Let's start by creating a new file called <code class="email">calibration.py</code>.</p><div class="informalexample"><pre class="programlisting">import sys, numpy

class CalibrationParameters(object):
    def __init__(self):
        self.f = 0.0
        self.td = 0.0
        self.gb = (0.0, 0.0, 0.0)
        self.ts = 0.0</pre></div><div class="book" title="Reading the gyroscope trace"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec37" class="calibre1"/>Reading the gyroscope trace</h3></div></div></div><p class="calibre8">Next, we'll <a id="id698" class="calibre1"/>define a class to read in the <code class="email">.csv</code> file generated by the Android app.</p><div class="informalexample"><pre class="programlisting">class GyroscopeDataFile(object):
    def __init__(self, filepath):
        self.filepath = filepath
        self.omega = {}

    def getfile_object(self):
        return open(self.filepath)</pre></div><p class="calibre8">We initialize the class with two main variables: the file path to read, and a dictionary of angular velocities. This dictionary will store mappings between the timestamp and the angular velocity at that instant. We'll eventually need to calculate actual angles from the angular<a id="id699" class="calibre1"/> velocity, but that will happen outside this class.</p><p class="calibre8">Now we add the <code class="email">parse</code> method. This method will actually read the file and populate the Omega dictionary.</p><div class="informalexample"><pre class="programlisting">    def parse(self):
        with self._get_file_object() as fp:
            firstline = fp.readline().strip()
            if not firstline == 'utk':
                raise Exception("The first line isn't valid")</pre></div><p class="calibre8">We validate that the first line of the csv file matches our expectation. If not, the csv file was probably not compatible and will error out over the next few lines.</p><div class="informalexample"><pre class="programlisting">            for line in fp.readlines():
                line = line.strip()
                parts = line.split(",")</pre></div><p class="calibre8">Here, we initiate a loop over the entire file. The <code class="email">strip</code> function removed any additional whitespace (tabs, spaces, newline characters, among others) that might be stored in the file.</p><p class="calibre8">After removing the whitespace, we split the string with commas (this is a comma-separated file!).</p><div class="informalexample"><pre class="programlisting">                timestamp = int(parts[3])
                ox = float(parts[0])
                oy = float(parts[1])
                oz = float(parts[2])
                print("%s: %s, %s, %s" % (timestamp,
                                          ox,
                                          oy,
                                          oz))
                self.omega[timestamp] = (ox, oy, oz)
    return</pre></div><p class="calibre8">Information read from the file is plain strings so we convert that into the appropriate numeric type and store it in <code class="email">self.omega</code>. We're now ready to parse the csv files and get started with numeric calculations.</p><p class="calibre8">Before we do that, we'll define a few more useful functions.</p><div class="informalexample"><pre class="programlisting">    def get_timestamps(self):
        return sorted(self.omega.keys())

    def get_signal(self, index):
        return [self.omega[k][index] for k in self.get_timestamps()]</pre></div><p class="calibre8">The <code class="email">get_timestamps</code> method on this class will return a sorted list of timestamps. Building <a id="id700" class="calibre1"/>on this, we also define a function called <code class="email">get_signal</code>. The angular velocity is composed of three signals. These signals are packed together in <code class="email">self.omega</code>. The <code class="email">get_signal</code> function lets us extract a specific component of the signal.</p><p class="calibre8">For example, <code class="email">get_signal(0)</code> returns the X component of angular velocity.</p><div class="informalexample"><pre class="programlisting">    def get_signal_x(self):
        return self.get_signal(0)

    def get_signal_y(self):
        return self.get_signal(1)

    def get_signal_z(self):
        return self.get_signal(2)</pre></div><p class="calibre8">These utility functions return only the specific signal we're looking at. We'll be using these signals to smooth out individual signals, calculate the angle, and so on.</p><p class="calibre8">Another issue we need to address is that the timestamps are discrete. For example, we might have angular velocities at timestamp N and the next reading might exist at N+500000 (remember, the timestamps are in nanoseconds). However, the video file might have a frame at N+250000. We need a way to interpolate between two angular velocity readings.</p><p class="calibre8">We'll use simple linear interpolation to estimate the angular velocity at any given moment.</p><div class="informalexample"><pre class="programlisting">    def fetch_approximate_omega(self, timestamp):
        if timestamp in self.omega:
            return self.omega[timestamp]</pre></div><p class="calibre8">This method takes in a timestamp and returns the estimated angular velocity. If the exact timestamp already exists, there is no estimation to do.</p><div class="informalexample"><pre class="programlisting">        i = 0
        sorted_timestamps = self.get_timestamps()
        for ts in sorted_timestamps:
            if  ts &gt; timestamp:
                break
            i += 1</pre></div><p class="calibre8">Here we're walking over the timestamps and finding the timestamp that is closest to the one requested.</p><div class="informalexample"><pre class="programlisting">        t_previous = sorted_timestamps[i]
        t_current = sorted_timestamps[i+1]
        dt = float(t_current – t_previous)
        slope = (timestamp – t_previous) / dt

        est_x = self.omega[t_previous][0]*(1-slope) + self.omega[t_current][0]*slope
        est_y = self.omega[t_previous][1]*(1-slope) + self.omega[t_current][1]*slope
        est_z = self.omega[t_previous][2]*(1-slope) + self.omega[t_current][2]*slope
        return (est_x, est_y, est_z)</pre></div><p class="calibre8">Once we<a id="id701" class="calibre1"/> have the two closest timestamps (<code class="email">i</code> and <code class="email">i+1</code> in the list <code class="email">sorted_timestamps</code>), we're ready to start linear interpolation. We calculate the estimated X, Y, and Z angular velocities and return these values.</p><p class="calibre8">This finishes our work on reading the gyroscope file!</p></div><div class="book" title="The training video"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec38" class="calibre1"/>The training video</h3></div></div></div><p class="calibre8">We'll also <a id="id702" class="calibre1"/>create a new class that lets us treat the entire video sequence as a single entity. We'll extract useful information from the video in a single pass and store it for future reference, making our code both faster and more memory efficient.</p><div class="informalexample"><pre class="programlisting">class GyroVideo(object):def__init__(self, mp4):
        self.mp4 = mp4
        self.frameInfo = []
        self.numFrames = 0
        self.duration = 0
        self.frameWidth = 0
        self.frameHeight = 0</pre></div><p class="calibre8">We initialize the class with some variables we'll be using throughout. Most of the variables are self-explanatory. <code class="email">frameInfo</code> stores details about every frame—like the timestamp of a given frame and keypoints (useful for calibration).</p><div class="informalexample"><pre class="programlisting">    def read_video(self, skip_keypoints=False):
        vidcap = cv2.VideoCapture(self.mp4)
        success, frame = vidcap.read()
        prev_frame = None
        previous_timestamp = 0
        frameCount = 0</pre></div><p class="calibre8">We define a new method that will do all the heavy lifting for us. We start by creating the OpenCV video reading object (<code class="email">VideoCapture</code>) and try to read a single frame.</p><div class="informalexample"><pre class="programlisting">        while success:
            current_timestamp = vidcap.get(0) * 1000 * 1000
            print "Processing frame#%d (%f ns)" % (frameCount, current_timestamp)</pre></div><p class="calibre8">The <code class="email">get</code> method<a id="id703" class="calibre1"/> on a <code class="email">VideoCapture</code> object returns information about the video sequence. Zero (0) happens to be the constant for fetching the timestamp in milliseconds. We convert this into nanoseconds and print out a helpful message!</p><div class="informalexample"><pre class="programlisting">            if not prev_frame:
                self.frameInfo.append({'keypoints': None,
                                       'timestamp': current_timestamp})
                prev_frame = frame
                previous_timestamp = current_timestamp
                continue</pre></div><p class="calibre8">If this is the first frame being read, we won't have a previous frame. We're also not interested in storing keypoints for the first frame. So we just move on to the next frame.</p><div class="informalexample"><pre class="programlisting">            if skip_keypoints:
                self.frameInfo.append({'keypoints': None,
                                       'timestamp': current_timestamp})
                continue</pre></div><p class="calibre8">If you set the <code class="email">skip_keypoints</code> parameter to <code class="email">true</code>, it'll just store the timestamps of each frame. You might then use this parameter to read a video after you've already calibrated your device and already have the values of the various unknowns.</p><div class="informalexample"><pre class="programlisting">            old_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
            new_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

            old_corners = cv2.goodFeaturesToTrack(old_gray, 1000, 0.3, 30)</pre></div><p class="calibre8">We convert the previous and the current frame into grayscale and extract some good features to track. We'll use these features and track them in the new frame. This gives us a visual estimate of how the orientation of the camera changed. We already have the gyroscope data for this; we just need to calibrate some unknowns. We achieve this by using the visual estimate.</p><div class="informalexample"><pre class="programlisting">            if old_corners == None:
                self.frameInfo.append({'keypoints': None,
                                       'timestamp': current_timestamp})
                frameCount += 1
                previous_timestamp = current_timestamp
                prev_frame = frame
                success, frame = vidcap.read()
                continue</pre></div><p class="calibre8">If no corners were found in the old frame, that's not a good sign. Was it a very blurry frame? Were there no good features to track? So we simply skip processing it.</p><p class="calibre8">If we <a id="id704" class="calibre1"/>did find keypoints to track, we use optical flow to identify where they are in the new frame:</p><div class="informalexample"><pre class="programlisting">            new_corners, status, err = cv2.calcOpticalFlowPyrLK(old_gray,
                                           new_gray,
                                           old_corners,
                                           None,
                                           winSize=(15,15)
                                           maxLevel=2,
                                           criteria=(cv2.TERM_CRITERIA_EPS
                                                     | cv2.TERM_CRITERIA_COUNT,
                                                     10, 0.03))</pre></div><p class="calibre8">This gives us the position of the corners in the new frame. We can then estimate the motion that happened between the previous frame and the current, and correlate it with the gyroscope data.</p><p class="calibre8">A common issue with <code class="email">goodFeaturesToTrack</code> is that the features aren't robust. They often move around, losing the position they were tracking. To get around this, we add another test just to ensure such random outliers don't make it to the calibration phase. This is done with the help of RANSAC.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note136" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre9">RANSAC</strong></span> stands for <span class="strong"><strong class="calibre9">Ran</strong></span>dom <span class="strong"><strong class="calibre9">Sa</strong></span>mple <span class="strong"><strong class="calibre9">C</strong></span>onsensus. The key idea of RANSAC is that the <a id="id705" class="calibre1"/>given dataset contains a set of inliers that fit perfectly to a given model. It gives you the set of points that most closely satisfy a given constraint. In our case, these inliers would account for the points moving from one set of positions to another. It does not matter how numerous the outliers of the data set are.</p></div><p class="calibre8">OpenCV comes with a utility function to calculate the perspective transform between two frames. While the transform is being estimated, the function also tries to figure out which points are outliers. We'll hook into this functionality for our purposes too!</p><div class="informalexample"><pre class="programlisting">            if len(old_corners) &gt; 4:
                homography, mask = cv2.findHomography(old_corners, new_corners,
                                                      cv2.RANSAC, 5.0)
                mask = mask.ravel()
                new_corners_homography = [new_corners[i] for i in xrange(len(mask)) if mask[i] == 1])
                old_corners_homography = [old_corners[i] for i in xrange(len(mask)) if mask[i] == 1])
                new_corners_homography = numpy.asarray(new_corners_homography)
                old_corners_homography = numpy.asarray(old_corners_homography)
            else:
                new_corners_homography = new_corners
                old_corners_homography = old_corners</pre></div><p class="calibre8">We <a id="id706" class="calibre1"/>need at least four keypoints to calculate the perspective transform between two frames. If there aren't enough points, we just store whatever we have. We get a better result if there are more points and some are eliminated.</p><div class="informalexample"><pre class="programlisting">            self.frameInfo.append({'keypoints': (old_corners_homography,
                                                 new_corners_homography),
                                   'timestamp': current_timestamp})
            frameCount += 1
            previous_timestamp = current_timestamp
            prev_frame = frame
            success, frame = vidcap.read()
        self.numFrames = frameCount
        self.duration = current_timstamp
        return</pre></div><p class="calibre8">Once we have everything figured out, we just store it in the frame information list. And this marks the end of our method!</p></div></div></div>

<div class="book" title="Calibration">
<div class="book" title="Handling rotations"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec78" class="calibre1"/>Handling rotations</h2></div></div></div><p class="calibre8">Let's <a id="id707" class="calibre1"/>take a look at how we rotate frames to stabilize them.</p><div class="book" title="Rotating an image"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec39" class="calibre1"/>Rotating an image</h3></div></div></div><p class="calibre8">Before <a id="id708" class="calibre1"/>we get into how images can be rotated for our project, let's look at rotating images in general. The goal is to produce images like the one below:</p><div class="mediaobject"><img src="../images/00131.jpeg" alt="Rotating an image" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Rotating an image in 2D is simple, there's only one axis. A 2D rotation can be achieved by using an affine transform.</p><div class="mediaobject"><img src="../images/00132.jpeg" alt="Rotating an image" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">In our<a id="id709" class="calibre1"/> project, we need to rotate images around all three axes. An affine transform is not sufficient to produce these, so we need to go towards a perspective transform. Also, rotations are linear transformations; this means we can split an arbitrary rotation into its component X, Y, and Z rotations and use that to compose the rotation.</p><p class="calibre8">To achieve this, we'll use the OpenCV Rodrigues function call to generate these transformation matrices. Let's start by writing a function that arbitrarily rotates an image.</p><div class="informalexample"><pre class="programlisting">def rotateImage(src, rx, ry, rz, f, dx=0, dy=0, dz=0, convertToRadians=False):
    if convertToRadians:
        rx = (rx) * math.pi / 180
        ry = (ry) * math.pi / 180
        rz = (rz) * math.pi / 180
    
    rx = float(rx)
    ry = float(ry)
    rz = float(rz)</pre></div><p class="calibre8">This method accepts a source image that needs to be rotated, the three rotation angles, an optional translation, the focal length in pixels, and whether the angles are in radians or not. If the angles are in degrees, we need to convert them to radians. We also force convert these into <code class="email">float</code>.</p><p class="calibre8">Next, we'll calculate the width and the height of the source image. These, along with the focal length, are used to transform the rotation matrix (which is in real world space) into image space.</p><div class="informalexample"><pre class="programlisting">    w = src.shape[1]
    h = src.shape[0]</pre></div><p class="calibre8">Now, we use the Rodrigues function to generate the rotation matrix:</p><div class="informalexample"><pre class="programlisting">    smallR = cv2.Rodrigues(np.array([rx, ry, rz]))[0]
    R = numpy.array([ [smallR[0][0], smallR[0][1], smallR[0][2], 0],
                      [smallR[1][0], smallR[1][1], smallR[1][2], 0],
                      [smallR[2][0], smallR[2][1], smallR[2][2], 0],
                      [0,            0,            0,            1]])</pre></div><p class="calibre8">The Rodrigues <a id="id710" class="calibre1"/>function takes a vector (a list) that contains the three rotation angles and returns the rotation matrix. The matrix returned is a 3x3 matrix. We'll convert that into a 4x4 homogeneous matrix so that we can apply transformations to it.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note137" class="calibre1"/>Note</h3><p class="calibre8">It's usually a good idea to keep dz equal to the focal length. This implies that the image was captured at just the right focal length and needs to be rotated about that point. You are free to change dz to other values, but usually setting it equal to F gives good results.</p></div><p class="calibre8">We now apply a simple translation to the matrix. The translation matrix is easily evaluated as follows:</p><div class="informalexample"><pre class="programlisting">    x = numpy.array([[1.0, 0,   0,   dx],
                     [0,   1.0, 0,   dy],
                     [0,   0,   1.0, dz],
                     [0,   0,   0,   1]])
    T = numpy.asmatrix(x)</pre></div><p class="calibre8">Until now, all transformations have happened in world space. We need to change these into image space. This is accomplished by the simple pinhole model of a camera.</p><div class="informalexample"><pre class="programlisting">    c = numpy.array([[f, 0, w/2, 0],
                     [0, f, h/2, 0],
                     [0, 0, 1,   0]])
    cameraMatrix = numpy.asmatrix(c)</pre></div><p class="calibre8">Combining these transforms is straightforward:</p><div class="informalexample"><pre class="programlisting">    transform = cameraMatrix * (T*R)</pre></div><p class="calibre8">This matrix can now be used in OpenCV's <code class="email">warpPerspective</code> method to rotate the source image.</p><div class="informalexample"><pre class="programlisting">    output = cv2.warpPerspective(src, transform, (w, h))
    return output</pre></div><p class="calibre8">The output of this isn't exactly what you want though, the images are rotated about (0, 0) in the image. We need to rotate the image about the center. To achieve this, we need to insert an<a id="id711" class="calibre1"/> additional translation matrix right <span class="strong"><em class="calibre10">before</em></span> the rotations happen.</p><div class="informalexample"><pre class="programlisting">    w = src.shape[1]
    h = src.shape[0]
    
    # New code:
    x = numpy.array([ [1, 0, -w/2],
                      [0, 1, -h/2],
                      [0, 0, 0],
                      [0, 0, 1]]
    A1 = numpy.asmatrix(x)
    ...</pre></div><p class="calibre8">Now, we insert the matrix A1 at the very beginning:</p><div class="informalexample"><pre class="programlisting">transform = cameraMatrix * (T*(R*A1))</pre></div><p class="calibre8">Now images should rotate around the center; this is exactly what we want and is a self-contained method that we can use to rotate images arbitrarily in 3D space using OpenCV.</p></div><div class="book" title="Accumulated rotations"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec40" class="calibre1"/>Accumulated rotations</h3></div></div></div><p class="calibre8">Rotating <a id="id712" class="calibre1"/>an image with a single rotation vector is quite straightforward. In this section, we'll extend that method so it is better suited for our project.</p><p class="calibre8">We have two data sources active when recording a video: the image capture and the gyroscope trace. These are captured at different rates—images every few milliseconds and gyroscope signals every few microseconds. To calculate the exact rotation required to stabilize an image, we need to accumulate the rotation of dozens of gyroscope signals. This means that the rotation matrix needs to have information on several different gyroscope data samples.</p><p class="calibre8">Also, the gyroscope and image sensors are not in sync; we will need to use linear interpolation on the gyroscope signals to bring them in sync.</p><p class="calibre8">Let's write a function that returns the transformation matrix.</p><div class="informalexample"><pre class="programlisting">def getAccumulatedRotation(w, h,
                           theta_x, theta_y, theta_z, timestamps,
                           prev, current,
                           f,
                           gyro_delay=None, gyro_drift=None, shutter_duration=None):
    if not gyro_delay:
        gyro_delay = 0

    if not gyro_drift:
        gyro_drift = (0, 0, 0)

    if not shutter_duration:
        shutter_duration = 0</pre></div><p class="calibre8">This<a id="id713" class="calibre1"/> function takes a lot of parameters. Let's go over each <a id="id714" class="calibre1"/>of them:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">w</code>, <code class="email">h</code>: We need to know the size of the image to convert it from world space to image space.</li><li class="listitem">theta_*: Currently, we have access to angular velocity. From there, we can evaluate actual angles and that is what this function accepts as parameters.</li><li class="listitem">Timestamps: The time each sample was taken.</li><li class="listitem"><code class="email">prev</code>, <code class="email">current</code>: Accumulate rotations between these timestamps. This will usually provide the timestamp of the previous frame and the current frame.</li><li class="listitem"><code class="email">f</code>, <code class="email">gyro_delay</code>, <code class="email">gyro_drift</code>, and <code class="email">shutter_duration</code> are used to improve the estimate of the rotation matrix. The last three of these are optional (and they get set to zero if you don't pass them).</li></ul></div><p class="calibre8">From the previous section, we know that we need to start by translating (or we'll get rotations about (0, 0)).</p><div class="informalexample"><pre class="programlisting">    x = numpy.array([[1, 0, -w/2],
                  [0, 1, -h/2],
                  [0, 0, 0],
                  [0, 0, 1]])
    A1 = numpy.asmatrix(x)
    transform = A1.copy()</pre></div><p class="calibre8">We'll use the "transform" matrix to accumulate rotations across multiple gyroscope samples.</p><p class="calibre8">Next, we offset the timestamps by using <code class="email">gyro_delay</code>. This is just adding (or subtracting, based on the sign of its value) to the timestamp.</p><div class="informalexample"><pre class="programlisting">    prev = prev + gyro_delay
    current = current + gyro_delay
    if prev in timestamps and current in timestamps:
        start_timestamp = prev
        end_timestamp = current
    else:
        (rot, start_timestamp, t_next) = fetch_closest_trio(theta_x,
                                                            theta_y,
                                                            theta_z,
                                                            timestamps,
                                                            prev)
        (rot, end_timestamp, t_next) = fetch_closest_trio(theta_x,
                                                          theta_y,
                                                          theta_z,
                                                          timestamps,
                                                          current)</pre></div><p class="calibre8">If the <a id="id715" class="calibre1"/>updated <code class="email">prev</code> and <code class="email">current</code> values exist in the timestamps (meaning we have values captured from the sensor at that time instant) – great! No need to interpolate. Otherwise, we use the function <code class="email">fetch_closest_trio</code> to interpolate the signals to the given timestamp.</p><p class="calibre8">This<a id="id716" class="calibre1"/> helper function returns three things:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The interpolated rotation for the requested timestamp</li><li class="listitem">The closest timestamp in the sensor data</li><li class="listitem">The timestamp right after it</li></ul></div><p class="calibre8">We use <code class="email">start_timestamp</code> and <code class="email">end_timestamp</code> for iterating now.</p><div class="informalexample"><pre class="programlisting">    for time in xrange(timestamps.index(start_timestamp), timestamps.index(end_timestamp)):
        time_shifted = timestamps[time] + gyro_delay
        trio, t_previous, t_current = fetch_closest_trio(theta_x, theta_y, theta_z, timestamps, time_shifted)
        gyro_drifted = (float(trio[0] + gyro_drift[0]),
                        float(trio[1] + gyro_drift[1]),
                        float(trio[2] + gyro_drift[2]))</pre></div><p class="calibre8">We iterate over each timestamp in the physical data. We add the gyroscope delay and use that to fetch the closest (interpolated) signals. Once that's done, we add the gyroscope drift per component. This is just a constant that should be added to compensate for errors in the gyroscope.</p><p class="calibre8">Using these rotation angles, we now calculate the rotation matrix, as in the previous section.</p><div class="informalexample"><pre class="programlisting">        smallR = cv2.Rodrigues(numpy.array([-float(gyro_drifted[1]),
                                            -float(gyro_drifted[0]),
                                            -float(gyro_drifted[2])]))[0]
        R = numpy.array([[smallR[0][0], smallR[0][1], smallR[0][2], 0],
                         [smallR[1][0], smallR[1][1], smallR[1][2], 0],
                         [smallR[2][0], smallR[2][1], smallR[2][2], 0],
                         [0,            0,            0,            1]])
        transform = R * transform</pre></div><p class="calibre8">This piece<a id="id717" class="calibre1"/> of code is almost the same as that in the previous section. There are a few key differences though. Firstly, we're providing negative values to Rodrigues. This is to negate the effect of motion. Secondly, the X and Y values are swapped. (<code class="email">gyro_drifted[1]</code> comes first, followed by <code class="email">gyro_drifted[0]</code>). This is required because the axes of the gyroscope and the ones used by these matrices are different.</p><p class="calibre8">This completes the iteration over the gyroscope samples between the specified timestamps. To complete this, we need to translate in the Z direction just like in the previous section. Since this can be hardcoded, let's do that:</p><div class="informalexample"><pre class="programlisting">    x = numpy.array([[1, 0, 0, 0],
                     [0, 1, 0, 0],
                     [0, 0, 1, f],
                     [0, 0, 0, 1]])
    T = numpy.asmatrix(x)</pre></div><p class="calibre8">We also need to use the camera matrix to convert from world space to image space.</p><div class="informalexample"><pre class="programlisting">    x = numpy.array([[f, 0, w/2, 0],
                     [0, f, h/2, 0],
                     [0, 0, 1,   0]])
    A2 = numpy.asmatrix(x)
    transform = A2 * (T*transform)
    return transform</pre></div><p class="calibre8">We first translate in the Z direction and then convert to image space. This section essentially lets you rotate frames of your video with the gyroscope parameters.</p></div></div></div>

<div class="book" title="Calibration">
<div class="book" title="The calibration class"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec79" class="calibre1"/>The calibration class</h2></div></div></div><p class="calibre8">With<a id="id718" class="calibre1"/> our data structures ready, we're in a good place to start the key piece of this project. The calibration builds on all the previously mentioned classes.</p><p class="calibre8">As always, we'll create a new class which encapsulates all the calibration-related tasks.</p><div class="informalexample"><pre class="programlisting">class CalibrateGyroStabilize(object):
    def __init__(self, mp4, csv):
        self.mp4 = mp4
        self.csv = csv</pre></div><p class="calibre8">The object requires two things: the video file and the gyroscope data file. These get stored in the object.</p><p class="calibre8">Before jumping directly into the calibration method, let's create some utility functions that will be helpful when calibrating.</p><div class="informalexample"><pre class="programlisting">    def get_gaussian_kernel(sigma2, v1, v2, normalize=True):
        gauss = [math.exp(-(float(x*x) / sigma2)) for x in range(v1, v2+1)]

        if normalize:
            total = sum(guass)
            gauss = [x/total for x in gauss]

        return gauss</pre></div><p class="calibre8">This method generates a Gaussian kernel of a given size. We'll use this to smooth out the angular velocity signals in a bit.</p><div class="informalexample"><pre class="programlisting">    def gaussian_filter(input_array):
        sigma = 10000
        r = 256
        kernel = get_gaussian_kernel(sigma, -r, r)
        return numpy.convolve(input_array, kernel, 'same')</pre></div><p class="calibre8">This <a id="id719" class="calibre1"/>function does the actual smoothing of a signal. Given an input signal, it generates the Gaussian kernel and convolves it with the input signal.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note138" class="calibre1"/>Note</h3><p class="calibre8">Convolutions are <a id="id720" class="calibre1"/>a mathematical tool to produce new functions. You can think of the gyroscope signal as a function; you give it a timestamp and it returns a value. To smooth it out, we need to combine it with another function. This function, called the Gaussian function, is a smooth bell curve. Both these functions have different time ranges on which they operate (the gyroscope function might return values between a time of 0 seconds and 50 seconds while the Gaussian function might just work for a time of 0 seconds to 5 seconds). Convolving these two functions produces a third function that behaves a bit like both, thereby effectively smoothing out the minor variations in the gyroscope signal.</p></div><p class="calibre8">Next, we write a function that calculates an error score giving two sets of points. This will be a building block in estimating how good the calibration has been.</p><div class="informalexample"><pre class="programlisting">    def calcErrorScore(set1, set2):
        if len(set1) != len(set2):
            raise Exception("The given sets need to have the exact same length")

        score = 0
        for first, second in zip(set1.tolist(), set2.tolist()):
            diff_x = math.pow(first[0][0] – second[0][0], 2)
            diff_y = math.pow(first[0][1] – second[0][1], 2)
            score += math.sqrt(diff_x + diff_y)

        return score</pre></div><p class="calibre8">This error score is straightforward: you have two lists of points and you calculate the distance between the corresponding points on the two lists and sum it up. A higher error score means the points on the two lists don't correspond perfectly.</p><p class="calibre8">This method, however,  only gives us the error on a single frame. We're concerned about errors across the whole video. We therefore write another method.</p><div class="informalexample"><pre class="programlisting">    def calcErrorAcrossVideo(videoObj, theta, timestamp, focal_length, gyro_delay=None, gyro_drift=None, rolling_shutter=None):
        total_error = 0
        for frameCount in xrange(videoObj.numFrames):
            frameInfo = videoObj.frameInfo[frameCount]
            current_timestamp = frameInfo['timestamp']

            if frameCount == 0:
                previous_timestamp = current_timestamp
                continue
            keypoints = frameInfo['keypoints']

            if not keypoints:
                continue</pre></div><p class="calibre8">We pass in <a id="id721" class="calibre1"/>the video object and all the details we have estimated (the theta, timestamps, focal length, gyroscope delay, and so on). With these details, we try to do the video stabilization and see what differences exists between the visually tracked keypoints and the gyroscope-based transformations.</p><p class="calibre8">Since we're calculating the error across the whole video, we need to iterate over each frame. If the frame's information does not have any keypoints in it, we simply ignore the frame. If the frame does have keypoints, here's what we do:</p><div class="informalexample"><pre class="programlisting">            old_corners = frameInfo['keypoints'][0]
            new_corners = frameInfo['keypoints'][1]
            transform = getAccumulatedRotation(videoObj.frameWidth,
                                               videoObj.frameHeight,
                                               theta[0], theta[1], theta[2],
                                               timestamps,
                                               int(previous_timestamp),
                                               int(current_timestamp),
                                               focal_length,
                                               gyro_delay,
                                               gyro_drift,
                                               rolling_shutter)</pre></div><p class="calibre8">The <code class="email">getAccumulatedRotation</code> function is something we'll write soon. The key idea of the function is to return a transformation matrix for the given theta (the angles we need to rotate to stabilize the video). We can apply this transform to <code class="email">old_corners</code> and compare it to <code class="email">new_corners</code>.</p><p class="calibre8">Since <code class="email">new_corners</code> was obtained visually, it is the ground truth. We want <code class="email">getAccumulatedRotation</code> to return a transformation that matches the visual ground truth perfectly. This means the error between <code class="email">new_corners</code> and the transformed <code class="email">old_corners</code> should be minimal. This is where <code class="email">calcErrorScore</code> helps us:</p><div class="informalexample"><pre class="programlisting">            transformed_corners = cv2.perspectiveTransform(old_corners, transform)
            error = calcErrorScore(new_corners, transformed_corners)
            total_error += error
            previous_timestamp = current_timestamp
        return total_error</pre></div><p class="calibre8">We're ready <a id="id722" class="calibre1"/>to calculate the error across the whole video! Now let's move to the calibration function:</p><div class="informalexample"><pre class="programlisting">    def calibrate(self):
        gdf = GyroscopeDataFile(csv)
        gdf.parse()

        signal_x = gdf.get_signal_x()
        signal_y = gdf.get_signal_y()
        signal_z = gdf.get_signal_z()
        timestamps = gdf.get_timestamps()</pre></div><p class="calibre8">The first step is to smooth out the noise in the angular velocity signals. This is the desired signal with smooth motion.</p><div class="informalexample"><pre class="programlisting">        smooth_signal_x = self.gaussian_filter(signal_x)
        smooth_signal_y = self.gaussian_filter(signal_y)
        smooth_signal_z = self.gaussian_filter(signal_z)</pre></div><p class="calibre8">We'll be writing the <code class="email">gaussian_filter</code> method soon; for now, let's just keep in mind that it returns a smoothed out signal.</p><div class="mediaobject"><img src="../images/00133.jpeg" alt="The calibration class" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Next, we calculate the difference between the physical signal and the desired signal. We need to do this separately for each component.</p><div class="informalexample"><pre class="programlisting">        g = [ [], [] [] ]
        g[0] = numpy.subtract(signal_x, smooth_signal_x).tolist()
        g[1] = numpy.subtract(signal_y, smooth_signal_y).tolist()
        g[2] = numpy.subtract(signal_z, smooth_signal_z).tolist()</pre></div><p class="calibre8">We also <a id="id723" class="calibre1"/>need to calculate the delta between the timestamps. We'll be using this for integration.</p><div class="informalexample"><pre class="programlisting">        dgt = self.diff(timestamps)</pre></div><p class="calibre8">Next, we integrate the angular velocities to get actual angles. Integration introduces errors into our equations but that's okay. It is good enough for our purposes.</p><div class="informalexample"><pre class="programlisting">        theta = [ [], [], [] ]
        for component in [0, 1, 2]:
            sum_of_consecutives = numpy.add( g[component][:-1], g[component][1:])
            dx_0 = numpy.divide(sum_of_consecutives, 2 * 1000000000)
            num_0 = numpy.multipy(dx_0, dgt)
            theta[component] = [0]
            theta[component].extend(numpy.cumsum(num_0))</pre></div><p class="calibre8">And that's it. We have calculated the amount of theta that will stabilize the image! However, this is purely from the gyroscope's view. We still need to calculate the unknowns so that we can use these thetas to stabilize the image.</p><p class="calibre8">To do this, we initialize some of the unknowns as variables with an arbitrary initial value (0 in most cases). Also, we load the video and process the keypoint information.</p><div class="informalexample"><pre class="programlisting">        focal_length = 1080.0
        gyro_delay = 0
        gyro_drift = (0, 0, 0)
        shutter_duration = 0

        videoObj = GyroVideo(mp4)
        videoObj.read_video()</pre></div><p class="calibre8">Now, we use SciPy's optimize method to minimize the error. To do this, we must first convert these unknowns into a Numpy array.</p><div class="informalexample"><pre class="programlisting">        parameters = numpy.asarray([focal_length,
                                    gyro_delay,
                                    gyro_drift[0], gyro_drift[1], gyro_drift[2]])</pre></div><p class="calibre8">Since we've not yet incorporated fixing the rolling shutter, we ignore that in the parameters list. Next, we call the actual optimization function:</p><div class="informalexample"><pre class="programlisting">        result = scipy.optimize.minimize(self.calcErrorAcrossVideoObjective,
                                         parameters,
                                         (videoObj, theta, timestamps),
                                         'Nelder-Mead')</pre></div><p class="calibre8">Executing<a id="id724" class="calibre1"/> this function takes a few seconds, but it produces the values of the unknowns for us. We can then extract these from the result as follows:</p><div class="informalexample"><pre class="programlisting">        focal_length = result['x'][0]
        gyro_delay = result['x'][1]
        gyro_drift = ( result['x'][2], result['x'][3], result['x'][4] )
        print "Focal length = %f" % focal_length
        print "Gyro delay   = %f" % gyro_delay
        print "Gyro drift   = (%f, %f, %f)" % gyro_drift</pre></div><p class="calibre8">With this, we're done with calibration! All we need to do is return all the relevant calculations we've done just now.</p><div class="informalexample"><pre class="programlisting">    return (delta_theta, timestamps, focal_length, gyro_delay, gyro_drift, shutter_duration)</pre></div><p class="calibre8">And that's a wrap!</p></div></div>

<div class="book" title="Calibration">
<div class="book" title="Undistorting images"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec80" class="calibre1"/>Undistorting images</h2></div></div></div><p class="calibre8">In the<a id="id725" class="calibre1"/> previous section, we calculated all the unknowns in our equations. Now, we can go ahead with fixing the shaky video.</p><p class="calibre8">We'll start off by creating a new method called <code class="email">stabilize_video</code>. This method will take a video file and a corresponding csv file.</p><div class="informalexample"><pre class="programlisting">def stabilize_video(mp4, csv):
    calib_obj = CalibrateGyroStabilize(mp4, csv)</pre></div><p class="calibre8">We create an object of the calibration class we just defined and pass it the required information. Now, we just need to call the calibrate function.</p><div class="informalexample"><pre class="programlisting">    delta_theta, timestamps, focal_length, gyro_delay, gyro_drift, shutter_duration = calib_obj.calibrate()</pre></div><p class="calibre8">This method call may take a while to execute, but we need to run this only once for every device. Once calculated, we can store these values in a text file and read them from there.</p><p class="calibre8">Once we have estimated all the unknowns, we start by reading the video file for each frame.</p><div class="informalexample"><pre class="programlisting">    vidcap = cv2.VideoCapture(mp4)</pre></div><p class="calibre8">Now we start iterating over each frame and correcting the rotations.</p><div class="informalexample"><pre class="programlisting">    frameCount = 0
    success, frame = vidcap.read()
    previous_timestamp = 0
    while success:
        print "Processing frame %d" % frameCount</pre></div><p class="calibre8">Next, we fetch the timestamp from the video stream and use that to fetch the closest rotation sample.</p><div class="informalexample"><pre class="programlisting">        current_timestamp = vidcap.get(cv2.CAP_PROP_POS_MSEC) * 1000 * 1000</pre></div><p class="calibre8">The <code class="email">VideoCapture</code> class returns timestamps in milliseconds. We convert that into nanoseconds to keep consistent units.</p><div class="informalexample"><pre class="programlisting">        rot, prev, current = fetch_closest_trio(delta_theta[0],delta_theta[1],delta_theta[2],timestamps,current_timestamps)</pre></div><p class="calibre8">With these pieces, we now fetch the accumulated rotation.</p><div class="informalexample"><pre class="programlisting">        rot = accumulateRotation(frame, delta_theta[0],delta_theta[1],delta_theta[2],timestamps, previous_timestamp,prev,focal_length,gyro_delay,gyro_drift,shutter_duration)</pre></div><p class="calibre8">Next, we write the transformed frame into a file and move on to the next frame:</p><div class="informalexample"><pre class="programlisting">        cv2.imwrite("/tmp/rotated%04d.png" % frameCount, rot)
        frameCount += 1
        previous_timestamp = prev
        success, frame = vidcap.read()
    return</pre></div><p class="calibre8">And this <a id="id726" class="calibre1"/>finishes our simple function to negate the shakiness of the device. Once we have all the images, we can combine them into a single video with <code class="email">ffmpeg</code>.</p><div class="informalexample"><pre class="programlisting">ffmpeg -f image2 -i image%04d.jpg output.mp4</pre></div></div></div>

<div class="book" title="Calibration">
<div class="book" title="Testing calibration results"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch07lvl2sec81" class="calibre1"/>Testing calibration results</h2></div></div></div><p class="calibre8">The <a id="id727" class="calibre1"/>effectiveness of the calibration depends on how accurately it can replicate motion on the video. For any frame, we have matching keypoints in the previous and current frames. This gives a sense of the general motion of the scene.</p><p class="calibre8">Using the estimated parameters, if we are able to use previous frames' keypoints to generate the current frames' keypoints, we can assume the calibration has been successful.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Rolling shutter compensation"><div class="book" id="1UU542-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec60" class="calibre1"/>Rolling shutter compensation</h1></div></div></div><p class="calibre8">At this <a id="id728" class="calibre1"/>point, our video is stable, however, when objects in the scene are moving quickly, the rolling shutter effects become more pronounced.</p><p class="calibre8">To fix this, we'll need to do a few things. First, incorporate the rolling shutter speed into our calibration code. Second, when warping images, we need to unwarp the rolling shutter as well.</p></div>

<div class="book" title="Rolling shutter compensation">
<div class="book" title="Calibrating the rolling shutter"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec82" class="calibre1"/>Calibrating the rolling shutter</h2></div></div></div><p class="calibre8">To <a id="id729" class="calibre1"/>start calibrating the rolling shutter duration, we need to tweak the error function to incorporate another term. Let's start by looking at the <code class="email">calcErrorAcrossVideo</code> method. The part we're interested in is:</p><div class="informalexample"><pre class="programlisting">def calcErrorAcrossVideo(videoObj, theta, timestamp, focal_length, gyro_delay=None, gyro_drift=None, rolling_shutter=None):
    total_error = 0
    ...
        transform = getAccumulatedRotation(...)
        transformed_corners = cv2.perspectiveTransform(old_corners, transform)
    ...</pre></div><p class="calibre8">Also, we'll <a id="id730" class="calibre1"/>need to add logic to transform a corner based on its location—a corner in the upper part of the image is transformed differently from a corner in the lower half.</p><p class="calibre8">So far, we have had a single transformation matrix and that was usually sufficient. However, now, we need to have multiple transformation matrices, one for each row. We could choose to do this for every row of pixels, however that is a bit excessive. We only need transforms for rows that contain a corner we're tracking.</p><p class="calibre8">We'll start by replacing the two lines mentioned above. We need to loop over each corner individually and warp it. Let's do this with a simple <code class="email">for</code> loop:</p><div class="informalexample"><pre class="programlisting">for pt in old_corners:
    x = pt[0][0]
    y = pt[0][1]

    pt_timestamp = int(current_timestamp) + rolling_shutter * (y-frame_height/2) / frame_height</pre></div><p class="calibre8">Here, we extract the x and y coordinates of the old corner and try to estimate the timestamp when this particular pixel was captured. Here, I'm assuming the rolling shutter is in the vertical direction, from the top of the frame to the bottom.</p><p class="calibre8">We use the current estimate of the rolling shutter duration and estimate subtract and add time based on the row the corner belongs to. It should be simple to adapt this for a horizontal rolling shutter as well. Instead of using <code class="email">y</code> and <code class="email">frameHeight</code>, you would have to use <code class="email">x</code> and <code class="email">frameWidth</code>—the calculation would stay the same. For now, we'll just assume this is going to be a vertical rolling shutter.</p><p class="calibre8">Now that we have the estimated timestamp of capture, we can get the rotation matrix for that instant (remember, the gyroscope produces a higher resolution data than the camera sensor).</p><div class="informalexample"><pre class="programlisting">    transform = getAccumulatedRotation(videoObj.frameWidth, videoObj.frameHeight, theta[0], theta[1], theta[2], timestamps, int(previous_timestamp), int(pt_timestamp), focal_length, gyro_delay, gyro_drift, doSub=True)</pre></div><p class="calibre8">This line is almost the same as the original we had; the only difference is that we've replaced <code class="email">current_timestamp</code> with <code class="email">pt_timestamp</code>.</p><p class="calibre8">Next, we need to transform this point based on the rolling shutter duration.</p><div class="informalexample"><pre class="programlisting">    output = transform * np.matrix("%f;%f;1.0" % (x, y)).tolist()
    tx = (output[0][0] / output[2][0]).tolist()[0][0]
    ty = (output[1][0] / output[2][0]).tolist()[0][0]
    transformed_corners.append( np.array([tx, ty]) )</pre></div><p class="calibre8">After<a id="id731" class="calibre1"/> transforming, we simply append it to the <code class="email">transformed_corners</code> list (just like we did earlier).</p><p class="calibre8">With this, we're done with the calibration part. Now, we move onto warping images.</p></div></div>

<div class="book" title="Rolling shutter compensation">
<div class="book" title="Warping with grid points"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec83" class="calibre1"/>Warping with grid points</h2></div></div></div><p class="calibre8">Let's<a id="id732" class="calibre1"/> start by writing a function that will do the warping for us. This function takes in these inputs:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The original image</li><li class="listitem">A bunch of points that should ideally line up in a perfect grid</li></ul></div><p class="calibre8">The size of the point list gives us the number of rows and columns to expect and the function returns a perfectly aligned image.</p><div class="mediaobject"><img src="../images/00134.jpeg" alt="Warping with grid points" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Let's define the function:</p><div class="informalexample"><pre class="programlisting">def meshwarp(src, distorted_grid):
    """
    src: The original image
    distorted_grid: The list of points that have been distorted
    """
    size = src.shape</pre></div><p class="calibre8">As mentioned earlier, this takes in an image and the list of control points. We store the size of the image for future reference.</p><div class="informalexample"><pre class="programlisting">    mapsize = (size[0], size[1], 1)
    dst = np.zeros(size, dtype=np.uint8)</pre></div><p class="calibre8">The size<a id="id733" class="calibre1"/> we stored earlier will most likely have three channels in it. So we create a new variable called <code class="email">mapsize</code>; this stores the size of the image but only one channel. We'll use this later for creating matrices for use by the remap function in OpenCV.</p><p class="calibre8">We also create a blank image of the same size as the original. Next, we look at calculating the number of rows and columns in the grid.</p><div class="informalexample"><pre class="programlisting">    quads_per_row = len(distorted_grid[0]) – 1
    quads_per_col = len(distorted_grid) – 1
    pixels_per_row = size[1] / quads_per_row
    pixels_per_col = size[0] / quads_per_col</pre></div><p class="calibre8">We'll use the variables in some loops soon.</p><div class="informalexample"><pre class="programlisting">    pt_src_all = []
    pt_dst_all = []</pre></div><p class="calibre8">These lists store all the source (distorted) points and the destination (perfectly aligned) points. We'll have to use <code class="email">distorted_grid</code> to populate <code class="email">pt_src_all</code>. We'll procedurally generate the destination based on the number of rows and columns in the input data.</p><div class="informalexample"><pre class="programlisting">    for ptlist in distorted_grid:
        pt_src_all.extend(ptlist)</pre></div><p class="calibre8">The distorted grid should be a list of lists. Each row is a list that contains its points.</p><p class="calibre8">Now, we generate the procedural destination points using the <code class="email">quads_per_*</code> variables we calculated earlier.</p><div class="informalexample"><pre class="programlisting">    for x in range(quads_per_row+1):
        for y in range(quads_per_col+1):
            pt_dst_all.append( [x*pixels_per_col,
                                y*pixels_per_row])</pre></div><p class="calibre8">This generates the ideal grid based on the number of points we passed to the method.</p><p class="calibre8">We then have all the required information to calculate the interpolation between the source and destination grids. We'll be using <code class="email">scipy</code> to calculate the interpolation for us. We then pass this to OpenCV's remap method and that applies it to an image.</p><p class="calibre8">To begin with, <code class="email">scipy</code> needs a representation of the expected output grid so we need to specify a dense grid that contains all the pixels of the image. This is done with:</p><div class="informalexample"><pre class="programlisting">gx, gt = np.mgrid[0:size[1], 0:size[0]]</pre></div><p class="calibre8">Once we have the base grid defined, we can use Scipy's <code class="email">interpolate</code> module to calculate the mapping for us.</p><div class="informalexample"><pre class="programlisting">g_out = scipy.interpolate.griddata(np.array(pt_dst_all),
                                   np.array(pt_src_all),
                                   (gx, gy), method='linear')</pre></div><p class="calibre8"><code class="email">g_out</code> contains <a id="id734" class="calibre1"/>both the <code class="email">x</code> and <code class="email">y</code> coordinates of the remapping; we need to split this into individual components for OpenCV's <code class="email">remap</code> method to work.</p><div class="informalexample"><pre class="programlisting">mapx = np.append([], [ar[:,0] for ar in g_out]).reshape(mapsize).astype('float32')
mapy = np.append([], [ar[:,1] for ar in g_out]).reshape(mapsize).astype('float32')</pre></div><p class="calibre8">These matrices are exactly what remap expects and we can now simply run it with the appropriate parameters.</p><div class="informalexample"><pre class="programlisting">    dst = cv2.remap(src, mapx, mapy, cv2.INTER_LINEAR)
    return dst</pre></div><p class="calibre8">And that completes our method. We can use this in our stabilization code and fix the rolling shutter as well.</p></div></div>

<div class="book" title="Rolling shutter compensation">
<div class="book" title="Unwarping with calibration"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec84" class="calibre1"/>Unwarping with calibration</h2></div></div></div><p class="calibre8">Here, we<a id="id735" class="calibre1"/> discuss how to warp images given a mesh for stabilizing the video. We split each frame into a 10x10 mesh. We warp the mesh and that results in warping the image (like control points). Using this approach, we should get good results and decent performance as well.</p><p class="calibre8">The actual unwarp happens in the <code class="email">accumulateRotation</code> method:</p><div class="informalexample"><pre class="programlisting">def accumulateRotation(src, theta_x, theta_y, theta_z, timestamps, prev, current, f, gyro_delay=None, gyro_drift=None, shutter_duration=None):
    ...
    transform = getAccumulatedRotation(src.shape[1], src.shape[0], theta_x, theta_y, theta_z, timestamps, prev, current, f, gyro_delay, gyro_drift)
    o = cv2.warpPerspective(src, transform (src.shape[1], src.shape[0]))
    return o</pre></div><p class="calibre8">Here, there's a single perspective transform happening. Now, instead, we have to do a different transform for each of the 10x10 control points and use the <code class="email">meshwarp</code> method to fix the rolling shutter. So replace the <code class="email">transform =</code> line with the contents below:</p><div class="informalexample"><pre class="programlisting">    ...
    pts = []
    transformed_pts = []
    for x in range(10):
        current_row = []
        current_row_transformed = []
        pixel_x = x * (src.shape[1] / 10)
        for y in range(10):
            pixel_y = y * (src.shape[0] / 10)
            current_row.append( [pixel_x, pixel_y] )
        pts.append(current_row)</pre></div><p class="calibre8">We <a id="id736" class="calibre1"/>have now generated the original grid in the <code class="email">pts</code> list. Now, we need to generate the transformed coordinates:</p><div class="informalexample"><pre class="programlisting">        ...
        for y in range(10):
            pixel_y = y * (src.shape[0] / 10
            if shutter_duration:
                y_timestamp = current + shutter_duration*(pixel_y - src.shape[0]/2)
            else:
                y_timestamp = current
        ...</pre></div><p class="calibre8">If a shutter duration is passed, we generate the timestamp at which this specific pixel was recorded. Now we can transform (<code class="email">pixel_x</code>, <code class="email">pixel_y</code>) based on the shutter rotation and append that to <code class="email">current_row_transformed</code>:</p><div class="informalexample"><pre class="programlisting">        ...
        transform = getAccumulatedRotation(src.shape[1], src.shape[0], theta_x, theta_y, theta_z, timestamps, prev, y_timestamp, f, gyro_delay, gyro_drift)
        output = cv2.perspectiveTransform(np.array([[pixel_x, pixel_y]], transform)
        current_row_transformed.append(output)
    pts.append(current_row)
    pts_transformed.append(current_row_transformed)
        ...</pre></div><p class="calibre8">This completes the grid for <code class="email">meshwarp</code>. Now all we need to do is generate the warped image. This is simple since we already have the required method:</p><div class="informalexample"><pre class="programlisting">    o = meshwarp(src, pts_transformed)
    return o</pre></div><p class="calibre8">And this completes our transformation. We now have rolling shutter incorporated into our undistortion as well.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="What's next?" id="1VSLM1-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec61" class="calibre1"/>What's next?</h1></div></div></div><p class="calibre8">What we have right now is a very barebones implementation of video stabilization. There are a few more things you can add to it to make it more robust, more automated and the output more pleasing to the eye. Here are a few things to get you started.</p></div>

<div class="book" title="What's next?" id="1VSLM1-940925703e144daa867f510896bffb69">
<div class="book" title="Identifying gyroscope axes"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec85" class="calibre1"/>Identifying gyroscope axes</h2></div></div></div><p class="calibre8">In <a id="id737" class="calibre1"/>this chapter, we've hard-coded the axes of the gyroscope. This might not be the case for all mobile phone manufacturers. Using a similar calibration technique, you should be able to find an axes configuration that minimizes errors across the video.</p></div></div>

<div class="book" title="What's next?" id="1VSLM1-940925703e144daa867f510896bffb69">
<div class="book" title="Estimating the rolling shutter direction"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec86" class="calibre1"/>Estimating the rolling shutter direction</h2></div></div></div><p class="calibre8">We've <a id="id738" class="calibre1"/>hard-coded the direction of the rolling shutter. Using specific techniques (like blinking an LED really fast at the camera), it is possible to estimate the direction of the rolling shutter and incorporate that into the calibration code. Certain camera sensors don't have the rolling shutter artifacts at all. This test can also identify if such a sensor is being used.</p></div></div>

<div class="book" title="What's next?" id="1VSLM1-940925703e144daa867f510896bffb69">
<div class="book" title="Smoother timelapses"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec87" class="calibre1"/>Smoother timelapses</h2></div></div></div><p class="calibre8">Now that we've stabilized the video, we can speed up (or slow down) the video much better. There<a id="id739" class="calibre1"/> are commercial packages that do similar tasks – now your OpenCV code can do it too!</p></div></div>

<div class="book" title="What's next?" id="1VSLM1-940925703e144daa867f510896bffb69">
<div class="book" title="Repository of calibration parameters"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec88" class="calibre1"/>Repository of calibration parameters</h2></div></div></div><p class="calibre8">You <a id="id740" class="calibre1"/>will have to calibrate every new device type you come across. If you move from one device type (say, a Samsung S5 to an iPhone 6), you'll have to run a calibration for this combination of lens and sensor. However, moving between different devices of the same kind does not require a re-calibration (such as moving from one iPhone 6 to another). If you're able to collect enough calibration results, your code can run perfectly on pretty much any device.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note139" class="calibre1"/>Note</h3><p class="calibre8">You could also figure out a fallback mechanism if the repository does not have the required parameters.</p></div></div></div>

<div class="book" title="What's next?" id="1VSLM1-940925703e144daa867f510896bffb69">
<div class="book" title="Incorporating translations"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch07lvl2sec89" class="calibre1"/>Incorporating translations</h2></div></div></div><p class="calibre8">Currently, we're <a id="id741" class="calibre1"/>only using rotations. This means that if you shake the camera in a single plane, the algorithm won't do much. By using inputs from the accelerometer and using the translation of keypoints, it should be possible to compensate for <a id="id742" class="calibre1"/>translation as well. This should produce higher quality video.</p></div></div>

<div class="book" title="What's next?" id="1VSLM1-940925703e144daa867f510896bffb69">
<div class="book" title="Additional tips"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_6"><a id="ch07lvl2sec90" class="calibre1"/>Additional tips</h2></div></div></div><p class="calibre8">Here are some additional things to keep in mind while working with Python and computer vision in general. They should help speed up your work and keep you safe from unexpected crashes!</p><div class="book" title="Use the Python pickle module"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec41" class="calibre1"/>Use the Python pickle module</h3></div></div></div><p class="calibre8">Python<a id="id743" class="calibre1"/> gives us a neat way to store Python objects as files on disk. In our project, we have the gyroscope calibration class. This class stores information like the video dimensions and keypoints across different frames. Calculating this information from scratch everytime you want to test your code is cumbersome. You can easily pickle this object into a file and read back the data when required.</p><p class="calibre8">Here is some sample code for pickling the video object in our code:</p><div class="informalexample"><pre class="programlisting">import pickle
fp = open("/path/to/file.data", "w")
videoObj = GyroVideo(mp4)
pickle.dump(videoObj, fp)
fp.close()</pre></div><p class="calibre8">To read the object back into the script:</p><div class="informalexample"><pre class="programlisting">import pickle
fp = open("/path/to/file.data", "r")
videoObj = pickle.load(fp)
fp.close()</pre></div><p class="calibre8">This saves time when iterating on code and verifying if something works as expected.</p></div><div class="book" title="Write out single images"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec42" class="calibre1"/>Write out single images</h3></div></div></div><p class="calibre8">When <a id="id744" class="calibre1"/>working with videos, you most often end up using something like the VideoWriter class from OpenCV. You feed it frames and it writes out a video file. While this is a perfectly valid way to get things done, you have more control if you write out individual frames to disk and then use a video encoder to combine the images into a video stream.</p><p class="calibre8">A simple way to combine multiple images is to use <code class="email">ffmpeg</code>.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">ffmpeg -i /tmp/image%04d.png -f image2 output.mp4</strong></span>
</pre></div></div><div class="book" title="Testing without the delta"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec43" class="calibre1"/>Testing without the delta</h3></div></div></div><p class="calibre8">In the <a id="id745" class="calibre1"/>project, we're trying to stabilize video – thus we're calculating the delta between the actual gyroscope signal and a smoothed out version of the signal.</p><p class="calibre8">You might want to try it out with just the actual gyroscope signal; this will totally keep the video still. This might be useful for situations where you want the camera to appear completely still.</p></div></div></div>
<div class="book" title="Summary" id="20R681-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec62" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we've covered quite a bit: talking to your gyroscope, using that to find unknowns, negating the effects of camera shake and rolling shutter.</p><p class="calibre8">We started out by creating an Android app that uses background tasks to initiate recording media into a video file. While doing this, we figured out how to extend OpenCV's camera view class to incorporate custom UI and responsiveness. With this, you can now create very sophisticated UIs with an OpenCV backend. Along with this, we also captured the gyroscope trace and stored it in a single file. The sampling rate of the gyroscope and the media were different – however, we did not care about it at this stage. We'll let the app store a higher density of gyroscope traces (every few hundred microseconds versus every few dozen milliseconds for the media).</p><p class="calibre8">Once we had the media/csv pair, we used Python and the numpy/scipy libraries to calibrate the system. We had three unknowns initially: the pixel focal length of the camera, the gyroscope delay (the offset between the gyroscope recordings and the media timestamps) and the gyroscope drift.</p><p class="calibre8">We devised an error function that takes the expected keypoints and the transformed keypoints and returns the amount of errors. We then used this to calculate errors across the whole video. Using this, and Scipy's optimize method, we were able to find the values for these unknowns. This calibration needs to happen only once for each device type.</p><p class="calibre8">Then we added another parameter to our calibration—the rolling shutter. Estimating the value of this unknown was similar to the previous three, however, incorporating the undistortion was a bit tricky. We had to create a new method called <code class="email">meshwarp</code> that takes a distorted grid. This method rectifies the grid and removes artifacts due to the rolling shutter. We worked on a vertical rolling shutter, however it should be easy to convert it to a horizontal rolling shutter.</p><p class="calibre8">We touched upon a lot of different areas: sensors, calibration, and geometric distortions. I hope this chapter gives you an insight into designing your own pipelines for working with images.</p></div></body></html>