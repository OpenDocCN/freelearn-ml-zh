<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Detecting Face Parts and Overlaying Masks"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Detecting Face Parts and Overlaying Masks</h1></div></div></div><p>In the previous chapter, we learned about object classification and how machine learning can be used to achieve it. In this chapter, we will learn how to detect and track different face parts. We will start the discussion by understanding the face detection pipeline and how it's built from the ground up. We will then use this framework to detect face parts, such as eyes, ears, mouth, and nose. We will then learn how to overlay funny masks on these face parts in a live video.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Working with Haar cascades</li><li class="listitem" style="list-style-type: disc">Integral images and why we need them</li><li class="listitem" style="list-style-type: disc">Building a generic face detection pipeline</li><li class="listitem" style="list-style-type: disc">Detecting and tracking face parts, such as eyes, ears, nose, and mouth in a live video stream from the webcam</li><li class="listitem" style="list-style-type: disc">Automatically overlaying facemasks, sunglasses, and a funny nose on a person's face in a video</li></ul></div><div class="section" title="Understanding Haar cascades"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec48"/>Understanding Haar cascades</h1></div></div></div><p>Haar cascades <a id="id298" class="indexterm"/>are cascade classifiers that are based on Haar features. What is a cascade classifier? It is simply a concatenation of a set of weak classifiers that can be used to create a strong classifier. Now, what do we mean by <span class="emphasis"><em>weak</em></span> and <span class="emphasis"><em>strong</em></span> classifiers? Weak classifiers are classifiers whose performances are limited. They don't have the ability to classify everything correctly. If you keep the problem really simple, they might perform at an acceptable level. Strong classifiers, on the other hand, are really good at classifying our data correctly. We will see how it all comes together in the next couple of paragraphs. Another important part of Haar cascades is <span class="emphasis"><em>Haar features</em></span>. These features are<a id="id299" class="indexterm"/> simple summations of rectangles and differences of those areas across the image. Let's consider the following figure:</p><div class="mediaobject"><img src="graphics/B04283_07_01.jpg" alt="Understanding Haar cascades"/></div><p>If we want to compute the Haar features of the region ABCD, we just need to compute the difference between the white pixels and the colored pixels in that region. As shown in in the preceding four figures, we use different patterns to build Haar features. There are a lot of other patterns that are used as well. We do this at multiple scales to make the system scale invariant. When we say <span class="emphasis"><em>multiple scales</em></span>, we just scale the image down to compute the same<a id="id300" class="indexterm"/> features again. This way, we can make it robust against size variations of a given object.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note25"/>Note</h3><p>As it turns out, this concatenation system is a very good method to detect objects in an image. In 2001, <span class="emphasis"><em>Paul Viola</em></span> and <span class="emphasis"><em>Michael Jones</em></span> published a seminal paper where they described a fast and effective method for object detection. If you are curious to learn more about it, you can check out their paper at <a class="ulink" href="http://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf">http://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf</a>.</p></div></div><p>Let's dive deeper into it to understand what they actually did. They basically described an algorithm that uses a boosted cascade of simple classifiers. This system is used to build a strong classifier that can perform really well. Why did they use these simple classifiers instead of complex classifiers that can be more accurate? Well, using this technique, they were able to avoid the problem of having to build a single classifier that can perform with high precision. These single-step classifiers tend to be complex and computationally intensive. The reason why their technique works so well is because the simple classifiers can be weak learners, which means that they don't need to be complex.</p><p>Consider the problem of building a table detector. We want to build a system that will automatically learn what a table looks like. Based on this knowledge, it should be able to identify whether there is a table in any given image. To build this system, the first step is to collect images that can be used to train our system. There are a lot of techniques available in the machine learning world that can be used to train a system like this. Keep in mind that we need to collect a lot of table and non-table images if we want our system to perform well. In machine learning lingo, table images are called <span class="strong"><strong>positive</strong></span> samples<a id="id301" class="indexterm"/> and the non-table images are called <a id="id302" class="indexterm"/>
<span class="strong"><strong>negative</strong></span> samples. Our system will ingest this data and then learn to differentiate between these two classes.</p><p>In order to build a real-time system, we need to keep our classifier nice and simple. The only concern is that simple classifiers are not very accurate. If we try to make them more accurate, then they will end up being computationally intensive and hence slow. This kind of trade-off between accuracy and speed is very common in machine learning. So, we will overcome this problem by concatenating a bunch of weak classifiers to create a strong and unified classifier. We don't need the weak classifiers to be very accurate. To ensure the quality of the overall classifier, <span class="emphasis"><em>Viola</em></span> and <span class="emphasis"><em>Jones</em></span> have described a nifty technique in the cascading step. You can go through the paper to understand the complete system.</p><p>Now that we understand the general pipeline, let's see how to build a system that can detect faces in a live video. The first step is to extract features from all the images. In this case, the algorithms need these features to learn and understand what faces look like. They used Haar features in their paper to build the feature vectors. Once we extract these features, we pass them through a cascade of classifiers. We just check all the different rectangular subregions and <a id="id303" class="indexterm"/>keep discarding the ones that don't have faces in them. This way, we arrive at the final answer quickly to see whether a given rectangle contains a face or not.</p></div></div>
<div class="section" title="What are integral images?"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec49"/>What are integral images?</h1></div></div></div><p>In order to extract these<a id="id304" class="indexterm"/> Haar features, we need to calculate the sum of the pixel values enclosed in many rectangular regions of the image. To make it scale invariant, we need to compute these areas at multiple scales (that is, for various rectangle sizes). If implemented naively, this would be a very computationally intensive process. We would have to iterate over all the pixels of each rectangle, including reading the same pixels multiple times if they are contained in different overlapping rectangles. If you want to build a system that can run in real time, you cannot spend so much time in computation. We need to find a way to avoid this huge redundancy during the area computation because we iterate over the same pixels multiple times. To avoid this, we can use something called <span class="strong"><strong>integral images</strong></span>. These images can be initialized in a linear time (by iterating only twice over the image) and can then be provided with the sum of pixels inside any rectangle of any size by reading only four values. To understand it better, let's take a look at the following figure:</p><div class="mediaobject"><img src="graphics/B04283_07_02.jpg" alt="What are integral images?"/></div><p>If we want to<a id="id305" class="indexterm"/> calculate the area of any rectangle in our image, we don't have to iterate through all the pixels in that region. Let's consider a rectangle formed by the top-left point in the image and any point P as the opposite corner. Let A<sub>P</sub> denote the area of this rectangle. For example, in the preceding figure, A<sub>B</sub> denotes the area of the 5 X 2 rectangle formed by taking the top-left point and B as opposite corners. Let's take a look at the following figure for clarity purposes:</p><div class="mediaobject"><img src="graphics/B04283_07_03.jpg" alt="What are integral images?"/></div><p>Let's take a<a id="id306" class="indexterm"/> look at the top-left diagram in the preceding figure. The colored pixels indicate the area between the top-left pixel and point A. This is denoted by A<sub>A</sub>. The remaining diagrams are denoted by their respective names: A<sub>B</sub>, A<sub>C</sub>, and A<sub>D</sub>. Now, if we want to calculate the area of the rectangle ABCD, as shown in the preceding figure, we will use the following formula:</p><p>Area of the rectangle ABCD = A<sub>C</sub> – (A<sub>B</sub> + A<sub>D</sub> - A<sub>A</sub>)</p><p>What's so special about this particular formula? As we know, extracting Haar features from the image includes computing these summations, and we would have to do it for a lot of rectangles <a id="id307" class="indexterm"/>at multiple scales in the image. A lot of these calculations are repetitive because we would be iterating over the same pixels over and over again. It is so slow that building a real-time system wouldn't be feasible. Hence, we need this formula. As you can see, we don't have to iterate over the same pixels multiple times. If we want to compute the area of any rectangle, all the values on the right-hand side of the previous equation are readily available in our integral image. We just use pick up the right values, substitute them in the previous equation, and extract the features.</p></div>
<div class="section" title="Overlaying a facemask in a live video"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec50"/>Overlaying a facemask in a live video</h1></div></div></div><p>OpenCV provides a<a id="id308" class="indexterm"/> nice face detection framework. We just need to load the cascade file and use it to detect the faces in an image. When we capture a video stream from the webcam, we can overlay funny masks on top of our faces. It will look something like this:</p><div class="mediaobject"><img src="graphics/B04283_07_04.jpg" alt="Overlaying a facemask in a live video"/></div><p>Let's take a look at the main parts of the code to see how to overlay the preceding mask on top of the face in the input video stream. The complete code is available in the downloadable code bundle provided along with this book:</p><div class="informalexample"><pre class="programlisting">int main(int argc, char* argv[])
{
    string faceCascadeName = argv[1];
    
    // Variable declarations and initializations
    
    // Iterate until the user presses the Esc key
    while(true)
    {
        // Capture the current frame
        cap &gt;&gt; frame;
        
        // Resize the frame
        resize(frame, frame, Size(), scalingFactor, scalingFactor, INTER_AREA);
        
        // Convert to grayscale
        cvtColor(frame, frameGray, CV_BGR2GRAY);
        
        // Equalize the histogram
        equalizeHist(frameGray, frameGray);
        
        // Detect faces
        faceCascade.detectMultiScale(frameGray, faces, 1.1, 2, 0|CV_HAAR_SCALE_IMAGE, Size(30, 30) );</pre></div><p>Let's see what <a id="id309" class="indexterm"/>happened here. We start reading input frames from the webcam and resize it to our size of choice. The captured frame is a color image and face detection works on grayscale images. So, we convert it to grayscale and equalize the histogram. Why do we need to equalize the histogram? We need to do this in order to compensate for any kind of issues, such as lighting, saturation, and so on. If the image is too bright or too dark, the detection will be poor. So, we need to equalize the histogram to ensure that our image has a healthy range of pixel values:</p><div class="informalexample"><pre class="programlisting">        // Draw green rectangle around the face
        for(int i = 0; i &lt; faces.size(); i++)
        {
            Rect faceRect(faces[i].x, faces[i].y, faces[i].width, faces[i].height);
            
            // Custom parameters to make the mask fit your face. You may have to play around with them to make sure it works.
            int x = faces[i].x - int(0.1*faces[i].width);
            int y = faces[i].y - int(0.0*faces[i].height);
            int w = int(1.1 * faces[i].width);
            int h = int(1.3 * faces[i].height);
            
            // Extract region of interest (ROI) covering your face
            frameROI = frame(Rect(x,y,w,h));</pre></div><p>At this point, we <a id="id310" class="indexterm"/>know where the face is. So, we extract the region of interest to overlay the mask in the right position:</p><div class="informalexample"><pre class="programlisting">            // Resize the face mask image based on the dimensions of the above ROI
            resize(faceMask, faceMaskSmall, Size(w,h));
            
            // Convert the above image to grayscale
            cvtColor(faceMaskSmall, grayMaskSmall, CV_BGR2GRAY);
            
            // Threshold the above image to isolate the pixels associated only with the face mask
            threshold(grayMaskSmall, grayMaskSmallThresh, 230, 255, CV_THRESH_BINARY_INV);</pre></div><p>We isolated the pixels associated with the face mask. Now, we want to overlay the mask in such a way that it doesn't look like a rectangle. We want the exact boundaries of the overlaid object so that it looks natural. Let's go ahead and overlay the mask now:</p><div class="informalexample"><pre class="programlisting">            // Create mask by inverting the above image (because we don't want the background to affect the overlay)
            bitwise_not(grayMaskSmallThresh, grayMaskSmallThreshInv);
            
            // Use bitwise "AND" operator to extract precise boundary of face mask
            bitwise_and(faceMaskSmall, faceMaskSmall, maskedFace, grayMaskSmallThresh);
            
            // Use bitwise "AND" operator to overlay face mask
            bitwise_and(frameROI, frameROI, maskedFrame, grayMaskSmallThreshInv);
            
            // Add the above masked images and place it in the original frame ROI to create the final image
            add(maskedFace, maskedFrame, frame(Rect(x,y,w,h)));
        }
     
    // code dealing with memory release and GUi

    return 1;
}</pre></div><div class="section" title="What happened in the code?"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec40"/>What happened in the code?</h2></div></div></div><p>The first thing to <a id="id311" class="indexterm"/>note is that this code takes two input arguments: the face cascade <code class="literal">xml</code> file and the mask image. You can use the <code class="literal">haarcascade_frontalface_alt.xml</code> and <code class="literal">facemask.jpg</code> files that are provided. We need a classifier model that can be used to detect faces in an image, and OpenCV provides a prebuilt xml file that can be used for this purpose. We use the <code class="literal">faceCascade.load()</code>function to load the <code class="literal">xml</code> file and also check whether the file has been loaded correctly.</p><p>We initiate the video capture object to capture the input frames from the webcam. We then convert it to grayscale to run the detector. The <code class="literal">detectMultiScale</code> function is used to extract the boundaries of all the faces in the input image. We may have to scale down the image according to our needs, so the second argument in this function takes care of this. This scaling factor is the jump that we take at each scale. Since we need to look for faces at multiple scales, the next size will be 1.1 times bigger than the current size. The last parameter is a threshold that specifies the number of adjacent rectangles that are needed to keep the current rectangle. It can be used to increase the robustness of the face detector.</p><p>We start the <code class="literal">while</code> loop and keep detecting the face in every frame until the user presses the <span class="emphasis"><em>Esc</em></span> key. Once we detect a face, we need to overlay a mask on it. We may have to modify the dimensions slightly to ensure that the mask fits nicely. This customization is slightly subjective and it depends on the mask that's being used. Now that we have extracted the region of interest, we need to place our mask on top of this region. If we overlay the mask with its white background, it will look weird. We need to extract the exact curvy boundaries of the mask and overlay it. We want the skull-mask pixels to be visible and the remaining area to be transparent.</p><p>As we can see, the input mask has a white background. So, we create a mask by applying a threshold to the mask image. Using trial and error, we can see that a threshold of 240 works well. In the image, all the pixels with an intensity value greater than 240 will become 0, and all the others will become 255. As far as the region of interest is concerned in the image, we need to black out all the pixels in this region. To do this, we simply use the inverse of the mask that was just created. In the last step, we just add the masked versions to produce the final output image.</p></div></div>
<div class="section" title="Get your sunglasses on"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec51"/>Get your sunglasses on</h1></div></div></div><p>Now that we <a id="id312" class="indexterm"/>understand how to detect faces, we can generalize this concept to detect different parts of the face. We will use an eye detector to overlay sunglasses in a live video. It's important to understand that the Viola-Jones framework can be applied to any object. The accuracy and robustness will depend on the uniqueness of the object. For example, a human face has very unique characteristics, so it's easy to train our system to be robust. On the other hand, an object such as a towel is too generic, and there are no distinguishing characteristics as such. So, it's more difficult to build a robust towel detector.</p><p>Once you build the eye detector and overlay glasses on top of it, it will look something like this:</p><div class="mediaobject"><img src="graphics/B04283_07_05.jpg" alt="Get your sunglasses on"/></div><p>Let's take a look at the main parts of the code:</p><div class="informalexample"><pre class="programlisting">int main(int argc, char* argv[])
{
    string faceCascadeName = argv[1];
    string eyeCascadeName = argv[2];

    // Variable declarations and initializations
 
    // Face detection code
        
        vector&lt;Point&gt; centers;
        
        // Draw green circles around the eyes
        for(int i = 0; i &lt; faces.size(); i++)
        {
            Mat faceROI = frameGray(faces[i]);
            vector&lt;Rect&gt; eyes;
            
            // In each face, detect eyes
            eyeCascade.detectMultiScale(faceROI, eyes, 1.1, 2, 0 |CV_HAAR_SCALE_IMAGE, Size(30, 30));</pre></div><p>As we can see here, we <a id="id313" class="indexterm"/>run the eye detector only in the face region. We don't need to search the entire image for eyes because we know that the eyes will always be on your face:</p><div class="informalexample"><pre class="programlisting">            // For each eye detected, compute the center
            for(int j = 0; j &lt; eyes.size(); j++)
            {
                Point center( faces[i].x + eyes[j].x + int(eyes[j].width*0.5), faces[i].y + eyes[j].y + int(eyes[j].height*0.5) );
                centers.push_back(center);
            }
        }
        
        // Overlay sunglasses only if both eyes are detected
        if(centers.size() == 2)
        {
            Point leftPoint, rightPoint;
            
            // Identify the left and right eyes
            if(centers[0].x &lt; centers[1].x)
            {
                leftPoint = centers[0];
                rightPoint = centers[1];
            }
            else
            {
                leftPoint = centers[1];
                rightPoint = centers[0];
            }</pre></div><p>We detect the eyes and store them only when we find both of them. We then use their coordinates to determine <a id="id314" class="indexterm"/>which one is the left eye and the right eye:</p><div class="informalexample"><pre class="programlisting">            // Custom parameters to make the sunglasses fit your face. You may have to play around with them to make sure it works.
            int w = 2.3 * (rightPoint.x - leftPoint.x);
            int h = int(0.4 * w);
            int x = leftPoint.x - 0.25*w;
            int y = leftPoint.y - 0.5*h;
            
            // Extract region of interest (ROI) covering both the eyes
            frameROI = frame(Rect(x,y,w,h));
            
            // Resize the sunglasses image based on the dimensions of the above ROI
            resize(eyeMask, eyeMaskSmall, Size(w,h));</pre></div><p>In the preceding code, we adjusted the size of the sunglasses to fit the scale of our faces in the webcam:</p><div class="informalexample"><pre class="programlisting">            // Convert the above image to grayscale
            cvtColor(eyeMaskSmall, grayMaskSmall, CV_BGR2GRAY);
            
            // Threshold the above image to isolate the foreground object
            threshold(grayMaskSmall, grayMaskSmallThresh, 245, 255, CV_THRESH_BINARY_INV);
            
            // Create mask by inverting the above image (because we don't want the background to affect the overlay)
            bitwise_not(grayMaskSmallThresh, grayMaskSmallThreshInv);
            
            // Use bitwise "AND" operator to extract precise boundary of sunglasses
            bitwise_and(eyeMaskSmall, eyeMaskSmall, maskedEye, grayMaskSmallThresh);
            
            // Use bitwise "AND" operator to overlay sunglasses
            bitwise_and(frameROI, frameROI, maskedFrame, grayMaskSmallThreshInv);
            
            // Add the above masked images and place it in the original frame ROI to create the final image
            add(maskedEye, maskedFrame, frame(Rect(x,y,w,h)));
        }

        // code for memory release and GUI

    return 1;
}</pre></div><div class="section" title="Looking inside the code"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec41"/>Looking inside the code</h2></div></div></div><p>If you notice, the flow of the <a id="id315" class="indexterm"/>code looks similar to the face detection code that we discussed earlier. We load the face detection cascade classifier as well as the eye detection cascade classifier. Now why do we need to load the face cascade classifier when we are detecting the eyes? Well, we don't really need to use the face detector, but it helps us limit our search for the eyes' location. We know that the eyes are always located on somebody's face, so we can limit the eye detection to the face region. The first step would be to detect the face and then run our eye detector code on this region. Since we would be operating on a smaller region, it would be faster and more efficient.</p><p>For each frame, we start by detecting the face. We then go ahead and detect the location of the eyes by operating on this region. After this step, we need to overlay the sunglasses. To do this, we need to resize the sunglasses' image to make sure that it fits our face. To get the proper scale, we can consider the distance between the two eyes that are being detected. We overlay the sunglasses only when we detect both the eyes. That's why we first run the eye detector, collect all the centers, and then overlay the sunglasses. Once we have this, we just need to overlay the sunglasses' mask. The principle used for masking is very similar to the principle that we used to overlay the facemask. You may have to customize the sizing and position of the sunglasses depending on what you want. You can play around with different types of sunglasses to see what they look like.</p></div></div>
<div class="section" title="Tracking your nose, mouth, and ears"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec52"/>Tracking your nose, mouth, and ears</h1></div></div></div><p>Now that you <a id="id316" class="indexterm"/>know how to track different things using the framework, you can try tracking your nose, mouth, and ears as well. Let's use a nose detector to overlay a funny nose on top:</p><div class="mediaobject"><img src="graphics/B04283_07_06.jpg" alt="Tracking your nose, mouth, and ears"/></div><p>You can refer to <a id="id317" class="indexterm"/>the code files for a complete implementation of this detector. There are cascade files called <code class="literal">haarcascade_mcs_nose.xml</code>, <code class="literal">haarcascade_mcs_mouth.xml</code>, <code class="literal">haarcascade_mcs_leftear.xml</code>, and <code class="literal">haarcascade_mcs_rightear.xml</code> that can be used to track the different face parts. So, you can play around with them and try to overlay a moustache or Dracula ears on yourself!</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec53"/>Summary</h1></div></div></div><p>In this chapter, we discussed Haar cascades and integral images. We learned how the face detection pipeline is built. We learned how to detect and track faces in a live video stream. We discussed how to use the face detection framework to detect various face parts, such as eyes, ears, nose, and mouth. We also learned how to overlay masks on top on the input image using the results of the face parts detection.</p><p>In the next chapter, we will learn about video surveillance, background removal, and morphological image processing.</p></div></body></html>