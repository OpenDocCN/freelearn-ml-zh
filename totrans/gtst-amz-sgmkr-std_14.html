<html><head></head><body>
		<div id="_idContainer159">
			<h1 id="_idParaDest-145"><em class="italic"><a id="_idTextAnchor144"/>Chapter 11</em>: Operationalize ML Projects with SageMaker Projects, Pipelines, and Model Registry</h1>
			<p>Data scientists used to spend too much time and effort maintaining and manually managing ML pipelines, a process that starts with data, processing, training, and evaluation and ends with model hosting with ongoing maintenance. SageMaker Studio provides features that aim to streamline these operations with <strong class="bold">continuous integration and continuous delivery</strong> (<strong class="bold">CI/CD</strong>) best practices. You will learn how to implement SageMaker projects, Pipelines, and the model registry to help operationalize the ML lifecycle with CI/CD. </p>
			<p>In this chapter, we will be learning about the following:</p>
			<ul>
				<li>Understanding ML operations and CI/CD</li>
				<li>Creating a SageMaker project</li>
				<li>Orchestrating an ML pipeline with SageMaker Pipelines</li>
				<li>Running CI/CD in SageMaker Studio</li>
			</ul>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor145"/>Technical requirements</h1>
			<p>For this chapter, you will need to ensure that the SageMaker project template permission is enabled in the Studio setting. If you have finished <a href="B17447_08_ePub_RK.xhtml#_idTextAnchor108"><em class="italic">Chapter 8</em></a>,<em class="italic"> Jumpstarting ML with SageMaker JumpStart and Autopilot</em>, you should have the permissions. You can verify it in the Studio <strong class="bold">domain</strong> view with the following steps: </p>
			<ol>
				<li>If either of the permissions is disabled as shown in <em class="italic">Figure 11.1</em>, you can click <strong class="bold">Edit Settings</strong> to change this.</li>
			</ol>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B17447_11_01.jpg" alt="Figure 11.1 – Checking and editing the SageMaker projects permissions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – Checking and editing the SageMaker projects permissions</p>
			<ol>
				<li value="2">Go to <strong class="bold">Step 2 Studio Settings</strong> to switch on the SageMaker projects and JumpStart permissions as shown in <em class="italic">Figure 11.2</em>. </li>
			</ol>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B17447_11_02.jpg" alt="Figure 11.2 – Enabling SageMaker project templates for the account and users&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – Enabling SageMaker project templates for the account and users</p>
			<ol>
				<li value="3">Then click <strong class="bold">Next</strong> to go to the next page and click <strong class="bold">Submit</strong>. </li>
			</ol>
			<p>This ensures SageMaker project template permissions are enabled for you.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor146"/>Understanding ML operations and CI/CD</h1>
			<p>In the ML lifecycle, there are many <a id="_idIndexMarker736"/>steps that require a skilled data scientist's hands-on interaction throughout, such as wrangling the dataset, training, and evaluating a model. These manual steps could affect an ML team's operations and speed to deploy models in production. Imagine your model training job takes a long time and finishes in the middle of the night. You either have to wait for your first data scientist to come in during the day to evaluate the model and deploy the model into production or have to employ an on-call rotation to have someone on standby at all times to monitor the model training and deployment. But neither option is ideal if you want an effective and efficient ML lifecycle. </p>
			<p><strong class="bold">Machine Learning Operations</strong> (<strong class="bold">MLOps</strong>) is critical to a team that wants to stay lean and scale well. MLOps helps you streamline and reduce manual human intervention as much as possible. It helps transform your ML lifecycle to enterprise-grade. It helps you scale and maintain the quality of your models that are put into production and it also helps you improve time to model delivery with automation.</p>
			<p>So, what exactly is MLOps?</p>
			<p>MLOps refers to a methodology to apply DevOps best practices to the ML lifecycle. <strong class="bold">DevOps</strong> stands<a id="_idIndexMarker737"/> for software<strong class="bold"> Development</strong> (<strong class="bold">Dev</strong>) and<a id="_idIndexMarker738"/> IT <strong class="bold">Operations</strong> (<strong class="bold">Ops</strong>). DevOps <a id="_idIndexMarker739"/>aims to increase a team's ability to deliver applications at a high pace with high quality using a set of engineering, practices, and patterns. It also promotes a new cultural and behavioral paradigm in an organization. MLOps recommends the following practices, which are built upon DevOps best practices with some modifications tailored to the nature of ML:</p>
			<ul>
				<li><strong class="bold">Continuous Integration</strong> (<strong class="bold">CI</strong>): In<a id="_idIndexMarker740"/> DevOps, developers constantly commit and merge their code changes into a central repository, after which tests are automatically run to validate the code. In ML, not only does the code need to be integrated and validated, but so does the training data and ML models. The training data needs to be versioned, model lineage needs to be tracked for traceability, and tests on data and models, besides the code, need to be implemented as well. </li>
				<li><strong class="bold">Continuous Delivery</strong> (<strong class="bold">CD</strong>): In DevOps, this<a id="_idIndexMarker741"/> is a practice where code is built, tested, and released for production in an automatic fashion. In MLOps, similar to what was discussed about continuous integration, the operations include data and models besides the ML source code. </li>
				<li><strong class="bold">Everything as code</strong>: In order to streamline and automate for CI and CD (CI/CD for short), everything needs to be implemented as code: the process, infrastructure, and configuration, instead of any manual setup and point-and-click process on screen. This practice also enables version control and reproducibility for your processes, infrastructures, and configurations.</li>
				<li><strong class="bold">Monitoring and logging</strong>: This practice encourages you to log all things related to your software/ML system for visibility and auditability. You not only log the ML metrics, data lineage, data versions, and model versions, but also log the CI/CD processes, and any errors for debugging and monitoring purposes. This enables the next practice.</li>
				<li><strong class="bold">Communication and collaboration</strong>: Because everything is code, and everything is automated and logged, you have a transparent environment that invites collaboration and communication. Instead of working in silos with a manual hand-off, which causes friction and opacity, your entire team can work more <a id="_idIndexMarker742"/>closely<a id="_idIndexMarker743"/> together on the system.  </li>
			</ul>
			<p>The key benefits that MLOps brings to the table are the following:</p>
			<ul>
				<li><strong class="bold">Faster time to market</strong>: Because <a id="_idIndexMarker744"/>now your model deployment is automatically created and deployed as part of the CI/CD process, your model training and deployment are streamlined without any handoff or manual processes. You can expect more iterations of refinement within the same timeframe and a quicker turnaround time for a mature product.</li>
				<li><strong class="bold">Productivity</strong>: A lot of manual processes are taken away from data scientists and ML developers so that they can focus on ML modeling where things cannot be automated.</li>
				<li><strong class="bold">Repeatability</strong>: Also, because everything is code and is automated, your ML lifecycle can be performed by anyone at any time with exactly the same output.</li>
				<li><strong class="bold">Reliability</strong>: With the tests and validations performed in the CI/CD process, you know that your models are high quality. You can also consistently produce high-quality models thanks to the repeatability CI/CD provides.</li>
				<li><strong class="bold">Auditability</strong>: As code, data, and models are versioned and lineage and processes are logged, you can tell exactly how the models were trained and deployed.</li>
				<li><strong class="bold">Better quality</strong>: Combining<a id="_idIndexMarker745"/> all the benefits above, MLOps enables us to spend more time creating better models and letting the system take care of the integration and delivery quickly, reliably, and repeatably.</li>
			</ul>
			<p>You may think: <em class="italic">MLOps seems too perfect to be easily adopted</em>. Yes, you do need to incorporate additional technology into your ML lifecycle to enable the CI/CD process. And yes, you need to implement many details to enable the logging and monitoring. It is also true that to adopt the <em class="italic">everything as code</em> practice, many iterations of testing on the infrastructure code and configuration are required at the beginning. The good news is, in SageMaker Studio, adopting MLOps practices for your ML project is made easy. SageMaker <a id="_idIndexMarker746"/>Studio has templatized the CI/CD processes <a id="_idIndexMarker747"/>for numerous use cases so that you can easily pick one and adopt the MLOps best practices and technologies from the templated ML use case for your use case. The features that enable MLOps and CI/CD are <strong class="bold">SageMaker projects</strong>, <strong class="bold">SageMaker Pipelines</strong>, and <strong class="bold">SageMaker Model Registry</strong>.</p>
			<p>Let's get started by creating a SageMaker project first. </p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor147"/>Creating a SageMaker project</h1>
			<p>A <strong class="bold">SageMaker project</strong> enables <a id="_idIndexMarker748"/>you to automate the model building and deployment pipelines with MLOps and CI/CD from SageMaker-provided templates and your own custom templates. With a SageMaker-provided template, all the initial setup and resource provisioning is handled by SageMaker so you can quickly adopt it for your use case. </p>
			<p>In this chapter, we will run an ML example with MLOps and CI/CD in SageMaker Studio. As we focus on MLOps and CI/CD in this chapter, we use a simple regression problem from the abalone dataset (<a href="https://archive.ics.uci.edu/ml/datasets/abalone">https://archive.ics.uci.edu/ml/datasets/abalone</a>) to predict the age of abalone from physical measurements. I will show you how you can create a project from SageMaker projects, and how each part of the MLOps system works. The MLOps system created from SageMaker projects enables automation of data validation, model building, model evaluation, deployment, and monitoring with a simple trigger from a code commit. This means that whenever we make any changes to the code base, the whole system will run through the complete ML lifecycle in SageMaker that we've learned about throughout this book automatically. You will see how much SageMaker has simplified MLOps for you. Let's open up SageMaker Studio and follow the steps given here:</p>
			<ol>
				<li value="1">On the <strong class="bold">Launcher</strong> page, click the <em class="italic">plus sign</em> on the <strong class="bold">New project</strong> card, as shown in <em class="italic">Figure 11.3</em>.</li>
			</ol>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B17447_11_03.jpg" alt="Figure 11.3 – Opening a new project in Launcher&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – Opening a new project in Launcher</p>
			<ol>
				<li value="2">There are<a id="_idIndexMarker749"/> MLOps templates for various use cases created by SageMaker (under <strong class="bold">SageMaker templates</strong>) for us to choose from, as shown in <em class="italic">Figure 11.4</em>. Let's select <strong class="bold">MLOps template for model building, training, deployment and monitoring</strong>. This template automates the entire model lifecycle, which includes model building, deployment, and monitoring workflows. Click <strong class="bold">Select project template</strong>.</li>
			</ol>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B17447_11_04.jpg" alt="Figure 11.4 – Choosing SageMaker managed templates&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – Choosing SageMaker managed templates</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Templates whose names contain <strong class="bold">with third-party Git repositories</strong> are designed to work with your external Git repositories or CI/CD software<a id="_idIndexMarker750"/> such as <strong class="bold">Jenkins</strong>. You will need to provide additional information in the next step. </p>
			<ol>
				<li value="3">Provide a<a id="_idIndexMarker751"/> name, description, and tags for the project on the <strong class="bold">Project details</strong> page. Click <strong class="bold">Create project</strong>.</li>
			</ol>
			<p>With this project template, SageMaker Studio is now provisioning cloud resources for MLOps and deploying the sample code. Let's illustrate the MLOps architecture with the diagram shown in <em class="italic">Figure 11.5</em>:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B17447_11_05.jpg" alt="Figure 11.5 – Architecture diagram of an MLOps setup with a SageMaker projects template&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5 – Architecture diagram of an MLOps setup with a SageMaker projects template</p>
			<p>The cloud<a id="_idIndexMarker752"/> resources created include the following:</p>
			<ul>
				<li>Three code repositories in <strong class="bold">AWS CodeCommit</strong>, a managed source control service that hosts private Git repositories. They can also be found in the AWS CodeCommit console: <a href="https://console.aws.amazon.com/codesuite/codecommit/repositories">https://console.aws.amazon.com/codesuite/codecommit/repositories</a>. Remember to switch to your own AWS Region from the URL.</li>
				<li>Three continuous delivery pipelines in <strong class="bold">AWS CodePipeline</strong>, a managed service that helps automate build, test, and release pipelines, can be found in the AWS CodePipeline console: <a href="https://console.aws.amazon.com/codesuite/codepipeline/pipelines">https://console.aws.amazon.com/codesuite/codepipeline/pipelines</a>. Remember to switch to your own AWS Region from the URL.</li>
				<li>Five event trigger rules in <strong class="bold">Amazon EventBridge</strong>, a managed service that makes it easier to build event-driven applications, can be found in the Amazon EventBridge console: <a href="https://console.aws.amazon.com/events/home#/rules">https://console.aws.amazon.com/events/home#/rules</a>. Remember to switch to your own AWS Region from the URL.</li>
			</ul>
			<p>These are essentially the backbone CI/CD framework that supports MLOps in SageMaker Studio. Repositories in CodeCommit are where we store, develop, and commit our code. Every commit to a code repository in CodeCommit is going to trigger, managed by rules in EventBridge, a run of the corresponding pipeline in CodePipeline to build, test, and deploy resources. </p>
			<p>Once the project<a id="_idIndexMarker753"/> creation is complete, you can see a portal for the project in the main working area as shown in <em class="italic">Figure 11.6</em>. </p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B17447_11_06.jpg" alt="Figure 11.6 – SageMaker project detail portal&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – SageMaker project detail portal</p>
			<p>This portal contains all the important resources and information that are associated to the project—code repositories in CodeCommit, ML pipelines from SageMaker Pipelines (which we will talk about soon), experiments tracked using SageMaker Experiments, models, hosted endpoints, and other settings.</p>
			<ol>
				<li value="4">We can clone the repositories from CodeCommit to a local SageMaker Studio directory. As the final step before we move on to describe the ML pipeline, let's clone the <strong class="source-inline">&lt;project-name-prefix&gt;-modelbuild</strong> repository, which contains the ML pipeline that builds, trains, and evaluates the ML model using the abalone<a id="_idIndexMarker754"/> dataset. Click the <strong class="bold">clone repo…</strong> hyperlink next to the <strong class="source-inline">&lt;project-name-prefix&gt;-modelbuild</strong> repository as highlighted with an arrow in <em class="italic">Figure 11.6</em>.</li>
			</ol>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B17447_11_07.jpg" alt="Figure 11.7 – Cloning a repository from CodeCommit to a local SageMaker Studio directory&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7 – Cloning a repository from CodeCommit to a local SageMaker Studio directory</p>
			<ol>
				<li value="5">In the popup shown in <em class="italic">Figure 11.7</em>, click <strong class="bold">Clone Repository</strong>. The repository will appear in the home directory <strong class="source-inline">~/&lt;project-name-prefix&gt;/&lt;project-name-prefix&gt;-modelbuild/</strong>.</li>
			</ol>
			<p>Let's look at the<a id="_idIndexMarker755"/> ML pipeline defined in this abalone example first, before we dive into the CI/CD part.</p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor148"/>Orchestrating an ML pipeline with SageMaker Pipelines</h1>
			<p>The<a id="_idIndexMarker756"/> template we're using contains an ML<a id="_idIndexMarker757"/> lifecycle pipeline that carries out data preprocessing, data quality checks, model training, model evaluation steps, and eventually model registration. This pipeline is a central piece of the MLOps process where the model is being created. The pipeline is defined in <strong class="source-inline">&lt;project-name-prefix&gt;-modelbuild</strong> using SageMaker Pipelines. <strong class="bold">SageMaker Pipelines</strong> is an orchestration tool for ML workflow in SageMaker. SageMaker Pipelines integrates with SageMaker Processing, training, Experiments, hosting, and the model registry. It provides reproducibility, repeatability, and tracks data/model lineage for auditability. Most importantly, you can visualize the workflow graph and runtime live in SageMaker Studio. The pipeline can be found under the <strong class="bold">Pipelines</strong> tab in the details portal as shown in <em class="italic">Figure 11.8</em>.</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B17447_11_08.jpg" alt="Figure 11.8 – A list of pipelines in the project &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 – A list of pipelines in the project </p>
			<p class="callout-heading">Note</p>
			<p class="callout">I have used the term <strong class="bold">pipeline</strong> a lot in this chapter. Let's settle this once and for all. I am referring to the pipeline from SageMaker Pipelines, shown in <em class="italic">Figure 11.8</em> and <em class="italic">Figure 11.9</em>, as the <strong class="bold">ML pipeline</strong>. Please, do not confuse an ML pipeline with a CI/CD pipeline from AWS CodePipeline, which is briefly mentioned in the last section and will be further discussed in the <em class="italic">Running CI/CD in SageMaker Studio</em> section.</p>
			<p>On double-clicking<a id="_idIndexMarker758"/> the pipeline, we can<a id="_idIndexMarker759"/> see the full execution graph and the live status of the pipeline, as shown in <em class="italic">Figure 11.9</em>. The corresponding pipeline code is in <strong class="source-inline">~/&lt;project-name-prefix&gt;/&lt;project-name-prefix&gt;-modelbuild/pipelines/abalone/pipeline.py</strong>. </p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B17447_11_09.jpg" alt="Figure 11.9 – Pipeline workflow and live status&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.9 – Pipeline workflow and live status</p>
			<p>Let's walk through <a id="_idIndexMarker760"/>the pipeline and how <a id="_idIndexMarker761"/>it is set up in the code. The pipeline contains the following steps (from top to bottom in the graph):</p>
			<ol>
				<li value="1">First is preprocessing the dataset with SageMaker Processing (<strong class="bold">PreprocessAbaloneData</strong> in <em class="italic">Figure 11.9</em>). In the pipeline code <strong class="source-inline">pipeline.py</strong> file, where we use classes and functions in the <strong class="source-inline">sagemaker.workflow</strong> module along with other <strong class="source-inline">sagemaker</strong> classes, a scikit-learn processor is defined to run a script <strong class="source-inline">preprocess.py</strong> in the same directory. Also, <strong class="source-inline">ProcessingStep</strong> is a class from the <strong class="source-inline">sagemaker.workflow.steps</strong> module:<p class="source-code"># Line 209 in pipeline.py</p><p class="source-code">step_process = ProcessingStep(</p><p class="source-code">    name="PreprocessAbaloneData",</p><p class="source-code">    processor=sklearn_processor,</p><p class="source-code">    outputs=[</p><p class="source-code">        ProcessingOutput(output_name="train", source="/opt/ml/processing/train"),</p><p class="source-code">        ProcessingOutput(output_name="validation", source="/opt/ml/processing/validation"),</p><p class="source-code">        ProcessingOutput(output_name="test", source="/opt/ml/processing/test"),</p><p class="source-code">    ],</p><p class="source-code">    code=os.path.join(BASE_DIR, "preprocess.py"),</p><p class="source-code">    job_arguments=["--input-data", input_data],</p><p class="source-code">)</p></li>
				<li>After the data<a id="_idIndexMarker762"/> is<a id="_idIndexMarker763"/> preprocessed, the pipeline checks against previously registered data quality and bias metrics and/or calculates the data quality and bias using SageMaker Clarify. Here, the output of the previous step <strong class="source-inline">step_process.properties.ProcessingOutputConfig.Outputs["train"]</strong> is used as the input baseline data. A <strong class="source-inline">QualityCheckStep()</strong> step object is instantiated here. This step computes the data quality statistics from the baseline training data and registers the statistics into the model registry once the model is created toward the end:<p class="source-code"># Line 238</p><p class="source-code">data_quality_check_config = DataQualityCheckConfig(</p><p class="source-code">baseline_dataset=step_process.properties.ProcessingOutputConfig.Outputs["train"].S3Output.S3Uri,</p><p class="source-code">    dataset_format=DatasetFormat.csv(header=False, output_columns_position="START"),</p><p class="source-code">    output_s3_uri=Join(on='/', values=['s3:/', default_bucket, base_job_prefix, ExecutionVariables.PIPELINE_EXECUTION_ID, 'dataqualitycheckstep'])</p><p class="source-code">)</p><p class="source-code">data_quality_check_step = QualityCheckStep(</p><p class="source-code">    name="DataQualityCheckStep",</p><p class="source-code">    skip_check=skip_check_data_quality,</p><p class="source-code">    register_new_baseline=register_new_baseline_data_quality,</p><p class="source-code">    quality_check_config=data_quality_check_config,</p><p class="source-code">    check_job_config=check_job_config,</p><p class="source-code">supplied_baseline_statistics=supplied_baseline_statistics_data_quality,</p><p class="source-code">supplied_baseline_constraints=supplied_baseline_constraints_data_quality,</p><p class="source-code">    model_package_group_name=model_package_group_name</p><p class="source-code">)</p></li>
				<li>At the same time, the<a id="_idIndexMarker764"/> pipeline <a id="_idIndexMarker765"/>also computes the data bias using a step instantiated from the <strong class="source-inline">ClarifyCheckStep()</strong> class:<p class="source-code">data_bias_check_config = DataBiasCheckConfig(</p><p class="source-code">    data_config=data_bias_data_config,</p><p class="source-code">    data_bias_config=data_bias_config,</p><p class="source-code">)</p><p class="source-code">data_bias_check_step = ClarifyCheckStep(</p><p class="source-code">    name="DataBiasCheckStep",</p><p class="source-code">    clarify_check_config=data_bias_check_config,</p><p class="source-code">    check_job_config=check_job_config,</p><p class="source-code">    skip_check=skip_check_data_bias,</p><p class="source-code">    register_new_baseline=register_new_baseline_data_bias,</p><p class="source-code">    model_package_group_name=model_package_group_name</p><p class="source-code">)</p></li>
			</ol>
			<p>These two checking steps are conditional based on the <strong class="source-inline">skip_check</strong> arguments. <strong class="source-inline">skip_check_data_quality</strong> and <strong class="source-inline">skip_check_data_bias</strong> are pipeline input parameters and can be configured for each run. For the first run, you may skip the checks because there are no baseline statistics to check against. <strong class="source-inline">register_new_baseline</strong> is also conditional from pipeline input parameters, but <a id="_idIndexMarker766"/>most of the time <a id="_idIndexMarker767"/>you would register new baseline statistics when you have a new dataset unless you have a specific reason not to update the statistics.</p>
			<ol>
				<li value="4">After the data quality and bias checks, a training job is created from a SageMaker estimator. In this example, the built-in XGBoost algorithm is used. <strong class="source-inline">TrainingStep</strong> is dependent on <strong class="source-inline">DataQualityCheckStep</strong> and <strong class="source-inline">DataBiasCheckStep</strong>, meaning that the training step waits for the two check steps to complete before starting, and takes the output from the preprocessing step, <strong class="source-inline">step_process</strong>:<p class="source-code"># Line 326</p><p class="source-code">step_train = TrainingStep(</p><p class="source-code">    name="TrainAbaloneModel",</p><p class="source-code">    depends_on=["DataQualityCheckStep", "DataBiasCheckStep"],</p><p class="source-code">    estimator=xgb_train,</p><p class="source-code">    inputs={</p><p class="source-code">        "train": TrainingInput(</p><p class="source-code">s3_data=step_process.properties.ProcessingOutputConfig.Outputs["train"].S3Output.S3Uri,</p><p class="source-code">            content_type="text/csv",</p><p class="source-code">        ),</p><p class="source-code">        "validation": TrainingInput(</p><p class="source-code">s3_data=step_process.properties.ProcessingOutputConfig.Outputs["validation"].S3Output.S3Uri,</p><p class="source-code">            content_type="text/csv",</p><p class="source-code">        ),</p><p class="source-code">    },</p><p class="source-code">)</p></li>
				<li>Next is<a id="_idIndexMarker768"/> to <a id="_idIndexMarker769"/>create a SageMaker Model from the training job using <strong class="source-inline">CreateModelStep()</strong>. <strong class="source-inline">CreateModelInput()</strong> takes instance types used for hosting purposes:<p class="source-code"># Line 346</p><p class="source-code">model = Model(</p><p class="source-code">    image_uri=image_uri,</p><p class="source-code">model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,</p><p class="source-code">    sagemaker_session=sagemaker_session,</p><p class="source-code">    role=role,</p><p class="source-code">)</p><p class="source-code">inputs = CreateModelInput(</p><p class="source-code">    instance_type="ml.m5.large",</p><p class="source-code">    accelerator_type="ml.eia1.medium",</p><p class="source-code">)</p><p class="source-code">step_create_model = CreateModelStep(</p><p class="source-code">    name="AbaloneCreateModel",</p><p class="source-code">    model=model,</p><p class="source-code">    inputs=inputs,</p><p class="source-code">)</p></li>
				<li>Once the SageMaker Model is created, two branches of model evaluation are performed. One<a id="_idIndexMarker770"/> is applied on a<a id="_idIndexMarker771"/> held-out test set for evaluation purposes using SageMaker Batch Transform <strong class="source-inline">Transformer</strong>:<p class="source-code"># Line 364</p><p class="source-code">transformer = Transformer(</p><p class="source-code">    model_name=step_create_model.properties.ModelName,</p><p class="source-code">    instance_type="ml.m5.xlarge",</p><p class="source-code">    instance_count=1,</p><p class="source-code">    accept="text/csv",</p><p class="source-code">    assemble_with="Line",</p><p class="source-code">    output_path=f"s3://{default_bucket}/AbaloneTransform",</p><p class="source-code">)</p><p class="source-code">step_transform = TransformStep(</p><p class="source-code">    name="AbaloneTransform",</p><p class="source-code">    transformer=transformer,</p><p class="source-code">    inputs=TransformInput(       data=step_process.properties.ProcessingOutputConfig.Outputs["test"].S3Output.S3Uri,</p><p class="source-code">    ...)</p><p class="source-code">)</p><p class="callout-heading">Note</p><p class="callout">The additional arguments in the <strong class="source-inline">TransformInput()</strong> class that have been omitted here in text but are available in <strong class="source-inline">pipeline.py</strong> are to configure Batch Transform input/output and to associate the output results with the input records. For more information, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html">https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html</a>.</p></li>
			</ol>
			<p>The output of<a id="_idIndexMarker772"/> the <a id="_idIndexMarker773"/>Batch Transform, which is the prediction, is then used to calculate model quality metrics such as mean absolute error, root mean squared error, and the r-squared value: </p>
			<p class="source-code">model_quality_check_config = ModelQualityCheckConfig(</p>
			<p class="source-code">    baseline_dataset=step_transform.properties.TransformOutput.S3OutputPath,</p>
			<p class="source-code">    dataset_format=DatasetFormat.csv(header=False),</p>
			<p class="source-code">    output_s3_uri=Join(on='/', values=['s3:/', default_bucket, base_job_prefix, ExecutionVariables.PIPELINE_EXECUTION_ID, 'modelqualitycheckstep']),</p>
			<p class="source-code">    problem_type='Regression',</p>
			<p class="source-code">    inference_attribute='_c0',</p>
			<p class="source-code">    ground_truth_attribute='_c1'</p>
			<p class="source-code">)</p>
			<p class="source-code">model_quality_check_step = QualityCheckStep(</p>
			<p class="source-code">    name="ModelQualityCheckStep",</p>
			<p class="source-code">    skip_check=skip_check_model_quality,</p>
			<p class="source-code">    register_new_baseline=register_new_baseline_model_quality,</p>
			<p class="source-code">    quality_check_config=model_quality_check_config,</p>
			<p class="source-code">    check_job_config=check_job_config,</p>
			<p class="source-code">supplied_baseline_statistics=supplied_baseline_statistics_model_quality,</p>
			<p class="source-code">supplied_baseline_constraints=supplied_baseline_constraints_model_quality,</p>
			<p class="source-code">    model_package_group_name=model_package_group_name</p>
			<p class="source-code">)</p>
			<ol>
				<li value="7">The other <a id="_idIndexMarker774"/>evaluation <a id="_idIndexMarker775"/>route <strong class="source-inline">EvaluateAbaloneModel</strong> and <strong class="source-inline">CheckMSEAbaloneEvalution</strong> aims to evaluate the test dataset and use the performance metric as a condition in the ML pipeline to only proceed to register the model in the model registry if the mean squared error is less than or equal to <strong class="source-inline">6.0</strong>:<p class="source-code"># Line 650</p><p class="source-code">cond_lte = ConditionLessThanOrEqualTo(</p><p class="source-code">    left=JsonGet(</p><p class="source-code">        step=step_eval,</p><p class="source-code">        property_file=evaluation_report,</p><p class="source-code">        json_path="regression_metrics.mse.value"</p><p class="source-code">    ),</p><p class="source-code">    right=6.0,</p><p class="source-code">)</p><p class="source-code">step_cond = ConditionStep(</p><p class="source-code">    name="CheckMSEAbaloneEvaluation",</p><p class="source-code">    conditions=[cond_lte],</p><p class="source-code">    if_steps=[step_register],</p><p class="source-code">    else_steps=[],</p><p class="source-code">)</p></li>
				<li>Two other checks <a id="_idIndexMarker776"/>are<a id="_idIndexMarker777"/> applied on models too in <strong class="source-inline">ModelBiasCheckStep</strong> and <strong class="source-inline">ModelExplainabilityCheckStep</strong>. They both use SageMaker Clarify to compute model bias and model explainability: <p class="source-code"># Line 450</p><p class="source-code">model_bias_check_step = ClarifyCheckStep(</p><p class="source-code">    name="ModelBiasCheckStep",</p><p class="source-code">    clarify_check_config=model_bias_check_config,</p><p class="source-code">    check_job_config=check_job_config,</p><p class="source-code">    skip_check=skip_check_model_bias,</p><p class="source-code">    register_new_baseline=register_new_baseline_model_bias,</p><p class="source-code">supplied_baseline_constraints=supplied_baseline_constraints_model_bias,</p><p class="source-code">    model_package_group_name=model_package_group_name</p><p class="source-code">) </p><p class="source-code"># Line 494</p><p class="source-code">model_explainability_check_step = ClarifyCheckStep(</p><p class="source-code">    name="ModelExplainabilityCheckStep",</p><p class="source-code">    clarify_check_config=model_explainability_check_config,</p><p class="source-code">    check_job_config=check_job_config,</p><p class="source-code">    skip_check=skip_check_model_explainability,</p><p class="source-code">register_new_baseline=register_new_baseline_model_explainability,</p><p class="source-code">supplied_baseline_constraints=supplied_baseline_constraints_model_explainability,</p><p class="source-code">    model_package_group_name=model_package_group_name</p><p class="source-code">)</p></li>
				<li>After the checks<a id="_idIndexMarker778"/> to<a id="_idIndexMarker779"/> confirm the model's performance, the model is registered in SageMaker Model Registry along with evaluation metrics, stored in the <strong class="source-inline">model_metrics</strong> variable, captured during the process, including performance metrics on test data, data bias, and model bias:<p class="source-code"># Line 635</p><p class="source-code">step_register = RegisterModel(</p><p class="source-code">    name="RegisterAbaloneModel",</p><p class="source-code">    estimator=xgb_train,</p><p class="source-code">model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,</p><p class="source-code">    content_types=["text/csv"],</p><p class="source-code">    response_types=["text/csv"],</p><p class="source-code">    inference_instances=["ml.t2.medium", "ml.m5.large"],</p><p class="source-code">    transform_instances=["ml.m5.large"],</p><p class="source-code">    model_package_group_name=model_package_group_name,</p><p class="source-code">    approval_status=model_approval_status,</p><p class="source-code">    model_metrics=model_metrics,</p><p class="source-code">    drift_check_baselines=drift_check_baselines</p><p class="source-code">)</p></li>
				<li>With the<a id="_idIndexMarker780"/> steps <a id="_idIndexMarker781"/>defined, they are put into the <strong class="source-inline">steps</strong> argument in a <strong class="source-inline">Pipeline</strong> object. Parameters that are exposed to users are placed in the <strong class="source-inline">parameters</strong> argument:<p class="source-code"># Line 666</p><p class="source-code">pipeline = Pipeline(</p><p class="source-code">    name=pipeline_name,</p><p class="source-code">    parameters=[</p><p class="source-code">        processing_instance_type,</p><p class="source-code">        processing_instance_count,</p><p class="source-code">        ...],</p><p class="source-code">    steps=[step_process, data_quality_check_step, data_bias_check_step, step_train, step_create_model, step_transform, model_quality_check_step, model_bias_check_step, model_explainability_check_step, step_eval, step_cond],</p><p class="source-code">    sagemaker_session=sagemaker_session,</p><p class="source-code">)</p></li>
			</ol>
			<p>You may wonder how SageMaker determines the order of the steps. SageMaker determines the order based on the data dependency and any explicit, custom dependency. We put the steps in a list of the <strong class="source-inline">steps</strong> argument and SageMaker takes care of the rest.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">After the project is created, the three CodePipeline pipelines are run automatically. Only the first pipeline, <strong class="source-inline">&lt;project-name-prefix&gt;-modelbuild</strong>, will proceed correctly. The other two pipelines, <strong class="source-inline">&lt;project-name-prefix&gt;-modeldeploy</strong> and <strong class="source-inline">&lt;project-name-prefix&gt;-modelmonitor</strong>, depend on the output of the first pipeline so they will fail in the first run. Don't worry about the failure status now. </p>
			<ol>
				<li value="11">At the end, a successfully executed pipeline creates and registers a model in SageMaker<a id="_idIndexMarker782"/> Model<a id="_idIndexMarker783"/> Registry. You can see the model in the model registry in the left sidebar, as shown in <em class="italic">Figure 11.10</em>. We will learn more about the model registry in later sections.</li>
			</ol>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B17447_11_010.jpg" alt="Figure 11.10 – Resulting model in SageMaker Model Registry&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.10 – Resulting model in SageMaker Model Registry</p>
			<p>There are several ways to run a pipeline. One is with the CI/CD process, which is how the pipeline initially runs after deployment from the template. We will talk more about the CI/CD process in the next section, <em class="italic">Running CI/CD in SageMaker Studio</em>. The following shows how to trigger the pipeline manually:</p>
			<ol>
				<li value="1">You can click <strong class="bold">Start an execution</strong> from <a id="_idIndexMarker784"/>the <a id="_idIndexMarker785"/>SageMaker Studio UI as depicted in <em class="italic">Figure 11.11</em>.</li>
			</ol>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B17447_11_011.jpg" alt="Figure 11.11 – Starting an execution of a pipeline in the pipeline list&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.11 – Starting an execution of a pipeline in the pipeline list</p>
			<ol>
				<li value="2">You can specify user inputs such as instance types, training data location, and other conditions for the checks, as shown in <em class="italic">Figure 11.12</em>. Click <strong class="bold">Start</strong> to start the workflow individually for a new dataset.</li>
			</ol>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B17447_11_012.jpg" alt="Figure 11.12 – Starting the execution of a pipeline with user inputs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.12 – Starting the execution of a pipeline with user inputs</p>
			<ol>
				<li value="3">You can also run<a id="_idIndexMarker786"/> a <a id="_idIndexMarker787"/>pipeline using the SageMaker Python SDK. The templatized code repository <strong class="source-inline">~/&lt;project-name-prefix&gt;/&lt;project-name-prefix&gt;-modelbuild/</strong> has an example notebook, <strong class="source-inline">sagemaker-pipelines-project.ipynb</strong>, explaining the code structure in greater detail and showing how to run a pipeline programmatically. You can open the notebook, as shown in <em class="italic">Figure 11.13</em>, and run it as an alternative.</li>
			</ol>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B17447_11_013.jpg" alt="Figure 11.13 – A screenshot of the sagemaker-pipelines-project.ipynb notebook that shows you details such as code structure in the repository, and runs the pipeline programmatically&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.13 – A screenshot of the sagemaker-pipelines-project.ipynb notebook that shows you details such as code structure in the repository, and runs the pipeline programmatically</p>
			<p>With SageMaker Pipelines, we can orchestrate steps that use SageMaker managed features to run <a id="_idIndexMarker788"/>an <a id="_idIndexMarker789"/>ML lifecycle. In the next section, let's see how the CI/CD system that the template creates uses SageMaker Pipelines for MLOps.</p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor149"/>Running CI/CD in SageMaker Studio</h1>
			<p>The ML <a id="_idIndexMarker790"/>pipeline<a id="_idIndexMarker791"/> we've seen running previously is just one part of our CI/CD system at work. The ML pipeline is triggered by a CI/CD pipeline in AWS CodePipeline. Let's dive into the three CI/CD pipelines that the SageMaker project template sets up for us.</p>
			<p>There are three CodePipeline pipelines:</p>
			<ul>
				<li><strong class="source-inline">&lt;project-name-prefix&gt;-modelbuild</strong>: The purpose of this pipeline is to run the ML pipeline and create an ML model in SageMaker Model Registry. This CI/CD pipeline runs the ML pipeline as a build step when triggered by a commit to the repository. The ML model in the SageMaker model registry needs to be approved in order to trigger the next pipeline, <strong class="source-inline">modeldeploy</strong>.</li>
				<li><strong class="source-inline">&lt;project-name-prefix&gt;-modeldeploy</strong>: The purpose of this pipeline is to deploy the latest approved ML model in the SageMaker model registry as a SageMaker endpoint. The build process deploys a staging endpoint first and requests manual approval before proceeding to deploy the model into production. This ensures the model and endpoint configuration are working correctly<a id="_idIndexMarker792"/> before deploying to production. Once<a id="_idIndexMarker793"/> the staging endpoint is deployed and becomes live with an <strong class="source-inline">InService</strong> status, it triggers the next pipeline, <strong class="source-inline">modelmonitor</strong>.</li>
				<li><strong class="source-inline">&lt;project-name-prefix&gt;-modelmonitor</strong>: The purpose of this pipeline is to deploy SageMaker Model Monitor to the two SageMaker endpoints created in the <strong class="source-inline">modeldeploy</strong> pipeline. This pipeline is triggered whenever a staging endpoint goes live and asks for manual approval on the model monitoring deployment for the staging endpoint before it deploys Model Monitor to the prod endpoint.</li>
			</ul>
			<p>Coming back to our previous ML pipeline execution, which is part of the <strong class="source-inline">modelbuild</strong> build process, we have a model created and <a id="_idIndexMarker794"/>registered in <strong class="bold">the model registry</strong>. This is the first checkpoint of the CI/CD system: <em class="italic">to manually verify the model performance metrics</em>. In order to proceed, we need to go to the model registry as shown in <em class="italic">Figure 11.10</em> to review the results. </p>
			<ol>
				<li value="1">From the view in <em class="italic">Figure 11.10</em>, double-click the model version entry in the model registry to see more detail about this model version, as shown in <em class="italic">Figure 11.14</em>. </li>
			</ol>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B17447_11_014.jpg" alt="Figure 11.14 – Detail page of a model version&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.14 – Detail page of a model version</p>
			<ol>
				<li value="2">We can <a id="_idIndexMarker795"/>view <a id="_idIndexMarker796"/>the model's performance in the <strong class="bold">Model quality</strong> tab, model explainability in the <strong class="bold">Explainability</strong> tab, and data bias in the <strong class="bold">Bias report</strong> tab. These are all relevant pieces of information to help us decide whether this is an acceptable model or not.</li>
				<li>Click the <strong class="bold">Update status</strong> button at the top right to approve or reject this model after review. For the sake of demonstration, we approve the model to proceed with the MLOps system, as shown in <em class="italic">Figure 11.15</em>. If we reject the model, nothing happens from this point.<div id="_idContainer150" class="IMG---Figure"><img src="image/B17447_11_015.jpg" alt="Figure 11.15 – Approve or reject a model version. You can put a comment in the box too&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 11.15 – Approve or reject a model version. You can put a comment in the box too</p>
			<ol>
				<li value="4">Model <a id="_idIndexMarker797"/>approval<a id="_idIndexMarker798"/> automatically triggers the execution of the <strong class="source-inline">modeldeploy</strong> pipeline. If you go to the CodePipeline console, you can see it in the <strong class="bold">In progress</strong> state, as shown in <em class="italic">Figure 11.16</em>.</li>
			</ol>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B17447_11_016.jpg" alt="Figure 11.16 – Model approval automatically triggers the modeldeploy pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.16 – Model approval automatically triggers the modeldeploy pipeline</p>
			<ol>
				<li value="5">As mentioned before, the <strong class="source-inline">modeldeploy</strong> pipeline first deploys a staging SageMaker endpoint for review. Once the endpoint is created (in 5-7 minutes), you can see a<a id="_idIndexMarker799"/> new <a id="_idIndexMarker800"/>event on the model version page, as shown in <em class="italic">Figure 11.17</em>. Click on <strong class="bold">Endpoint: &lt;project-name-prefix&gt;-staging</strong> to find out more information about the endpoint. You can test out the endpoint. </li>
			</ol>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B17447_11_017.jpg" alt="Figure 11.17 – Model version showing the latest event in the deployment of the staging endpoint&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.17 – Model version showing the latest event in the deployment of the staging endpoint</p>
			<ol>
				<li value="6">After confirming the endpoint's status, we can approve the staging endpoint deployment in the CodePipeline console. Click the pipeline name from <em class="italic">Figure 11.16</em>. We can see the current progress of the pipeline is pending in the <strong class="bold">DeployStaging</strong> stage, as shown in <em class="italic">Figure 11.18</em>. Click the <strong class="bold">Review</strong> button in the <strong class="bold">ApproveDeployment</strong> step to approve/reject the deployment.</li>
			</ol>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/B17447_11_018.jpg" alt="Figure 11.18 – Manual approval required by the modeldeploy pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.18 – Manual approval required by the modeldeploy pipeline</p>
			<ol>
				<li value="7">Approve or reject <a id="_idIndexMarker801"/>the<a id="_idIndexMarker802"/> deployment with any comments in the popup, as shown in <em class="italic">Figure 11.19</em>. As the endpoint is live and working, let's approve the staging deployment. </li>
			</ol>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B17447_11_019.jpg" alt="Figure 11.19 – Approve/reject a staging deployment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.19 – Approve/reject a staging deployment</p>
			<ol>
				<li value="8">The <strong class="source-inline">modeldeploy</strong> pipeline moves on to the final stage, <strong class="bold">DeployProd</strong>, to deploy <a id="_idIndexMarker803"/>the<a id="_idIndexMarker804"/> model to a production endpoint. Once deployed, the pipeline is updated to the <strong class="bold">Succeeded</strong> status. You can see a new event on the model version page, as shown in <em class="italic">Figure 11.20</em>. Also notice <strong class="bold">Last Stage</strong> is now <strong class="bold">prod</strong>.</li>
			</ol>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B17447_11_020.jpg" alt="Figure 11.20 – Model version is now updated to prod&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.20 – Model version is now updated to prod</p>
			<ol>
				<li value="9">When we approve the staging deployment, the <strong class="source-inline">modelmonitor</strong> pipeline is triggered to deploy SageMaker Model Monitor to the staging endpoint. We can see in the CodePipeline console that the <strong class="source-inline">modelmonitor</strong> pipeline is <strong class="bold">In progress</strong>, as shown in <em class="italic">Figure 11.21</em>.</li>
			</ol>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B17447_11_021.jpg" alt="Figure 11.21 – Staging endpoint deployment triggers the modelmonitor pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.21 – Staging endpoint deployment triggers the modelmonitor pipeline</p>
			<ol>
				<li value="10">The <strong class="source-inline">modelmonitor</strong> pipeline<a id="_idIndexMarker805"/> also requires manual <a id="_idIndexMarker806"/>approval in the DeployStaging stage. We should review the endpoint to see if Model Monitor is enabled. As shown in <em class="italic">Figure 11.22</em>, we can see in the <strong class="bold">Data quality</strong> tab that Model Monitor is indeed enabled and scheduled. We do not have a live traffic setup yet for the endpoint, and the monitoring schedule will only kick in at the top of the hour, so let's proceed and approve DeployStaging in the CodePipeline console similar to <em class="italic">step 6</em> and <em class="italic">step 7</em>. </li>
			</ol>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B17447_11_022.jpg" alt="Figure 11.22 – Reviewing the Model Monitor schedule for the staging endpoint&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.22 – Reviewing the Model Monitor schedule for the staging endpoint</p>
			<ol>
				<li value="11">Lastly, the<a id="_idIndexMarker807"/> DeployProd stage will also deploy SageMaker <a id="_idIndexMarker808"/>Model Monitor to the prod endpoint. This marks the end of the complete MLOps and CI/CD system. </li>
			</ol>
			<p>The three CI/CD pipelines in CodePipeline constitute a common MLOps system that enables continuous integration and continuous delivery of an ML model in response to any code changes to the <strong class="source-inline">modelbuild</strong> repository and to any manual ML pipeline runs. You do not have to worry about the complicated implementation as these steps take place automatically, thanks to the SageMaker projects template.</p>
			<p>SageMaker Projects make it easy to bring a robust MLOps system to your own ML use case with the templatized code and repositories. You don't have to build a sophisticated system. You can just choose a template provided by SageMaker projects that suits your use case and follow the README files in the repositories in CodeCommit to customize the configuration and code for your own use case. For example, we can update the model training in <strong class="source-inline">pipeline.py</strong> to use a different set of hyperparameters as shown in the following <a id="_idIndexMarker809"/>code <a id="_idIndexMarker810"/>block and commit the change to the <strong class="source-inline">modelbuild</strong> repository: </p>
			<p class="source-code"># Line 315 in <strong class="source-inline">pipeline.py</strong></p>
			<p class="source-code">xgb_train.set_hyperparameters(</p>
			<p class="source-code">    objective="reg:linear",</p>
			<p class="source-code">    num_round=70, # was 50</p>
			<p class="source-code">    max_depth=7, # was 7</p>
			<p class="source-code">    eta=0.2,</p>
			<p class="source-code">    gamma=4,</p>
			<p class="source-code">    min_child_weight=6,</p>
			<p class="source-code">    subsample=0.7,</p>
			<p class="source-code">    silent=0)</p>
			<p>You can see a new execution from the <strong class="source-inline">modelbuild</strong> pipeline with the latest commit message, as shown in <em class="italic">Figure 11.23</em>. </p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B17447_11_023.jpg" alt="Figure 11.23 – A new modelbuild execution is triggered by a commit to the repository&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.23 – A new modelbuild execution is triggered by a commit to the repository</p>
			<p>The CI/CD pipelines are going to be run as we described in this chapter once again to deliver a new model/endpoint automatically (except the manual approval steps) after we update the<a id="_idIndexMarker811"/> version<a id="_idIndexMarker812"/> of the core training algorithm. You can apply this to any changes to the ML pipeline, in the <strong class="source-inline">modelbuild</strong> pipeline, or configurations in the other two CI/CD pipelines.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor150"/>Summary</h1>
			<p>In this chapter, we described what MLOps is and what it does in the ML lifecycle. We discussed the benefits MLOps brings to the table. We showed you how you can easily spin up a sophisticated MLOps system powered by SageMaker projects from the SageMaker Studio IDE. We deployed a model build/deploy/monitor template from SageMaker projects and experienced what <em class="italic">everything as code</em> really means. </p>
			<p>We made a complete run of the CI/CD process to learn how things work in this MLOps system. We learned in great detail how an ML pipeline is implemented with SageMaker Pipelines and other SageMaker managed features. We also learned how the SageMaker model registry works to version control ML models. </p>
			<p>Furthermore, we showed how to monitor the CI/CD process and approve deployments in CodePipeline, which gives you great control over the quality of the models and deployment. With the MLOps system, you can enjoy the benefits we discussed: faster time to market, productivity, repeatability, reliability, auditability, and high-quality models.</p>
			<p>This example also perfectly summarizes what we've learned about Amazon SageMaker Studio throughout the book. Amazon SageMaker Studio is a purpose-built ML IDE that makes building ML models with an end-to-end ML lifecycle easy with its rich user interface. With the 11 chapters, code examples, and real-world ML use cases in this book, you've learned how to use SageMaker Studio and many SageMaker features for preparing data, building, training, deploying ML models, and running an MLOps system for a production-grade ML project. You now can start building your own ML projects in Amazon SageMaker Studio.</p>
		</div>
	</body></html>