- en: Chapter 12. Recommendation Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章 推荐系统
- en: 'In our final chapter, we''ll tackle one of the most ubiquitous problems prevalent
    in the e-commerce world: making effective product recommendations to customers.
    Recommendation systems, also referred to as recommender systems, often rely on
    the notion of similarity between objects in an approach known as collaborative
    filtering. Its basic premise is that customers can be considered similar to each
    other if they share most of the products that they have purchased; equally, items
    can be considered similar to each other if they share a large number of customers
    who purchased them.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的最后一章中，我们将解决电子商务世界中普遍存在的问题之一：向客户做出有效的产品推荐。推荐系统，也称为推荐器系统，通常依赖于对象之间相似性的概念，这种方法被称为协同过滤。其基本前提是，如果顾客购买的产品大部分相同，则可以认为他们彼此相似；同样，如果它们有大量共同购买者，则可以认为项目彼此相似。
- en: There are a number of different ways to quantify this notion of similarity,
    and we will present some of the commonly used alternatives. Whether we want to
    recommend movies, books, hotels, or restaurants, building a recommender system
    often involves dealing with very large datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的方法可以量化这种相似性的概念，我们将介绍一些常用的替代方案。无论我们想要推荐电影、书籍、酒店还是餐厅，构建推荐系统通常涉及处理非常大的数据集。
- en: Rating matrix
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评分矩阵
- en: A **recommendation system** usually involves having a set of users, *U = {u[1]*
    *, u[2]* *, …, u[m]* *},* that have varying preferences on a set of items, *I
    = {i[1], i[2], …, i[n]* *}*. The number of users, *|U| = m*, is usually different
    from the number of items, *|I| = n*. In addition, users can often express their
    preference by rating items on some scale. As an example, we can think of users
    as being restaurant patrons in a city, and the items being the restaurants that
    they visit. Under this setup, the preferences of the users could be expressed
    as ratings on a five-star scale. Of course, our generalization does not require
    that the items be physical items or that the users be actual people—this is simply
    an abstraction that is commonly used for the recommender system problem.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**推荐系统**通常涉及一组用户，*U = {u[1]* *, u[2]* *, …, u[m]* *}，他们对一组项目，*I = {i[1], i[2],
    …, i[n]* *}，有不同的偏好。用户的数量，*|U| = m*，通常与项目的数量，*|I| = n*，不同。此外，用户通常可以通过对某些项目进行评分来表达他们的偏好。例如，我们可以将用户视为城市中的餐厅顾客，项目是他们访问的餐厅。在这种设置下，用户的偏好可以用五星级评分来表示。当然，我们的推广并不要求项目是实物或用户是真实的人——这只是一个在推荐系统问题中常用的抽象。'
- en: 'As an illustration, think of a dating website in which users rate other users;
    here, the *items* that are being rated are the profiles of the actual users themselves.
    Let''s return to our example of a restaurant recommender system and build some
    example data. A natural data structure that is popular for recommendation systems
    is the **rating matrix**. This is an *m × n* matrix where the rows represent the
    users and the columns represent the items. Each entry, *e[i], [j]*, of the matrix
    represents the rating made by the user *i* for item *j*. What follows is a simple
    example:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 作为说明，想象一个用户评分的交友网站；在这里，被评分的*项目*是实际用户的个人资料。让我们回到我们的餐厅推荐系统示例，并构建一些示例数据。对于推荐系统来说，一种流行的自然数据结构是**评分矩阵**。这是一个*m
    × n*矩阵，其中行代表用户，列代表项目。矩阵中的每个条目*e[i]，[j]*代表用户*i*对项目*j*的评分。以下是一个简单的例子：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we have used a 10-point scale as a rating system, where 10 is the highest
    rating and 1 is the lowest. An alternative rating scale is a binary rating scale,
    where 1 indicates a positive rating and 0 indicates a negative rating. This second
    approach would yield a binary rating matrix. How might we be able to use this
    rating matrix in order to inform a simple recommender system for other users?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了一个10点评分系统作为评价标准，其中10是最高评价，1是最低评价。另一种评分系统是二进制评分系统，其中1表示正面评价，0表示负面评价。第二种方法将产生一个二进制评分矩阵。我们如何能够利用这个评分矩阵来为其他用户提供一个简单的推荐系统呢？
- en: Concretely, suppose that a new user, Silvan, has rated a few restaurants and
    we would like to make a recommendation for a suitable restaurant to which he has
    not been. Alternatively, we might want to propose a list of the top three restaurants
    or even predict whether Silvan will like a specific restaurant he is currently
    considering.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，假设一个新用户Silvan已经对几家餐厅进行了评分，我们希望为他推荐一家他尚未光顾的餐厅。或者，我们可能想要提出前三家餐厅的列表，甚至预测Silvan是否会喜欢他目前正在考虑的特定餐厅。
- en: One way to think about this problem is to find users that have similar views
    to Silvan on the restaurants that he has already rated. Then, we could use their
    ratings on restaurants that Silvan has not yet rated in order to predict Silvan's
    rating for those restaurants. This seems promising, but we should first think
    about how we might quantify this notion of similarity between two users based
    on their item ratings.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个问题的方法之一是找到与Silvan在已评分餐厅上有相似观点的用户。然后，我们可以使用他们对Silvan尚未评分的餐厅的评分来预测Silvan对这些餐厅的评分。这似乎很有希望，但我们应该首先考虑如何量化基于他们项目评分的两个用户之间的相似性概念。
- en: Measuring user similarity
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量用户相似度
- en: Even with a very large database of users, chances are that, for a real-world
    recommender system, it will be rare—if not massively unlikely—to find two people
    who would rate all the items in our item set with the exact same score. That being
    said, we can still say that some users are more similar than others based on how
    they rate different items. For example, in our restaurant rating matrix, we can
    see that Ines and Oliver rated the first four restaurants poorly and the last
    four restaurants highly, and so their tastes can be considered far more similar
    compared to a pair such as Thibault and Pedro, who sometimes agree and sometimes
    have completely opposite views on a particular restaurant.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 即使拥有一个非常庞大的用户数据库，对于现实世界的推荐系统来说，找到两个对项目集内所有项目给出完全相同评分的人的可能性很小——如果不是极其不可能的话。话虽如此，我们仍然可以根据用户对不同项目的评分情况，说一些用户比其他用户更相似。例如，在我们的餐厅评分矩阵中，我们可以看到Ines和Oliver对前四家餐厅的评价较差，而对后四家餐厅的评价较高，因此他们的口味可以被认为是与Thibault和Pedro这样的配对相比要相似得多，后者有时意见一致，有时对特定餐厅的意见完全相反。
- en: By representing a user as their particular row in the rating matrix, we can
    think of a user as being a vector in an *n* dimensional space, *n* being the number
    of items. Thus, we can use different distance measures appropriate for vectors
    in order to measure the similarity of two different users. Note that the notion
    of distance is inversely proportional to the notion of similarity, and we can
    thus use measures of distance as measures of similarity by interpreting a large
    distance between two vectors as analogous to a low similarity score.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将用户表示为评分矩阵中的特定行，我们可以将用户视为一个在*n*维空间中的向量，其中*n*是项目的数量。因此，我们可以使用适合向量距离度量的不同度量来衡量两个不同用户的相似度。请注意，距离的概念与相似度的概念成反比，因此我们可以通过将两个向量之间的大距离解释为低相似度分数来将距离度量作为相似度度量。
- en: 'The most familiar distance metric for two vectors, *a* and *b,* is the **Euclidean
    distance**:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个向量*a*和*b*，最熟悉的距离度量是**欧几里得距离**：
- en: '![Measuring user similarity](img/00198.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![测量用户相似度](img/00198.jpeg)'
- en: 'We can use R''s built-in `dist()` function to compute all the pair-wise distances
    in our rating matrix as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用R的内置`dist()`函数来计算评分矩阵中的所有成对距离，如下所示：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The result is a lower triangular matrix because the Euclidean distance is a
    symmetric function. Thus, the entry for (`maria`, `pedro`) is exactly the same
    as for (`pedro`, `maria`), and so we only need to display one of these. Here,
    we can explicitly see that Ines and Oliver are the two most similar users as the
    distance between them is the smallest. Note that we can also talk about the distances
    between items in terms of the similarity of the ratings they received from different
    users. All we have to do to compute this is to transpose the rating matrix:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个下三角矩阵，因为欧几里得距离是一个对称函数。因此，(`maria`, `pedro`)的条目与(`pedro`, `maria`)的条目完全相同，所以我们只需要显示其中一个。在这里，我们可以清楚地看到Ines和Oliver是两个最相似的用户，因为他们之间的距离是最小的。请注意，我们也可以根据从不同用户那里收到的评分相似性来谈论项目之间的距离。为了计算这一点，我们只需要转置评分矩阵：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we can see, the two most dissimilar restaurants (that is to say, those with
    the largest difference between them) are the *Acropolis* and *Berny's*. Looking
    back at the rating matrix, we should easily see why this is the case. The former
    restaurant has received largely positive reviews across our user base, whereas
    the reviews have been poor for the latter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，两个最不相似的餐厅（即它们之间差异最大的餐厅）是*Acropolis*和*Berny's*。回顾评分矩阵，我们应该很容易看出为什么是这样。前者在我们的用户基础中收到了大量正面评价，而后者则收到了较差的评价。
- en: 'A commonly used alternative to the Euclidean distance (or L2 norm, as it is
    also known) is the **cosine distance**. This metric measures the cosine of the
    smallest angle between two vectors. If the vectors are parallel to each other,
    meaning that their angle is 0, then the cosine distance is 0 as well. If the two
    vectors are at a right angle to each other, then they have the largest distance
    according to this metric. The cosine distance is given by:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离（或称为L2范数）的一个常用替代方法是**余弦距离**。这个度量衡量的是两个向量之间最小角度的余弦值。如果两个向量相互平行，即它们的夹角为0，那么余弦距离也是0。如果两个向量相互垂直，那么根据这个度量，它们之间的距离最大。余弦距离由以下公式给出：
- en: '![Measuring user similarity](img/00199.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![测量用户相似度](img/00199.jpeg)'
- en: 'Here, the numerator is the dot product between the two vectors and the denominator
    is the product of the magnitudes (typically computed via the L2 norm) of the two
    vectors. The cosine distance isn''t available as a method in the `dist()` function
    of R''s base distribution, but we can install the `proxy` package, which enhances
    this function with a number of new distance metrics in order to compute the cosine
    distances for our rating matrix:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，分子是两个向量的点积，分母是两个向量模长的乘积（通常通过L2范数计算）。余弦距离在R的基础分布的`dist()`函数中不是一个可用方法，但我们可以安装`proxy`包，它通过添加一些新的距离度量来增强这个函数，以便计算我们的评分矩阵的余弦距离：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Suppose instead that our users rated restaurants on a binary scale. We can
    convert our rating matrix into a binary rating matrix by considering all ratings
    above 5 to be positive and assigning them a new score of 1\. The remaining ratings
    are all converted to a score of 0\. For two binary vectors, the **Jaccard similarity**
    is given by the cardinality of the logical intersection divided by the cardinality
    of the logical union. The Jaccard distance is then computed as 1 minus this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的用户对餐厅的评分是二进制的。我们可以通过将所有高于5的评分视为正面，并赋予它们新的分数1，将我们的评分矩阵转换为二进制评分矩阵。其余的评分都转换为分数0。对于两个二进制向量，**Jaccard相似度**由逻辑交集的基数除以逻辑并集的基数给出。Jaccard距离则是这个值的1减去：
- en: '![Measuring user similarity](img/00200.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![测量用户相似度](img/00200.jpeg)'
- en: 'In a nutshell, what this is computing is one minus the ratio of the number
    of positions in which the two vectors both have a positive rating over the total
    number of positions in which either of the two vectors have a positive rating.
    Two binary vectors that agree in all their positive positions will be identical
    and thus have a distance of 0\. Using the `proxy` package, we can show the `Jaccard`
    distance for our restaurant patrons as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这计算的是两个向量在位置上都有正面评分的数量与两个向量中任意一个有正面评分的总位置数量的比率之差。两个在所有正面位置上达成一致的二进制向量将是相同的，因此距离为0。使用`proxy`包，我们可以如下显示我们的餐厅顾客的`Jaccard`距离：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The study of measurement and distance metrics is broad and there are many suitable
    metrics that have been applied to the recommender system setting. The definitive
    reference for distance metrics is the *Encyclopedia of Distances*, *Michel Marie
    Deza and Elena Deza*, *Springer*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 测量和距离度量的研究范围很广，有许多适合的度量已经应用于推荐系统设置。距离度量的权威参考是*《距离度量百科全书》*，*米歇尔·玛丽·德扎和伊莲娜·德扎*，*斯普林格*。
- en: Collaborative filtering
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协同过滤
- en: Having covered distances, we are ready to delve into the topic of **collaborative
    filtering**, which will help us to define a strategy for making recommendations.
    Collaborative filtering describes an algorithm, or more precisely a family of
    algorithms, that aims to create recommendations for a test user given only information
    about the ratings of other users via the rating matrix, as well as any ratings
    that the test user has already made.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在覆盖了距离之后，我们准备深入探讨**协同过滤**这一主题，这将帮助我们定义一个制定推荐策略的方法。协同过滤描述了一种算法，或者更确切地说，是一系列算法，旨在根据其他用户的评分信息（通过评分矩阵）以及测试用户已经做出的任何评分，为测试用户创建推荐。协同过滤旨在根据其他用户的评分信息（通过评分矩阵）以及测试用户已经做出的任何评分，为测试用户创建推荐。
- en: There are two very common variants of collaborative filtering, **memory-based
    collaborative filtering** and **model-based collaborative filtering**. With memory-based
    collaborative filtering, the entire history of all the ratings made by all the
    users is remembered and must be processed in order to make a recommendation. The
    prototypical memory-based collaborative filtering method is **user-based collaborative
    filtering**. Although this approach uses all the ratings available, the downside
    is that it can be computationally expensive as the entire database is used in
    order to make rating predictions for our test user.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤有两种非常常见的变体，**基于记忆的协同过滤**和**基于模型的协同过滤**。在基于记忆的协同过滤中，所有用户所有评分的历史都被记住，并且必须处理这些评分以做出推荐。典型的基于记忆的协同过滤方法是**基于用户的协同过滤**。尽管这种方法使用了所有可用的评分，但缺点是它可能计算成本高昂，因为整个数据库都用于为我们的测试用户做出评分预测。
- en: The alternative approach to this is embodied in model-based collaborative filtering.
    Here, we first create a model of the rating preferences of our users, such as
    a set of clusters of users who like similar items, and then use the model to generate
    recommendations. We will study **item-based collaborative filtering**, which is
    the most well-known, model-based collaborative filtering method.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相反的方法是模型基础的协同过滤。在这里，我们首先创建一个模型来模拟用户的评分偏好，例如一组喜欢相似物品的用户集群，然后使用该模型生成推荐。我们将研究**基于物品的协同过滤**，这是最著名的模型基础协同过滤方法。
- en: User-based collaborative filtering
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于用户的协同过滤
- en: User-based collaborative filtering is commonly described as a memory-based or
    lazy learning approach. Unlike most of the models we have built in this book,
    which assume that we will fit the data to a particular model and then use that
    model to make predictions, lazy learning simply uses the training data itself
    to make predictions directly. We saw an example of lazy learning with k-nearest
    neighbors in [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*.
    In fact, the premise of the user-based collaborative filtering approach builds
    directly on the k-nearest neighbors approach.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于用户的协同过滤通常被描述为基于记忆或懒惰学习的方法。与本书中我们构建的大多数模型不同，这些模型假设我们将数据拟合到特定的模型，然后使用该模型进行预测，而懒惰学习只是直接使用训练数据本身进行预测。我们在[第1章](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "第1章。为预测建模做准备")中看到了懒惰学习的例子，即使用k近邻算法，在“为预测建模做准备”中。实际上，基于用户的协同过滤方法的前提直接建立在k近邻方法之上。
- en: However, in user-based collaborative filtering, when we want to make recommendations
    for a new user, we will first pick a set of similar users using a particular distance
    metric. Then, we try to infer the ratings that our target user would assign to
    items that he or she has not yet rated as an average of the ratings made by similar
    users on those items. We usually refer to this set of similar users as the user's
    **neighborhood**. Thus, the idea is that a user will prefer items that their neighborhood
    prefers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在基于用户的协同过滤中，当我们想要为新用户做出推荐时，我们首先会使用特定的距离度量选择一组相似用户。然后，我们试图推断目标用户将分配给尚未评分的物品的评分，作为相似用户对这些物品评分的平均值。我们通常将这组相似用户称为用户的**邻域**。因此，这种想法是，用户将更喜欢他们的邻域所偏好的物品。
- en: Typically, there are two ways to define the user's neighborhood. We can compute
    a fixed neighborhood by finding the k-nearest neighbors. These are the *k* users
    in our database that have the smallest distance between them and our target user.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有两种定义用户邻域的方法。我们可以通过找到k近邻来计算一个固定的邻域。这些是我们数据库中与目标用户距离最小的k个用户。
- en: Alternatively, we can specify a similarity threshold and pick all the users
    in our database whose distance from our target user does not exceed this threshold.
    This second approach has the advantage that we will be making recommendations
    by employing users that are as close to our target user as we want, and therefore
    our confidence in our recommendation can be high. On the other hand, there might
    be very few users that satisfy our requirement, meaning that we will be relying
    on the recommendations of these few users. Worse, there might be no users in our
    database who are sufficiently similar to our target user and we might not be able
    to actually make a recommendation at all. If we don't mind our method sometimes
    failing to make a recommendation, for example because we have a backup plan to
    handle these cases, the second approach might be a good choice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以指定一个相似性阈值，并选择数据库中与目标用户距离不超过此阈值的所有用户。这种第二种方法的优势在于，我们将通过使用尽可能接近目标用户的用户来进行推荐，因此我们可以对我们的推荐有很高的信心。另一方面，可能只有很少的用户满足我们的要求，这意味着我们将依赖于这些少数用户的推荐。更糟糕的是，可能没有足够相似于目标用户的用户在我们的数据库中，我们可能根本无法做出任何推荐。如果我们不介意我们的方法有时无法做出推荐，例如因为我们有备用计划来处理这些情况，那么第二种方法可能是一个不错的选择。
- en: Another important consideration to make in a real-world setting is the problem
    of sparse ratings. In our simple restaurant example, every user had rated every
    restaurant. This rarely, if ever, happens in a real situation, simply because
    the number of items is usually too big for a user to rate them all. If we think
    of e-commerce websites such as Amazon.com, for example, it is easy to imagine
    that the maximum number of products that any user has rated is still only a small
    fraction of the overall number of products on sale.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的场景中，另一个重要的考虑因素是稀疏评分问题。在我们的简单餐厅示例中，每个用户都对每家餐厅进行了评分。这种情况在现实中很少发生，甚至几乎不会发生，因为通常物品的数量太多，以至于用户无法对它们全部进行评分。如果我们以亚马逊.com等电子商务网站为例，例如，很容易想象任何用户已评分的产品数量仍然是销售产品总数的极小部分。
- en: To compute distance metrics between users in order to determine similarity,
    we usually incorporate only the items that both users have rated. Consequently,
    in practice, we often make comparisons between users in a smaller number of dimensions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算用户之间的距离度量以确定相似性，我们通常只包含两个用户都评分的项目。因此，在实践中，我们经常在更少的维度上对用户进行比较。
- en: 'Once we have decided on a distance metric and how to form a neighborhood of
    users that are similar to our test user, we then use this neighborhood to compute
    the missing ratings for the test user. The easiest way to do this is to simply
    compute the average rating for each item in the user neighborhood and report this
    value. Thus, for test user *t* and an item *j*, for which the test user has not
    yet made a rating, we can predict the test user''s rating for that item,![User-based
    collaborative filtering](img/00201.jpeg) as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了距离度量标准以及如何形成与我们的测试用户相似的用户邻域，我们就使用这个邻域来计算测试用户的缺失评分。这样做最简单的方法是计算用户邻域中每个项目的平均评分，并报告这个值。因此，对于测试用户
    *t* 和一个测试用户尚未评分的项目 *j*，我们可以预测测试用户对该项目的评分，![基于用户的协同过滤](img/00201.jpeg)，如下所示：
- en: '![User-based collaborative filtering](img/00202.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![基于用户的协同过滤](img/00202.jpeg)'
- en: This equation expresses the simple idea that the predicted rating of our test
    user *t* for item *j* is just the average of the ratings made by the test user's
    neighborhood on this item. Suppose we had a new user for our restaurant scenario
    and this user had already rated a few of the restaurants. Then, imagine that,
    from those ratings, we discovered that our new user's neighborhood comprised Oliver
    and Thibault. If we wanted to make a prediction for what rating the test user
    would make on the restaurant *El Pollo Loco*, this would be done by averaging
    the ratings of Oliver and Thibault for this restaurant, which in this case would
    be the average of 2 and 4, yielding a rating of 3.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程表达了这样一个简单想法：我们的测试用户 *t* 对项目 *j* 的预测评分只是测试用户邻域对该项目的评分的平均值。假设我们有一个新的餐厅场景用户，并且这个用户已经对几家餐厅进行了评分。然后，想象一下，从这些评分中，我们发现新用户的邻域包括Oliver和Thibault。如果我们想要预测测试用户对餐厅*El
    Pollo Loco*的评分，这将是通过平均Oliver和Thibault对该餐厅的评分来完成的，在这种情况下，这将是对2和4的平均值，得到评分为3。
- en: If our objective was to obtain a top-N list of recommendations for our user,
    we would repeat this process for all the items in the database, sort them in descending
    order so that the highest rated items appeared first, and then pick out the top
    *N* items from this list. In practice, we would only need to check the items that
    at least one of the users in the new user's neighborhood has rated in order to
    simplify this computation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的目标是获取用户的前N个推荐列表，我们将对数据库中的所有项目重复此过程，按降序排列，以便评分最高的项目排在最前面，然后从这个列表中挑选出前N个项目。在实践中，我们只需要检查新用户邻域中至少有一个用户评价过的项目，以简化这个计算。
- en: 'We can make some improvements to this very simple approach. A first possible
    improvement comes from the observation that some users will tend to consistently
    rate items more strictly or more leniently than other users, and we would like
    to smooth out this variation. In practice, we often use *Z*-score normalization,
    which takes into account the variance of ratings. We can also center each rating
    made by a user by subtracting that user''s average rating across all the items.
    In the rating matrix, this means subtracting the mean of each row from the elements
    of the row. Let''s apply this last transformation to our restaurant rating matrix
    and see the results:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对这种非常简单的方法进行一些改进。首先可能的改进来自于观察，一些用户可能会比其他用户更严格或更宽松地持续评价项目，我们希望平滑这种变化。在实践中，我们经常使用*Z*分数标准化，它考虑了评分的方差。我们还可以通过减去用户对所有项目的平均评分来对用户的每个评分进行中心化。在评分矩阵中，这意味着从每一行的元素中减去该行的平均值。让我们将这个最后的转换应用到我们的餐厅评分矩阵上，看看结果如何：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Even though both Ines and Gertrude originally rated `Berny's` with the same
    rating of 1, the centering operation has Ines rating this restaurant with a lower
    score than Gertrude. This is because Ines tends to make higher ratings on average
    than Gertrude, and so the rating of 1 for Ines could be interpreted as a stronger
    negative rating than Gertrude's.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Ines和Gertrude最初都给`Berny's`评了相同的1分，但中心化操作使得Ines给这家餐厅的评分低于Gertrude。这是因为Ines的平均评分通常高于Gertrude，因此Ines的1分可以解释为比Gertrude的更强的负面评分。
- en: Another area of improvement concerns the way in which we incorporate the ratings
    of our new user's neighborhood to create the final ratings. By treating the ratings
    of all the neighboring users as equal, we are ignoring the fact that our distance
    metric may show that certain users in the neighborhood of the new user are more
    similar to the new user than others.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个改进的领域涉及我们如何将新用户邻域的评分纳入以创建最终的评分。通过将所有相邻用户的评分视为相等，我们忽略了这样一个事实，即我们的距离度量可能表明新用户邻域中的某些用户比其他用户更相似。
- en: As we have already seen in the example on Jaccard similarity and Jaccard distance,
    we can often define a similarity metric from a distance metric by inverting it
    in some way, such as subtracting from 1 or taking the reciprocal. Consequently,
    for the distance metric of our choice, we can define its corresponding similarity
    metric and denote it with *sim(u,t)*. A user similarity metric takes high values
    for similar users, which are users for whom a distance metric takes low values.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在Jaccard相似性和Jaccard距离的例子中已经看到的，我们通常可以通过以某种方式反转距离度量来定义一个相似度度量。例如，从1中减去或取倒数。因此，对于我们选择的距离度量，我们可以定义其相应的相似度度量，并用*sim(u,t)*表示。用户相似度度量对相似用户具有高值，这些用户在距离度量中具有低值。
- en: 'With this clarification established, we can incorporate the similarity between
    users *u* and *t* in our previous equation by taking a weighted average of the
    ratings made by the neighboring users of the new user as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在明确了这一点之后，我们可以通过取新用户邻域中相邻用户的评分的加权平均来将用户u和t之间的相似性纳入我们之前的方程中，如下所示：
- en: '![User-based collaborative filtering](img/00203.jpeg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![基于用户的协同过滤](img/00203.jpeg)'
- en: Other reasons why we might want to incorporate weights in the ratings made by
    other users include trust. For example, we might trust a user that has been using
    our restaurant recommendation service for a long time more who a more recent user.
    Equally, we might also want to consider the total number of items that a user
    has rated in common with the new user. For example, if a user has only rated two
    items in common with the new user, then even, if the corresponding ratings made
    are identical, the evidence that these two users are indeed very similar is limited.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能希望在用户评分中包含权重的原因还包括信任度。例如，我们可能更信任那些长期使用我们的餐厅推荐服务的用户，而不是新用户。同样，我们也可能想要考虑用户与新用户共同评价的物品总数。例如，如果一个用户只与一个新用户共同评价了两件物品，那么即使相应的评分是相同的，这两个用户确实非常相似的证据也是有限的。
- en: All in all, the single largest difficulty with user-based collaborative filtering
    is that making recommendations for a test user requires access to the whole database
    of users in order to determine the user neighborhood. This is done by performing
    a similarity computation between the test user and every other user, an expensive
    process computationally. Next, we'll look at item-based collaborative filtering,
    which attempts to ameliorate this situation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，基于用户的协同过滤最大的困难在于，为测试用户做出推荐需要访问整个用户数据库，以便确定用户邻域。这是通过在测试用户和每个其他用户之间执行相似性计算来完成的，这是一个计算上昂贵的步骤。接下来，我们将探讨基于物品的协同过滤，它试图改善这种情况。
- en: Item-based collaborative filtering
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于物品的协同过滤
- en: Item-based collaborative filtering is a model-based approach to collaborative
    filtering. The central idea underlying this method is that, instead of looking
    at other users similar to the test user, we will directly recommend items that
    are similar to the items that have received a high rating by the test user. As
    we are directly comparing items, instead of first comparing users, to recommend
    items, we can build up a model to describe the similarity of items and then use
    the model rather than the entire database to make recommendations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 基于物品的协同过滤是一种基于模型的协同过滤方法。这种方法的核心思想是，我们不会像对待测试用户那样查看其他相似用户，而是直接推荐那些与测试用户评价高的物品相似的物品。由于我们是直接比较物品，而不是首先比较用户来推荐物品，因此我们可以建立一个模型来描述物品之间的相似性，然后使用该模型而不是整个数据库来做出推荐。
- en: The process of building an item-based similarity model involves computing a
    similarity matrix for all pairs of items in our database. If we have *N* items,
    then we will end up with a similarity matrix with *N²* elements in total. To reduce
    the size of our model, we can store a list of the similarity values of the top
    *k* most similar items for every item in the database.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 建立基于物品的相似性模型的过程包括计算数据库中所有物品对之间的相似性矩阵。如果我们有 *N* 件物品，那么我们将得到一个包含 *N²* 个元素的相似性矩阵。为了减少模型的大小，我们可以存储数据库中每个物品的前
    *k* 个最相似物品的相似性值列表。
- en: As *k* will be far smaller than *N*, we will have a very substantial reduction
    in the size of the data that we need to keep for our model. For every item in
    our database, this list of the *k* most similar items is analogous to the neighborhood
    of users for the user-based collaborative filtering approach. The same discussion
    regarding normalizing with respect to the bias and variance of user ratings in
    user-based collaborative filtering can be applied here. That is, we can compute
    item-to-item similarities after we normalize our rating matrix.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *k* 将远小于 *N*，我们将大大减少我们需要为模型保留的数据量。对于数据库中的每个物品，这个包含 *k* 个最相似物品的列表类似于基于用户的协同过滤方法中的用户邻域。关于在基于用户的协同过滤中对用户评分的偏差和方差进行归一化的讨论也适用于此处。也就是说，我们可以在归一化评分矩阵后计算物品到物品的相似性。
- en: This approach is not without its shortcomings. In the memory-based recommender,
    a new user rating can automatically be incorporated into the recommendation process
    because that approach uses the entire database (rating matrix). Model-based collaborative
    filtering requires us to periodically retrain the model to incorporate information
    from these new ratings. In addition, the fact that the modeling process discards
    some information from the original rating matrix means that it can sometimes make
    non-optimal recommendations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法并非没有缺点。在基于内存的推荐系统中，由于该方法使用整个数据库（评分矩阵），新的用户评分可以自动纳入推荐过程。基于模型的协同过滤要求我们定期重新训练模型以纳入这些新评分的信息。此外，建模过程丢弃原始评分矩阵中的一些信息，这意味着它有时会做出非最优的推荐。
- en: Despite these drawbacks, the space and time performance of item-based collaborative
    filtering means that it has been very successfully applied in a number of real-world
    settings. Model retraining can be done offline and automatically scheduled, and
    the non-optimality of recommendations can often be tolerated.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些缺点，基于物品的协同过滤在空间和时间性能上的表现意味着它在许多现实世界的场景中得到了非常成功的应用。模型的重训练可以在离线状态下进行，并且可以自动安排，而且推荐的非最优性通常是可以容忍的。
- en: 'We can devise an analogous equation to what we saw for user-based collaborative
    filtering that explains how to predict a new rating using the item-based collaborative
    filtering model. Suppose we want to estimate the rating that our test user, *t*,
    would give to an item, *I*. Suppose also that we have already chosen a similarity
    function, *sim(i,j)*, between a pair of items, *i* and *j*, and from this we constructed
    our model. Using the model, we can retrieve the stored item neighborhood for the
    item in which we are interested, *S(i)*. To compute the predicted rating that
    our test user will make on this item, we calculate the weighted sum of the ratings
    our user has made on items that are similar to it:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以设计一个类似于用户基于协同过滤的方程，解释如何使用基于物品的协同过滤模型来预测新的评分。假设我们想要估计我们的测试用户*t*对物品*I*的评分。假设我们已选择了一对物品*i*和*j*之间的相似度函数*sim(i,j)*，并据此构建了我们的模型。使用该模型，我们可以检索到我们感兴趣的物品的存储物品邻域*S(i)*。为了计算我们的测试用户对这一物品的预测评分，我们计算用户对与其相似的物品所做评分的加权总和：
- en: '![Item-based collaborative filtering](img/00204.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![基于物品的协同过滤](img/00204.jpeg)'
- en: While this approach won't work if the user hasn't rated any items similar to
    the item in question, it does not require finding users that have similar preferences
    to the test user.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户没有对与问题项类似的任何项进行评分时，这种方法可能不会奏效，但它不需要找到与测试用户有相似偏好的用户。
- en: Singular value decomposition
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: In a real-world recommender system, the rating matrix will eventually become
    very large as more users are added to the system and the list of items being offered
    grows. As a result, we may want to apply a dimensionality reduction technique
    to this matrix. Ideally, we would like to retain as much information as possible
    from the original matrix while doing this. One such method that has applications
    across a wide range of disciplines uses **singular value decomposition**, or **SVD**
    as it is commonly abbreviated to.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的推荐系统中，随着更多用户被添加到系统中以及提供的物品列表的增长，评分矩阵最终会变得非常大。因此，我们可能希望对此矩阵应用一种降维技术。理想情况下，我们希望在降维的同时尽可能保留原始矩阵中的信息。这种方法在许多学科领域都有应用，它就是**奇异值分解**，或简称**SVD**。
- en: SVD is a matrix factorization technique that has a number of useful applications,
    one of which is dimensionality reduction. It is related to the PCA method of dimensionality
    reduction that we saw in [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*,
    and many people confuse the two. SVD actually describes just a mathematical method
    of factorizing matrices. In fact, some implementations of PCA use SVD to compute
    the principal components.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: SVD是一种矩阵分解技术，具有许多有用的应用，其中之一就是降维。它与我们在[第1章](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "第1章。为预测建模做准备")中看到的降维方法PCA有关，*为预测建模做准备*，许多人混淆了这两个概念。实际上，SVD只是描述了分解矩阵的数学方法。事实上，一些PCA的实现使用SVD来计算主成分。
- en: 'Let''s begin by looking at how this process works. SVD is a matrix factorization
    process, so we start with an original matrix representing our data and express
    this as a product of matrices. In a dimensionality reduction scenario, our input
    data matrix would be the matrix where the rows are data points and the columns
    are the features; thus, in R, this would just be a data frame. In our recommender
    systems scenario, the matrix we use is our rating matrix. Suppose that we call
    our rating matrix *D* and we have *m* users (rows) rating *n* items (columns).
    The SVD factorization of this matrix is given by:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看这个过程是如何工作的。奇异值分解是一个矩阵分解过程，所以我们从一个代表我们数据的原始矩阵开始，并将其表示为矩阵的乘积。在降维场景中，我们的输入数据矩阵将是行代表数据点、列代表特征的矩阵；因此，在
    R 中，这只是一个数据框。在我们的推荐系统场景中，我们使用的矩阵是我们的评分矩阵。假设我们称我们的评分矩阵为 *D*，并且我们有 *m* 个用户（行）对 *n*
    个物品（列）进行评分。这个矩阵的奇异值分解由以下公式给出：
- en: '![Singular value decomposition](img/00205.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![奇异值分解](img/00205.jpeg)'
- en: In the previous equation, *U* and *V* are square matrices and the matrix *Σ*
    is a matrix with the same dimensionality as our input matrix, *D*. In addition,
    it is a diagonal matrix, meaning that all the elements of the matrix are zero
    except those on the leading diagonal. These elements are conventionally ordered
    from largest to smallest and are known as the **singular values** of the matrix
    *D*, giving rise to the name singular value decomposition.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个方程中，*U* 和 *V* 是方阵，而矩阵 *Σ* 是与我们的输入矩阵 *D* 具有相同维度的矩阵。此外，它是一个对角矩阵，这意味着矩阵的所有元素都是零，除了主对角线上的元素。这些元素通常按从大到小的顺序排列，被称为矩阵
    *D* 的**奇异值**，从而产生了奇异值分解的名称。
- en: Note
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Readers familiar with linear algebra will know that the eigenvalues of a matrix
    are often also described as containing information about the important dimensions
    of that matrix. It turns out that the eigenvalues of a matrix are related to the
    singular values through the following relationship—the singular values of a matrix,
    *D,* are the same as the square roots of the eigenvalues of the matrix product
    *D* × *D^T*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉线性代数的读者会知道，矩阵的特征值通常也描述为包含有关该矩阵重要维度的信息。实际上，矩阵的特征值与奇异值通过以下关系相关联——矩阵 *D* 的奇异值等于矩阵乘积
    *D* × *D^T* 的特征值的平方根。
- en: 'We can easily perform SVD on a matrix in R via the `svd()` function, which
    is available with R''s `base` package. Let''s see this using our existing `ratingMatrix`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 R 的 `svd()` 函数轻松地对矩阵进行奇异值分解，该函数是 R 的 `base` 包的一部分。让我们用我们现有的 `ratingMatrix`
    来看看这个例子：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The singular values are returned as a vector `d`, from which we can easily
    construct the diagonal matrix using the `diag()` function. To verify that this
    factorization really is the one that we expected, we can reconstruct our original
    rating matrix by simply multiplying the matrix factors that we have obtained:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异值以向量 `d` 返回，我们可以使用 `diag()` 函数轻松地构造对角矩阵。为了验证这个因子分解确实是我们预期的，我们可以通过简单地乘以我们获得的矩阵因子来重建我们的原始评分矩阵：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'One thing to note here is that, if we were to attempt a direct equality check
    with our original matrix, we would most likely fail. This is due to rounding errors
    that are introduced when we store the factorized matrices. We can check that our
    two matrices are nearly equal using the `all.equal()` function:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一点是，如果我们尝试直接用原始矩阵进行等式检查，我们很可能会失败。这是由于我们在存储因子矩阵时引入的舍入误差造成的。我们可以使用 `all.equal()`
    函数来检查我们的两个矩阵几乎相等：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The reader is encouraged to decrease the size of the tolerance and note that,
    after several decimal points, the equality check fails. Though the two matrices
    are not exactly equal, the difference is so small that this will not impact us
    in any significant way. Now, once we have this factorization, let's investigate
    our singular values. The first singular value of 35.6 is much larger than the
    smallest singular value of 1.3.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励读者减小容差的大小，并注意，在几个小数点之后，等式检查会失败。尽管两个矩阵并不完全相等，但差异非常小，这不会对我们产生任何重大影响。现在，一旦我们有了这个因子分解，让我们来研究我们的奇异值。35.6
    的第一个奇异值远大于 1.3 的最小奇异值。
- en: 'We can perform dimensionality reduction by keeping the top singular values
    and throwing the rest away. To do this, we''d like to know how many singular values
    we should keep and how many we should discard. One approach to this problem is
    to compute the square of the singular values, which can be thought of as the vector
    of **matrix energy**, and then pick the top singular values that preserve at least
    90 % of the overall energy of the original matrix. This is easy to do with R as
    we can use the `cumsum()` function for creating a cumulative sum and the singular
    values are already ordered from largest to smallest:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过保留最大的奇异值并丢弃其余的值来执行降维。为了做到这一点，我们需要知道应该保留多少个奇异值以及应该丢弃多少个。解决这个问题的一个方法是通过计算奇异值的平方，这可以被视为**矩阵能量**的向量，然后选择至少保留原始矩阵90%总能量的前几个奇异值。在R中这样做很容易，因为我们可以使用`cumsum()`函数来创建累积和，而奇异值已经按照从大到小的顺序排列：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Keeping the first two singular values will retain 92 % of the energy of our
    original matrix. Using just two values, we can reconstruct our rating matrix and
    observe the differences:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 保留前两个奇异值将保留我们原始矩阵92%的能量。仅使用两个值，我们可以重建我们的评分矩阵并观察差异：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we can see, there are a few differences in the absolute values, but most
    of the patterns of different users have been retained to a large extent. Discarding
    singular values effectively introduces zeros in the leading diagonal of matrix
    *D* in the factorization, so that this matrix ends up with entire rows and columns
    that only contain zeros. Consequently, we can truncate not only this matrix, but
    rows from the matrices *U* and *V*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，绝对值上有一些差异，但大多数不同用户的模式都得到了很大程度的保留。丢弃奇异值有效地在矩阵*D*的领先对角线上引入了零，因此这个矩阵最终只有包含零的整个行和列。因此，我们不仅可以截断这个矩阵，还可以截断矩阵*U*和*V*的行。
- en: Thus, we reduce the size of the data that we have to store.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们减少了需要存储的数据的大小。
- en: Predicting recommendations for movies and jokes
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测电影和笑话的推荐
- en: In this chapter, we will focus on building recommender systems using two different
    datasets. To do this, we shall use the `recommenderlab` package. This provides
    us with not only the algorithms to perform the recommendations, but also with
    the data structures to store the sparse rating matrices efficiently. The first
    datasets we will use contains anonymous user reviews for jokes from the *Jester
    Online Joke recommender system*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于使用两个不同的数据集来构建推荐系统。为此，我们将使用`recommenderlab`包。这个包不仅提供了执行推荐的算法，还提供了存储稀疏评分矩阵的高效数据结构。我们将使用的第一个数据集包含了来自*Jester
    Online Joke推荐系统*的匿名用户对笑话的评论。
- en: The joke ratings fall on a continuous scale (-10 to +10). A number of datasets
    collected from the Jester system can be found at [http://eigentaste.berkeley.edu/dataset/](http://eigentaste.berkeley.edu/dataset/).
    We will use the datasets labeled on the website as *Dataset 2+*. This datasets
    contains ratings made by 50,692 users on 150 jokes. As is typical with a real-world
    application, the rating matrix is very sparse in that each user rated only a fraction
    of all the jokes; the minimum number of ratings made by a user is 8\. We will
    refer to this data set as the jester datasets.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 笑话评分落在连续的量表上（-10到+10）。可以从[http://eigentaste.berkeley.edu/dataset/](http://eigentaste.berkeley.edu/dataset/)找到来自Jester系统的多个数据集。我们将使用网站上标记为*Dataset
    2+*的数据集。这个数据集包含了50,692个用户对150个笑话的评分。与现实世界的应用典型情况一样，评分矩阵非常稀疏，因为每个用户只对部分笑话进行了评分；用户做出的最低评分数量是8。我们将把这个数据集称为jester数据集。
- en: The second datasets can be found at [http://grouplens.org/datasets/movielens/](http://grouplens.org/datasets/movielens/).
    This website contains data on user ratings for movies that were made on the *MovieLens*
    website at [http://movielens.org](http://movielens.org). Again, there is more
    than one datasets on the website; we will use the one labeled *MovieLens1M*. This
    contains ratings on a five-point scale (1-5) made by 6,040 users on 3,706 movies.
    The minimum number of movie ratings per user is 20\. We will refer to this datasets
    as the movie dataset.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个数据集可以在[http://grouplens.org/datasets/movielens/](http://grouplens.org/datasets/movielens/)找到。这个网站包含了在[http://movielens.org](http://movielens.org)的*MovieLens*网站上制作的用户对电影的评分数据。同样，网站上不止一个数据集；我们将使用标记为*MovieLens1M*的数据集。这个数据集包含了6,040个用户对3,706部电影在五点量表（1-5）上的评分。每个用户对电影的最低评分数量是20。我们将把这个数据集称为电影数据集。
- en: Tip
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: These two datasets are actually very well-known open source datasets, to the
    point that the `recommenderlab` package itself includes smaller versions of them
    as part of the package. Readers who would like to skip the process of loading
    and preprocessing the data, or who would like to run the examples that follow
    on smaller datasets due to computational constraints, are encouraged to try them
    out using `data(Jester5k)` or `data(MovieLense)`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个数据集实际上是众所周知的开源数据集，以至于 `recommenderlab` 包本身将它们的小版本包含在包中。对于那些想跳过加载数据和预处理过程，或者由于计算限制而想在小数据集上运行后续示例的读者，我们鼓励他们尝试使用
    `data(Jester5k)` 或 `data(MovieLense)`。
- en: Loading and pre-processing the data
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据和预处理
- en: Our first goal in building our recommender systems is to load the data in R,
    preprocess it, and convert it into a rating matrix. More precisely, in each case,
    we will be creating a `realRatingMatrix` object, which is the specific data structure
    that the `recommenderlab` package uses to store numerical ratings. We will start
    with the jester datasets. If we download and unzip the archive from the website,
    we'll see that the file `jesterfinal151cols.csv` contains the ratings. More specifically,
    each row in this file corresponds to the ratings made by a particular user, and
    each column corresponds to a particular joke.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建我们的推荐系统时，我们的第一个目标是使用R加载数据，对其进行预处理，并将其转换为评分矩阵。更确切地说，在每种情况下，我们将创建一个 `realRatingMatrix`
    对象，这是 `recommenderlab` 包用于存储数值评分的特定数据结构。我们将从jester数据集开始。如果我们从网站上下载并解压存档，我们会看到文件
    `jesterfinal151cols.csv` 包含评分。更具体地说，该文件中的每一行对应于特定用户做出的评分，每一列对应于一个特定的笑话。
- en: 'The columns are comma-separated and there is no header row. In fact, the format
    is almost already a rating matrix, were it not for the fact that the first column
    is a special column and contains the total number of ratings made by a particular
    user. We will load this data into a data table using the function `fread()`, which
    is a fast implementation of `read.table()` and efficiently loads a data file into
    a data table. We''ll then drop the first column efficiently using the `data.table`
    syntax:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 列之间用逗号分隔，没有标题行。实际上，格式几乎已经是一个评分矩阵，如果不是因为第一列是一个特殊的列，它包含特定用户做出的总评分数。我们将使用函数 `fread()`
    将此数据加载到数据表中，这是一个 `read.table()` 的快速实现，并有效地将数据文件加载到数据表中。然后，我们将使用 `data.table` 语法高效地删除第一列：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The last line used the assignment operator `:=` to set the first column, `V1`,
    to `NULL`, which is how we drop a column on a data table. We now have one final
    preprocessing step left to do on our data table, `jester`, before we are ready
    to convert it to a `realRatingMatrix` object. Specifically, we will convert this
    into a matrix and replace all occurrences of the rating of 99 with `NA`, as 99
    was the special rating used to represent missing values:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行使用了赋值运算符 `:=` 将第一列 `V1` 设置为 `NULL`，这就是我们在数据表上删除列的方法。在我们准备好将数据表 `jester`
    转换为 `realRatingMatrix` 对象之前，我们还需要在数据表上执行一个最后的预处理步骤。具体来说，我们将将其转换为矩阵，并将所有评分为99的实例替换为
    `NA`，因为99是用于表示缺失值的特殊评分：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Depending on the computational resources of the computer available to us (most
    notably, the available memory), we may want to try to process a single datasets
    in its entirety instead of loading both datasets at once. Here, we have chosen
    to work with the two datasets in parallel in order to showcase the main steps
    in the analysis and highlight any differences or particularities of an individual
    datasets with respect to a particular step.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们可用的计算机的计算资源（尤其是可用内存），我们可能想要尝试一次性处理整个数据集，而不是同时加载两个数据集。在这里，我们选择并行处理这两个数据集，以便展示分析的主要步骤，并突出显示单个数据集相对于特定步骤的差异或特殊性。
- en: Let's move on to the MovieLens data. Downloading the MovieLens1M archive and
    unzipping reveals three main data files. The `users.dat` file contains background
    information about the users, such as age and gender. The `movies.dat` data file,
    in turn, contains information about the movies being rated, namely the title and
    a list of genres (for example, *comedy*) to which the movie belongs.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续到MovieLens数据。下载MovieLens1M存档并解压后，会显示三个主要数据文件。`users.dat` 文件包含有关用户的信息，例如年龄和性别。`movies.dat`
    数据文件反过来包含有关被评分的电影的信息，即电影的标题和属于（例如，*喜剧*）的电影类型列表。
- en: 'We are mainly interested in the `ratings.dat` file, which contains the ratings
    themselves. Unlike the raw jester data, here each line corresponds to a single
    rating made by a user. The line format contains the User ID, Movie ID, rating,
    and timestamp, all separated by two colon characters, `::`. Unfortunately, `fread()`
    requires a separator with a single character, so we will specify a single colon.
    The double-colon separator in the raw data results in us creating extra columns
    with `NA` values that we will have to remove, as well as the final column that
    contains the timestamp:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要关注`ratings.dat`文件，其中包含评分本身。与原始的Jester数据不同，这里每一行对应一个用户做出的单个评分。行格式包含用户ID、电影ID、评分和时间戳，所有这些信息都由两个冒号字符`::`分隔。不幸的是，`fread()`需要一个单字符分隔符，因此我们将指定一个单冒号。原始数据中的双冒号分隔符会导致我们创建包含`NA`值的额外列，以及包含时间戳的最后一列，这些列我们都需要删除：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As we can see, we are now left with three columns, where the first is the `UserID`,
    the second is the `MovieID`, and the last is the rating. We will now aggregate
    all the ratings made by a user in order to form an object that can be interpreted
    as or converted to, a rating matrix. We should aggregate the data in a way that
    minimizes memory usage. We will do this by building a sparse matrix using the
    `sparseMatrix()` command from the `Matrix` package.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们现在剩下三列，其中第一列是`UserID`，第二列是`MovieID`，最后一列是评分。我们现在将聚合用户做出的所有评分，以形成一个可以解释为或转换为评分矩阵的对象。我们应该以最小化内存使用的方式聚合数据。我们将通过使用`Matrix`包中的`sparseMatrix()`命令构建稀疏矩阵来实现这一点。
- en: This package is loaded automatically when we use the `recommenderlab` package,
    as, it is one of its dependencies. To build a sparse matrix using this function,
    we can simply specify a vector of row coordinates, a vector of matching column
    coordinates, and a vector with the nonzero values that fill up the sparse matrix.
    Remember, as our matrix is sparse, all we need are the locations and values for
    entries that are nonzero.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用`recommenderlab`包时，该包会自动加载，因为它是其依赖之一。要使用此函数构建稀疏矩阵，我们可以简单地指定一个行坐标向量、一个匹配的列坐标向量和填充稀疏矩阵的非零值向量。记住，由于我们的矩阵是稀疏的，我们只需要非零条目的位置和值。
- en: 'Right now, it is slightly inconvenient that we cannot directly interpret the
    User IDs and Movie IDs as coordinates. This is because, if we have a user with
    a User ID value of 1 and a user with a User ID value of 3, R will automatically
    create a user with a User ID value of 2 and create an empty row, even though that
    user does not actually exist in the training data. The situation is similar for
    columns. Consequently, we must first make factors out of our `UserID` and `MovieID`
    columns before proceeding to create our rating matrix as described earlier. Here
    is the code for building our rating matrix for the MovieLens data:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们无法直接将用户ID和电影ID解释为坐标，这有些不方便。这是因为，如果我们有一个用户ID为1的用户和一个用户ID为3的用户，R会自动创建一个用户ID为2的用户并创建一个空行，尽管这个用户实际上并不存在于训练数据中。列的情况也类似。因此，在创建我们的评分矩阵之前，我们必须首先将`UserID`和`MovieID`列转换为因子。以下是构建MovieLens数据评分矩阵的代码：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It is a good exercise to check that the dimensions of the result correspond
    to our expectations on the number of users and movies respectively.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 检查结果的维度是否与我们对用户和电影数量的预期相符是一个很好的练习。
- en: Exploring the data
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: 'Before building and evaluating recommender systems using the two datasets we
    have loaded, it is a good idea to get a feel for the data. For one thing, we can
    make use of the `getRatings()` function to retrieve the ratings from a rating
    matrix. This is useful in order to construct a histogram of item ratings. Additionally,
    we can also normalize the ratings with respect to each user, as we discussed earlier.
    The following code snippet shows how we can compute ratings and normalized ratings
    for the jester data. We can then do the same for the MovieLens data and produce
    histograms for the ratings:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用我们已加载的两个数据集构建和评估推荐系统之前，了解数据是很重要的。一方面，我们可以使用`getRatings()`函数从评分矩阵中检索评分，这有助于构建项目评分的直方图。此外，我们还可以根据我们之前讨论的，对每个用户的评分进行归一化。以下代码片段显示了如何计算Jester数据的评分和归一化评分。然后我们可以对MovieLens数据进行同样的操作，并生成评分的直方图：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following plot shows the different histograms:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了不同的直方图：
- en: '![Exploring the data](img/00206.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![探索数据](img/00206.jpeg)'
- en: In the Jester data, we can see that ratings above zero are more prominent than
    ratings below zero, and the most common rating is 10, the maximum rating. The
    normalized ratings create a more symmetric distribution centered on zero. For
    the MovieLens data with the 5-point rating scale, 4 is the most prominent rating
    and higher ratings are far more common than low ratings.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jester数据中，我们可以看到零以上的评分比零以下的评分更为突出，最常见的评分是10，即最大评分。归一化评分创建了一个以零为中心的更对称的分布。对于具有5点评分尺度的MovieLens数据，4是最突出的评分，而高于4的评分比低于4的评分更为常见。
- en: 'We can also look for the distribution of the number of items rated per user
    and the average rating per item by looking at the row counts and the column means
    of the rating matrix respectively. Again, the following code snippet shows how
    to compute these for the jester data, and we follow up with histograms showing
    the results for both datasets:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过查看评分矩阵的行数和列均值来查找每个用户评分的项目数量和每个项目的平均评分。同样，以下代码片段展示了如何计算jester数据中的这些值，并且我们随后用直方图展示了两个数据集的结果：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The histograms of the Jester and MovieLens data are shown here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了Jester和MovieLens数据的直方图：
- en: '![Exploring the data](img/00207.jpeg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![探索数据](img/00207.jpeg)'
- en: Both datasets show a curve in the average ratings per user that looks like a
    power curve. Most of the users have rated very few items, but a small number of
    very committed users have actually rated a large number of items. In the Jester
    case, some have rated the maximum number of jokes in the datasets. This is an
    exception and only occurs because the number of items (jokes) in this datasets
    is relatively small. The distribution of the average joke rating is between -3
    and 4, but for movies we see the whole range of the spectrum, indicating that
    some users have rated all the movies they considered completely awful or totally
    great. We can find the average of these distributions in order to determine the
    average number of items rated per user and the average rating of each item.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 两个数据集在用户平均评分中显示出的曲线看起来像幂曲线。大多数用户评分的项目很少，但少数非常投入的用户实际上评分了大量的项目。在Jester案例中，有些人评了数据集中最多的笑话。这是一个例外，并且只发生在这个数据集中项目（笑话）的数量相对较小的情况下。平均笑话评分的分布介于-3和4之间，但对于电影，我们看到整个光谱的范围，这表明一些用户已经对他们认为完全糟糕或完全出色的电影进行了评分。我们可以找到这些分布的平均值，以确定每个用户平均评分的项目数量和每个项目的平均评分。
- en: 'Note that we need to remove `NA` values from consideration in the Jester datasets,
    as some columns may not have ratings in them:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们需要从Jester数据集中移除`NA`值，因为某些列可能没有评分：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Evaluating binary top-N recommendations
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估二元前N个推荐
- en: 'We now have some sense of what our data looks like for both datasets so we
    can start building some models. We will begin by looking at the problem of making
    top-N recommendations for a binary recommender system, which is simpler to do
    than when we have more granular data for ratings. Recall that a top-N recommendation
    is nothing but a list of *N* recommendations that are more likely to interest
    a user. To do this, we will use the jester datasets and create a binary version
    of our rating matrix. We''ll call any rating that is 5 or above a positive rating.
    As this may result in some users having no positive ratings, we''ll also prune
    the rating matrix and keep only users with at least 10 positive ratings under
    this scheme:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对两个数据集的数据有了大致的了解，可以开始构建一些模型。我们将从研究为二元推荐系统制作前N个推荐的问题开始，这比我们有更细粒度的评分数据时更容易实现。回想一下，前N个推荐只不过是一个列表，其中包含*N*个更有可能引起用户兴趣的推荐。为此，我们将使用jester数据集并创建我们评分矩阵的二元版本。我们将任何评分在5或以上的评分视为正面评分。由于这可能导致一些用户没有正面评分，我们将修剪评分矩阵，并只保留至少有10个正面评分的用户：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'One of the advantages of the `recommenderlab` package is that it makes it very
    easy for us to compare results from several algorithms. The process of training
    and evaluating multiple algorithms for top-N recommendations begins by creating
    a list containing the definitions of the algorithms that we want to use. Each
    element in the list is given a name of our choice but must itself be a list containing
    a set of parameters for configuring a known algorithm. Concretely, the `name`
    parameter of this inner parameter list must be one that the `recommenderlab` package
    recognizes. It is possible to create and register one''s own algorithm with this
    package, but our focus will be on existing implementations that more than suffice
    for our intents and purposes:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`recommenderlab` 包的一个优点是它使我们能够很容易地比较几个算法的结果。为 top-N 推荐训练和评估多个算法的过程是从创建一个包含我们想要使用的算法定义的列表开始的。列表中的每个元素都给出了我们选择的名称，但本身必须是一个包含配置已知算法参数的集合的列表。具体来说，这个内部参数列表的
    `name` 参数必须是 `recommenderlab` 包所认可的。使用此包可以创建并注册自己的算法，但我们的重点将放在现有的实现上，这些实现对于我们的意图和目的已经足够：'
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The RANDOM algorithm is a baseline algorithm that makes recommendations randomly.
    The POPULAR algorithm is another baseline algorithm that can sometimes be tough
    to beat. This proposes items in descending order of global popularity, so that
    for a top-1 recommendation, it will recommend the item with the highest average
    rating in the datasets. We have chosen to try out two variants of user-based collaborative
    filtering for this example. The first one uses the cosine distance and specifies
    50 as the number of nearest neighbors to use. The second one is identical but
    uses the Jaccard distance instead.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 随机算法是一个基线算法，它随机做出推荐。流行算法是另一个基线算法，有时很难击败。它按全球流行度降序提出项目，因此对于 top-1 推荐，它将推荐数据集中平均评分最高的项目。我们选择尝试两种基于用户的协同过滤变体。第一个使用余弦距离，并指定
    50 个最近邻的数量。第二个与第一个相同，但使用 Jaccard 距离。
- en: 'Next, we define an evaluation scheme via the function `evaluationScheme()`.
    This function records how we will split our data into training and test sets,
    the number of ratings we will take as given from our test users via the `given`
    parameter, and how many runs we want to execute. We will do a straight 80-20 split
    for our training and test set, consider 10 ratings from our test users as known
    ratings, and evaluate over a single run:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过函数 `evaluationScheme()` 定义一个评估方案。此函数记录我们将如何将数据分为训练集和测试集，我们将通过 `given`
    参数从测试用户那里接受的评分数量，以及我们想要执行多少次运行。我们将对训练集和测试集进行直接的 80-20 分割，将测试用户的 10 个评分视为已知评分，并在单次运行中进行评估：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note that the `given` parameter must be at least as large as the smallest number
    of items rated by a user in our datasets. We previously filtered the datasets
    to ensure we have 10 items per user, so we are covered in our case. Finally, we
    will evaluate our list of algorithms in turn with our evaluation scheme using
    the `evaluate()` function. Aside from an evaluation scheme and a list of algorithms,
    we will also specify the range of *N* values to use when making top-N recommendations
    via the `n` parameter. We will do this for values 1 through 20:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`given` 参数必须至少与我们的数据集中用户评分的最小数量相同。我们之前已过滤数据集以确保每个用户有 10 个项目，所以在这个案例中我们已经覆盖了。最后，我们将使用
    `evaluate()` 函数依次用我们的评估方案评估我们的算法列表。除了评估方案和算法列表之外，我们还将指定在通过 `n` 参数进行 top-N 推荐时使用的
    *N* 值的范围。我们将对 1 到 20 的值进行此操作：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We now have a list of four objects that represent the evaluation results of
    each algorithm on our data. We can get important measures such as precision by
    looking at the confusion matrices. Note that, as we have run this experiment for
    the top-N recommendations, where *N* is in the range 1-20, we expect to have 20
    such confusion matrices for each algorithm.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个列表，其中包含代表每个算法在我们数据上的评估结果的四个对象。我们可以通过查看混淆矩阵来获取重要度量，如精确度。请注意，由于我们已经为 top-N
    推荐运行了此实验，其中 *N* 在 1-20 的范围内，我们预计每个算法将有 20 个这样的混淆矩阵。
- en: 'The function `getConfusionMatrix()`, when applied to one of these objects,
    can be used to retrieve the folded confusion matrices in tabular format so that
    each row represents the confusion matrix for a particular value of *N*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当将函数 `getConfusionMatrix()` 应用到这些对象之一时，可以用来检索折叠的混淆矩阵，以便每行代表 *N* 的特定值的混淆矩阵：
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: To visualize this data and compare our algorithms, we can try plotting the results
    directly using the `plot()` function. For our evaluation results, the default
    is a plot of the **true positive rate** (**TPR**) versus the **false positive
    rate**(**FPR**). This is simply the ROC curve, as we know from [Chapter 4](part0035_split_000.html#11C3M1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 4. Generalized Linear Models"), *Neural Networks*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这些数据并比较我们的算法，我们可以尝试直接使用`plot()`函数绘制结果。对于我们的评估结果，默认是绘制**真正例率**（**TPR**）与**假正例率**（**FPR**）的曲线。正如我们从[第4章](part0035_split_000.html#11C3M1-c6198d576bbb4f42b630392bd61137d7
    "第4章. 广义线性模型")，*神经网络*中了解的那样，这只是一个ROC曲线。
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here is the ROC curve for the binary Jester data:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这是二值Jester数据的ROC曲线：
- en: '![Evaluating binary top-N recommendations](img/00208.jpeg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![评估二值top-N推荐](img/00208.jpeg)'
- en: 'The graph shows that the user-based collaborative filtering algorithms perform
    better than the two baseline algorithms, but there is very little to separate
    these two, with the cosine distance marginally outperforming the Jaccard distance.
    We can complement this view of our results by also plotting a precision recall
    curve:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示，基于用户的协同过滤算法比两个基线算法表现更好，但这两者之间几乎没有区别，余弦距离略优于Jaccard距离。我们可以通过绘制精确度-召回率曲线来补充我们对结果的观点：
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following is the precision recall curve for the binary Jester data:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是二值Jester数据的精确度-召回率曲线：
- en: '![Evaluating binary top-N recommendations](img/00209.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![评估二值top-N推荐](img/00209.jpeg)'
- en: The precision recall curve paints a similar picture, with the user-based collaborative
    filtering algorithm that uses the cosine distance coming out as the winner. Note
    that the trade-off between precision and recall surfaces in a top-N recommender
    system via the number of recommendations that the system makes. The way our evaluation
    scheme works is that we treat users in the test data as new users in the system
    that just contributed a certain number of ratings. We hold out as many ratings
    as the `given` parameter allows. Then, we apply our model to see if the ratings
    we suggest will agree with the ratings that remain. We order our suggestions in
    descending order of confidence so that in a top-1 recommendation system, we will
    suggest the item we believe has the best chance of interesting the user. Increasing
    *N,* therefore, is like casting a wider net. We will be less precise in our suggestions
    but are more likely to find something the user will like.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度-召回率曲线描绘了类似的画面，使用余弦距离的基于用户的协同过滤算法成为赢家。请注意，在top-N推荐系统中，精确度和召回率之间的权衡通过系统做出的推荐数量体现出来。我们的评估方案的工作方式是，我们将测试数据中的用户视为系统中刚刚贡献了一定数量评分的新用户。我们保留与`given`参数允许的评分数量一样多的评分。然后，我们应用我们的模型来查看我们建议的评分是否会与剩余的评分一致。我们按信心度降序排列建议，以便在top-1推荐系统中，我们将建议我们认为最有可能会引起用户兴趣的项目。增加*N*，因此，就像撒更宽的网一样。我们的建议将不那么精确，但更有可能找到用户会喜欢的东西。
- en: Note
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: An excellent and freely available resource for recommendation systems is *Chapter
    9* from the online textbook *Mining of Massive Datasets* by *Jure Leskovec*, *Anand
    Rajaraman*, and *Jeffrey David Ullman*. The website is [http://www.mmds.org/](http://www.mmds.org/).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统的一个优秀且免费的资源是来自在线教材《大规模数据集挖掘》（*Mining of Massive Datasets*）的第9章，该教材由*Jure
    Leskovec*、*Anand Rajaraman*和*Jeffrey David Ullman*编写。网站地址为[http://www.mmds.org/](http://www.mmds.org/).
- en: Evaluating non-binary top-N recommendations
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估非二值top-N推荐
- en: 'In this section, we will use the movies dataset to see how we perform in the
    non-binary scenario. First, we will define our algorithms as before:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用电影数据集来观察我们在非二值场景下的表现。首先，我们将定义我们的算法，就像之前一样：
- en: '[PRE25]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This time, our algorithms will work with normalized ratings by specifying the
    `normalize` parameter. We will only be using the cosine distance for user-based
    collaborative filtering as the Jaccard distance only applies in the binary setting.
    Furthermore, we will also try out item-based collaborative filtering as well as
    SVD-based recommendations. Instead of directly splitting our data, we demonstrate
    how we can perform ten-fold cross-validation by modifying our evaluation scheme.
    We will continue to investigate making top-N recommendations in the range of 1
    to 20\. Evaluating a moderately-sized dataset with five algorithms using 10-fold
    cross-validation means that we can expect this process to take quite a long time
    to finish, depending on the computing power we have at our disposal:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们的算法将通过指定`normalize`参数来使用归一化评分。我们只将使用余弦距离进行基于用户的协同过滤，因为Jaccard距离仅适用于二进制设置。此外，我们还将尝试基于物品的协同过滤以及基于SVD的推荐。我们不会直接分割我们的数据，而是通过修改我们的评估方案来展示如何进行十折交叉验证。我们将继续研究在1到20范围内的顶级N推荐。使用10折交叉验证评估一个中等规模的数据集和五个算法意味着我们可以预期这个过程将花费相当长的时间来完成，这取决于我们可用的计算能力：
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To conserve space, we have truncated the output that shows us the amount of
    time spent running each iteration for the different algorithms. Note that the
    most expensive algorithm during training is the item-based collaborative filtering
    algorithm, as this is building a model and not just performing lazy learning.
    Once the process terminates, we can plot the results in the same way as we did
    for our binarized Jester dataset to compare the performance of our algorithms:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省空间，我们已截断显示不同算法每个迭代运行时间的输出。请注意，在训练过程中最昂贵的算法是基于物品的协同过滤算法，因为这是在构建模型，而不仅仅是进行懒惰学习。一旦过程终止，我们可以像为我们的二值化Jester数据集所做的那样绘制结果，以比较我们算法的性能：
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is the ROC curve for the MovieLens data:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这是MovieLens数据的ROC曲线：
- en: '![Evaluating non-binary top-N recommendations](img/00210.jpeg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![评估非二进制顶级N推荐](img/00210.jpeg)'
- en: As we can see, user-based collaborative filtering is the clear winner here.
    SVD performs in a similar manner to the POPULAR baseline, though the latter starts
    to become better when *N* is high. Finally, we see item-based collaborative filtering
    performing far worse than these, outperforming only the random baseline. What
    is clear from these experiments is that tuning recommendation systems can often
    be a very time-consuming, resource-intensive endeavor.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，基于用户的协同过滤在这里是明显的赢家。SVD的表现与POPULAR基线相似，尽管后者在*N*较高时开始变得更好。最后，我们看到基于物品的协同过滤的表现远逊于这些，仅优于随机基线。从这些实验中可以清楚地看出，调整推荐系统可能是一个非常耗时、资源密集的过程。
- en: All the algorithms that we specified can be tuned in various ways and we have
    explored a number of parameters, from the size of the neighborhood to the similarity
    metric, that will influence the results. In addition, we've seen that, even for
    the top-N scenario alone, there are several ways that we can evaluate our recommendation
    system; thus, if we want to try out a number of these for comparison, we will
    again need to spend more time on model training.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定的所有算法都可以以各种方式进行调优，我们已经探索了许多参数，从邻域大小到相似度度量，这些都会影响结果。此外，我们注意到，即使是对于顶级N场景，也有几种方法可以评估我们的推荐系统；因此，如果我们想尝试其中的一些进行比较，我们还需要在模型训练上花费更多的时间。
- en: The reader is encouraged to repeat these experiments using different parameters
    and evaluation schemes in order to get a feel for the process of designing and
    training recommendation systems. In addition, by visiting the websites of our
    two datasets, the reader can find additional links to similar datasets commonly
    used for learning about recommendation systems, such as the book-crossing datasets.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励读者使用不同的参数和评估方案重复这些实验，以便了解设计和训练推荐系统的过程。此外，通过访问我们的两个数据集的网站，读者可以找到到类似数据集的链接，这些数据集通常用于学习推荐系统，例如book-crossing数据集。
- en: 'For completeness, we will plot the precision recall curve for the MovieLens
    data:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们将绘制MovieLens数据的精确度召回率曲线：
- en: '[PRE28]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is the precision recall curve for the MovieLens data:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这是MovieLens数据的精确度召回率曲线：
- en: '![Evaluating non-binary top-N recommendations](img/00211.jpeg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![评估非二进制顶级N推荐](img/00211.jpeg)'
- en: Evaluating individual predictions
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估单个预测
- en: Another way to evaluate a recommendation system is to ask it to predict the
    specific values of a portion of the known ratings made by a set of test users
    using the remainder of their ratings. In this way, we can measure accuracy by
    taking average distance measures over the predicted ratings. These include the
    **mean squared error** (**MSE**) and the **Root Mean Square Error** (**RMSE**),
    which we have seen before, and the **mean average error**(**MAE**), which is just
    the average of the absolute errors. We will do this for the regular (unbinarized)
    Jester datasets.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 评估推荐系统的另一种方法是要它预测一组测试用户使用他们剩余的评分所做出的部分已知评分的具体值。通过这种方式，我们可以通过在预测评分上取平均距离度量来衡量准确性。这些包括我们之前见过的**均方误差**（**MSE**）和**均方根误差**（**RMSE**），以及**平均绝对误差**（**MAE**），它只是绝对误差的平均值。我们将为此常规（非二值化）的Jester数据集执行此操作。
- en: 'We begin as before by defining an evaluation scheme:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像以前一样，首先定义一个评估方案：
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we will define individual user- and item-based collaborative filtering
    recommenders using the `Recommender()` and `getData()` functions. The logic behind
    these is that the `getData()` function will extract the ratings set aside for
    training by the evaluation scheme and the `Recommender()` function will use the
    data to train a model:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`Recommender()`和`getData()`函数定义基于用户和项目的协同过滤推荐器。这些背后的逻辑是，`getData()`函数将提取评估方案保留用于训练的评分集，而`Recommender()`函数将使用这些数据来训练一个模型：
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can then use these models to predict those ratings that were classified
    as known (there are as many of these as the `given` parameter specifies) in our
    test data:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些模型来预测测试数据中那些被分类为已知（其数量与`给定`参数指定的数量相同）的评分：
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, we can use the known ratings to compute prediction accuracy on the
    ratings kept for testing:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用已知的评分来计算保留用于测试的评分的预测准确性：
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can see that the performance of the two algorithms is fairly close. User-based
    collaborative filtering performs better when we penalize larger errors (via the
    RMSE and MSE) through squaring. From the perspective of the mean average error,
    item-based collaborative filtering is marginally better.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这两个算法的性能相当接近。当我们通过平方来惩罚较大的误差（通过RMSE和MSE）时，基于用户的协同过滤表现更好。从平均绝对误差的角度来看，基于项目的协同过滤略微更好。
- en: Consequently, in this case, we might make our decision as to which type of recommendation
    system to use on the basis of the error behavior that more closely matches our
    business needs. In this section, we used the default parameter values for the
    two algorithms, but by using the `parameter` parameter in the `Recommender()`
    function, we can play around with different configurations as we did before. This
    is left as an exercise for the reader.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，我们可能会基于与我们业务需求更接近的错误行为来决定使用哪种类型的推荐系统。在本节中，我们使用了两个算法的默认参数值，但通过在`Recommender()`函数中使用`parameter`参数，我们可以像以前一样尝试不同的配置。这留作读者的练习。
- en: Other approaches to recommendation systems
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他推荐系统方法
- en: In this chapter, we concentrated our efforts on building recommendation systems
    by following the collaborative filtering paradigm. This is a very popular approach
    due to its many advantages. By essentially mimicking word-of-mouth recommendations,
    it requires virtually no knowledge about the items being recommended nor any background
    about the users in question.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们集中精力通过遵循协同过滤范例来构建推荐系统。这是一个非常受欢迎的方法，因为它具有许多优点。通过本质上模仿口碑推荐，它几乎不需要了解被推荐的项目或有关用户的任何背景知识。
- en: Moreover, collaborative filtering systems incorporate new ratings as they arise,
    either through a memory approach, or via the regular retraining of a model-based
    approach. Thus, they naturally become better for their users over time as they
    learn more information and adapt to changing preferences. On the other hand, they
    are not without their disadvantages, not the least of which is the fact that they
    will not take into account any information about the items and their content even
    when it is available.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，协同过滤系统会随着新评分的出现而纳入新评分，无论是通过记忆方法，还是通过基于模型的常规重新训练。因此，随着时间的推移，它们会自然地变得对用户更好，因为它们学习到更多信息并适应不断变化的偏好。另一方面，它们并非没有缺点，其中最不重要的是，即使有可用信息，它们也不会考虑任何关于项目和它们内容的信息。
- en: '**Content-based recommendation systems** try to suggest items to users that
    are similar to those that users like on the basis of content. The key premise
    behind this idea is that, if it is known that a user happens to like novels by
    *George R. R. Martin*, a fantasy and science fiction author, it makes sense that
    a recommendation service for books might suggest a similar author, such as *Robert
    Jordan*, for example.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于内容的推荐系统**试图根据内容向用户推荐与用户喜欢的项目相似的项目。这一想法背后的关键前提是，如果知道用户恰好喜欢乔治·R·R·马丁（*George
    R. R. Martin*）的小说，那么书籍推荐服务可能建议一个类似作者，例如*罗伯特·乔丹*，是有意义的。'
- en: Collaborative filtering systems, by their nature, require some sort of feedback
    system in order for the recommender to record a particular rating. In particular,
    they are ideal for leveraging **explicit feedback**, whereby the user logs an
    actual rating or score. **Implicit feedback** is indirect feedback, such as believing
    that a user likes a particular movie solely on the basis that they chose to rent
    that movie. Content-based recommendation systems are better suited to implicit
    feedback as they will use information about the content of the items to improve
    their knowledge about the user's preferences.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤系统本质上需要某种反馈系统，以便推荐器记录特定的评分。特别是，它们非常适合利用**显式反馈**，即用户记录实际的评分或分数。**隐式反馈**是间接反馈，例如，仅基于用户选择租借某部电影就认为用户喜欢这部电影。基于内容的推荐系统更适合隐式反馈，因为它们将使用有关项目内容的信息来改善对用户偏好的了解。
- en: In addition, content-based recommendation systems often make use of a user profile
    in which the user may record what he or she likes in the form of a list of keywords,
    for example. Moreover, preference keywords can be learned from queries made by
    the user in the item database, if searching is supported.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，基于内容的推荐系统通常使用用户配置文件，用户可以记录他们喜欢的关键词列表，例如。此外，如果支持搜索，偏好关键词可以从用户在项目数据库中提出的查询中学习。
- en: Certain types of content are more amenable to the content-based approach. The
    classic scenario for a content-based recommender is when the content is in the
    form of text. Examples include book and news article recommendation systems. With
    text-based content, we can use techniques from the field of information retrieval
    in order to build up an understanding of how different items are similar to each
    other. For example, we have seen ways to analyze text using the bag of words feature
    when we looked at sentiment analysis in [Chapter 8](part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 8. Dimensionality Reduction"), *Probabilistic Graphical Models*, and
    topic modeling in [Chapter 10](part0076_split_000.html#28FAO2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 10. Probabilistic Graphical Models"), *Topic Modeling*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 某些类型的内容更适合基于内容的方法。基于内容的推荐系统的经典场景是当内容以文本形式存在时。例如，包括书籍和新闻文章推荐系统。基于文本内容，我们可以使用信息检索领域的技巧来构建对不同项目之间相似性的理解。例如，当我们查看第8章（part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7
    "第8章. 维度约简"）、*概率图模型*和第10章（part0076_split_000.html#28FAO2-c6198d576bbb4f42b630392bd61137d7
    "第10章. 概率图模型"）的*主题建模*时，我们看到了使用词袋特征分析文本的方法。
- en: Of course, content such as images and video is much less amenable to this method.
    For general products, the content-based approach requires textual descriptions
    of all the items in the database, which is one of its drawbacks. Furthermore,
    with content-based recommendations, we are often likely to consistently suggest
    items that are too similar; that is to say that our recommendations might not
    be sufficiently varied. For instance, we might consistently recommend books by
    the same author or news articles with the same topic precisely because their content
    is so similar.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，图像和视频等内容的这种方法的适用性要低得多。对于通用产品，基于内容的方法需要数据库中所有项目的文本描述，这是其缺点之一。此外，基于内容的推荐往往可能持续推荐过于相似的项目；也就是说，我们的推荐可能不够多样化。例如，我们可能会持续推荐同一作者的书籍或同一主题的新闻文章，正是因为它们的内容非常相似。
- en: By contrast, the collaborative filtering paradigm uses empirically found relationships
    between users and items on the basis of preferences alone. Consequently, it can
    be far less predictable (though, in some contexts, this is not necessarily good).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，协同过滤范式仅基于偏好来使用用户和项目之间经验上发现的关系。因此，它可能远不如可预测（尽管在某些情况下，这并不一定是好事）。
- en: One of the classic difficulties faced by collaborative filtering and content-based
    recommendation systems alike is the **cold start problem**. If we are basing the
    recommendations we supply using ratings made by users or on the content that they
    somehow indicated they like, how do we deal with new users and new items for which
    we have no ratings at all? One way to handle this is to use heuristics or rules
    of thumb, for example, by suggesting items that most users will like, just as
    the POPULAR algorithm does.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤和基于内容的推荐系统都面临的一个经典难题是**冷启动问题**。如果我们是基于用户提供的评分或他们以某种方式表示喜欢的内客来提供推荐，那么我们如何处理没有评分的新用户和新物品呢？处理这个问题的一种方法是通过启发式方法或经验法则，例如，通过建议大多数用户都会喜欢的物品，就像POPULAR算法所做的那样。
- en: '**Knowledge-based recommendation systems** avoid this issue entirely by basing
    their recommendations on rules and other sources of information about users and
    items. These systems usually behave quite predictably, have reliable quality,
    and can enforce a particular business practice, such as a sales-driven policy,
    with regard to making recommendations. Such recommenders often ask users specific
    questions in an interactive attempt to learn their preferences and use rules or
    constraints to identify items that should be recommended.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于知识的推荐系统**通过基于规则和其他关于用户和物品的信息来源来制定推荐，从而完全避免了这个问题。这些系统通常表现得很可预测，质量可靠，并且可以在制定推荐时强制执行特定的商业实践，例如销售驱动政策。这类推荐器通常会通过交互式提问来了解用户的偏好，并使用规则或约束来识别应该推荐的物品。'
- en: Often, this results in a system that, although predictable, can explain its
    output. This means that it can justify its recommendations to a user, which is
    a property that most examples of recommenders that follow the other paradigms
    lack. One important drawback of the knowledge-based paradigm, besides the initial
    effort necessary to design it, is that it is static and cannot adapt to changes
    or trends in user behavior.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这会导致一个虽然可预测但可以解释其输出的系统。这意味着它可以向用户证明其推荐的合理性，这是大多数遵循其他范式的推荐器所缺乏的特性。除了设计它所需的初始努力之外，基于知识的范式的一个重要缺点是它是静态的，无法适应用户行为的变化或趋势。
- en: Finally, it is well worth mentioning that we can design hybrid recommendation
    systems that incorporate more than one approach. An example of this is a recommender
    that uses collaborative filtering for most users but has a knowledge-based component
    for making recommendations to users that are new to the system. Another possibility
    for a hybrid recommendation system is to build a number of recommenders and integrate
    them into an ensemble using a voting scheme for the final recommendation.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得一提的是，我们可以设计混合推荐系统，结合多种方法。一个例子是，一个使用协同过滤为大多数用户推荐，但对于新加入系统的用户使用基于知识的组件进行推荐的推荐器。混合推荐系统的另一种可能性是构建多个推荐器，并使用投票方案将它们集成到一个集成中，以进行最终推荐。
- en: Note
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'A good all-round book that covers a wide variety of different recommender system
    paradigms and examples is *Recommender Systems: An Introduction* by *Dietmar Jannach*
    and others. This is published by *Cambridge University Press*.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一本涵盖广泛不同推荐系统范式和示例的优秀全面书籍是*Dietmar Jannach*和其他人合著的*《推荐系统：入门》*。这本书由*剑桥大学出版社*出版。
- en: Summary
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the process of building and evaluating recommender
    systems in R using the `recommenderlab` package. We focused primarily on the paradigm
    of collaborative filtering, which in a nutshell formalizes the idea of recommending
    items to users through word-of-mouth. As a general rule, we found that user-based
    collaborative filtering performs quite quickly but requires all the data to make
    predictions. Item-based collaborative filtering can be slow to train a model but
    makes predictions very quickly once the model is trained. It is useful in practice
    because it does not require us to store all the data. In some scenarios, the trade-off
    in accuracy between these two can be high, but in others the difference is acceptable.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用`recommenderlab`包在R中构建和评估推荐系统的过程。我们主要关注协同过滤范式，它简而言之就是通过口碑推荐物品给用户。一般来说，我们发现基于用户的协同过滤执行速度相当快，但需要所有数据来做出预测。基于物品的协同过滤在训练模型时可能较慢，但一旦模型训练完成，预测速度就非常快。它在实践中很有用，因为它不需要我们存储所有数据。在某些场景中，这两种方法在准确性之间的权衡可能很高，但在其他情况下，这种差异是可以接受的。
- en: The process of training recommendation systems is quite resource-intensive and
    a number of important parameters come into play in the design, such as the metrics
    used to quantify similarity and distance between items and users. Finally, we
    touched upon alternatives to the collaborative filtering paradigm. Content-based
    recommendation systems are designed to leverage similarity between items on the
    basis of their content. As such, they are ideally suited to the domain of text.
    Knowledge-based recommendation systems are designed to make recommendations to
    users on the basis of a set of rules or constraints that have been designed by
    experts. These can be combined with the other approaches in order to address the
    cold-start problem for new users or items.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 训练推荐系统的过程非常资源密集，设计过程中涉及到许多重要参数，例如用于量化物品和用户之间相似性和距离的度量标准。最后，我们简要讨论了协作过滤范式的替代方案。基于内容的推荐系统旨在利用物品内容之间的相似性。因此，它们非常适合文本领域。基于知识的推荐系统旨在根据专家设计的规则或约束为用户提供推荐。这些方法可以与其他方法结合使用，以解决新用户或物品的冷启动问题。
- en: In the next chapter, we will cover the idea of applying the techniques and practices
    already covered in this book to very large data sources and point out the specific
    challenges encountered when working with big data.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍如何将本书中已经介绍的技术和实践应用于非常大量的数据源，并指出在处理大数据时遇到的具体挑战。
