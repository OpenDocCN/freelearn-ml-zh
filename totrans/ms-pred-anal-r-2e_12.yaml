- en: Chapter 12. Recommendation Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our final chapter, we''ll tackle one of the most ubiquitous problems prevalent
    in the e-commerce world: making effective product recommendations to customers.
    Recommendation systems, also referred to as recommender systems, often rely on
    the notion of similarity between objects in an approach known as collaborative
    filtering. Its basic premise is that customers can be considered similar to each
    other if they share most of the products that they have purchased; equally, items
    can be considered similar to each other if they share a large number of customers
    who purchased them.'
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of different ways to quantify this notion of similarity,
    and we will present some of the commonly used alternatives. Whether we want to
    recommend movies, books, hotels, or restaurants, building a recommender system
    often involves dealing with very large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Rating matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **recommendation system** usually involves having a set of users, *U = {u[1]*
    *, u[2]* *, …, u[m]* *},* that have varying preferences on a set of items, *I
    = {i[1], i[2], …, i[n]* *}*. The number of users, *|U| = m*, is usually different
    from the number of items, *|I| = n*. In addition, users can often express their
    preference by rating items on some scale. As an example, we can think of users
    as being restaurant patrons in a city, and the items being the restaurants that
    they visit. Under this setup, the preferences of the users could be expressed
    as ratings on a five-star scale. Of course, our generalization does not require
    that the items be physical items or that the users be actual people—this is simply
    an abstraction that is commonly used for the recommender system problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an illustration, think of a dating website in which users rate other users;
    here, the *items* that are being rated are the profiles of the actual users themselves.
    Let''s return to our example of a restaurant recommender system and build some
    example data. A natural data structure that is popular for recommendation systems
    is the **rating matrix**. This is an *m × n* matrix where the rows represent the
    users and the columns represent the items. Each entry, *e[i], [j]*, of the matrix
    represents the rating made by the user *i* for item *j*. What follows is a simple
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have used a 10-point scale as a rating system, where 10 is the highest
    rating and 1 is the lowest. An alternative rating scale is a binary rating scale,
    where 1 indicates a positive rating and 0 indicates a negative rating. This second
    approach would yield a binary rating matrix. How might we be able to use this
    rating matrix in order to inform a simple recommender system for other users?
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, suppose that a new user, Silvan, has rated a few restaurants and
    we would like to make a recommendation for a suitable restaurant to which he has
    not been. Alternatively, we might want to propose a list of the top three restaurants
    or even predict whether Silvan will like a specific restaurant he is currently
    considering.
  prefs: []
  type: TYPE_NORMAL
- en: One way to think about this problem is to find users that have similar views
    to Silvan on the restaurants that he has already rated. Then, we could use their
    ratings on restaurants that Silvan has not yet rated in order to predict Silvan's
    rating for those restaurants. This seems promising, but we should first think
    about how we might quantify this notion of similarity between two users based
    on their item ratings.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring user similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even with a very large database of users, chances are that, for a real-world
    recommender system, it will be rare—if not massively unlikely—to find two people
    who would rate all the items in our item set with the exact same score. That being
    said, we can still say that some users are more similar than others based on how
    they rate different items. For example, in our restaurant rating matrix, we can
    see that Ines and Oliver rated the first four restaurants poorly and the last
    four restaurants highly, and so their tastes can be considered far more similar
    compared to a pair such as Thibault and Pedro, who sometimes agree and sometimes
    have completely opposite views on a particular restaurant.
  prefs: []
  type: TYPE_NORMAL
- en: By representing a user as their particular row in the rating matrix, we can
    think of a user as being a vector in an *n* dimensional space, *n* being the number
    of items. Thus, we can use different distance measures appropriate for vectors
    in order to measure the similarity of two different users. Note that the notion
    of distance is inversely proportional to the notion of similarity, and we can
    thus use measures of distance as measures of similarity by interpreting a large
    distance between two vectors as analogous to a low similarity score.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most familiar distance metric for two vectors, *a* and *b,* is the **Euclidean
    distance**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Measuring user similarity](img/00198.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use R''s built-in `dist()` function to compute all the pair-wise distances
    in our rating matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a lower triangular matrix because the Euclidean distance is a
    symmetric function. Thus, the entry for (`maria`, `pedro`) is exactly the same
    as for (`pedro`, `maria`), and so we only need to display one of these. Here,
    we can explicitly see that Ines and Oliver are the two most similar users as the
    distance between them is the smallest. Note that we can also talk about the distances
    between items in terms of the similarity of the ratings they received from different
    users. All we have to do to compute this is to transpose the rating matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the two most dissimilar restaurants (that is to say, those with
    the largest difference between them) are the *Acropolis* and *Berny's*. Looking
    back at the rating matrix, we should easily see why this is the case. The former
    restaurant has received largely positive reviews across our user base, whereas
    the reviews have been poor for the latter.
  prefs: []
  type: TYPE_NORMAL
- en: 'A commonly used alternative to the Euclidean distance (or L2 norm, as it is
    also known) is the **cosine distance**. This metric measures the cosine of the
    smallest angle between two vectors. If the vectors are parallel to each other,
    meaning that their angle is 0, then the cosine distance is 0 as well. If the two
    vectors are at a right angle to each other, then they have the largest distance
    according to this metric. The cosine distance is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Measuring user similarity](img/00199.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the numerator is the dot product between the two vectors and the denominator
    is the product of the magnitudes (typically computed via the L2 norm) of the two
    vectors. The cosine distance isn''t available as a method in the `dist()` function
    of R''s base distribution, but we can install the `proxy` package, which enhances
    this function with a number of new distance metrics in order to compute the cosine
    distances for our rating matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose instead that our users rated restaurants on a binary scale. We can
    convert our rating matrix into a binary rating matrix by considering all ratings
    above 5 to be positive and assigning them a new score of 1\. The remaining ratings
    are all converted to a score of 0\. For two binary vectors, the **Jaccard similarity**
    is given by the cardinality of the logical intersection divided by the cardinality
    of the logical union. The Jaccard distance is then computed as 1 minus this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Measuring user similarity](img/00200.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In a nutshell, what this is computing is one minus the ratio of the number
    of positions in which the two vectors both have a positive rating over the total
    number of positions in which either of the two vectors have a positive rating.
    Two binary vectors that agree in all their positive positions will be identical
    and thus have a distance of 0\. Using the `proxy` package, we can show the `Jaccard`
    distance for our restaurant patrons as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The study of measurement and distance metrics is broad and there are many suitable
    metrics that have been applied to the recommender system setting. The definitive
    reference for distance metrics is the *Encyclopedia of Distances*, *Michel Marie
    Deza and Elena Deza*, *Springer*.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having covered distances, we are ready to delve into the topic of **collaborative
    filtering**, which will help us to define a strategy for making recommendations.
    Collaborative filtering describes an algorithm, or more precisely a family of
    algorithms, that aims to create recommendations for a test user given only information
    about the ratings of other users via the rating matrix, as well as any ratings
    that the test user has already made.
  prefs: []
  type: TYPE_NORMAL
- en: There are two very common variants of collaborative filtering, **memory-based
    collaborative filtering** and **model-based collaborative filtering**. With memory-based
    collaborative filtering, the entire history of all the ratings made by all the
    users is remembered and must be processed in order to make a recommendation. The
    prototypical memory-based collaborative filtering method is **user-based collaborative
    filtering**. Although this approach uses all the ratings available, the downside
    is that it can be computationally expensive as the entire database is used in
    order to make rating predictions for our test user.
  prefs: []
  type: TYPE_NORMAL
- en: The alternative approach to this is embodied in model-based collaborative filtering.
    Here, we first create a model of the rating preferences of our users, such as
    a set of clusters of users who like similar items, and then use the model to generate
    recommendations. We will study **item-based collaborative filtering**, which is
    the most well-known, model-based collaborative filtering method.
  prefs: []
  type: TYPE_NORMAL
- en: User-based collaborative filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: User-based collaborative filtering is commonly described as a memory-based or
    lazy learning approach. Unlike most of the models we have built in this book,
    which assume that we will fit the data to a particular model and then use that
    model to make predictions, lazy learning simply uses the training data itself
    to make predictions directly. We saw an example of lazy learning with k-nearest
    neighbors in [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*.
    In fact, the premise of the user-based collaborative filtering approach builds
    directly on the k-nearest neighbors approach.
  prefs: []
  type: TYPE_NORMAL
- en: However, in user-based collaborative filtering, when we want to make recommendations
    for a new user, we will first pick a set of similar users using a particular distance
    metric. Then, we try to infer the ratings that our target user would assign to
    items that he or she has not yet rated as an average of the ratings made by similar
    users on those items. We usually refer to this set of similar users as the user's
    **neighborhood**. Thus, the idea is that a user will prefer items that their neighborhood
    prefers.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, there are two ways to define the user's neighborhood. We can compute
    a fixed neighborhood by finding the k-nearest neighbors. These are the *k* users
    in our database that have the smallest distance between them and our target user.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can specify a similarity threshold and pick all the users
    in our database whose distance from our target user does not exceed this threshold.
    This second approach has the advantage that we will be making recommendations
    by employing users that are as close to our target user as we want, and therefore
    our confidence in our recommendation can be high. On the other hand, there might
    be very few users that satisfy our requirement, meaning that we will be relying
    on the recommendations of these few users. Worse, there might be no users in our
    database who are sufficiently similar to our target user and we might not be able
    to actually make a recommendation at all. If we don't mind our method sometimes
    failing to make a recommendation, for example because we have a backup plan to
    handle these cases, the second approach might be a good choice.
  prefs: []
  type: TYPE_NORMAL
- en: Another important consideration to make in a real-world setting is the problem
    of sparse ratings. In our simple restaurant example, every user had rated every
    restaurant. This rarely, if ever, happens in a real situation, simply because
    the number of items is usually too big for a user to rate them all. If we think
    of e-commerce websites such as Amazon.com, for example, it is easy to imagine
    that the maximum number of products that any user has rated is still only a small
    fraction of the overall number of products on sale.
  prefs: []
  type: TYPE_NORMAL
- en: To compute distance metrics between users in order to determine similarity,
    we usually incorporate only the items that both users have rated. Consequently,
    in practice, we often make comparisons between users in a smaller number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have decided on a distance metric and how to form a neighborhood of
    users that are similar to our test user, we then use this neighborhood to compute
    the missing ratings for the test user. The easiest way to do this is to simply
    compute the average rating for each item in the user neighborhood and report this
    value. Thus, for test user *t* and an item *j*, for which the test user has not
    yet made a rating, we can predict the test user''s rating for that item,![User-based
    collaborative filtering](img/00201.jpeg) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![User-based collaborative filtering](img/00202.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This equation expresses the simple idea that the predicted rating of our test
    user *t* for item *j* is just the average of the ratings made by the test user's
    neighborhood on this item. Suppose we had a new user for our restaurant scenario
    and this user had already rated a few of the restaurants. Then, imagine that,
    from those ratings, we discovered that our new user's neighborhood comprised Oliver
    and Thibault. If we wanted to make a prediction for what rating the test user
    would make on the restaurant *El Pollo Loco*, this would be done by averaging
    the ratings of Oliver and Thibault for this restaurant, which in this case would
    be the average of 2 and 4, yielding a rating of 3.
  prefs: []
  type: TYPE_NORMAL
- en: If our objective was to obtain a top-N list of recommendations for our user,
    we would repeat this process for all the items in the database, sort them in descending
    order so that the highest rated items appeared first, and then pick out the top
    *N* items from this list. In practice, we would only need to check the items that
    at least one of the users in the new user's neighborhood has rated in order to
    simplify this computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can make some improvements to this very simple approach. A first possible
    improvement comes from the observation that some users will tend to consistently
    rate items more strictly or more leniently than other users, and we would like
    to smooth out this variation. In practice, we often use *Z*-score normalization,
    which takes into account the variance of ratings. We can also center each rating
    made by a user by subtracting that user''s average rating across all the items.
    In the rating matrix, this means subtracting the mean of each row from the elements
    of the row. Let''s apply this last transformation to our restaurant rating matrix
    and see the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Even though both Ines and Gertrude originally rated `Berny's` with the same
    rating of 1, the centering operation has Ines rating this restaurant with a lower
    score than Gertrude. This is because Ines tends to make higher ratings on average
    than Gertrude, and so the rating of 1 for Ines could be interpreted as a stronger
    negative rating than Gertrude's.
  prefs: []
  type: TYPE_NORMAL
- en: Another area of improvement concerns the way in which we incorporate the ratings
    of our new user's neighborhood to create the final ratings. By treating the ratings
    of all the neighboring users as equal, we are ignoring the fact that our distance
    metric may show that certain users in the neighborhood of the new user are more
    similar to the new user than others.
  prefs: []
  type: TYPE_NORMAL
- en: As we have already seen in the example on Jaccard similarity and Jaccard distance,
    we can often define a similarity metric from a distance metric by inverting it
    in some way, such as subtracting from 1 or taking the reciprocal. Consequently,
    for the distance metric of our choice, we can define its corresponding similarity
    metric and denote it with *sim(u,t)*. A user similarity metric takes high values
    for similar users, which are users for whom a distance metric takes low values.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this clarification established, we can incorporate the similarity between
    users *u* and *t* in our previous equation by taking a weighted average of the
    ratings made by the neighboring users of the new user as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![User-based collaborative filtering](img/00203.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Other reasons why we might want to incorporate weights in the ratings made by
    other users include trust. For example, we might trust a user that has been using
    our restaurant recommendation service for a long time more who a more recent user.
    Equally, we might also want to consider the total number of items that a user
    has rated in common with the new user. For example, if a user has only rated two
    items in common with the new user, then even, if the corresponding ratings made
    are identical, the evidence that these two users are indeed very similar is limited.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, the single largest difficulty with user-based collaborative filtering
    is that making recommendations for a test user requires access to the whole database
    of users in order to determine the user neighborhood. This is done by performing
    a similarity computation between the test user and every other user, an expensive
    process computationally. Next, we'll look at item-based collaborative filtering,
    which attempts to ameliorate this situation.
  prefs: []
  type: TYPE_NORMAL
- en: Item-based collaborative filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Item-based collaborative filtering is a model-based approach to collaborative
    filtering. The central idea underlying this method is that, instead of looking
    at other users similar to the test user, we will directly recommend items that
    are similar to the items that have received a high rating by the test user. As
    we are directly comparing items, instead of first comparing users, to recommend
    items, we can build up a model to describe the similarity of items and then use
    the model rather than the entire database to make recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: The process of building an item-based similarity model involves computing a
    similarity matrix for all pairs of items in our database. If we have *N* items,
    then we will end up with a similarity matrix with *N²* elements in total. To reduce
    the size of our model, we can store a list of the similarity values of the top
    *k* most similar items for every item in the database.
  prefs: []
  type: TYPE_NORMAL
- en: As *k* will be far smaller than *N*, we will have a very substantial reduction
    in the size of the data that we need to keep for our model. For every item in
    our database, this list of the *k* most similar items is analogous to the neighborhood
    of users for the user-based collaborative filtering approach. The same discussion
    regarding normalizing with respect to the bias and variance of user ratings in
    user-based collaborative filtering can be applied here. That is, we can compute
    item-to-item similarities after we normalize our rating matrix.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is not without its shortcomings. In the memory-based recommender,
    a new user rating can automatically be incorporated into the recommendation process
    because that approach uses the entire database (rating matrix). Model-based collaborative
    filtering requires us to periodically retrain the model to incorporate information
    from these new ratings. In addition, the fact that the modeling process discards
    some information from the original rating matrix means that it can sometimes make
    non-optimal recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these drawbacks, the space and time performance of item-based collaborative
    filtering means that it has been very successfully applied in a number of real-world
    settings. Model retraining can be done offline and automatically scheduled, and
    the non-optimality of recommendations can often be tolerated.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can devise an analogous equation to what we saw for user-based collaborative
    filtering that explains how to predict a new rating using the item-based collaborative
    filtering model. Suppose we want to estimate the rating that our test user, *t*,
    would give to an item, *I*. Suppose also that we have already chosen a similarity
    function, *sim(i,j)*, between a pair of items, *i* and *j*, and from this we constructed
    our model. Using the model, we can retrieve the stored item neighborhood for the
    item in which we are interested, *S(i)*. To compute the predicted rating that
    our test user will make on this item, we calculate the weighted sum of the ratings
    our user has made on items that are similar to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Item-based collaborative filtering](img/00204.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: While this approach won't work if the user hasn't rated any items similar to
    the item in question, it does not require finding users that have similar preferences
    to the test user.
  prefs: []
  type: TYPE_NORMAL
- en: Singular value decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a real-world recommender system, the rating matrix will eventually become
    very large as more users are added to the system and the list of items being offered
    grows. As a result, we may want to apply a dimensionality reduction technique
    to this matrix. Ideally, we would like to retain as much information as possible
    from the original matrix while doing this. One such method that has applications
    across a wide range of disciplines uses **singular value decomposition**, or **SVD**
    as it is commonly abbreviated to.
  prefs: []
  type: TYPE_NORMAL
- en: SVD is a matrix factorization technique that has a number of useful applications,
    one of which is dimensionality reduction. It is related to the PCA method of dimensionality
    reduction that we saw in [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*,
    and many people confuse the two. SVD actually describes just a mathematical method
    of factorizing matrices. In fact, some implementations of PCA use SVD to compute
    the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by looking at how this process works. SVD is a matrix factorization
    process, so we start with an original matrix representing our data and express
    this as a product of matrices. In a dimensionality reduction scenario, our input
    data matrix would be the matrix where the rows are data points and the columns
    are the features; thus, in R, this would just be a data frame. In our recommender
    systems scenario, the matrix we use is our rating matrix. Suppose that we call
    our rating matrix *D* and we have *m* users (rows) rating *n* items (columns).
    The SVD factorization of this matrix is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Singular value decomposition](img/00205.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous equation, *U* and *V* are square matrices and the matrix *Σ*
    is a matrix with the same dimensionality as our input matrix, *D*. In addition,
    it is a diagonal matrix, meaning that all the elements of the matrix are zero
    except those on the leading diagonal. These elements are conventionally ordered
    from largest to smallest and are known as the **singular values** of the matrix
    *D*, giving rise to the name singular value decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Readers familiar with linear algebra will know that the eigenvalues of a matrix
    are often also described as containing information about the important dimensions
    of that matrix. It turns out that the eigenvalues of a matrix are related to the
    singular values through the following relationship—the singular values of a matrix,
    *D,* are the same as the square roots of the eigenvalues of the matrix product
    *D* × *D^T*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily perform SVD on a matrix in R via the `svd()` function, which
    is available with R''s `base` package. Let''s see this using our existing `ratingMatrix`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The singular values are returned as a vector `d`, from which we can easily
    construct the diagonal matrix using the `diag()` function. To verify that this
    factorization really is the one that we expected, we can reconstruct our original
    rating matrix by simply multiplying the matrix factors that we have obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'One thing to note here is that, if we were to attempt a direct equality check
    with our original matrix, we would most likely fail. This is due to rounding errors
    that are introduced when we store the factorized matrices. We can check that our
    two matrices are nearly equal using the `all.equal()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The reader is encouraged to decrease the size of the tolerance and note that,
    after several decimal points, the equality check fails. Though the two matrices
    are not exactly equal, the difference is so small that this will not impact us
    in any significant way. Now, once we have this factorization, let's investigate
    our singular values. The first singular value of 35.6 is much larger than the
    smallest singular value of 1.3.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can perform dimensionality reduction by keeping the top singular values
    and throwing the rest away. To do this, we''d like to know how many singular values
    we should keep and how many we should discard. One approach to this problem is
    to compute the square of the singular values, which can be thought of as the vector
    of **matrix energy**, and then pick the top singular values that preserve at least
    90 % of the overall energy of the original matrix. This is easy to do with R as
    we can use the `cumsum()` function for creating a cumulative sum and the singular
    values are already ordered from largest to smallest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Keeping the first two singular values will retain 92 % of the energy of our
    original matrix. Using just two values, we can reconstruct our rating matrix and
    observe the differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, there are a few differences in the absolute values, but most
    of the patterns of different users have been retained to a large extent. Discarding
    singular values effectively introduces zeros in the leading diagonal of matrix
    *D* in the factorization, so that this matrix ends up with entire rows and columns
    that only contain zeros. Consequently, we can truncate not only this matrix, but
    rows from the matrices *U* and *V*.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we reduce the size of the data that we have to store.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting recommendations for movies and jokes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will focus on building recommender systems using two different
    datasets. To do this, we shall use the `recommenderlab` package. This provides
    us with not only the algorithms to perform the recommendations, but also with
    the data structures to store the sparse rating matrices efficiently. The first
    datasets we will use contains anonymous user reviews for jokes from the *Jester
    Online Joke recommender system*.
  prefs: []
  type: TYPE_NORMAL
- en: The joke ratings fall on a continuous scale (-10 to +10). A number of datasets
    collected from the Jester system can be found at [http://eigentaste.berkeley.edu/dataset/](http://eigentaste.berkeley.edu/dataset/).
    We will use the datasets labeled on the website as *Dataset 2+*. This datasets
    contains ratings made by 50,692 users on 150 jokes. As is typical with a real-world
    application, the rating matrix is very sparse in that each user rated only a fraction
    of all the jokes; the minimum number of ratings made by a user is 8\. We will
    refer to this data set as the jester datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The second datasets can be found at [http://grouplens.org/datasets/movielens/](http://grouplens.org/datasets/movielens/).
    This website contains data on user ratings for movies that were made on the *MovieLens*
    website at [http://movielens.org](http://movielens.org). Again, there is more
    than one datasets on the website; we will use the one labeled *MovieLens1M*. This
    contains ratings on a five-point scale (1-5) made by 6,040 users on 3,706 movies.
    The minimum number of movie ratings per user is 20\. We will refer to this datasets
    as the movie dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These two datasets are actually very well-known open source datasets, to the
    point that the `recommenderlab` package itself includes smaller versions of them
    as part of the package. Readers who would like to skip the process of loading
    and preprocessing the data, or who would like to run the examples that follow
    on smaller datasets due to computational constraints, are encouraged to try them
    out using `data(Jester5k)` or `data(MovieLense)`.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and pre-processing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first goal in building our recommender systems is to load the data in R,
    preprocess it, and convert it into a rating matrix. More precisely, in each case,
    we will be creating a `realRatingMatrix` object, which is the specific data structure
    that the `recommenderlab` package uses to store numerical ratings. We will start
    with the jester datasets. If we download and unzip the archive from the website,
    we'll see that the file `jesterfinal151cols.csv` contains the ratings. More specifically,
    each row in this file corresponds to the ratings made by a particular user, and
    each column corresponds to a particular joke.
  prefs: []
  type: TYPE_NORMAL
- en: 'The columns are comma-separated and there is no header row. In fact, the format
    is almost already a rating matrix, were it not for the fact that the first column
    is a special column and contains the total number of ratings made by a particular
    user. We will load this data into a data table using the function `fread()`, which
    is a fast implementation of `read.table()` and efficiently loads a data file into
    a data table. We''ll then drop the first column efficiently using the `data.table`
    syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The last line used the assignment operator `:=` to set the first column, `V1`,
    to `NULL`, which is how we drop a column on a data table. We now have one final
    preprocessing step left to do on our data table, `jester`, before we are ready
    to convert it to a `realRatingMatrix` object. Specifically, we will convert this
    into a matrix and replace all occurrences of the rating of 99 with `NA`, as 99
    was the special rating used to represent missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Depending on the computational resources of the computer available to us (most
    notably, the available memory), we may want to try to process a single datasets
    in its entirety instead of loading both datasets at once. Here, we have chosen
    to work with the two datasets in parallel in order to showcase the main steps
    in the analysis and highlight any differences or particularities of an individual
    datasets with respect to a particular step.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the MovieLens data. Downloading the MovieLens1M archive and
    unzipping reveals three main data files. The `users.dat` file contains background
    information about the users, such as age and gender. The `movies.dat` data file,
    in turn, contains information about the movies being rated, namely the title and
    a list of genres (for example, *comedy*) to which the movie belongs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are mainly interested in the `ratings.dat` file, which contains the ratings
    themselves. Unlike the raw jester data, here each line corresponds to a single
    rating made by a user. The line format contains the User ID, Movie ID, rating,
    and timestamp, all separated by two colon characters, `::`. Unfortunately, `fread()`
    requires a separator with a single character, so we will specify a single colon.
    The double-colon separator in the raw data results in us creating extra columns
    with `NA` values that we will have to remove, as well as the final column that
    contains the timestamp:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we are now left with three columns, where the first is the `UserID`,
    the second is the `MovieID`, and the last is the rating. We will now aggregate
    all the ratings made by a user in order to form an object that can be interpreted
    as or converted to, a rating matrix. We should aggregate the data in a way that
    minimizes memory usage. We will do this by building a sparse matrix using the
    `sparseMatrix()` command from the `Matrix` package.
  prefs: []
  type: TYPE_NORMAL
- en: This package is loaded automatically when we use the `recommenderlab` package,
    as, it is one of its dependencies. To build a sparse matrix using this function,
    we can simply specify a vector of row coordinates, a vector of matching column
    coordinates, and a vector with the nonzero values that fill up the sparse matrix.
    Remember, as our matrix is sparse, all we need are the locations and values for
    entries that are nonzero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Right now, it is slightly inconvenient that we cannot directly interpret the
    User IDs and Movie IDs as coordinates. This is because, if we have a user with
    a User ID value of 1 and a user with a User ID value of 3, R will automatically
    create a user with a User ID value of 2 and create an empty row, even though that
    user does not actually exist in the training data. The situation is similar for
    columns. Consequently, we must first make factors out of our `UserID` and `MovieID`
    columns before proceeding to create our rating matrix as described earlier. Here
    is the code for building our rating matrix for the MovieLens data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: It is a good exercise to check that the dimensions of the result correspond
    to our expectations on the number of users and movies respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before building and evaluating recommender systems using the two datasets we
    have loaded, it is a good idea to get a feel for the data. For one thing, we can
    make use of the `getRatings()` function to retrieve the ratings from a rating
    matrix. This is useful in order to construct a histogram of item ratings. Additionally,
    we can also normalize the ratings with respect to each user, as we discussed earlier.
    The following code snippet shows how we can compute ratings and normalized ratings
    for the jester data. We can then do the same for the MovieLens data and produce
    histograms for the ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot shows the different histograms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring the data](img/00206.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the Jester data, we can see that ratings above zero are more prominent than
    ratings below zero, and the most common rating is 10, the maximum rating. The
    normalized ratings create a more symmetric distribution centered on zero. For
    the MovieLens data with the 5-point rating scale, 4 is the most prominent rating
    and higher ratings are far more common than low ratings.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also look for the distribution of the number of items rated per user
    and the average rating per item by looking at the row counts and the column means
    of the rating matrix respectively. Again, the following code snippet shows how
    to compute these for the jester data, and we follow up with histograms showing
    the results for both datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The histograms of the Jester and MovieLens data are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring the data](img/00207.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Both datasets show a curve in the average ratings per user that looks like a
    power curve. Most of the users have rated very few items, but a small number of
    very committed users have actually rated a large number of items. In the Jester
    case, some have rated the maximum number of jokes in the datasets. This is an
    exception and only occurs because the number of items (jokes) in this datasets
    is relatively small. The distribution of the average joke rating is between -3
    and 4, but for movies we see the whole range of the spectrum, indicating that
    some users have rated all the movies they considered completely awful or totally
    great. We can find the average of these distributions in order to determine the
    average number of items rated per user and the average rating of each item.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we need to remove `NA` values from consideration in the Jester datasets,
    as some columns may not have ratings in them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating binary top-N recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now have some sense of what our data looks like for both datasets so we
    can start building some models. We will begin by looking at the problem of making
    top-N recommendations for a binary recommender system, which is simpler to do
    than when we have more granular data for ratings. Recall that a top-N recommendation
    is nothing but a list of *N* recommendations that are more likely to interest
    a user. To do this, we will use the jester datasets and create a binary version
    of our rating matrix. We''ll call any rating that is 5 or above a positive rating.
    As this may result in some users having no positive ratings, we''ll also prune
    the rating matrix and keep only users with at least 10 positive ratings under
    this scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the advantages of the `recommenderlab` package is that it makes it very
    easy for us to compare results from several algorithms. The process of training
    and evaluating multiple algorithms for top-N recommendations begins by creating
    a list containing the definitions of the algorithms that we want to use. Each
    element in the list is given a name of our choice but must itself be a list containing
    a set of parameters for configuring a known algorithm. Concretely, the `name`
    parameter of this inner parameter list must be one that the `recommenderlab` package
    recognizes. It is possible to create and register one''s own algorithm with this
    package, but our focus will be on existing implementations that more than suffice
    for our intents and purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The RANDOM algorithm is a baseline algorithm that makes recommendations randomly.
    The POPULAR algorithm is another baseline algorithm that can sometimes be tough
    to beat. This proposes items in descending order of global popularity, so that
    for a top-1 recommendation, it will recommend the item with the highest average
    rating in the datasets. We have chosen to try out two variants of user-based collaborative
    filtering for this example. The first one uses the cosine distance and specifies
    50 as the number of nearest neighbors to use. The second one is identical but
    uses the Jaccard distance instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define an evaluation scheme via the function `evaluationScheme()`.
    This function records how we will split our data into training and test sets,
    the number of ratings we will take as given from our test users via the `given`
    parameter, and how many runs we want to execute. We will do a straight 80-20 split
    for our training and test set, consider 10 ratings from our test users as known
    ratings, and evaluate over a single run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `given` parameter must be at least as large as the smallest number
    of items rated by a user in our datasets. We previously filtered the datasets
    to ensure we have 10 items per user, so we are covered in our case. Finally, we
    will evaluate our list of algorithms in turn with our evaluation scheme using
    the `evaluate()` function. Aside from an evaluation scheme and a list of algorithms,
    we will also specify the range of *N* values to use when making top-N recommendations
    via the `n` parameter. We will do this for values 1 through 20:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We now have a list of four objects that represent the evaluation results of
    each algorithm on our data. We can get important measures such as precision by
    looking at the confusion matrices. Note that, as we have run this experiment for
    the top-N recommendations, where *N* is in the range 1-20, we expect to have 20
    such confusion matrices for each algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function `getConfusionMatrix()`, when applied to one of these objects,
    can be used to retrieve the folded confusion matrices in tabular format so that
    each row represents the confusion matrix for a particular value of *N*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: To visualize this data and compare our algorithms, we can try plotting the results
    directly using the `plot()` function. For our evaluation results, the default
    is a plot of the **true positive rate** (**TPR**) versus the **false positive
    rate**(**FPR**). This is simply the ROC curve, as we know from [Chapter 4](part0035_split_000.html#11C3M1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 4. Generalized Linear Models"), *Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the ROC curve for the binary Jester data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating binary top-N recommendations](img/00208.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The graph shows that the user-based collaborative filtering algorithms perform
    better than the two baseline algorithms, but there is very little to separate
    these two, with the cosine distance marginally outperforming the Jaccard distance.
    We can complement this view of our results by also plotting a precision recall
    curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the precision recall curve for the binary Jester data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating binary top-N recommendations](img/00209.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The precision recall curve paints a similar picture, with the user-based collaborative
    filtering algorithm that uses the cosine distance coming out as the winner. Note
    that the trade-off between precision and recall surfaces in a top-N recommender
    system via the number of recommendations that the system makes. The way our evaluation
    scheme works is that we treat users in the test data as new users in the system
    that just contributed a certain number of ratings. We hold out as many ratings
    as the `given` parameter allows. Then, we apply our model to see if the ratings
    we suggest will agree with the ratings that remain. We order our suggestions in
    descending order of confidence so that in a top-1 recommendation system, we will
    suggest the item we believe has the best chance of interesting the user. Increasing
    *N,* therefore, is like casting a wider net. We will be less precise in our suggestions
    but are more likely to find something the user will like.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An excellent and freely available resource for recommendation systems is *Chapter
    9* from the online textbook *Mining of Massive Datasets* by *Jure Leskovec*, *Anand
    Rajaraman*, and *Jeffrey David Ullman*. The website is [http://www.mmds.org/](http://www.mmds.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating non-binary top-N recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will use the movies dataset to see how we perform in the
    non-binary scenario. First, we will define our algorithms as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, our algorithms will work with normalized ratings by specifying the
    `normalize` parameter. We will only be using the cosine distance for user-based
    collaborative filtering as the Jaccard distance only applies in the binary setting.
    Furthermore, we will also try out item-based collaborative filtering as well as
    SVD-based recommendations. Instead of directly splitting our data, we demonstrate
    how we can perform ten-fold cross-validation by modifying our evaluation scheme.
    We will continue to investigate making top-N recommendations in the range of 1
    to 20\. Evaluating a moderately-sized dataset with five algorithms using 10-fold
    cross-validation means that we can expect this process to take quite a long time
    to finish, depending on the computing power we have at our disposal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To conserve space, we have truncated the output that shows us the amount of
    time spent running each iteration for the different algorithms. Note that the
    most expensive algorithm during training is the item-based collaborative filtering
    algorithm, as this is building a model and not just performing lazy learning.
    Once the process terminates, we can plot the results in the same way as we did
    for our binarized Jester dataset to compare the performance of our algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the ROC curve for the MovieLens data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating non-binary top-N recommendations](img/00210.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, user-based collaborative filtering is the clear winner here.
    SVD performs in a similar manner to the POPULAR baseline, though the latter starts
    to become better when *N* is high. Finally, we see item-based collaborative filtering
    performing far worse than these, outperforming only the random baseline. What
    is clear from these experiments is that tuning recommendation systems can often
    be a very time-consuming, resource-intensive endeavor.
  prefs: []
  type: TYPE_NORMAL
- en: All the algorithms that we specified can be tuned in various ways and we have
    explored a number of parameters, from the size of the neighborhood to the similarity
    metric, that will influence the results. In addition, we've seen that, even for
    the top-N scenario alone, there are several ways that we can evaluate our recommendation
    system; thus, if we want to try out a number of these for comparison, we will
    again need to spend more time on model training.
  prefs: []
  type: TYPE_NORMAL
- en: The reader is encouraged to repeat these experiments using different parameters
    and evaluation schemes in order to get a feel for the process of designing and
    training recommendation systems. In addition, by visiting the websites of our
    two datasets, the reader can find additional links to similar datasets commonly
    used for learning about recommendation systems, such as the book-crossing datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'For completeness, we will plot the precision recall curve for the MovieLens
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the precision recall curve for the MovieLens data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating non-binary top-N recommendations](img/00211.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Evaluating individual predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way to evaluate a recommendation system is to ask it to predict the
    specific values of a portion of the known ratings made by a set of test users
    using the remainder of their ratings. In this way, we can measure accuracy by
    taking average distance measures over the predicted ratings. These include the
    **mean squared error** (**MSE**) and the **Root Mean Square Error** (**RMSE**),
    which we have seen before, and the **mean average error**(**MAE**), which is just
    the average of the absolute errors. We will do this for the regular (unbinarized)
    Jester datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin as before by defining an evaluation scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define individual user- and item-based collaborative filtering
    recommenders using the `Recommender()` and `getData()` functions. The logic behind
    these is that the `getData()` function will extract the ratings set aside for
    training by the evaluation scheme and the `Recommender()` function will use the
    data to train a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use these models to predict those ratings that were classified
    as known (there are as many of these as the `given` parameter specifies) in our
    test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can use the known ratings to compute prediction accuracy on the
    ratings kept for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the performance of the two algorithms is fairly close. User-based
    collaborative filtering performs better when we penalize larger errors (via the
    RMSE and MSE) through squaring. From the perspective of the mean average error,
    item-based collaborative filtering is marginally better.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, in this case, we might make our decision as to which type of recommendation
    system to use on the basis of the error behavior that more closely matches our
    business needs. In this section, we used the default parameter values for the
    two algorithms, but by using the `parameter` parameter in the `Recommender()`
    function, we can play around with different configurations as we did before. This
    is left as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: Other approaches to recommendation systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we concentrated our efforts on building recommendation systems
    by following the collaborative filtering paradigm. This is a very popular approach
    due to its many advantages. By essentially mimicking word-of-mouth recommendations,
    it requires virtually no knowledge about the items being recommended nor any background
    about the users in question.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, collaborative filtering systems incorporate new ratings as they arise,
    either through a memory approach, or via the regular retraining of a model-based
    approach. Thus, they naturally become better for their users over time as they
    learn more information and adapt to changing preferences. On the other hand, they
    are not without their disadvantages, not the least of which is the fact that they
    will not take into account any information about the items and their content even
    when it is available.
  prefs: []
  type: TYPE_NORMAL
- en: '**Content-based recommendation systems** try to suggest items to users that
    are similar to those that users like on the basis of content. The key premise
    behind this idea is that, if it is known that a user happens to like novels by
    *George R. R. Martin*, a fantasy and science fiction author, it makes sense that
    a recommendation service for books might suggest a similar author, such as *Robert
    Jordan*, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative filtering systems, by their nature, require some sort of feedback
    system in order for the recommender to record a particular rating. In particular,
    they are ideal for leveraging **explicit feedback**, whereby the user logs an
    actual rating or score. **Implicit feedback** is indirect feedback, such as believing
    that a user likes a particular movie solely on the basis that they chose to rent
    that movie. Content-based recommendation systems are better suited to implicit
    feedback as they will use information about the content of the items to improve
    their knowledge about the user's preferences.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, content-based recommendation systems often make use of a user profile
    in which the user may record what he or she likes in the form of a list of keywords,
    for example. Moreover, preference keywords can be learned from queries made by
    the user in the item database, if searching is supported.
  prefs: []
  type: TYPE_NORMAL
- en: Certain types of content are more amenable to the content-based approach. The
    classic scenario for a content-based recommender is when the content is in the
    form of text. Examples include book and news article recommendation systems. With
    text-based content, we can use techniques from the field of information retrieval
    in order to build up an understanding of how different items are similar to each
    other. For example, we have seen ways to analyze text using the bag of words feature
    when we looked at sentiment analysis in [Chapter 8](part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 8. Dimensionality Reduction"), *Probabilistic Graphical Models*, and
    topic modeling in [Chapter 10](part0076_split_000.html#28FAO2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 10. Probabilistic Graphical Models"), *Topic Modeling*.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, content such as images and video is much less amenable to this method.
    For general products, the content-based approach requires textual descriptions
    of all the items in the database, which is one of its drawbacks. Furthermore,
    with content-based recommendations, we are often likely to consistently suggest
    items that are too similar; that is to say that our recommendations might not
    be sufficiently varied. For instance, we might consistently recommend books by
    the same author or news articles with the same topic precisely because their content
    is so similar.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, the collaborative filtering paradigm uses empirically found relationships
    between users and items on the basis of preferences alone. Consequently, it can
    be far less predictable (though, in some contexts, this is not necessarily good).
  prefs: []
  type: TYPE_NORMAL
- en: One of the classic difficulties faced by collaborative filtering and content-based
    recommendation systems alike is the **cold start problem**. If we are basing the
    recommendations we supply using ratings made by users or on the content that they
    somehow indicated they like, how do we deal with new users and new items for which
    we have no ratings at all? One way to handle this is to use heuristics or rules
    of thumb, for example, by suggesting items that most users will like, just as
    the POPULAR algorithm does.
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge-based recommendation systems** avoid this issue entirely by basing
    their recommendations on rules and other sources of information about users and
    items. These systems usually behave quite predictably, have reliable quality,
    and can enforce a particular business practice, such as a sales-driven policy,
    with regard to making recommendations. Such recommenders often ask users specific
    questions in an interactive attempt to learn their preferences and use rules or
    constraints to identify items that should be recommended.'
  prefs: []
  type: TYPE_NORMAL
- en: Often, this results in a system that, although predictable, can explain its
    output. This means that it can justify its recommendations to a user, which is
    a property that most examples of recommenders that follow the other paradigms
    lack. One important drawback of the knowledge-based paradigm, besides the initial
    effort necessary to design it, is that it is static and cannot adapt to changes
    or trends in user behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is well worth mentioning that we can design hybrid recommendation
    systems that incorporate more than one approach. An example of this is a recommender
    that uses collaborative filtering for most users but has a knowledge-based component
    for making recommendations to users that are new to the system. Another possibility
    for a hybrid recommendation system is to build a number of recommenders and integrate
    them into an ensemble using a voting scheme for the final recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A good all-round book that covers a wide variety of different recommender system
    paradigms and examples is *Recommender Systems: An Introduction* by *Dietmar Jannach*
    and others. This is published by *Cambridge University Press*.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the process of building and evaluating recommender
    systems in R using the `recommenderlab` package. We focused primarily on the paradigm
    of collaborative filtering, which in a nutshell formalizes the idea of recommending
    items to users through word-of-mouth. As a general rule, we found that user-based
    collaborative filtering performs quite quickly but requires all the data to make
    predictions. Item-based collaborative filtering can be slow to train a model but
    makes predictions very quickly once the model is trained. It is useful in practice
    because it does not require us to store all the data. In some scenarios, the trade-off
    in accuracy between these two can be high, but in others the difference is acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: The process of training recommendation systems is quite resource-intensive and
    a number of important parameters come into play in the design, such as the metrics
    used to quantify similarity and distance between items and users. Finally, we
    touched upon alternatives to the collaborative filtering paradigm. Content-based
    recommendation systems are designed to leverage similarity between items on the
    basis of their content. As such, they are ideally suited to the domain of text.
    Knowledge-based recommendation systems are designed to make recommendations to
    users on the basis of a set of rules or constraints that have been designed by
    experts. These can be combined with the other approaches in order to address the
    cold-start problem for new users or items.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover the idea of applying the techniques and practices
    already covered in this book to very large data sources and point out the specific
    challenges encountered when working with big data.
  prefs: []
  type: TYPE_NORMAL
