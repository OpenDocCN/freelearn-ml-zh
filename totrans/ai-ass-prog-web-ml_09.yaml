- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Augment Web Apps with AI Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several ways that a web app can be augmented with AI services: you
    could leverage an existing Web API exposing a model, or build it yourself and
    have it call a model.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason you would want to add AI to your app in the first place is to make
    it smarter. Not smarter for its own sake, but to make it more useful to the user.
    For example, if you have a web app that allows users to search for products, you
    could add a feature that suggests products based on the user’s previous purchases.
    In fact, why limit yourself to previous purchases? Why not suggest products based
    on the user’s previous searches? Or, what if the user could take a picture of
    a product and the app would suggest similar products?
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are a lot of possibilities for augmenting your web app
    with AI that would improve the user experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Discuss different model formats like Pickle and ONNX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to use both Pickle and ONNX to persist your model as a file using
    Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consume a model stored in ONNX format and expose it via a REST API using JavaScript
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business domain, e-commerce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We keep working on our e-commerce domain, but our business focus is on ratings.
    A good or bad rating can influence how many units are sold of a specific product.
    The logical domain consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Products**: the products to be rated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ratings**: the actual ratings and meta information like comments, dates and
    more'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem and data domain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The problem to figure out is how we use this rating data and learn from it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Insights**: We could, for example, get the insights that we should start/stop
    selling a certain product. There might be other insights as certain products sell
    well in certain parts of the world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Technical** **problem**: The technical aspect of this is figuring out how
    to ingest the data, train a model from it, and then figure out how a web application
    can leverage said model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature breakdown
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at this from a feature standpoint, we need to see this as consisting
    of three major parts.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data ingestion and training**: this needs a separate interface, maybe it’s
    done without a user interface and it’s just static data being fed into code capable
    of training a model from it. With that understanding, we can outline the steps
    like so:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Clean data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Create features
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Train model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Run predictions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consuming the model**: Once the model is trained, it needs to be exposed,
    preferably through a web endpoint. To get there, we think we need these set of
    steps:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert the model to suitable format if needed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a Web API
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Expose model through Web API
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy model, there’s a step here where we need to bring the API online
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction**: For the prediction part, this is a functionality that’s meant
    for “back office” and not customer facing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build user interface to run predictions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Build underlying code that talks to the Web API to make predictions possible
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You could go with either prompt approach here, either get suggestions from a
    prompt comment or use the chat interface.
  prefs: []
  type: TYPE_NORMAL
- en: For prompt pattern, we’ll use the “Exploratory pattern” as described in *Chapter
    2*.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine we are dealing with the following data in the `sales_rating.csv` file
    that is the result of merging two datasets, one containing sales data and the
    other containing rating data. The data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding data shows a dataset with `product_id`, `avg_rating`, and `sold`
    columns. Our theory is that the average rating of a product is correlated with
    the number of sales. It seems like a fair assumption that a product with a high
    rating will sell more than a product with a low rating. By creating a model, we
    can come closer to determining whether it’s likely that our assumption is true
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: Coming up with a plan
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can ask Copilot about the steps we need to take to create a model. We type
    these prompts as comments:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to ask Copilot to come up with text answers about something else
    other than code, you can type the text as a comment in a code file or inside of
    a Markdown file. It also helps to place yourself on the next row after the comment
    and start the row with “a:” for answer. You can also ask questions using Copilot
    Chat:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Tell me the answer of life, the universe and everything.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: The above reference is from the book *Hitchhikers guide to the Galaxy* by Douglas
    Adams.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s ask Copilot to help us with the steps to create a model.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: What are the steps to create a model in Python given a CSV file?
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: import pandas and scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: read the CSV file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: create a test and training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: create a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how good is the model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: predict
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: save the model to pkl
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OK, we have a plan, so let’s execute it and use Copilot to help us with the
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Importing libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let’s start by creating a new notebook in VS Code. Let’s call it `sales.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: '`.ipynb` is the file format for Jupyter Notebook that you as a data science
    practitioner are most likely familiar with. The idea is that you work on a file
    with cells, where some cells are text cells and others are code cells you can
    run.'
  prefs: []
  type: TYPE_NORMAL
- en: To use certain libraries, we need to install them. We can use Copilot to help
    us with that. We can leverage the fact that we are data science practitioners
    and therefore have an understanding of what libraries we need. Therefore, we will
    suggest the pandas and scikit-learn libraries below to help us with our task.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Install pandas and scikit-learn in a notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s import the libraries we need.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Import pandas and scikit-learn in a notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Reading the CSV file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the libraries we need, let’s read the CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Read a CSV file in a notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: The CSV file is now loaded into a pandas DataFrame, a structure holding columns
    and rows. Let’s take a look at the first few rows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Great, we have the data loaded into a DataFrame. Next, we need to split the
    data into test and training datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Creating test and training datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning, we typically split the data into test and training datasets.
    The training dataset is used to train the model and the test dataset is used to
    test the model. We can use Copilot to help us with this.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Create test and training dataset in a notebook, suggested test size of 0.2.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 0.2 is a good test size. It means that 20% of the data will be used for testing
    and 80% will be used for training.
  prefs: []
  type: TYPE_NORMAL
- en: You may need to start typing “test_size” and “training” on the respective rows,
    then press *TAB* to accept the suggestion.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have things set up, let’s create a model next.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating a model involves selecting the algorithm we want to use and then training
    the model. Let’s suggest a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Create a model in a notebook using the `LinearRegression` algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we created a model using the `LinearRegression` algorithm
    and then trained the model using the training dataset with the `fit` method. Also,
    note how we are using the `avg_rating` column as the input and the `sold` column
    as the output. This is because we want to predict the number of sales based on
    the average rating.
  prefs: []
  type: TYPE_NORMAL
- en: How good is the model?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, we want to find out how good the model is. To find this out,
    let’s ask Copilot to help us.
  prefs: []
  type: TYPE_NORMAL
- en: You can either use a comment style and get suggestions inline, or use Copilot
    Chat. In both cases, Copilot will understand from the context.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: How good is the model in a notebook? Show the code.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'It seems that calling `score` on the model and passing in the `test` dataset
    is how we find out how good a model is. We can even ask Copilot about this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: What does score do?
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want, you keep querying and have things explained in more detail, like
    with the below prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: What does that mean in simpler terms?
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Predict
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We won’t know how good the model is until we try to predict something, even
    if we could refer to the score value. Let’s craft a prompt for this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Predict in a notebook. Show the code.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: OK, we got a variable called `predictions` that contains the predictions, but
    it is hard to understand if the predictions are reasonable. Let’s try asking Copilot
    and see what it suggests.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: How do you suggest we show the predictions in a notebook?
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Any other ways you suggest we show the predictions in a notebook except for
    printing?
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve gotten more information on our various options, let’s use that
    knowledge in a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Show me how to print and plot predictions vs the actual data in a notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this code in a notebook will produce the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![plot](img/B21232_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Plot showing predictions vs actual data'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the plot, it looks like the line is a good fit for the data. We can also
    print predictions to see the actual values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Saving the model to a .pkl file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have one step left: we need to save the model to a file. We have a few different
    choices for how to save the model. We can save it as a pickle file or an ONNX
    file, for example. Let’s start with saving it as a pickle file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, let’s craft a prompt for this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Save the model to a pkl in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: For the completion of the above, you may need to start typing “import” to get
    the suggestion. Additionally, it’s a good bet it’s going to use the pickle library,
    so on the next line you can start typing “pickle” and press *TAB* to accept the
    suggestion.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a REST API in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, we have a `model.pkl` file that contains our model. We can expose
    the model using this file via a REST API. Let’s ask Copilot to help us with this.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Create a REST API in Python that exposes the model in a pkl file.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Note how we use several prompts above. First, we set the context by adding the
    prompt “Create a REST API in Python that exposes the model in a pkl file” at the
    top of the file. Then we use various helper prompts like “load the model”, “app”,
    and “routes” to help us with the code. We end up with an API that exposes the
    model via a REST API.
  prefs: []
  type: TYPE_NORMAL
- en: Converting the model to ONNX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, you’ve seen how we can use Pickle to save a model and then load it in
    Python. However, Pickle has drawbacks, including being Python-specific. ONNX,
    on the other hand, is a format that is not Python-specific and can be used in
    other languages. Let’s see how we can use the ONNX format to work with our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to solve our task in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the model to ONNX format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load the model and create a REST API in JavaScript.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a model in ONNX format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because we’re using a notebook, we need to install the dependencies, import
    the libraries we need, and then convert the model to ONNX format.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: What dependencies do I need for ONNX? Show me the Python code for installing
    those dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Show me how to import the libraries I need for ONNX and show me how to convert
    the model to ONNX format.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Now I should have a `model.onnx` file that contains the model in ONNX format.
    Let’s see if we can load it in JavaScript next.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the ONNX model in JavaScript
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we need to load the ONNX model in JavaScript and specifically Node.js,
    as we’re doing this on the backend. We can use the `onnxruntime` library to do
    this. Let’s ask Copilot to help us with this.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Show me a step-by-step guide to load the ONNX model in JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: install onnxruntime in JavaScript
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: load the ONNX model in JavaScript
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: predict using ONNX model in JavaScript
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a plan, let’s execute it.
  prefs: []
  type: TYPE_NORMAL
- en: Installing onnxruntime in JavaScript
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because we’re building a Node.js project, let’s first, in a new folder, create
    an `app.js` file and run the `npm init -y` terminal command to create a new Node.js
    project.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in our plan is to install `onnxruntime` in JavaScript. We can
    use npm to perform this task.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Install onnxruntime in JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Great, this installs the `onnxruntime` library in our project. Next, we need
    to load the ONNX model.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the ONNX model in JavaScript
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have `onnxruntime` installed, we can load the ONNX model.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Load the ONNX model in JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we load the model from the ONNX file and then we make
    predictions using the model with the input `4.5` to represent the average rating
    to see what sales we can expect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assignment: Build a REST API in JavaScript that consumes the model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Take the model we created in the previous section and add the code to the notebook
    to turn it into an ONNX file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new folder in the repo called and create a new file called `app.js`
    in that folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the POST /predict route to the `server.js` file and ensure it returns a
    prediction given the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s some starter prompts you can try to help you with this assignment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt**: Create a REST API in JavaScript using Express'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt**: Create a POST /predict route in a REST API in JavaScript using
    Express'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt**: Load the model from ONNX in a REST API in JavaScript using Express'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt**: Predict using the ONNX model in a REST API in JavaScript using
    Express'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: See repo [[https://github.com/PacktPublishing/AI-Assisted-Software-Development-with-GitHub-Copilot-and-ChatGPT/tree/main/09](https://github.com/PacktPublishing/AI-Assisted-Software-Development-with-GitHub-Copilot-and-ChatGPT/tree/main/09)]
    and the *09* folder for the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Quiz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What’s the difference between Pickle and ONNX?
  prefs: []
  type: TYPE_NORMAL
- en: Pickle is Python-specific and ONNX is not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pickle can be used in JavaScript and ONNX can’t.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ONNX is less efficient than Pickle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered various model formats like Pickle and ONNX and how
    to persist your model as a file using Python. Storing a model as a file is useful
    because it allows you to integrate it with other applications.
  prefs: []
  type: TYPE_NORMAL
- en: Then we discussed the pros and cons of different formats for storing models
    like Pickle and ONNX. We came to the conclusion that ONNX is probably the better
    choice because it’s not Python-specific and can be used in other languages.
  prefs: []
  type: TYPE_NORMAL
- en: Then we covered how to load a model stored in ONNX format using JavaScript and
    create a REST API to make the model available to other applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll go into more detail of how we can use GitHub Copilot
    and get the most out of it. We’ll cover both tips and tricks and features that
    help make you faster and more productive.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/aicode](https://packt.link/aicode)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code510410532445718281.png)'
  prefs: []
  type: TYPE_IMG
