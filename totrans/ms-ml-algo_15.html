<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Advanced Policy Estimation Algorithms</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will continue our exploration of the world of <strong>Reinforcement Learning</strong> (<strong>RL</strong>), focusing our attention on complex algorithms that can be employed to solve difficult problems. As this is still the introductory part of RL (the whole topic is extremely large), the structure of the chapter is based on many practical examples that can be used as a basis to work on more complex scenarios.</p>
<p>The topics that will be discussed in this chapter are:</p>
<ul>
<li><span>TD</span>(λ) algorithm</li>
<li>Action-Critic TD(0)</li>
<li>SARSA</li>
<li>Q-learning</li>
<li>Q-learning with a simple visual input and a neural network</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TD(λ) algorithm</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we introduced the temporal difference strategy, and we discussed a simple example called TD(0). In the case of TD(0), the discounted reward is approximated by using a one-step backup. Hence, if the agent performs an action <em>a<sub>t</sub></em> in the state <em>s<sub>t</sub></em>, and the transition to the state <em>s<sub>t</sub><sub>+1</sub></em> is observed, the approximation becomes the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/aa44f3c3-5483-4558-a869-23c89e357ce9.png" style="width:11.92em;height:1.58em;"/></div>
<p class="mce-root">If the task is episodic (as in many real-life scenarios) and has <em>T(e<sub>i</sub>)</em> steps, the complete backup for the episode <em>e<sub>i</sub></em> is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/99c8410c-cb8c-4dd6-8873-2a1648151699.png" style="width:38.58em;height:2.00em;"/></div>
<p>The previous expression ends when the MDP process reaches an absorbing state; therefore, <em>R<sub>t</sub></em> is the actual value of the discounted reward. The difference between TD(0) and this choice is clear: in the first case, we can update the value function after each transition, whereas with a complete backup, we need to wait for the end of the episode. We can say that this method (which is called Monte Carlo, because it's based on the idea of averaging the overall reward of an entire sequence) is exactly the opposite of TD(0); therefore, it's reasonable to think about an intermediate solution, based on <em>k</em>-step backups. In particular, our goal is to find an online algorithm that can exploit the backups once they are available.</p>
<p>Let's imagine a sequence of four steps. The agent is in the first state and observes a transition; at this point, only a one-step backup is possible, and it's a good idea to update the value function in order to improve the convergence speed. After the second transition, the agent can use a two-step backup; however, it can also consider the first one-step backup in addition to the newer, longer one. So, we have two approximations:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b661dfb8-7ffc-4f7f-a4fc-45b5503b9c5b.png" style="width:18.75em;height:3.58em;"/></div>
<p>Which of the preceding is the most reliable? Obviously, the second one depends on the first one (in particular, when the value function is almost stabilized), and so on until the end of the episode. Hence, the most common strategy is to employ a weighted average that assigns a different level of importance to each backup (assuming the longest backup has <em>k</em> steps):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/48597f36-c27f-4ee8-919c-dd9aa6a04017.png" style="width:26.17em;height:2.92em;"/></div>
<p>Watkins (in <em>Learning from Delayed Rewards</em>, <em>Watkins C.I.C.H.</em><span>, </span><em>Ph.D. Thesis</em>, <em>University of Cambridge</em>, <em>1989</em>) proved that this approach (with or without averaging) has the fundamental property of reducing the absolute error of the expected <em>R<sub>t</sub><sup>k</sup></em>, with respect to the optimal value function, <em>V(s; π)</em>. In fact, he proved that the following inequality holds:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a0fda46a-9ff2-4faf-a08b-b5303e14a459.png" style="width:32.67em;height:1.92em;"/></div>
<p>As <em>γ</em> is bounded between 0 and 1, the right-hand side is always smaller than the maximum absolute error <em>V(t) - V(s;π)</em>, where <em>V(s)</em> is the value of a state during an episode. Therefore, the expected discounted return of a <em>k</em>-step backup (or of a combination of different backups) yields a more accurate estimation of the optimal value function if the policy is chosen to be greedy with respect to it. This is not surprising, as a longer backup incorporates more actual returns, but the importance of this theorem resides in its validity when an average of different <em>k</em>-step backups are employed. In other words, it provides us with the mathematical proof that an intuitive approach actually converges, and it can also effectively improve both the convergence speed and the final accuracy.</p>
<p class="mce-root">However, managing <em>k</em> coefficients is generally problematic, and in many cases, useless. The main idea behind TD(λ) is to employ a single factor, <span><em>λ</em>, that can be tuned in order to meet specific requirements. The theoretical analysis (or <em>forward view</em>, as referred to by Sutton and Barto) is based, in a general case, on an exponentially decaying average. If we consider a geometric series with <em>λ</em> bounded between 0 and 1 (exclusive), we get:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4e979110-e2a8-4991-8e52-26b61805f81b.png" style="width:31.67em;height:3.58em;"/></div>
<p>Hence, we can consider the averaged discounted return <em>R<sub>t</sub><sup>(λ)</sup></em> with infinite backups as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fae3add0-ff56-49ca-9f3c-1549039fb66f.png" style="width:26.08em;height:3.83em;"/></div>
<p>Before defining the finite case, it's helpful to understand how <em><span>R</span><sub>t</sub><sup>(λ)</sup></em> was built. As <em>λ</em> is bounded between 0 and 1, the factors decay proportionally to <span><em>λ</em>, so the first backup has the highest impact, and all of the subsequent ones have smaller and smaller influences on the estimation. This means that, in general, we are assuming that the estimation of <em>R<sub>t</sub></em> has more importance to the <em>immediate</em> backups (which become more and more precise), and we exploit the longer ones only to improve the estimated value. Now, it should be clear that <em>λ</em> = 0 is equivalent to TD(0), because only the one-step backup remains in the sum (remember that <em>0<sup>0</sup> = 1</em>), while higher values involve all of the remaining backups. </span><span>Let's now consider an episode <em>e<sub>i</sub></em> whose length is <em>T(e<sub>i</sub>)</em>.</span></p>
<p><span>Conventionally, if the agent reached an absorbing state at <em>t = T(e<sub>i</sub>)</em>, all of the remaining <em>t</em>+<em>i</em> returns are equal to <em>R</em><sub><em>t</em> </sub>(this is straightforward, as all of the possible rewards have already been collected); therefore, we can truncate <em>R<sub>t</sub><sup>(λ)</sup></em>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fe22f4da-626c-4928-ab5c-6d920feedfba.png" style="width:26.33em;height:4.50em;"/></div>
<p class="mce-root">The first term of the previous expression involves all of the non-terminal states, while the second is equal to <em>R<sub>t</sub></em> discounted proportionally to the distance between the first time step and the final state. Again, if <span><em>λ</em> = 0, we obtain TD(0), but we are now also authorized to consider <em>λ</em> = <em>1</em> (because the sum is always extended to a finite number of elements). When λ = 1, we obtain <em>R<sub>t</sub><sup>(λ)</sup></em> = <em>R<sub>t</sub></em>, which means that we need to wait until the end of the episode to get the actual discounted reward. As explained previously, this method is normally not a first-choice solution, because when the episodes are very long, the agent selects the actions with a value function that is not up to date in the majority of cases. Therefore, <em>TD(λ)</em> is normally employed with <em>λ</em> values less than 1, in order to obtain the advantage of an online update, together with a correction based on the new states. To achieve this goal without looking at the future (we want to update <em>V(s)</em> as soon as new pieces of information are available), we need to introduce the concept of <em>eligibility trace</em><strong> </strong><em>e(s)</em> (sometimes, in the context of computational neuroscience, <em>e(s)</em> is also called <em>stimulus trace</em>).</span></p>
<p class="mce-root"><span>An eligibility trace for a state <em>s</em> is a function of time that returns the weight (greater than 0) of the specific state. Let's imagine a sequence, <em>s<sub>1</sub>, s<sub>2</sub>, ..., s<sub>n</sub></em>, and consider a state, <em>s<sub>i</sub></em>. After a backup <em>V(s<sub>i</sub>)</em> is updated, the agent continues its exploration. When is a new update of <em>s<sub>i</sub></em> (given longer backups) important? If <em>s<sub>i</sub></em> is not visited anymore, the effect of longer backups must be smaller and smaller, and <em>s<sub>i</sub></em> is said to not be eligible for changes in <em>V(s)</em>. This is a consequence of the previous assumption that shorter backups must generally have higher importance. So, if <em>s<sub>i</sub></em> is an initial state (or is immediately after the initial state) and the agent moves to other states, the effect of <em>s<sub>i</sub></em> must decay. Conversely, if <em>s<sub>i</sub></em> is revisited, it means that the previous estimation of <em>V(s<sub>i</sub>)</em> is probably wrong, and hence <em>s<sub>i</sub></em> is eligible for a change. (To better understand this concept, imagine a sequence, <em>s<sub>1</sub>, s<sub>2</sub>, s<sub>1</sub>, ...</em>. It's clear that when the agent is in <em>s<sub>1</sub></em>, as well as in <em>s<sub>2</sub></em>, it cannot select the right action; therefore, it's necessary to reevaluate <em>V(s)</em> until the agent is able to move forward.)</span></p>
<p class="mce-root"><span>The most common strategy (which is also discussed in <em>Reinforcement Learning</em>, <em>Sutton R. S.</em>,<em>‎ Barto A. G.</em>, <em>The MIT Press</em>) is to define the eligibility traces in a recursive fashion. After each time step, <em>e<sub>t</sub>(s)</em> decays by a factor equal to <em>γλ</em> (to meet the requirement imposed by the forward view); but, when the state <em>s</em> is revisited, <em>e<sub>t</sub>(s)</em> is also increased by 1 (<em>e<sub>t</sub>(s)</em> =<em> γλe<sub>t-1</sub>(s)</em> + <em>1</em>). In this way, we impose a jump in the trend of <em>e(s)</em> whenever we desire to emphasize its impact. However, as <em>e(s)</em> decays independently of the jumps, the states that are visited and revisited later have a lower impact than the ones that are revisited very soon. The reason for this choice is very intuitive: the importance of a state revisited after a long sequence is clearly lower than the importance of a state that is revisited after a few steps. In fact, the estimation of <em>R<sub>t</sub></em> is obviously wrong if the agent moves back and forth between two states at the beginning of the episode, but the error becomes less significant when the agent revisits a state after having explored other areas. For example, a policy can allow an initial phase in order to reach a partial goal, and then it can force the agent to move back to reach a terminal state.</span></p>
<p class="mce-root"><span>Exploiting the eligibility traces, <em>TD(λ)</em> can achieve a very fast convergence in more complex environments, with a trade-off between a one-step TD method and a Monte Carlo one (which is normally avoided). At this point, the reader might wonder if we are sure about the convergence, and luckily, the answer is positive. Dayan proved (in<em> The convergence of TD (λ) for General λ</em>, <em>Dayan P.</em>, <em>Machine Learning 8</em>, <em>3–4/1992</em>) that <em>TD(λ)</em> converges for a generic <em>λ</em> with only a few specific assumptions and the fundamental condition that the policy is GLIE. The proof is very technical, and it's beyond the scope of this book; however, the most important assumptions (which are generally met) are:</span></p>
<ul>
<li>The <strong>Markov Decision Process</strong> (<strong>MDP</strong>) has absorbing states (in other words, all of the episodes end in a finite number of steps).</li>
<li>All of the transition probabilities are not-null (all states can be visited an infinite number of times).</li>
</ul>
<p>The first condition is obvious, the absence of absorbing states yields infinite explorations, which are not compatible with a TD method (sometimes it's possible to prematurely end an episode, but this can either be unacceptable (in some contexts) or a sub-optimal choice (in many others)). Moreover, Sutton and Barto (in the aforementioned book) proved that TD(λ) is equivalent to employing the weighted average of discounted return approximations, but without the constraint of looking ahead in the future (which is clearly impossible).</p>
<p>The complete<span> </span>TD(λ)<span> </span>algorithm (with an optional forced termination of the episode) is:</p>
<ol>
<li>Set an initial deterministic random policy, <em>π(s)</em></li>
<li>Set the initial value array, <em>V(s) = 0 ∀ s ∈ S</em></li>
</ol>
<ol start="3">
<li>Set the initial eligibility trace array, <em>e(s) = 0 </em><span><em>∀ s ∈ S</em></span></li>
<li>Set the number of episodes, <em>N</em><sub><em>episodes</em></sub></li>
<li>Set a maximum number of steps per episode, <em>N</em><sub><em>max</em></sub></li>
<li>Set a constant, α (<span>α = <em>0.1</em>)</span></li>
<li><span>Set a constant, γ (γ</span><span> = <em>0.9</em>)</span></li>
<li><span>Set a constant, λ (λ</span><span> = <em>0.5</em>)</span></li>
<li>Set a counter, <em>e</em> = <em>0</em></li>
<li>For <em>i</em> = <em>1</em> to <em>N<sub>episodes</sub></em>:
<ol>
<li>Create an empty state list, <em>L</em></li>
<li>Observe the initial state, <em>s<sub>i</sub></em>, and append <em>s<sub>i</sub></em> to <em>L</em></li>
<li>While <em>s<sub>j</sub></em><span> </span>is non-terminal and <em>e &lt; N<sub>max:</sub></em>
<ol>
<li><em>e += 1</em></li>
<li>Select the action, <em>a<sub>t</sub><span> </span>= </em><span><em>π(s<sub>i</sub>)</em></span></li>
<li>Observe the transition, <em>(a<sub>t</sub>, s<sub>i</sub>) → (s<sub>j</sub>, r<sub>ij</sub>)</em></li>
<li>Compute the TD error as <em>TD<sub>error</sub> = r<sub>ij</sub> + γV(s<sub>j</sub>) - V(s<sub>i</sub>)</em></li>
<li>Increment the eligibility trace, <em>e(s<sub>i</sub>) += 1.0</em></li>
<li>For <em>s</em> in <em>L</em>:
<ol>
<li>Update the value, <em>V(s) += α · TD<sub>error</sub> · e(s)</em></li>
<li>Update the eligibility trace, <em>e(s) *= γλ</em></li>
</ol>
</li>
<li>Set <em>s<sub>i</sub></em><span> </span>= <em>s</em><sub><em>j</em></sub></li>
<li><span>Append </span><em>s<sub>j</sub></em><span> to <em>L</em></span></li>
</ol>
</li>
<li>Update the policy to be greedy with respect to the value function, <span><em>π(s) = argmax<sub>a</sub> Q(s, a)</em></span></li>
</ol>
</li>
</ol>
<p>The reader can better understand the logic of this algorithm by considering the TD error and its back-propagation. Even if this is only a comparison, it's possible to imagine the behavior of TD(λ) as similar to the <strong>Stochastic Gradient Descent</strong> (<strong>SGD</strong>) algorithms employed to train a neural network. In fact, the error is propagated to the previous states (analogous to the lower layers of an MLP) and affects them proportionally to their importance, which is defined by their eligibility traces. Hence, a state with a higher eligibility trace can be considered more responsible for the error; therefore, the corresponding value must be corrected proportionally. This isn't a formal explanation, but it can simplify comprehension of the dynamics without an excessive loss of rigor.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TD(λ) in a more complex Checkerboard environment</h1>
                </header>
            
            <article>
                
<p>At this point, we want to test the TD(λ) algorithm with a slightly more complex tunnel environment. In fact, together with the absorbing states, we will also consider some intermediate positive states, which can be imagined as <em>checkpoints</em>. An agent should learn the optimal path from any cell to the final state, trying to pass through the highest number of checkpoints possible. Let's start by defining the new structure:</p>
<pre>import numpy as np<br/><br/>width = 15<br/>height = 5<br/><br/>y_final = width - 1<br/>x_final = height - 1<br/><br/>y_wells = [0, 1, 3, 5, 5, 6, 7, 9, 10, 11, 12, 14]<br/>x_wells = [3, 1, 2, 0, 4, 3, 1, 3, 1, 2, 4, 1]<br/><br/>y_prizes = [0, 3, 4, 6, 7, 8, 9, 12]<br/>x_prizes = [2, 4, 3, 2, 1, 4, 0, 2]<br/><br/>standard_reward = -0.1<br/>tunnel_rewards = np.ones(shape=(height, width)) * standard_reward<br/><br/>def init_tunnel_rewards():<br/>    for x_well, y_well in zip(x_wells, y_wells):<br/>        tunnel_rewards[x_well, y_well] = -5.0<br/><br/>    for x_prize, y_prize in zip(x_prizes, y_prizes):<br/>        tunnel_rewards[x_prize, y_prize] = 1.0<br/><br/>    tunnel_rewards[x_final, y_final] = 5.0<br/><br/>init_tunnel_rewards()</pre>
<p>The reward structure is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/45b37fed-90f7-4956-8996-7738cbbfb0a5.png" style="width:72.75em;height:28.33em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Reward schema in the new tunnel environment</div>
<p>At this point, we can proceed to initialize all of the constants (in particular, we have chosen <em>λ = 0.6</em>, which is an intermediate solution that guarantees an awareness close to a Monte Carlo method, without compromising the learning speed):</p>
<pre>import numpy as np<br/><br/>nb_actions = 4<br/>max_steps = 1000<br/>alpha = 0.25<br/>lambd = 0.6<br/>gamma = 0.95<br/><br/>tunnel_values = np.zeros(shape=(height, width))<br/>eligibility_traces = np.zeros(shape=(height, width))<br/>policy = np.random.randint(0, nb_actions, size=(height, width)).astype(np.uint8)</pre>
<div class="packt_infobox">As in Python, the keyword <kbd>lambda</kbd> is reserved; we used the truncated expression <kbd>lambd</kbd> to declare the constant.</div>
<p>As we want to start from a random cell, we need to repeat the same procedure presented in the previous chapter; but, in this case, we are also including the checkpoint states:</p>
<pre>import numpy as np<br/><br/>xy_grid = np.meshgrid(np.arange(0, height), np.arange(0, width), sparse=False)<br/>xy_grid = np.array(xy_grid).T.reshape(-1, 2)<br/><br/>xy_final = list(zip(x_wells, y_wells)) + list(zip(x_prizes, y_prizes))<br/>xy_final.append([x_final, y_final])<br/><br/>xy_start = []<br/><br/>for x, y in xy_grid:<br/>    if (x, y) not in xy_final:<br/>        xy_start.append([x, y])<br/>        <br/>xy_start = np.array(xy_start)<br/><br/>def starting_point():<br/>    xy = np.squeeze(xy_start[np.random.randint(0, xy_start.shape[0], size=1)])<br/>    return xy[0], xy[1]</pre>
<p>We can now define the <kbd>episode()</kbd> function, which implements a complete TD(λ) cycle. As we don't want the agent to roam around trying to pass through the checkpoints an infinite number of times, we have decided to reduce the reward during the exploration, to incentivize the agent to pass through only the necessary checkpoints—trying, at the same time, to reach the final state as soon as possible:</p>
<pre>import numpy as np<br/><br/>def is_final(x, y):<br/>    if (x, y) in zip(x_wells, y_wells) or (x, y) == (x_final, y_final):<br/>        return True<br/>    return False<br/><br/>def episode():<br/>    (i, j) = starting_point()<br/>    x = y = 0<br/>    <br/>    e = 0<br/>    <br/>    state_history = [(i, j)]<br/>    <br/>    init_tunnel_rewards()<br/>    total_reward = 0.0<br/>    <br/>    while e &lt; max_steps:<br/>        e += 1<br/>        <br/>        action = policy[i, j]<br/>            <br/>        if action == 0:<br/>            if i == 0:<br/>                x = 0<br/>            else:<br/>                x = i - 1<br/>            y = j<br/>                <br/>        elif action == 1:<br/>            if j == width - 1:<br/>                y = width - 1<br/>            else:<br/>                y = j + 1<br/>            x = i<br/>                <br/>        elif action == 2:<br/>            if i == height - 1:<br/>                x = height - 1<br/>            else:<br/>                x = i + 1<br/>            y = j<br/>                <br/>        else:<br/>            if j == 0:<br/>                y = 0<br/>            else:<br/>                y = j - 1<br/>            x = i<br/>                <br/>        reward = tunnel_rewards[x, y]<br/>        total_reward += reward<br/>        <br/>        td_error = reward + (gamma * tunnel_values[x, y]) - tunnel_values[i, j]<br/>        eligibility_traces[i, j] += 1.0<br/>        <br/>        for sx, sy in state_history:<br/>            tunnel_values[sx, sy] += (alpha * td_error * eligibility_traces[sx, sy])<br/>            eligibility_traces[sx, sy] *= (gamma * lambd)<br/>        <br/>        if is_final(x, y):<br/>            break<br/>        else:<br/>            i = x<br/>            j = y<br/>            <br/>            state_history.append([x, y])<br/>            <br/>            tunnel_rewards[x_prizes, y_prizes] *= 0.85<br/>            <br/>    return total_reward<br/><br/></pre>
<pre>def policy_selection():<br/>    for i in range(height):<br/>        for j in range(width):<br/>            if is_final(i, j):<br/>                continue<br/>            <br/>            values = np.zeros(shape=(nb_actions, ))<br/>            <br/>            values[0] = (tunnel_rewards[i - 1, j] + (gamma * tunnel_values[i - 1, j])) if i &gt; 0 else -np.inf<br/>            values[1] = (tunnel_rewards[i, j + 1] + (gamma * tunnel_values[i, j + 1])) if j &lt; width - 1 else -np.inf<br/>            values[2] = (tunnel_rewards[i + 1, j] + (gamma * tunnel_values[i + 1, j])) if i &lt; height - 1 else -np.inf<br/>            values[3] = (tunnel_rewards[i, j - 1] + (gamma * tunnel_values[i, j - 1])) if j &gt; 0 else -np.inf<br/>            <br/>            policy[i, j] = np.argmax(values).astype(np.uint8)</pre>
<p>The <kbd>is_final()</kbd> and <kbd>policy_selection()</kbd> <span>functions </span>are the same ones defined in the previous chapter, and need no explanation. Even if it's not really necessary, we have decided to implement a forced termination after a number of steps, equal to <kbd>max_steps</kbd>. This is helpful at the beginning because as the policy is not <em>ε</em>-greedy, the agent can remain stuck in a looping exploration that never ends. We can now train the model for a fixed number of episodes (alternatively, it's possible to stop the process when the value array doesn't change anymore):</p>
<pre>n_episodes = 5000<br/><br/>total_rewards = []<br/><br/>for _ in range(n_episodes): <br/>    e_reward = episode()<br/>    total_rewards.append(e_reward)<br/>    policy_selection()</pre>
<p>The <kbd>episode()</kbd> function returns the total rewards; therefore, it's useful to check how the agent learning process evolved:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/08409634-f125-445e-baef-8f952ac52c77.png" style="width:66.08em;height:36.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Total rewards achieved by the agent</div>
<p>At the beginning (for about 500 episodes), the agent employs an unacceptable policy that yields very negative total rewards. However, in about 1,000 iterations, the algorithm reaches an optimal policy that is only slightly improved by the following episodes. The oscillations are due to the different starting points; however, the total rewards are never negative, and as the checkpoint weights decay, this is a positive signal, indicating that the agent reaches the final positive state. To have a confirmation of this hypothesis, we can plot the learned value function:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/939757a7-49a4-4c84-8b7e-1d8b10fdeaf7.png" style="width:74.75em;height:28.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final value matrix</div>
<p>The values are coherent with our initial analysis; in fact, they tend to be higher when the cell is close to a checkpoint, but, at the same time, the global configuration (considering a policy greedy with respect to <em>V(s)</em>) forces the agent to reach the ending state whose surrounding values are the highest. The last step is checking the actual policy, with a particular focus on the checkpoints:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6f326296-76bc-4afa-9773-9b73d6ac581e.png" style="width:75.25em;height:29.25em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final policy</div>
<p>As it's possible to observe, the agent tries to pass through the checkpoints, but when it's close to the final state, it (correctly) prefers to end the episode as soon as possible. I invite the reader to repeat the experiment using different values for the constant <em>λ</em>, and changing the environment dynamics for the checkpoints. What happens if their values remain the same? Is it possible to improve the policy with a higher <em>λ</em>?</p>
<div class="packt_infobox">It's important to remember that, as we are extensively using random values, successive experiments can yield different results due to different initial conditions. However, the algorithm should always converge to an optimal policy when the number of episodes is high enough.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Actor-Critic TD(0) in the checkerboard environment</h1>
                </header>
            
            <article>
                
<p>In this example, we want to employ an alternative algorithm called <em>Actor-Critic</em>, together with TD(0). In this method, the agent is split into two components, a Critic, which is responsible for evaluating the quality of the value estimation, and an actor, which selects and performs an action. As pointed out by Dayan (in <em>Theoretical Neuroscience</em>, <em>Dayan P</em><span>., </span><em>Abbott L</em><span>. </span><em>F</em><span>., </span><em>The MIT Press</em>), the dynamics of an Actor-Critic approach are similar to the interleaving policy evaluation and policy improvement steps. In fact, the knowledge of the Critic is obtained through an iterative process, and its initial evaluations are normally sub-optimal.</p>
<p>The structural schema is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0ddd636c-72f0-4b0d-a647-c56bc60322dc.png" style="width:25.42em;height:19.75em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Actor-Critic schema</div>
<p class="mce-root">In this particular case, it's preferable to employ a <em>ε</em>-greedy soft policy, based on the softmax function. The model stores a matrix (or an approximating function) called <em>policy importance</em>, where each entry <em>p<sub>i</sub>(s, a)</em> is a value representing the preference for a specific action in a certain state. The actual stochastic policy is obtained by applying the softmax with a simple trick to increase the numerical stability when the exponentials become very large:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a2711708-7293-46b6-a984-5519dc987c4e.png" style="width:48.00em;height:4.17em;"/></div>
<p>After performing the action <em>a</em> in the state <em>s<sub>i</sub></em> and observing the transition to the state <em>s<sub>j</sub></em> with a reward <em>r<sub>ij</sub></em>, the Critic evaluates the TD error:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/412ea371-91ba-4bb1-9eff-9ed4517c265b.png" style="width:17.08em;height:1.50em;"/></div>
<p class="mce-root">If <em>V(s<sub>i</sub>) &lt; r<sub>ij</sub> + γV(s<sub>j</sub>)</em>, the transition is considered positive, because the value is increasing. Conversely, when <em><span>V(s</span><sub>i</sub><span>) &gt; r</span><sub>ij</sub><span> + γV(s</span><sub>j</sub></em><span><em>)</em>, the Critic evaluates the action as negative, because the previous value was higher than the new estimation. A more general approach is based on the concept of <em>advantage</em>, which is defined as:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/484d36a2-70f6-4ee6-931d-624bf0f71188.png" style="width:17.25em;height:1.50em;"/></div>
<p>Normally, one of the terms from the previous expression can be approximated. In our case, we cannot compute the <em>Q</em> function directly; hence, we approximate it with the term <em>r<sub>ij</sub> + γV(s<sub>j</sub>)</em>. It's clear that the role of the advantage is analogous to the one of the TD error (which is an approximation) and must represent the confirmation that an action in a certain state is a good or bad choice. An analysis of all <strong>advantage Actor-Critic</strong> (<span><strong>A3C</strong>)</span> algorithms (in other words, improvements of the standard <em>policy gradient</em> algorithm) is beyond the scope of this book. However, the reader can find some helpful pieces of information in <em>High-Dimensional Continuous Control Using Generalized Advantage Estimation</em>, <em>Schulman J</em><span>., </span><em>Moritz P</em><span>., </span><em>Levine S</em><span>., </span><em>Jordan M</em><span>. </span><em>I</em><span>., </span><em>Abbeel P</em><span>., </span><em>ICLR 2016.</em></p>
<p class="mce-root"><span>Of course, an Actor-Critic correction is not sufficient. To improve the policy, it's necessary to employ a standard algorithm (such as TD(0), TD(λ), or least square regression, which can be implemented using a neural network) in order to learn the correct value function, <em>V(s)</em>. As for many other algorithms, this process can converge only after a sufficiently high number of iterations, which must be exploited to visit the states many times, experimenting with all possible actions.</span></p>
<p class="mce-root"><span>Hence, with a TD(0) approach, the first step after evaluating the TD error is updating <em>V(s)</em> using the rule defined in the previous chapter:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c695ed0f-e214-4e0d-8e35-f3e2b1727564.png" style="width:31.75em;height:1.42em;"/></div>
<p class="mce-root"><span>The second step is more pragmatic; in fact, the main role of the Critic is actually to criticize every action, deciding when it's better to increase or decrease the probability of selecting it again in a certain state. This goal can be achieved by simply updating the policy importance:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/63fdf0cc-74d2-4e4e-b770-ccf7aa03529a.png" style="width:17.67em;height:1.58em;"/></div>
<p class="mce-root">The role of the learning rate <em>ρ</em> is extremely important; in fact, incorrect values (in other words, values that are too high) can yield initial wrong corrections that may compromise the convergence. It's essential to not forget that the value function is almost completely unknown at the beginning, and therefore the Critic has no chance to increase the right probability with awareness. For this reason, I always suggest to start with very small value (<em><span>ρ = </span>0.001</em>) and increase it only if the convergence speed of the algorithm is effectively improved.</p>
<p class="mce-root">As the policy is based on the softmax function, after a Critic update, the values will always be renormalized, resulting in an actual probability distribution. After an adequately large number of iterations, with the right choice of both <span><em>ρ</em> and <em>γ</em></span>, the model is able to learn both a stochastic policy and a value function. Therefore, it's possible to employ the trained agent by always selecting the action with the highest probability (which corresponds to an implicitly greedy behavior):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7046f44f-1702-4122-b3b1-3154667a5506.png" style="width:12.50em;height:1.50em;"/></div>
<p>Let's now apply this algorithm to the tunnel environment. The first step is defining the constants (as we are looking for a long sighted agent, we are setting the discount factor <em>γ = 0.99</em>):</p>
<pre>import numpy as np<br/><br/>tunnel_values = np.zeros(shape=(height, width))<br/><br/>gamma = 0.99<br/>alpha = 0.25<br/>rho = 0.001</pre>
<p>At this point, we need to define the policy importance array, and a function to generate the softmax policy:</p>
<pre>import numpy as np<br/><br/>nb_actions = 4<br/><br/>policy_importances = np.zeros(shape=(height, width, nb_actions))<br/><br/>def get_softmax_policy():<br/>    softmax_policy = policy_importances - np.amax(policy_importances, axis=2, keepdims=True)<br/>    return np.exp(softmax_policy) / np.sum(np.exp(softmax_policy), axis=2, keepdims=True)</pre>
<p>The functions needed to implement a single training step are very straightforward, and the reader should already be familiar with their structure:</p>
<pre>import numpy as np<br/><br/>def select_action(epsilon, i, j):<br/>    if np.random.uniform(0.0, 1.0) &lt; epsilon:<br/>        return np.random.randint(0, nb_actions)<br/>    <br/>    policy = get_softmax_policy()<br/>    return np.argmax(policy[i, j])<br/><br/>def action_critic_episode(epsilon):<br/>    (i, j) = starting_point()<br/>    x = y = 0<br/>    <br/>    e = 0<br/>    <br/>    while e &lt; max_steps:<br/>        e += 1<br/>        <br/>        action = select_action(epsilon, i, j)<br/>            <br/>        if action == 0:<br/>            if i == 0:<br/>                x = 0<br/>            else:<br/>                x = i - 1<br/>            y = j<br/>                <br/>        elif action == 1:<br/>            if j == width - 1:<br/>                y = width - 1<br/>            else:<br/>                y = j + 1<br/>            x = i<br/>                <br/>        elif action == 2:<br/>            if i == height - 1:<br/>                x = height - 1<br/>            else:<br/>                x = i + 1<br/>            y = j<br/>                <br/>        else:<br/>            if j == 0:<br/>                y = 0<br/>            else:<br/>                y = j - 1<br/>            x = i<br/>                <br/>        reward = tunnel_rewards[x, y]<br/>        td_error = reward + (gamma * tunnel_values[x, y]) - tunnel_values[i, j]<br/>        <br/>        tunnel_values[i, j] += (alpha * td_error)<br/>        policy_importances[i, j, action] += (rho * td_error)<br/>        <br/>        if is_final(x, y):<br/>            break<br/>        else:<br/>            i = x<br/>            j = y</pre>
<p>At this point, we can train the model with 50,000 iterations, and 30,000 explorative ones (with a linear decay of the exploration factor):</p>
<pre>n_episodes = 50000<br/>n_exploration = 30000<br/><br/>for t in range(n_episodes):<br/>    epsilon = 0.0<br/>    <br/>    if t &lt;= n_exploration:<br/>        epsilon = 1.0 - (float(t) / float(n_exploration))<br/>        <br/>    action_critic_episode(epsilon)</pre>
<p>The resulting greedy policy is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/7c061246-8480-4b13-ba41-f488adbcd9e2.png" style="width:74.92em;height:29.00em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"> Final greedy policy</div>
<p class="mce-root">The final greedy policy is consistent with the objective, and the agent always reaches the final positive state by avoiding the wells. This kind of algorithm can appear more complex than necessary; however, in complex situations, it turns out to be extremely effective. In fact, the learning process can be dramatically improved, thanks to the fast corrections performed by the Critic. Moreover, the author has noticed that the Actor-Critic is more robust to wrong (or noisy) evaluations. As the policy is learned separately, the effect of small variations in <em>V(s)</em> cannot easily change the probabilities <em>π(s, a)</em> (in particular, when an action is generally much <em>stronger</em> than the others). On the other hand, as discussed previously, it's necessary to avoid a premature convergence in order to let the algorithm modify the importance/probabilities, without an excessive number of iterations. The right trade-off can be found only after a complete analysis of each specific scenario, and unfortunately, there are no general rules that work in every case. My suggestion is to test various configurations, starting with small values (and, for example, a discount factor of <em>γ ∈ [0.7, 0.9]</em>), evaluating the total reward achieved after the same exploration period.</p>
<p class="mce-root">Complex deep learning models (such as asynchronous A3C; see <em>Asynchronous Methods for Deep Reinforcement Learning</em>, <em>Mnih V</em><span>., </span><em>Puigdomènech Badia A</em><span>., </span><em>Mirza M</em><span>., </span><em>Graves A</em><span>., </span><em>Lillicrap T</em><span>. </span><em>P</em><span>., </span><em>Harley T</em><span>., </span><em>Silver D</em><span>., </span><em>Kavukcuoglu K</em><span>.,</span> <em>arXiv:1602.01783 [cs.LG]</em> for further information) are based on a single network that outputs both the softmax policy (whose actions are generally proportional to their probability) and the value. Instead of employing an explicitly <em>ε</em>-greedy soft policy, it's possible to add a <em>maximum-entropy constraint</em> to the global cost function:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/21653b7e-8e87-40d4-9800-65b36bdbe1ba.png" style="width:40.67em;height:3.00em;"/></div>
<p>As the entropy is at the maximum when all of the actions have the same probability, this constraint (with an appropriate weight) forces the algorithm to increase the exploration probability until an action becomes dominant and there's no more need to avoid a greedy selection. This is a sound and easy way to employ an <em>adaptive ε-greedy policy</em>, because as the model works with each state separately, the states where the uncertainty is very low can become greedy; it's possible to automatically keep a high entropy whenever it's necessary to continue the exploration, in order to maximize the reward.</p>
<p class="mce-root">The effect of double correction, together with a maximum-entropy constraint, improves the convergence speed of the model, encourages the exploration during the initial iterations, and yields very high final accuracy. I invite the reader to implement this variant with other scenarios and algorithms. In particular, at the end of this chapter, we are going to experiment with an algorithm based on a neural network. As the example is pretty simple, I suggest using Tensorflow to create a small network based on the Actor-Critic approach. The reader can employ a <em>mean squared error</em> loss for the value and softmax cross entropy for the policy. Once the models work successfully with our toy examples, it will be possible to start working with more complex scenarios (like the ones proposed in OpenAI Gym at <a href="https://gym.openai.com/">https://gym.openai.com/</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SARSA algorithm</h1>
                </header>
            
            <article>
                
<p><strong>SARSA</strong> (whose name is derived from the sequence <em>state-action-reward-state-action</em>) is a natural extension of TD(0) to the estimation of the <em>Q</em> function. Its standard formulation (which is sometimes called one-step SARSA, or SARSA(0), for the same reasons explained in the previous chapter) is based on a single next reward, <em>r<sub>t+1</sub></em>, which is obtained by executing the action <em>a<sub>t</sub></em> in the state <em>s<sub>t</sub></em>. The temporal difference computation is based on the following update rule:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/539d7ddc-fad5-4e37-bd39-5d52684fa197.png" style="width:37.58em;height:1.50em;"/></div>
<p>The equation is equivalent to TD(0), and if the policy is chosen to be GLIE, it has been proven (in <em>Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms</em>, <em>Singh S<span>.</span></em><span>,</span><em><span> </span>Jaakkola T<span>.</span></em><span>,</span><em><span> </span>Littman M<span>. </span>L<span>.</span></em><span>,</span><em><span> </span>Szepesvári C<span>.</span></em><span>,</span><em><span> </span>Machine Learning</em>, <em>39/2000</em>) that SARSA converges to an optimal policy, <em>π<sup>opt</sup>(s)</em>, with the probability 1, when all couples (state, action) are experienced an infinite number of times. This means that if the policy is updated to be greedy with respect to the current value function induced by <em>Q</em>, it holds that:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2ba03afc-b4f4-4b09-8055-2e3910d0588a.png" style="width:21.25em;height:3.25em;"/></div>
<p>The same result is valid for the <em>Q</em> function. In particular, the most important conditions required by the proof are:</p>
<ul>
<li>The learning rate, <em>α ∈ [0, 1]</em>, with the constraints <em>Σ</em><span><em>α = ∞</em> and <em>Σα<sup>2</sup> &lt; ∞</em></span></li>
<li>The variance of the rewards must be finite</li>
</ul>
<p>The first condition is particularly important when <span><em>α</em> is a function of the state and the time step; however, in many cases, it is a constant bounded between 0 and 1, and hence, <em>Σα<sup>2</sup> = ∞</em>. A common way to solve this problem (above all when a large number of iterations are required) is to let the learning rate decay (in other words, exponentially) during the training process. Instead, to mitigate the effect of very large rewards, it's possible to clip them in a suitable range (<em>[-1, 1]</em>). In many cases, it's not necessary to employ these strategies, but in more complex scenarios, they can become crucial in order to ensure the convergence of the algorithm. </span>Moreover, as pointed out in the previous chapter, these kinds of algorithms need a long exploration phase before starting to stabilize the policy. The most common strategy is to employ a <em>ε</em>-greedy policy, with a temporal decay of the exploration factor. During the first iterations, the agent must explore without caring about the returns of the actions. In this way, it's possible to assess the actual values before the beginning of a final refining phase characterized by a purely greedy exploration, based on a more precise approximation of <em>V(s)</em>.</p>
<p>The complete<span> </span>SARSA(0)<span> </span>algorithm (with an optional forced termination of the episode) is:</p>
<ol>
<li>Set an initial deterministic random policy, <em>π(s)</em></li>
<li>Set the initial value array, <em>Q(s, a) = 0 ∀ s ∈ S</em> and <span><em>∀ a ∈ A</em></span></li>
<li>Set the number of episodes, <em>N</em><sub><em>episodes</em></sub></li>
<li>Set a maximum number of steps per episode, <em>N</em><sub><em>max</em></sub></li>
<li>Set a constant, <em>α</em> (<span><em>α = 0.1</em>)</span></li>
<li><span>Set a constant, <em>γ</em> (<em>γ</em></span><span><em> = 0.9</em>)</span></li>
<li>Set an initial exploration factor, <span><em>ε<sup>(0)</sup></em> (<em>ε<sup>(0)</sup> = 1.0</em>)</span></li>
<li>Define a policy to let the exploration factor <em>ε</em> decay <span>(linear or exponential)</span></li>
<li>Set a counter, <em>e = 0</em></li>
<li>For <em>i = 1</em> to <em>N<sub>episodes</sub></em>:<br/>
<ol>
<li>Observe the initial state,<span> </span><em>s</em><sub><em>i</em></sub></li>
<li>While <em>s<sub>j</sub></em><span> </span>is non-terminal and <em>e &lt; N<sub>max</sub></em>:
<ol>
<li><em>e += 1</em></li>
<li>Select the action, <em>a<sub>t</sub><span> </span>= </em><span><em>π(s<sub>i</sub>)</em>, with an exploration factor <em>ε</em><sup><em>(e)</em></sup></span></li>
<li>Observe the transition, <em>(a<sub>t</sub>, s<sub>i</sub>) → (s<sub>j</sub>, r<sub>ij</sub>)</em></li>
<li>Select the action, <em>a<sub>t+1</sub> = <span>π(s</span><sub>j</sub></em><span><em>)</em>, with an exploration factor <em>ε</em></span><sup><em>(e)</em></sup></li>
<li>Update the <em>Q(s<sub>t</sub>, a<sub>t</sub>)</em> function (if <em>s<sub>j</sub></em> is terminal, set <em><span>Q(s</span><sub>t+1</sub><span>, a</span><sub>t+1</sub></em><span><em>) = 0</em>)</span></li>
<li>Set <em>s<sub>i</sub><span> </span>= s</em><sub><em>j</em></sub></li>
</ol>
</li>
</ol>
</li>
</ol>
<div class="packt_infobox">The concept of eligibility trace can also be extended to SARSA (and other TD methods); however, that is beyond the scope of this book. A reader who is interested can find all of the algorithms (together with their mathematical formulations) in <em>Sutton R. S.,‎ Barto A. G., Reinforcement Learning, A Bradford Book</em>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SARSA in the checkerboard environment</h1>
                </header>
            
            <article>
                
<p>We can now test the SARSA algorithm in the original tunnel environment (all of the elements that are not redefined are the same as the previous chapter). The first step is defining the <em>Q(s, a)</em> array and the constants employed in the training process:</p>
<pre>import numpy as np<br/><br/>nb_actions = 4<br/><br/>Q = np.zeros(shape=(height, width, nb_actions))<br/><br/>x_start = 0<br/>y_start = 0<br/><br/>max_steps = 2000<br/>alpha = 0.25</pre>
<p>As we want to employ a <em>ε</em>-greedy policy, we can set the starting point to <kbd>(0, 0)</kbd>, forcing the agent to reach the positive final state. We can now define the functions needed to perform a training step:</p>
<pre>import numpy as np<br/><br/>def is_final(x, y):<br/>    if (x, y) in zip(x_wells, y_wells) or (x, y) == (x_final, y_final):<br/>        return True<br/>    return False<br/><br/>def select_action(epsilon, i, j):<br/>    if np.random.uniform(0.0, 1.0) &lt; epsilon:<br/>        return np.random.randint(0, nb_actions)<br/>    return np.argmax(Q[i, j])<br/><br/>def sarsa_step(epsilon):<br/>    e = 0<br/>    <br/>    i = x_start<br/>    j = y_start<br/>    <br/>    while e &lt; max_steps:<br/>        e += 1<br/>        <br/>        action = select_action(epsilon, i, j)<br/>    <br/>        if action == 0:<br/>            if i == 0:<br/>                x = 0<br/>            else:<br/>                x = i - 1<br/>            y = j<br/>                <br/>        elif action == 1:<br/>            if j == width - 1:<br/>                y = width - 1<br/>            else:<br/>                y = j + 1<br/>            x = i<br/>                <br/>        elif action == 2:<br/>            if i == height - 1:<br/>                x = height - 1<br/>            else:<br/>                x = i + 1<br/>            y = j<br/>                <br/>        else:<br/>            if j == 0:<br/>                y = 0<br/>            else:<br/>                y = j - 1<br/>            x = i<br/>                <br/>        action_n = select_action(epsilon, x, y)<br/>        reward = tunnel_rewards[x, y]<br/>        <br/>        if is_final(x, y):<br/>            Q[i, j, action] += alpha * (reward - Q[i, j, action])<br/>            break<br/>            <br/>        else:<br/>            Q[i, j, action] += alpha * (reward + (gamma * Q[x, y, action_n]) - Q[i, j, action])<br/>            <br/>            i = x<br/>            j = y</pre>
<p>The <kbd>select_action()</kbd> function has been designed to select a random action with the probability <em>ε</em>, and a greedy one with respect to <em>Q(s, a)</em>, with the probability <em>1 - </em><span><em>ε</em>. The <kbd>sarsa_step()</kbd> function is straightforward, and executes a complete episode updating the <em>Q(s, a)</em> (that's why this is an online algorithm). At this point, it's possible to train the model for 20,000 episodes and employ a linear decay for <em>ε</em> during the first 15,000 episodes (when t &gt; 15,000, <em>ε</em> is set equal to 0 in order to employ a purely greedy policy):</span></p>
<pre>n_episodes = 20000<br/>n_exploration = 15000<br/><br/>for t in range(n_episodes):<br/>    epsilon = 0.0<br/>    <br/>    if t &lt;= n_exploration:<br/>        epsilon = 1.0 - (float(t) / float(n_exploration))<br/>        <br/>    sarsa_step(epsilon)</pre>
<p>As usual, let's check the learned values (considering that the policy is greedy, we're going to plot <em>V(s)</em> <em>= max<sub>a</sub> Q(s, a)</em>):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1f28d7a1-70b6-4a99-973d-2e12cf37714f.png" style="width:74.50em;height:29.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final value matrix (as <em>V(s) = max<sub>a</sub> Q(s, a)</em>)</div>
<p>As expected, the Q function has been learned in a consistent way, and we can get a confirmation plotting the resulting policy:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7736ad03-4864-455a-b93d-832fb9048459.png" style="width:73.75em;height:28.58em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final policy</div>
<p>The policy is coherent with the initial objective, and the agent avoids all negative absorbing states, always trying to move towards the final positive state. However, some paths seem longer than expected. As an exercise, I invite the reader to retrain the model for a larger number of iterations, adjusting the exploration period. Moreover, <em>is it possible to improve the model by increasing (or decreasing) the discount factor γ?</em> Remember that <em>γ → 0</em> leads to a short-sighted agent, which is able to select actions only considering the immediate reward, while<em> </em><span><em>γ → 1</em> forces the agent to take into account a larger number of future rewards. This particular example is based on a long environment, because the agent always starts from <em>(0, 0)</em> and must reach the farthest point; therefore, all intermediate states have less importance, and it's helpful to look at the future to pick the optimal actions. Using random starts can surely improve the policy for all initial states, but it's interesting to investigate how different <em>γ</em> values can affect the decisions; hence, I suggest repeating the experiment in order to evaluate the various configurations and increase awareness about the different factors that are involved in a TD algorithm.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Q-learning</h1>
                </header>
            
            <article>
                
<p>This algorithm was proposed by Watkins (in <em>Learning from delayed rewards</em>, <em>Watkins C.I.C.H.</em>,<em><span> </span>Ph.D. Thesis</em>, <em>University of Cambridge</em>, <em>1989</em>; and further analyzed in <em>Watkins C.I.C.H.</em>, <em>Dayan P.</em>, <em>Technical Note Q-Learning</em>, <em>Machine Learning 8</em>, <em>1992</em>) as a more efficient alternative to SARSA. The main feature of <em>Q-learning</em> is that the TD update rule is immediately greedy with respect to the <em>Q(s<sub>t+1</sub>, a)</em> function:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d568224d-83a3-42bf-85c1-5b5ae44789d1.png" style="width:37.17em;height:1.42em;"/></div>
<p class="mce-root">The key idea is to compare the current <em>Q(s<sub>t</sub>, a<sub>t</sub>)</em> value with the maximum <em>Q</em> value achievable when the agent is in the successor state. In fact, as the policy must be GLIE, the convergence speed can be increased by avoiding wrong estimations due to the selection of a <em>Q</em> value that won't be associated with the final action. By choosing the maximum <em>Q</em> value, the algorithm will move towards the optimal solution faster than SARSA, and also, the convergence proof is less restrictive. In fact, Watkins and Dayan (in the aforementioned papers) proved that, if <em>|r<sub>i</sub>| &lt; R</em>, the learning rate <em>α ∈ [0, 1[</em> (in this case, <span>α must be always smaller than 1) with the same constraints imposed for SARSA (<em>Σα = ∞</em> and <em>Σα<sup>2</sup> &lt; ∞</em>), then the estimated <em>Q</em> function converges with probability 1 to the optimal one:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/825eda8d-d7a9-4362-815d-f39440a16f79.png" style="width:33.33em;height:3.42em;"/></div>
<p>As discussed for SARSA, the conditions on the rewards and the learning rate can be managed by employing a clipping function and a temporal decay, <span>respectively</span>. In almost all deep Q-learning applications, these are extremely important factors to guarantee the convergence; therefore, I invite the reader to consider them whenever the training process isn't able to converge to an acceptable solution. </p>
<p>The complete<span> </span>Q-learning<span> </span>algorithm (with an optional forced termination of the episode) is:</p>
<ol>
<li>Set an initial deterministic random policy, <em>π(s)</em></li>
<li>Set the initial value array, <em>Q(s, a) = 0 ∀ s ∈ S</em> and <span><em>∀ a ∈ A</em></span></li>
<li>Set the number of episodes, <em>N</em><sub><em>episodes</em></sub></li>
<li>Set a maximum number of steps per episode, <em>N</em><sub><em>max</em></sub></li>
<li>Set a constant, <em>α</em> (<span><em>α = 0.1</em>)</span></li>
<li><span>Set a constant, <em>γ</em> (<em>γ</em></span><span><em> = 0.9</em>)</span></li>
<li>Set an initial exploration factor, <span><em>ε<sup>(0)</sup></em> (<em>ε<sup>(0)</sup> = 1.0</em>)</span></li>
<li>Define a policy to let the exploration factor <em>ε</em> decay (linear or exponential)</li>
<li>Set a counter, <em>e = 0</em></li>
<li>For <em>i = 1</em> to <em>N<sub>episodes</sub></em>:<br/>
<ol>
<li>Observe the initial state,<span> </span><em>s</em><sub><em>i</em></sub></li>
<li>While <em>s<sub>j</sub></em><span> </span>is non-terminal and <em>e &lt; N<sub>max</sub></em>:
<ol>
<li><em>e += 1</em></li>
<li>Select the action, <em>a<sub>t</sub><span> </span>= </em><span><em>π(s<sub>i</sub>)</em>, with an exploration factor <em>ε</em><sup><em>(e)</em></sup></span></li>
<li>Observe the transition (<em>a<sub>t</sub>, s<sub>i</sub></em>) → (<em>s<sub>j</sub>, r<sub>ij</sub></em>)</li>
<li>Select the action, <em>a<sub>t+1</sub><span> </span>= <span>π(s</span><sub>j</sub></em><span><em>)</em>, with an exploration factor <em>ε</em></span><sup><em>(e)</em></sup></li>
<li>Update the <em>Q(s<sub>t</sub>, a<sub>t</sub>)</em> function (if <em>s<sub>j</sub></em><span> </span>is terminal, set<span> </span><em><span>Q(s</span><sub>t+1</sub><span>, a</span><sub>t+1</sub></em><span><em>) = 0</em>) using <em>max<sub>a</sub> Q(s<sub>t+1</sub>, a)</em></span></li>
<li>Set <em>s<sub>i</sub><span> </span>= s</em><sub><em>j</em></sub></li>
</ol>
</li>
</ol>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Q-learning in the checkerboard environment</h1>
                </header>
            
            <article>
                
<p>Let's repeat the previous experiment with the Q-learning algorithm. As all of the constants are the same (as well as the choice of a <em>ε</em>-greedy policy and the starting point set to <em>(0, 0)</em>), we can directly define the function that implements the training for a single episode:</p>
<pre>import numpy as np<br/><br/>def q_step(epsilon):<br/>    e = 0<br/>    <br/>    i = x_start<br/>    j = y_start<br/>    <br/>    while e &lt; max_steps:<br/>        e += 1<br/>        <br/>        action = select_action(epsilon, i, j)<br/>    <br/>        if action == 0:<br/>            if i == 0:<br/>                x = 0<br/>            else:<br/>                x = i - 1<br/>            y = j<br/>                <br/>        elif action == 1:<br/>            if j == width - 1:<br/>                y = width - 1<br/>            else:<br/>                y = j + 1<br/>            x = i<br/>                <br/>        elif action == 2:<br/>            if i == height - 1:<br/>                x = height - 1<br/>            else:<br/>                x = i + 1<br/>            y = j<br/>                <br/>        else:<br/>            if j == 0:<br/>                y = 0<br/>            else:<br/>                y = j - 1<br/>            x = i<br/>                <br/>        reward = tunnel_rewards[x, y]<br/>        <br/>        if is_final(x, y):<br/>            Q[i, j, action] += alpha * (reward - Q[i, j, action])<br/>            break<br/>            <br/>        else:<br/>            Q[i, j, action] += alpha * (reward + (gamma * np.max(Q[x, y])) - Q[i, j, action])<br/>            <br/>            i = x<br/>            j = y</pre>
<p>We can now train the model for 5,000 iterations, with 3,500 explorative ones:</p>
<pre>n_episodes = 5000<br/>n_exploration = 3500<br/><br/>for t in range(n_episodes):<br/>    epsilon = 0.0<br/>    <br/>    if t &lt;= n_exploration:<br/>        epsilon = 1.0 - (float(t) / float(n_exploration))<br/>        <br/>    q_step(epsilon)</pre>
<p>The resulting value matrix (defined as in the SARSA experiment) is:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b87746f0-69ad-4765-9ce2-70d810465dcc.png" style="width:75.33em;height:29.08em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final value matrix</div>
<p>Again, the learned <em>Q</em> function (and obviously, also the greedy <em>V(s)</em>) is coherent with the initial objective (in particular, considering the starting point set to <em>(0, 0)</em>), and the resulting policy can immediately confirm this result:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/098a6096-5934-4fb8-9601-78c6f2bbec35.png" style="width:76.42em;height:29.67em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final policy</div>
<p>The behavior of Q-learning is not very different from SARSA (even if the convergence is faster), and some initial states are not perfectly managed. This is a consequence of our choice; therefore, I invite the reader to repeat the exercise using random starts and comparing the training speed of Q-learning and SARSA.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Q-learning using a neural network</h1>
                </header>
            
            <article>
                
<p>Now, we want to test the Q-learning algorithm using a smaller checkerboard environment and a neural network (with Keras). The main difference from the previous examples is that now, the state is represented by a screenshot of the current configuration; hence, the model has to learn how to associate a value with each input image and action. This isn't actual deep Q-learning (which is based on Deep Convolutional Networks, and requires more complex environments that we cannot discuss in this book), but it shows how such a model can learn an optimal policy with the same input provided to a human being. In order to reduce the training time, we are considering a square checkerboard environment, with four negative absorbing states and a positive final one:</p>
<pre>import numpy as np<br/><br/>width = 5<br/>height = 5<br/>nb_actions = 4<br/><br/>y_final = width - 1<br/>x_final = height - 1<br/><br/>y_wells = [0, 1, 3, 4]<br/>x_wells = [3, 1, 2, 0] <br/><br/>standard_reward = -0.1<br/>tunnel_rewards = np.ones(shape=(height, width)) * standard_reward<br/><br/>for x_well, y_well in zip(x_wells, y_wells):<br/>    tunnel_rewards[x_well, y_well] = -5.0<br/><br/>tunnel_rewards[x_final, y_final] = 5.0</pre>
<p>A graphical representation of the rewards is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8795c029-0797-4dd0-ba56-81f67b20ff0c.png" style="width:24.08em;height:24.42em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Rewards in the smaller checkerboard environment</div>
<p>As we want to provide the network with a graphical input, we need to define a function to create a matrix representing the tunnel:</p>
<pre>import numpy as np<br/><br/>def reset_tunnel():<br/>    tunnel = np.zeros(shape=(height, width), dtype=np.float32)<br/><br/>    for x_well, y_well in zip(x_wells, y_wells):<br/>        tunnel[x_well, y_well] = -1.0<br/><br/>    tunnel[x_final, y_final] = 0.5<br/>    <br/>    return tunnel</pre>
<p>The <kbd>reset_tunnel()</kbd> function sets all values equal to 0, except for (which is marked with <kbd>-1</kbd>) and the final state (defined by <kbd>0.5</kbd>). The position of the agent (defined with the value <kbd>1</kbd>) is directly managed by the training function. At this point, we can create and compile our neural network. As the problem is not very complex, we are employing an MLP:</p>
<pre>from keras.models import Sequential<br/>from keras.layers import Dense, Activation<br/><br/>model = Sequential()<br/><br/>model.add(Dense(8, input_dim=width * height))<br/>model.add(Activation('tanh'))<br/>          <br/>model.add(Dense(4))<br/>model.add(Activation('tanh'))<br/><br/>model.add(Dense(nb_actions))<br/>model.add(Activation('linear'))<br/><br/>model.compile(optimizer='rmsprop',<br/>              loss='mse')</pre>
<p>The input is a flattened array, while the output is the <em>Q</em> function (all of the values corresponding to each action). The network is trained using RMSprop and a mean squared error loss function (our goal is to reduce the MSE between the actual value and the prediction). In order to train and query the network, it's helpful to create two dedicated functions:</p>
<pre>import numpy as np<br/><br/>def train(state, q_value):<br/>    model.train_on_batch(np.expand_dims(state.flatten(), axis=0), np.expand_dims(q_value, axis=0))<br/><br/>def get_Q_value(state):<br/>    return model.predict(np.expand_dims(state.flatten(), axis=0))[0]<br/><br/>def select_action_neural_network(epsilon, state):<br/>    Q_value = get_Q_value(state)<br/>    <br/>    if np.random.uniform(0.0, 1.0) &lt; epsilon:<br/>        return Q_value, np.random.randint(0, nb_actions)<br/>    <br/>    return Q_value, np.argmax(Q_value)</pre>
<p>The behavior of these functions is straightforward. The only element that may be new to the reader is the use of the <kbd>train_on_batch()</kbd> method. Contrary to <kbd>fit()</kbd>, this function allows us to perform a single training step, given a batch of input-output couples (in our case, we always have a single couple). As our goal is finding an optimal path to the final state, starting from every possible cell, we are going to employ random starts:</p>
<pre>import numpy as np<br/><br/>xy_grid = np.meshgrid(np.arange(0, height), np.arange(0, width), sparse=False)<br/>xy_grid = np.array(xy_grid).T.reshape(-1, 2)<br/><br/>xy_final = list(zip(x_wells, y_wells))<br/>xy_final.append([x_final, y_final])<br/><br/>xy_start = []<br/><br/>for x, y in xy_grid:<br/>    if (x, y) not in xy_final:<br/>        xy_start.append([x, y])<br/>        <br/>xy_start = np.array(xy_start)<br/><br/>def starting_point():<br/>    xy = np.squeeze(xy_start[np.random.randint(0, xy_start.shape[0], size=1)])<br/>    return xy[0], xy[1]</pre>
<p>Now, we can define the functions needed to perform a single training step:</p>
<pre>import numpy as np<br/><br/>def is_final(x, y):<br/>    if (x, y) in zip(x_wells, y_wells) or (x, y) == (x_final, y_final):<br/>        return True<br/>    return False<br/><br/>def q_step_neural_network(epsilon, initial_state): <br/>    e = 0<br/>    total_reward = 0.0<br/>    <br/>    (i, j) = starting_point()<br/>    <br/>    prev_value = 0.0<br/>    tunnel = initial_state.copy()<br/>    tunnel[i, j] = 1.0<br/>    <br/>    while e &lt; max_steps:<br/>        e += 1<br/>        <br/>        q_value, action = select_action_neural_network(epsilon, tunnel)<br/>    <br/>        if action == 0:<br/>            if i == 0:<br/>                x = 0<br/>            else:<br/>                x = i - 1<br/>            y = j<br/>                <br/>        elif action == 1:<br/>            if j == width - 1:<br/>                y = width - 1<br/>            else:<br/>                y = j + 1<br/>            x = i<br/>                <br/>        elif action == 2:<br/>            if i == height - 1:<br/>                x = height - 1<br/>            else:<br/>                x = i + 1<br/>            y = j<br/>                <br/>        else:<br/>            if j == 0:<br/>                y = 0<br/>            else:<br/>                y = j - 1<br/>            x = i<br/>                <br/>        reward = tunnel_rewards[x, y]<br/>        total_reward += reward<br/>        <br/>        tunnel_n = tunnel.copy()<br/>        tunnel_n[i, j] = prev_value<br/>        tunnel_n[x, y] = 1.0<br/>        <br/>        prev_value = tunnel[x, y]<br/>        <br/>        if is_final(x, y):<br/>            q_value[action] = reward<br/>            train(tunnel, q_value)<br/>            break<br/>            <br/>        else:<br/>            q_value[action] = reward + (gamma * np.max(get_Q_value(tunnel_n)))<br/>            train(tunnel, q_value)<br/>            <br/>            i = x<br/>            j = y<br/>            <br/>            tunnel = tunnel_n.copy()<br/>            <br/>    return total_reward</pre>
<p>The <kbd>q_step_neural_network()</kbd> function is very similar to the one defined in the previous example. The only difference is the management of the visual state. Every time there's a transition, the value <kbd>1.0</kbd> (denoting the agent) is moved from the old position to the new one, and the value of the previous cell is reset to its default (saved in the <kbd>prev_value</kbd> variable). Another secondary difference is the absence of <em>α</em> because there's already a learning rate set in the SGD algorithm, and it doesn't make sense to add another parameter to the model. We can now train the model for 10,000 iterations, with 7,500 explorative ones:</p>
<pre>n_episodes = 10000<br/>n_exploration = 7500<br/><br/>total_rewards = []<br/><br/>for t in range(n_episodes):<br/>    tunnel = reset_tunnel()<br/>    <br/>    epsilon = 0.0<br/>    <br/>    if t &lt;= n_exploration:<br/>        epsilon = 1.0 - (float(t) / float(n_exploration))<br/>        <br/>    t_reward= q_step_neural_network(epsilon, tunnel)<br/>    total_rewards.append(t_reward)</pre>
<p>When the training process has finished, we can analyze the total rewards, in order to understand whether the network has successfully learned the <em>Q</em> functions:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1497 image-border" src="assets/ae1981ff-5be7-44eb-a44e-cce3327b5944.png" style="width:41.75em;height:23.25em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Total rewards obtained by the neural network Q-learning algorithm</div>
<p class="mce-root">It's clear that the model is working well, because after the exploration period, the total reward becomes stationary around <kbd>4</kbd>, with small oscillations due to the different path lengths (however, the final plot can be different because of the internal random state employed by Keras). To see a confirmation, let's generate the trajectories for all of the possible initial states, using the greedy policy (equivalent to <em>ε = 0</em>):</p>
<pre class="mce-root">import numpy as np<br/><br/>trajectories = []<br/>tunnels_c = []<br/><br/>for i, j in xy_start:<br/>    tunnel = reset_tunnel()<br/><br/>    prev_value = 0.0<br/><br/>    trajectory = [[i, j, -1]]<br/><br/>    tunnel_c = tunnel.copy()<br/>    tunnel[i, j] = 1.0<br/>    tunnel_c[i, j] = 1.0<br/><br/>    final = False<br/>    e = 0<br/><br/>    while not final and e &lt; max_steps:<br/>        e += 1<br/><br/>        q_value = get_Q_value(tunnel)<br/>        action = np.argmax(q_value)<br/><br/>        if action == 0:<br/>            if i == 0:<br/>                x = 0<br/>            else:<br/>                x = i - 1<br/>            y = j<br/><br/>        elif action == 1:<br/>            if j == width - 1:<br/>                y = width - 1<br/>            else:<br/>                y = j + 1<br/>            x = i<br/><br/>        elif action == 2:<br/>            if i == height - 1:<br/>                x = height - 1<br/>            else:<br/>                x = i + 1<br/>            y = j<br/><br/>        else:<br/>            if j == 0:<br/>                y = 0<br/>            else:<br/>                y = j - 1<br/>            x = i<br/><br/>        trajectory[e - 1][2] = action<br/>        trajectory.append([x, y, -1])<br/><br/>        tunnel[i, j] = prev_value<br/><br/>        prev_value = tunnel[x, y]<br/><br/>        tunnel[x, y] = 1.0<br/>        tunnel_c[x, y] = 1.0<br/><br/>        i = x<br/>        j = y<br/><br/>        final = is_final(x, y)<br/>    <br/>    trajectories.append(np.array(trajectory))<br/>    tunnels_c.append(tunnel_c)<br/>    <br/>trajectories = np.array(trajectories) </pre>
<p>Twelve random trajectories are shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e1324c22-d2fc-442e-9242-04366531da33.png" style="width:63.58em;height:36.25em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Twelve trajectories generated using the greedy policy</div>
<p class="mce-root">The agent always follows the optimal policy, independent from the initial state, and never ends up in a well. Even if the example is quite simple, it's helpful to introduce the reader to the concept of deep Q-learning (for further details, the reader can check the introductory paper, <em>Deep Reinforcement Learning</em>: <em>An Overview</em>, <em>Li Y.</em>, <em>arXiv:1701.07274 [cs.LG]</em>).</p>
<p class="mce-root">In a general case, the environment can be a more complex game (like Atari or Sega), and the number of possible actions is very limited. Moreover, there's no possibility to employ random starts, but it's generally a good practice to skip a number of initial frames, in order to avoid a bias to the estimator. Clearly, the network must be more complex (involving convolutions to better learn the geometric dependencies), and the number of iterations must be extremely large. Many other tricks and specific algorithms can be employed in order to speed up the convergence, but due to a lack of space, they are beyond the scope of this book.</p>
<p class="mce-root">However, the general process and its logic are almost the same, and it's not difficult to understand why some strategies are preferable, and how the accuracy can be improved. As an exercise, I invite the reader to create more complex environments, with or without checkpoints and stochastic rewards. It's not surprising to see how the model will be able to easily learn the dynamics with a sufficiently large number of episodes. Moreover, as suggested in the Actor-Critic section, it's a good idea to use Tensorflow to implement such a model, comparing the performances with Q-learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we presented the natural evolution of TD(0), based on an average of backups with different lengths. The algorithm, called TD(λ), is extremely powerful, and it assures a faster convergence than TD(0), with only a few (non-restrictive) conditions. We also showed how to implement the Actor-Critic method with TD(0), in order to learn about both a stochastic policy and a value function.</p>
<p>In further sections, we discussed two methods based on the estimation of the <em>Q</em> function: SARSA and Q-learning. They are very similar, but the latter has a greedy approach, and its performance (in particular, the training speed) results in it being superior to SARSA. The Q-learning algorithm is one of the most important models for the latest developments. In fact, it was the first RL approach employed with a Deep Convolutional Network to solve complex environments (like Atari games). For this reason, we also presented a simple example, based on an MLP that processes a visual input and outputs the <em>Q</em> values for each action.</p>
<p>The world of RL is extremely fascinating, and hundreds of researchers work every day to improve algorithms and solve more and more complex problems. I invite the reader to check the references in order to find useful resources that can be exploited to obtain a deeper understanding of the models and their developments. Moreover, I suggest reading the blog posts written by the Google DeepMind team, which is one of the pioneers in the field of deep RL. I also suggest searching for the papers freely available on <em>arXiv</em>.</p>
<p>I'm happy to end this book with this topic, because I believe that RL can provide new and more powerful tools that will dramatically change our lives!</p>


            </article>

            
        </section>
    </body></html>