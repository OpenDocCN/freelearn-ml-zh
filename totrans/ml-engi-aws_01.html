<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer035">
<h1 class="chapter-number" id="_idParaDest-16"><a id="_idTextAnchor016"/><a id="_idTextAnchor017"/>1</h1>
<h1 id="_idParaDest-17"><a id="_idTextAnchor018"/>Introduction to ML Engineering on AWS</h1>
<p>Most of us started our <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) journey by training our first ML model using a sample dataset on our<a id="_idIndexMarker000"/> laptops or home computers. Things are somewhat straightforward until we need to work with much larger datasets and run our ML experiments in the cloud. It also becomes more challenging once we need to deploy our trained models to production-level inference endpoints or web servers. There are a lot of things to consider when designing and building ML systems and these are just some of the challenges data scientists and ML engineers face when working on real-life requirements. That said, we must use the right platform, along with the right set of tools, when performing ML experiments and deployments in the cloud. </p>
<p>At this point, you might be wondering why we should even use a cloud platform when running our workloads. <em class="italic">Can’t we build this platform ourselves</em>? Perhaps you might be thinking that building and operating your own data center is a relatively easy task. In the past, different teams and companies have tried setting up infrastructure within their data centers and on-premise hardware. Over time, these companies started migrating their workloads to the cloud as they realized how hard and expensive it was to manage and operate data centers. A good example of this would be the <em class="italic">Netflix</em> team, which migrated their resources to the <strong class="bold">AWS</strong> cloud. Migrating to the cloud allowed them to scale better and allowed them to have a significant increase in service availability.</p>
<p>The <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) platform <a id="_idIndexMarker001"/>provides a lot of services and capabilities that can be used by professionals and companies around the world to manage different types of workloads in the cloud. These past couple of years, AWS has announced and released a significant number of services, capabilities, and features that can be used for production-level ML experiments and deployments as well. This is due to the increase in ML workloads being migrated to the cloud globally. As we go through each of the chapters in this book, we will have a better understanding of how different services are used to solve the challenges when productionizing ML models. </p>
<p>The following diagram shows the hands-on journey for this chapter:</p>
<div>
<div class="IMG---Figure" id="_idContainer007">
<img alt="Figure 1.1 – Hands-on journey for this chapter " height="699" src="image/B18638_01_001.jpg" width="1299"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Hands-on journey for this chapter</p>
<p>In this introductory chapter, we will focus on getting our feet wet by trying out different options when building an ML model on AWS. As shown in the preceding diagram, we will use a variety of <strong class="bold">AutoML</strong> services and solutions to build ML models that can help us predict if a hotel booking will be cancelled or not based on the information available. We will start by setting up a <strong class="bold">Cloud9</strong> environment, which will help us run our code through an <strong class="bold">integrated development environment</strong> (<strong class="bold">IDE</strong>) in our browser. In this environment, we will generate a realistic synthetic dataset using a <strong class="bold">deep learning</strong> model called the <strong class="bold">Conditional Generative Adversarial Network</strong>. We will upload this dataset to <strong class="bold">Amazon S3</strong> using the <strong class="bold">AWS CLI</strong>. Inside the Cloud9 environment, we will also install <strong class="bold">AutoGluon</strong> and run an <strong class="bold">AutoML</strong> experiment to train and generate multiple models using the synthetic dataset. Finally, we will use <strong class="bold">SageMaker Canvas</strong> and <strong class="bold">SageMaker Autopilot</strong> to run AutoML experiments using the uploaded dataset in S3. If you are wondering what these fancy terms are, keep reading as we demystify each of these in this chapter. </p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>What is expected from ML engineers?</li>
<li>How ML engineers can get the most out of AWS</li>
<li>Essential prerequisites</li>
<li>Preparing the dataset</li>
<li>AutoML with AutoGluon</li>
<li>Getting started with SageMaker and SageMaker Canvas</li>
<li>No-code machine learning with SageMaker Canvas</li>
<li>AutoML with SageMaker Autopilot</li>
</ul>
<p>In addition to getting our feet wet using key ML services, libraries, and tools to perform AutoML experiments, this introductory chapter will help us gain a better understanding of several ML and ML engineering concepts that will be relevant to the succeeding chapters of this book. With this in mind, let’s get started!</p>
<h1 id="_idParaDest-18"><a id="_idTextAnchor019"/>Technical requirements</h1>
<p>Before we start, we must have an AWS account. If you do not have an AWS account yet, simply create an account here: <a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>. You may proceed with the next steps once the account is ready.</p>
<p>The Jupyter notebooks, source code, and other files for each chapter are available in this book’s GitHub repository: <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS</a>.</p>
<h1 id="_idParaDest-19"><a id="_idTextAnchor020"/>What is expected from ML engineers?</h1>
<p>ML engineering <a id="_idIndexMarker002"/>involves using ML and <strong class="bold">software engineering</strong> concepts and techniques to design, build, and manage production-level ML systems, along with pipelines. In a team working to build ML-powered applications, <strong class="bold">ML engineers</strong> are generally expected to build and operate the ML infrastructure that’s used to train and deploy models. In some cases, data scientists may also need to work on infrastructure-related requirements, especially if there is no clear delineation between the roles and responsibilities of ML engineers and data scientists in an organization.</p>
<p>There are several things an ML engineer should consider when designing and building ML systems and platforms. These would include the <em class="italic">quality</em> of the deployed ML model, along with the <em class="italic">security</em>, <em class="italic">scalability</em>, <em class="italic">evolvability</em>, <em class="italic">stability</em>, and <em class="italic">overall cost</em> of the ML infrastructure used. In this book, we will discuss the different strategies and best practices to achieve the different objectives of an ML engineer.</p>
<p>ML engineers <a id="_idIndexMarker003"/>should also be capable of designing and building automated ML workflows using a variety of solutions. Deployed models degrade over time and <strong class="bold">model retraining</strong> becomes essential in ensuring the quality of deployed ML models. Having automated ML pipelines in place helps enable automated model retraining and deployment. </p>
<p class="callout-heading">Important note</p>
<p class="callout">If you are excited to learn more about how to build custom ML pipelines on AWS, then you should check out the last section of this book: <em class="italic">Designing and building end-to-end MLOps pipelines</em>. You should find several chapters dedicated to deploying complex ML pipelines on AWS!</p>
<h1 id="_idParaDest-20"><a id="_idTextAnchor021"/>How ML engineers can get the most out of AWS</h1>
<p>There are <a id="_idIndexMarker004"/>many services and capabilities in the AWS platform that an ML engineer can choose from. Professionals who are already familiar with using virtual machines can easily spin up <strong class="bold">EC2</strong> instances and run ML experiments using deep learning frameworks inside these virtual private servers. Services such as <strong class="bold">AWS Glue</strong>, <strong class="bold">Amazon EMR</strong>, and <strong class="bold">AWS Athena</strong> can be utilized by ML engineers and data engineers for different data management and processing needs. Once the ML models need to be deployed into dedicated inference endpoints, a variety of options become available:</p>
<div>
<div class="IMG---Figure" id="_idContainer008">
<img alt="Figure 1.2 – AWS machine learning stack " height="925" src="image/B18638_01_002.jpg" width="1619"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – AWS machine learning stack</p>
<p>As shown in the <a id="_idIndexMarker005"/>preceding diagram, data scientists, developers, and ML engineers can make use <a id="_idIndexMarker006"/>of multiple services and capabilities from the <strong class="bold">AWS machine learning stack</strong>. The services grouped under <strong class="bold">AI services</strong> can easily be used by developers with minimal ML experience. To use the services listed here, all we need would be some experience working with data, along with the software development skills required to use SDKs and APIs. If we want to quickly build ML-powered applications with features such as language translation, text-to-speech, and product recommendation, then we can easily do that using the services under the AI Services bucket. In the middle, we have <strong class="bold">ML services</strong> and<a id="_idIndexMarker007"/> their capabilities, which help solve the more custom ML requirements of data scientists and ML engineers. To use the services and capabilities listed here, a solid understanding of the ML process is <a id="_idIndexMarker008"/>needed. The last layer, <strong class="bold">ML frameworks and infrastructure</strong>, offers the highest level of flexibility and customizability as this includes the ML infrastructure and framework support needed by more advanced use cases. </p>
<p>So, how can ML engineers make the most out of the AWS machine learning stack? The ability of ML engineers to design, build, and manage ML systems improves as they become more familiar with the services, capabilities, and tools available in the AWS platform. They may start with AI services to quickly build AI-powered applications on AWS. Over time, these ML engineers will make use of the different services, capabilities, and infrastructure from the lower two layers as they become more comfortable dealing with intermediate ML engineering requirements.</p>
<h1 id="_idParaDest-21"><a id="_idTextAnchor022"/>Essential prerequisites</h1>
<p>In this section, we will prepare the following: </p>
<ul>
<li>The Cloud9 environment</li>
<li>The S3 bucket</li>
<li>The synthetic dataset, which will be generated using a deep learning model</li>
</ul>
<p>Let’s get started.</p>
<h2 id="_idParaDest-22"><a id="_idTextAnchor023"/>Creating the Cloud9 environment</h2>
<p>One of the <a id="_idIndexMarker009"/>more convenient options when performing ML experiments inside a virtual private server is to use the <strong class="bold">AWS Cloud9</strong> service. AWS Cloud9 allows developers, data scientists, and ML engineers to manage and run code within a development environment using a browser. The code is stored and executed inside an EC2 instance, which provides an environment similar to what most developers have.</p>
<p class="callout-heading">Important note</p>
<p class="callout">It is recommended to use an <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) user with limited permissions instead of the root account when running the examples in this book. We will discuss this along with other security best practices in detail in <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a><em class="italic">, Security, Governance, and Compliance Strategies</em>. If you are just starting to use AWS, you may proceed with using the root account in the meantime.</p>
<p>Follow these steps to create a Cloud9 environment where we will generate the synthetic dataset and run the <strong class="bold">AutoGluon AutoML</strong> experiment:</p>
<ol>
<li>Type <strong class="source-inline">cloud9</strong> in the search bar. Select <strong class="bold">Cloud9</strong> from the list of results:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer009">
<img alt="Figure 1.3 – Navigating to the Cloud9 console " height="809" src="image/B18638_01_003.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – Navigating to the Cloud9 console</p>
<p class="list-inset">Here, we <a id="_idIndexMarker010"/>can see that the region is currently set to <strong class="bold">Oregon</strong> (<strong class="source-inline">us-west-2</strong>). Make sure that you change this to where you want the resources to be created.</p>
<ol>
<li value="2">Next, click <strong class="bold">Create environment</strong>.</li>
<li>Under the <strong class="bold">Name environment</strong> field, specify a name for the Cloud9 environment (for example, <strong class="source-inline">mle-on-aws</strong>) and click <strong class="bold">Next step</strong>.</li>
<li>Under <strong class="bold">Environment type</strong>, choose <strong class="bold">Create a new EC2 instance for environment (direct access)</strong>. Select <strong class="bold">m5.large</strong> for <strong class="bold">Instance type</strong> and then <strong class="bold">Ubuntu Server (18.04 LTS)</strong> for <strong class="bold">Platform</strong>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer010">
<img alt="Figure 1.4 – Configuring the Cloud9 environment settings " height="941" src="image/B18638_01_004.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – Configuring the Cloud9 environment settings</p>
<p class="list-inset">Here, we <a id="_idIndexMarker011"/>can see that there are other options for the instance type. In the meantime, we will stick with <strong class="bold">m5.large</strong> as it should be enough to run the hands-on solutions in this chapter.</p>
<ol>
<li value="5">For the <strong class="bold">Cost-saving setting</strong> option, choose <strong class="bold">After four hours</strong> from the list of drop-down options. This means that the server where the Cloud9 environment is running will automatically shut down after 4 hours of inactivity.</li>
<li>Under <strong class="bold">Network settings (advanced)</strong>, select the default VPC of the region for the <strong class="bold">Network (VPC)</strong> configuration. It should have a format similar to <strong class="source-inline">vpc-abcdefg (default)</strong>. For the <strong class="bold">Subnet </strong>option, choose the option that has a format similar to <strong class="source-inline">subnet-abcdefg | Default in us-west-2a</strong>. </li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">It is recommended that you use the default VPC since the networking configuration is simple. This will help you avoid issues, especially if you’re just getting started with VPCs. If you encounter any VPC-related issues when launching a Cloud9 instance, you may need to check if the selected subnet has been configured with internet access via the route table configuration in the VPC console. You may retry launching the instance using another subnet or by using a new VPC altogether. If you are planning on creating a new VPC, navigate to <a href="https://go.aws/3sRSigt">https://go.aws/3sRSigt</a> and create a <strong class="bold">VPC with a Single Public Subnet</strong>. If none of these options work, you may try launching the Cloud9 instance in another region. We’ll discuss <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>) networks in detail in <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a><em class="italic">, Security, Governance, and Compliance Strategies</em>.</p>
<ol>
<li value="7">Click <strong class="bold">Next Step</strong>.</li>
<li>On the review page, click <strong class="bold">Create environment</strong>. This should redirect you to the Cloud9 environment, which should take a minute or so to load. The Cloud9 <strong class="bold">IDE</strong> is shown in the following screenshot. This is where we can write our code and run the scripts and commands needed to work on some of the hands-on solutions<a id="_idIndexMarker012"/> in this book:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer011">
<img alt="Figure 1.5 – AWS Cloud9 interface " height="816" src="image/B18638_01_005.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – AWS Cloud9 interface</p>
<p class="list-inset">Using <a id="_idIndexMarker013"/>this IDE is fairly straightforward as it looks very similar to code editors such as <strong class="bold">Visual Studio Code</strong> and <strong class="bold">Sublime Text</strong>. As shown in the preceding screenshot, we can find the <strong class="bold">menu bar</strong> at the top (<strong class="bold">A</strong>). The <strong class="bold">file tree</strong> can be found on the left-hand side (<strong class="bold">B</strong>). The <strong class="bold">editor</strong> covers a major portion of the screen in the middle (<strong class="bold">C</strong>). Lastly, we can find the <strong class="bold">terminal</strong> at the bottom (<strong class="bold">D</strong>).</p>
<p class="callout-heading">Important note</p>
<p class="callout">If this is your first time using AWS Cloud9, here is a 4-minute introduction video from AWS to help you get started: <a href="https://www.youtube.com/watch?v=JDHZOGMMkj8">https://www.youtube.com/watch?v=JDHZOGMMkj8</a>.</p>
<p>Now that we have our Cloud9 environment ready, it is time we configure it with a larger storage space.</p>
<h2 id="_idParaDest-23"><a id="_idTextAnchor024"/>Increasing Cloud9’s storage</h2>
<p>When a<a id="_idIndexMarker014"/> Cloud9 instance is created, the attached volume only starts with 10GB of disk space. Given that we will be installing different libraries and frameworks while running ML experiments in this instance, we will need more than 10GB of disk space. We will resize the volume programmatically using the <strong class="source-inline">boto3</strong> library.</p>
<p class="callout-heading">Important note</p>
<p class="callout">If this is your first time using the <strong class="source-inline">boto3</strong> library, it is the <strong class="bold">AWS SDK for Python</strong>, which gives us a way to programmatically manage the different AWS resources in our AWS accounts. It is a service-level SDK that helps us list, create, update, and delete AWS resources such as EC2 instances, S3 buckets, and EBS volumes.</p>
<p>Follow these steps to download and run some scripts to increase the volume disk space from 10GB to 120GB:</p>
<ol>
<li value="1">In the terminal of our Cloud9 environment (right after the <strong class="source-inline">$</strong> sign at the bottom of the screen), run the following bash command:<pre class="source-code"><strong class="bold">wget -O resize_and_reboot.py https://bit.ly/3ea96tW</strong></pre></li>
</ol>
<p class="list-inset">This will download the script file located at <a href="https://bit.ly/3ea96tW">https://bit.ly/3ea96tW</a>. Here, we are simply using a URL shortener, which would map the shortened link to <a href="https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter01/resize_and_reboot.py">https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter01/resize_and_reboot.py</a>. </p>
<p class="callout-heading">Important note</p>
<p class="callout">Note that we are using the big <strong class="source-inline">O</strong> flag instead of a small <strong class="source-inline">o</strong> or a zero (<strong class="source-inline">0</strong>) when using the <strong class="source-inline">wget</strong> command.</p>
<ol>
<li value="2">What’s <a id="_idIndexMarker015"/>inside the file we just downloaded? Let’s quickly inspect the file before we run the script. Double-click the <strong class="source-inline">resize_and_reboot.py</strong> file in the file tree (located on the left-hand side of the screen) to open the Python script file in the editor pane. As shown in the following screenshot, the <strong class="source-inline">resize_and_reboot.py</strong> script has three major sections. The first block of code focuses on importing the prerequisites needed to run the script. The second block of code focuses on resizing the volume of a selected EC2 instance using the <strong class="source-inline">boto3</strong> library. It makes use of the <strong class="source-inline">describe_volumes()</strong> method to get the volume ID of the current instance, and then makes use of the <strong class="source-inline">modify_volume()</strong> method to update the volume size to 120GB. The last section involves a single line of code that simply reboots the EC2 instance. This line of code uses the <strong class="source-inline">os.system()</strong> method to run the <strong class="source-inline">sudo reboot</strong> shell command:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer012">
<img alt="Figure 1.6 – The resize_and_reboot.py script file " height="880" src="image/B18638_01_006.jpg" width="1617"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – The resize_and_reboot.py script file</p>
<p class="list-inset">You can find the <strong class="source-inline">resize_and_reboot.py</strong> script file in this book’s GitHub repository: <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/resize_and_reboot.py">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/resize_and_reboot.py</a>. Note that for this script to work, the <strong class="source-inline">EC2_INSTANCE_ID</strong> environment variable must be set to select the correct target instance. We’ll set this environment variable a few steps from now before we run the <strong class="source-inline">resize_and_reboot.py</strong> script.</p>
<ol>
<li value="3">Next, run the<a id="_idIndexMarker016"/> following command in the terminal:<pre class="source-code"><strong class="bold">python3 -m pip install --user --upgrade boto3</strong></pre></li>
</ol>
<p class="list-inset">This will upgrade the version of <strong class="source-inline">boto3</strong> using <strong class="source-inline">pip</strong>. </p>
<p class="callout-heading">Important note</p>
<p class="callout">If this is your first time using <strong class="source-inline">pip</strong>, it is the package installer for Python. It makes it convenient to install different packages and libraries using the command line.</p>
<p class="list-inset">You may use <strong class="source-inline">python3 -m pip show boto3</strong> to check the version you are using. This book assumes that you are using version <strong class="source-inline">1.20.26</strong> or later.</p>
<ol>
<li value="4">The<a id="_idIndexMarker017"/> remaining statements focus on getting the Cloud9 environment’s <strong class="source-inline">instance_id</strong> from the instance metadata service and storing this value in the <strong class="source-inline">EC2_INSTANCE_ID</strong> variable. Let’s run the following in the terminal: <pre class="source-code"><strong class="bold">TARGET_METADATA_URL=http://169.254.169.254/latest/meta-data/instance-id</strong></pre><pre class="source-code"><strong class="bold">export EC2_INSTANCE_ID=$(curl -s $TARGET_METADATA_URL)</strong></pre><pre class="source-code"><strong class="bold">echo $EC2_INSTANCE_ID</strong></pre></li>
</ol>
<p class="list-inset">This should give us an EC2 instance ID with a format similar to <strong class="source-inline">i-01234567890abcdef</strong>. </p>
<ol>
<li value="5">Now that we have the <strong class="source-inline">EC2_INSTANCE_ID</strong> environment variable set with the appropriate value, we can run the following command: <pre class="source-code"><strong class="bold">python3 resize_and_reboot.py</strong></pre></li>
</ol>
<p class="list-inset">This will run the Python script we downloaded earlier using the <strong class="source-inline">wget</strong> command. After performing the volume resize operation using <strong class="source-inline">boto3</strong>, the script will reboot the instance. You should see a <strong class="bold">Reconnecting…</strong> notification at the top of the <a id="_idIndexMarker018"/>page while the Cloud9 environment’s EC2 instance is being restarted.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Feel free to run the <strong class="source-inline">lsblk</strong> command after the instance has been restarted. This should help you verify that the volume of the Cloud9 environment instance has been resized to 120GB.</p>
<p>Now that we have successfully resized the volume to 120GB, we should be able to work on the next set of solutions without having to worry about disk space issues inside our Cloud9 environment. </p>
<h2 id="_idParaDest-24"><a id="_idTextAnchor025"/>Installing the Python prerequisites</h2>
<p>Follow these steps to<a id="_idIndexMarker019"/> install and update several Python packages inside the Cloud9 environment:</p>
<ol>
<li value="1">In the terminal of our Cloud9 environment (right after the <strong class="source-inline">$</strong> sign at the bottom of the screen), run the following commands to update <strong class="source-inline">pip</strong>, <strong class="source-inline">setuptools</strong>, and <strong class="source-inline">wheel</strong>:<pre class="source-code"><strong class="bold">python3 -m pip install -U pip</strong></pre><pre class="source-code"><strong class="bold">python3 -m pip install -U setuptools wheel</strong></pre></li>
</ol>
<p class="list-inset">Upgrading these versions will help us make sure that the other installation steps work smoothly. This book assumes that you are using the following versions or later: <strong class="source-inline">pip</strong> – <strong class="source-inline">21.3.1</strong>, <strong class="source-inline">setuptools</strong> – <strong class="source-inline">59.6.0</strong>, and <strong class="source-inline">wheel</strong> – <strong class="source-inline">0.37.1</strong>.</p>
<p class="callout-heading">Important note</p>
<p class="callout">To check the versions, you may use the <strong class="source-inline">python3 -m pip show &lt;package&gt;</strong> command in the terminal. Simply replace <strong class="source-inline">&lt;package&gt;</strong> with the name of the package. An example of this would be <strong class="source-inline">python3 -m pip show wheel</strong>. If you want to install a specific version of a package, you may use <strong class="source-inline">python3 -m pip install -U &lt;package&gt;==&lt;version&gt;</strong>. For example, if you want to install <strong class="source-inline">wheel</strong> version <strong class="source-inline">0.37.1</strong>, you can run <strong class="source-inline">python3 -m pip install -U wheel==0.37.1</strong>.</p>
<ol>
<li value="2">Next, install <strong class="source-inline">ipython</strong> by running the following command. <strong class="bold">IPython</strong> provides a lot of handy utilities that help professionals use Python interactively. We will see how easy it is to use IPython later in the <em class="italic">Performing your first AutoGluon AutoML experiment</em> section:<pre class="source-code"><strong class="bold">python3 -m pip install ipython</strong></pre></li>
</ol>
<p class="list-inset">This book assumes that you are using <strong class="source-inline">ipython</strong> – <strong class="source-inline">7.16.2</strong> or later.</p>
<ol>
<li value="3">Now, let’s install <strong class="source-inline">ctgan</strong>. CTGAN allows us to utilize <strong class="bold">Generative Adversarial Network</strong> (<strong class="bold">GAN</strong>) deep learning models to generate synthetic datasets. We will discuss this shortly in the <em class="italic">Generating a synthetic dataset using a deep learning model</em> section, after we have installed the Python prerequisites:<pre class="source-code"><strong class="bold">python3 -m pip install ctgan==0.5.0</strong></pre></li>
</ol>
<p class="list-inset">This book<a id="_idIndexMarker020"/> assumes that you are using <strong class="source-inline">ctgan</strong> – <strong class="source-inline">0.5.0</strong>.</p>
<p class="callout-heading">Important note</p>
<p class="callout">This step may take around 5 to 10 minutes to complete. While waiting, let’s talk about what CTGAN is. <strong class="bold">CTGAN</strong> is an open<a id="_idIndexMarker021"/> source library that uses deep learning to learn about the properties of an existing dataset and generates a new dataset with columns, values, and properties similar to the original dataset. For more information, feel free to check its GitHub <a id="_idIndexMarker022"/>page here: <a href="https://github.com/sdv-dev/CTGAN">https://github.com/sdv-dev/CTGAN</a>.</p>
<ol>
<li value="4">Finally, install <strong class="source-inline">pandas_profiling</strong> by running the following command. This allows <a id="_idIndexMarker023"/>us to easily generate a profile report for our dataset, which will help us with our <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>) work. We will see this in action in the <em class="italic">Exploratory data analysis</em> section, after we have generated the synthetic dataset:<pre class="source-code"><strong class="bold">python3 -m pip install pandas_profiling</strong></pre></li>
</ol>
<p class="list-inset">This book assumes that you are using <strong class="source-inline">pandas_profiling</strong> – <strong class="source-inline">3.1.0</strong> or later.</p>
<p>Now that we have finished installing the Python prerequisites, we can start generating a realistic synthetic dataset using a deep learning model!</p>
<h1 id="_idParaDest-25"><a id="_idTextAnchor026"/>Preparing the dataset</h1>
<p>In this chapter, we will<a id="_idIndexMarker024"/> build multiple ML models that will <em class="italic">predict whether a hotel booking will be cancelled or not based on the information available</em>. Hotel cancellations cause a lot of issues for hotel owners and managers, so trying to predict which reservations will be cancelled is a good use of our ML skills.</p>
<p>Before we start with our ML experiments, we will need a dataset that can be used when training our ML models. We will generate a realistic synthetic dataset similar to the <em class="italic">Hotel booking demands</em> dataset from <em class="italic">Nuno Antonio</em>, <em class="italic">Ana de Almeida</em>, and <em class="italic">Luis Nunes</em>. </p>
<p>The synthetic dataset<a id="_idIndexMarker025"/> will have a total of 21 columns. Here are some of the columns:</p>
<ul>
<li><strong class="source-inline">is_cancelled</strong>: Indicates<a id="_idIndexMarker026"/> whether the hotel booking was cancelled or not</li>
<li><strong class="source-inline">lead_time</strong>: [<em class="italic">arrival date</em>] – [<em class="italic">booking date</em>]  </li>
<li><strong class="source-inline">adr</strong>: Average daily rate</li>
<li><strong class="source-inline">adults</strong>: Number of adults</li>
<li><strong class="source-inline">days_in_waiting_list</strong>: Number of days a booking stayed on the waiting list before getting confirmed</li>
<li><strong class="source-inline">assigned_room_type</strong>: The type of room that was assigned </li>
<li><strong class="source-inline">total_of_special_requests</strong>: The total<a id="_idIndexMarker027"/> number of special requests made by the customer</li>
</ul>
<p>We will not discuss each of the fields in detail, but this should help us understand what data is available for us to use. For more information, you can find the original version of this dataset at <a href="https://www.kaggle.com/jessemostipak/hotel-booking-demand">https://www.kaggle.com/jessemostipak/hotel-booking-demand</a> and <a href="https://www.sciencedirect.com/science/article/pii/S2352340918315191">https://www.sciencedirect.com/science/article/pii/S2352340918315191</a>.</p>
<h2 id="_idParaDest-26"><a id="_idTextAnchor027"/>Generating a synthetic dataset using a deep learning model</h2>
<p>One of the<a id="_idIndexMarker028"/> cool applications of <a id="_idIndexMarker029"/>ML would be having a <strong class="bold">deep learning</strong> model “absorb” the properties of an existing dataset and generate a new dataset with a similar set of fields and properties. We will use a pre-trained <strong class="bold">Generative Adversarial Network</strong> (<strong class="bold">GAN</strong>) model to generate the synthetic dataset. </p>
<p class="callout-heading">Important note</p>
<p class="callout"><strong class="bold">Generative modeling</strong> involves <a id="_idIndexMarker030"/>learning patterns from the values of an input dataset, which are then used to generate a new dataset with a similar set of values. GANs are popular when it comes to generative modeling. For example, research papers have focused on how GANs can be used to generate “deepfakes,” where realistic images of humans are generated from a source dataset.</p>
<p>Generating and using a synthetic dataset has a lot of benefits, including the following: </p>
<ul>
<li>The ability to generate a much larger dataset than the original dataset that was used to train the model</li>
<li>The ability to anonymize any sensitive information in the original dataset</li>
<li>Being able to have a cleaner version of the dataset after data generation</li>
</ul>
<p>That said, let’s start generating the synthetic dataset by running the following commands in the terminal of our Cloud9 environment (right after the <strong class="source-inline">$</strong> sign at the bottom of the screen):</p>
<ol>
<li value="1">Continuing<a id="_idIndexMarker031"/> from where we<a id="_idIndexMarker032"/> left off in the <em class="italic">Installing the Python prerequisites</em> section, run the following command to create an empty directory named <strong class="source-inline">tmp</strong> in the current working directory:<pre class="source-code">mkdir -p <strong class="bold">tmp</strong></pre></li>
</ol>
<p class="list-inset">Note that this is different from the <strong class="source-inline">/tmp</strong> directory.</p>
<ol>
<li value="2">Next, let’s download the <strong class="source-inline">utils.py</strong> file using the <strong class="source-inline">wget</strong> command:<pre class="source-code">wget -O <strong class="bold">utils.py</strong> https://bit.ly/3CN4owx</pre></li>
</ol>
<p class="list-inset">The <strong class="source-inline">utils.py</strong> file contains the <strong class="source-inline">block()</strong> function, which will help us read and troubleshoot the logs generated by our scripts.</p>
<ol>
<li value="3">Run the following command to download the pre-built GAN model into the Cloud9 environment:<pre class="source-code">wget -O <strong class="bold">hotel_bookings.gan.pkl</strong> https://bit.ly/3CHNQFT</pre></li>
</ol>
<p class="list-inset">Here, we have a serialized pickle file that contains the properties of the deep learning model.</p>
<p class="callout-heading">Important note</p>
<p class="callout">There are a variety of ways to save and load ML models. One of the options would be to use the <strong class="bold">Pickle</strong> module<a id="_idIndexMarker033"/> to serialize a Python object and store it in a file. This file can later be loaded and deserialized back to a Python object with a similar set of properties.</p>
<ol>
<li value="4">Create an empty <strong class="source-inline">data_generator.py</strong> script file using the <strong class="source-inline">touch</strong> command:<pre class="source-code">touch <strong class="bold">data_generator.py</strong></pre></li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">Before proceeding, make sure that the <strong class="source-inline">data_generator.py</strong>, <strong class="source-inline">hotel_bookings.gan.pkl</strong>, and <strong class="source-inline">utils.py</strong> files are in the same directory so that the synthetic data generator script works.</p>
<ol>
<li value="5">Double-click the <strong class="source-inline">data_generator.py</strong> file in the file tree (located on the left-hand <a id="_idIndexMarker034"/>side of the Cloud9 environment) to <a id="_idIndexMarker035"/>open the empty Python script file in the editor pane.</li>
<li>Add the following lines of code to import the prerequisites needed to run the script:<pre class="source-code">from ctgan import <strong class="bold">CTGANSynthesizer</strong></pre><pre class="source-code">from pandas_profiling import <strong class="bold">ProfileReport</strong></pre><pre class="source-code">from utils import block, debug</pre></li>
<li>Next, let’s add the following lines of code to load the pre-trained GAN model:<pre class="source-code">with <strong class="bold">block</strong>('LOAD CTGAN'):</pre><pre class="source-code">    pkl = './hotel_bookings.gan.pkl'</pre><pre class="source-code">    gan = <strong class="bold">CTGANSynthesizer.load(pkl)</strong></pre><pre class="source-code">    print(<strong class="bold">gan.__dict__</strong>)</pre></li>
<li>Run the following command in the terminal (right after the <strong class="source-inline">$</strong> sign at the bottom of the screen) to test if our initial blocks of code in the script are working as intended:<pre class="source-code">python3 <strong class="bold">data_generator.py</strong></pre></li>
</ol>
<p class="list-inset">This should give us a set of logs similar to what is shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer013">
<img alt="Figure 1.7 – GAN model successfully loaded by the script " height="722" src="image/B18638_01_007.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – GAN model successfully loaded by the script</p>
<p class="list-inset">Here, we <a id="_idIndexMarker036"/>can see that the <a id="_idIndexMarker037"/>pre-trained GAN model was loaded successfully using the <strong class="source-inline">CTGANSynthesizer.load()</strong> method. Here, we can also see what <strong class="source-inline">block</strong> (from the <strong class="source-inline">utils.py</strong> file we downloaded earlier) does to improve the readability of our logs. It simply helps mark the start and end of the execution of a block of code so that we can easily debug our scripts.</p>
<ol>
<li value="9">Let’s go back to the editor pane (where we are editing <strong class="source-inline">data_generator.py</strong>) and add the following lines of code:<pre class="source-code">with block('GENERATE SYNTHETIC DATA'):</pre><pre class="source-code">    synthetic_data = <strong class="bold">gan.sample</strong>(10000)</pre><pre class="source-code">    print(synthetic_data)</pre></li>
</ol>
<p class="list-inset">When we run the script later, these lines of code will generate <strong class="source-inline">10000</strong> records and store them inside the <strong class="source-inline">synthetic_data</strong> variable.</p>
<ol>
<li value="10">Next, let’s add the following block of code, which will save the generated data to a CSV file inside the <strong class="source-inline">tmp</strong> directory:<pre class="source-code">with block('SAVE TO CSV'):</pre><pre class="source-code">    target_location = "tmp/bookings.all.csv"</pre><pre class="source-code">    print(target_location)</pre><pre class="source-code">    synthetic_data.to_csv(</pre><pre class="source-code">        target_location, </pre><pre class="source-code">        index=False</pre><pre class="source-code">    )</pre></li>
<li>Finally, let’s add the following lines of code to complete the script:<pre class="source-code">with block('GENERATE PROFILE REPORT'):</pre><pre class="source-code">    profile = <strong class="bold">ProfileReport</strong>(synthetic_data)</pre><pre class="source-code">    target_location = "tmp/profile-report.xhtml"</pre><pre class="source-code">    profile.to_file(target_location)</pre></li>
</ol>
<p class="list-inset">This block of code will analyze the synthetic dataset and generate a profile report to help us analyze the properties of our dataset.</p>
<p class="callout-heading">Important note</p>
<p class="callout">You can find a copy of the <strong class="source-inline">data_generator.py</strong> file here: <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/data_generator.py">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/data_generator.py</a>.</p>
<ol>
<li value="12">With<a id="_idIndexMarker038"/> everything ready, let’s<a id="_idIndexMarker039"/> run the following command in the terminal (right after the <strong class="source-inline">$</strong> sign at the bottom of the screen):<pre class="source-code">python3 <strong class="bold">data_generator.py</strong></pre></li>
</ol>
<p class="list-inset">It should take about a minute or so for the script to finish. Running the script should give us a set of logs similar to what is shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer014">
<img alt="Figure 1.8 – Logs generated by data_generator.py " height="960" src="image/B18638_01_008.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – Logs generated by data_generator.py</p>
<p class="list-inset">As we<a id="_idIndexMarker040"/> can see, running <a id="_idIndexMarker041"/>the <strong class="source-inline">data_generator.py</strong> script generates multiple blocks of logs, which should make it easy for us to read and debug what’s happening while the script is running. In addition to loading the CTGAN model, the script will generate the synthetic dataset using the deep learning model (<strong class="bold">A</strong>), save the generated data in a CSV file inside the <strong class="source-inline">tmp</strong> directory (<strong class="source-inline">tmp/bookings.all.csv</strong>) (<strong class="bold">B</strong>), and generate a profile report using <strong class="source-inline">pandas_profiling</strong> (<strong class="bold">C</strong>).</p>
<p>Wasn’t that easy? Before proceeding to the next section, feel free to use the file tree (located on the left-hand side of the Cloud9 environment) to check the generated files stored in the <strong class="source-inline">tmp</strong> directory.  </p>
<h2 id="_idParaDest-27"><a id="_idTextAnchor028"/>Exploratory data analysis</h2>
<p>At this point, we should have a <a id="_idIndexMarker042"/>synthetic dataset with <strong class="source-inline">10000</strong> rows. You might be wondering what our data looks like. Does our dataset contain invalid values? Do we have to worry about missing records? We must have a good understanding of our dataset since we may need to clean and process the data first before we do any model training work. EDA is a key step when analyzing datasets before they can be used to train ML models. There are different ways to analyze datasets and generate reports — using <strong class="source-inline">pandas_profiling</strong> is one of the faster ways to perform EDA.</p>
<p>That said, let’s check the report that was generated by the <strong class="source-inline">pandas_profiling</strong> Python library. Right-click on <strong class="source-inline">tmp/profile-report.xhtml</strong> in the file tree (located on the left-hand side of the Cloud9 environment) and then select <strong class="bold">Preview</strong> from the list of options. We should find a report similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer015">
<img alt="Figure 1.9 – Generated report " height="882" src="image/B18638_01_009.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – Generated report</p>
<p>The report has multiple sections: <strong class="bold">Overview</strong>, <strong class="bold">Variables</strong>, <strong class="bold">Interactions</strong>, <strong class="bold">Correlations</strong> <strong class="bold">Missing Values</strong>, and <strong class="bold">Sample</strong>. In the <strong class="bold">Overview</strong> section, we can find a quick summary of the dataset statistics and the variable types. This includes the number of variables, number of records (observations), number of missing cells, number of duplicate rows, and other relevant statistics. In the <strong class="bold">Variables</strong> section, we can find the statistics and the distribution of values for each variable (column) in the dataset. In the <strong class="bold">Interactions</strong> and <strong class="bold">Correlations</strong> sections, we can see different patterns and observations regarding the potential relationship of the variables in the dataset. In the <strong class="bold">Missing values</strong> section, we can see if there are columns <a id="_idIndexMarker043"/>with missing values that we need to take care of. Finally, in the <strong class="bold">Sample</strong> section, we can see the first 10 and last 10 rows of the dataset.</p>
<p>Feel free to read through the report before proceeding to the next section.</p>
<h2 id="_idParaDest-28"><a id="_idTextAnchor029"/>Train-test split</h2>
<p>Now that we have finished<a id="_idIndexMarker044"/> performing EDA, what do we do next? Assuming that our data is clean and ready for model training, do we just use all of the 10,000 records that were generated to train and build our ML model? Before we train our binary classifier model, we must split our dataset into training and test sets:</p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="Figure 1.10 – Train-test split " height="888" src="image/B18638_01_010.jpg" width="1463"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.10 – Train-test split</p>
<p>As we can see, the <strong class="bold">training set</strong> is <a id="_idIndexMarker045"/>used to build the model and update its parameters during the training phase. The <strong class="bold">test set</strong> is then used to evaluate the final version of the model on data it has <a id="_idIndexMarker046"/>not seen before. What’s not shown here is the <strong class="bold">validation set</strong>, which<a id="_idIndexMarker047"/> is used to evaluate a model to fine-tune the <strong class="bold">hyperparameters</strong> during the<a id="_idIndexMarker048"/> model training phase. In practice, the ratio when dividing the dataset into training, validation, and test sets is generally around <strong class="bold">60:20:20</strong>, where the training set gets the majority of the records. In this chapter, we will no longer need to divide the training set further into smaller training and validation sets since the AutoML tools and services will automatically do this for us.  </p>
<p class="callout-heading">Important note</p>
<p class="callout">Before proceeding with the hands-on solutions in this section, we must have an idea of what hyperparameters and parameters are. <strong class="bold">Parameters</strong> are<a id="_idIndexMarker049"/> the numerical values that a model uses when performing predictions. We can think of model predictions as functions such as <strong class="source-inline">y = m * x</strong>, where <strong class="source-inline">m</strong> is a parameter, <strong class="source-inline">x</strong> is a single predictor variable, and <strong class="source-inline">y</strong> is the target variable. For example, if we are testing the relationship between cancellations (<strong class="source-inline">y</strong>) and income (<strong class="source-inline">x</strong>), then <strong class="source-inline">m</strong> is the parameter that defines this relationship. If <strong class="source-inline">m</strong> is positive, cancellations go up as income goes up. If it is negative, cancellations lessen as income increases. On the other hand, <strong class="bold">hyperparameters</strong> are<a id="_idIndexMarker050"/> configurable values that are tweaked before the model is trained. These variables affect how our chosen ML models “model” the relationship. Each ML model has its own set of hyperparameters, depending on the algorithm used. These concepts will make more sense once we have looked at a few more examples in <a href="B18638_02.xhtml#_idTextAnchor041"><em class="italic">Chapter 2</em></a>, <em class="italic">Deep Learning AMIs</em>, and <a href="B18638_03.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>, <em class="italic">Deep Learning Containers</em>. </p>
<p>Now, let’s create a script that will help us<a id="_idIndexMarker051"/> perform the train-test split:</p>
<ol>
<li value="1">In the terminal of our Cloud9 environment (right after the <strong class="source-inline">$</strong> sign at the bottom of the screen), run the following command to create an empty file called <strong class="source-inline">train_test_split.py</strong>:<pre class="source-code">touch <strong class="bold">train_test_split.py</strong></pre></li>
<li>Using the file tree (located on the left-hand side of the Cloud9 environment), double-click the <strong class="source-inline">train_test_split.py</strong> file to open the file in the editor pane. </li>
<li>In the editor pane, add the following lines of code to import the prerequisites to run the script:<pre class="source-code">import pandas as pd</pre><pre class="source-code">from utils import block, debug</pre><pre class="source-code">from sklearn.model_selection import train_test_split</pre></li>
<li>Add the following block of code, which will read the contents of a CSV file and store it inside a <strong class="bold">pandas</strong> <strong class="source-inline">DataFrame</strong>:<pre class="source-code">with block('LOAD CSV'):</pre><pre class="source-code">    generated_df = pd.<strong class="bold">read_csv</strong>('tmp/bookings.all.csv')</pre></li>
<li>Next, let’s use the <strong class="source-inline">train_test_split()</strong> function from scikit-learn to divide the dataset we have generated into a training set and a test set:<pre class="source-code">with block('TRAIN-TEST SPLIT'):</pre><pre class="source-code">    train_df, test_df = <strong class="bold">train_test_split</strong>(</pre><pre class="source-code">        generated_df, </pre><pre class="source-code">        test_size=0.3, </pre><pre class="source-code">        random_state=0</pre><pre class="source-code">    )</pre><pre class="source-code">    print(train_df)</pre><pre class="source-code">    print(test_df)</pre></li>
<li>Lastly, add the following lines of code to save the training and test sets into their respective CSV <a id="_idIndexMarker052"/>files inside the <strong class="source-inline">tmp</strong> directory:<pre class="source-code">with block('SAVE TO CSVs'):</pre><pre class="source-code">    train_df.<strong class="bold">to_csv</strong>('tmp/<strong class="bold">bookings.train.csv</strong>', </pre><pre class="source-code">                    index=False)</pre><pre class="source-code">    test_df.<strong class="bold">to_csv</strong>('tmp/<strong class="bold">bookings.test.csv</strong>', </pre><pre class="source-code">                   index=False)</pre></li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">You can find a copy of the <strong class="source-inline">train_test_split.py</strong> file here: <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/train_test_split.py">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/train_test_split.py</a>.</p>
<ol>
<li value="7">Now that we have completed our script file, let’s run the following command in the terminal (right after the <strong class="source-inline">$</strong> sign at the bottom of the screen):<pre class="source-code">python3 <strong class="bold">train_test_split.py</strong></pre></li>
</ol>
<p class="list-inset">This should generate a set of logs similar to what is shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<img alt="Figure 1.11 – Train-test split logs   " height="762" src="image/B18638_01_011.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.11 – Train-test split logs</p>
<p class="list-inset">Here, we can see <a id="_idIndexMarker053"/>that our training dataset contains 7,000 records, while the test set contains 3,000 records. </p>
<p>With this, we can upload our dataset to <strong class="bold">Amazon S3</strong>.</p>
<h2 id="_idParaDest-29"><a id="_idTextAnchor030"/>Uploading the dataset to Amazon S3</h2>
<p>Amazon S3 is the<a id="_idIndexMarker054"/> object storage service for AWS and is where<a id="_idIndexMarker055"/> we can store different types of files, such as dataset CSV files and output artifacts. When using the different services of AWS, it is important to note that these services sometimes require the input data and files to be stored in an S3 bucket first or in a resource created using another service. </p>
<p>Uploading the dataset to S3 should be easy. Continuing where we left off in the <em class="italic">Train-test split</em> section, we will run the following commands in the terminal:</p>
<ol>
<li value="1">Run the following commands in the terminal. Here, we are going to create a new S3 bucket that will contain the data we will be using in this chapter. Make sure that you replace the value of <strong class="source-inline">&lt;INSERT BUCKET NAME HERE&gt;</strong> with a bucket name that is globally unique across all AWS users:<pre class="source-code">BUCKET_NAME="<strong class="bold">&lt;INSERT BUCKET NAME HERE&gt;</strong>"</pre><pre class="source-code">aws s3 mb s3://$BUCKET_NAME</pre></li>
</ol>
<p class="list-inset">For more information on S3 bucket naming rules, feel free to check out <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml">https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml</a>.</p>
<ol>
<li value="2">Now that the<a id="_idIndexMarker056"/> S3 bucket has been created, let’s <a id="_idIndexMarker057"/>upload the training and test datasets using the <strong class="bold">AWS CLI</strong>:<pre class="source-code">S3=<strong class="bold">s3://$BUCKET_NAME/datasets/bookings</strong></pre><pre class="source-code">TRAIN=<strong class="bold">bookings.train.csv</strong></pre><pre class="source-code">TEST=<strong class="bold">bookings.test.csv</strong></pre><pre class="source-code">aws s3 cp <strong class="bold">tmp/bookings.train.csv</strong> <strong class="bold">$S3/$TRAIN</strong></pre><pre class="source-code">aws s3 cp <strong class="bold">tmp/bookings.test.csv</strong> <strong class="bold">$S3/$TEST</strong></pre></li>
</ol>
<p>Now that everything is ready, we can proceed with the exciting part! It’s about time we perform multiple <strong class="bold">AutoML</strong> experiments using a variety of solutions and services.</p>
<h1 id="_idParaDest-30"><a id="_idTextAnchor031"/>AutoML with AutoGluon</h1>
<p>Previously, we discussed what <strong class="bold">hyperparameters</strong> are. When training and tuning ML models, it is important<a id="_idIndexMarker058"/> for us to <a id="_idIndexMarker059"/>know that the performance of an ML model depends on the algorithm, the training data, and the hyperparameter configuration that’s used when training the model. Other input configuration parameters may also affect the performance of the model, but we’ll focus on these three for now. Instead of training a single model, teams build multiple models using a variety of hyperparameter configurations. Changes and tweaks in the hyperparameter configuration affect the performance of a model – some lead to better performance, while others lead to worse performance. It takes time to try out all possible combinations of hyperparameter configurations, especially if the model tuning process is not automated.</p>
<p>These past couple of years, several libraries, frameworks, and services have allowed teams to make the <a id="_idIndexMarker060"/>most out of <strong class="bold">automated machine learning</strong> (<strong class="bold">AutoML</strong>) to automate different parts of the ML process. Initially, AutoML tools focused on automating the <strong class="bold">hyperparameter optimization</strong> (<strong class="bold">HPO</strong>) processes to obtain the optimal combination of<a id="_idIndexMarker061"/> hyperparameter values. Instead of spending hours (or even days) manually trying different combinations of hyperparameters when running training jobs, we’ll just need to configure, run, and wait for this automated program to help us find the optimal set of hyperparameter values. For years, several tools and libraries that focus on automated hyperparameter optimization were available for ML practitioners for use. After a while, other aspects and processes of the ML workflow were automated and included in the AutoML pipeline. </p>
<p>There are several tools and services available for AutoML and one of the most popular options is <strong class="bold">AutoGluon</strong>. With <strong class="bold">AutoGluon</strong>, we can train multiple models using different algorithms and evaluate them with just a few lines of code:</p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<img alt="Figure 1.12 – AutoGluon leaderboard – models trained using a variety of algorithms " height="381" src="image/B18638_01_012.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.12 – AutoGluon leaderboard – models trained using a variety of algorithms</p>
<p>Similar to what is <a id="_idIndexMarker062"/>shown in the preceding screenshot, we can also compare the generated models using a leaderboard. In this chapter, we’ll use AutoGluon with a tabular dataset. However, it is important to note that AutoGluon also supports performing AutoML tasks for text and image data. </p>
<h2 id="_idParaDest-31"><a id="_idTextAnchor032"/>Setting up and installing AutoGluon</h2>
<p>Before using AutoGluon, we <a id="_idIndexMarker063"/>need to install it. It should take a minute or so to complete the installation process:</p>
<ol>
<li value="1">Run the following commands in the terminal to install and update the prerequisites before we install AutoGluon:<pre class="source-code">python3 -m pip install -U <strong class="bold">"mxnet&lt;2.0.0"</strong></pre><pre class="source-code">python3 -m pip install <strong class="bold">numpy</strong></pre><pre class="source-code">python3 -m pip install <strong class="bold">cython</strong></pre><pre class="source-code">python3 -m pip install pyOpenSSL --upgrade</pre></li>
</ol>
<p class="list-inset">This book assumes that you are using the following versions or later: <strong class="source-inline">mxnet</strong> – <strong class="source-inline">1.9.0</strong>, <strong class="source-inline">numpy</strong> – <strong class="source-inline">1.19.5</strong>, and <strong class="source-inline">cython</strong> – <strong class="source-inline">0.29.26</strong>.</p>
<ol>
<li value="2">Next, run the following command to install <strong class="source-inline">autogluon</strong>:<pre class="source-code">python3 -m pip install <strong class="bold">autogluon</strong></pre></li>
</ol>
<p class="list-inset">This book assumes<a id="_idIndexMarker064"/> that you are using <strong class="source-inline">autogluon</strong> version <strong class="source-inline">0.3.1</strong> or later.</p>
<p class="callout-heading">Important note</p>
<p class="callout">This step may take around 5 to 10 minutes to complete. Feel free to grab a cup of coffee or tea!</p>
<p>With AutoGluon installed in our Cloud9 environment, let’s proceed with our first AutoGluon AutoML experiment.</p>
<h2 id="_idParaDest-32"><a id="_idTextAnchor033"/>Performing your first AutoGluon AutoML experiment</h2>
<p>If you <a id="_idIndexMarker065"/>have used <strong class="bold">scikit-learn</strong> or other ML libraries and frameworks before, using AutoGluon should be easy and fairly straightforward since it uses a very similar set of methods, such as <strong class="source-inline">fit()</strong> and <strong class="source-inline">predict()</strong>. Follow these steps:</p>
<ol>
<li value="1">To start, run the following command in the terminal:<pre class="source-code"><strong class="bold">ipython</strong></pre></li>
</ol>
<p class="list-inset">This will open the <strong class="bold">IPython</strong> <strong class="bold">Read-Eval-Print-Loop</strong> (<strong class="bold">REPL</strong>)/interactive shell. We will use this similar to how we use the <strong class="bold">Python shell</strong>.</p>
<ol>
<li value="2">Inside the console, type in (or copy) the following block of code. Make sure that you press <em class="italic">Enter</em> after typing the closing parenthesis:<pre class="source-code">from autogluon.tabular import (</pre><pre class="source-code">    <strong class="bold">TabularDataset</strong>,</pre><pre class="source-code">    <strong class="bold">TabularPredictor</strong></pre><pre class="source-code">)</pre></li>
<li>Now, let’s<a id="_idIndexMarker066"/> load the synthetic data stored in the <strong class="source-inline">bookings.train.csv</strong> and <strong class="source-inline">bookings.test.csv</strong> files into the <strong class="source-inline">train_data</strong> and <strong class="source-inline">test_data</strong> variables, respectively, by running the following statements:<pre class="source-code">train_loc = 'tmp/bookings.train.csv'</pre><pre class="source-code">test_loc = 'tmp/bookings.test.csv'</pre><pre class="source-code"><strong class="bold">train_data</strong> = TabularDataset(train_loc)</pre><pre class="source-code"><strong class="bold">test_data</strong> = TabularDataset(test_loc)</pre></li>
</ol>
<p class="list-inset">Since the parent class of AutoGluon, <strong class="source-inline">TabularDataset</strong>, is a pandas DataFrame, we can use different methods on <strong class="source-inline">train_data</strong> and <strong class="source-inline">test_data</strong> such as <strong class="source-inline">head()</strong>, <strong class="source-inline">describe()</strong>, <strong class="source-inline">memory_usage()</strong>, and more.</p>
<ol>
<li value="4">Next, run the following lines of code:<pre class="source-code">label = '<strong class="bold">is_cancelled</strong>'</pre><pre class="source-code">save_path = '<strong class="bold">tmp</strong>'</pre><pre class="source-code">tp = TabularPredictor(label=label, path=save_path)</pre><pre class="source-code">predictor = tp.<strong class="bold">fit</strong>(train_data)</pre></li>
</ol>
<p class="list-inset">Here, we specify <strong class="source-inline">is_cancelled</strong> as the target variable of the AutoML task and the <strong class="source-inline">tmp</strong> directory as the location where the generated models will be stored. This block of code will use the training data we have provided to train multiple models using different algorithms. AutoGluon will automatically detect that we are dealing with a binary classification problem and generate multiple binary classifier models using a variety of ML algorithms.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Inside the <strong class="source-inline">tmp/models</strong> directory, we should find <strong class="source-inline">CatBoost</strong>, <strong class="source-inline">ExtraTreesEntr</strong>, and <strong class="source-inline">ExtraTreesGini</strong>, along with other directories corresponding to the algorithms used in the AutoML task. Each of these directories contains a <strong class="source-inline">model.pkl</strong> file that contains the serialized model. Why do we have multiple models? Behind the scenes, AutoGluon runs a significant number of training experiments using a variety of algorithms, along with different combinations of hyperparameter values, to produce the “best” model. The “best” model is selected using a certain evaluation metric that helps identify which model performs better than the rest. For example, if the evaluation metric that’s used is <em class="italic">accuracy</em>, then a model with an accuracy score of 90% (which gets 9 correct answers every 10 tries) is “better” than a model with an accuracy score of 80% (which gets 8 correct answers every 10 tries). That said, once the models have been generated and evaluated, AutoGluon simply chooses the model with the highest evaluation metric value (for example, <em class="italic">accuracy</em>) and tags it as the “best model.”</p>
<ol>
<li value="5">Now that we have our “best model” ready, what do we do next? The next step is for us to evaluate the “best model” using the test dataset. That said, let’s prepare the<a id="_idIndexMarker067"/> test dataset for inference by removing the target label:<pre class="source-code"><strong class="bold">y_test</strong> = test_data[label]</pre><pre class="source-code"><strong class="bold">test_data_no_label</strong> = test_data.drop(columns=[label])</pre></li>
<li>With everything ready, let’s use the <strong class="source-inline">predict()</strong> method to predict the <strong class="source-inline">is_cancelled</strong> column value of the test dataset provided as the payload:<pre class="source-code"><strong class="bold">y_pred</strong> = predictor.<strong class="bold">predict</strong>(test_data_no_label)</pre></li>
<li>Now that we have the actual <em class="italic">y</em> values (<strong class="source-inline">y_test</strong>) and the predicted <em class="italic">y</em> values (<strong class="source-inline">y_pred</strong>), let’s quickly check the performance of the trained model by using the <strong class="source-inline">evaluate_predictions()</strong> method:<pre class="source-code">predictor.<strong class="bold">evaluate_predictions</strong>(</pre><pre class="source-code">    y_true=<strong class="bold">y_test</strong>, </pre><pre class="source-code">    y_pred=<strong class="bold">y_pred</strong>, </pre><pre class="source-code">    auxiliary_metrics=True</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">The previous block of code should yield performance metric values similar to the following:</p>
<pre class="list-inset1 source-code">{'<strong class="bold">accuracy</strong>': 0.691...,
 '<strong class="bold">balanced_accuracy</strong>': 0.502...,
 '<strong class="bold">mcc</strong>': 0.0158...,
 '<strong class="bold">f1</strong>': 0.0512...,
 '<strong class="bold">precision</strong>': 0.347...,
 '<strong class="bold">recall</strong>': 0.0276...}</pre>
<p class="list-inset">In this step, we compare the actual values with the predicted values for the target column using a variety of formulas that compare how close these values are to each other. Here, the goal of the trained models is to make “the least number of mistakes” as possible over unseen data. Better models generally have better scores for performance metrics such as <strong class="bold">accuracy</strong>, <strong class="bold">Matthews correlation coefficient</strong> (<strong class="bold">MCC</strong>), and <strong class="bold">F1-score</strong>. We won’t go into the details of how model performance metrics work here. Feel free to check out <a href="https://bit.ly/3zn2crv">https://bit.ly/3zn2crv</a> for more information.</p>
<ol>
<li value="8">Now that we are <a id="_idIndexMarker068"/>done with our quick experiment, let’s exit the <strong class="bold">IPython</strong> shell:<pre class="source-code">exit()</pre></li>
</ol>
<p>There’s more we can do<a id="_idIndexMarker069"/> using AutoGluon but this should help us appreciate how easy it is to use AutoGluon for AutoML experiments. There are other methods we can use, such as <strong class="source-inline">leaderboard()</strong>, <strong class="source-inline">get_model_best()</strong>, and <strong class="source-inline">feature_importance()</strong>, so feel free to check out <a href="https://auto.gluon.ai/stable/index.xhtml">https://auto.gluon.ai/stable/index.xhtml</a> for more information.</p>
<h1 id="_idParaDest-33"><a id="_idTextAnchor034"/>Getting started with SageMaker and SageMaker Studio</h1>
<p>When performing ML and ML engineering on AWS, professionals should consider using one or more of the capabilities and features of <strong class="bold">Amazon SageMaker</strong>. If this is your first time learning about<a id="_idIndexMarker070"/> SageMaker, it is a fully managed ML service that helps significantly speed up the process of preparing, training, evaluating, and deploying ML models. </p>
<p>If you are wondering what these capabilities are, check out some of the capabilities tagged under <strong class="bold">ML SERVICES</strong> in <em class="italic">Figure 1.2</em> from the <em class="italic">How ML engineers can get the most out of AWS</em> section. We will tackle several capabilities of SageMaker as we go through the different chapters of this book. In the meantime, we will start with SageMaker Studio as we will need to set it up first before we work on the SageMaker Canvas and SageMaker Autopilot examples.</p>
<h2 id="_idParaDest-34"><a id="_idTextAnchor035"/>Onboarding with SageMaker Studio</h2>
<p><strong class="bold">SageMaker Studio</strong> provides <a id="_idIndexMarker071"/>a feature-rich IDE for ML practitioners. One of the great things about SageMaker Studio is its tight integration with the other capabilities of SageMaker, which allows us to manage different SageMaker resources by just using the interface.</p>
<p>For us to have a good idea of what it looks like and how it works, let’s proceed with setting up and configuring SageMaker Studio: </p>
<ol>
<li value="1">In the<a id="_idIndexMarker072"/> search bar of <a id="_idIndexMarker073"/>the AWS console, type <strong class="source-inline">sagemaker studio</strong>. Select <strong class="bold">SageMaker Studio</strong> under <strong class="bold">Features</strong>.</li>
<li>Choose <strong class="bold">Standard setup</strong>, as shown in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer019">
<img alt="Figure 1.13 – Setup SageMaker Domain " height="491" src="image/B18638_01_013.jpg" width="1114"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.13 – Setup SageMaker Domain</p>
<p class="list-inset">As we <a id="_idIndexMarker074"/>can see, <strong class="bold">Standard setup</strong> should give us more <a id="_idIndexMarker075"/>configuration options to tweak over <strong class="bold">Quick setup</strong>. Before clicking the <strong class="bold">Configure</strong> button, make sure that you are using the same region where the S3 bucket and training and test datasets are located.</p>
<ol>
<li value="3">Under <strong class="bold">Authentication</strong>, select <strong class="bold">AWS Identity and Access Management (IAM)</strong>. For the default execution role under <strong class="bold">Permission</strong>, choose <strong class="bold">Create a new role</strong>. Choose <strong class="bold">Any S3 bucket</strong>. Then, click <strong class="bold">Create role</strong>. </li>
<li>Under <strong class="bold">Network and Storage Section</strong>, select the default VPC and choose a subnet (for example, <strong class="source-inline">us-west-2a</strong>), similar to what is shown in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer020">
<img alt="Figure 1.14 – Network and Storage Section " height="596" src="image/B18638_01_014.jpg" width="684"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.14 – Network and Storage Section</p>
<p class="list-inset">Here, we<a id="_idIndexMarker076"/> have also configured the SageMaker Domain to <a id="_idIndexMarker077"/>use the default SageMaker internet access by selecting <strong class="bold">Public Internet Only</strong>. Under <strong class="bold">Encryption key</strong>, we leave this unchanged by choosing <strong class="bold">No Custom Encryption</strong>. Review the configuration and then click <strong class="bold">Next</strong>.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Note that for production environments, the security configuration specified in the last few steps needs to be reviewed and upgraded further. In the meantime, this should do the trick since we’re dealing with a sample dataset. We will discuss how to secure environments in detail in <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a><em class="italic">, Security, Governance, and Compliance Strategies</em>.</p>
<ol>
<li value="5">Under <strong class="bold">Studio settings</strong>, leave everything as-is and click <strong class="bold">Next</strong>.</li>
<li>Similarly, under <strong class="bold">General settings</strong> | <strong class="bold">RStudio Workbench</strong>, click <strong class="bold">Submit</strong>.</li>
</ol>
<p>Once you have <a id="_idIndexMarker078"/>completed these steps, you should see the <strong class="bold">Preparing SageMaker Domain</strong> loading <a id="_idIndexMarker079"/>message. This step should take around 3 to 5 minutes to complete. Once complete, you should see a notification stating <strong class="bold">The SageMaker Domain is ready</strong>.</p>
<h2 id="_idParaDest-35"><a id="_idTextAnchor036"/>Adding a user to an existing SageMaker Domain</h2>
<p>Now that<a id="_idIndexMarker080"/> our <strong class="bold">SageMaker Domain</strong> is ready, let’s <a id="_idIndexMarker081"/>create a user. Creating a user is straightforward. So, let’s begin: </p>
<ol>
<li value="1">On the <strong class="bold">SageMaker Domain/Control Panel</strong> page, click <strong class="bold">Add user</strong>.</li>
<li>Specify the name of the user under <strong class="bold">Name</strong>. Under <strong class="bold">Default execution role</strong>, select the execution role that you created in the previous step. Click <strong class="bold">Next</strong>.</li>
<li>Under <strong class="bold">Studio settings</strong> | <strong class="bold">SageMaker Projects and JumpStart</strong>, click <strong class="bold">Next.</strong></li>
<li>Under <strong class="bold">RStudio settings</strong> | <strong class="bold">Rstudio Workbench</strong>, click <strong class="bold">Submit.</strong></li>
</ol>
<p>This should do the trick for now. In <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a><em class="italic">, Security, Governance, and Compliance Strategies</em>, we will review how we can improve the configuration here to improve the security of our environment. </p>
<h1 id="_idParaDest-36"><a id="_idTextAnchor037"/>No-code machine learning with SageMaker Canvas</h1>
<p>Before<a id="_idIndexMarker082"/> we proceed with using the more<a id="_idIndexMarker083"/> comprehensive set of SageMaker capabilities to perform ML experiments and deployments, let’s start by building a model using <strong class="bold">SageMaker Canvas</strong>. One of the great things about SageMaker Canvas is that no coding work is needed to build models and use them to perform predictions. Of course, <strong class="bold">SageMaker Autopilot</strong> would<a id="_idIndexMarker084"/> have a more powerful and flexible set of features, but SageMaker Canvas should help business analysts, data scientists, and junior ML engineers understand the ML process and get started building models right away.</p>
<p>Since our dataset has already been uploaded to the S3 bucket, we can start building and training our first SageMaker Canvas model:</p>
<ol>
<li value="1">On the <strong class="bold">SageMaker Domain/Control Panel</strong> page, locate the row of the user we just created and click <strong class="bold">Launch app</strong>. Choose <strong class="bold">Canvas</strong> from the list of options available in the drop-down menu, as shown in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer021">
<img alt="Figure 1.15 – Launching SageMaker Canvas " height="661" src="image/B18638_01_015.jpg" width="1589"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.15 – Launching SageMaker Canvas</p>
<p class="list-inset">As we can see, we <a id="_idIndexMarker085"/>can launch SageMaker<a id="_idIndexMarker086"/> Canvas from the <strong class="bold">SageMaker Domain/Control Panel</strong> page. We can launch SageMaker Studio here as well, which we’ll do later in this chapter.</p>
<ol>
<li value="2">Click <strong class="bold">New model</strong>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer022">
<img alt="Figure 1.16 – The SageMaker Canvas Models page " height="1096" src="image/B18638_01_016.jpg" width="1346"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.16 – The SageMaker Canvas Models page</p>
<p class="list-inset">Here, we have the SageMaker Canvas <strong class="bold">Models</strong> page, which should list the models we have trained. Since we have not trained anything yet, we should see the <strong class="bold">You haven’t created any models yet</strong> message.</p>
<ol>
<li value="3">In<a id="_idIndexMarker087"/> the <strong class="bold">Create new model</strong> popup <a id="_idIndexMarker088"/>window, specify the name of the model (for example, <strong class="source-inline">first-model</strong>) and click <strong class="bold">Create</strong>.</li>
<li>When you see the <strong class="bold">Getting Started</strong> guide window, click <strong class="bold">Skip intro</strong>.</li>
<li>Click <strong class="bold">Import data to canvas</strong>. Locate the S3 bucket we created earlier in the <em class="italic">Uploading the dataset to S3</em> section. After that, locate the <strong class="source-inline">booking.train.csv</strong> and <strong class="source-inline">booking.test.csv</strong> files inside the <strong class="source-inline">Amazon S3/&lt;S3 BUCKET&gt;/datasets/bookings</strong> folder of the S3 bucket.</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer023">
<img alt="Figure 1.17 – Choose files to import " height="982" src="image/B18638_01_017.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.17 – Choose files to import</p>
<p class="list-inset">Select the necessary CSV files, as shown in the preceding screenshot, and click <strong class="bold">Import data</strong>.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Note that you may have a hard time locating the S3 bucket we created in the <em class="italic">Uploading the dataset to S3</em> section if you have a significant number of S3 buckets in your account. Feel free to use the search box (with the <strong class="bold">Search Amazon S3</strong> placeholder) located on the right-hand side, just above the table that lists the different S3 buckets and resources.</p>
<ol>
<li value="6">Once the<a id="_idIndexMarker089"/> files have been imported, click<a id="_idIndexMarker090"/> the radio button of the row that contains <strong class="source-inline">bookings.train.csv</strong>. Click <strong class="bold">Select dataset</strong>.</li>
<li>In the <strong class="bold">Build</strong> tab, click and open the <strong class="bold">Target column</strong> drop-down under <strong class="bold">Select a column to predict</strong>. Select <strong class="source-inline">is_cancelled</strong> from the list of drop-down options for the <strong class="bold">Target column</strong> field.</li>
<li>Next, click <strong class="bold">Preview model</strong> (under the <strong class="bold">Quick build</strong> button), as highlighted in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer024">
<img alt="Figure 1.18 – The Build tab " height="499" src="image/B18638_01_018.jpg" width="810"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.18 – The Build tab</p>
<p class="list-inset">After a few minutes, we should get an estimated accuracy of around 70%. Note that you might get a different set of numbers in this step.</p>
<ol>
<li value="9">Click <strong class="bold">Quick build</strong> and wait for the model to be ready.</li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">This step may take up to 15 minutes to complete. While waiting, let’s quickly discuss the difference between <strong class="bold">Quick build</strong> and <strong class="bold">Standard build</strong>. Quick build uses fewer records for training and generally lasts around 2 to 15 minutes, while Standard build lasts much longer – generally around 2 to 4 hours. It is important to note that models that are trained using Quick build can’t be shared with other data scientists or ML engineers in SageMaker Studio. On the other hand, models trained using Standard build can be shared after the build has been completed.</p>
<ol>
<li value="10">Once the <a id="_idIndexMarker091"/>results are available, you <a id="_idIndexMarker092"/>may open the <strong class="bold">Scoring</strong> tab by clicking the tab highlighted in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer025">
<img alt="Figure 1.19 – The Analyze tab " height="504" src="image/B18638_01_019.jpg" width="810"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.19 – The Analyze tab</p>
<p class="list-inset">We should see a quick chart showing the number of records that were used to analyze the model, along with the number of correct versus incorrect predictions the model has made.</p>
<p class="callout-heading">Important note</p>
<p class="callout">At this point, we have built an ML model that we can use to predict whether a booking will be cancelled or not. Since the accuracy score in this example is only around 70%, we’re expecting the model to get about 7 correct answers every 10 tries. In <a href="B18638_11.xhtml#_idTextAnchor231"><em class="italic">Chapter 11</em></a><em class="italic">, Machine Learning Pipelines with SageMaker Pipelines</em>, we will train an improved version of this model with an accuracy score of around 88%.</p>
<ol>
<li value="11">Once we are done checking the different numbers and charts in the <strong class="bold">Analyze</strong> tab, we can proceed by clicking the <strong class="bold">Predict</strong> button.</li>
<li>Click <strong class="bold">Select dataset</strong>. Under <strong class="bold">Select dataset for predictions</strong>, choose <strong class="source-inline">bookings.test.csv</strong> and click <strong class="bold">Generate predictions</strong>.</li>
<li>Once<a id="_idIndexMarker093"/> the <strong class="bold">Status</strong> column value is set to <strong class="bold">Ready</strong>, hover <a id="_idIndexMarker094"/>over the <strong class="bold">Status</strong> column of the row, click the 3 dots (which will appear after hovering over the row), and then select <strong class="bold">Preview</strong> from the list of options:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer026">
<img alt="Figure 1.20 – Batch prediction results " height="984" src="image/B18638_01_020.jpg" width="1639"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.20 – Batch prediction results</p>
<p class="list-inset">We should see a table of values, similar to what is shown in the preceding screenshot. In the first column, we should have the predicted values for the <strong class="source-inline">is_cancelled</strong> field for each of the rows of our test dataset. In the second column, we should find the probability of the prediction being correct.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Note that we can also perform a single prediction by using the interface provided after clicking <strong class="bold">Single prediction</strong> under <strong class="bold">Predict target values</strong>.</p>
<ol>
<li value="14">Finally, let’s <a id="_idIndexMarker095"/>log out of our session. Click the <strong class="bold">Account</strong> icon in <a id="_idIndexMarker096"/>the left sidebar and select the <strong class="bold">Log out</strong> option. </li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">Make sure that you always log out of the current session after using SageMaker Canvas to avoid any unexpected charges. For more information, go to <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-log-out.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-log-out.xhtml</a>.</p>
<p>Wasn’t that easy? Now that we have a good idea of how to use SageMaker Canvas, let’s run an AutoML experiment using SageMaker Autopilot.</p>
<h1 id="_idParaDest-37"><a id="_idTextAnchor038"/>AutoML with SageMaker Autopilot</h1>
<p><strong class="bold">SageMaker Autopilot</strong> allows <a id="_idIndexMarker097"/>ML practitioners to build high-quality ML models without having to write a single line of code. Of course, it is possible to programmatically configure, run, and manage SageMaker Autopilot experiments using the <strong class="bold">SageMaker Python SDK</strong>, but <a id="_idIndexMarker098"/>we will focus on using the SageMaker Studio interface to run the AutoML experiment. Before jumping into configuring our first Autopilot experiment, let’s see what happens behind the scenes:</p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<img alt="Figure 1.21 – AutoML with SageMaker Autopilot   " height="400" src="image/B18638_01_021.jpg" width="884"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.21 – AutoML with SageMaker Autopilot</p>
<p>In the preceding <a id="_idIndexMarker099"/>diagram, we can see the different steps that are performed by SageMaker Autopilot when we run the AutoML experiment. It starts with the <strong class="bold">data pre-processing</strong> step and proceeds with the <strong class="bold">generation of candidate models</strong> (pipeline and algorithm pair) step. Then, it continues to perform the <strong class="bold">feature engineering</strong> and <strong class="bold">model tuning</strong> steps, which would yield multiple trained models from different model families, hyperparameter values, and model performance metric values. The generated model with the best performance metric values is tagged as the “best model” by the Autopilot job. Next, two reports are generated: the <strong class="bold">explainability report</strong> and the <strong class="bold">insights report</strong>. Finally, the model is deployed to an inference endpoint.</p>
<p>Let’s dive a bit deeper into what is happening in each step:</p>
<ul>
<li><strong class="bold">Data pre-processing</strong>: Data is cleaned automatically and missing values are automatically imputed.</li>
<li><strong class="bold">Candidate definition generation</strong>: Multiple “candidate definitions” (composed of a data processing job and a training job) are generated, all of which will be used on the dataset. </li>
<li><strong class="bold">Feature engineering</strong>: Here, data transformations are applied to perform automated feature engineering.</li>
<li><strong class="bold">Model tuning</strong>: The <strong class="bold">Automatic Model Tuning</strong> (hyperparameter tuning) capability of SageMaker is used to generate multiple models using a variety of hyperparameter configuration values to find the “best model.”</li>
<li><strong class="bold">Explainability report generation</strong>: The model explainability report, which makes use of <a id="_idIndexMarker100"/>SHAP values to help explain the behavior of the generated model, is generated using tools provided by <strong class="bold">SageMaker Clarify</strong> (another capability of SageMaker focused on AI <strong class="bold">fairness</strong> and <strong class="bold">explainability</strong>). We’ll dive a bit deeper into this topic later in <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a>, <em class="italic">Security, Governance, and Compliance Strategies</em>.</li>
<li><strong class="bold">Insights report generation</strong>: The insights report, which includes data insights such as scalar metrics, which help us understand our dataset better, is generated.</li>
<li><strong class="bold">Model deployment</strong>: The best model is deployed to a dedicated inference endpoint. Here, the value of the objective metric is used to determine which is the best model out of all the models trained during the model tuning step.</li>
</ul>
<p class="callout-heading">Important note</p>
<p class="callout">If you are wondering if AutoML solutions would fully “replace” data scientists, then a quick answer to your question would be “no” or “not anytime soon.” There are specific areas of the ML process that require domain knowledge to be available to data scientists. AutoML solutions help provide a good starting point that data scientists and ML practitioners can build on top of. For example, white box AutoML solutions such as SageMaker Autopilot can generate scripts and notebooks that can be modified by data scientists and ML practitioners to produce custom and complex data processing, experiment, and deployment flows and pipelines. </p>
<p>Now that we have a better idea of what happens during an Autopilot experiment, let’s run our first Autopilot experiment:</p>
<ol>
<li value="1">On the <strong class="bold">Control Panel</strong> page, click the <strong class="bold">Launch app</strong> drop-down menu and choose <strong class="bold">Studio</strong> from the list of drop-down options, as shown in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer028">
<img alt="Figure 1.22 – Opening SageMaker Studio " height="667" src="image/B18638_01_022.jpg" width="1495"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.22 – Opening SageMaker Studio</p>
<p class="list-inset">Note that it may<a id="_idIndexMarker101"/> take around 5 minutes for <strong class="bold">SageMaker Studio</strong> to load if this is your first time opening it.</p>
<p class="callout-heading">Important note</p>
<p class="callout">AWS releases updates and upgrades for SageMaker Studio regularly. To ensure that you are using the latest version, make sure that you shut down and update SageMaker Studio and Studio Apps. For more information, go to <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update.xhtml</a>.</p>
<ol>
<li value="2">Open the <strong class="bold">File</strong> menu and click <strong class="bold">Experiment</strong> under the <strong class="bold">New</strong> submenu:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer029">
<img alt="Figure 1.23 – Using the File menu to create a new experiment   " height="550" src="image/B18638_01_023.jpg" width="1382"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.23 – Using the File menu to create a new experiment</p>
<p class="list-inset">Here, we have multiple options under the <strong class="bold">New</strong> submenu. We will explore the other options throughout this book. </p>
<p>In the next set of<a id="_idIndexMarker102"/> steps, we will configure the Autopilot experiment, similar to what is shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<img alt="Figure 1.24 – Configuring the Autopilot experiment " height="836" src="image/B18638_01_024.jpg" width="1635"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.24 – Configuring the Autopilot experiment</p>
<p class="list-inset">Here, we can see the different configuration options that are available before running the Autopilot experiment. Note that the actual Autopilot experiment settings form only has a single column instead of two.</p>
<ol>
<li value="1">Specify <a id="_idIndexMarker103"/>the <strong class="bold">Experiment name</strong> value (for example, <strong class="source-inline">first-automl-job</strong>).</li>
<li>Under <strong class="bold">Input data</strong>, locate and select the <strong class="source-inline">bookings.train.csv</strong> we uploaded earlier by clicking <strong class="bold">Browse</strong>.</li>
<li>In the <strong class="bold">Target</strong> drop-down menu, choose <strong class="bold">is_cancelled</strong>. Click<strong class="bold"> Next: Training method</strong>. </li>
<li>Leave everything else as is, and then click <strong class="bold">Next</strong>:<strong class="bold"> Deployment and advanced settings</strong>.</li>
<li>Make sure that the <strong class="bold">Auto deploy</strong>? configuration is set to Yes.</li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">You may opt to set the <strong class="bold">Auto deploy</strong> configuration to <strong class="bold">No</strong> instead so that an inference endpoint will not be created by the Autopilot job. If you have set this to <strong class="bold">Yes</strong> make sure that you delete the inference endpoint if you are not using it.</p>
<ol>
<li value="6">Under <strong class="bold">Advanced Settings</strong> (<strong class="bold">optional</strong>) <strong class="bold">&gt; Runtime</strong>, set <strong class="bold">Max Candidates</strong> to <strong class="bold">20</strong> (or alternatively, setting both <strong class="bold">Max trial runtime Minutes</strong> and <strong class="bold">Max job runtime Minutes</strong> to <strong class="bold">20</strong>). Click <strong class="bold">Next: Review and create</strong>. </li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">Setting the value for <strong class="bold">Max Candidates</strong> to <strong class="source-inline">20</strong> means that Autopilot will train and consider only 20 candidate models for this Autopilot job. Of course, we can set this to a higher number, which would increase the chance of finding a candidate with a higher evaluation metric score (for example, a model that performs better). However, this would mean that it would take longer for Autopilot to run since we’ll be running more training jobs. Since we are just trying out this capability, we should be fine setting <strong class="bold">Max Candidates</strong> to <strong class="source-inline">20</strong> in the meantime.</p>
<ol>
<li value="7">Review <a id="_idIndexMarker104"/>all the configuration parameters we have set in the previous steps and click <strong class="bold">Create experiment</strong>. When asked if you want to auto-deploy the best model, click <strong class="bold">Confirm</strong>. Once the AutoML job has started, we should see a loading screen similar to the following:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer031">
<img alt="Figure 1.25 – Waiting for the AutoML job to complete " height="617" src="image/B18638_01_025.jpg" width="1649"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.25 – Waiting for the AutoML job to complete</p>
<p class="list-inset">Here, we can see that the Autopilot job involves the following steps: </p>
<ol>
<li><strong class="bold">Pre-processing </strong></li>
<li><strong class="bold">Candidate Definitions Generated</strong></li>
<li><strong class="bold">Feature Engineering</strong></li>
<li><strong class="bold">Model Tuning </strong></li>
<li><strong class="bold">Explainability Report Generated </strong></li>
<li><strong class="bold">Insights Report Generated</strong></li>
<li><strong class="bold">Deploying Model</strong></li>
</ol>
<p class="list-inset">If we have set the <strong class="bold">Auto deploy</strong> configuration to <strong class="bold">Yes,</strong> the best model is deployed automatically into an inference endpoint that will run 24/7. </p>
<p class="callout-heading">Important note</p>
<p class="callout">This step may take around 30 minutes to 1 hour to complete. Feel free to get a cup of coffee or tea!</p>
<p class="list-inset">After about <a id="_idIndexMarker105"/>an hour, we should see a list of trials, along with several models that have been generated by multiple training jobs, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer032">
<img alt="Figure 1.26 – Autopilot job results " height="927" src="image/B18638_01_026.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.26 – Autopilot job results</p>
<p class="list-inset">We should also see two buttons on the top right-hand side of the page: <strong class="bold">Open candidate generation notebook</strong> and <strong class="bold">Open data exploration notebook</strong>. Since these two notebooks are generated early in the process, we may see the buttons appear about 10 to 15 minutes after the experiment started.</p>
<ol>
<li value="8">Click the <strong class="bold">Open candidate generation notebook</strong> and <strong class="bold">Open data exploration notebook</strong> buttons to open the notebooks that were generated by SageMaker <a id="_idIndexMarker106"/>Autopilot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer033">
<img alt="Figure 1.27 – The Data Exploration Report (left) and the Candidate Definition Notebook (right) " height="657" src="image/B18638_01_027.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.27 – The Data Exploration Report (left) and the Candidate Definition Notebook (right)</p>
<p class="list-inset">Here, we can see the <strong class="bold">Data Exploration Report</strong> on the left-hand side and the <strong class="bold">Candidate Definition Notebook</strong> on the right. The <strong class="bold">Data Exploration Report</strong> helps data scientists and ML engineers identify issues in the given dataset. It contains a column analysis report that shows the percentage of missing values, along with some count statistics and descriptive statistics. On the other hand, the <strong class="bold">Candidate Definition Notebook</strong> contains the suggested ML algorithm, along with the prescribed hyperparameter ranges. In addition to these, it contains the recommended pre-processing steps before the training step starts.</p>
<p class="list-inset">The great thing about these generated notebooks is that we can modify certain sections of these notebooks as needed. This makes SageMaker Autopilot easy for beginners to use while still allowing intermediate users to customize certain parts of the AutoML process.</p>
<p class="callout-heading">Important note</p>
<p class="callout">If you want to know more about SageMaker Autopilot, including the output artifacts generated by the AutoML experiment, check out <a href="B18638_06.xhtml#_idTextAnchor132"><em class="italic">Chapter 6</em></a>, <em class="italic">SageMaker Training and Debugging Solutions</em>, of the book <em class="italic">Machine Learning with Amazon SageMaker Cookbook</em>. You should find several recipes there that focus on programmatically running and managing an Autopilot experiment using the <strong class="bold">SageMaker Python SDK</strong>.</p>
<ol>
<li value="9">Navigate <a id="_idIndexMarker107"/>back to the tab containing the results of the Autopilot job. Right-click on the row with the <strong class="bold">Best Model</strong> tag and choose <strong class="bold">Open in model details</strong> from the options in the context menu. This should open a page similar to what is shown in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer034">
<img alt="Figure 1.28 – The model details page " height="947" src="image/B18638_01_028.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.28 – The model details page</p>
<p class="list-inset">Here, we can see that <strong class="bold">reserved_room_type, lead_time, and adr</strong> are the most important features that affect the chance of a hotel booking getting canceled. </p>
<p class="callout-heading">Note</p>
<p class="callout">Note that you may get a different set of results from what we have in this section.</p>
<p class="list-inset">We should<a id="_idIndexMarker108"/> see the following information on the model details page as well: </p>
<ul>
<li>Problem type </li>
<li>Algorithm used</li>
<li>Location of the input and output artifacts</li>
<li>Model metric values </li>
<li>Hyperparameter values used to train the model</li>
</ul>
<p class="callout-heading">Important note</p>
<p class="callout">Make sure that you delete the inference endpoint(s) created after running the SageMaker Autopilot experiment. To find the running inference endpoints, simply navigate to <a href="https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints">https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints</a> and manually delete the unused resources. Note that the link provided assumes that the inference endpoint has been created in the <strong class="bold">Oregon</strong> (<strong class="bold">us-west-2</strong>) region. We will skip performing sample predictions using the inference endpoint for now. We will cover this, along with deployment strategies, in <a href="B18638_07.xhtml#_idTextAnchor151"><em class="italic">Chapter 7</em></a>,<em class="italic"> SageMaker Deployment Solutions</em>. </p>
<p>At this point, we should have a good grasp of how to use several AutoML solutions such as <strong class="bold">AutoGluon</strong>, <strong class="bold">SageMaker Canvas</strong>, and <strong class="bold">SageMaker Autopilot</strong>. As we saw in the hands-on solutions of this section, we have a significant number of options when using SageMaker Autopilot to influence the process of finding the best model. If we are more comfortable with a simpler UI with fewer options, then we may use SageMaker Canvas instead. If we are more comfortable developing and engineering ML solutions through code, then we can consider using AutoGluon as well.</p>
<h1 id="_idParaDest-38"><a id="_idTextAnchor039"/>Summary</h1>
<p>In this chapter, we got our feet wet by performing multiple AutoML experiments using a variety of services, capabilities, and tools on AWS. This included using AutoGluon within a Cloud9 environment and SageMaker Canvas and SageMaker Autopilot to run AutoML experiments. The solutions presented in this chapter helped us have a better understanding of the fundamental ML and ML engineering concepts as well. We were able to see some of the steps in the ML process in action, such as EDA, train-test split, model training, evaluation, and prediction. </p>
<p>In the next chapter, we will focus on how the <strong class="bold">AWS Deep Learning AMIs</strong> help speed up the ML experimentation process. We will also take a closer look at how AWS pricing works for EC2 instances so that we are better equipped when managing the overall cost of running ML workloads in the cloud. </p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor040"/>Further reading</h1>
<p>For more information regarding the topics that were covered in this chapter, check out the following resources:</p>
<ul>
<li><em class="italic">AutoGluon: AutoML for Text, Image, and Tabular Data</em> (<a href="https://auto.gluon.ai/stable/index.xhtml">https://auto.gluon.ai/stable/index.xhtml</a>)</li>
<li><em class="italic">Automate model development with Amazon SageMaker Autopilot</em> (https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.xhtml)</li>
<li><em class="italic">SageMaker Canvas Pricing</em> (https://aws.amazon.com/sagemaker/canvas/pricing/)</li>
<li><em class="italic">Machine Learning with Amazon SageMaker Cookbook</em>, by Joshua Arvin Lat (<a href="https://www.amazon.com/Machine-Learning-Amazon-SageMaker-Cookbook/dp/1800567030/">https://www.amazon.com/Machine-Learning-Amazon-SageMaker-Cookbook/dp/1800567030/</a>)</li>
</ul>
</div>
<div>
<div id="_idContainer036">
</div>
</div>
</div>
</body></html>