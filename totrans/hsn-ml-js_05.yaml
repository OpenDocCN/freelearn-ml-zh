- en: Classification Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类算法
- en: 'Classification problems involve detecting patterns in data and using those
    patterns to assign a data point to a group of similar data points. If that''s
    too abstract, here are some examples of classification problems: analyzing an
    email to determine whether it''s spam; detecting the language of a piece of text;
    reading an article and categorizing it as finance, sports, politics, opinion pieces,
    or crime; and determining whether a review of your product posted on Twitter is
    positive or negative (this last example is commonly called **sentiment analysis**).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题涉及在数据中检测模式，并使用这些模式将数据点分配到一组相似的数据点中。如果这还不够具体，这里有一些分类问题的例子：分析一封电子邮件以确定它是否为垃圾邮件；检测一段文本的语言；阅读一篇文章并将其分类为财经、体育、政治、观点文章或犯罪；以及确定你在Twitter上发布的关于产品的评论是正面还是负面（这个最后的例子通常被称为**情感分析**）。
- en: Classification algorithms are tools that solve classification problems. By definition,
    they are supervised learning algorithms, as they'll always need a labeled training
    set to build a model from. There are lots of classification algorithms, each designed
    with a specific principle in mind or for a particular type of input data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 分类算法是解决分类问题的工具。根据定义，它们是监督学习算法，因为它们始终需要一个标记的训练集来构建模型。有许多分类算法，每个都是基于特定的原则设计的，或者针对特定类型的输入数据。
- en: 'In this chapter, we''ll discuss four classifiers: **k-Nearest Neighbors** (**KNN**),
    Naive Bayes, **Support Vector Machines** (**SVMs**), and random forest. Here''s
    a brief introduction to each of the algorithms:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论四种分类器：**k-最近邻**（**KNN**）、朴素贝叶斯、**支持向量机**（**SVMs**）和随机森林。以下是每个算法的简要介绍：
- en: The KNN algorithm is one of the simplest classifiers, and works well when your
    dataset has numerical features and clustered patterns. It is similar in nature
    to the k-means clustering algorithm, in that it relies on plotting data points
    and measuring distances from point to point.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KNN算法是最简单的分类器之一，当你的数据集具有数值特征和聚类模式时，它表现得很好。在本质上，它与k-means聚类算法相似，因为它依赖于绘制数据点和测量点与点之间的距离。
- en: The Naive Bayes classifier is an effective and versatile classifier based on
    Bayesian probability. While it can be used for numerical data, it's most commonly
    used in text classification problems, such as spam detection and sentiment analysis.
    Naive Bayes classifiers, when implemented properly, can be both fast and highly
    accurate for narrow domains. The Naive Bayes classifier is one of my go-to algorithms
    for classification.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器是基于贝叶斯概率的有效且通用的分类器。虽然它可以用于数值数据，但它最常用于文本分类问题，如垃圾邮件检测和情感分析。当正确实现时，朴素贝叶斯分类器对于狭窄领域既快又高度准确。朴素贝叶斯分类器是我首选的分类算法之一。
- en: SVMs are, in spirit, a very advanced form of the KNN algorithm. The SVM graphs
    your data and attempts to find dividing lines between the categories you've labeled.
    Using some non-trivial mathematics, the SVM can linearize non-linear patterns,
    so this tool can be effective for both linear and non linear data.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVMs在精神上是非常先进的KNN算法形式。SVM绘制你的数据并试图找到你已标记的类别之间的分隔线。通过一些非平凡的数学方法，SVM可以将非线性模式线性化，因此这个工具对于线性和非线性数据都有效。
- en: Random forests are a relatively recent development in classification algorithms,
    but they are effective and versatile and therefore a go-to classifier for many
    researchers, myself included. Random forests build an ensemble of decision trees
    (another type of classifier we'll discuss later), each with a random subset of
    the data's features. Decision trees can handle both numerical and categorical
    data, they can perform both regression and classification tasks, and they also
    assist in feature selection, so they are becoming many researchers' first tool
    to grab when facing new problems.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林是分类算法中相对较新的发展，但它们既有效又灵活，因此许多研究人员（包括我自己）的首选分类器。随机森林构建了一个决策树集合（我们稍后将要讨论的另一种分类器），每个决策树都包含数据特征的一个随机子集。决策树可以处理数值和分类数据，它们可以执行回归和分类任务，并且还帮助进行特征选择，因此它们正成为许多研究人员面对新问题时首先抓取的工具。
- en: k-Nearest Neighbor
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-最近邻
- en: The KNN is a simple, fast, and straightforward classification algorithm. It
    is very useful for categorized numerical datasets where the data is naturally
    clustered. It will feel similar in some ways to the k-means clustering algorithm,
    with the major distinction being that k-means is an unsupervised algorithm while
    KNN is a supervised learning algorithm.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 是一个简单、快速且直接的分类算法。它对于自然聚类的分类数值数据集非常有用。在某些方面，它将类似于 k-means 聚类算法，但主要区别在于 k-means
    是一个无监督算法，而 KNN 是一个监督学习算法。
- en: 'If you were to perform a KNN analysis manually, here''s how it would go: first,
    plot all your training data on a graph, and label each point with its category
    or label. When you wish to classify a new, unknown point, put it on the graph
    and find the *k* closest points to it (the *nearest neighbors*). The number *k*
    should be an odd number in order to avoid ties; three is a good starting point,
    but some applications will need more and some can get away with one. Report whatever
    the majority of the *k* nearest neighbors are classified as, and that will be
    the result of the algorithm.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你手动执行 KNN 分析，过程如下：首先，将所有训练数据点绘制在图上，并给每个点标注其类别或标签。当你想要对一个新的、未知的数据点进行分类时，将其放在图上，并找到距离它最近的
    *k* 个点（即 *最近邻*）。*k* 应该是一个奇数，以避免平局；3 是一个不错的起点，但某些应用可能需要更多，而某些应用则可以用 1 来完成。报告大多数
    *k* 个最近邻被分类为何种类别，这将作为算法的结果。
- en: Finding the *k* nearest neighbors to a test point is straightforward, but can
    use some optimizations if your training data is very large. Typically, when evaluating
    a new point, you would calculate the Euclidean distance (the typical, high school
    geometry distance measure we introduced in [Chapter 4](84fd2c4d-41b4-46c4-82e5-4d8e55bb0066.xhtml),
    *Grouping with Clustering Algorithms*) between your test point and every other
    training point, and sort them by distance. This algorithm is quite fast because
    the training data is generally not more than 10,000 points or so.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 找到测试点的 *k* 个最近邻是直接的，但如果你的训练数据非常大，则可以使用一些优化。通常，在评估一个新点时，你会计算它与每个其他训练点之间的欧几里得距离（我们在第
    4 章[84fd2c4d-41b4-46c4-82e5-4d8e55bb0066.xhtml]，*使用聚类算法进行分组*中介绍的高中几何距离度量），并按距离排序。这个算法相当快，因为训练数据通常不超过
    10,000 个点。
- en: If you have many training examples (in the order of millions) or you really
    need the algorithm to be lightning-fast, there are two optimizations you can make.
    The first is to skip the square root operation in the distance measure, and use
    the squared distance instead. While modern CPUs are very fast, the square root
    operation is still much slower than multiplication and addition, so you can save
    a few milliseconds by avoiding the square root. The second optimization is to
    only consider points within some bounding rectangle of distance to your test point;
    for instance, only consider points within +/- 5 units in each dimension from the
    test point's location. If your training data is dense, this optimization will
    not affect results but will speed up the algorithm because it will avoid calculating
    distances for many points.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有很多训练示例（以百万计）或者你真的需要算法非常快，你可以进行两种优化。第一种是跳过距离度量中的平方根运算，而使用平方距离。虽然现代 CPU 非常快，但平方根运算仍然比乘法和加法慢得多，所以你可以通过避免平方根来节省几毫秒。第二种优化是只考虑距离测试点某个边界矩形内的点；例如，只考虑每个维度上距离测试点位置
    +/- 5 个单位的点。如果你的训练数据密集，这种优化不会影响结果，但会加快算法速度，因为它将避免计算许多点的距离。
- en: 'The following is the KNN algorithm as a high-level description:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对 KNN 算法的高级描述：
- en: Record all training data and their labels
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录所有训练数据和它们的标签
- en: Given a new point to evaluate, generate a list of its distances to all training
    points
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个要评估的新点，生成它到所有训练点的距离列表
- en: Sort the list of distances in order of closest to farthest
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照从近到远的顺序对距离列表进行排序
- en: Throw out all but the *k* nearest distances
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃除了 *k* 个最近距离之外的所有距离
- en: Determine which label represents the majority of your *k* nearest neighbors;
    this is the result of the algorithm
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定哪个标签代表了你的 *k* 个最近邻中的大多数；这是算法的结果
- en: A more efficient version avoids maintaining a large list of distances that need
    to be sorted by limiting the list of distances to *k* items. Let's now write our
    own implementation of the KNN algorithm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更高效的版本通过限制距离列表只包含 *k* 项来避免维护一个需要排序的大距离列表。现在让我们编写我们自己的 KNN 算法实现。
- en: Building the KNN algorithm
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 KNN 算法
- en: 'Since the KNN algorithm is quite simple, we''ll build our own implementation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于KNN算法相当简单，我们将构建自己的实现：
- en: Create a new folder and name it `Ch5-knn`.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的文件夹，并将其命名为`Ch5-knn`。
- en: 'To the folder, add the following `package.json` file. Note that this file is
    a little different from previous examples because we have added a dependency for
    the `jimp` library, which is an image processing library that we''ll use in the
    second example:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到该文件夹中添加以下`package.json`文件。请注意，这个文件与之前的示例略有不同，因为我们为`jimp`库添加了一个依赖项，这是一个我们将用于第二个示例的图像处理库：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Run the `yarn install` command to download and install all the dependencies,
    and then create subfolders called `src`, `dist`, and `files`.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`yarn install`命令以下载和安装所有依赖项，然后创建名为`src`、`dist`和`files`的子文件夹。
- en: Inside the `src` folder, create an `index.js` file and an `knn.js` file.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`src`文件夹内，创建一个`index.js`文件和一个`knn.js`文件。
- en: You will also need a `data.js` file. For these examples, I'm using a larger
    dataset than can be printed in this book, so you should take a minute to download
    the `Ch5-knn/src/data.js` file from this book's GitHub account.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要一个`data.js`文件。对于这些示例，我使用了一个比这本书能打印的更大的数据集，所以你应该花一分钟时间从这个书的GitHub账户下载`Ch5-knn/src/data.js`文件。
- en: 'Let''s start with the `knn.js` file. Like the k-means example in the previous
    chapter, we will need a distance-measuring function. Let''s use the one from [Chapter
    4](84fd2c4d-41b4-46c4-82e5-4d8e55bb0066.xhtml), *Grouping with Clustering Algorithms*;
    add the following to the beginning of `knn.js`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`knn.js`文件开始。就像前一章中的k-means示例一样，我们需要一个距离测量函数。让我们使用来自[第4章](84fd2c4d-41b4-46c4-82e5-4d8e55bb0066.xhtml)，*使用聚类算法进行分组*的函数；将以下内容添加到`knn.js`的开头：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you really need a performance optimization for your KNN implementation, this
    is where you might omit the `Math.sqrt` operation and return just the squared
    distance. I reiterate, however, that because this is such a fast algorithm by
    nature, you should only need to do this if you're working on an extreme problem
    with a lot of data or with very strict speed requirements.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的需要对你的KNN实现进行性能优化，你可能在这里省略`Math.sqrt`操作，只返回平方距离。然而，我再次强调，由于这个算法本质上非常快，你应该只有在处理极端问题、大量数据或非常严格的速度要求时才需要这样做。
- en: 'Next, let''s add the stub of our KNN class. Add the following to `knn.js`,
    beneath the distance function:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们添加我们的KNN类的骨架。将以下内容添加到`knn.js`中，在距离函数下方：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The constructor accepts three arguments: the `k`*,* or the number of neighbors
    to consider when classifying your new point; the training data split up into the
    data points alone; and a corresponding array of their labels.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数接受三个参数：`k`或分类新点时考虑的邻居数量；将训练数据拆分为单独的数据点；以及它们对应标签的数组。
- en: 'Next, we need to add an internal method that considers a test point and calculates
    a sorted list of distances from the test point to the training points. We''ll
    call this a **distance map**. Add the following to the body of the KNN class:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要添加一个内部方法，该方法考虑一个测试点并计算从测试点到训练点的距离的排序列表。我们将称之为**距离图**。将以下内容添加到KNN类的主体中：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This method could be easier to read, but the simpler version is not efficient
    for very large training sets. What we're doing here is maintaining a list of points
    that might be the KNNs and storing them in `map`. By maintaining a variable called
    `maxDistanceInMap`, we can loop over every training point and make a simple comparison
    to see whether the point should be added to our candidates list. If the point
    we're iterating over is closer than the farthest of our candidates, we can add
    the point to the list, re-sort the list, remove the farthest point to keep the
    list small, and then update `mapDistanceInMap`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法可能更容易阅读，但简单的版本对于非常大的训练集来说并不高效。我们在这里做的是维护一个可能包含KNN的点列表，并将它们存储在`map`中。通过维护一个名为`maxDistanceInMap`的变量，我们可以遍历每个训练点，进行简单的比较以确定该点是否应该添加到我们的候选列表中。如果我们正在迭代的点比我们的候选点中最远的点更近，我们可以将该点添加到列表中，重新排序列表，移除最远的点以保持列表较小，然后更新`mapDistanceInMap`。
- en: If that sounds like a lot of work, a simpler version might loop over all points,
    add each one with its distance measurement to the map, sort the map, and then
    return the first *k* items. The downside of that implementation is that for a
    dataset of a million points, you'd need to build a distance map of a million points
    and then sort that giant list in memory. In our version, you only ever hold *k*
    items as candidates, so you never need to store a separate million-point map.
    Our version does require a call to `Array.sort` whenever an item is added to the
    map. This is inefficient in its own way, as the sort function is called for each
    addition to the map. Fortunately, the sort operation is only for *k* items, where
    *k* might be something like 3 or 5\. The computational complexity of the sorting
    algorithm is most likely `O(n log n)` (for a quicksort or mergesort implementation),
    so it only takes about 30 data points for the more sophisticated version to be
    more efficient than the simple version when *k = 3*, and for *k = 5*, that happens
    at around 3,000 data points. However, both versions are so fast that for a dataset
    smaller than 3,000 points, you won't notice the difference.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这听起来像是一项繁重的工作，一个更简单的版本可能会遍历所有点，将每个点及其距离测量值添加到映射中，对映射进行排序，然后返回前*k*个条目。这种实现的缺点是，对于一个包含一百万个点的数据集，你需要构建一个包含一百万个点的距离映射，然后在内存中对这个巨大的列表进行排序。在我们的版本中，你只需要始终保留*k*个候选项，因此你永远不需要存储一个单独的一百万点映射。我们的版本确实需要在将项目添加到映射时调用`Array.sort`。这种方式本身就不太高效，因为每次添加到映射时都会调用排序函数。幸运的是，排序操作仅针对*k*个项，其中*k*可能类似于3或5。排序算法的计算复杂度很可能是`O(n
    log n)`（对于快速排序或归并排序实现），因此当*k=3*时，更复杂的版本在约30个数据点时比简单版本更高效，当*k=5*时，这种情况发生在大约3,000个数据点时。然而，两种版本都非常快，对于小于3,000个点的数据集，你不会注意到任何区别。
- en: Finally, we tie the algorithm together with a `predict` method. The `predict`
    method must accept a test point, and at the very least return the determined label
    for the `point`. We will also add some additional output to the method, and report
    the labels of the *k* nearest neighbors as well as the number of votes each label
    contributed.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将算法与一个`predict`方法结合起来。`predict`方法必须接受一个测试点，并且至少返回该点的确定标签。我们还将向该方法添加一些额外的输出，并报告*k*个最近邻的标签以及每个标签所贡献的投票数。
- en: 'Add the following to the body of the KNN class:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下内容添加到KNN类的主体中：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This method requires a little bit of datatype juggling in JavaScript, but is
    simple in concept. First, we generate our distance map using the method we just
    implemented. Then, we remove all data except for the *k* nearest points and store
    that in a `votes` variable. If you're using 3 as *k*, then `votes` will be an
    array of length three.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法在JavaScript中需要进行一些数据类型转换，但在概念上很简单。首先，我们使用我们刚刚实现的方法生成我们的距离映射。然后，我们移除所有数据，只保留*k*个最近点，并将这些数据存储在`votes`变量中。如果你使用*k=3*，那么`votes`将是一个长度为三的数组。
- en: 'Now that we have our *k* nearest neighbors, we need to figure out which label
    represents the majority of the neighbors. We''ll do this by reducing our votes
    array into an object called `voteCounts`. To get a picture of what we want `voteCounts`
    to look like, imagine that we''re looking for the three nearest neighbors and
    the possible categories are `Male` or `Female`. The `voteCounts` variable might
    look like this: `{"Female": 2, "Male": 1}`.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们有了*k*个最近邻，我们需要确定哪个标签代表了大多数邻居。我们将通过将投票数组缩减成一个名为`voteCounts`的对象来完成这项工作。为了了解我们希望`voteCounts`看起来像什么，想象我们在寻找三个最近邻，可能的类别是`Male`或`Female`。`voteCounts`变量可能看起来像这样：`{"Female":
    2, "Male": 1}`。'
- en: Our job is still not done, however—after reducing our votes into a vote-count
    object, we still need to sort that and determine the majority label. We do this
    by mapping the vote counts object back into an array and then sorting the array
    based on vote counts.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的工作还没有完成——在将我们的投票汇总成一个投票计数对象之后，我们仍然需要对其进行排序并确定多数标签。我们通过将投票计数对象映射回一个数组，然后根据投票数对数组进行排序来完成这项工作。
- en: There are other ways to approach this problem of tallying votes; any method
    you can think of will work, as long as you can return the majority vote at the
    end of the day. I like thinking about data in terms of structure and the transformations
    necessary to get from one structure to the next, but as long as you can report
    the top vote, the algorithm will work.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有其他方法可以处理这个投票计数问题；任何你能想到的方法都可以工作，只要你在最后能够返回多数投票。我喜欢从结构和从一种结构转换到另一种结构所需的转换来思考数据，但只要你能报告最高票，算法就会工作。
- en: That's all we need to do in the `knn.js` file. The algorithm is complete, requiring
    fewer than 70 lines of code.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在`knn.js`文件中，我们只需要做这些。算法已经完成，只需要少于70行代码。
- en: Let's set up our `index.js` file and get ready to run some examples. Remember
    that you need to download the `data.js` file first—see Packt's GitHub account
    or my personal GitHub account at [https://github.com/bkanber/MLinJSBook](https://github.com/bkanber/MLinJSBook).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置`index.js`文件并准备运行一些示例。记住，你首先需要下载`data.js`文件——请参阅Packt的GitHub账户或我的个人GitHub账户[https://github.com/bkanber/MLinJSBook](https://github.com/bkanber/MLinJSBook)。
- en: 'Add the following to the top of `index.js`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下内容添加到`index.js`的顶部：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let's try our algorithm out on a few simple examples.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在几个简单的例子上尝试我们的算法。
- en: Example 1 – Height, weight, and gender
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 1 – 身高、体重和性别
- en: 'KNN, like k-means, can work on high-dimensional data—but, like k-means, we
    can only graph example data in a two-dimensional plane so we''ll keep our examples
    simple. The first question we''ll tackle is: can we predict a person''s biological
    sex given only their height and weight?'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: KNN，就像k-means一样，可以处理高维数据——但是，就像k-means一样，我们只能在二维平面上绘制示例数据，所以我们将保持示例简单。我们将要解决的第一个问题是：我们能否仅根据一个人的身高和体重预测其生物性别？
- en: 'I''ve downloaded some data for this example from a national longitudinal survey
    on people''s perception of their weight. Included in the data are the respondents''
    height, weight, and gender. This is what the data looks like, when graphed:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我从这个例子中下载了一些数据，数据来自一项关于人们对自身体重感知的全国性纵向调查。数据中包括受访者的身高、体重和性别。以下是数据在图表中的样子：
- en: '![](img/39a20c3b-71d0-40d6-b896-6a43a084a953.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/39a20c3b-71d0-40d6-b896-6a43a084a953.png)'
- en: Just by looking at the preceding charted data, you can get a sense as to why
    KNN is so effective at evaluating clustered data. It's true that there's no neat
    boundary between male and female, but if you were to evaluate a new data point
    of a 200 pound, 72 inches-tall person, it's clear that all the training data around
    that point is male and it's likely your new point is male, too. Conversely, a
    new respondent at 125 pounds and a height of 62 inches is well into the female
    area of the graph, though there are a couple of males with those characteristics
    as well. The middle of the graph, around 145 pounds and 65 inches tall, is the
    most ambiguous, with an even split of male and female training points. I would
    expect the algorithm to be uncertain about new points in that area. Because there
    is no clear dividing line in this dataset, we would need more features or more
    dimensions to get a better resolution of the boundaries.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 只需看一下前面的图表数据，你就可以感受到KNN在评估聚类数据时为什么如此有效。确实，男性和女性之间没有清晰的边界，但如果你要评估一个体重200磅、身高72英寸的新数据点，很明显，围绕该点的所有训练数据都是男性，你的新点也很可能是男性。相反，一个体重125磅、身高62英寸的新受访者已经进入了图表中的女性区域，尽管也有几个男性具有这些特征。图表的中间，大约在145磅和65英寸高，是最模糊的，男性和女性的训练点分布均匀。我预计算法在这个区域的新点将不确定。因为在这个数据集中没有清晰的分割线，我们需要更多的特征或更多的维度来获得更好的边界分辨率。
- en: 'In any case, let''s try out a few examples. We''ll pick five points that we
    expect to be definitely male, definitely female, probably male, probably female,
    and indeterminable. Add the following code to `index.js`, beneath the two import
    lines:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，让我们尝试几个例子。我们将选择五个我们预期肯定是男性、肯定是女性、可能是男性、可能是女性和无法确定的点。将以下代码添加到`index.js`文件中，在两个导入行下面：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Run `yarn start` from the command line and you should see the following output.
    Since the KNN is not stochastic, meaning it does not use any random conditions
    in its evaluation, you should see exactly the same output as I do—with the possible
    exception of the ordering of votes and their indexes, if two votes have the same
    distance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令行中运行`yarn start`，你应该看到以下输出。由于KNN不是随机的，这意味着它在评估时没有使用任何随机条件，你应该看到与我完全相同的输出——除非两个投票有相同的距离，否则投票顺序和它们的索引可能会有所不同。
- en: If you get an error when you run `yarn start`, make sure your `data.js` file
    has been correctly downloaded and installed.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行`yarn start`时出现错误，请确保你的`data.js`文件已经正确下载并安装。
- en: 'Here''s the output from the preceding code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是前面代码的输出：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The algorithm has determined genders just as we would have done, visually, by
    looking at the chart. Feel free to play with this example more and experiment
    with different values of `k` to see how results might differ for any given test
    point.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法已经确定了性别，就像我们通过查看图表进行视觉判断一样。您可以随意玩转这个例子，并尝试不同的 `k` 值，看看结果可能会有何不同。
- en: Let's now look at a second example of KNN in action. This time, we'll choose
    a problem where `k = 1` really shines.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一个 KNN 实际应用的第二个例子。这次，我们将选择一个 `k = 1` 真正发光的问题。
- en: Example 2 – Decolorizing a photo
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 2 – 去色照片
- en: The KNN algorithm is very susceptible to local noise and isn't very useful when
    there is a lot of overlap between classes expected. It is typically not very useful
    for more advanced tasks, such as psychographic, demographic, or behavioral analysis.
    But it's a very useful tool to keep handy in your toolbox, because it can assist
    with lower-level tasks very easily.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 算法非常容易受到局部噪声的影响，当预期类别之间存在大量重叠时，它并不很有用。它通常对更高级的任务，如心理分析、人口统计或行为分析，并不很有用。但它是您工具箱中一个非常有用的工具，因为它可以很容易地帮助完成底层任务。
- en: In this example, we'll use our KNN class to de colorize a photo. Specifically,
    we're going to take colorful input photos and restrict them to a color scheme
    of only 16 colors. We'll use KNN here to select the appropriate replacement color
    for a pixel, given that pixel's original color.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用我们的 KNN 类去色照片。具体来说，我们将从彩色输入照片中提取，并限制它们只使用 16 种颜色方案。我们将使用 KNN 来选择给定像素的适当替代颜色。
- en: 'Our workflow will look like this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作流程将如下所示：
- en: Use the `jimp` library to read an input image
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `jimp` 库读取输入图像
- en: 'Loop over each pixel in the image and:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历图像中的每个像素：
- en: Find the most similar color in our 16-color scheme
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的 16 种颜色方案中找到最相似的颜色
- en: Replace that pixel with the new color
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用新颜色替换那个像素
- en: Write a new output file, based on the 16-color scheme
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于十六色方案写入一个新输出文件
- en: 'Before we start, *verify* that the following exists in your `data.js` file.
    If you downloaded the `data.js` file from the GitHub for this book, then this
    should already be in there. However, if you sourced your gender survey data from
    a different place, you will need the following in the `data.js` file:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，*验证* 以下内容是否存在于您的 `data.js` 文件中。如果您从 GitHub 下载了这本书的 `data.js` 文件，那么它应该已经在那里了。然而，如果您从其他地方获取了性别调查数据，您需要在
    `data.js` 文件中包含以下内容：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The preceding color definitions represent a common color scheme of 16 colors.
    You can also experiment with color schemes on your own; you can use this approach
    to colorize to shades of blue, or to warm colors, or to sepia tones, and so on.
    You can also allow for far more than 16 colors by increasing the training data
    size.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上述颜色定义代表一个常见的 16 种颜色方案。您也可以自己尝试不同的颜色方案；您可以使用这种方法将图像着色为蓝色调、暖色调或棕褐色调等。您还可以通过增加训练数据的大小来允许远超过
    16 种颜色。
- en: 'Let''s start by writing a couple of helper functions. Create a new file, in
    the `src` folder, called `decolorize.js`. Make sure you added `jimp` to your `package.json`—if
    you''re unsure, run `yarn add jimp` from the command line. Add the following imports
    to the top of the file:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先编写几个辅助函数。在 `src` 文件夹中创建一个名为 `decolorize.js` 的新文件。确保您已将 `jimp` 添加到您的 `package.json`
    中——如果您不确定，请在命令行中运行 `yarn add jimp`。将以下导入添加到文件顶部：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, create and export a function that accepts an image filename and writes
    a new file with the decolorized image. I''ve left some gentle comments in the
    code snippet that describe the workflow; most of the code is just juggling data
    formats. In general, our approach is to open and read the input file, iterate
    over all pixels, use a KNN to find a substitute color for that pixel, write the
    new color to the pixel, and then finally write a new output file using the modified
    colors:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，创建并导出一个函数，该函数接受一个图像文件名并写入一个去色图像的新文件。我在代码片段中留下了一些温和的注释，描述了工作流程；大部分代码只是处理数据格式。一般来说，我们的方法是打开并读取输入文件，遍历所有像素，使用
    KNN 为该像素找到一个替代颜色，将新颜色写入像素，然后最终使用修改后的颜色写入一个新输出文件：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We now have a function that will accept a filename and create a new de-colorized
    photo. If you haven't already, create a folder called `files` in the `Ch5-knn`
    directory. Find a few of your favorite pictures and add them to the `files` folder.
    Or, you can use the image examples from the book's GitHub, which are `landscape.jpeg`,
    `lily.jpeg`, and `waterlilies.jpeg`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个函数，它将接受一个文件名并创建一个新的去色照片。如果你还没有创建，请在`Ch5-knn`目录下创建一个名为`files`的文件夹。找到一些你喜欢的图片并将它们添加到`files`文件夹中。或者，你可以使用书中GitHub上的图像示例，它们是`landscape.jpeg`、`lily.jpeg`和`waterlilies.jpeg`。
- en: 'Finally, open up `index.js` and add the following to the bottom of the file:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，打开`index.js`文件，并将以下内容添加到文件底部：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you are using your own example files, make sure to update the filenames shown
    in bold in the preceding code.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用自己的示例文件，请确保更新前面代码中显示的粗体文件名。
- en: 'Run the code with `yarn start` and you should see output like the following
    (you may have the results from the other KNN experiment in your output, as well):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`yarn start`运行代码，你应该会看到以下输出（你的输出中可能还有其他KNN实验的结果）：
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If there are any errors with filenames or permissions, resolve them. Look in
    the `files` folder for your new photos. I don't know which format you're reading
    this book in and how these images will look to you, but the following is my `landscape.jpeg`
    file, original and processed.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有任何关于文件名或权限的错误，请解决它们。在`files`文件夹中查找你的新照片。我不知道你用哪种格式阅读这本书，以及这些图片将如何显示给你，但以下是我的`landscape.jpeg`文件，原始和处理的。
- en: 'The original:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 原始：
- en: '![](img/7e0e320d-9170-48f2-8979-7553f8e10b02.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e0e320d-9170-48f2-8979-7553f8e10b02.jpeg)'
- en: 'And the de-colorized version:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 去色版本：
- en: '![](img/be92c1f9-c64a-47f7-ba1c-31d0acb46818.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be92c1f9-c64a-47f7-ba1c-31d0acb46818.jpeg)'
- en: I think it did a very good job on the foreground and the scenery, however, the
    limited color palette definitely affects the sky, water, and mountains in the
    background. Try adding another 8 or 16 colors to the training data to see what
    happens.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为它在前景和风景方面做得非常好，然而，有限的调色板无疑影响了背景中的天空、水和山脉。尝试向训练数据中添加8或16种颜色，看看会发生什么。
- en: I like this project as a KNN example because it shows you that **machine learning**
    (**ML**) algorithms don't always have to be used for sophisticated analyses. Many
    of them can be used as part of your everyday toolbox, trained with smaller models
    to help you with simpler data-processing tasks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢这个项目作为KNN示例，因为它表明**机器学习**（**ML**）算法并不总是用于复杂分析。其中许多可以作为你日常工具箱的一部分使用，用较小的模型训练，帮助你处理更简单的数据处理任务。
- en: I should also make a note here about measuring the distance between colors.
    The approach we have taken, using the Euclidean distance formula to measure distances
    between RGB values, is not perceptually accurate. The RGB space is slightly warped
    when it comes to human visual perception, so our Euclidean distance measurements
    are not totally accurate. For our purposes, they are close enough because we are
    downgrading to a very low resolution. If you need perceptually-accurate image
    processing, you will either need to transform all RGB values into a more accurate
    color space, such as *Lab*,or update your distance function to measure perceptual
    distance rather than just the geometric distance between points.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我还应该在这里记一笔关于测量颜色之间距离的事情。我们采用的方法，即使用欧几里得距离公式来测量RGB值之间的距离，在感知上并不准确。当涉及到人类视觉感知时，RGB空间略有扭曲，因此我们的欧几里得距离测量并不完全准确。就我们的目的而言，它们已经足够接近了，因为我们正在降低到非常低的分辨率。如果你需要进行感知上准确的照片处理，你可能需要将所有RGB值转换到一个更准确的颜色空间，例如*Lab*，或者更新你的距离函数来测量感知距离，而不仅仅是点之间的几何距离。
- en: 'Let''s move on from KNN and look at a more sophisticated way to classify objects,
    based on centuries-old probability theory that is still powerful today: Bayesian
    classification.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从KNN转向一个更复杂的方法来分类对象，基于几个世纪前的概率理论，至今仍然强大：贝叶斯分类。
- en: Naive Bayes classifier
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 天真贝叶斯分类器
- en: A Naive Bayes classifier is a type of probabilistic classifier, or an algorithm
    that assigns a probability distribution to the potential outcomes. As opposed
    to a binary classification, such as `Male` or `Female`, the probabilistic classifier
    tells you there is an 87% chance this data point is `Male` and a 13% chance it
    is `Female`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 天真贝叶斯分类器是一种概率分类器，或者是一种将概率分布分配给潜在结果的算法。与`Male`或`Female`这样的二进制分类不同，概率分类器会告诉你这个数据点有87%的概率是`Male`，有13%的概率是`Female`。
- en: Not all probabilistic classifiers are Bayesian, nor are they all necessarily
    naive. The term *Naive*, in this context, is not a veiled insult to the classifier—it's
    a mathematical term that has a meaning in probability theory, which we'll discuss
    further later. The term *Bayes* or *Bayesian* means that the principles used in
    the classifier were first published by Reverend Thomas Bayes, an 18th century
    mathematician, popular for his *Bayes theorem *in probability theory.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有概率分类器都是贝叶斯分类器，它们也不一定都是朴素分类器。在这个上下文中，“朴素”一词并不是对分类器的隐晦侮辱——它是一个在概率论中有特定意义的数学术语，我们稍后会进一步讨论。术语“贝叶斯”或“贝叶斯派”意味着分类器中使用的原理最初是由18世纪的数学家托马斯·贝叶斯牧师发表的，他因其在概率论中的**贝叶斯定理**而闻名。
- en: Let's first have a probability refresher. First, you should know that probability
    can work with both *continuous distributions* and *discrete distributions*. Continuous
    distributions are those where your variable is a number and can have any value.
    Discrete distributions have only a fixed number of possible states, even if that
    number is large. Continuous values are things such as *activity of 54.21 minutes
    per week; $23.34 per share; 18 total logins*. Discrete values are *true*/*false*;
    *Hollywood*, *gossip*, *politics*, *sports*, *local events*, or *world news*,
    or even the frequency of individual words in an article. Most of the theorems
    in probability can be used both for continuous and discrete distributions, though
    the implementation details between the two will differ.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来复习一下概率。首先，你应该知道概率可以与**连续分布**和**离散分布**一起工作。连续分布是变量是一个数字，可以具有任何值的分布。离散分布只有有限数量的可能状态，即使这个数量很大。连续值是像*每周54.21分钟的活动；每股23.34美元；总共18次登录*这样的东西。离散值是*真/假*；*好莱坞*，*八卦*，*政治*，*体育*，*本地事件*，或者*世界新闻*，甚至是文章中单个单词的频率。概率论中的大多数定理都可以用于连续和离散分布，尽管两者之间的实现细节会有所不同。
- en: In discrete probability, which we will use for our example, you work with the
    probability of various *events* occurring. An event is a set of possible outcomes
    from an experiment. The classical illustrative example of this involves a pack
    of playing cards; imagine you draw a card at random from a shuffled deck. What's
    the probability that the card you pulled is a heart? When we ask this question,
    we're asking about the probability of a certain event, specifically *that the
    card is a heart*. We can give our event a label, such as `H` for heart, and then
    we can shorten the phrase *probability that the card is a heart* to simply, `P(H)`.
    The answer is 13/52, or 1/4, or 0.25, so you could also say that `P(H) = 0.25`.
    There are many other possible events in our scenario. What's the chance the card
    is the five of diamonds? What's the chance the card is black? What's the chance
    the card is a face card? What's the chance the value is less than five? All of
    those are types of events, and each one has its own probability.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们例子中使用的离散概率中，你处理的是各种**事件**发生的概率。事件是从实验中可能得到的结果的集合。这个经典的说明性例子涉及一副扑克牌；想象你从洗好的牌堆中随机抽取一张牌。你抽到的牌是红心的概率是多少？当我们提出这个问题时，我们是在询问某个事件的概率，具体来说是**这张牌是红心**的概率。我们可以给我们的事件一个标签，比如用`H`表示红心，然后我们可以将短语“这张牌是红心的概率”简短地表示为`P(H)`。答案是13/52，或者1/4，或者0.25，所以你也可以说`P(H)
    = 0.25`。在我们的场景中还有许多其他可能的事件。抽到方块5的概率是多少？抽到黑桃的概率是多少？抽到人头牌的概率是多少？数值小于5的概率是多少？所有这些都是事件类型，每个事件都有其自己的概率。
- en: Not all events are independent. For instance, let's say the experiment is *did
    you drink a soda yesterday?*, and we are surveying Americans. We can define the
    event `S` as *drank a soda yesterday*. By surveying everyone in America (or at
    least a representative sample), we find that nearly 50% of respondents said yes!
    (It is actually 48%, according to Yale University.) So we can say that the probability
    of `S` is 50%, or `P(S) = 0.5`. We can also define an event as `S',`, which is
    the probability of the event *did not drink a soda yesterday*, or the inverse.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有事件都是独立的。例如，假设实验是“你昨天喝了汽水吗？”，并且我们在调查美国人。我们可以定义事件`S`为“昨天喝了汽水”。通过调查美国所有人（或者至少是一个代表性样本），我们发现近50%的受访者表示是的！（根据耶鲁大学的数据，实际上是48%）所以我们可以这样说，事件`S`的概率是50%，或者`P(S)
    = 0.5`。我们还可以定义一个事件为`S'`，这是事件“昨天没有喝汽水”的概率，或者其逆事件。
- en: 'We want to develop more insight into citizens'' eating habits, so we add another
    question to the survey: did you eat at a fast food restaurant yesterday? We''ll
    name this event `M`, for McDonald''s, and we find that `P(M) = 0.25`, or a quarter
    of the nation.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望更深入地了解公民的饮食习惯，所以我们向调查中添加了另一个问题：你昨天在快餐店吃饭了吗？我们将这个事件命名为`M`，代表麦当劳，我们发现`P(M)
    = 0.25`，即全国的四分之一。
- en: 'We can now ask more sophisticated questions, such as: does eating fast food
    affect whether people drink soda? We can ask about the probability that someone
    drank a soda given that they ate fast food yesterday. This is called the **conditional
    probability** of `S` events given `M`, or `P(S|M)`.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以问更复杂的问题，例如：吃快餐是否会影响人们喝汽水？我们可以询问在昨天吃了快餐的情况下，某人喝了汽水的概率。这被称为`S`事件在`M`条件下的**条件概率**，即`P(S|M)`。
- en: If we asked the questions about drinking a soda and eating fast food in the
    same survey, then we can calculate `P(S|M)` by finding the probability of a respondent
    doing both events (this is written `P(S ∩ M)`, pronounced *probability of S intersect
    M*), and dividing by `P(M)`. The full formula is `P(S|M) = P(S ∩ M) / P(M)`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在同一份调查中询问关于喝汽水和吃快餐的问题，那么我们可以通过找到同时进行这两个事件的受访者的概率（这写成`P(S ∩ M)`，发音为*概率S交集M*），然后除以`P(M)`来计算`P(S|M)`。完整的公式是`P(S|M)
    = P(S ∩ M) / P(M)`。
- en: Let's say that 20% of respondents both drank a soda and ate fast food. We can
    now calculate that `P(S|M) = 0.2 / 0.25 = 0.8`. The probability of having drunk
    a soda yesterday is 80%, given that you ate fast food yesterday.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有20%的受访者既喝了汽水又吃了快餐。我们现在可以计算出`P(S|M) = 0.2 / 0.25 = 0.8`。如果你昨天吃了快餐，那么昨天喝了汽水的概率是80%。
- en: Note that this is *not* the probability that you drank a soda *while* eating
    fast food. To answer that question, you'd have to go to a fast food restaurant
    and survey the people there. Our version is less committal in terms of causation.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这*不是*你在吃快餐时喝了汽水的概率。要回答这个问题，你必须去快餐店并对那里的人进行调查。我们的版本在因果关系方面承诺较少。
- en: 'Now you want to ask the reverse question: what''s the probability of someone
    having eaten fast food given that they drank a soda yesterday? This is asking
    about `P(M|S)`. We could just reverse the preceding formula, but let''s say that
    we lost the original survey data and can no longer determine `P(S ∩ M)`.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你想问相反的问题：如果某人昨天喝了汽水，那么他们昨天吃快餐的概率是多少？这是在询问`P(M|S)`。我们本可以直接反转前面的公式，但假设我们失去了原始的调查数据，无法再确定`P(S
    ∩ M)`。
- en: 'We can use the Bayes theorem to correctly reverse our probability:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用贝叶斯定理来正确地反转我们的概率：
- en: '*P(M|S) = P(S|M) * P(M) / P(S)*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(M|S) = P(S|M) * P(M) / P(S)*'
- en: 'Fortunately, we remember those three values and find that:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们记得这三个值，并发现：
- en: '*P(M|S) = 0.8 * 0.25 / 0.5  = 0.4*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(M|S) = 0.8 * 0.25 / 0.5 = 0.4*'
- en: The probability that someone ate fast food yesterday, knowing that they drank
    a soda, is 40%. That's up from the baseline of 25% for anyone eating fast food.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 知道某人昨天喝了汽水，那么他们昨天吃快餐的概率是40%。这比任何吃快餐的人的基准概率25%要高。
- en: How does this apply to naive Bayes classifiers? We use the preceding conditional
    probability theorems to relate features to their respective classes. In a spam
    filter, we ask the question: what's the probability that this document is spam
    given that it has the word *credit *in it*?* And what's the probability this document
    is spam given that it has the word *transfer* in it*?* We ask that question for
    every word in the document, and then we combine those probabilities to get the
    overall probability that the document is spam. The naive Bayes classifier is naive
    because it assumes that the events are all independent. Truthfully, this is a
    bad assumption. Emails with the word *credit *are more likely to also have the
    word *transfer* in them, but in practice it turns out that these classifiers are
    still very accurate despite the incorrect assumption.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这如何应用于朴素贝叶斯分类器？我们使用前面的条件概率定理将特征与其相应的类别联系起来。在垃圾邮件过滤器中，我们提出这样的问题：这个文档包含单词*credit*时，它是垃圾邮件的概率是多少？*以及这个文档包含单词*transfer*时，它是垃圾邮件的概率是多少？*我们对文档中的每个单词都提出这样的问题，然后我们将这些概率结合起来，得到文档是垃圾邮件的整体概率。朴素贝叶斯分类器之所以被称为朴素，是因为它假设所有事件都是独立的。实际上，这是一个错误的假设。包含单词*credit*的电子邮件更有可能也包含单词*transfer*，但在实践中，这些分类器仍然非常准确，尽管存在错误的假设。
- en: Tokenization
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词
- en: We also must briefly discuss the concept of *tokenization*. We will discuss
    tokenization in depth in Chapter 10, *Natural Language Processing in Practice*
    when we discuss natural language programming, but we do need a short introduction
    to it now. Tokenization is the act of breaking up a document into individual *tokens*.
    You can think of a token as a word, but not all words are necessarily tokens and
    not all tokens are necessarily words.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须简要讨论一下**分词**的概念。我们将在第10章“实践中的自然语言处理”中深入讨论分词，当我们讨论自然语言编程时，但我们现在确实需要对其有一个简短的介绍。分词是将文档分解成单个**分词**的行为。你可以把分词想象成单词，但并非所有单词都是分词，也不是所有分词都是单词。
- en: The simplest tokenizer would be to split a document up by spaces. The result
    would be an array of words, including their capitalization and their punctuation.
    A slightly more advanced tokenizer might convert everything to lowercase and remove
    any non-alphanumeric characters. Now the tokens are all lowercase words, numbers,
    and words with numbers in them. Your tokenizer can remove common words, such as *and*
    and *the*—this is called **stopword filtering***.* You can also *stem* as part
    of your tokenizer, which is to remove extraneous endings from a word. For instance
    *parties*, *partied*, and *party* might all become *parti*. This is a great dimensionality
    reduction technique, and helps your classifier focus on the meaning of words rather
    than the particular tense or usage. You can take it even farther by *lemmatizing*,
    which is similar to stemming but actually grammatically transforms words to their
    root form, so that *running*, *runs*, and *ran* would all become *run*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的分词器可能是通过空格分割文档。结果将是一个包含单词、它们的字母大小写和标点符号的数组。一个稍微高级一点的分词器可能会将所有内容转换为小写并删除任何非字母数字字符。现在，所有分词都是小写单词、数字以及包含数字的单词。你的分词器可以删除常见单词，如“和”和“的”——这被称为**停用词过滤**。你还可以在分词器中实现**词干提取**，即从单词中删除不必要的结尾。例如，“parties”、“partied”和“party”都可能变成“parti”。这是一个很好的降维技术，有助于你的分类器关注单词的意义而不是特定的时态或用法。你可以通过**词形还原**更进一步，这与词干提取类似，但实际上是将单词语法上转换为它们的词根形式，因此“running”、“runs”和“ran”都会变成“run”。
- en: 'Tokenization can take more advanced forms. A token does not need to be a single
    word; it can be pairs or trios of words. These are called **bigrams** and **trigrams**,
    respectively. Tokens can also be generated from metadata. Email spam filters,
    in particular, do very well when some information from the message''s headers
    are included as tokens: whether the email passed or failed its SPF check, whether
    it has a valid DKIM key, the sender''s domain, and so on. Tokenizers can also
    modify tokens from certain fields; for instance, it was found that prefixing tokens
    from email subject lines (as opposed to body content) improved spam filtering
    performance. Rather than tokenizing *buy pharmaceuticals now* as *buy*, *pharmaceuticals*,
    *now*, you can tokenize those as *SUBJ_buy*, *SUBJ_pharmaceuticals*, *SUBJ_now*.
    The effect of this prefixing is to allow the classifier to consider subject and
    body words separately, which may increase performance.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 分词可以采取更高级的形式。一个分词不一定是单个单词；它可以是单词的对或三联组。这些分别被称为**二元组**和**三元组**。分词也可以从元数据生成。特别是电子邮件垃圾邮件过滤器，当将消息头中的某些信息作为分词包含在内时，表现非常好：电子邮件是否通过了SPF检查，是否有有效的DKIM密钥，发送者的域名等等。分词器还可以修改某些字段中的分词；例如，发现给电子邮件主题行中的分词加上前缀（与正文内容相反）可以提高垃圾邮件过滤性能。与其将“现在购买药品”分词为“购买”、“药品”、“现在”，不如将其分词为“SUBJ_buy”、“SUBJ_pharmaceuticals”、“SUBJ_now”。这种前缀的效果是允许分类器分别考虑主题和正文中的单词，这可能会提高性能。
- en: Do not underestimate the importance of the tokenizer. Often, you can get significant
    accuracy improvements by being thoughtful about your tokenizer algorithm. In this
    example, we'll use a simple, intuitive one that is still quite effective.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 不要低估分词器的重要性。通常，通过深思熟虑地选择分词器算法，你可以获得显著的准确率提升。在这个例子中，我们将使用一个简单、直观但仍相当有效的分词器。
- en: Building the algorithm
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建算法
- en: 'Let''s now build the naive Bayes classifier. These are the steps that are to
    be followed to build the algorithm :'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建朴素贝叶斯分类器。以下是构建算法需要遵循的步骤：
- en: 'Create a new folder for the project called `Ch5-Bayes`. As usual, create `src`
    and `data` and `dist` folders, and add the following `package.json` file:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为项目创建一个名为`Ch5-Bayes`的新文件夹。像往常一样，创建`src`、`data`和`dist`文件夹，并添加以下`package.json`文件：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Once you've added the `package.json` file, run `yarn install` from the command
    line to install all the project dependencies.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦添加了`package.json`文件，请在命令行中运行`yarn install`来安装所有项目依赖项。
- en: Navigate to the book's GitHub account, and download the four files in the `data`
    folder. They should be called `train_negative.txt`, `train_positive.txt`, `test_negative.txt`,
    and `test_positive.txt`. These files contain reviews of movies from [https://www.imdb.com/](https://www.imdb.com/),
    and are pre-sorted into positive reviews and negative reviews, using IMDB's star-rating
    system. We will use this data to train and later validate an algorithm to detect
    movie review sentiment.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到书籍的GitHub账户，并在`data`文件夹中下载四个文件。这些文件应命名为`train_negative.txt`、`train_positive.txt`、`test_negative.txt`和`test_positive.txt`。这些文件包含来自[https://www.imdb.com/](https://www.imdb.com/)的电影评论，并使用IMDB的星级评分系统预先分类为正面评论和负面评论。我们将使用这些数据来训练并验证一个检测电影评论情感的算法。
- en: 'Create a `bayes.js` file in the `src` folder. Add the following tokenizer function
    to the top of the file:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`src`文件夹中创建一个`bayes.js`文件。将以下分词器函数添加到文件顶部：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This function accepts a string as an input, and returns an array of tokens as
    the output. The string is first converted to lowercase, because our analysis is
    case-sensitive. Then any characters that are not word or number characters are
    removed and replaced with spaces. We split the string up by spaces to get an array
    of tokens. Next, we filter out any tokens that are three characters or shorter
    (so the words *the* and *was* would be removed, while words like *this* and *that*
    are preserved). The last line of the tokenizer filters out non-unique tokens;
    we will consider only the existence of words in documents, not the number of times
    those words are used.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受一个字符串作为输入，并返回一个标记数组作为输出。首先将字符串转换为小写，因为我们的分析是区分大小写的。然后删除任何非单词或数字字符，并用空格替换。通过空格分割字符串以获取标记数组。接下来，过滤掉任何长度为三个字符或更短的标记（因此会移除像*the*和*was*这样的单词，而像*this*和*that*这样的单词会被保留）。分词器的最后一行过滤掉非唯一标记；我们只考虑文档中单词的存在，而不是这些单词的使用次数。
- en: Note that the `filter` function in the tokenizer does not preserve word order.
    To preserve word order, you would need to add `.reverse()` before and after the
    final filter line. However, our algorithm doesn't consider word order, so preserving
    it is not necessary.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，分词器中的`filter`函数不会保留单词顺序。为了保留单词顺序，你需要在最后的过滤行前后添加`.reverse()`。然而，我们的算法不考虑单词顺序，因此保留它是不必要的。
- en: 'Create the `BayesClassifier` class and export it from `bayes.js`. Add the following
    to the file:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`BayesClassifier`类并将其从`bayes.js`文件中导出。将以下内容添加到文件中：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The constructor for the classifier accepts only a `tokenizer` function, however,
    it defaults to the simple preceding tokenizer we created. Making the tokenizer
    configurable like this will allow you to experiment with better tokenizers that
    fit your particular dataset.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的构造函数只接受一个`tokenizer`函数，但是默认为我们创建的简单分词器。将分词器配置为可配置的，这样你就可以尝试适合你特定数据集的更好的分词器。
- en: Training a Naive Bayes classifier is a straightforward process. First, simply
    count the number of documents in each category that you have seen. If your training
    set has 600 positive movie reviews and 400 negative movie reviews, then you should
    have 600 and 400 as your document counts, respectively. Next, tokenize the document
    to be trained. You must always make sure to use the same tokenizer during training
    as you do during evaluation. For each token in the training document, record how
    many times you've seen that token amongst all documents in the category. For example,
    if your training data has 600 positive movie reviews and the word *beautiful* appears
    in 100 of them, you would need to maintain a count of 100 for the token *beautiful* in
    the *positive* category. If the token *beautiful* only appears three times in
    your negative review training data, then you must maintain that count separately.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 训练朴素贝叶斯分类器是一个简单的过程。首先，简单地计算你看到的每个类别的文档数量。如果你的训练集包含600条正面电影评论和400条负面电影评论，那么你应该分别有600和400作为你的文档计数。接下来，对要训练的文档进行分词。你必须始终确保在训练期间使用与评估期间相同的分词器。对于训练文档中的每个标记，记录你在该类别中所有文档中看到该标记的次数。例如，如果你的训练数据有600条正面电影评论，而单词*beautiful*出现在其中的100条，那么你需要在*positive*类别中为标记*beautiful*维护一个计数为100。如果标记*beautiful*在你的负面评论训练数据中只出现了三次，那么你必须单独维护这个计数。
- en: 'Let''s translate this into code. It''s a very simple operation, but we are
    also dividing up the work between many small count and incrementing functions;
    we will use these counting functions in our evaluation stage as well:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个翻译成代码。这是一个非常简单的操作，但我们也在将工作分配给许多小的计数和递增函数；我们将在评估阶段也使用这些计数函数：
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As you can see, the `train()` method is quite simple: increment the document
    count for the given label (for example, `spam` or `not spam`, `positive sentiment`
    or `negative sentiment`); then, for each token in the document, increment the
    token count for the given label (for example, *beautiful* was seen 100 times in
    positive-sentiment documents, and was seen three times in negative-sentiment documents).
    These counts are maintained in an instance variable called `this.database` in
    the `BayesClassifier` class.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`train()`方法相当简单：增加给定标签的文档计数（例如，`垃圾邮件`或`非垃圾邮件`，`正面情感`或`负面情感`）；然后，对于文档中的每个标记，增加给定标签的标记计数（例如，*beautiful*在正面情感文档中出现了100次，在负面情感文档中出现了3次）。这些计数保存在`BayesClassifier`类的一个实例变量`this.database`中。
- en: In order to make a prediction on a new document, we'll need to consider each
    of the labels we encountered during training separately, calculate a probability
    for that label, and return the most probable label. Let's work backwards in terms
    of implementing the prediction; we'll start by adding the `predict` method and
    then work backwards, filling in all the other methods we'll need.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对新文档进行预测，我们需要单独考虑我们在训练过程中遇到的每个标签，计算该标签的概率，并返回最可能的标签。让我们从实现预测的反向工作开始；我们首先添加`predict`方法，然后反向工作，填充我们需要的所有其他方法。
- en: 'First, add the following `predict` method to the `BayesClassifier` class:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将以下`predict`方法添加到`BayesClassifier`类中：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This method accepts an input string or document, and returns a `result` object
    with the most likely label or category, the probability of that label or category,
    and an array of all the probabilities for all labels encountered during training.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法接受一个输入字符串或文档，并返回一个`result`对象，其中包含最可能的标签或类别，该标签或类别的概率，以及训练过程中遇到的所有标签的概率数组。
- en: 'Next, add the method that `predict` relies upon to calculate the probability
    of each label on the input document:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，添加`predict`方法所依赖的方法，用于计算输入文档中每个标签的概率：
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This method tokenizes the input text, and then generates an array of all labels
    and their probabilities, sorted in order of most probable to least probable. You
    will now need to add these two methods to the class—first, the simple `getAllLabels()`
    method:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将输入文本进行分词，然后生成一个包含所有标签及其概率的数组，按最可能到最不可能的顺序排序。现在您需要将这两个方法添加到类中——首先，是简单的`getAllLabels()`方法：
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And then add the more complex `calculateLabelProbability`, which is responsible
    for calculating the probability of an individual label fitting a document:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后添加更复杂的`calculateLabelProbability`函数，该函数负责计算单个标签适合文档的概率：
- en: '[PRE20]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The inline comments in the `calculateLabelProbability` method illuminate the
    specifics of how the method works, but the basic goal of this step is to calculate
    a probability for each token in the document, and then to combine the individual
    token probabilities into one overall probability for the label.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`calculateLabelProbability`方法中的内联注释说明了该方法的具体工作方式，但这一步的基本目标是计算文档中每个标记的概率，然后将单个标记概率组合成一个标签的整体概率。'
- en: For instance, if a movie review states *beautiful [but] awful garbage*, this
    method is responsible for looking at all of the tokens (*but* is omitted by the
    tokenizer) and determining how well they fit a given label (for example, *positive*
    or *negative*).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一部电影评论说*beautiful [but] awful garbage*，这个方法负责查看所有标记（*but*被分词器省略）并确定它们与给定标签（例如，*正面*或*负面*）的匹配程度。
- en: Let's imagine we're running this method for the *positive *category label. The
    word *beautiful* would get a strong score, maybe 90%, but the tokens *awful* and
    *garbage* would both get weak scores, for instance 5%. This method would then
    report that the probability of the *positive* label is low for this document.
    On the other hand, when this method is run for the *negative* category label,
    the *beautiful* token gets a low score but both *awful* and *garbage* get high
    scores, so the method will return a high probability of the document being negative.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们正在为 *正面* 类别标签运行这个方法。单词 *beautiful* 会得到一个强烈的分数，可能是 90%，但标记 *awful* 和 *garbage*
    都会得到弱的分数，例如 5%。这种方法会报告说，对于这个文档，*正面* 标签的概率很低。另一方面，当这个方法为 *负面* 类别标签运行时，*beautiful*
    标记得到一个低的分数，但 *awful* 和 *garbage* 都得到高的分数，所以该方法会返回文档为负面的高概率。
- en: This method involves a couple of tricks. The first one is an accuracy enhancement.
    If a token is ambiguous (a word such as *that* or *movie*, something that applies
    equally to all categories), it is removed from consideration. We do this by filtering
    out token scores that are close to 50%; specifically, we ignore all tokens that
    have a score between 35-65%. This is a very effective technique and increases
    accuracy by about 10%. The reason it works so well is that it filters out noise
    in those marginal tokens. If the word *movie* has a positive score of 55% but
    is generally seen in both positive and negative documents, it'll skew all documents
    toward the positive category. Our approach is to instead only consider the most
    impactful tokens.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法涉及一些技巧。第一个技巧是准确性增强。如果一个标记是模糊的（例如像 *that* 或 *movie* 这样的词，它适用于所有类别），它就会被从考虑中移除。我们通过过滤掉接近
    50% 的标记分数来实现这一点；具体来说，我们忽略所有分数在 35-65% 之间的标记。这是一个非常有效的技术，可以提高大约 10% 的准确性。它之所以工作得很好，是因为它过滤掉了那些边缘标记中的噪声。如果单词
    *movie* 有一个正面的分数 55%，但它通常出现在正面和负面的文档中，它会使所有文档都偏向正面类别。我们的方法是不考虑那些最具影响力的标记。
- en: 'The second trick is our log sum approach. Normally, the way to combine individual
    words or token probabilities into an overall probability looks like this—assuming
    you already have an array variable called `tokenScores`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个技巧是我们的对数和技巧。通常，将单个单词或标记概率组合成整体概率的方法如下——假设你已经有一个名为 `tokenScores` 的数组变量：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Put another way, assume you have probabilities for individual tokens called
    `p1`, `p2`, `p3`, ... `pN`; the way to get the combined probability of all those
    tokens would be:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，假设你有一些称为 `p1`、`p2`、`p3`、... `pN` 的单个标记的概率；获取所有这些标记的联合概率的方法是：
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This approach has some issues when dealing with small, floating-point numbers.
    If you start multiplying small, floating-point numbers by each other, you risk
    creating numbers so small that floating-point math can't deal with it, and you
    get *floating-point underflow*, or NaN in JavaScript. The solution is to convert
    this calculation to log space, and manage the whole calculation by adding natural
    log values of each of the probabilities and removing the log at the end.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在处理小的浮点数时有一些问题。如果你开始将小的浮点数相互相乘，你可能会得到非常小的数，以至于浮点数学无法处理它，这会导致 *浮点下溢*，或者在
    JavaScript 中是 NaN。解决方案是将这个计算转换为对数空间，并通过添加每个概率的自然对数值来管理整个计算，并在最后移除对数。
- en: 'The final piece of the puzzle is to generate the probabilities of each individual
    token given a label. This is where the Bayes theorem truly comes into play. What
    we''re looking for is a probability like `P(L|W)`, or the probability that the
    document has a **label** given a **word**. We need this probability for each token
    in the document, and for each label that we''re considering. However, we don''t
    have the `P(L|W)` value on hand, so we can use Bayes'' theorem to get an equivalent
    expression:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 拼图的最后一块是生成给定标签的每个单个标记的概率。这正是贝叶斯定理真正发挥作用的地方。我们寻找的是类似于 `P(L|W)` 的概率，或者说是给定一个 **单词**
    的 **标签** 的概率。我们需要为文档中的每个标记以及我们考虑的每个标签计算这个概率。然而，我们手头没有 `P(L|W)` 的值，所以我们可以使用贝叶斯定理来得到一个等价的表达式：
- en: '*P(L|W) = P(W|L)P(L) / P(W|L)P(L) + P(W|L'')P(L'')*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(L|W) = P(W|L)P(L) / P(W|L)P(L) + P(W|L'')P(L'')*'
- en: This may look complicated, but it's not bad. We are transforming the `P(L|W)` goal
    into much easier probabilities, such as `P(W|L)` (the probability the word appears
    given a label, or its frequency in that label) and `P(L)` (the probability of
    any given label). The denominator also uses the inverse probabilities, `P(W|L')`
    (the probability the word appears in any other label) and `P(L')` (the probability
    of any other label).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来很复杂，但实际上并不糟糕。我们正在将`P(L|W)`的目标转化为更容易计算的概率，例如`P(W|L)`（给定标签时单词出现的概率，或在该标签中的频率）和`P(L)`（任何给定标签的概率）。分母也使用了逆概率，`P(W|L')`（单词出现在任何其他标签中的概率）和`P(L')`（任何其他标签的概率）。
- en: We make this transformation because we can get the word frequencies just by
    counting tokens and labels when we see them during training; we do not need to
    record which tokens appear in which documents, and we can keep our database simple
    and fast.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行这种转换是因为我们可以在训练过程中通过计数token和标签来获得单词频率；我们不需要记录哪些token出现在哪些文档中，我们可以保持我们的数据库简单且快速。
- en: The preceding expression is what we've been calling the *token score*, or the
    probability that a document has a label, given that the document has a word in
    it. Making things a little more concrete, we can ask the question `P("positive
    review" | "beautiful")`, or the probability that a document is a positive movie
    review, given that the word beautiful is in it.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的表达式就是我们所说的*token score*，或者说是给定文档中包含一个单词时，文档具有某个标签的概率。为了使问题更加具体，我们可以提出这样的问题：`P("positive
    review" | "beautiful")`，或者说是给定单词beautiful时，文档是正面电影评论的概率。
- en: If there is a 50/50 chance of reviews being positive or negative, and we see
    the word *beautiful* in 10% of positive reviews and only 1% of negative reviews,
    then our `P(L|W)` probability is around 91%. (The calculation for this was `(0.1
    * 0.5) / ( (0.1 * 0.5) + (0.01 * 0.5) )`, using the preceding formula.) You can
    interpret this 91% figure as the *positivity* of the word *beautiful*. By analyzing
    all words in a document in this manner, we can combine their positivity scores
    to get an overall probability that a document is positive. The same holds for
    any type of classification, whether it's positive/negative movie reviews, spam/ham
    emails, or English/French/Spanish language detection.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果评论是正面或负面的概率各占50%，并且我们在10%的正面评论中看到了单词*beautiful*，而在1%的负面评论中只看到了它，那么我们的`P(L|W)`概率大约是91%。（这个计算的公式是`(0.1
    * 0.5) / ( (0.1 * 0.5) + (0.01 * 0.5) )`，使用前面的公式。）你可以将这个91%的数字理解为单词*beautiful*的*积极性*。通过以这种方式分析文档中的所有单词，我们可以将它们的积极性分数结合起来，得到一个文档是正面的整体概率。这同样适用于任何类型的分类，无论是正面/负面电影评论，垃圾邮件/非垃圾邮件，还是英语/法语/西班牙语语言检测。
- en: There is one other thing we need to consider when calculating token scores.
    What do we do if we've never seen a token before? Or if we've only seen it once
    or twice? The best approach for us is to adjust the token score that we calculate
    by a weighted average; we want to weight the average so that rare words are pulled
    toward a 50/50 score.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算token分数时，我们还需要考虑另一件事。如果我们以前从未见过一个token，或者我们只见过它一两次，我们应该怎么办？对我们来说，最好的方法是调整我们计算出的token分数的加权平均值；我们希望加权平均，使得罕见单词的分数接近50/50。
- en: 'Let''s implement all of the preceding logic. This method is long, but as you
    can see, much of the work is simply grabbing the correct counts for the various
    variables we need to calculate. We also define a *strength* for our rare word
    weighting; we define the strength as three so that we must see the token in question
    three times for it to have an equivalent weight as the default 50/50 weighting:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现前面提到的所有逻辑。这个方法可能比较长，但正如你所见，大部分工作只是简单地获取我们需要计算的各种变量的正确计数。我们还定义了一个*强度*用于罕见单词加权；我们将强度定义为三，这意味着我们必须看到这个token三次，它才能具有与默认的50/50加权等效的权重：
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To review the way this algorithm works, here is a brief summary:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾这个算法的工作方式，这里有一个简要的总结：
- en: 'To train:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 训练：
- en: Accept an input document and known label or category
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受一个输入文档和已知的标签或类别
- en: Tokenize the input document into an array of tokens
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入文档分词成一个token数组
- en: Record the total number of documents you've seen for this specific label
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录你看到这个特定标签的文档总数
- en: For each token, record the number of times you've seen this token *with this
    specific label*
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个token，记录你看到这个token与这个特定标签一起出现的次数
- en: 'To predict:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 预测：
- en: Accept an input document and tokenize it
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受一个输入文档并将其分词
- en: For each possible label (all the labels you encountered during training), and
    for each token in the document, calculate the *token score* for that token (mathematically,
    the probability of a document having that label, given that specific token)
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个可能的标签（你在训练过程中遇到的全部标签），以及文档中的每个标记，计算该标记的*标记分数*（从数学上讲，给定特定标记的文档具有该标签的概率）
- en: You may need to filter token scores for significance
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能需要过滤标记分数的重要性
- en: You may need to adjust token scores for rare words
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能需要调整罕见词的标记分数
- en: For each possible label, combine the token scores into a single, overall label
    probability (for example, the probability that the document is in this category
    or label)
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个可能的标签，将标记分数组合成一个单一的、整体的标签概率（例如，文档属于这个类别或标签的概率）
- en: Report the label with the highest overall probability
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 报告具有最高整体概率的标签
- en: With all of our code added, we're ready to train and test our Naive Bayes classifier.
    We'll train it on IMDB movie reviews, and try to guess the sentiment of never-before-seen
    reviews.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加了所有代码后，我们准备训练和测试我们的朴素贝叶斯分类器。我们将使用IMDB电影评论来训练它，并尝试猜测从未见过的评论的情感。
- en: Example 3 – Movie review sentiment
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 3 – 电影评论情感
- en: We're going to use our Naive Bayes classifier to tackle the *sentiment analysis*
    problem, or the problem of inspecting a piece of text and determining whether
    it has an overall positive or negative sentiment. This is a common analysis done
    in advertising, marketing, and public relations; most brand managers want to know
    whether people on Twitter have good things or bad things to say about their brand
    or product.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们的朴素贝叶斯分类器来解决*情感分析*问题，或者检查一段文本并确定它是否具有整体正面或负面情感的问题。这在广告、营销和公共关系中是一个常见的分析；大多数品牌经理想知道推特上的人对他们的品牌或产品是好评还是差评。
- en: The training data for this example will come from [https://www.imdb.com/](https://www.imdb.com/).
    We'll train our classifier on positive and negative movie reviews, and then use
    our classifier to check untrained (but pre-labeled) reviews to see how many it
    gets right.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本例的训练数据将来自[https://www.imdb.com/](https://www.imdb.com/)。我们将使用正面和负面的电影评论来训练我们的分类器，然后使用我们的分类器来检查未经训练（但已预先标记）的评论，看看它能正确识别多少。
- en: 'If you haven''t done so yet, download the data files from the `data` directory
    from this project''s GitHub page. You will need all four text files: `train_positive.txt`,
    `train_negative.txt`, `test_positive.txt`, and `test_negative.txt`. We will use
    the two training files for training, and the two test files for validation.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，请从本项目的GitHub页面上的`data`目录下载数据文件。你需要所有四个文本文件：`train_positive.txt`、`train_negative.txt`、`test_positive.txt`和`test_negative.txt`。我们将使用两个训练文件进行训练，两个测试文件进行验证。
- en: 'Next, create an `index.js` file in the `src` folder. Add the following code
    to the top of the file:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在`src`文件夹中创建一个`index.js`文件。将以下代码添加到文件顶部：
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We import the `readline` and `fs` libraries to help us process the training
    files. Next, create a `utility` function to help us train the classifier:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入`readline`和`fs`库来帮助我们处理训练文件。接下来，创建一个`utility`函数来帮助我们训练分类器：
- en: '[PRE25]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This `helper` function accepts a filename, a label, and an instance of a `BayesClassifier`
    class. It reads an input file line by line, and trains the classifier on each
    line for the given label. All the logic is wrapped up in a promise so that we
    can externally detect when the trainer has completed.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`helper`函数接受一个文件名、一个标签和一个`BayesClassifier`类的实例。它逐行读取输入文件，并针对给定的标签在每个标签上训练分类器。所有逻辑都被封装在一个承诺中，这样我们就可以在外部检测到训练器何时完成。
- en: 'Next, add a helper utility to test the classifier. In order to test the classifier,
    it must be trained first. The testing function will open up a test file with a
    known label, and test each line in the file using the classifier''s `predict`
    method. The utility will count how many examples the classifier got right and
    how many it got wrong, and report back:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，添加一个辅助实用工具来测试分类器。为了测试分类器，它必须首先被训练。测试函数将打开一个已知标签的测试文件，并使用分类器的`predict`方法测试文件中的每一行。实用工具将计算分类器正确和错误识别的例子数量，并报告：
- en: '[PRE26]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We also wrap this in a promise, and make sure to deliver the results as part
    of the resolution of the promise, so we can inspect the results from without.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也将这个操作封装在一个承诺中，并确保将结果作为承诺解决的一部分提供，这样我们就可以从外部检查结果。
- en: 'Finally, add some bootstrap code. This code will train the classifier on the
    two training files, wait for training to complete, and then test the classifier
    on the two test files, reporting the overall results when finished:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，添加一些引导代码。这段代码将在两个训练文件上训练分类器，等待训练完成，然后在对两个测试文件进行测试后报告整体结果：
- en: '[PRE27]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once this code is added, you can run the program by issuing `yarn start` from
    the command line. You should see output like the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦添加了这段代码，你就可以通过在命令行中输入`yarn start`来运行程序。你应该会看到以下类似的输出：
- en: '[PRE28]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This simple, probabilistic classifier has an accuracy of over 91%! A 9% error
    rate may not seem impressive, but in the ML world this is actually a very good
    result, especially considering the ease of implementation and speed of operation
    of the classifier. These results are why the Naive Bayes classifier is so popular
    when classifying text. With more thoughtful tokenization, especially in narrow
    fields, such as spam detection, you can get the accuracy of a Naive Bayes classifier
    over 95%.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单、概率性的分类器准确率超过91%！9%的错误率可能看起来并不令人印象深刻，但在机器学习世界中，这实际上是一个非常良好的结果，特别是考虑到分类器的实现简便性和操作速度。正是这些结果使得朴素贝叶斯分类器在文本分类中非常受欢迎。通过更细致的标记化，尤其是在狭窄领域，如垃圾邮件检测，你可以将朴素贝叶斯分类器的准确率提高到95%以上。
- en: 'Let''s see what an individual example looks like. You can add the following
    code to the `index.js` file if you would like to test out some documents on your
    own:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个单独的例子是什么样的。如果你想在自己的文档上测试一些文档，可以将以下代码添加到`index.js`文件中：
- en: '[PRE29]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Running the preceding code results in the following code:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码会产生以下代码：
- en: '[PRE30]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The classifier works as expected. Our strongly negative statement has a 97%
    probability of being negative. Our positive statement has an 86% probability of
    being positive. And our indifferent statement, even though it returns the negative
    label, also reports an even 50/50 probability split between positive and negative
    sentiments.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器按预期工作。我们强烈的负面陈述有97%的概率是负面的。我们的正面陈述有86%的概率是正面的。即使我们的中立陈述返回了负面标签，也报告了正面和负面情绪的50/50概率分割。
- en: We did all this, and achieved great accuracy, by simply counting the number
    of times we saw words in documents and using centuries-old probability theory
    to interpret the data. We didn't need a neural network, an advanced framework,
    or a deep natural language programming knowledge to get these results; for these
    reasons, the Naive Bayes classifier should be one of the core algorithms you pay
    attention to when researching ML.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过简单地计算我们在文档中看到单词的次数，并使用几个世纪的概率理论来解释数据，就完成了所有这些工作，并取得了很高的准确率。我们不需要神经网络、高级框架或深度自然语言编程知识来获得这些结果；因此，朴素贝叶斯分类器应该是你在研究机器学习时应该关注的核心算法之一。
- en: 'In the following sections, we''ll take a look at two more classification algorithms
    that should not be ignored: the SVM and the random forest.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨两个不应被忽视的分类算法：SVM和随机森林。
- en: Support Vector Machine
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: An SVM is a numerical classifier that in some ways is similar to the KNN algorithm,
    although the SVM is far more mathematically advanced. Rather than comparing a
    test point to the points closest to it, an SVM attempts to draw boundary lines
    between the classes of data points, creating regions where all points inside that
    region will be considered a member of that class.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）是一种数值分类器，在某些方面与KNN算法相似，尽管SVM在数学上更为先进。SVM不是将测试点与其最近的点进行比较，而是试图在数据点的类别之间绘制边界线，创建一个区域，其中该区域内的所有点都将被视为该类别的成员。
- en: 'Consider this image (from Wikipedia''s article on SVMs). The two categories
    of data points are separated by a straight line. The line that separates the classes
    is chosen as the line of *maximum margin*, meaning this dividing line has the
    most room on either side of it, as compared to any other separating line you can
    draw:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这张图片（来自维基百科关于SVM的文章）。数据点的两个类别由一条直线分开。分隔类别的线被选为*最大间隔线*，这意味着这条分割线在其两侧都有最多的空间，与你可以绘制的任何其他分割线相比：
- en: '![](img/45f79df8-1acd-4185-971b-6e4054d2de29.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/45f79df8-1acd-4185-971b-6e4054d2de29.png)'
- en: The SVM, exactly as implemented here, is useful in some limited situations,
    but is not a powerful tool, because it requires that the classes be *linearly
    separable*; that is, it requires that you can draw a straight line through the
    two classes. This SVM is also a *binary classifier*, meaning it only works with
    two categories or classes.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这里实现的那样，SVM在某些有限情况下是有用的，但它不是一个强大的工具，因为它要求类别必须是*线性可分的*；也就是说，它要求你可以在两个类别之间画一条直线。这个SVM也是一个*二元分类器*，意味着它只处理两个类别或类别。
- en: 'Consider the following data (this image and the one after are both courtesy
    of Shiyu Ji, licensed under Creative Commons CC BY-SA 4.0). While there are only
    two classes, they are not linearly separable; only a circle or ellipse can separate
    the two classes:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下数据（此图像及之后的图像均由Shiyu Ji提供，并授权使用Creative Commons CC BY-SA 4.0许可）。尽管只有两个类别，但它们并不是线性可分的；只有圆形或椭圆形才能将这两个类别分开：
- en: '![](img/82c62bf3-1bbb-4414-83c3-d57062f504ab.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82c62bf3-1bbb-4414-83c3-d57062f504ab.png)'
- en: 'While the SVM has been around since the 1960s, it wasn''t until 1992 that researchers
    figured out how to approach this problem. By using a technique called the **kernel
    trick**, you can transform the non-linearly-separable data into linearly-separable
    data in a higher number of dimensions. In this case, transforming the data through
    a kernel will add a third dimension, and it''s that new third dimension that becomes
    linearly separable:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然SVM自20世纪60年代以来就存在，但直到1992年研究人员才找到了解决这个问题的方法。通过使用一种称为**核技巧**的技术，可以将非线性可分的数据转换成更高维度的线性可分数据。在这种情况下，通过核转换数据将增加一个第三维度，而正是这个新的第三维度变得线性可分：
- en: '![](img/d7619f08-a3f9-402b-a36f-a1ad21502235.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7619f08-a3f9-402b-a36f-a1ad21502235.png)'
- en: After applying the kernel trick, the data has been mapped onto three dimensions.
    The class of red data points have been pulled downward in the third dimension,
    while the purple points have been pulled upward. It's now possible to draw a plane
    (the three-dimensional equivalent of a straight line in two dimensions) that separates
    the two categories.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 应用核技巧后，数据已经被映射到三维空间。红色数据点在第三维度上被向下拉，而紫色点则被向上拉。现在可以绘制一个平面（在二维空间中直线的三维等价物），以分离这两个类别。
- en: Through appropriate selection of kernels and parameters, the support vector
    machine can work its way through all sorts of shapes of data. While the support
    vector machine will always draw a line, plane, or hyperplane (a higher-dimensional
    version of a plane) through the data—these are always *straight—*the algorithm
    first transforms the data into something that can be separated by straight lines.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 通过适当选择核和参数，支持向量机可以处理各种形状的数据。虽然支持向量机总是会在数据上绘制一条线、一个平面或超平面（平面的更高维版本）——这些总是*直线*——但算法首先将数据转换成可以用直线分离的形式。
- en: There are many types of kernels that can be used with an SVM. Each kernel transforms
    the data in a different manner, and the appropriate selection of kernel will depend
    on the shape of your data. In our case, we will use the *radial basis function
    kernel*, which is a good general-purpose kernel to use for clustered data. The
    SVM itself has settings and parameters that you must tweak, such as the error
    cost parameter, but keep in mind that the kernel you select may also have its
    own configurable parameters. The radial basis function, for instance, uses a parameter
    called **gamma**, which controls the curvature of the kernel.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 可以与SVM一起使用的核类型有很多。每种核以不同的方式转换数据，而适当的核选择将取决于你的数据形状。在我们的案例中，我们将使用*径向基函数核*，这是一种适用于聚类数据的良好通用核。SVM本身有设置和参数需要调整，例如错误成本参数，但请记住，你选择的核也可能有自己的可配置参数。例如，径向基函数使用一个称为**gamma**的参数，它控制核的曲率。
- en: Because SVMs require a lot of math, we won't attempt to build our own. Instead,
    we'll use an off-the-shelf library with a popular, classical dataset. The dataset
    we'll use is called the `iris flower` dataset. This particular dataset was created
    around 1936 by Edgar Anderson (a botanist) and Ronald Fisher (a statistician and
    biologist). Anderson chose three species of iris flowers, specifically the *Iris
    setosa,* the *Iris versicolor,* and the *Iris virginica*. For each species, Anderson
    chose 50 samples and measured the petal length, petal width, sepal length, and
    sepal width, and recorded the measurements along with the species name (a *sepal*
    is the green leaf that protects the flower bud before it blooms).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SVM需要大量的数学知识，我们不会尝试自己构建。相反，我们将使用一个现成的库和一个流行的经典数据集。我们将使用的数据集被称为`iris flower`数据集。这个特定的数据集是在1936年由Edgar
    Anderson（一位植物学家）和Ronald Fisher（一位统计学家和生物学家）创建的。Anderson选择了三种鸢尾花物种，具体是* Iris setosa*、*
    Iris versicolor* 和 * Iris virginica*。对于每种物种，Anderson选择了50个样本，并测量了花瓣长度、花瓣宽度、萼片长度和萼片宽度，并记录了测量值以及物种名称（*萼片*是保护花蕾在开花前生长的绿色叶子）。
- en: 'The `Iris` dataset is a common toy or test dataset for many ML algorithms for
    a few reasons. It''s a small dataset: there are only 150 samples, four dimensions
    or features, and three categories. The data is multidimensional, but with only
    four features, it is still easy to visualize and understand intuitively. The pattern
    in the data is also interesting and poses a non-trivial challenge for classifiers:
    one species (*Iris setosa*) is clearly separated from the other two, but *Iris
    versicolor* and *Iris virginica* are more intermingled.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`Iris`数据集是许多机器学习算法的常见玩具或测试数据集，原因有几个。这是一个小数据集：只有150个样本，四个维度或特征，以及三个类别。数据是多维的，但只有四个特征，仍然容易可视化和直观理解。数据中的模式也很有趣，对分类器提出了非平凡的挑战：一种物种（*
    Iris setosa*）与其他两种物种明显分离，但 * Iris versicolor* 和 * Iris virginica* 则更为交织。'
- en: 'Because the data is four-dimensional, it cannot be visualized directly, but
    we can plot each combination of two features separately into a grid. This image
    is courtesy of Wikipedian Nicoguaro, and is licensed CC BY 4.0:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 因为数据是四维的，不能直接可视化，但我们可以将两个特征的所有组合分别绘制到网格中。此图像由维基百科用户Nicoguaro提供，并授权为CC BY 4.0：
- en: '![](img/a9554a32-a247-4bae-85a1-f90aac3b5203.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a9554a32-a247-4bae-85a1-f90aac3b5203.png)'
- en: You can see why this dataset would be interesting to researchers. In several
    dimensions, such as sepal length versus sepal width, the *Iris versicolor* and
    *Iris virginica* overlap a great deal; in others, they look nearly linearly separable,
    for instance, in the petal length versus petal width plots.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以理解为什么这个数据集对研究人员来说很有趣。在几个维度上，例如花瓣长度与花瓣宽度的比较，* Iris versicolor* 和 * Iris virginica*
    有很大的重叠；在其他维度上，它们看起来几乎是线性可分的，例如在花瓣长度与花瓣宽度图上。
- en: Let's finally implement an SVM to solve this problem for us.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们实现一个支持向量机（SVM）来帮我们解决这个问题。
- en: 'Create a new folder called `Ch5-SVM` and add the following `package.json` file:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`Ch5-SVM`的新文件夹，并添加以下`package.json`文件：
- en: '[PRE31]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Once the file is in place, run `yarn install` to install all the dependencies.
    Rather than using a `data.js` file, we will use the `Iris` dataset that comes
    with the `MLJS` library.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文件就绪，运行`yarn install`来安装所有依赖项。我们不会使用`data.js`文件，而是将使用`MLJS`库附带的`Iris`数据集。
- en: 'Next, create an `src` folder and an `index.js` file. At the top of `index.js`,
    import the following:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建一个`src`文件夹和一个`index.js`文件。在`index.js`的顶部，导入以下内容：
- en: '[PRE32]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we need to extract the data from the `IrisDataset` library. This implementation
    of the SVM algorithm requires our labels to be integers (it doesn''t support strings
    as labels), so we must simply map the species names from the dataset to integers:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要从`IrisDataset`库中提取数据。这个SVM算法的实现要求我们的标签必须是整数（它不支持将字符串作为标签），因此我们必须将数据集中的物种名称映射到整数：
- en: '[PRE33]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s also write a simple function that measures accuracy, or more specifically
    *los**s* (or error). This function must accept an array of expected values as
    well as an array of actual values, and return the proportion of incorrect guesses:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也编写一个简单的函数来衡量准确度，或者更具体地说，*损失*（或误差）。这个函数必须接受一个预期值的数组以及一个实际值的数组，并返回错误猜测的比例：
- en: '[PRE34]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We''re now ready to implement the SVM class. We will test our classifier in
    two ways: first, we''ll train the classifier on the full dataset and then test
    it on the full dataset; this will test the algorithm''s ability to fit data. Then
    we will use a cross-validation method to train the classifier on only subsets
    of the data and test it on unseen data; this will test the algorithm''s ability
    to generalize its learning.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备实现 SVM 类。我们将以两种方式测试我们的分类器：首先，我们将在整个数据集上训练分类器，然后在整个数据集上测试它；这将测试算法拟合数据的能力。然后我们将使用交叉验证方法，只对数据的子集进行训练，并在未见过的数据上进行测试；这将测试算法泛化其学习的能力。
- en: 'Add the following code to `index.js`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下代码添加到 `index.js`：
- en: '[PRE35]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We initialize the SVM with some reasonable parameters. We choose the radial
    basis function as our kernel, we choose a specific algorithm called **CSVC** for
    our SVM (this is the most common SVM algorithm), and we choose values of 1 for
    cost and 0.25 for gamma. Cost and gamma will both have similar effects on how
    the classifier draws boundaries around your classes: the larger the values, the
    tighter the curves and boundaries around the clusters will be.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一些合理的参数初始化 SVM。我们选择径向基函数作为我们的核函数，我们选择一个称为 **CSVC** 的特定算法作为我们的 SVM（这是最常见的
    SVM 算法），我们选择成本为 1，gamma 为 0.25。成本和 gamma 都将对分类器围绕类别绘制边界的方式产生类似的影响：值越大，围绕聚类的曲线和边界就越紧密。
- en: 'The `svm.crossValidation` method accepts three arguments: the data, the labels,
    and the number of segments to divide the data into, reserving one segment for
    validation on each pass.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`svm.crossValidation` 方法接受三个参数：数据、标签以及将数据分割成多少个段，每个遍历保留一个段用于验证。'
- en: 'Run `yarn start` from the command line and you should see the following:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 从命令行运行 `yarn start`，你应该会看到以下内容：
- en: '[PRE36]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This is a very strong result. The SVM was able to correctly recall 99% of training
    examples, meaning only a couple of data points were guessed incorrectly after
    being fully trained. When crossvalidating, we see a loss of only 3%; only perhaps
    five examples out of 150 were guessed incorrectly. The crossvalidation step is
    important because it more accurately represents what real-world performance would
    be; you should tune your algorithm's parameters so that the crossvalidated accuracy
    is maximized.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常强的结果。SVM 能够正确回忆起 99% 的训练示例，这意味着在完全训练后，只有几个数据点被错误地猜测。在交叉验证时，我们只看到 3% 的损失；只有可能五例中的
    150 个例子被错误地猜测。交叉验证步骤很重要，因为它更准确地代表了现实世界的性能；你应该调整算法的参数，以便交叉验证的准确率最大化。
- en: 'It is easy to get 100% accuracy for the fully-trained algorithm: we can simply
    overfit the data and memorize the category of each datapoint. Change the values
    of both gamma and cost to 50 and re-run the algorithm. You should see something
    like this:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对于完全训练的算法，获得 100% 的准确率很容易：我们可以简单地过度拟合数据并记住每个数据点的类别。将 gamma 和 cost 的值都改为 50 并重新运行算法。你应该会看到类似以下内容：
- en: '[PRE37]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: By cranking up the cost and the gamma, we're drawing really tight boundaries
    around our existing data points. With a high enough value of cost and gamma, we
    might even be drawing individual circles around each and every data point! The
    result is a perfect score when testing the fully-trained classifier (for example,
    every training point has been memorized), but an awful score when cross-validating
    the dataset. Our cross-validation uses 80% of the data for training and reserves
    20% for validation; in this case, we've overfit the training data so much that
    the classifier simply cannot categorize unseen data points. The classifier has
    memorized the data, but has not learned from it.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提高成本和 gamma 的值，我们正在围绕现有数据点绘制非常紧密的边界。当成本和 gamma 的值足够高时，我们甚至可能为每个数据点绘制单独的圆圈！当测试完全训练的分类器时（例如，每个训练点都已记住），结果是完美的分数，但在交叉验证数据集时，分数会很糟糕。我们的交叉验证使用
    80% 的数据用于训练，并保留 20% 用于验证；在这种情况下，我们过度拟合了训练数据，以至于分类器根本无法对未见过的数据点进行分类。分类器记住了数据，但没有从中学习。
- en: 'As a rule of thumb, a good starting point for the cost value is around 1\.
    A higher cost will penalize training errors more harshly, meaning that your classification
    boundaries will try to more tightly wrap the training data. The cost parameter
    attempts to balance the simplicity of the boundaries with the recall of the training
    data: a lower cost will favor simpler, smoother boundaries, while a higher cost
    will favor higher training accuracy even if it means drawing more complex boundaries.
    This might lead to large sections of the sample space being misclassified with
    real-world data, especially if your dataset is highly dispersed. A higher cost
    value works better for very tightly clustered and neatly separated data; the more
    you trust your data, the higher you can make the cost. Values between 0.01 and
    100 are most common for the cost parameter, though there are certainly cases where
    you may need a larger or smaller cost.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 作为经验法则，成本值的良好起点大约是1。较高的成本会更严厉地惩罚训练错误，这意味着你的分类边界将试图更紧密地包裹训练数据。成本参数试图在边界的简单性和训练数据的召回率之间取得平衡：较低的成本将倾向于更简单、更平滑的边界，而较高的成本将倾向于更高的训练准确率，即使这意味着绘制更复杂的边界。这可能会导致样本空间的大部分区域在现实世界数据中被错误分类，特别是如果你的数据集高度分散。较高的成本值对于非常紧密聚集和清晰分离的数据效果更好；你越信任你的数据，你可以将成本设置得越高。成本参数最常见的是介于0.01和100之间，尽管当然也有可能需要更大或更小的成本值。
- en: Similarly, the gamma value also controls the shape and curvature of the SVM's
    boundaries, however, this value influences the data preprocessing when applying
    the kernel trick to transform the data. The result is similar to that of the cost
    parameter, but arises from a completely different mechanism. The gamma parameter
    essentially controls the influence of a single training example. Lower values
    for gamma will result in smoother, broader boundaries around training points,
    while higher values will result in closer, tighter boundaries. One common rule
    of thumb for gamma is to set it to roughly 1/M, where M is the number of features
    in your data. In our case, we have four features or dimensions in our data, so
    we've set gamma to 1/4 or 0.25.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，gamma值也控制SVM边界的形状和曲率，然而，这个值在应用核技巧转换数据时的数据预处理中产生影响。结果是与成本参数相似，但源于完全不同的机制。gamma参数本质上控制单个训练样本的影响。gamma值较低将导致训练点周围的边界更平滑、更宽，而较高的值将导致边界更紧密。gamma的一个常见经验法则是将其设置为大约1/M，其中M是数据中的特征数量。在我们的例子中，我们的数据有四个特征或维度，因此我们将gamma设置为1/4或0.25。
- en: 'When training an SVM for the first time, you should always use cross-validation
    to tune your parameters. As with any ML algorithm, you''ll have to tune the parameters
    to fit your data set and make sure that you''re sufficiently generalizing the
    problem and not overfitting your data. Tune and test parameters methodically:
    for instance, choose five possible values for cost and five possible values for
    gamma, test all 25 combinations with cross-validation, and choose the parameters
    with the highest accuracy.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 当第一次训练支持向量机（SVM）时，你应该始终使用交叉验证来调整你的参数。与任何机器学习（ML）算法一样，你必须调整参数以适应你的数据集，并确保你对问题进行了充分的泛化，而不是过度拟合你的数据。有系统地调整和测试参数：例如，选择五个可能的成本值和五个可能的gamma值，使用交叉验证测试所有25种组合，并选择具有最高准确率的参数。
- en: 'Next, we''ll take a look at a modern workhorse of ML: the random forest.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨机器学习的一个现代工作马：随机森林。
- en: Random forest
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: The random forest algorithm is modern, versatile, robust, accurate, and is deserving
    of consideration for nearly any new classification task that you might encounter.
    It won't always be the best algorithm for a given problem domain, and it has issues
    with high dimensional and very large datasets. Give it more than 20-30 features
    or more than, say, 100,000 training points and it will certainly struggle in terms
    of resources and training time.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法是现代的、多才多艺的、健壮的、准确的，并且对于你可能会遇到的几乎所有新的分类任务都值得考虑。它不总是给定问题域的最佳算法，并且在高维和非常大的数据集上存在问题。如果你有超过20-30个特征或超过，比如说，10万个训练点，它肯定会在资源和训练时间上遇到困难。
- en: 'However, the random forest is virtuous in many ways. It can easily handle features
    of different types, meaning that some features can be numerical and others can
    be categorical; you can blend features such as `number_of_logins: 24` with features
    such as `account_type: guest`. A random forest is very robust to noise and therefore
    performs well with real-world data. Random forests are designed to avoid overfitting,
    and therefore are quite easy to train and implement, requiring less tweaking and
    tuning than other algorithms. Random forests also automatically evaluate the importance
    of each feature of your data, and therefore can help you reduce dimensionality
    or select better features *for free*, so to speak. And while random forests can
    be expensive for high dimensional data, in my experience, most real-world ML problems
    involve only about a dozen features and a few thousand training points, which
    random forests can handle. These virtues make the random forest a great go-to
    algorithm for general-purpose classification tasks.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，随机森林在许多方面都是优秀的。它可以轻松处理不同类型的特征，这意味着一些特征可以是数值型的，而其他特征可以是分类型的；你可以将如`number_of_logins:
    24`这样的特征与如`account_type: guest`这样的特征混合。随机森林对噪声非常鲁棒，因此在实际数据上表现良好。随机森林旨在避免过拟合，因此训练和实现起来非常简单，需要的调整和微调比其他算法少。随机森林还会自动评估你数据中每个特征的重要性，因此可以免费帮助你降低维度或选择更好的特征。尽管随机森林在高维数据上可能成本较高，但根据我的经验，大多数现实世界的机器学习问题只涉及大约十几个特征和几千个训练点，而随机森林可以处理这些。这些优点使随机森林成为通用分类任务的优秀算法选择。'
- en: I am therefore heartbroken to report that, at the time of writing, I have found
    no high-quality random forest classifiers in the JavaScript ecosystem. Regardless,
    I'm going to continue writing this section—and even show you one existing library
    that I believe may have some bugs or problems—in the hopes that by the time you
    read this everything will be fixed and high-quality random forests will be readily
    available in JavaScript.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我非常难过地报告说，在写作的时候，我在JavaScript生态系统中没有找到高质量的随机森林分类器。无论如何，我将继续撰写这一部分——甚至向你展示一个我认为可能存在一些错误或问题的现有库——希望在你阅读这段文字的时候，一切都已经修复，高质量的随机森林将在JavaScript中轻松可用。
- en: Random forests are a type of *ensemble* classifier built on top of decision
    trees. An ensemble classifier comprises several or many individual classifiers
    that all vote on the prediction. In [Chapter 2](94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml),
    *Data Exploration*, we ran the k-means algorithm several times with different
    random initial conditions in order to avoid getting caught in local optima; that
    was a rudimentary example of ensemble classification.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种基于决策树的**集成**分类器。集成分类器由多个或许多个单独的分类器组成，它们都对预测进行投票。在[第2章](94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml)“数据探索”中，我们多次运行了k-means算法，并使用不同的随机初始条件，以避免陷入局部最优；这是一个基本的集成分类示例。
- en: 'A random forest is an ensemble of *decision trees*. You''re probably already
    familiar with decision trees: in everyday life, decision trees are more commonly
    called **flowcharts**. In an ML context, decision trees are automatically trained
    and built by an algorithm, rather than drawn by hand.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一组**决策树**。你可能已经熟悉决策树了：在日常生活中，决策树更常被称为**流程图**。在机器学习（ML）的背景下，决策树是由算法自动训练和构建的，而不是手工绘制。
- en: 'First, let''s discuss a single decision tree. Decision trees predate random
    forests, but have historically been of only moderate usefulness to ML. The concept
    behind a decision tree is the same as a hand-drawn flowchart. When a decision
    tree evaluates a data point, it''ll check each feature in turn: *is petal length
    less than 1.5 centimeters? If so, check the sepal length; if not, check the petal
    width.* Eventually, the decision tree will come to a final leaf or node where
    no more decisions are possible, and the tree will predict the category of the
    data point.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们讨论单个决策树。决策树在随机森林之前就已经存在，但历史上对机器学习的贡献仅限于中等水平。决策树背后的概念与手绘流程图相同。当决策树评估一个数据点时，它会依次检查每个特征：*花瓣长度是否小于1.5厘米？如果是，检查萼片长度；如果不是，检查花瓣宽度。*最终，决策树会到达一个最终的叶子或节点，在那里不再可能做出决策，然后树会预测数据点的类别。
- en: Decision trees are automatically trained by using a couple of concepts from
    information theory, such as information gain, entropy, and a metric called **Gini
    impurity**. In essence, these techniques are used to determine what the most important
    branching decisions are. A decision tree wants to be as small and simple as possible,
    so these techniques are used to determine how best to split the dataset between
    decisions and when. Should the first branch in the tree check petal width or sepal
    length? If it checks sepal length, should it split at 2.0 centimeters or 1.5 centimeters?
    Which comparisons will result in the best splits for the whole dataset? This training
    is done recursively, and each feature and each training point is evaluated to
    determine its effect on the whole.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通过使用信息理论中的几个概念自动训练，例如信息增益、熵以及一个称为**基尼不纯度**的度量。本质上，这些技术用于确定最重要的分支决策是什么。决策树希望尽可能小和简单，因此这些技术用于确定如何最好地在决策之间分割数据集。树的第一个分支应该检查花瓣宽度还是萼片长度？如果它检查萼片长度，应该是在2.0厘米还是1.5厘米处分割？哪些比较将导致整个数据集的最佳分割？这种训练是递归进行的，每个特征和每个训练点都会被评估以确定其对整体的影响。
- en: The result is a lightning-fast classifier that is also easy to understand and
    debug. Unlike a neural network, where the influence of each neuron is highly abstract,
    and unlike a Bayesian classifier, which requires skill in probability to understand,
    a decision tree can be rendered as a flowchart and interpreted directly by a researcher.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个既快速又易于理解和调试的分类器。与神经网络不同，其中每个神经元的影響非常抽象，也与贝叶斯分类器不同，后者需要概率方面的技能才能理解，决策树可以被表示为流程图，并由研究人员直接解释。
- en: Unfortunately, a decision tree by itself is not very accurate, they are not
    robust to changes in training data or noise, they can get trapped in local optima,
    and there are certain categories of problems that a decision tree cannot handle
    well (like the classic XOR problem, which will result in a very complex tree).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 很不幸，决策树本身并不非常准确，它们对训练数据或噪声的变化不稳健，可能会陷入局部最优，而且有一些问题类别决策树处理得并不好（比如经典的XOR问题，会导致树变得非常复杂）。
- en: In the mid-1990s, researchers figured out two new ensemble approaches to decision
    trees. First, the technique of *sample bagging* (or *bootstrap aggregating*) was
    developed. In this approach, you create a number of decision trees, each based
    on a totally random subset of the training data (with replacement), and you use
    the majority vote of all the trees when coming up with a prediction. Bagging works
    because the variance due to noise is high for a single tree, but for many uncorrelated
    trees the noise tends to cancel out. Think of concertgoers singing along to their
    favorite band in an arena—the crowd always sounds in tune because the people singing
    sharp get canceled out by the people singing flat.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪90年代中期，研究人员找到了两种新的决策树集成方法。首先，开发了**样本袋装法**（或**自助聚集**）技术。在这种方法中，你创建多个决策树，每个树基于训练数据的完全随机子集（有放回），并在做出预测时使用所有树的多数投票。袋装法之所以有效，是因为单个树的噪声方差很高，但对于许多不相关的树，噪声往往会相互抵消。想象一下在体育馆里，观众们跟着他们最喜欢的乐队一起唱歌——人群总是听起来很和谐，因为唱得尖锐的人会被唱得平的人所抵消。
- en: The random forest builds on the idea of bagging by not only randomizing the
    samples that are given to each tree, but also the *features* that are given to
    each tree. As opposed to *sample bagging*, you could call this *feature bagging*.
    If you build a random forest of 50 trees for our `Iris` dataset (which has four
    features and 150 data points), you might expect each tree to have only 100 unique
    data points and only two of the four features. Like sample bagging, feature bagging
    serves to decouple each of the decision trees and reduce the overall variance
    of the ensemble. Feature bagging also serves to identify the most important features,
    and if you need to save resources you can always remove the least important features
    from the dataset. When you attempt to predict a data point, each of the 50 trees
    will submit its vote; some trees will be wildly incorrect, but the ensemble as
    a whole will make a very good prediction that is robust to noise.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林建立在bagging（袋装法）的基础上，不仅随机化每个树接收到的样本，还随机化每个树接收到的*特征*。与*样本袋装法*相反，你可以称之为*特征袋装法*。如果你为我们的`Iris`数据集（该数据集有四个特征和150个数据点）构建一个包含50棵树的随机森林，你可能期望每棵树只有100个独特的数据点，并且只有四个特征中的两个。像样本袋装法一样，特征袋装法旨在解耦每个决策树，并减少集成整体的方差。特征袋装法还旨在识别最重要的特征，如果你需要节省资源，你总是可以从数据集中移除最不重要的特征。当你尝试预测一个数据点时，每棵树都会提交它的投票；有些树可能会非常错误，但整个集成将做出一个非常好的预测，对噪声具有鲁棒性。
- en: Let's build a random forest and test our `Iris` data against it. You should
    already have the random forest and cross-validation libraries installed in your
    `package.json` file from the SVM section; if not, you should `yarn add` both `ml-cross-validation`
    and `ml-random-forest`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个随机森林，并用我们的`Iris`数据对其进行测试。你应该已经在`package.json`文件中安装了随机森林和交叉验证库，从SVM部分开始；如果没有，你应该使用`yarn
    add`安装`ml-cross-validation`和`ml-random-forest`。
- en: 'At the top of the existing `index.js` file for the `Ch5-SVM` example, import
    the appropriate classes:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Ch5-SVM`示例的现有`index.js`文件顶部，导入适当的类：
- en: '[PRE38]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You should already have `labels` and `data` set up from the SVM section. Now,
    add the following to the bottom of the file, beneath the SVM example:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经从SVM部分设置了`labels`和`data`。现在，将以下内容添加到文件底部，在SVM示例下方：
- en: '[PRE39]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Similar to the SVM example, we're evaluating the random forest in two ways.
    We first train the forest on the full training data and evaluate its recall, then
    we use cross-validation to get an idea of its real-world performance. In this
    example, we're using MLJS's cross-validation and confusion matrix tools to evaluate
    the classifier's performance.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 与SVM示例类似，我们以两种方式评估随机森林。我们首先在全部训练数据上训练森林，并评估其召回率，然后我们使用交叉验证来了解其实际性能。在这个例子中，我们使用MLJS的交叉验证和混淆矩阵工具来评估分类器的性能。
- en: 'Run the code with `yarn start` and you should see something like the following:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`yarn start`运行代码，你应该会看到以下类似的内容：
- en: '[PRE40]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Unfortunately, the accuracy of this algorithm is very poor. In fact, this performance
    is atypical of random forests, especially for the `Iris` dataset, which should
    be very easy for an algorithm to interpret.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 很不幸，这个算法的准确性非常差。实际上，这种表现并不典型，尤其是对于`Iris`数据集，这个数据集对于算法来说应该非常容易解释。
- en: 'I wanted to be certain that these poor results were an implementation problem
    rather than a conceptual one, so I ran the same exact Iris data through a familiar
    random forest library that I use daily, using the same options and parameters,
    and I got very different results: **my random forest has a cross-validated loss
    of 2% only**. Unfortunately, I must blame this poor accuracy not on the random
    forest itself, but this specific implementation of the algorithm. While I did
    spend some time looking into the matter, I was not able to quickly identify the
    issue with this implementation. There is a possibility I am misusing this tool,
    however, it is more likely that there''s a minus sign where there should be a
    plus (or something similarly silly and disastrous) somewhere in the library. My
    personal prediction for the performance of a random forest on the `Iris` dataset
    was around 95% accuracy, my familiar random forest library resulted in 98% accuracy,
    yet this library resulted in only 70% accuracy.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要确定这些糟糕的结果是由于实现问题而不是概念问题，所以我用相同的Iris数据集通过我日常使用的熟悉的随机森林库运行，使用相同的选项和参数，但得到了非常不同的结果：**我的随机森林的交叉验证损失仅为2%**。不幸的是，我必须将这个糟糕的准确率归咎于算法的具体实现，而不是随机森林本身。虽然我花了一些时间调查这个问题，但我并没有能够快速地识别出这个实现的问题。有可能我误用了这个工具，然而，更有可能的是，在库的某个地方有一个负号应该是一个正号（或者类似愚蠢且灾难性的错误）。我对于随机森林在`Iris`数据集上的性能的个人预测是大约95%的准确率，我熟悉的随机森林库得到了98%的准确率，但这个库只得到了70%的准确率。
- en: Even worse is the fact that I was unable to find a single random forest library
    in JavaScript that works for the `Iris` dataset. There are a couple of random
    forest libraries out there, but none that are modern, maintained, and correct.
    Andrej Karpathy has an abandoned random forest library that seems to work, but
    it can only handle binary classifications (only 1 and -1 as labels), and a few
    other random forest libraries are limited in similar ways. The `MLJS` random forest
    library that we used previously is the closest thing to a working, maintained
    library that I've found, so I hope the issue—whatever it is—will be discovered
    and resolved by the time you read this.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，我无法在JavaScript中找到一个适用于`Iris`数据集的随机森林库。虽然有几个随机森林库，但没有一个是现代的、维护良好的和正确的。Andrej
    Karpathy有一个废弃的随机森林库似乎可以工作，但它只能处理二分类（只有1和-1作为标签），还有几个其他随机森林库在类似的方式下有限制。《MLJS》随机森林库是我们之前使用的最接近一个工作、维护良好的库，所以我希望无论问题是什么，它都会在你阅读这篇文章的时候被发现并解决。
- en: I do not want you to be discouraged from using random forests. If you are working
    in languages other than JavaScript, there are many random forest libraries available
    to you. You should become familiar with them as they'll quickly become your go-to
    first choice for the majority of classification problems. In terms of JavaScript,
    while random forests are harder to build from scratch than Bayesian classifiers,
    they are still quite achievable. If you are able to correctly implement a decision
    tree, or port one from a different language, building a random forest becomes
    very easy—the trees do most of the work in the forest.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我不希望你们因为使用随机森林而气馁。如果你在除了JavaScript以外的语言中工作，有许多随机森林库可供选择。你应该熟悉它们，因为它们很快就会成为你大多数分类问题的首选。至于JavaScript，虽然从零开始构建随机森林比构建贝叶斯分类器更难，但它们仍然是可以实现的。如果你能够正确实现决策树，或者从不同的语言中移植一个，构建随机森林就会变得非常简单——森林中的树做了大部分工作。
- en: While JavaScript's ML toolset is advancing all the time, this random forest
    example perfectly highlights that there's still work to be done. You must proceed
    cautiously. I started writing this example with the expectation of 95% accuracy
    or greater, based on my prior experience with random forests. But what if I had
    no expectations or experience going into it? Would I have just accepted the 70%
    accuracy from this tool? Would I have convinced myself that a random forest is
    the wrong tool for the job? Would it have discouraged me from using random forests
    in the future? Maybe! The ML in the JavaScript ecosystem will have more land mines
    like this one; look out for them.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然JavaScript的机器学习工具集一直在进步，但这个随机森林的例子完美地突出了还有很多工作要做。你必须谨慎行事。我开始写这个例子时，预期至少有95%的准确率，基于我对随机森林的先前经验。但如果没有期望或经验呢？我会接受这个工具的70%准确率吗？我会说服自己随机森林不适合这项工作吗？这会让我在将来不愿意使用随机森林吗？也许吧！JavaScript生态系统中的机器学习会有更多这样的地雷；小心它们。
- en: 'Before we finish this chapter, I would like to revisit the confusion matrix
    we just saw, as this may be a new concept for you. We discussed precision, recall,
    and accuracy in an earlier chapter. The confusion matrix is the raw data from
    which these values can be derived for any classification. Here''s the confusion
    matrix from the random forest, again:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束这一章之前，我想回顾一下我们刚才看到的混淆矩阵，因为这可能对你来说是一个新概念。我们在前面的章节中讨论了精确度、召回率和准确率。混淆矩阵是这些值可以从任何分类中得出的原始数据。这是随机森林的混淆矩阵，再次呈现：
- en: '[PRE41]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'If we organize this into a table, it might look like this:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些组织成表格，可能看起来是这样的：
- en: '|  | **Guessed *I. setosa*** | **Guessed *I. versicolor*** | **Guessed *I.
    virginica*** |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | **Guessed *I. setosa*** | **Guessed *I. versicolor*** | **Guessed *I.
    virginica*** |'
- en: '| Actual *I. setosa* | 43 | 6 | 1 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Actual *I. setosa* | 43 | 6 | 1 |'
- en: '| Actual *I. versicolor* | 8 | 11 | 31 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 实际 *I. versicolor* | 8 | 11 | 31 |'
- en: '| Actual *I. virginica* | 1 | 2 | 47 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 实际 *I. virginica* | 1 | 2 | 47 |'
- en: The confusion matrix is the matrix (or table) of guesses versus actual categories.
    In a perfect world, you want the confusion matrix to be all zeros except for the
    diagonal. The confusion matrix tells us that the random forest did a pretty good
    job of guessing *Iris setosa* and *Iris virginica*, but it got most *Iris versicolor*
    wrong and incorrectly labeled them as *Iris virginica*. This is not too surprising,
    considering the shape of the data; recall that the latter two species overlap
    quite a bit (however, a random forest still should have been able to resolve this).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是猜测与实际类别之间的矩阵（或表格）。在理想的世界里，你希望混淆矩阵除了对角线外都是零。混淆矩阵告诉我们随机森林在猜测*Iris setosa*和*Iris
    virginica*方面做得相当不错，但它错误地将大多数*Iris versicolor*标记为*Iris virginica*。考虑到数据的形状，这并不太令人惊讶；回想一下，后两种物种重叠相当多（然而，随机森林仍然应该能够解决这个问题）。
- en: 'The code we wrote for the random forest also printed out the individual predictions
    for each data point, which looked like this:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为随机森林编写的代码还打印出了每个数据点的个别预测，看起来是这样的：
- en: '[PRE42]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The numbers are not exactly the same as in the confusion matrix, because these
    predictions came from the fully-trained tree while the confusion matrix came from
    the cross-validation process; but you can see that they are still similar. The
    first 50 predictions should all be 0s (for *Iris setosa*), and they mostly are.
    The next 50 predictions should be all 1s, but they are primarily 2s; the confusion
    matrix tells us the same thing (that most `I. versicolor` were incorrectly labeled
    `I. virginica`). The last 50 predictions should be all 2s, and are for the most
    part correct. The confusion matrix is a more compact and intuitive way of looking
    at the difference between expected and actual guesses, and this is exactly the
    type of information you'll need when fine-tuning an algorithm.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字并不完全与混淆矩阵中的数字相同，因为这些预测来自完全训练的树，而混淆矩阵来自交叉验证过程；但你可以看到它们仍然很相似。前50个预测应该都是0（对于*Iris
    setosa*），而且大多数确实是。接下来的50个预测应该是全部1，但主要是2；混淆矩阵告诉我们同样的事情（即大多数`I. versicolor`被错误地标记为`I.
    virginica`）。最后50个预测应该是全部2，大部分是正确的。混淆矩阵是查看预期猜测与实际猜测之间差异的一种更紧凑和直观的方式，这正是你在微调算法时需要的信息。
- en: In short, the random forest is an excellent classifier algorithm that currently
    has no convincing JavaScript implementation. I encourage you to be part of JavaScript's
    evolution and build your own random forest, or at least keep this algorithm in
    mind for the future.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，随机森林是一种优秀的分类算法，但目前还没有令人信服的JavaScript实现。我鼓励您成为JavaScript进化的参与者，构建自己的随机森林，或者至少将这个算法记在心里以备将来使用。
- en: Summary
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Classification algorithms are a type of supervised learning algorithm whose
    purpose is to analyze data and assign unseen data points to a pre-existing category,
    label, or classification. Classification algorithms are a very popular subset
    of ML, and there are many classification algorithms to choose from.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 分类算法是一种监督学习算法，其目的是分析数据并将未见过的数据点分配到预存在的类别、标签或分类中。分类算法是机器学习（ML）中一个非常受欢迎的子集，有众多分类算法可供选择。
- en: Specifically, we discussed the simple and intuitive k-nearest-neighbor algorithm,
    which compares a data point to its neighbors on a graph. We discussed the excellent
    and very popular Naive Bayes classifier, which is a classic probability-based
    classifier that dominates the text classification and sentiment analysis problem
    spaces (though it can be used for many other types of problems). We also discussed
    the support vector machine, an advanced geometric classifier that works well for
    non-linearly-separable data. Finally, we discussed the random forest classifier,
    a robust and powerful ensemble technique that relies on decision trees but unfortunately
    has only a questionable implementation in JavaScript.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们讨论了简单直观的k近邻算法，该算法在图上比较数据点与其邻居。我们还讨论了优秀且非常受欢迎的朴素贝叶斯分类器，它是一种经典的基于概率的分类器，在文本分类和情感分析问题空间中占据主导地位（尽管它也可以用于许多其他类型的问题）。我们还讨论了支持向量机，这是一种适用于非线性可分数据的先进几何分类器。最后，我们讨论了随机森林分类器，这是一种强大且稳健的集成技术，依赖于决策树，但不幸的是，在JavaScript中只有一种有疑问的实现。
- en: We also discussed cross-validation and the confusion matrix, two powerful techniques
    for evaluating the accuracy of your models.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了交叉验证和混淆矩阵，这两种强大的技术可以用来评估你模型的准确性。
- en: In the next chapter, we'll look at association rules, which give us some more
    predictive power. If someone buys bread and butter from a store, are they more
    likely to also buy milk, or to buy deli meat? Association rules can help us model
    and interpret those relationships.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨关联规则，这些规则为我们提供了更多的预测能力。如果有人在商店购买了面包和黄油，他们更有可能还会购买牛奶，还是购买熟食肉类？关联规则可以帮助我们建模和解释这些关系。
