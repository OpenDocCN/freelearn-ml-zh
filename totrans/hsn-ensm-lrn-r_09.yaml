- en: Chapter 9. Ensembling Regression Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapters 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, to [Chapters 8](part0057_split_000.html#1MBG21-2006c10fab20488594398dc4871637ee
    "Chapter 8. Ensemble Diagnostics"), *Ensemble Diagnostics*, were devoted to learning
    different types of ensembling methods. The discussion was largely based on the
    classification problem. If the regressand/output of the supervised learning problem
    is a numeric variable, then we have a regression problem, which will be addressed
    here. The housing price problem is selected for demonstration purposes throughout
    the chapter, and the dataset is chosen from a Kaggle competition: [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/).
    The data consists of numerous variables, including as many as 79 independent variables,
    with the price of the house as the output/dependent variable. The dataset needs
    some pre-processing as some variables have missing dates, some variables have
    lots of levels, with a few of them only occurring very rarely, and some variables
    have missing data in more than 20% of observations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The pre-processing techniques will be succeeded by variable reduction methods
    and then we will fit important regression models: linear regression, neural network,
    and regression trees. An ensemble extension of the regression tree will first
    be provided, and then we will apply the bagging and random forest methods. Various
    boosting methods will be used to improve the prediction. Stacked ensemble methods
    will be applied in the concluding section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data pre-processing and visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variable reduction techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging and Random Forests for the regression data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacked ensemble methods for regression data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will need the following R packages for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`adabag`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`caret`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`caretEnsemble`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ClustofVar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FactoMinR`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gbm`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ipred`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`missForest`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nnet`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NeuralNetTools`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plyr`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rpart`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RSADBE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-processing the housing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset was selected from [www.kaggle.com](http://www.kaggle.com) and the
    title of the project is **House Prices: Advanced Regression Techniques**. The
    main files we will be using are `test.csv` and `train.csv`, and the files are
    available in the companion bundle package. A description of the variables can
    be found in the `data_description.txt` file. Further details, of course, can be
    obtained at [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/).
    The train dataset contains 1460 observations, while the test dataset contains
    1459 observations. The price of the property is known only in the train dataset
    and are not available for those in the test dataset. We will use the train dataset
    for model development only. The datasets are first loaded into an R session and
    a beginning inspection is done using the `read.csv`, `dim`, `names`, and `str`
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The dimensions of the data frames give the number of variables and the number
    of observations. The details of all the variables can be found in the `data_description.txt`
    file. It can be seen that what we have on hand is a comprehensive dataset. Now,
    we ran the option of `na.strings = "NA"` in the `read.csv` import function, and
    quite naturally, this implied that we have missing data. When we have missing
    data in both the training and test data partitions, the author recommends combining
    the covariates in the partitions and then examining them further. The covariates
    are first combined and then we find the number of missing observations for each
    of the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `rbind` function combines the data in the training and testing datasets.
    The `is.na(x)` code inspects the absence of the values for every element of `x`,
    and the `sum` applied tells us the number of missing observations for the variable.
    The function is then applied for every variable of `housing` using the `sapply`
    function. The count of missing observations for the variables is sorted in descending
    order using the `sort` function with the argument `dec=TRUE`, and hence it enables
    us to find the variables with the most missing numbers in the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reader might be wondering about the rationale behind the collation of the
    observations. The intuitive reasoning behind the collation is that while some
    variables might have missing data, more in the training data than in the test
    data, or the other way around, it is important that the overall missing percentage
    does not exceed a certain threshold of the observations. Although we have missing
    data imputation techniques, using them when the missing data percentage is too
    high might cause us to miss out on the important patterns of the features. Consequently,
    we arbitrarily make a choice of restricting the variables if more than 10% of
    the values are missing. If the missing percentage of any variable exceeds 10%,
    we will avoid analyzing that variable further. First, we identify the variables
    that exceed 10%, and then we remove them from the master data frame. The following
    R code block gives us the desired result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The variables that have more than 10% missing observations are first identified
    and then stored in the `miss_variables` character vector, and we have 11 variables
    that meet this criterion. Such variables are eliminated with the `NULL` assignment
    for them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we find the number of levels (distinct) of factor variables. We define
    a function, `find_df`, which will find the number of levels of a factor variable.
    For numeric and integer variables, it will return `1`. The purpose of this exercise
    will become clear soon enough. The `find_df` function is created in the next block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to inspect `67` variables, following the elimination of `11` variables
    with more than 10% missing observations. Some of these might not be factor variables.
    The `find_df` function shows that, for factor variables, the number of levels
    varies from 2-25\. A quick problem now arises for the `Condition2` and `Exterior1st`
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In many practical problems, it appears that there will be **factor variables**
    that have some levels that occur very infrequently. Now, if we have new levels
    in the test/validation partition, it is not possible to make predictions. From
    a statistical perspective, we have a technical problem: losing too many **degrees
    of freedom**. A rudimentary approach is pursued here, and we will simply put together
    all the observations in the `Others` umbrella. A `Truncate_Factor` function is
    created, and this has two arguments: `x` and `alpha`. The `x` object is the variable
    to be given to the function, and `alpha` is the specified fraction below which
    any variable frequency would be pooled to obtain `Others`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If there are certain levels of a factor that are new in the test dataset, no
    analytical method will be able to incorporate the influence. Thus, in cases where
    we have too many infrequent levels, the chances of some levels not being included
    in the training dataset will be high and the prediction will not yield the output
    for the test observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Truncate_Factor` function is now created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can now see that the `Others` level is more frequent and if we randomly create
    partitions, it is very likely that the problem of unknown levels will not occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may recollect that we have eliminated the variables that have excessive
    missing observations thus far. This does not mean that we are free of missing
    data, as can be quickly noticed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `474` values can''t be ignored. Missing data imputation is an important
    way of filling the missing values. Although the EM algorithm is a popular method
    to achieve that, we will apply the Random Forests technique to simulate the missing
    observations. The `missForest` package was introduced in [Chapter 4](part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee
    "Chapter 4. Random Forests"), *Random Forests*, and an example was used to simulate
    the missing values. We will apply this function to the housing data frame. Since
    the default number of variables chosen in this function is `mtry=5` and we have
    68 variables in housing, the number of variables chosen for splitting a node is
    changed to about p/3 and hence the option of `mtry=20` is seen in the next R block.
    On a machine with 8 GB of RAM, the next single-line code takes several hours to
    run. Next, we will apply the `missForest` function, save the imputed object for
    future reference, and create the test and training dataset with imputed values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The reader should certainly run the `missForest` code line on their local machine.
    However, to save time, the reader can also skip the line and then load the `ht_imp`
    and `htest_imp` objects from the code bundle. The next section will show a way
    of visualizing a large dataset and two data reduction methods.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization and variable reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, the housing data underwent a lot of analytical pre-processing,
    and we are now ready to further analyze this. First, we begin with visualization.
    Since we have a lot of variables, the visualization on the R visual device is
    slightly difficult. As seen in earlier chapters, to visualize the random forests
    and other large, complex structures, we will initiate a PDF device and store the
    graphs in it. In the housing dataset, the main variable is the housing price and
    so we will first name the output variable `SalePrice`. We need to visualize the
    data in a way that facilitates the relationship between the numerous variables
    and the `SalePrice`. The independent variables can be either numeric or categorical.
    If the variables are numeric, a scatterplot will indicate the kind of relationship
    between the variable and the `SalePrice` regressand. If the independent variable
    is categorical/factor, we will visualize the boxplot at each level of the factor.
    The `pdf`, `plot`, and `boxplot` functions will help in generating the required
    plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ht_imp` object is loaded from the `ht_imp_author.Rdata` file. Note that,
    if you run the `missForest` function on your own and work on that file, then the
    results will be different from `ht_imp_author.Rdata`. The `pdf` function is known
    to initiate a file of the same name, as seen many times earlier. For the numeric
    variable, the `if` condition is checked and a scatter plot is displayed with the
    `xlab` taking the actual name of the variable as a name for the label along the
    *x* axis. The `title` function slaps the output of the `paste` function, and the
    `paste` function ensures that we have a suitable title for the generated plot.
    Similar conditions are tested for the factor variables. We will now look at some
    of the interesting plots. The first plot of `SalePrice` with `MSSubClass (`see
    the `Visualizing_Housing_Data.pdf` file) is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization and variable reduction](img/00372.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Scatter Plot of Sales Price against MSSubClass'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note here that although we specified the `MSSubClass` variable as a numeric
    variable, the scatterplot does not give the same impression. Here, the values
    of the `MSSubClass` variable are cluttered around a specific point and then the
    scale jumps to the next value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, it does not appear to be a continuous variable and this can be easily
    verified using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Exercise**: The reader should convert the `MSSubClass` variable to a factor
    and then apply `Truncate_Factor` to reduce the noise. Identify other numeric variables
    exhibiting this property in the `Visualizing_Housing_Data.pdf` file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at the boxplot for the `MSZoning` factor variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization and variable reduction](img/00373.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Box plots of Sales Price at Three Levels of MSZoning'
  prefs: []
  type: TYPE_NORMAL
- en: The points beyond the whiskers indicate the presence of outliers. However, with
    complex problems, the interpretation is also likely to go awfully wrong. The notches
    are a useful trick in the display of boxplots. If the notches do not overlap for
    two levels of variables, it means that the levels are significant and the information
    is therefore useful, as seen in the display of the boxplot of `SalePrice` against
    the `MSZoning` levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next display of the scatterplot of `SalePrice` against `LotArea` is taken
    up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization and variable reduction](img/00374.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Scatter Plot of Sales Price against Lot Area'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, the scatterplot shows that there is no meaningful relationship between
    the two variables `SalePrice` and `LotArea`. A different type of display is seen
    between `SalePrice` and `TotalBsmtSF` in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization and variable reduction](img/00375.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Scatter Plot of Sales Price against TotalBsmtSF'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can clearly see an outlier in the `TotalBsmtSF` value at the extreme right
    of the figure. There is also a cluttering of values at 0 with `TotalBsmtSF`, which
    might be controlled by some other variable. Alternatively, it may be discovered
    that there is a zero-inflation of the variable and it therefore could be a mixture
    variable. Similarly, all other plots can be interpreted. The correlation between
    the `SalePrice` and other numeric variables is obtained next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Exercise**: Interpret all the relationships in the `Visualizing_Housing_Data.pdf`
    file and sort the correlations by their absolute value in the preceding R code.'
  prefs: []
  type: TYPE_NORMAL
- en: We made use of the variable of interest for the visualization, and in turn this
    led to useful insights. As previously stated, *p = 68* is a lot of covariates/independent
    variables. With big data, the complexity will increase in the north direction,
    and it is known that for many practical applications we have thousands of independent
    variables. While most visualization techniques are insightful, a shortcoming is
    that we seldom get insights into higher order relationships. For instance, when
    it comes to three or more variables, a relationship is seldom richly brought out
    in graphical displays. It is then important to deploy methods that will reduce
    the number of variables without being at the expense of information. The two methods
    of data reduction to be discussed here are **principal component analysis** and
    **variable clustering**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**) is a method drawn from the larger
    pool of **multivariate statistics**. This is useful in data reduction as, given
    the original number of variables, it tries to give a new set of variables that
    covers most of the variance of the original data in as few new variables as possible.
    A brief explanation of PCA is given here.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a random vector of observations ![Visualization and variable
    reduction](img/00376.jpeg). Given the random vector ![Visualization and variable
    reduction](img/00377.jpeg), PCA finds a new vector of *principal components* ![Visualization
    and variable reduction](img/00378.jpeg) such that each *Yi* is a linear combination
    of ![Visualization and variable reduction](img/00379.jpeg). Furthermore, the principal
    components are such that the variance of *Y1* is higher than the variance of *Y2*
    and both are uncorrelated; the variance of *Y2* is higher than the variance of
    *Y3* and *Y1; Y2* and *Y3* are uncorrelated, and so forth. This relates to ![Visualization
    and variable reduction](img/00380.jpeg), none of which are correlated with each
    other. The principal components are set up so that most of the variance of ![Visualization
    and variable reduction](img/00381.jpeg) is accumulated in the first few principal
    components (see Chapter 15 of Tattar, et al. (2016) for more information on this).
    As a result, we can achieve a lot of data reduction. However, the fundamental
    premise of PCA is that ![Visualization and variable reduction](img/00382.jpeg)
    is a vector of continuous random variables. In our dataset, we also have factor
    variables. Consequently, we can't use PCA for our purposes. A crude method is
    to ignore the factor variables and simply run the data reduction on the continuous
    variables. Instead, we would use `factor analysis for mixed data`, and the software
    functions to carry this out are available in the `FactoMineR` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since data reduction only needs to be performed on the covariates and we do
    not have longitudinal data, the data reduction is applied on the entire set of
    observations available, and not only on the training dataset. The rationale for
    carrying out the data reduction on the entire dataset is the same as for truncating
    the number of levels of a factor variable. The `housing_impute` data frame is
    available in `housing_covariates_impute.Rdata`. We will first load it and then
    apply the `FAMD` function to carry out the factor analysis for mixed data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `FAMD` function, the `ncp` option is set as equal to 68, since that
    is the number of variables we have. We would also like to look at how the principal
    components respond to the dataset. If the `graph=TRUE` option is selected, the
    function will display the related graphs. The `colnames` of `housing_cov_famd$eig`
    is changed as the default names don''t do justice to the output it generates.
    We can see from the Eigen value analysis that the overall 68 components do not
    complete with the entire variation available in the data. Furthermore, even for
    the 50% of variance explained by the components, we need to pick 26 of them. As
    a consequence, the data reduction here does not seem to be productive. However,
    this does not mean that performance will be poor in the next set of analyses.
    When applying the `pareto.chart` function from the quality control package `qcc,
    on frequency data` gives a Pareto chart. As demonstrated by the percentages, it
    is clear that if we need 90% of the variance in the original variables to be explained
    by the principal components, then we will need nearly 60 principal components.
    Consequently, the number of variables reduced is only 8 and the interpretation
    is also an additional complexity. This is not good news. However, we will still
    save the data of principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Visualization and variable reduction](img/00383.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Pareto Chart for Contribution of Principal Components'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Explore the use of the `PCAmix` function from the R package `PCAmix`
    to reduce the number of variables through principal component analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Variable clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Variables can be grouped together as we do with observations. To achieve this,
    we will use the `kmeansvar` function from the `ClustOfVar` package. The variable
    clustering package needs to be specified as the quantitative (numeric) variable
    separately, and the qualitative (factor) variables also need to be specified separately.
    In addition, we need to specify how many variable clusters we need. The `init`
    option helps in its specification here. The `is.numeric` and `is.factor` functions
    are used to identify the numeric and factor variables and the variable clusters
    are set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Oops! It is an error. It is important to recollect that all infrequent levels
    of factor variables have been labeled as Others. It might be the case that there
    are other levels that have the same name across variables, which is a very common
    label choice in survey data, including options such as Very Dissatisfied < Dissatisfied
    < OK < Good < Excellent. This choice of variable levels can be the same across
    multiple questions. However, we need the names of the levels to be distinct across
    all variables. A manual renaming of labels will be futile and an excessive waste
    of time. Consequently, we will approach the problem with a set of names that will
    be unique across the variables, namely the variable names themselves. The variable
    names will be concatenated with the variable levels and thus we will have distinct
    factor levels throughout. Using the `paste0` function and `mapvalues` from the
    `plyr` package, we will carry out the level renaming manipulation first and then
    apply `kmeansvar` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The important question is that although we have marked the variables in groups,
    how do we use them? The answer is provided in the coefficients of the variables
    within each group. To display the coefficients, run `$coef` in adjunct to the
    `clustvar` object `Housing_VarClust`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, for the observations in the data, the corresponding variables are multiplied
    by the coefficients of the variable clusters to obtain a single vector for that
    variable cluster. Consequently, we will then have reduced the 68 variables to
    4 variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Obtain the cluster variables for the `housing_covariates` data
    frame using the coefficients displayed previously.'
  prefs: []
  type: TYPE_NORMAL
- en: The data pre-processing for the housing problem is now complete. In the next
    section, we will build the base learners for the regression data.
  prefs: []
  type: TYPE_NORMAL
- en: Regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sir Francis Galton invented the simple linear regression model near the end
    of the nineteenth century. The example used looked at how a parent's height influences
    the height of their child. This study used data and laid the basis of regression
    analysis. The correlation between the height of parents and children is well known,
    and using data on 928 pairs of height measurements, a linear regression was developed
    by Galton. In an equivalent form, however, the method might have been in informal
    use before Galton officially invented it. The simple linear regression model consists
    of a single input (independent) variable and the output is also a single output.
  prefs: []
  type: TYPE_NORMAL
- en: In this supervised learning method, the target variable/output/dependent variable
    is a continuous variable, and it can also take values in intervals, including
    non-negative and real numbers. The input/independent variable has no restrictions
    and as such it can be numeric, categorical, or in any other form we used earlier
    for the classification problem. Interestingly though, linear regression models
    started much earlier than classification regression models such as logistic regression
    models. Machine learning problems are more often conceptualized based on the classification
    problem, and the ensemble methods, especially boosting, have been developed by
    using classification as the motive. The primary reason for this is that the error
    improvisation gives a nice intuition, and the secondary reason might be due to
    famous machine learning examples such as digit recognition, spam classification,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The simple linear regression extension is the multiple linear regression where
    we allow more than one independent variable. We will drop the convention of simple
    and multiple regression altogether and adhere simply to regression. As a base
    learner, the linear regression model is introduced first. Interesting datasets
    will be used to kick-start the linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In more formal terms, let ![Linear regression model](img/00384.jpeg) be a set
    of *p* independent variables, and *Y* be the variable of interest. We need to
    understand the regressand *Y* in terms of the regressors ![Linear regression model](img/00385.jpeg).
    The linear regression model is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression model](img/00386.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The relationship between *Y* and the regressors is in linear form; ![Linear
    regression model](img/00387.jpeg) is the intercept term; ![Linear regression model](img/00388.jpeg)
    are the regression coefficients; and ![Linear regression model](img/00389.jpeg)
    is the error term. It needs to be mentioned that the linearity is in terms of
    the regression coefficients. It is also important to note that the regressors
    can come in any form and can sometimes be taken as other forms, including log,
    exponential, and quadratic. The error term ![Linear regression model](img/00390.jpeg)
    is often assumed to follow the normal distribution with unknown variance and zero
    mean. More details about the linear regression model can be found in Draper and
    Smith (1999), Chatterjee and Hadi (2012), and Montgomery, et al. (2005). For information
    on the implementation of this technique using R software, see Chapter 12 of Tattar,
    et al. (2016) or Chapter 6 of Tattar (2017).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will explain the core notions of the linear regression model using
    the Galton dataset. The data is loaded from the `RSADBE` package and, using the
    `lm` function, we can build the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Linear regression model](img/00391.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Height of Child against Height of Parent - Scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: What does this code block tell us? First, we will load the galton data from
    the RSADBE package and then look at the `cor` correlation between the height of
    parent and child. The correlation is `0.46`, which seems to be a strong, positive
    correlation. The plot scatterplot indicates the positive correlation too, and
    consequently we proceed to build the linear regression model of the height of
    the child as a function of the height of the parent. It is advisable to look at
    the p-value associated with the model first, which in this case is given in the
    last line of `summary(cp_lm)` as `<2e-16`. The smaller p-value means that we reject
    the null hypothesis of the model being insignificant, and hence the current fitted
    model is useful. The p-values associated with the intercept and variable term
    are both `<2e-16`, and that again means that the terms are significant. The regression
    coefficient of `0.6463` implies that if a parent is an inch taller, the child's
    height would increase by a magnitude of the regression coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: The value of `Multiple R-squared` (technically simple R-squared) and `Adjusted
    R-squared` are both `0.21`, which is expected as we have a single variable in
    the model. The interpretation of R-squared is that if we multiply it by 100 (so
    21%, in this case), the resulting number is the percentage of variation in the
    data (height of the child) as explained by the fitted value. The higher the value
    of this metric, the better the model is. In this example, it means that the height
    of the parent explains only about 21% of the variation of the child's height.
    This means that we need to consider other variables. In this case, one starting
    point might be to consider the height of both parents. The multiple R-square value
    will keep on increasing if you add more variables, and hence it is preferable
    to use the more robust adjusted R-square value. Is it possible to obtain a perfect
    R-square, for example 1, or 100%?
  prefs: []
  type: TYPE_NORMAL
- en: 'A dataset named `Mantel` is available online in the bundle package, and we
    will build a linear regression model to check for its R-square. To do this, we
    import the dataset and run the `lm` function over it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the R-square is perfect. Let's have some fun before we
    embark on the serious task of analyzing the housing price data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the `galton` dataset, we will add a new variable called `frankenstein`,
    and this variable will be the residuals from the fitted model `cp_lm`. A new dataset
    will be created, which will augment the `galton` dataset with the residuals; the
    linear model will then be fitted using the `lm` function and its R-square will
    be checked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t ignore the warning function. You may recall that such a warning function
    was not displayed for the `Mantel` dataset. This is because this warning can be
    eliminated by adding a little noise to the `frankenstein` variable, consequently
    making him more monstrous:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We have thus mastered the art of obtaining a perfect R-square. Playtime is
    over now; let''s move on to the housing dataset. We previously saved the housing
    dataset for the train and test blocks as the `ht_imp.Rdata` and `htest_imp.Rdata`
    files. The author''s filename version has been modified by renaming the filenames
    as `_author` to make things clearer. We then separate the training block into
    training and testing ones. Then, we use the `load` function to import the data,
    partition it with the `sample` function, and then use the `lm` function to build
    the regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy assessment of the fitted linear model will be carried out after
    fitting three more base learners. The adjusted R-square value is about 87%. However,
    we have 68 variables, and we can see from the p-value of the previous summary
    that a lot of variables don't have p-values less than either 0.05 or 0.1\. Consequently,
    we need to get rid of the insignificant variables. The step function can be slapped
    on many fitted regression models to eliminate the insignificant variables while
    retaining most of the model characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the step function in the R session leads to a huge display of output
    in the console. The initial output is lost to the space restrictions. Consequently,
    the author ran the script with the option of **Compile Report from R Script in
    RStudio**, chose the option of MS Word as the report output format, and saved
    that file. An abbreviated version of the results from that file is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `model` is summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The small module covering the `step` function is available in the `Housing_Step_LM.R`
    file and the output generated by using R Markdown is saved in the file named `Housing_Step_LM.docx`.
    The output of the `step` function runs over forty-three pages, but we don't have
    to inspect the variables left out at each step. It suffices to say that a lot
    of insignificant variables have been eliminated without losing the traits of the
    model. The accuracy assessment of the validated partition will be seen later.
    Next, we will extend the linear regression model to the nonlinear model and work
    out the neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Build linear regression models using the principal component
    and variable cluster variables. Does the accuracy – the R-square – with the set
    of relevant variables improve the linear regression model?'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The neural network architecture was introduced in the *Statistical/machine learning
    models* section of [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*.
    Neural networks are capable of handling nonlinear relationships, the choice of
    the number of hidden neurons, the choice of transfer functions, and the learning
    rate (or decay rate) provides a great flexibility in building useful regression
    models. Haykin (2009) and Ripley (1996) provide two detailed explanations of the
    theory of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have looked at the use of neural networks for classification problems and
    have also seen the stack ensemble models in action. For the regression model,
    we need to tell the `nnet` function that the output/dependent variable is a continuous
    variable through the `linout=TRUE` option. Here, we will build a neural network
    with five hidden neurons, `size=5`, and run the function for a maximum of 100
    iterations, `maxit=100`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the neural network architecture is not very useful. However, sometimes
    we are asked to display what we have built. Thus, we will use the `plotnet` function
    from the `NeuralNetTools` package to generate the network. Since there are too
    many variables (68 in this case), we save the plot to the `Housing_NN.pdf` PDF
    file and the reader can open it and zoom into the plot to inspect it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The prediction of the neural network will be performed shortly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 1**: Build neural networks with different decay options; the default
    is 0\. Vary the decay value in the range of 0-0.2, with increments of 0.01, 0.05,
    and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 2**: Improve the neural network fit using `reltol` values, decay
    values, and a combination of these variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Regression tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The regression tree forms the third base learner for the housing dataset and
    provides the decision tree structure for the regression problems. The advantages
    of the decision tree naturally get carried over to the regression tree. As seen
    in [Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, the options for many decision trees are also
    available for the regression tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `rpart` function from the `rpart` library with the default
    settings to build the regression tree. Using the plot and text functions, we set
    up the regression tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![Regression tree](img/00392.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Regression Tree for the Sales Price of Houses'
  prefs: []
  type: TYPE_NORMAL
- en: 'Which variables are important here? The answer to this is provided by the variable
    importance metric. We extract the variable importance from `HT_rtree` and the
    variable with the highest bar length is the most important of all the variables.
    We will now use the `barplot` function for the `HT_rtree`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![Regression tree](img/00393.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Variable Importance of the Regression Tree of Housing Model'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Explore the pruning options for a regression tree.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the performance of the three base learners for the validation
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction for regression models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We separated the housing training dataset into two sections: train and validate.
    Now we will use the built models and check how well they are performing. We will
    do this by looking at the MAPE metric : `|Actual-Predicted|/Actual`. Using the
    `predict` function with the `newdata` option, the predictions are first obtained,
    and then the MAPE is calculated for the observations in the validated section
    of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The linear regression model `HT_LM_01` and the most efficient linear model (by
    AIC) `HT_LM_Final` both give the same accuracy (up to two digits) and the MAPE
    is `0.11` for these two models. The neural network model `HT_NN` (with five hidden
    neurons) results in a MAPE of `0.37`, which is a bad result. This again reinforces
    the well-known fact that complexity does not necessarily mean accuracy. The accuracy
    of the regression tree `HT_rtree` is `0.17`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The predicted prices are visualized in the following program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![Prediction for regression models](img/00394.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Predicting Housing Sales Prices'
  prefs: []
  type: TYPE_NORMAL
- en: Now we have set up the base learners, it is time to build the ensembles out
    of them. We will now build ensemble models based on the homogeneous base learner
    of the decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging and Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, and [Chapter 4](part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee
    "Chapter 4. Random Forests"), *Random Forests*, demonstrate how to improve the
    stability and accuracy of the basic decision tree. In this section, we will primarily
    use the decision tree as base learners and create an ensemble of trees in the
    same way that we did in [Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, and [Chapter 4](part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee
    "Chapter 4. Random Forests"), *Random Forests*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `split` function is the primary difference between bagging and random forest
    algorithms for classification and regression trees. Thus, unsurprisingly, we can
    continue to use the same functions and packages for the regression problem as
    the counterparts that were used in the classification problem. We will first use
    the `bagging` function from the `ipred` package to set up the bagging algorithm
    for the housing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The trees in the bagging object can be saved to a PDF file in the same way
    as in [Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Since variable importance is not given directly by the `ipred` package, and
    it is always an important measure to know which variables are important, we run
    a similar loop and program to what was used in [Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, to get the variable importance plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![Bagging and Random Forests](img/00395.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Variable Importance Plot of the Bagging Algorithm for the Housing
    Data'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Compare *Figure 10* with *Figure 8* to decide whether we have
    an overfitting problem in the regression tree.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Did bagging improve the prediction performance? This is the important criterion
    that we need to evaluate. Using the `predict` function with the `newdata` option,
    we again calculate the MAPE as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The simple regression tree had a MAPE of 17%, and now it is down to 13%. This
    leads us into the next exercise.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise:** Use some of the pruning options with `rpart.control` to improve
    the performance of bagging.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step following bagging is the random forest. We will use the `randomForest`
    function from the package of the same name. Here, we explore 500 trees for this
    forest. For the regression data, the default setting for the number of covariates
    to be randomly sampled for splitting a node is `mtry = p/3`, where `p` is the
    number of covariates. We will use the default choice. The `randomForest` function
    is used to set up the tree ensemble and then `plot_rf`, defined in [Chapter 4](part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee
    "Chapter 4. Random Forests"), *Random Forests*, is used to save the trees of the
    forest to a PDF file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The variable importance plot for the random forest is given next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bagging and Random Forests](img/00396.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Variable Importance of the Random Forest for the Housing Data'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Find the difference between the two variable importance plots
    `%lncMSE` and `IncNodePurity`. Also, compare the variable importance plot of the
    random forest with the bagging plot and comment on this.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How accurate is our forest? Using the `predict` function, we will get our answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This is simply brilliant stuff, and the random forest has significantly improved
    the accuracy by drastically reducing the MAPE from `0.17` to `0.038`. This is
    the outright winner of all the models built thus far.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: In spite of the increased accuracy, try to build forests based
    on pruned trees and calculate the accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how boosting changes the performance of the trees next.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 5](part0042_split_000.html#181NK1-2006c10fab20488594398dc4871637ee
    "Chapter 5. The Bare Bones Boosting Algorithms"), *Boosting*, introduced the boosting
    method for trees when we had a categorical variable of interest. The adaptation
    of boosting to the regression problem requires lot of computational changes. For
    more information, refer to papers by Zemel and Pitassi (2001), [http://papers.nips.cc/paper/1797-a-gradient-based-boosting-algorithm-for-regression-problems.pdf](http://papers.nips.cc/paper/1797-a-gradient-based-boosting-algorithm-for-regression-problems.pdf)
    , or Ridgeway, et al. (1999), [http://dimacs.rutgers.edu/Research/MMS/PAPERS/BNBR.pdf](http://dimacs.rutgers.edu/Research/MMS/PAPERS/BNBR.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `gbm` function from the `gbm` library will be used to boost the weak learners
    generated by using random forests. We generate a thousand trees, `n.trees=1e3`,
    and use the `shrinkage` factor of `0.05`, and then boost the regression trees
    using the gradient boosting algorithm for regression data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This summary gives the variable importance in descending order. The performance
    of the boosting can be looked into using the `gbm.perf` function and since our
    goal was always to generate a technique that performs well on new data, the out-of-bag
    curve is also laid over as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![Boosting regression models](img/00397.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Boosting Convergence for the Housing Data'
  prefs: []
  type: TYPE_NORMAL
- en: 'The boosting method has converged at iteration **137**. Next, we look at the
    performance of the boosting procedure on the validated data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The MAPE has decreased from 17% to 11%. However, the random forest continues
    to be the most accurate model thus far.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking methods for regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression models, neural networks, and regression trees are the three
    methods that will be stacked here. We will require the `caret` and `caretEnsemble`
    packages to do this task. The stacked ensemble methods have been introduced in
    detail in [Chapter 7](part0051_split_000.html#1GKCM1-2006c10fab20488594398dc4871637ee
    "Chapter 7. The General Ensemble Technique"), *The General Ensemble Technique*.
    First, we specify the control parameters for the training task, specify the list
    of algorithms, and create the stacked ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The neural network is specified through `caretModelSpec`. `Emodels` needs to
    be resampled for further analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The `dotplot` is displayed next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stacking methods for regression models](img/00398.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: R-square, MAE, and RMSE for Housing Data'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see from *Figure 13* that the R-square is similar for the three models,
    although MAE and RMSE are significantly different across the three models. The
    model correlations can be found using the `modelCor` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We now apply the ensemble method to the validation data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the results from the neural network are default and we did not specify
    the size of the hidden layers. The MAPE of 16% is not desirable and we are better
    off using the random forest ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Perform the stacked ensemble methods on the principal components
    and variable cluster data.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we extended most of the models and methods learned earlier
    in the book. The chapter began with a detailed example of housing data, and we
    carried out the visualization and pre-processing. The principal component method
    helps in reducing data, and the variable clustering method also helps with the
    same task. Linear regression models, neural networks, and the regression tree
    were then introduced as methods that will serve as base learners. Bagging, boosting,
    and random forest algorithms are some methods that helped to improve the models.
    These methods are based on homogeneous ensemble methods. This chapter then closed
    with the stacking ensemble method for the three heterogeneous base learners.
  prefs: []
  type: TYPE_NORMAL
- en: A different data structure of censored observations will be the topic of the
    next chapter. Such data is referred to as survival data, and it commonly appears
    in the study of clinical trials.
  prefs: []
  type: TYPE_NORMAL
