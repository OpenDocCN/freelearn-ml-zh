["```py\n#extract search tweets\nextractTweets <- function(searchTerm,tweetCount){\n # search term tweets\n tweets = searchTwitter(searchTerm,n=tweetCount)\n tweets.df = twListToDF(tweets)\n tweets.df$text <- sapply(tweets.df$text,function(x) iconv(x,to='UTF-8'))\n\n return(tweets.df)\n}\n\n#extract timeline tweets\nextractTimelineTweets <- function(username,tweetCount){\n # timeline tweets\n twitterUser <- getUser(username)\n tweets = userTimeline(twitterUser,n=tweetCount)\n tweets.df = twListToDF(tweets)\n tweets.df$text <- sapply(tweets.df$text,function(x) iconv(x,to='UTF-8'))\n\n return(tweets.df)\n}\n\n```", "```py\n# clean and transform tweets\ntransformTweets <- function(tweetDF){\n tweetCorpus <- Corpus(VectorSource(tweetDF$text))\n tweetCorpus <- tm_map(tweetCorpus, tolower)\n tweetCorpus <- tm_map(tweetCorpus, removePunctuation)\n tweetCorpus <- tm_map(tweetCorpus, removeNumbers)\n\n # remove URLs\n removeURL <- function(x) gsub(\"http://[[:alnum:]]*\", \"\", x)\n tweetCorpus <- tm_map(tweetCorpus, removeURL) \n\n # remove stop words\n twtrStopWords <- c(stopwords(\"english\"),'rt','http','https')\n tweetCorpus <- tm_map(tweetCorpus, removeWords, twtrStopWords)\n\n tweetCorpus <- tm_map(tweetCorpus, PlainTextDocument)\n\n #convert back to dataframe\n tweetDataframe <- data.frame(text=unlist(sapply(tweetCorpus, \n `[`, \"content\")), stringsAsFactors=F)\n\n #split each doc into words\n splitText <- function(x) {\n word.list = str_split(x, '\\\\s+')\n words = unlist(word.list)\n }\n\n # attach list of words to the data frame\n tweetDataframe$wordList = sapply(\n tweetDataframe$text,\n function(text) splitText(text))\n\n return (tweetDataframe)\n}\n\n```", "```py\nanalyzeTrendSentiments <- function(search,tweetCount){ \n\n #extract tweets\n tweetsDF <- extractTweets(search,tweetCount)\n\n # transformations\n transformedTweetsDF <- transformTweets(tweetsDF)\n\n #score the words \n transformedTweetsDF$sentiScore = sapply(transformedTweetsDF$wordList,function(wordList) scoreTweet(wordList))\n\n transformedTweetsDF$search <- search\n\n return(transformedTweetsDF) \n}\n\n```", "```py\nlibrary(twitteR)\nlibrary(stringr)\nlibrary(tm)\nlibrary(ggplot2)\n\nconsumerSecret = \"XXXXXXXXXX\"\nconsumerKey = \"XXXXXXXXXXXXXXXXXXXXXXXXX\"\n\nsetup_twitter_oauth(consumer_key = consumerKey,consumer_secret = consumerSecret)\n\n# list of positive/negative words from opinion lexicon\npos.words = scan(file= 'positive-words.txt', what='character', comment.char=';')\n\nneg.words = scan(file= 'negative-words.txt', what='character', comment.char=';')\n\n#extract 1500 tweets on the given topic\nmakeInIndiaSentiments <- analyzeTrendSentiments(\"makeinindia\",1500)\n\n#plot the aggregated scores on a histogram\nqplot(makeInIndiaSentiments $sentiScore)\n\n```", "```py\n# load labeled dataset\nlabeledDSFilePath = \"labeled_tweets.csv\"\nlabeledDataset = read.csv(labeledDSFilePath, header = FALSE)\n\n# transform polarity labels\nlabeledDataset$V1 = sapply(labeledDataset$V1, \n function(x) \n if(x==4) \n x <- \"positive\" \n else if(x==0) \n x<-\"negative\" \n else x<- \"none\")\n\n#select required columns only\nrequiredColumns <- c(\"V1\",\"V6\")\n\n# extract only positive/negative labeled tweets \ntweets<-as.matrix(labeledDataset[labeledDataset$V1 \n %in% c(\"positive\",\"negative\")\n ,requiredColumns])\n\n```", "```py\nindexes <- createDataPartition(tweets[,1], p=0.7, list = FALSE)\n\ntrain.data <- tweets[indexes,]\ntest.data <- tweets[-indexes,]\n\n```", "```py\ntrain.dtMatrix <- create_matrix(train.data[,2], \n language=\"english\" , \n removeStopwords=TRUE, \n removeNumbers=TRUE,\n stemWords=TRUE,\n weighting = tm::weightTfIdf)\n\ntest.dtMatrix <- create_matrix(test.data[,2], \n language=\"english\" , \n removeStopwords=TRUE, \n removeNumbers=TRUE,\n stemWords=TRUE,\n weighting = tm::weightTfIdf,\n originalMatrix=train.dtMatrix)\n\ntest.data.size <- nrow(test.data)\n\n```", "```py\n>  trace(\"create_matrix\",edit=T) \n\n```", "```py\nsentiment classifier using the default values and then prints a confusion matrix, along with other statistics for evaluation, as shown in the following code snippet:\n```", "```py\nsvm.model <- svm(train.dtMatrix, as.factor(train.data[,1]))\n\n## view inital model details\nsummary(svm.model)\n\n## predict and evaluate results\nsvm.predictions <- predict(svm.model, test.dtMatrix)\n\ntrue.labels <- as.factor(test.data[,1])\n\nconfusionMatrix(data=svm.predictions, reference=true.labels, positive=\"positive\")\n\n```", "```py\n## hyperparameter optimizations\n\n# run grid search\ncost.weights <- c(0.1, 10, 100)\ngamma.weights <- c(0.01, 0.25, 0.5, 1)\ntuning.results <- tune(svm, train.dtMatrix, as.factor(train.data[,1]), kernel=\"radial\", \n ranges=list(cost=cost.weights, gamma=gamma.weights))\n\n# view optimization results\nprint(tuning.results)\n\n# plot results\nplot(tuning.results, cex.main=0.6, cex.lab=0.8,xaxs=\"i\", yaxs=\"i\")\n\n```", "```py\nradial bias kernel (or rbf for short) for hyperparameter optimization. The motivation for using rbf was due to its better performance with respect to *specificity* and *sensitivity* even though the overall accuracy was comparable to *linear* kernels. We urge our readers to try out linear kernels and observe the difference in the overall results. Please note that, for text classification, linear kernels usually perform better than other kernels, not only in terms of accuracy but in performance as well\n```", "```py\n# get best model and evaluate predictions\nsvm.model.best = tuning.results$best.model\n\nsvm.predictions.best <- predict(svm.model.best, test.dtMatrix)\n\nconfusionMatrix(data=svm.predictions.best, reference=true.labels, positive=\"positive\")\n\n```", "```py\n# plot best model evaluation metric curves\nsvm.predictions.best <- predict(svm.model.best, test.dtMatrix, decision.values = T)\n\nsvm.prediction.values <- attributes(svm.predictions.best)\n$decision.values\n\npredictions <- prediction(svm.prediction.values, true.labels)\n\npar(mfrow=c(1,2))\nplot.roc.curve(predictions, title.text=\"SVM ROC Curve\")\nplot.pr.curve(predictions, title.text=\"SVM Precision/Recall Curve\")\n\n```", "```py\nboosting.model <- train_model(train.container, \"BOOSTING\"\n , maxitboost=500)\nboosting.classify <- classify_model(test.container, boosting.model)\n\n```", "```py\npredicted.labels <- boosting.classify[,1]\ntrue.labels <- as.factor(test.data[,1])\n\nconfusionMatrix(data = predicted.labels, \n reference = true.labels, \n positive = \"positive\")\n\n```", "```py\n# Cross validation\nN=10\nset.seed(42)\ncross_validate(train.container,N,\"BOOSTING\"\n , maxitboost=500)\n\n```"]