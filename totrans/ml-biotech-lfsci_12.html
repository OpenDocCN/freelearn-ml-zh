<html><head></head><body>
		<div id="_idContainer362">
			<h1 id="_idParaDest-143"><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.1.1">Chapter 10: Exploring Time Series Analysis</span></h1>
			<p><span class="koboSpan" id="kobo.2.1">In the previous chapter, we discussed using deep learning and its robust applicability when it comes to unstructured data in the form of natural language – a type of sequential data. </span><span class="koboSpan" id="kobo.2.2">Another type of sequential data that we will now turn our attention to is time series data. </span><span class="koboSpan" id="kobo.2.3">We can think of time series data as being standard datasets yet containing a time-based feature, thus unlocking a new set of possibilities when it comes to developing predictive models.</span></p>
			<p><span class="koboSpan" id="kobo.3.1">One of the most common applications in time series data is a process known as time series analysis. </span><span class="koboSpan" id="kobo.3.2">We can define time series analysis as an area of data </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">exploration</span></strong><span class="koboSpan" id="kobo.5.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.6.1">forecasting</span></strong><span class="koboSpan" id="kobo.7.1"> in which datasets are ordered or indexed using a particular </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">time interval</span></strong><span class="koboSpan" id="kobo.9.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.10.1">timestamp</span></strong><span class="koboSpan" id="kobo.11.1">. </span><span class="koboSpan" id="kobo.11.2">There are many examples of time series data that we encounter in the biotechnology and life sciences industries daily. </span><span class="koboSpan" id="kobo.11.3">Some of the more laboratory-based areas of focus include gene expression and chromatography, as well as non-lab areas such as demand forecasting and stock price analysis.</span></p>
			<p><span class="koboSpan" id="kobo.12.1">Throughout this chapter, we will explore several different areas when it comes to gaining a better understanding of the analysis of time series data, as well as developing a model capable of consuming this data and developing a robust, predictive model. </span></p>
			<p><span class="koboSpan" id="kobo.13.1">As we explore these areas, we will cover the following topics:</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.14.1">Understanding time series data</span></li>
				<li><span class="koboSpan" id="kobo.15.1">Exploring the components of a time series dataset</span></li>
				<li><span class="koboSpan" id="kobo.16.1">Tutorial – forecasting product demand using Prophet and LSTM</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.17.1">With that in mind, let’s go ahead and get started!</span></p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.18.1">Understanding time series data</span></h1>
			<p><span class="koboSpan" id="kobo.19.1">When it comes to using </span><strong class="bold"><span class="koboSpan" id="kobo.20.1">time series</span></strong><span class="koboSpan" id="kobo.21.1"> data, there are endless ways to visualize and display </span><a id="_idIndexMarker1075"/><span class="koboSpan" id="kobo.22.1">data to effectively communicate a thought or idea. </span><span class="koboSpan" id="kobo.22.2">In most of the data we have used so far, we have handled features and labels in which a certain set of features generally corresponded to a label of interest. </span><span class="koboSpan" id="kobo.22.3">When it comes to time series data, we tend to forego the idea of a class or label and focus more on trends within the data instead. </span><span class="koboSpan" id="kobo.22.4">One of the most common applications of time series data is the idea of </span><strong class="bold"><span class="koboSpan" id="kobo.23.1">demand forecasting</span></strong><span class="koboSpan" id="kobo.24.1">. </span><span class="koboSpan" id="kobo.24.2">Demand forecasting, as its </span><a id="_idIndexMarker1076"/><span class="koboSpan" id="kobo.25.1">name suggests, comprises the many methods and tools available to help predict demand for a given good or service ahead of time. </span><span class="koboSpan" id="kobo.25.2">Throughout this section, we will learn about the many aspects of time series analysis using a dataset concerning the demand forecasting of a given biotechnology product.</span></p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.26.1">Treating time series data as a structured dataset</span></h2>
			<p><span class="koboSpan" id="kobo.27.1">There are </span><a id="_idIndexMarker1077"/><span class="koboSpan" id="kobo.28.1">many different biotechnology products on the market today, ranging from agricultural genetically modified crops, all the way to monoclonal antibody therapeutics. </span><span class="koboSpan" id="kobo.28.2">In this section, we will investigate the sales data of a human therapeutic by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.29.1">dataset_demand-forecasting_ts.csv</span></strong><span class="koboSpan" id="kobo.30.1"> dataset, which belongs to a small biotech start-up:</span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.31.1">With this in mind, let’s go ahead and dive into the data. </span><span class="koboSpan" id="kobo.31.2">We will begin by importing the libraries of interest, importing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.32.1">CSV</span></strong><span class="koboSpan" id="kobo.33.1"> file, and taking a glance at the first few rows of data:</span><p class="source-code"><span class="koboSpan" id="kobo.34.1">import pandas as pd</span></p><p class="source-code"><span class="koboSpan" id="kobo.35.1">df = pd.read_csv(“dataset_demand-forecasting_ts.csv”)</span></p><p class="source-code"><span class="koboSpan" id="kobo.36.1">df.head()</span></p><p><span class="koboSpan" id="kobo.37.1">This will result in the following output:</span></p><div id="_idContainer345" class="IMG---Figure"><span class="koboSpan" id="kobo.38.1"><img src="image/B17761_10_001.jpg" alt="Figure 10.1 – The first few rows of the forecasting dataset "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.39.1">Figure 10.1 – The first few rows of the forecasting dataset</span></p><p><span class="koboSpan" id="kobo.40.1">Relative to the many other datasets we have worked with in the past, this one seems </span><a id="_idIndexMarker1078"/><span class="koboSpan" id="kobo.41.1">much simpler in the sense that we are working with only two columns: </span><strong class="source-inline"><span class="koboSpan" id="kobo.42.1">Date</span></strong><span class="koboSpan" id="kobo.43.1"> and the number of </span><strong class="source-inline"><span class="koboSpan" id="kobo.44.1">Sales</span></strong><span class="koboSpan" id="kobo.45.1"> for any given day. </span><span class="koboSpan" id="kobo.45.2">We can also see that the sales have been aggregated by day, starting on </span><strong class="source-inline"><span class="koboSpan" id="kobo.46.1">2014-01-01</span></strong><span class="koboSpan" id="kobo.47.1">. </span><span class="koboSpan" id="kobo.47.2">If we check the end of the dataset using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.48.1">tail()</span></strong><span class="koboSpan" id="kobo.49.1"> function, we will see that the dataset ends on </span><strong class="source-inline"><span class="koboSpan" id="kobo.50.1">2020-12-23</span></strong><span class="koboSpan" id="kobo.51.1"> – essentially providing us with 6 years’ worth of sales data to work with.</span></p></li>
				<li><span class="koboSpan" id="kobo.52.1">We can visualize the time series data using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.53.1">Plotly</span></strong><span class="koboSpan" id="kobo.54.1"> library:</span><p class="source-code"><span class="koboSpan" id="kobo.55.1">import plotly.express as px</span></p><p class="source-code"><span class="koboSpan" id="kobo.56.1">import plotly.graph_objects as go</span></p><p class="source-code"><span class="koboSpan" id="kobo.57.1">fig = px.line(df, x=”Date”, y=”Sales”, title=’Single Product Demand’, width=800, height=400)</span></p><p class="source-code"><span class="koboSpan" id="kobo.58.1">fig.update_traces(line_color=’#4169E1’)</span></p><p class="source-code"><span class="koboSpan" id="kobo.59.1">fig.show()</span></p><p><span class="koboSpan" id="kobo.60.1">Upon executing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.61.1">fig.show()</span></strong><span class="koboSpan" id="kobo.62.1"> function, we will receive the following output:</span></p><div id="_idContainer346" class="IMG---Figure"><span class="koboSpan" id="kobo.63.1"><img src="image/B17761_10_002.jpg" alt="Figure 10.2 – Time series plot of the sales dataset "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.64.1">Figure 10.2 – Time series plot of the sales dataset</span></p><p><span class="koboSpan" id="kobo.65.1">We can </span><a id="_idIndexMarker1079"/><span class="koboSpan" id="kobo.66.1">immediately make a few initial observations regarding the dataset:</span></p><ul><li><span class="koboSpan" id="kobo.67.1">There is a significant amount of noise and variability within the data.</span></li><li><span class="koboSpan" id="kobo.68.1">The sales gradually increase over time (I should have invested in them!).</span></li><li><span class="koboSpan" id="kobo.69.1">There seems to be an element of seasonality in which sales peak around December.</span></li></ul><p><span class="koboSpan" id="kobo.70.1">To explore these ideas a bit more and dive deeper into the data, we will need to deconstruct the time series aspect. </span><span class="koboSpan" id="kobo.70.2">Using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.71.1">Date</span></strong><span class="koboSpan" id="kobo.72.1"> column, we can break the dataset down into years, months, and days to get a better sense of the repetitive or </span><strong class="bold"><span class="koboSpan" id="kobo.73.1">seasonal</span></strong><span class="koboSpan" id="kobo.74.1"> nature of this data. </span></p><p class="callout-heading"><span class="koboSpan" id="kobo.75.1">Important note</span></p><p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.76.1">Seasonality</span></strong><span class="koboSpan" id="kobo.77.1"> within </span><a id="_idIndexMarker1080"/><span class="koboSpan" id="kobo.78.1">datasets refers to the seasonal characteristics relating to that time of the year. </span><span class="koboSpan" id="kobo.78.2">For example, datasets relating to the flu shot often show increased rates in the fall relative to the spring or summer in preparation for the winter (flu season).</span></p></li>
				<li><span class="koboSpan" id="kobo.79.1">First, we </span><a id="_idIndexMarker1081"/><span class="koboSpan" id="kobo.80.1">will need to use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.81.1">to_datetime()</span></strong><span class="koboSpan" id="kobo.82.1"> function to convert </span><strong class="source-inline"><span class="koboSpan" id="kobo.83.1">string</span></strong><span class="koboSpan" id="kobo.84.1"> into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.85.1">date</span></strong><span class="koboSpan" id="kobo.86.1"> type:</span><p class="source-code"><span class="koboSpan" id="kobo.87.1">def get_features(dataframe):</span></p><p class="source-code"><span class="koboSpan" id="kobo.88.1">    dataframe[“sales”] = dataframe[“sales”]</span></p><p class="source-code"><span class="koboSpan" id="kobo.89.1">    dataframe[“Date”] = pd.to_datetime(dataframe[‘Date’])</span></p><p class="source-code"><span class="koboSpan" id="kobo.90.1">    dataframe[‘year’] = dataframe.Date.dt.year</span></p><p class="source-code"><span class="koboSpan" id="kobo.91.1">    dataframe[‘month’] = dataframe.Date.dt.month</span></p><p class="source-code"><span class="koboSpan" id="kobo.92.1">    dataframe[‘day’] = dataframe.Date.dt.day</span></p><p class="source-code"><span class="koboSpan" id="kobo.93.1">    dataframe[‘dayofyear’] = dataframe.Date.dt.dayofyear</span></p><p class="source-code"><span class="koboSpan" id="kobo.94.1">    dataframe[‘dayofweek’] = dataframe.Date.dt.dayofweek</span></p><p class="source-code"><span class="koboSpan" id="kobo.95.1">    dataframe[‘weekofyear’] = dataframe.Date.dt.weekofyear</span></p><p class="source-code"><span class="koboSpan" id="kobo.96.1">    return dataframe </span></p><p class="source-code"><span class="koboSpan" id="kobo.97.1">df = get_features(df)</span></p><p class="source-code"><span class="koboSpan" id="kobo.98.1">df.head()</span></p><p><span class="koboSpan" id="kobo.99.1">Upon executing this command, we will receive the following DataFrame as output:</span></p><div id="_idContainer347" class="IMG---Figure"><span class="koboSpan" id="kobo.100.1"><img src="image/B17761_10_003.jpg" alt="Figure 10.3 – The first five rows of the sales dataset showing new features "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.101.1">Figure 10.3 – The first five rows of the sales dataset showing new features</span></p></li>
				<li><span class="koboSpan" id="kobo.102.1">Here, we </span><a id="_idIndexMarker1082"/><span class="koboSpan" id="kobo.103.1">can see that we were able to break down the time series aspect and yield a little more data than we originally started with. </span><span class="koboSpan" id="kobo.103.2">Let’s go ahead and plot the data by </span><strong class="source-inline"><span class="koboSpan" id="kobo.104.1">year</span></strong><span class="koboSpan" id="kobo.105.1">:</span><p class="source-code"><span class="koboSpan" id="kobo.106.1">plt.figure(figsize=(10,5))</span></p><p class="source-code"><span class="koboSpan" id="kobo.107.1">ax = sns.boxplot(x=’year’, y=’sales’, data=df)</span></p><p class="source-code"><span class="koboSpan" id="kobo.108.1">ax.set_xlabel(‘Year’, fontsize = 16)</span></p><p class="source-code"><span class="koboSpan" id="kobo.109.1">ax.set_ylabel(‘Sales’, fontsize = 16)</span></p><p><span class="koboSpan" id="kobo.110.1">After plotting our data, we will receive the following boxplot, which shows the sales for each given year. </span><span class="koboSpan" id="kobo.110.2">From a statistical perspective, we can confirm our initial observation that the sales are gradually increasing every year:</span></p><div id="_idContainer348" class="IMG---Figure"><span class="koboSpan" id="kobo.111.1"><img src="image/B17761_10_004.png.jpg" alt="Figure 10.4 – Boxplot showing the increasing sales every year "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.112.1">Figure 10.4 – Boxplot showing the increasing sales every year</span></p></li>
				<li><span class="koboSpan" id="kobo.113.1">Let’s </span><a id="_idIndexMarker1083"/><span class="koboSpan" id="kobo.114.1">go ahead and plot the same graph for each given </span><strong class="source-inline"><span class="koboSpan" id="kobo.115.1">month</span></strong><span class="koboSpan" id="kobo.116.1"> instead:</span><p class="source-code"><span class="koboSpan" id="kobo.117.1">plt.figure(figsize=(10,5))</span></p><p class="source-code"><span class="koboSpan" id="kobo.118.1">ax = sns.boxplot(x=’month’, y=’sales’, data=df)</span></p><p class="source-code"><span class="koboSpan" id="kobo.119.1">ax.set_xlabel(‘Month’, fontsize = 16)</span></p><p class="source-code"><span class="koboSpan" id="kobo.120.1">ax.set_ylabel(‘Sales’, fontsize = 16)</span></p><p><span class="koboSpan" id="kobo.121.1">Upon changing the </span><em class="italic"><span class="koboSpan" id="kobo.122.1">x</span></em><span class="koboSpan" id="kobo.123.1">-axis from years to months, we will receive the following graph, confirming our observation that the sales data tends to peak around the January (</span><strong class="bold"><span class="koboSpan" id="kobo.124.1">1</span></strong><span class="koboSpan" id="kobo.125.1">)/December (</span><strong class="bold"><span class="koboSpan" id="kobo.126.1">12</span></strong><span class="koboSpan" id="kobo.127.1">) timeframes:</span></p><div id="_idContainer349" class="IMG---Figure"><span class="koboSpan" id="kobo.128.1"><img src="image/B17761_10_005.jpg" alt="Figure 10.5 – Boxplot showing the seasonal sales for every month "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.129.1">Figure 10.5 – Boxplot showing the seasonal sales for every month</span></p><p><span class="koboSpan" id="kobo.130.1">Earlier, we noted that the dataset contained a great deal of noise in the sense that </span><a id="_idIndexMarker1084"/><span class="koboSpan" id="kobo.131.1">there was a great deal of fluctuation within the data. </span><span class="koboSpan" id="kobo.131.2">We can address this noise and normalize the data by taking a </span><strong class="bold"><span class="koboSpan" id="kobo.132.1">rolling average</span></strong><span class="koboSpan" id="kobo.133.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.134.1">moving average</span></strong><span class="koboSpan" id="kobo.135.1">) – a calculation that’s used to help us analyze data points by creating a series of average values. </span></p></li>
				<li><span class="koboSpan" id="kobo.136.1">We can implement this directly in our DataFrame using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.137.1">rolling()</span></strong><span class="koboSpan" id="kobo.138.1"> function:</span><p class="source-code"><span class="koboSpan" id="kobo.139.1">df[“Rolling_20”] = df[“sales”].rolling(window=20).mean()</span></p><p class="source-code"><span class="koboSpan" id="kobo.140.1">df[“Rolling_100”] = df[“sales”].rolling(window=100).mean()</span></p></li>
				<li><span class="koboSpan" id="kobo.141.1">Notice that in the preceding code, we used two examples to demonstrate the idea of a rolling average by using window values of 20 and 100. </span><span class="koboSpan" id="kobo.141.2">Using </span><strong class="source-inline"><span class="koboSpan" id="kobo.142.1">Plotly Go</span></strong><span class="koboSpan" id="kobo.143.1">, we can plot the original raw data and the two rolling averages onto a single plot:</span><p class="source-code"><span class="koboSpan" id="kobo.144.1">fig = go.Figure()</span></p><p class="source-code"><span class="koboSpan" id="kobo.145.1">fig.add_trace(go.Scatter(x=df[“Date”], y=df[“sales”], mode=’lines’, name=’Raw Data’, line=dict(color=”#bec2ed”)))</span></p><p class="source-code"><span class="koboSpan" id="kobo.146.1">fig.add_trace(go.Scatter(x=df[“Date”], y=df[“Rolling_20”], mode=’lines’, name=’Rolling 20’, line=dict(color=”#858eed”,dash=”dash”)))</span></p><p class="source-code"><span class="koboSpan" id="kobo.147.1">fig.add_trace(go.Scatter(x=df[“Date”], y=df[“Rolling_100”], mode=’lines’, name=’Rolling 100’, line=dict(color=”#d99543”)))</span></p><p class="source-code"><span class="koboSpan" id="kobo.148.1">fig.update_layout(width=800, height=500)</span></p><p><span class="koboSpan" id="kobo.149.1">Upon </span><a id="_idIndexMarker1085"/><span class="koboSpan" id="kobo.150.1">executing this code, we will receive the following output:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer350" class="IMG---Figure">
					<span class="koboSpan" id="kobo.151.1"><img src="image/B17761_10_006.jpg" alt="Figure 10.6 – Boxplot showing the rolling average of the sales data "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.152.1">Figure 10.6 – Boxplot showing the rolling average of the sales data</span></p>
			<p><span class="koboSpan" id="kobo.153.1">Notice that the raw dataset is plotted faintly in the background, overlayed by the dashed curve representing the </span><strong class="bold"><span class="koboSpan" id="kobo.154.1">rolling average</span></strong><span class="koboSpan" id="kobo.155.1"> with a </span><strong class="source-inline"><span class="koboSpan" id="kobo.156.1">window</span></strong><span class="koboSpan" id="kobo.157.1"> value of 20, as well as the solid curve in the foreground representing the </span><strong class="bold"><span class="koboSpan" id="kobo.158.1">rolling average</span></strong><span class="koboSpan" id="kobo.159.1"> with a </span><strong class="source-inline"><span class="koboSpan" id="kobo.160.1">window</span></strong><span class="koboSpan" id="kobo.161.1"> value of 100. </span><span class="koboSpan" id="kobo.161.2">Using rolling averages can be useful when you’re trying to visualize and understand your data, as well as building forecasting models, as we will see later in this chapter.</span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.162.1">Important note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.163.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.164.1">rolling average</span></strong><span class="koboSpan" id="kobo.165.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.166.1">moving average</span></strong><span class="koboSpan" id="kobo.167.1">) is a calculation that’s used to smoothen out a </span><a id="_idIndexMarker1086"/><span class="koboSpan" id="kobo.168.1">noisy dataset by taking the moving mean throughout a particular range. </span><span class="koboSpan" id="kobo.168.2">The range, which is often referred to as the window, is generally the last </span><em class="italic"><span class="koboSpan" id="kobo.169.1">x</span></em><span class="koboSpan" id="kobo.170.1"> number of data points.</span></p>
			<p><span class="koboSpan" id="kobo.171.1">Time series </span><a id="_idIndexMarker1087"/><span class="koboSpan" id="kobo.172.1">data is very different from much of the datasets we have explored so far within this book. </span><span class="koboSpan" id="kobo.172.2">Unlike other datasets, time series data is generally thought to be consistent with several </span><strong class="bold"><span class="koboSpan" id="kobo.173.1">components</span></strong><span class="koboSpan" id="kobo.174.1">, all of which we will explore in the next section.</span></p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.175.1">Exploring the components of a time series dataset</span></h1>
			<p><span class="koboSpan" id="kobo.176.1">In this section, we will explore the four main items that are generally regarded as the </span><a id="_idIndexMarker1088"/><span class="koboSpan" id="kobo.177.1">components of a time series dataset and visualize them. </span><span class="koboSpan" id="kobo.177.2">With that in mind, let’s go ahead and get started! </span></p>
			<p><span class="koboSpan" id="kobo.178.1">Time series datasets generally consist of four main components: </span><strong class="bold"><span class="koboSpan" id="kobo.179.1">level</span></strong><span class="koboSpan" id="kobo.180.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.181.1">long-term</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.182.1">trends</span></strong><span class="koboSpan" id="kobo.183.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.184.1">seasonality</span></strong><span class="koboSpan" id="kobo.185.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.186.1">irregular noise</span></strong><span class="koboSpan" id="kobo.187.1">, which we can break down into a method known as time series </span><strong class="bold"><span class="koboSpan" id="kobo.188.1">decomposition</span></strong><span class="koboSpan" id="kobo.189.1">. </span><span class="koboSpan" id="kobo.189.2">The main </span><a id="_idIndexMarker1089"/><span class="koboSpan" id="kobo.190.1">purpose behind decomposition is to gain a better perspective of the dataset by thinking about the data more abstractly. </span><span class="koboSpan" id="kobo.190.2">We can think of time series components as being either additive or multiplicative:</span></p>
			<p><span class="koboSpan" id="kobo.191.1"><img src="image/Formula_B17761_10_001.png" alt=""/></span></p>
			<p><span class="koboSpan" id="kobo.192.1"><img src="image/Formula_B17761_10_002.png" alt=""/></span></p>
			<p><span class="koboSpan" id="kobo.193.1">We can </span><a id="_idIndexMarker1090"/><span class="koboSpan" id="kobo.194.1">define each of the components as follows:</span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.195.1">Level</span></strong><span class="koboSpan" id="kobo.196.1">: Average </span><a id="_idIndexMarker1091"/><span class="koboSpan" id="kobo.197.1">values of a dataset over time</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.198.1">Long-term Trends</span></strong><span class="koboSpan" id="kobo.199.1">: General </span><a id="_idIndexMarker1092"/><span class="koboSpan" id="kobo.200.1">direction of the data showing an increase or decrease</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.201.1">Seasonal Trends</span></strong><span class="koboSpan" id="kobo.202.1">: Short-term </span><a id="_idIndexMarker1093"/><span class="koboSpan" id="kobo.203.1">repetitive nature (days, weeks, months)</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.204.1">Irregular Trends</span></strong><span class="koboSpan" id="kobo.205.1">: The </span><a id="_idIndexMarker1094"/><span class="koboSpan" id="kobo.206.1">noise within the data showing random fluctuations</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.207.1">We can explore and visualize these compounds a little more closely using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.208.1">statsmodels</span></strong><span class="koboSpan" id="kobo.209.1"> library in conjunction with our dataset by performing the following simple steps:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.210.1">First, we will need to reshape our dataset by only keeping the sales column, dropping any missing values, and setting the date column as the DataFrame’s </span><strong class="bold"><span class="koboSpan" id="kobo.211.1">index</span></strong><span class="koboSpan" id="kobo.212.1">:</span><p class="source-code"><span class="koboSpan" id="kobo.213.1">dftmp = pd.DataFrame({‘data’: df.Rolling_100.values},</span></p><p class="source-code"><span class="koboSpan" id="kobo.214.1">                      index=df.Date)</span></p><p class="source-code"><span class="koboSpan" id="kobo.215.1">dftmp = dftmp.dropna()</span></p><p class="source-code"><span class="koboSpan" id="kobo.216.1">dftmp.head()</span></p></li>
				<li><span class="koboSpan" id="kobo.217.1">We can check the first few rows to see that the date is now our index:</span><div id="_idContainer353" class="IMG---Figure"><span class="koboSpan" id="kobo.218.1"><img src="image/B17761_10_007.jpg" alt="Figure 10.7 – First few rows of the reshaped dataset "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.219.1">Figure 10.7 – First few rows of the reshaped dataset</span></p></li>
				<li><span class="koboSpan" id="kobo.220.1">Next, we </span><a id="_idIndexMarker1095"/><span class="koboSpan" id="kobo.221.1">will import the </span><strong class="source-inline"><span class="koboSpan" id="kobo.222.1">seasonal_decompose</span></strong><span class="koboSpan" id="kobo.223.1"> function from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.224.1">statsmodels</span></strong><span class="koboSpan" id="kobo.225.1"> library and apply it to our </span><strong class="source-inline"><span class="koboSpan" id="kobo.226.1">dataframe</span></strong><span class="koboSpan" id="kobo.227.1">:</span><p class="source-code"><span class="koboSpan" id="kobo.228.1">from statsmodels.tsa.seasonal import seasonal_decompose</span></p><p class="source-code"><span class="koboSpan" id="kobo.229.1">result = seasonal_decompose(dftmp, model=’multiplicative’, period=365)</span></p></li>
				<li><span class="koboSpan" id="kobo.230.1">Finally, we can plot the result using the built-in </span><strong class="source-inline"><span class="koboSpan" id="kobo.231.1">plot() </span></strong><span class="koboSpan" id="kobo.232.1">function and view the results:</span><p class="source-code"><span class="koboSpan" id="kobo.233.1">result.plot()</span></p><p class="source-code"><span class="koboSpan" id="kobo.234.1">pyplot.show()</span></p><p><span class="koboSpan" id="kobo.235.1">Using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.236.1">show()</span></strong><span class="koboSpan" id="kobo.237.1"> function will give us the following output:</span></p></li>
			</ol>
			<p class="figure-caption"><span class="koboSpan" id="kobo.238.1">  </span></p>
			<div>
				<div id="_idContainer354" class="IMG---Figure">
					<span class="koboSpan" id="kobo.239.1"><img src="image/B17761_10_008.png.jpg" alt="Figure 10.8 – Results of the seasonal decomposition function "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.240.1">Figure 10.8 – Results of the seasonal decomposition function</span></p>
			<p><span class="koboSpan" id="kobo.241.1">Here, we can see the four components we spoke of earlier in this section. </span><span class="koboSpan" id="kobo.241.2">In the </span><a id="_idIndexMarker1096"/><span class="koboSpan" id="kobo.242.1">first plot, we can see the rolling average we calculated in the previous section. </span><span class="koboSpan" id="kobo.242.2">This is then followed by the </span><strong class="bold"><span class="koboSpan" id="kobo.243.1">long-term trend</span></strong><span class="koboSpan" id="kobo.244.1">, which shows a steady increase throughout the dataset. </span><span class="koboSpan" id="kobo.244.2">We can then see the </span><strong class="bold"><span class="koboSpan" id="kobo.245.1">seasonality</span></strong><span class="koboSpan" id="kobo.246.1"> behind the dataset, confirming that sales tend to increase around the December and January timeframes. </span><span class="koboSpan" id="kobo.246.2">Finally, we can see the </span><strong class="bold"><span class="koboSpan" id="kobo.247.1">residual</span></strong><span class="koboSpan" id="kobo.248.1"> data or </span><strong class="bold"><span class="koboSpan" id="kobo.249.1">noise</span></strong><span class="koboSpan" id="kobo.250.1"> within the dataset. </span><span class="koboSpan" id="kobo.250.2">We can define this noise as items that did not contribute to the other main categories.</span></p>
			<p><span class="koboSpan" id="kobo.251.1">Decomposing a dataset is generally done to gain a better sense of the data and some of its main characteristics, which can often reshape how you think of the dataset and any given forecasting model that can be developed. </span><span class="koboSpan" id="kobo.251.2">We will learn how to develop two common forecasting models in the next section.</span></p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.252.1">Tutorial – forecasting demand using Prophet and LSTM</span></h1>
			<p><span class="koboSpan" id="kobo.253.1">In this tutorial, we will use the sales dataset from the previous section to develop two </span><a id="_idIndexMarker1097"/><span class="koboSpan" id="kobo.254.1">robust demand forecasting models. </span><span class="koboSpan" id="kobo.254.2">Our main objective will be to use the sales data to predict demand at a future date. </span><strong class="bold"><span class="koboSpan" id="kobo.255.1">Demand forecasting</span></strong><span class="koboSpan" id="kobo.256.1"> is generally done to predict the number of units to </span><a id="_idIndexMarker1098"/><span class="koboSpan" id="kobo.257.1">be sold on either a given date or location. </span><span class="koboSpan" id="kobo.257.2">Companies around the world, especially those that handle temperature-sensitive or time-sensitive medications, rely on models such as these </span><a id="_idIndexMarker1099"/><span class="koboSpan" id="kobo.258.1">to optimize their supply chains and ensure patient needs are met.</span></p>
			<p><span class="koboSpan" id="kobo.259.1">First, we will </span><a id="_idIndexMarker1100"/><span class="koboSpan" id="kobo.260.1">explore Facebook’s famous </span><strong class="bold"><span class="koboSpan" id="kobo.261.1">Prophet</span></strong><span class="koboSpan" id="kobo.262.1"> library, followed </span><a id="_idIndexMarker1101"/><span class="koboSpan" id="kobo.263.1">by developing a custom </span><strong class="bold"><span class="koboSpan" id="kobo.264.1">Long Short-term Memory</span></strong><span class="koboSpan" id="kobo.265.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.266.1">LSTM</span></strong><span class="koboSpan" id="kobo.267.1">) deep learning model. </span><span class="koboSpan" id="kobo.267.2">With this in mind, let’s go ahead and investigate how to use the Prophet model.</span></p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.268.1">Using Prophet for time series modeling</span></h2>
			<p><span class="koboSpan" id="kobo.269.1">Prophet is a model </span><a id="_idIndexMarker1102"/><span class="koboSpan" id="kobo.270.1">that gained a great deal of traction within the </span><a id="_idIndexMarker1103"/><span class="koboSpan" id="kobo.271.1">data science community when it was first released in 2017. </span><span class="koboSpan" id="kobo.271.2">As an open source library available in both </span><strong class="bold"><span class="koboSpan" id="kobo.272.1">R</span></strong><span class="koboSpan" id="kobo.273.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.274.1">Python</span></strong><span class="koboSpan" id="kobo.275.1">, the model was quickly adopted and widely used as one </span><a id="_idIndexMarker1104"/><span class="koboSpan" id="kobo.276.1">of the main forecasting models for time series data. </span><span class="koboSpan" id="kobo.276.2">One of the greatest benefits behind this model is also one of its consequences – its high-level nature of abstraction, allowing users to make a forecast with only a few lines of code. </span><span class="koboSpan" id="kobo.276.3">This limited variability can be a great way to make a quick forecast but can hinder the model development process, depending on the dataset at hand.</span></p>
			<p><span class="koboSpan" id="kobo.277.1">Over the next few pages, we will develop a Prophet model that’s been fitted with our data to forecast future sales and validate the results by comparing them to the actual sales data. </span><span class="koboSpan" id="kobo.277.2">Let’s get started:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.278.1">To begin, let’s go ahead and use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">rolling()</span></strong><span class="koboSpan" id="kobo.280.1"> function to get a rolling average </span><a id="_idIndexMarker1105"/><span class="koboSpan" id="kobo.281.1">of our dataset. </span><span class="koboSpan" id="kobo.281.2">Then, we </span><a id="_idIndexMarker1106"/><span class="koboSpan" id="kobo.282.1">can overlay this value on the raw values:</span><p class="source-code"><span class="koboSpan" id="kobo.283.1">df[“AverageSales”] = df[“Sales”].rolling(window=20).mean()</span></p><p class="source-code"><span class="koboSpan" id="kobo.284.1">fig = go.Figure()</span></p><p class="source-code"><span class="koboSpan" id="kobo.285.1">fig.add_trace(go.Scatter(x=df[“Date”], y=df[“Sales”], mode=’lines’, name=’Raw Data’, line=dict(color=”#bec2ed”)))</span></p><p class="source-code"><span class="koboSpan" id="kobo.286.1">fig.add_trace(go.Scatter(x=df[“Date”], y=df[“AverageSales”], mode=’lines’, name=’Rolling 20’, line=dict(color=”#3d43f5”)))</span></p><p class="source-code"><span class="koboSpan" id="kobo.287.1">fig.update_layout(width=800, height=500)</span></p></li>
				<li><span class="koboSpan" id="kobo.288.1">This will result in the following output:</span><div id="_idContainer355" class="IMG---Figure"><span class="koboSpan" id="kobo.289.1"><img src="image/B17761_10_009.jpg" alt="Figure 10.9 – The rolling average relative to the raw dataset "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.290.1">Figure 10.9 – The rolling average relative to the raw dataset</span></p><p><span class="koboSpan" id="kobo.291.1">Here, we can see that the dataset is now far less </span><strong class="bold"><span class="koboSpan" id="kobo.292.1">noisy</span></strong><span class="koboSpan" id="kobo.293.1"> and easier to work with. </span><span class="koboSpan" id="kobo.293.2">We can </span><a id="_idIndexMarker1107"/><span class="koboSpan" id="kobo.294.1">use the </span><strong class="bold"><span class="koboSpan" id="kobo.295.1">Prophet</span></strong><span class="koboSpan" id="kobo.296.1"> library with our dataset to create a forecast in four basic steps:</span></p></li>
				<li><span class="koboSpan" id="kobo.297.1">First, we will need to reshape the DataFrame to integrate it with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.298.1">Prophet</span></strong><span class="koboSpan" id="kobo.299.1"> library. </span><span class="koboSpan" id="kobo.299.2">The library expects the DataFrame to contain two columns – </span><strong class="source-inline"><span class="koboSpan" id="kobo.300.1">ds</span></strong><span class="koboSpan" id="kobo.301.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.302.1">y</span></strong><span class="koboSpan" id="kobo.303.1"> – in which </span><strong class="source-inline"><span class="koboSpan" id="kobo.304.1">ds</span></strong><span class="koboSpan" id="kobo.305.1"> is the date stamp and </span><strong class="source-inline"><span class="koboSpan" id="kobo.306.1">y</span></strong><span class="koboSpan" id="kobo.307.1"> is the value that we are working with. </span><span class="koboSpan" id="kobo.307.2">We can reshape this DataFrame into a new DataFrame using the following code:</span><p class="source-code"><span class="koboSpan" id="kobo.308.1">df2 = df[[“Date”, “AverageSales”]]</span></p><p class="source-code"><span class="koboSpan" id="kobo.309.1">df2 = df2.dropna()</span></p><p class="source-code"><span class="koboSpan" id="kobo.310.1">df2.columns = [“ds”, “y”]</span></p></li>
				<li><span class="koboSpan" id="kobo.311.1">Similar to the implementation of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.312.1">sklearn</span></strong><span class="koboSpan" id="kobo.313.1"> library, we can create an instance of the Prophet model and </span><strong class="source-inline"><span class="koboSpan" id="kobo.314.1">fit</span></strong><span class="koboSpan" id="kobo.315.1"> that to our dataset:</span><p class="source-code"><span class="koboSpan" id="kobo.316.1">m = Prophet()</span></p><p class="source-code"><span class="koboSpan" id="kobo.317.1">m.fit(df2)</span></p></li>
				<li><span class="koboSpan" id="kobo.318.1">Next, we can call the </span><strong class="source-inline"><span class="koboSpan" id="kobo.319.1">make_future_dataframe()</span></strong><span class="koboSpan" id="kobo.320.1"> function and the number of periods of interest. </span><span class="koboSpan" id="kobo.320.2">This will yield a DataFrame containing a column of dates:</span><p class="source-code"><span class="koboSpan" id="kobo.321.1">future = m.make_future_dataframe(periods=365*2)</span></p></li>
				<li><span class="koboSpan" id="kobo.322.1">Finally, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.323.1">predict()</span></strong><span class="koboSpan" id="kobo.324.1"> function to make a forecast using the future variable as an input parameter. </span><span class="koboSpan" id="kobo.324.2">This will return a number of different statistical values related to the dataset: </span><p class="source-code"><span class="koboSpan" id="kobo.325.1">forecast = m.predict(future)</span></p><p class="source-code"><span class="koboSpan" id="kobo.326.1">forecast[[‘ds’, ‘yhat’, ‘yhat_lower’, ‘yhat_upper’]].tail()</span></p><p><span class="koboSpan" id="kobo.327.1">We can limit the scope of the dataset to a few of the columns and retrieve the following DataFrame:</span></p><div id="_idContainer356" class="IMG---Figure"><span class="koboSpan" id="kobo.328.1"><img src="image/B17761_10_010.jpg" alt="Figure 10.10 – The output of the forecasting function from Prophet "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.329.1">Figure 10.10 – The output of the forecasting function from Prophet</span></p></li>
				<li><span class="koboSpan" id="kobo.330.1">Now, we </span><a id="_idIndexMarker1108"/><span class="koboSpan" id="kobo.331.1">can visualize our predictions using the built-in </span><strong class="source-inline"><span class="koboSpan" id="kobo.332.1">plot()</span></strong><span class="koboSpan" id="kobo.333.1"> function from our </span><strong class="bold"><span class="koboSpan" id="kobo.334.1">Prophet</span></strong><span class="koboSpan" id="kobo.335.1"> instance:</span><p class="source-code"><span class="koboSpan" id="kobo.336.1">fig1 = m.plot(forecast)</span></p><p><span class="koboSpan" id="kobo.337.1">This will result in the following output, which shows the original raw dataset, the future forecasting, as well as some upper and lower boundaries:</span></p><div id="_idContainer357" class="IMG---Figure"><span class="koboSpan" id="kobo.338.1"><img src="image/B17761_10_011.jpg" alt="Figure 10.11 – Graphical representation of the forecasted data "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.339.1">Figure 10.11 – Graphical representation of the forecasted data</span></p></li>
				<li><span class="koboSpan" id="kobo.340.1">Alternatively, we can test the model’s capabilities by training the model using a portion of the data – for example, everything up to 2018. </span><span class="koboSpan" id="kobo.340.2">We can then </span><a id="_idIndexMarker1109"/><span class="koboSpan" id="kobo.341.1">use the forecasting model to predict the remaining time to compare the output with the actual data. </span><span class="koboSpan" id="kobo.341.2">Upon completing this, we will receive the following output:</span></li>
			</ol>
			<div>
				<div id="_idContainer358" class="IMG---Figure">
					<span class="koboSpan" id="kobo.342.1"><img src="image/B17761_10_012.jpg" alt="Figure 10.12 – Graphical representation of the training and testing data "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.343.1">Figure 10.12 – Graphical representation of the training and testing data</span></p>
			<p><span class="koboSpan" id="kobo.344.1">Here, we can see that the dashed line, which represents the forecasted sales, was quite close to the actual values. </span><span class="koboSpan" id="kobo.344.2">We can also see that the model did not forecast the extremes of the curve, so it likely needs additional tuning to reach a more realistic forecast. </span><span class="koboSpan" id="kobo.344.3">However, the high-level nature of </span><strong class="bold"><span class="koboSpan" id="kobo.345.1">Prophet</span></strong><span class="koboSpan" id="kobo.346.1"> can be limiting in this area.</span></p>
			<p><span class="koboSpan" id="kobo.347.1">From this, we can see that preparing the data and implementing the model was quite fast and that we were able to complete this in only a few lines of code. </span><span class="koboSpan" id="kobo.347.2">In the next section, we will learn how to develop an </span><strong class="bold"><span class="koboSpan" id="kobo.348.1">LSTM</span></strong><span class="koboSpan" id="kobo.349.1"> using </span><strong class="bold"><span class="koboSpan" id="kobo.350.1">Keras</span></strong><span class="koboSpan" id="kobo.351.1">.</span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.352.1">Using LSTM for time series modeling</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.353.1">LSTM</span></strong><span class="koboSpan" id="kobo.354.1"> models </span><a id="_idIndexMarker1110"/><span class="koboSpan" id="kobo.355.1">first gained their popularity in 1997, and then again in recent years with </span><a id="_idIndexMarker1111"/><span class="koboSpan" id="kobo.356.1">the increase in computational capabilities. </span><span class="koboSpan" id="kobo.356.2">As you may recall, LSTMs are a type of </span><strong class="bold"><span class="koboSpan" id="kobo.357.1">Recurrent Neural Network</span></strong><span class="koboSpan" id="kobo.358.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.359.1">RNN</span></strong><span class="koboSpan" id="kobo.360.1">) that </span><a id="_idIndexMarker1112"/><span class="koboSpan" id="kobo.361.1">can remember and forget patterns within a dataset. </span><span class="koboSpan" id="kobo.361.2">One of the main benefits of this model is its mid to low-level nature in the sense that more code is required for a full implementation, relative to that of </span><strong class="bold"><span class="koboSpan" id="kobo.362.1">Prophet</span></strong><span class="koboSpan" id="kobo.363.1">. </span><span class="koboSpan" id="kobo.363.2">Users gain a great deal of control over the model development process, enabling them to cater the model to almost any type of dataset, and any type of use case. </span><span class="koboSpan" id="kobo.363.3">With that in mind, let’s get started:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.364.1">Using the same dataset, we can go ahead and create a rolling average using a </span><strong class="source-inline"><span class="koboSpan" id="kobo.365.1">window</span></strong><span class="koboSpan" id="kobo.366.1"> of </span><strong class="source-inline"><span class="koboSpan" id="kobo.367.1">20</span></strong><span class="koboSpan" id="kobo.368.1"> to reduce the noise in our dataset. </span><span class="koboSpan" id="kobo.368.2">Then, we can remove the missing values that result from this:</span><p class="source-code"><span class="koboSpan" id="kobo.369.1">df[‘Sales’] = df[“Sales”].rolling(window=20).mean()</span></p><p class="source-code"><span class="koboSpan" id="kobo.370.1">df = df.dropna()</span></p></li>
				<li><span class="koboSpan" id="kobo.371.1">Using </span><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">MinMaxScaler</span></strong><span class="koboSpan" id="kobo.373.1"> from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">sklearn</span></strong><span class="koboSpan" id="kobo.375.1"> library, we can go ahead and scale our dataset:</span><p class="source-code"><span class="koboSpan" id="kobo.376.1">ds = df[[“Sales”]].values</span></p><p class="source-code"><span class="koboSpan" id="kobo.377.1">scaler = MinMaxScaler(feature_range=(0, 1))</span></p><p class="source-code"><span class="koboSpan" id="kobo.378.1">ds = scaler.fit_transform(ds)</span></p></li>
				<li><span class="koboSpan" id="kobo.379.1">Next, we will need to split the data into our training and testing sets. </span><span class="koboSpan" id="kobo.379.2">Remember that our objective here is to provide the model with some sample historical data and see if we can accurately forecast future demand. </span><span class="koboSpan" id="kobo.379.3">Let’s go ahead and use 75% of the dataset to train the model and see if we can forecast the remaining 25%:</span><p class="source-code"><span class="koboSpan" id="kobo.380.1">train_size = int(len(ds) * 0.75)</span></p><p class="source-code"><span class="koboSpan" id="kobo.381.1">test_size = len(ds) - train_size</span></p><p class="source-code"><span class="koboSpan" id="kobo.382.1">train = ds[0: train_size,:]</span></p><p class="source-code"><span class="koboSpan" id="kobo.383.1">test = ds[train_size : len(ds), :]</span></p></li>
				<li><span class="koboSpan" id="kobo.384.1">Given </span><a id="_idIndexMarker1113"/><span class="koboSpan" id="kobo.385.1">that we are working with time series data, we will need to use a </span><strong class="source-inline"><span class="koboSpan" id="kobo.386.1">lookback</span></strong><span class="koboSpan" id="kobo.387.1"> to train the model in iterations. </span><span class="koboSpan" id="kobo.387.2">Let’s go ahead and select a </span><strong class="source-inline"><span class="koboSpan" id="kobo.388.1">lookback</span></strong><span class="koboSpan" id="kobo.389.1"> value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.390.1">100</span></strong><span class="koboSpan" id="kobo.391.1"> and use our </span><strong class="source-inline"><span class="koboSpan" id="kobo.392.1">dataset_generator</span></strong><span class="koboSpan" id="kobo.393.1"> function to create </span><a id="_idIndexMarker1114"/><span class="koboSpan" id="kobo.394.1">our training and testing sets. </span><span class="koboSpan" id="kobo.394.2">We can think of a </span><strong class="source-inline"><span class="koboSpan" id="kobo.395.1">lookback</span></strong><span class="koboSpan" id="kobo.396.1"> value as the range of how far back in the data the model should look to train:</span><p class="source-code"><span class="koboSpan" id="kobo.397.1">lookback = 100</span></p><p class="source-code"><span class="koboSpan" id="kobo.398.1">X_train, y_train = dataset_generator(train, lookback)</span></p><p class="source-code"><span class="koboSpan" id="kobo.399.1">X_test, y_test = dataset_generator(test, lookback)</span></p></li>
				<li><span class="koboSpan" id="kobo.400.1">As you may recall from our previous implementation of an LSTM model, we needed to </span><strong class="source-inline"><span class="koboSpan" id="kobo.401.1">reshape</span></strong><span class="koboSpan" id="kobo.402.1"> our data prior to using the data as input:</span><p class="source-code"><span class="koboSpan" id="kobo.403.1">X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))</span></p><p class="source-code"><span class="koboSpan" id="kobo.404.1">X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))</span></p></li>
				<li><span class="koboSpan" id="kobo.405.1">Finally, with the data prepared, we can go ahead and prepare the model itself. </span><span class="koboSpan" id="kobo.405.2">Given that we are only working with a single feature, we can keep our model relatively simple. </span><span class="koboSpan" id="kobo.405.3">First, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.406.1">Sequential</span></strong><span class="koboSpan" id="kobo.407.1"> class from Keras, and then add an </span><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">LSTM</span></strong><span class="koboSpan" id="kobo.409.1"> layer with two nodes, followed by a </span><strong class="source-inline"><span class="koboSpan" id="kobo.410.1">Dense</span></strong><span class="koboSpan" id="kobo.411.1"> layer with a single output value:</span><p class="source-code"><span class="koboSpan" id="kobo.412.1">model = Sequential()</span></p><p class="source-code"><span class="koboSpan" id="kobo.413.1">model.add(LSTM(2, input_shape=(1, lookback)))</span></p><p class="source-code"><span class="koboSpan" id="kobo.414.1">model.add(Dense(1))</span></p></li>
				<li><span class="koboSpan" id="kobo.415.1">Next, we </span><a id="_idIndexMarker1115"/><span class="koboSpan" id="kobo.416.1">can use an </span><strong class="source-inline"><span class="koboSpan" id="kobo.417.1">Adam</span></strong><span class="koboSpan" id="kobo.418.1"> optimizer with a learning rate of </span><strong class="source-inline"><span class="koboSpan" id="kobo.419.1">0.001</span></strong><span class="koboSpan" id="kobo.420.1"> and compile the model:</span><p class="source-code"><span class="koboSpan" id="kobo.421.1">opt = tf.keras.optimizers.Adam(learning_rate=0.001)</span></p><p class="source-code"><span class="koboSpan" id="kobo.422.1">model.compile(loss=’mean_squared_error’, optimizer=opt)</span></p></li>
				<li><span class="koboSpan" id="kobo.423.1">Recall </span><a id="_idIndexMarker1116"/><span class="koboSpan" id="kobo.424.1">that we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.425.1">summary()</span></strong><span class="koboSpan" id="kobo.426.1"> function to take a look at the compiled model:</span><p class="source-code"><span class="koboSpan" id="kobo.427.1">model.summary()</span></p><p><span class="koboSpan" id="kobo.428.1">This will result in the following output, which provides a glimpse into the inner workings of the model:</span></p><div id="_idContainer359" class="IMG---Figure"><span class="koboSpan" id="kobo.429.1"><img src="image/B17761_10_013.jpg" alt="Figure 10.13 – Summary of the Keras model "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.430.1">Figure 10.13 – Summary of the Keras model</span></p></li>
				<li><span class="koboSpan" id="kobo.431.1">With the model compiled, we can go ahead and begin the training process. </span><span class="koboSpan" id="kobo.431.2">We can call the </span><strong class="source-inline"><span class="koboSpan" id="kobo.432.1">fit()</span></strong><span class="koboSpan" id="kobo.433.1"> function to fit the model on the training dataset for 10 epochs:</span><p class="source-code"><span class="koboSpan" id="kobo.434.1">history = model.fit(X_train, y_train, epochs=10, batch_size=1, verbose=2)</span></p></li>
				<li><span class="koboSpan" id="kobo.435.1">The model </span><a id="_idIndexMarker1117"/><span class="koboSpan" id="kobo.436.1">training process should be relatively quick. </span><span class="koboSpan" id="kobo.436.2">Once it’s been completed, we </span><a id="_idIndexMarker1118"/><span class="koboSpan" id="kobo.437.1">can take a look at the </span><strong class="source-inline"><span class="koboSpan" id="kobo.438.1">loss</span></strong><span class="koboSpan" id="kobo.439.1"> value by visualizing the results in a graph:</span><p class="source-code"><span class="koboSpan" id="kobo.440.1">plt.figure(figsize=(10,6))</span></p><p class="source-code"><span class="koboSpan" id="kobo.441.1">plt.plot(history.history[“loss”], linewidth=2)</span></p><p class="source-code"><span class="koboSpan" id="kobo.442.1">plt.title(“Model Loss”, fontsize=15)</span></p><p class="source-code"><span class="koboSpan" id="kobo.443.1">plt.xlabel(“# Epochs”, fontsize=15)</span></p><p class="source-code"><span class="koboSpan" id="kobo.444.1">plt.ylabel(“Mean Squared Error”, fontsize=15)</span></p><p><span class="koboSpan" id="kobo.445.1">This will result in the following output, showing the progressive decrease in loss over time:</span></p><div id="_idContainer360" class="IMG---Figure"><span class="koboSpan" id="kobo.446.1"><img src="image/B17761_10_014.png.jpg" alt="Figure 10.14 – Model loss over time "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.447.1">Figure 10.14 – Model loss over time</span></p><p><span class="koboSpan" id="kobo.448.1">Here, we can see that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.449.1">loss</span></strong><span class="koboSpan" id="kobo.450.1"> value decreases quite consistently, finally plateauing at around the 9-10 epoch marker. </span><span class="koboSpan" id="kobo.450.2">Notice that we specified a learning rate of </span><strong class="source-inline"><span class="koboSpan" id="kobo.451.1">0.001</span></strong><span class="koboSpan" id="kobo.452.1"> in the optimizer. </span><span class="koboSpan" id="kobo.452.2">Had we increased this value to 0.01, or decreased the value to 0.0001, the output of this graph would be very different. </span><span class="koboSpan" id="kobo.452.3">We can use the learning rate as a powerful parameter </span><a id="_idIndexMarker1119"/><span class="koboSpan" id="kobo.453.1">to optimize the performance of our model. </span><span class="koboSpan" id="kobo.453.2">Go ahead and give this a try to see what the graphical output of the loss would be.</span></p></li>
				<li><span class="koboSpan" id="kobo.454.1">With </span><a id="_idIndexMarker1120"/><span class="koboSpan" id="kobo.455.1">the model training complete, we can go ahead and use the model to forecast the values of interest:</span><p class="source-code"><span class="koboSpan" id="kobo.456.1">X_train_forecast = scaler.inverse_transform(model.predict(X_train))</span></p><p class="source-code"><span class="koboSpan" id="kobo.457.1">y_train = scaler.inverse_transform([y_train.ravel()])</span></p><p class="source-code"><span class="koboSpan" id="kobo.458.1">X_test_forecast = scaler.inverse_transform(model.predict(X_test))</span></p><p class="source-code"><span class="koboSpan" id="kobo.459.1">y_test = scaler.inverse_transform([y_test.ravel()])</span></p></li>
				<li><span class="koboSpan" id="kobo.460.1">With the data all set, we can visualize the data by plotting the results using </span><strong class="source-inline"><span class="koboSpan" id="kobo.461.1">matplotlib</span></strong><span class="koboSpan" id="kobo.462.1">. </span><span class="koboSpan" id="kobo.462.2">First, let’s plot the original dataset using </span><strong class="source-inline"><span class="koboSpan" id="kobo.463.1">lightgrey</span></strong><span class="koboSpan" id="kobo.464.1">:</span><p class="source-code"><span class="koboSpan" id="kobo.465.1">plt.plot(list(range(0, len(ds))), scaler.inverse_transform(ds), label=”Original”, color=”lightgrey”)</span></p></li>
				<li><span class="koboSpan" id="kobo.466.1">Next, we can plot the training values using </span><strong class="source-inline"><span class="koboSpan" id="kobo.467.1">blue</span></strong><span class="koboSpan" id="kobo.468.1">:</span><p class="source-code"><span class="koboSpan" id="kobo.469.1">train_y_plot = X_train_forecast</span></p><p class="source-code"><span class="koboSpan" id="kobo.470.1">train_x_plot = [i+lookback for i in list(range(0, len(X_train_forecast)))]</span></p><p class="source-code"><span class="koboSpan" id="kobo.471.1">plt.plot(train_x_plot, train_y_plot , label=”Train”, color=”blue”)</span></p></li>
				<li><span class="koboSpan" id="kobo.472.1">Finally, we </span><a id="_idIndexMarker1121"/><span class="koboSpan" id="kobo.473.1">can plot the forecasted values using </span><strong class="source-inline"><span class="koboSpan" id="kobo.474.1">darkorange</span></strong><span class="koboSpan" id="kobo.475.1"> and a dashed </span><a id="_idIndexMarker1122"/><span class="koboSpan" id="kobo.476.1">line to distinguish it from its two counterparts:</span><p class="source-code"><span class="koboSpan" id="kobo.477.1">test_y_plot = X_test_forecast</span></p><p class="source-code"><span class="koboSpan" id="kobo.478.1">test_x_plot = [i+lookback*2 for i in</span></p><p class="source-code"><span class="koboSpan" id="kobo.479.1">               list(range(len(X_train_forecast), </span></p><p class="source-code"><span class="koboSpan" id="kobo.480.1">               len(X_train_forecast)+len(X_test_forecast)))]</span></p><p class="source-code"><span class="koboSpan" id="kobo.481.1">plt.plot(test_x_plot, test_y_plot , label=”Forecast”, </span></p><p class="source-code"><span class="koboSpan" id="kobo.482.1">         color=”darkorange”, linewidth=2, linestyle=”--”)</span></p><p class="source-code"><span class="koboSpan" id="kobo.483.1">plt.legend()</span></p><p><span class="koboSpan" id="kobo.484.1">Upon executing this code, we will get the following output:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer361" class="IMG---Figure">
					<span class="koboSpan" id="kobo.485.1"><img src="image/B17761_10_015.png.jpg" alt="Figure 10.15 – Training and testing datasets using the LSTM model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.486.1">Figure 10.15 – Training and testing datasets using the LSTM model</span></p>
			<p><span class="koboSpan" id="kobo.487.1">Here, we can see that this relatively simple </span><strong class="bold"><span class="koboSpan" id="kobo.488.1">LSTM</span></strong><span class="koboSpan" id="kobo.489.1"> model was quite effective in making </span><a id="_idIndexMarker1123"/><span class="koboSpan" id="kobo.490.1">a forecast using the training dataset we provided. </span><span class="koboSpan" id="kobo.490.2">The model was not only able to capture </span><a id="_idIndexMarker1124"/><span class="koboSpan" id="kobo.491.1">the general direction of the values, but also managed to capture the seasonality of the values as well.</span></p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.492.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.493.1">In this chapter, we attempted to analyze and understand time series data, as well as developing two predictive forecasting models in less than 15 pages. </span><span class="koboSpan" id="kobo.493.2">We began our journey by exploring and decomposing time series data into smaller features that can generally be used with shallow machine learning models. </span><span class="koboSpan" id="kobo.493.3">We then investigated the components of a time series dataset to understand the underlying makeup. </span><span class="koboSpan" id="kobo.493.4">Finally, we developed two of the most common forecasting models that are used in the industry – the Prophet model, by Facebook, and an LSTM model using Keras.</span></p>
			<p><span class="koboSpan" id="kobo.494.1">Throughout the last few chapters, we have developed various technical solutions to solve common business problems. </span><span class="koboSpan" id="kobo.494.2">In the next chapter, we will explore the first step in making models such as these available to end users using the Flask framework.</span></p>
		</div>
	</body></html>