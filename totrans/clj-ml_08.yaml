- en: Chapter 8. Anomaly Detection and Recommendation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will study a couple of modern forms of applied machine learning.
    We will first explore the problem of *anomaly detection* and we will discuss *recommendation
    systems* later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Anomaly detection** is a machine learning technique in which we determine
    whether a given set of values for some selected features that represent the system
    are unexpectedly different from the normally observed values of the given features.
    There are several applications of anomaly detection, such as detection of structural
    and operational defects in manufacturing, network intrusion detection systems,
    system monitoring, and medical diagnosis.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recommendation systems** are essentially information systems that seek to
    predict a given user''s liking or preference for a given item. Over recent years,
    there have been a vast number of recommendation systems, or **recommender systems**,
    that have been built for several business and social applications to provide a
    better experience for their users. Such systems can provide a user with useful
    recommendations depending on the items that the user has previously rated or liked.
    Most existing recommendation systems today provide recommendations to users about
    online products, music, and social media. There are also a significant number
    of financial and business applications on the Web that use recommendation systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, both anomaly detection and recommendation systems are applied
    forms of machine learning problems, which we have previously encountered in this
    book. Anomaly detection is in fact an extension of binary classification, and
    recommendation is actually an extended form of linear regression. We will study
    more about these similarities in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting anomalies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Anomaly detection* is essentially the identification of items or observed
    values that do not conform to an expected pattern (for more information, refer
    to "A Survey of Outlier Detection Methodologies"). The pattern could be determined
    by values that have been previously observed, or by some limits across which the
    input values can vary. In the context of machine learning, anomaly detection can
    be performed in both supervised and unsupervised environments. Either way, the
    problem of anomaly detection is to find input values that are significantly different
    from other input values. There are several applications of this technique, and
    in the broad sense, we can use anomaly detection for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: To detect problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To detect a new phenomenon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To monitor unusual behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The observed values that are found to be different from the other values are
    called outliers, anomalies, or exceptions. More formally, we define an **outlier**
    as an observation that lies outside the overall pattern of a distribution. By
    *outside*, we mean an observation that has a high numerical or statistical distance
    from the rest of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of outliers can be depicted by the following plots, where the
    red crosses mark normal observations and the green crosses mark the anomalous
    observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting anomalies](img/4351OS_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: One possible approach to anomaly detection is to use a *probability distribution
    model*, which is built from the training data to detect anomalies. Techniques
    that use this approach are termed as *statistical methods* of anomaly detection.
    In this approach, an anomaly will have a low probability with respect to the overall
    probability distribution of the rest of the sample data. Hence, we try to fit
    a model onto the available sample data and use this formulated model to detect
    anomalies. The main problem with this approach is that it's hard to find a standard
    distribution model for stochastic data.
  prefs: []
  type: TYPE_NORMAL
- en: Another method that can be used to detect anomalies is a *proximity-based approach*.
    In this approach, we determine the proximity, or nearness, of a set of observed
    values with respect to the rest of the values in the sample data. For example,
    we could use the **K-Nearest Neighbors** (**KNN**) algorithm to determine the
    distances of a given observed value to its *k* nearest values. This technique
    is much simpler than estimating a statistical model over the sample data. This
    is because it's easier to determine a single measure, which is the proximity of
    an observed value, than it is to fit a standard model on the available training
    data. However, determining the proximity of a set of input values could be inefficient
    for larger datasets. For example, the KNN algorithm has a time complexity of ![Detecting
    anomalies](img/4351OS_08_02.jpg), and computing the proximity of a given set of
    values to its *k* nearest values could be inefficient for a large value of *k*.
    Also, the KNN algorithm could be sensitive to the value of the neighbors *k*.
    If the value of *k* is too large, clusters of values with less than *k* individual
    sets of input values could be falsely classified as anomalies. On the other hand,
    if *k* is too small, some anomalies that have a few neighbors with a low proximity
    may not be detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also determine whether a given set of observed values is an anomaly
    based on the density of data around it. This approach is termed as the **density-based
    approach** to anomaly detection. A given set of input values can be classified
    as an anomaly if the data around the given values is low. In anomaly detection,
    the density-based and proximity-based approaches are closely related. In fact,
    the density of data is generally defined in terms of the proximity or distance
    of a given set of values with respect to the rest of the data. For example, if
    we use the KNN algorithm to determine the proximity or distance of a given set
    of values to the rest of the data, we can define the density as the reciprocal
    of the average distance to the *k* nearest values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting anomalies](img/4351OS_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Clustering-based approaches* can also be used to detect anomalies. Essentially,
    clustering can be used to determine groups or clusters of values in the sample
    data. The items in a cluster can be assumed to be closely related, and anomalies
    are values that cannot be related to previously encountered values in the clusters
    in the sample data. Thus, we could determine all the clusters in the sample data
    and then mark the smallest clusters as anomalies. Alternatively, we can form clusters
    from the sample data and determine the clusters, if any, of a given set of previously
    unseen values.'
  prefs: []
  type: TYPE_NORMAL
- en: If a set of input values does not belong to any cluster, it's definitely an
    anomalous observation. The advantage of clustering techniques is that they can
    be used in combination with other machine learning techniques that we previously
    discussed. On the other hand, the problem with this approach is that most clustering
    techniques are sensitive to the number of clusters that have been chosen. Also,
    algorithmic parameters of clustering techniques, such as the average number of
    items in a cluster and number of clusters, cannot be determined easily. For example,
    if we are modeling some unlabeled data using the KNN algorithm, the number of
    clusters *K* would have to be determined either by trial-and-error or by scrutinizing
    the sample data for obvious clusters. However, both these techniques are not guaranteed
    to perform well on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In models where the sample values are all supposed to conform to some mean value
    with some allowable tolerance, the **Gaussian** or **normal distribution** is
    often used as the distribution model to train an anomaly detector. This model
    has two parameters—the mean ![Detecting anomalies](img/4351OS_08_04.jpg) and the
    variance ![Detecting anomalies](img/4351OS_08_05.jpg). This distribution model
    is often used in statistical approaches to anomaly detection, where the input
    variables are normally found statistically close to some predetermined mean value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Probability Density Function** (**PDF**) is often used by density-based
    methods of anomaly detection. This function essentially describes the likelihood
    that an input variable will take on a given value. For a random variable *x*,
    we can formally define the PDF as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting anomalies](img/4351OS_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The PDF can also be used in combination with a normal distribution model for
    the purpose of anomaly detection. The PDF of a normal distribution is parameterized
    by the mean ![Detecting anomalies](img/4351OS_08_04.jpg) and variance ![Detecting
    anomalies](img/4351OS_08_05.jpg) of the distribution, and can be formally expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting anomalies](img/4351OS_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will now demonstrate a simple implementation of an anomaly detector in Clojure,
    which is based on the PDF for a normal distribution as we previously discussed.
    For this example, we will use Clojure atoms to maintain all states in the model.
    Atoms are used to represent an atomic state in Clojure. By *atomic*, we mean that
    the underlying state changes completely or doesn't change at all—the changes in
    state are thus *atomic*.
  prefs: []
  type: TYPE_NORMAL
- en: We now define some functions to help us manipulate the features of the model.
    Essentially, we intend to represent these features and their values as a map.
    To manage the state of this map, we use an atom. Whenever the anomaly detector
    is fed a set of feature values, it must first check for any previous information
    on the features in the new set of values, and then it should start maintaining
    the state of any new features when it is necessary. As a function on its own cannot
    contain any external state in Clojure, we will use closures to bind state and
    functions together. In this implementation, almost all the functions return other
    functions, and the resulting anomaly detector will also be used just like a function.
    In summary, we will model the state of the anomaly detector using an atom, and
    then bind this atom to a function using a closure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start off by defining a function that initializes our model with some state.
    This state is essentially a map wrapped in an atom by using the `atom` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `accumulator` function defined in the preceding code initializes an atom
    and returns a function that applies the `update-totals` function to a value `n`.
    The value `n` represents a value of an input variable in our model. The `update-totals`
    function also returns a function that takes a single argument, and then it updates
    the state in the atom by using the `update-in` function. The function returned
    by the `accumulator` function will use the `update-totals` function to update
    the state of the mean and variance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now implement the following PDF function for normal distribution that can
    be used to monitor sudden changes in the feature values of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `density` function defined in the preceding code is a direct translation
    of the PDF function for normal distribution. It uses functions and constants from
    the `Math` namespace such as, `sqrt`, `exp`, and `PI` to find the PDF of the model
    by using the accumulated mean and variance of the model. We will define the the
    `density-detector` function as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `density-detector` function defined in the preceding code initializes the
    state of our anomaly detector using the `accumulator` function, and it uses the
    `density` function on the state maintained by the accumulator to determine the
    PDF of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are dealing with maps wrapped in atoms, we can implement a couple
    of functions to perform this check by using the `contains?`, `assoc-in`, and `swap!`
    functions, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `get-or-add-key` function defined in the preceding code looks up a given
    key in an atom containing a map by using the `contains?` function. Note the use
    of the `@` operator to dereference an atom into its wrapped value. If the key
    is found in the map, we simply call the map as a function as `(@a key)`. If the
    key is not found, we use the `swap!` and `assoc-in` functions to add a new key-value
    pair to the map in the atom. The value of this key-value pair is generated from
    the `create-fn` parameter that is passed to the g`et-or-add-key` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `get-or-add-key` and `density-detector` functions we have defined,
    we can implement the following functions that return functions while detecting
    anomalies in the sample data so as to create the effect of maintaining the state
    of the PDF distribution of the model within these functions themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `atom-hash-map` function defined in the preceding code uses the `get-key`
    function with an arbitrary initialization function `create-fn` to maintain the
    state of a map in an atom. The detector function uses the `density-detector` function
    that we previously defined to initialize the state of every new feature in the
    input values that are fed to it. Note that this function returns a function that
    will accept a map with key-value parameters as the features. We can inspect the
    behavior of the implemented anomaly detector in the REPL as shown in the following
    code and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code and output, we created a new instance of our
    anomaly detector by using the `detector` function. The `detector` function returns
    a function that accepts a map of key-value pairs of features. When we feed the
    map with `{:x 10 :y 10 :z 10}`, the anomaly detector returns a PDF of `1.0` since
    all samples in the data so far have the same feature values. The anomaly detector
    will always return this value as long as the number of features and the values
    of these features remains the same in all sample inputs fed to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we feed the anomaly detector with a set of features with different values,
    the PDF is observed to change to a finite number, as shown in the following code
    and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When the features show a large degree of variation, the detector has a sudden
    and large decrease in the PDF of its distribution model, as shown in the following
    code and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In summary, anomalous sample values can be detected when the PDF of the normal
    distribution model returned by the anomaly detector described previously has a
    large difference from its previous values. We can extend this implementation to
    check some kind of threshold value so that the result is quantized. The system
    thus detects an anomaly only when this threshold value of the PDF is crossed.
    When dealing with real-world data, all we would have to do is somehow represent
    the feature values we are modeling as a map and determine the threshold value
    to use via trial-and-error method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anomaly detection can be used in both supervised and unsupervised machine learning
    environments. In supervised learning, the sample data will be labeled. Interestingly,
    we could also use binary classification, among other supervised learning techniques,
    to model this kind of data. We can choose between anomaly detection and classification
    to model labeled data by using the following guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose binary classification when the number of positive and negative examples
    in the sample data is almost equal. Conversely, choose anomaly detection if there
    are a very small number of positive or negative examples in the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose anomaly detection when there are many sparse classes and a few dense
    classes in the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose supervised learning techniques such as classification when positive samples
    that may be encountered by the trained model will be similar to positive samples
    that the model has already seen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building recommendation systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recommendation systems are information filtering systems whose goal is to provide
    its users with useful recommendations. To determine these recommendations, a recommendation
    system can use historical data about the user's activity, or it can use recommendations
    that other users liked (for more information, refer to "A Taxonomy of Recommender
    Agents on the Internet"). These two approaches are the basis of the two types
    of algorithms used by recommendation systems—**content-based filtering** and **collaborative
    filtering**. Interestingly, some recommendation systems even use a combination
    of these two techniques to provide users with recommendations. Both these techniques
    aim to recommend items, or domain objects that are managed or exchanged by user-centric
    applications, to its users. Such applications include several websites that provide
    users with online content and information, such as online shopping and media.
  prefs: []
  type: TYPE_NORMAL
- en: In *content-based filtering*, recommendations are determined by finding similar
    items by using a particular user's rating. Each item is represented as a set of
    discrete features or characteristics, and each item is also rated by several users.
    Thus, for each user, we have several sets of input variables to represent the
    characteristics of each item and a set of output variables that represent the
    user's rating for the item. This information can be used to recommend items with
    similar features or characteristics as items that were previously rated by a user.
  prefs: []
  type: TYPE_NORMAL
- en: '*Collaborative filtering* methods are based on collecting data about a given
    user''s behavior, activities, or preferences and using this information to recommend
    items to users. The recommendation is based on how similar a user''s behavior
    is to that of other users. In effect, a user''s recommendations are based on her
    past behavior as well as decisions made by other users in the system. A collaborative
    filtering technique will use the preferences of similar users to determine the
    features of all available items in the system, and then it will recommend items
    with similar features as the items that a given set of users are observed to like.'
  prefs: []
  type: TYPE_NORMAL
- en: Content-based filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned earlier, content-based filtering systems provide users with
    recommendations based on their past behavior as well as the characteristics of
    items that are positively rated or liked by the given user. We can also take into
    account the items that were disliked by the given user. An item is generally represented
    by several discrete attributes. These attributes are analogous to the input variables
    or features of a classification or linear regression based machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we want to build a recommendation system that uses content-based
    filtering to recommend online products to its users. Each product can be characterized
    and identified by several known characteristics, and users can provide a rating
    for each characteristic of every product. The feature values of the products can
    have values between the 0 and 10, and the ratings provided by users for the products
    will have values within the range of 0 and 5\. We can visualize the sample data
    for this recommendation system in a tabular representation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Content-based filtering](img/4351OS_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding table, the system has ![Content-based filtering](img/4351OS_08_09.jpg)
    products and ![Content-based filtering](img/4351OS_08_10.jpg) users. Each product
    is defined by ![Content-based filtering](img/4351OS_08_11.jpg) features, each
    of which will have a value in the range of 0 and 10, and each product is also
    rated by a user. Let the rating of each product ![Content-based filtering](img/4351OS_08_12.jpg)
    by a user ![Content-based filtering](img/4351OS_08_13.jpg) be represented as ![Content-based
    filtering](img/4351OS_08_14.jpg). Using the input values ![Content-based filtering](img/4351OS_08_15.jpg),
    or rather the input vector ![Content-based filtering](img/4351OS_08_16.jpg), and
    the rating ![Content-based filtering](img/4351OS_08_14.jpg) of a user ![Content-based
    filtering](img/4351OS_08_13.jpg), we can estimate a parameter vector ![Content-based
    filtering](img/4351OS_08_17.jpg) that we can use to to predict a user''s rating.
    Thus, content-based filtering in fact applies a copy of linear regression to each
    user''s rating and each product''s feature values to estimate a regression model
    that can in turn be used to estimate the users rating for some unrated products.
    In effect, we learn the parameter ![Content-based filtering](img/4351OS_08_17.jpg)
    using the independent variables ![Content-based filtering](img/4351OS_08_16.jpg)
    and the dependent variable ![Content-based filtering](img/4351OS_08_14.jpg) and
    for all the users the system. Using the estimated parameter ![Content-based filtering](img/4351OS_08_17.jpg)
    and some given values for the independent variables, we can predict the value
    of the dependent variable for any given user. The optimization problem for content-based
    filtering can thus be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Content-based filtering](img/4351OS_08_18.jpg)![Content-based filtering](img/4351OS_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The optimization problem defined in the preceding equation can be applied to
    all users of the system to produce the following optimization problem for Uusers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Content-based filtering](img/4351OS_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In simple terms, the parameter vector ![Content-based filtering](img/4351OS_08_20.jpg)
    tries to scale or transform the input variables to match the output variable of
    the model. The second term that is added is for *regularization*. Interestingly,
    the optimization problem defined in the receding equation is analogous to that
    of linear regression, and thus content-based filtering can be considered as an
    extension of linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: The key issue with content-based filtering is whether a given recommendation
    system can learn from a user's preferences or ratings. Direct feedback can be
    used by asking for the rating of items in the system that they like, although
    these ratings can also be implied from a user's past behavior. Also, a content-based
    filtering system that is trained for a set of users and a specific category of
    items cannot be used to predict the same user's ratings for a different category
    of items. For example, it's a difficult problem to use a user's preference for
    news to predict the user's liking for online shopping products.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other major form of recommendation is *collaborative filtering*, in which
    data about the behavior of several users with similar interests is analyzed and
    used to predict recommendations for these users. The main advantage of this technique
    is that the system does not rely on the values for the feature variables of its
    items, and consequently such a system does not need to know about the characteristics
    of the items that are provided by it. The features of the items are in fact determined
    dynamically using the users rating for these items and the behavior of the system
    users. We will examine more about the advantage in the latter part of this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'An essential part of the model used by collaborative filtering depends on the
    behavior of its users. To build this part of the model, we can use the following
    methods to determine the user''s rating for the items in the model in an explicit
    manner:'
  prefs: []
  type: TYPE_NORMAL
- en: Asking the users to rate the items on a specific scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asking users to mark items as favorites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Presenting a small number of items to users and asking them to order them according
    to how much they like or dislike these items
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asking users to create a list of items or the kinds of items that they like.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alternatively, this information could also be gathered from a user''s activity
    in an implicit fashion. Examples of this method of modeling the behavior of a
    system''s users with a given set of items or products are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Observing the items that a user views
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the number of times a particular user views
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the user's social network and discovering users with similar interests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, consider a recommendation system for an online shopping example
    that we discussed in the previous section. We we can use collaborative filtering
    to dynamically determine the feature values of the products available and predict
    the products that the user will be interested in. The sample data for such a system
    that uses collaborative filtering can be visualized by using the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collaborative filtering](img/4351OS_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the data shown in the preceding table, the features of the products are unknown.
    The only available data is the ratings of the users and the behavior models of
    the users.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization problem for collaborative filtering and a product''s user
    can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collaborative filtering](img/4351OS_08_23.jpg)![Collaborative filtering](img/4351OS_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation is seen to be the converse of the optimization problem
    that we defined for content-based filtering. Instead of estimating the parameter''s
    vector ![Collaborative filtering](img/4351OS_08_17.jpg), collaborative filtering
    seeks to determine the values of the features of a product ![Collaborative filtering](img/4351OS_08_16.jpg).
    Similarly, we can define the optimization problem for multiple users as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collaborative filtering](img/4351OS_08_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using collaborative filtering, we can estimate the features of the products
    ![Collaborative filtering](img/4351OS_08_26.jpg),![Collaborative filtering](img/4351OS_08_27.jpg),
    and then use these feature values to improve the behavior model of the users ![Collaborative
    filtering](img/4351OS_08_28.jpg). The improved user behavior models can then be
    used to again produce better feature values of the items. This process is then
    repeated until the feature values and behavior models converge to some appropriate
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that in this process, the algorithm never needed to know the initial feature
    values of its items, and it only needed to initially estimate the behavior model
    of the user to provide the user with useful recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Collaborative filtering can also be combined with content-based filtering in
    some special cases. Such approaches are called **hybrid methods** of recommendation.
    There are several ways in which we can combine or hybridize the two models of
    recommendation, and they are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Results from the two models can be combined numerically in a weighted manner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Either one of these two models can be chosen appropriately at a given time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Show users a combined result of recommendations from the two models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Slope One algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now study the Slope One algorithm for collaborative filtering. Also,
    we will demonstrate how we can implement it concisely in Clojure.
  prefs: []
  type: TYPE_NORMAL
- en: The Slope One algorithm is one of the simplest forms of *item-based collaborative
    filtering*, which is essentially a collaborative filtering technique in which
    the users explicitly rate each item they like (for more information, refer to
    *Slope One Predictors for Online Rating-Based Collaborative Filtering*). Generally,
    item-based collaborative filtering techniques will use the user's ratings and
    past behavior of users to estimate a simple regression model for each user. Thus,
    we estimate a function ![Using the Slope One algorithm](img/4351OS_08_29.jpg)
    for all users ![Using the Slope One algorithm](img/4351OS_08_13.jpg) in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Slope One algorithm uses a simpler predictor ![Using the Slope One algorithm](img/4351OS_08_30.jpg)
    to model the regression pattern of a user's behavior, and is thus less computationally
    expensive. The parameter ![Using the Slope One algorithm](img/4351OS_08_31.jpg)
    can be estimated by calculating the differences in user ratings between two items.
    Since the definition of the Slope One algorithm is simple, it can be implemented
    easily and efficiently. Interestingly, this algorithm is less susceptible to overfitting
    than other collaborative filtering techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a simple recommendation system with two items and two users. We can
    visualize this sample data with the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the Slope One algorithm](img/4351OS_08_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the data shown in the preceding table, the difference in the ratings of **Item
    A** and **Item B** can be found by using the ratings provided by **User 1**. This
    difference is found to be ![Using the Slope One algorithm](img/4351OS_08_33.jpg).
    Thus, we can add this difference to the rating of **Item A** by **User 2** to
    predict his/her rating of **Item B**, which is equal to ![Using the Slope One
    algorithm](img/4351OS_08_34.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s extend the preceding example to three items and three users. The table
    for this data can be visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the Slope One algorithm](img/4351OS_08_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this example, the average difference in ratings between **Item A** and **Item
    B** for **User 2** (-1) and **User 1** (+2) is ![Using the Slope One algorithm](img/4351OS_08_36.jpg).
    Hence, on average, **Item A** is rated better than **Item B** by ![Using the Slope
    One algorithm](img/4351OS_08_37.jpg). Similarly, the average rating difference
    between **Item A** and **Item C** is ![Using the Slope One algorithm](img/4351OS_08_38.jpg).
    We can predict the rating of **User 3** for **Item A** using his/her rating for
    **Item B** and the average difference of ratings for **Item A** and **Item B**.
    This value comes out to ![Using the Slope One algorithm](img/4351OS_08_39.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now describe a concise implementation the Slope One algorithm in Clojure.
    First off, we need to define our sample data. This can be done using a nested
    map, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code shown, we bind the value `nil` to the `?` symbol, and
    use it to define a nested map `data`, in which each key represents a user and
    its value represents a map of the user''s ratings with the item names as the keys.
    We will define some of the following utility methods to help us manipulate the
    nested map represented by `data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `flatten-to-vec` function defined in the preceding code simply converts
    a map to a flat vector using the `reduce` and `conj` functions. We can also define
    `flatten-to-vec`, by using a functional composition of the standard `vec`, `flatten`,
    and `seq` functions, as `(def flatten-to-vec (comp vec flatten seq))`. Since we
    are dealing with maps, we can define some of the following functions to map any
    function to the values of these maps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `map-vals` function defined in the preceding code can be used to mutate
    the values of a given map. This function uses the `assoc!` function to replace
    the value stored by a given key in the map, and it uses the `reduce` function
    to compose and apply the `assoc!` function to all the key-value pairs in the map.
    In Clojure, most collections, including maps, are persistent and immutable. Note
    the use of the `transient` function to convert a persistent and immutable map
    into a mutable one and the use of the `persistent!` function that converts a transient
    mutable collection to a persistent one. By isolating mutation, the performance
    of this function is improved while retaining the guarantee of immutability for
    the code that uses this function. The `map-nested-vals` function defined in the
    preceding code simply applies the `map-vals` function to the second level values
    in a nested map.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine the behavior of the `map-vals` and `map-nested-vals` functions
    in the REPL as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown the preceding REPL output, the `inc` function is applied to the values
    of maps `{:foo 1 :bar 2}` and `{:foo {:bar 3}}`. We now define a function to produce
    a trained model from the sample data by using the Slope One algorithm, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The train function defined in the preceding code first finds the differences
    between the ratings of all the items in the model using the `for` macro, and then
    it adds the frequencies of ratings of the items and the differences in their ratings
    using the `update-fn` closure.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main difference between a function and a macro is that a macro doesn't evaluate
    its parameters while being executed. Also, macros are resolved and expanded at
    compile time and functions are called at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `update-fn` function uses the `update-in` function to replace the value
    of a key in a map. Note the use of the `fnil` function, which essentially returns
    a function that checks for the value `nil` and replaces it with the second argument.
    This is used to treat the values represented by the `?` symbol that has the value
    `nil` in the nested map data. Lastly, the `train` function applies the `map-nested-vals`
    and `get-in` functions to the map of rating differences returned in the previous
    step. Finally, it returns a map with the keys `:freqs` and `:differences`, which
    contain maps that represent the frequencies of items and differences in ratings
    with respect to other items in the model respectively. We can now use this trained
    model to predict the ratings of the given items by various users. To do this,
    we will implement a function in the following code that uses the value returned
    by the `train` function defined in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `predict` function defined in the preceding code uses the `get-in` function
    to retrieve the sum of frequencies and differences of each item in the maps returned
    by the `train` function. This function then averages these rating differences
    by using a composition of the `reduce` and `/` (division) functions. The behavior
    of the `predict` function can be examined in the REPL, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding REPL output, the `predict` function used the value
    returned by the `train` function to predict the rating of `Item B` by a user who
    has given `Item A` a rating of `2`. The `predict` function estimates the rating
    of `Item B` as `3/2`. We can now implement a function in the following code that
    wraps around the `predict` function to find the ratings of all items in our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `mapmap` function defined in the preceding code simply applied two functions
    to a given sequence and returns a map with keys that are created using the first
    function `kf` and with a value generated by the second function `vf`. If only
    a single function is passed to the `mapmap` function, it uses the `identity` function
    to generate keys in the map returned by it. The `known-items` function defined
    in the preceding code will determine all items in a model using the keys function
    on the map represented by the :`differences` key in the value returned by the
    `train` function. Finally, the `predictions` function uses the value returned
    by the `train` and `known-items` functions to determine all items in the model
    and then predict all unrated items for a particular user. The function also takes
    an optional third argument, which is a vector of the item names whose ratings
    are to be predicted, in order to return the predictions of all items with names
    present in the vector `items`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can examine the behavior of the preceding function in the REPL, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding output, the `known-items` function returns the names
    of all items in the model. We can now try out the predictions function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that when we skip the last optional parameter of the `predictions` function,
    the map returned by this function will contain all items that are not previously
    rated by a particular user. This can be asserted in the REPL by using the `keys`
    function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: To conclude, we have demonstrated how we can implement the Slope One algorithm
    using nested maps and standard Clojure functions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed anomaly detection and recommendation. We also
    implemented a simple anomaly detector and recommendation engine. The topics covered
    in this chapter can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We explored anomaly detection and how we can implement an anomaly detector using
    the PDF in Clojure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We studied recommendation systems that use both content-based and collaborative
    filtering techniques. We also studied the various optimization problems in these
    techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also studied the Slope One algorithm, which is a form of collaborative filtering,
    and also described a concise implementation of this algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following chapter, we will discuss more applications of machine learning
    techniques that can be applied to large and complex data-centric applications.
  prefs: []
  type: TYPE_NORMAL
