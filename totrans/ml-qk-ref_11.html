<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Advanced Methods</h1>
                </header>
            
            <article>
                
<p class="mce-root">We have made it to the final chapter of this book. However, this doesn't mean that we can ignore the topics that we are going to discuss in the upcoming sections. These topics are state of the art and will separate you from the rest.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Kernel principal component analysis</li>
<li>Independent component analysis</li>
<li>Compressed sensing</li>
<li>Bayesian multiple imputations</li>
<li>Self-organizing maps</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we understood what <strong>principal component analysis</strong> <span>(</span><strong>PCA</strong><span>) </span>is, how it works, and when we should be deploying it. However, as a dimensionality reduction technique, do you think that you can put this to use in every scenario? Can you recall the roadblock or the underlying assumption behind it that we discussed?</p>
<p>Yes, the most important assumption behind PCA is that it works for datasets that are linearly separable. However, in the real world, you don't get this kind of dataset very often. We need a method to capture non-linear data patterns.</p>
<p class="mce-root"/>
<p>On the left-hand side, we have got a dataset in which there are two classes. We can see that once we arrive at the projections and establish the components, PCA doesn't have an effect on it and that it is not able to separate it by a line in a 2D dimension. That is, PCA can only function well when we have got low-level dimensions and linearly separable data. The following plot shows the dataset of two classes:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-897 image-border" src="assets/a232d621-ee4e-41be-8803-acf75c70efd6.png" style="width:36.00em;height:16.08em;"/></p>
<p>This is why we bring in the kernel method: so that we can merge it with PCA to achieve it.</p>
<p>Just to recap what you learned about the kernel method, we will discuss it and its importance in brief:</p>
<ul>
<li>We have got data in a low dimensional space. However, at times, it's difficult to achieve classification (green and red) when we have got non-linear data (as shown in the following diagram). This being said, we do have a clear understanding that having a tool that can map the data from a lower to a higher dimension will result in a proper classification. This tool is called the <strong>kernel method</strong>.</li>
<li>The same dataset turns out to be linearly separable in the new feature space.</li>
</ul>
<p style="padding-left: 60px">The following diagram shows data in low and high dimensional spaces:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-857 image-border" src="assets/c8e0b260-2472-4be2-a5ab-c9dbd95c0e2b.png" style="width:45.42em;height:19.75em;"/></p>
<p class="CDPAlignLeft CDPAlign">To classify the green and red points in the preceding diagram, the feature mapping function has to take the data and change is from being 2D to 3D, that is, <em>Φ = R<sup>2</sup> → R<sup>3</sup></em><span>. T</span><span>he equation for this is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a66ab011-d5a2-4bcd-9ac0-a5942e1c9c6d.png" style="width:24.17em;height:1.67em;"/></p>
<p>The goal of the kernel method is to figure out and choose kernel function <em>K</em>. This is so that we can find the geometry feature in the new dimension and classify data patterns. Let's see how this is done:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ce27e77c-769b-44f4-bef5-13c8c45a8758.png" style="width:24.33em;height:1.67em;"/></p>
<p class="CDPAlignCenter CDPAlign" style="padding-left: 90px"><img class="fm-editor-equation" src="assets/b741c251-2a86-48a1-9ca2-1d36852fe146.png" style="width:13.67em;height:1.50em;"/></p>
<p class="CDPAlignCenter CDPAlign" style="padding-left: 90px"><img class="fm-editor-equation" src="assets/ab6c39e4-d612-4194-a8fc-d36429518393.png" style="width:9.58em;height:1.67em;"/></p>
<p class="CDPAlignCenter CDPAlign" style="padding-left: 90px"><img class="fm-editor-equation" src="assets/21979279-0315-447f-b2fc-e8a34ca7c792.png" style="width:4.83em;height:1.67em;"/></p>
<p class="CDPAlignCenter CDPAlign" style="padding-left: 90px"><img class="fm-editor-equation" src="assets/a28c212f-cb59-4bd0-a194-ec0a992eb0cc.png" style="width:4.17em;height:1.17em;"/></p>
<p>Here, Phi is a feature mapping function. But do we always need to know the feature mapping function? Not really. Kernel function <em>K</em> does the trick. With a given kernel function, <em>K</em>, we can come up with a feature space, <em>H</em>. Two of the popular kernel functions are Gaussian and polynomial kernel functions.</p>
<p>Picking an apt kernel function will enable us to figure out the characteristics of the data in the new feature space quite well.</p>
<p>Now that we have made ourselves familiar with the kernel trick, let's move on to the Kernel PCA.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kernel PCA</h1>
                </header>
            
            <article>
                
<p>The Kernel PCA is an algorithm that not only keeps the main spirit of PCA as it is, but goes a step further to make use of the kernel trick so that it is operational for non-linear data:</p>
<ol>
<li>Let's define the covariance matrix of the data in the feature space, which is the product of the mapping function and the transpose of the mapping function:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/390c4461-854c-493d-8ffd-a0e4e8c7a67c.png" style="width:12.75em;height:3.67em;"/></p>
<p style="padding-left: 60px">It is similar to the one we used for PCA.</p>
<ol start="2">
<li>The next step is to solve the following equation so that we can compute principal components:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5c3b4a63-2c96-4b9e-bec7-23d242aab163.png" style="width:4.83em;height:1.17em;"/></p>
<p style="padding-left: 60px">Here, <em>C<sub>F</sub></em> is the covariance matrix of the data in feature space, <em>v</em> is the eigenvector, and <em>λ</em> (lambda) is the eigenvalues.</p>
<ol start="3">
<li>Let's put the value of <em>step 1</em> into <em>step 2 </em><span>– </span>that is, the value of<span> <em>C<sub>F</sub></em></span><span> in the equation of <em>step 2.</em> The e</span>igenvector will be as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/703773c2-f899-49f5-b428-f29b89542f81.png" style="width:34.00em;height:3.67em;"/></p>
<p style="padding-left: 60px">Here, <img class="fm-editor-equation" src="assets/4607835d-fe85-40be-ba82-34c117ed45a4.png" style="width:5.83em;height:2.33em;"/> is a scalar number.</p>
<p class="mce-root"/>
<ol start="4">
<li>Now, let's add the kernel function into the equation. Let's multiply <em>Φ(x<sub>k</sub>) </em><span>on both sides of the formula, </span><img class="fm-editor-equation" src="assets/dcda92c5-f517-4edb-838c-8a34df198137.png" style="font-size: 1em;color: #333333;width:4.83em;height:1.17em;"/>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/350e63a8-acbb-4abc-9650-2c1a679c3a25.png" style="width:14.50em;height:1.50em;"/></p>
<ol start="5">
<li>Let's put the value of <em>v</em> from the equation in <em>step 3</em> into the equation of <em>step 4</em>, as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d1c66f74-0346-4b50-bfa4-e479e9618888.png" style="width:29.17em;height:3.50em;"/></p>
<ol start="6">
<li>Now, we call <em>K </em><img class="fm-editor-equation" src="assets/4f568c8c-7d71-4ba3-98f9-3af69361f697.png" style="width:9.67em;height:1.42em;"/>. Upon simplifying the equation from <em>step 5</em> by keying in the value of <em>K</em>, we get the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cf4a358d-3282-40ed-89e8-5d402a80317c.png" style="width:8.25em;height:1.33em;"/></p>
<p style="padding-left: 60px">On doing eigen decomposition, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ae3ae0fb-89df-4a34-8075-56d9f82e8481.png" style="width:8.00em;height:1.17em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">On normalizing the feature space for centering, we get the following result:</p>
<p class="CDPAlignCenter CDPAlign" style="padding-left: 60px"><img class="alignnone size-full wp-image-898 image-border" src="assets/2cf064cb-9d15-463d-9965-6c1ffbd533ad.png" style="width:35.50em;height:14.83em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, let's execute the Kernel PCA in Python. We will keep this simple and work on the Iris dataset. We will also see how we can utilize the new compressed dimension in the model:</p>
<ol>
<li>Let's load the libraries:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np # linear algebra<br/>import pandas as pd # data processing<br/>import matplotlib.pyplot as plt<br/>from sklearn import datasets</pre>
<ol start="2">
<li>Then, load the data and create separate objects for the explanatory and target variables:</li>
</ol>
<pre style="padding-left: 60px">iris = datasets.load_iris()<br/>X = iris.data <br/>y = iris.target</pre>
<ol start="3">
<li>Let's have a look at the explanatory data:</li>
</ol>
<pre style="padding-left: 60px">X</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-859 image-border" src="assets/350741b5-b4ee-47cd-b509-36a0b61ff7a0.png" style="width:19.33em;height:23.92em;"/></p>
<ol start="4">
<li>Let's split the data into train and test sets, as follows:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)</pre>
<ol start="5">
<li>Now, we can standardize the data:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</pre>
<ol start="6">
<li>Let's have a look at <kbd>X_train</kbd>:</li>
</ol>
<pre style="padding-left: 60px">X_train</pre>
<p style="padding-left: 60px">The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-856 image-border" src="assets/6b2e88a9-d310-4d32-a791-ccd5fcec18fa.png" style="width:34.50em;height:21.33em;"/></p>
<ol start="7">
<li>Now, let's apply the kernel PCA on this. Here, we are trying to condense the data into just two components. The kernel that's been chosen here is the radial basis function:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.decomposition import KernelPCA<br/>kpca = KernelPCA(n_components = 2, kernel = 'rbf')<br/>X_train2 = kpca.fit_transform(X_train)<br/>X_test2 = kpca.transform(X_test)</pre>
<p style="padding-left: 60px">We have got the new train and test data with the help of the kernel PCA.</p>
<ol start="8">
<li>Let's see what the data looks like:</li>
</ol>
<pre style="padding-left: 60px">X_train2</pre>
<p style="padding-left: 60px">We get the following as output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1002 image-border" src="assets/f108b9c3-cb99-4b75-9b9e-d87476ddfae8.png" style="width:26.08em;height:25.00em;"/></p>
<p>Now, we've got two components here. Earlier, <kbd>X_train</kbd> showed us four variables. Now, the data has been shrunk into two fields.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Independent component analysis</h1>
                </header>
            
            <article>
                
<p><strong>Independent component analysis</strong> (<strong>ICA</strong>) is similar to PCA in terms of dimensionality reduction. However, it originated from the signal processing world wherein they had this problem that multiple signals were being transmitted from a number of sources, and there were a number of devices set up to capture it. However, the problem was that the captured signal by the device was not very clear as it happened to be a mix of a number of sources. They needed to have clear and independent reception for the different devices that gave birth to ICA. Heralt and Jutten came up with this in.</p>
<p>The difference between PCA and ICA is that PCA focuses upon finding uncorrelated factors, whereas ICA is all about deriving independent factors. Confused? Let me help you. Uncorrelated factors imply that there is no linear relationship between them, whereas independence means that two factors have got no bearing on each other. For example, scoring good marks in mathematics is independent of which state you live in. </p>
<p>An underlying assumption for this algorithm is that the variables are linear mixtures of unknown latent and independent variables.</p>
<p>The data <em>x<sub>i </sub>(t)</em> is modeled using hidden variables <em>s<sub>i</sub>(t)</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/78e5eed1-8082-4bd9-b049-53969f9be9df.png" style="width:8.42em;height:3.17em;"/></p>
<p>Here, <em>i= 1,2,3..........n.</em></p>
<p>It can also be written in the form of matrix decomposition as <strong>x=As</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-854 image-border" src="assets/285cf751-6a6b-4b1d-854b-6a4aa1d20554.png" style="width:35.42em;height:30.67em;"/></p>
<p>Here, we have the following:</p>
<ul>
<li><strong>A</strong>: Constant mixing matrix</li>
<li><strong>s</strong>: Latent factor matrices, which are independent of each other</li>
</ul>
<p>We have to estimate the values of both <strong>A</strong> and <strong>s</strong> while we have got <strong>X</strong>.</p>
<p>In other words, our goal is to find <em>W</em>, which is <em>W= A</em><sup><em>-1</em></sup>, which is an unmixing matrix.</p>
<p>Here, <em>s<sub>ij</sub> </em>has to be statistically independent of and non-Gaussian (not following normal distribution).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing for ICA</h1>
                </header>
            
            <article>
                
<p>The preprocessing of ICA can be done as follows:</p>
<ul>
<li><strong>Centering</strong>: The first step is to center <em>x</em>. That is, we need to subtract its mean vector from <em>x</em> so as to make <em>x</em> a zero-mean variable.</li>
<li><strong>Whitening</strong><span>: Before putting the data through ICA, we are supposed to whiten the data. This means that the data has to be uncorrelated. G</span><span>eometrically speaking, it tends to restore the initial shape of the data and only the resultant matrix needs to be rotated.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Approach</h1>
                </header>
            
            <article>
                
<p><span>To find out what unmixing matrices are independent, we have to bank upon non-Gaussianity. Let's see how we can do this.</span></p>
<p>Here, we will need to maximize the kurtosis, which will turn the distribution into a non-Gaussian. This will result in independent components. The following diagram shows an image of fast ICA:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/490fddd1-61b4-48b2-92fd-f42080453314.png" style="width:18.00em;height:16.17em;"/></p>
<p>For this, we have the <kbd>FastICA</kbd> library in Python.</p>
<p class="mce-root"/>
<p>Let's look at how we can execute this in Python. We will work with the same Iris data. This might not be an ideal dataset for executing ICA, but this is being done for directional purposes. To <span>execute the code in Python, we will need to perform the following steps:</span></p>
<ol>
<li>First, we need to load the library:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np # linear algebra<br/>import pandas as pd # data processing<br/>import matplotlib.pyplot as plt<br/>from sklearn import datasets</pre>
<ol start="2">
<li>Now, we need to load the data:</li>
</ol>
<pre style="padding-left: 60px">iris = datasets.load_iris()<br/>X = iris.data <br/>y = iris.target</pre>
<ol start="3">
<li>Let's partition the data into train and test sets:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)</pre>
<ol start="4">
<li>Let's make the data a standard scalar:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</pre>
<ol start="5">
<li>Now, we need to load in the ICA library:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.decomposition import FastICA</pre>
<ol start="6">
<li>We carry out ICA as follows. We will stick to three components here:</li>
</ol>
<pre style="padding-left: 60px">ICA = FastICA(n_components=3, random_state=10,whiten= True) <br/>X=ICA.fit_transform(X_train)</pre>
<ol start="7">
<li>We will then plot the results, as follows:</li>
</ol>
<pre style="padding-left: 60px">plt.figure(figsize=(8,10))<br/>plt.title('ICA Components')<br/>plt.scatter(X[:,0], X[:,1])<br/>plt.scatter(X[:,1], X[:,2])<br/>plt.scatter(X[:,2], X[:,0])</pre>
<p style="padding-left: 60px">The output for this is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-858 image-border" src="assets/f973e18d-de80-4e93-9c42-6f8e91373c7f.png" style="width:24.58em;height:29.25em;"/></p>
<p>We can see the three different components here (by color).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compressed sensing</h1>
                </header>
            
            <article>
                
<p>Compressed sensing is one of the easiest problems to solve in the area of information theory and signal processing. <span>It is a signal acquisition and reconstruction technique where the signal is compressible. The signal must be sparse. Compressed sensing tries to fit samples of a signal to functions, and it has a preference to use as few basic functions as possible to match the samples</span><span>. </span>This is described in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-850 image-border" src="assets/e2564922-b0cf-4d3d-8b76-7214e7f225be.png" style="width:25.83em;height:10.83em;"/></p>
<p>This is one of the prime equations that we see in linear algebra, where <strong>y</strong> is a <strong>M x 1</strong> matrix, phi is a <strong>M x N</strong> matrix that has got a number of columns that is higher than the number of rows, and <strong>x</strong> is a <strong>N x 1</strong> matrix comprising <strong>k</strong> non-zero entries. There are so many unknowns, which is expressed as an <strong>N</strong> length vector and <strong>M</strong> measurements, wherein <strong>M &lt;&lt; N</strong>. In this type of equation, we know that many solutions are possible since the null space of this matrix is non-trivial. Hence, this equation can accommodate many solutions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Our goal</h1>
                </header>
            
            <article>
                
<p>Our goal is to find out the solution with a least possible non-zero entry of all of the solutions. That is, the solution should give us as few non-zeros as possible. Are you wondering where this can be applied? There are plenty of applications for it. The areas where it can be applied are as follows:</p>
<ul>
<li>Signal representation</li>
<li>Medical imaging</li>
<li>Sparse channel estimation</li>
</ul>
<p>Let's say that we have got a time signal. This signal is highly sparse, but we know a little bit about it as it has a few frequencies. Can you sense what it is from the earlier equation? Yes, it can be deemed as <em>X</em>.</p>
<p>Let's call this<span> </span><strong>unknown</strong><span> </span>signal<span> </span><em><span class="math"><span class="MathJax"><span><span class="mrow"><span class="mi">X</span></span></span></span></span></em>. Now, even though we don't know the whole signal, we can still make observations about it, or samples, as shown in the following code:</p>
<pre>import numpy as np<br/>import matplotlib as mpl<br/>import matplotlib.pyplot as plt<br/>import scipy.optimize as spopt<br/>import scipy.fftpack as spfft<br/>import scipy.ndimage as spimg<br/>import cvxpy as cvx</pre>
<p>This will form a random equation:</p>
<pre>x = np.sort(np.random.uniform(0, 15, 30))<br/>y = 5 + 0.5 * x + 0.1 * np.random.randn(len(x))</pre>
<p>Now, we need to fit the <kbd>l1</kbd> norm. We get the following output:</p>
<pre> <br/>l1 = lambda x0, x, y: np.sum(np.abs(x0[0] * x + x0[1] - y))<br/>opt1 = spopt.fmin(func=l1, x0=[1, 1], args=(x, y))</pre>
<p>Then, we need to fit the <kbd>l2</kbd> norm. We get the following output:</p>
<pre class="mce-root"><br/>l2 = lambda x0, x, y: np.sum(np.power(x0[0] * x + x0[1] - y, 2))<br/>opt2 = spopt.fmin(func=l2, x0=[1, 1], args=(x, y))<br/><br/>y2 = y.copy()<br/>y2[3] += 5<br/>y2[13] -= 10<br/>xopt12 = spopt.fmin(func=l1, x0=[1, 1], args=(x, y2))<br/>xopt22 = spopt.fmin(func=l2, x0=[1, 1], args=(x, y2))</pre>
<p>By summing up the two sinusoids, we get the following output:</p>
<pre>n = 10000<br/>t = np.linspace(0, 1/5, n)<br/>y = np.sin(1250 * np.pi * t) + np.sin(3000 * np.pi * t)<br/>yt = spfft.dct(y, norm='ortho')<br/>plt.figure(figsize=[10,5])<br/>plt.plot(t,y)<br/>plt.title('Original signal')<br/>plt.xlabel('Time (s)')<br/>plt.ylabel('y')</pre>
<p>Now, let's take the sample out of <kbd>n</kbd>:</p>
<pre>m = 1000 # 10% sample<br/>ran = np.random.choice(n, m, replace=False) # random sample of indices<br/>t2 = t[ran]<br/>y2 = y[ran]</pre>
<p>Let's create the <kbd>idct</kbd> matrix operator:</p>
<pre># create idct matrix operator<br/>A = spfft.idct(np.identity(n), norm='ortho', axis=0)<br/>A = A[ran]<br/># do L1 optimization<br/>vx = cvx.Variable(n)<br/>objective = cvx.Minimize(cvx.norm(vx, 1))<br/>constraints = [A*vx == y2]<br/>prob = cvx.Problem(objective, constraints)<br/>result = prob.solve(verbose=True)</pre>
<p>The output for this is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-849 image-border" src="assets/ba0f4f4d-2003-4acb-b05f-88f6d596e91a.png" style="width:48.25em;height:33.92em;"/></p>
<p>To reconstruct the signal, we must do the following:</p>
<pre>x = np.array(vx.value)<br/>x = np.squeeze(x)<br/>signal = spfft.idct(x, norm='ortho', axis=0)</pre>
<p>That is how we reconstruct the signal.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Self-organizing maps</h1>
                </header>
            
            <article>
                
<p><strong>Self-organizing maps</strong> (<strong>SOM</strong>) were invented by Teuvo Kohonen in the 1980s. Sometimes, they are known as <strong>Kohonen maps</strong>. So, why do they exist? The prime motive for these kind of maps is to reduce dimensionality through a neural network. The following diagram shows the different 2D patterns from the input layers:</p>
<p class="CDPAlignCenter CDPAlign"> <img src="assets/45326737-dbab-470b-8d50-9d6a959d22bb.png" style="width:35.25em;height:22.00em;"/></p>
<p>They take the number of columns as input. As we can see from the 2D output, it transforms and reduces the amount of columns in the dataset into 2D.</p>
<p>The following link leads to the the 2D output: <a href="https://www.cs.hmc.edu/~kpang/nn/som.html">https://www.cs.hmc.edu/~kpang/nn/som.html</a></p>
<p>The depiction of the preceding diagram in 2D talks about a health of the country based on various factors. That is, it shows whether they are rich or poor. Some other factors that are taken into account are education, quality of life, sanitation, inflation, and health. Therefore, it forms a huge set of columns or dimensions. Countries such as Belgium and Sweden seem to show similar traits, depicting that they have got a good score on the health indicator.</p>
<p>Since this is an unsupervised learning technique, the data wasn't labeled. Based on patterns alone, the neural network is able to understand which country should be placed where.</p>
<p>Similar to the situation we just covered, opportunities are aplenty where self-organizing maps can be utilized. It can be thought as being similar in nature to K-means clustering.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SOM</h1>
                </header>
            
            <article>
                
<p>Let's go through process of how SOMs learn:</p>
<ol>
<li><span> Each node's weights are initialized by small standardized random values. These act like coordinates for different output nodes.</span></li>
<li>The first row's input (taking the first row from all of the variables) is fed into the first node.</li>
<li>Now, we have got two vectors. If <em>V</em> is the current input vector and <em>W</em> is the node's weight vector, then we calculate the Euclidean distance, like so:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/14175655-4cfa-4bfe-bfa3-fb1b6980397d.png" style="width:9.08em;height:3.17em;"/></p>
<ol start="4">
<li><span>The node that has a weight vector closest to the input vector is tagged as the <strong>best-matching unit</strong> (<strong>BMU</strong>).</span></li>
<li>A similar operation is carried out for all the rows of input and weight vectors. BMUs are found for all.</li>
<li>O<span><span>nce the BMU has been determined for every iteration, the other nodes within the BMU's</span> neighborhood<span> are computed. Nodes within the same radius will have their weights updated. A green arrow indicates the radius. Slowly, the</span> neighborhood<span> will shrink to the size of just one node, as shown in the following diagram:</span></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1003 image-border" src="assets/c16e2611-8dcc-4f93-85bd-f848c0c03343.png" style="width:18.25em;height:18.25em;"/></p>
<ol start="7">
<li>The most interesting part of the Kohonen algorithm is that the radius of the neighborhood keeps on shrinking. It takes place through the exponential decay function. <span>The value of lambda</span><span> is dependent on sigma. </span><span>The number of iterations that have been chosen for the algorithm to run is given by the following equation:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/521300be-39b7-464b-b6cf-54de2bbe33e7.png" style="width:8.83em;height:2.33em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1022 image-border" src="assets/19c08a6b-1031-49ce-8524-5cc78e877c20.png" style="width:34.58em;height:17.83em;"/></p>
<ol start="8">
<li>The weights get updated via the following equation:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a0e055c8-766e-4e64-8f0a-63c0da661b15.png" style="width:19.17em;height:1.25em;"/></p>
<p style="padding-left: 60px">Here, this is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/97390ac7-2127-4b75-abf3-b28943ae5e8e.png" style="width:10.08em;height:2.92em;"/></p>
<p style="padding-left: 60px"><em>t= 1, 2...</em> can be explained as follows:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li><em>L(t)</em>: Learning rate</li>
<li><em>D</em>: Distance of a node from BMU</li>
<li><em>σ</em>: Width of the function </li>
</ul>
</li>
</ul>
<p>Now, let's carry out one use case of this in Python. We will try to detect fraud in a credit card dataset:</p>
<ol>
<li>Let's load the libraries:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</pre>
<ol start="2">
<li>Now, it's time to load the data:</li>
</ol>
<pre style="padding-left: 60px">data = pd.read_csv('Credit_Card_Applications.csv')<br/>X = data.iloc[:, :-1].values<br/>y = data.iloc[:, -1].values</pre>
<ol start="3">
<li>Next, we will standardize the data:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.preprocessing import MinMaxScaler<br/>sc = MinMaxScaler(feature_range = (0, 1))<br/>X = sc.fit_transform(X)</pre>
<ol start="4">
<li>Let's import the <kbd>minisom</kbd> library and key in the hyperparameters, that is, learning rate, sigma, length, and number of iterations:</li>
</ol>
<pre style="padding-left: 60px">from minisom import MiniSom<br/>som = MiniSom(x = 10, y = 10, input_len = 15, sigma = 1.0, learning_rate = 0.5)<br/>som.random_weights_init(X)<br/>som.train_random(data = X, num_iteration = 100)    </pre>
<ol start="5">
<li>Let's visualize the results:</li>
</ol>
<pre style="padding-left: 60px">from pylab import bone, pcolor, colorbar, plot, show<br/>bone()<br/>pcolor(som.distance_map().T)<br/>colorbar()<br/>markers = ['o', 's']<br/>colors = ['r', 'g']<br/>for i, x in enumerate(X):<br/> w = som.winner(x)<br/> plot(w[0] + 0.5,<br/> w[1] + 0.5,<br/> markers[y[i]],<br/> markeredgecolor = colors[y[i]],<br/> markerfacecolor = 'None',<br/> markersize = 10,<br/> markeredgewidth = 2)<br/>show()</pre>
<p style="padding-left: 60px">The following output will be generated from the preceding code:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c7c81ab6-292a-4327-94f0-8311d8093be0.png" style="width:25.75em;height:18.00em;"/></p>
<p>We can see that the nodes that have a propensity toward fraud have got white backgrounds. This means that we can track down those customers with the help of those nodes:</p>
<pre>mappings = som.win_map(X)<br/>frauds = np.concatenate((mappings[(8,1)], mappings[(6,8)]), axis = 0)<br/>frauds = sc.inverse_transform(frauds)                                         </pre>
<p> This will give you the pattern of frauds. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bayesian multiple imputation</h1>
                </header>
            
            <article>
                
<p>Bayesian multiple imputation has got the spirit of the Bayesian framework. It is required to specify a parametric model for the complete data and a prior distribution over unknown model parameters, <em>θ</em>. Subsequently, <em>m</em> independent trials are drawn from the missing data, as given by the observed data using Bayes' Theorem. Markov Chain Monte Carlo can be used to simulate the entire joint posterior distribution of the missing data. BMI follows a normal distribution while generating imputations for the missing values.</p>
<p>Let's say that the data is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>Y = (Yobs, Ymiss),</em></p>
<p>Here, <em>Yobs</em> is the observed <em>Y</em> and <em>Ymiss</em> is the missing <em>Y.</em></p>
<p class="mce-root"/>
<p> If <em>P(Y|θ)</em> is the parametric model, the parameter <em>θ</em> is the mean and the covariance matrix that parameterizes a normal distribution. If this is the case, let <em>P(θ)</em> be the prior:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/432ba803-d702-4821-9978-219b8a355fa9.png" style="width:40.58em;height:2.50em;"/></p>
<p>Let's make use of the <kbd>Amelia</kbd> package in R and execute this:</p>
<pre>library(foreign)<br/>dataset = read.spss("World95.sav", to.data.frame=TRUE)<br/><br/>library(Amelia)<br/><br/>myvars &lt;- names(dataset) %in% c("COUNTRY", "RELIGION", "REGION","CLIMATE") <br/>newdata &lt;- dataset[!myvars]</pre>
<p class="mce-root"><span><span>Now, let's make the imputation:</span></span></p>
<pre class="mce-root">impute.out &lt;- amelia(newdata, m=4)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have studied the Kernel PCA, along with ICA. We also studied compressed sensing, the goals of compressed sensing, and self-organizing maps and how they work. Finally, we concluded with Bayesian multiple imputations.</p>


            </article>

            
        </section>
    </body></html>