["```py\nimport nltk\n\n>>> nltk.download()\n```", "```py\nfrom nltk.corpus import gutenberg\n\n>>> print(gutenberg.fileids())\n[u'austen-emma.txt', u'austen-persuasion.txt', u'austen-sense.txt', u'bible-kjv.txt', u'blake-poems.txt', u'bryant-stories.txt', u'burgess-busterbrown.txt', u'carroll-alice.txt', u'chesterton-ball.txt', u'chesterton-brown.txt', u'chesterton-thursday.txt', u'edgeworth-parents.txt', u'melville-moby_dick.txt', u'milton-paradise.txt', u'shakespeare-caesar.txt', u'shakespeare-hamlet.txt', u'shakespeare-macbeth.txt', u'whitman-leaves.txt']\n```", "```py\n>>> print(gutenberg.raw('milton-paradise.txt'))\n[Paradise Lost by John Milton 1667] \n\nBook I \n\nOf Man's first disobedience, and the fruit \nOf that forbidden tree whose mortal taste \nBrought death into the World, and all our woe, \nWith loss of Eden, till one greater Man \nRestore us, and regain the blissful seat, \nSing, Heavenly Muse, that, on the secret top...\n\n>>> print(gutenberg.sents('milton-paradise.txt')[0:2])\n[[u'[', u'Paradise', u'Lost', u'by', u'John', u'Milton', u'1667', u']'], [u'Book', u'I']]\n\n>>> print(gutenberg.words('milton-paradise.txt')[0:20])\n[u'[', u'Paradise', u'Lost', u'by', u'John', u'Milton', u'1667', u']', u'Book', u'I', u'Of', u'Man', u\"'\", u's', u'first', u'disobedience', u',', u'and', u'the', u'fruit']\n```", "```py\nfrom nltk.corpus import brown\n\n>>> print(brown.categories())\n[u'adventure', u'belles_lettres', u'editorial', u'fiction', u'government', u'hobbies', u'humor', u'learned', u'lore', u'mystery', u'news', u'religion', u'reviews', u'romance', u'science_fiction']\n\n>>> print(brown.sents(categories='editorial')[0:100])\n[[u'Assembly', u'session', u'brought', u'much', u'good'], [u'The', u'General', u'Assembly', u',', u'which', u'adjourns', u'today', u',', u'has', u'performed', u'in', u'an', u'atmosphere', u'of', u'crisis', u'and', u'struggle', u'from', u'the', u'day', u'it', u'convened', u'.'], ...]\n```", "```py\nfrom nltk.tokenize import sent_tokenize\n\n>>> generic_text = 'Lorem ipsum dolor sit amet, amet minim temporibus in sit. Vel ne impedit consequat intellegebat.'\n\n>>> print(sent_tokenize(generic_text))\n['Lorem ipsum dolor sit amet, amet minim temporibus in sit.',\n 'Vel ne impedit consequat intellegebat.']\n\n>>> english_text = 'Where is the closest train station? I need to reach London'\n\n>>> print(sent_tokenize(english_text, language='english'))\n['Where is the closest train station?', 'I need to reach London']\n\n>>> spanish_text = u'¿Dónde está la estación más cercana? Inmediatamente me tengo que ir a Barcelona.'\n\n>>> for sentence in sent_tokenize(spanish_text, language='spanish'):\n>>>    print(sentence)\n¿Dónde está la estación más cercana?\nInmediatamente me tengo que ir a Barcelona.\n```", "```py\nfrom nltk.tokenize import TreebankWordTokenizer\n\n>>> simple_text = 'This is a simple text.'\n\n>>> tbwt = TreebankWordTokenizer()\n\n>>> print(tbwt.tokenize(simple_text))\n['This', 'is', 'a', 'simple', 'text', '.']\n\n>>> complex_text = 'This isn\\'t a simple text'\n\n>>> print(tbwt.tokenize(complex_text))\n['This', 'is', \"n't\", 'a', 'simple', 'text']\n\n```", "```py\nfrom nltk.tokenize import RegexpTokenizer\n\n>>> complex_text = 'This isn\\'t a simple text.'\n\n>>> ret = RegexpTokenizer('[a-zA-Z0-9\\'\\.]+')\n>>> print(ret.tokenize(complex_text))\n['This', \"isn't\", 'a', 'simple', 'text.']\n```", "```py\n>>> complex_text = 'This isn\\'t a simple text. Count 1, 2, 3 and then go!'\n\n>>> ret = RegexpTokenizer('[a-zA-Z\\']+')\n>>> print(ret.tokenize(complex_text))\n['This', \"isn't\", 'a', 'simple', 'text', 'Count', 'and', 'the', 'go']\n```", "```py\nfrom nltk.corpus import stopwords\n\n>>> sw = set(stopwords.words('english'))\n```", "```py\n>>> print(sw)\n{u'a',\n u'about',\n u'above',\n u'after',\n u'again',\n u'against',\n u'ain',\n u'all',\n u'am',\n u'an',\n u'and',\n u'any',\n u'are',\n u'aren',\n u'as',\n u'at',\n u'be', ...\n```", "```py\n>>> complex_text = 'This isn\\'t a simple text. Count 1, 2, 3 and then go!'\n\n>>> ret = RegexpTokenizer('[a-zA-Z\\']+')\n>>> tokens = ret.tokenize(complex_text)\n>>> clean_tokens = [t for t in tokens if t not in sw]\n>>> print(clean_tokens)\n['This', \"isn't\", 'simple', 'text', 'Count', 'go']\n```", "```py\nfrom langdetect import detect\n\n>>> print(detect('This is English'))\nen\n\n>>> print(detect('Dies ist Deutsch'))\nde\n```", "```py\nfrom langdetect import detect_langs\n\n>>> print(detect_langs('I really love you mon doux amour!'))\n[fr:0.714281321163, en:0.285716747181]\n```", "```py\nfrom nltk.stem.snowball import SnowballStemmer\n\n>>> ess = SnowballStemmer('english', ignore_stopwords=True)\n>>> print(ess.stem('flies'))\nfli\n\n>>> fss = SnowballStemmer('french', ignore_stopwords=True)\n>>> print(fss.stem('courais'))\ncour\n```", "```py\nfrom nltk.stem.snowball import PorterStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\n\n>>> print(ess.stem('teeth'))\nteeth\n\n>>> ps = PorterStemmer()\n>>> print(ps.stem('teeth'))\nteeth\n\n>>> ls = LancasterStemmer()\n>>> print(ls.stem('teeth'))\ntee\n```", "```py\n>>> print(ps.stem('teen'))\nteen\n\n>>> print(ps.stem('teenager'))\nteenag\n\n>>> print(ls.stem('teen'))\nteen\n\n>>> print(ls.stem('teenager'))\nteen\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n>>> corpus = [\n 'This is a simple test corpus',\n 'A corpus is a set of text documents',\n 'We want to analyze the corpus and the documents',\n 'Documents can be automatically tokenized'\n]\n\n>>> cv = CountVectorizer()\n>>> vectorized_corpus = cv.fit_transform(corpus)\n>>> print(vectorized_corpus.todense())\n[[0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0]\n [1 1 0 0 0 1 1 0 0 0 0 0 0 2 0 1 0 1 1]\n [0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0]]\n```", "```py\n>>> print(cv.vocabulary_)\n{u'and': 1, u'be': 3, u'we': 18, u'set': 9, u'simple': 10, u'text': 12, u'is': 7, u'tokenized': 16, u'want': 17, u'the': 13, u'documents': 6, u'this': 14, u'of': 8, u'to': 15, u'can': 4, u'test': 11, u'corpus': 5, u'analyze': 0, u'automatically': 2}\n```", "```py\n>>> vector = [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1]\n>>> print(cv.inverse_transform(vector))\n[array([u'corpus', u'is', u'simple', u'test', u'this', u'want', u'we'], \n dtype='<U13')]\n```", "```py\n>>> ret = RegexpTokenizer('[a-zA-Z0-9\\']+')\n>>> sw = set(stopwords.words('english'))\n>>> ess = SnowballStemmer('english', ignore_stopwords=True)\n\n>>> def tokenizer(sentence):\n>>>    tokens = ret.tokenize(sentence)\n>>>    return [ess.stem(t) for t in tokens if t not in sw]\n\n>>> cv = CountVectorizer(tokenizer=tokenizer)\n>>> vectorized_corpus = cv.fit_transform(corpus)\n>>> print(vectorized_corpus.todense())\n[[0 0 1 0 0 1 1 0 0 0]\n [0 0 1 1 1 0 0 1 0 0]\n [1 0 1 1 0 0 0 0 0 1]\n [0 1 0 1 0 0 0 0 1 0]]\n```", "```py\n>>> cv = CountVectorizer(tokenizer=tokenizer, ngram_range=(1, 2))\n>>> vectorized_corpus = cv.fit_transform(corpus)\n>>> print(vectorized_corpus.todense())\n[[0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0]\n [1 1 0 0 0 1 1 0 0 0 0 0 0 2 0 1 0 1 1]\n [0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0]]\n\n>>> print(cv.vocabulary_)\n{u'and': 1, u'be': 3, u'we': 18, u'set': 9, u'simple': 10, u'text': 12, u'is': 7, u'tokenized': 16, u'want': 17, u'the': 13, u'documents': 6, u'this': 14, u'of': 8, u'to': 15, u'can': 4, u'test': 11, u'corpus': 5, u'analyze': 0, u'automatically': 2}\n```", "```py\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n\n>>> tfidfv = TfidfVectorizer()\n>>> vectorized_corpus = tfidfv.fit_transform(corpus)\n>>> print(vectorized_corpus.todense())\n[[ 0\\.          0\\.          0\\.          0\\.          0\\.          0.31799276\n 0\\.          0.39278432  0\\.          0\\.          0.49819711  0.49819711\n 0\\.          0\\.          0.49819711  0\\.          0\\.          0\\.          0\\.        ]\n [ 0\\.          0\\.          0\\.          0\\.          0\\.          0.30304005\n 0.30304005  0.37431475  0.4747708   0.4747708   0\\.          0.\n 0.4747708   0\\.          0\\.          0\\.          0\\.          0\\.          0\\.        ]\n [ 0.31919701  0.31919701  0\\.          0\\.          0\\.          0.20373932\n 0.20373932  0\\.          0\\.          0\\.          0\\.          0\\.          0.\n 0.63839402  0\\.          0.31919701  0\\.          0.31919701  0.31919701]\n [ 0\\.          0\\.          0.47633035  0.47633035  0.47633035  0.\n 0.30403549  0\\.          0\\.          0\\.          0\\.          0\\.          0.\n 0\\.          0\\.          0\\.          0.47633035  0\\.          0\\.        ]]\n```", "```py\n>>> print(tfidfv.vocabulary_)\n{u'and': 1, u'be': 3, u'we': 18, u'set': 9, u'simple': 10, u'text': 12, u'is': 7, u'tokenized': 16, u'want': 17, u'the': 13, u'documents': 6, u'this': 14, u'of': 8, u'to': 15, u'can': 4, u'test': 11, u'corpus': 5, u'analyze': 0, u'automatically': 2}\n```", "```py\n>>> tfidfv = TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1, 2), norm='l2')\n>>> vectorized_corpus = tfidfv.fit_transform(corpus)\n>>> print(vectorized_corpus.todense())\n[[ 0\\.          0\\.          0\\.          0\\.          0.30403549  0\\.          0.\n 0\\.          0\\.          0\\.          0\\.          0.47633035  0.47633035\n 0.47633035  0.47633035  0\\.          0\\.          0\\.          0\\.          0\\.        ]\n [ 0\\.          0\\.          0\\.          0\\.          0.2646963   0.\n 0.4146979   0.2646963   0\\.          0.4146979   0.4146979   0\\.          0.\n 0\\.          0\\.          0.4146979   0.4146979   0\\.          0\\.          0\\.        ]\n [ 0.4146979   0.4146979   0\\.          0\\.          0.2646963   0.4146979\n 0\\.          0.2646963   0\\.          0\\.          0\\.          0\\.          0.\n 0\\.          0\\.          0\\.          0\\.          0\\.          0.4146979\n 0.4146979 ]\n [ 0\\.          0\\.          0.47633035  0.47633035  0\\.          0\\.          0.\n 0.30403549  0.47633035  0\\.          0\\.          0\\.          0\\.          0.\n 0\\.          0\\.          0\\.          0.47633035  0\\.          0\\.        ]]\n\n>>> print(tfidfv.vocabulary_)\n{u'analyz corpus': 1, u'set': 9, u'simpl test': 12, u'want analyz': 19, u'automat': 2, u'want': 18, u'test corpus': 14, u'set text': 10, u'corpus set': 6, u'automat token': 3, u'corpus document': 5, u'text document': 16, u'token': 17, u'document automat': 8, u'text': 15, u'test': 13, u'corpus': 4, u'document': 7, u'simpl': 11, u'analyz': 0}\n```", "```py\nfrom nltk.corpus import reuters\n\n>>> print(reuters.categories())\n[u'acq', u'alum', u'barley', u'bop', u'carcass', u'castor-oil', u'cocoa', u'coconut', u'coconut-oil', u'coffee', u'copper', u'copra-cake', u'corn', u'cotton', u'cotton-oil', u'cpi', u'cpu', u'crude', u'dfl', u'dlr', u'dmk', u'earn', u'fuel', u'gas', u'gnp', u'gold', u'grain', u'groundnut', u'groundnut-oil', u'heat', u'hog', u'housing', u'income', u'instal-debt', u'interest', u'ipi', u'iron-steel', u'jet', u'jobs', u'l-cattle', u'lead', u'lei', u'lin-oil', u'livestock', u'lumber', u'meal-feed', u'money-fx', u'money-supply', u'naphtha', u'nat-gas', u'nickel', u'nkr', u'nzdlr', u'oat', u'oilseed', u'orange', u'palladium', u'palm-oil', u'palmkernel', u'pet-chem', u'platinum', u'potato', u'propane', u'rand', u'rape-oil', u'rapeseed', u'reserves', u'retail', u'rice', u'rubber', u'rye', u'ship', u'silver', u'sorghum', u'soy-meal', u'soy-oil', u'soybean', u'strategic-metal', u'sugar', u'sun-meal', u'sun-oil', u'sunseed', u'tea', u'tin', u'trade', u'veg-oil', u'wheat', u'wpi', u'yen', u'zinc']\n```", "```py\nimport numpy as np\n\n>>> Xr = np.array(reuters.sents(categories=['rubber']))\n>>> Xc = np.array(reuters.sents(categories=['cotton']))\n>>> Xw = np.concatenate((Xr, Xc))\n```", "```py\n>>> X = []\n\n>>> for document in Xw:\n>>>    X.append(' '.join(document).strip().lower())\n```", "```py\n>>> Yr = np.zeros(shape=Xr.shape)\n>>> Yc = np.ones(shape=Xc.shape)\n>>> Y = np.concatenate((Yr, Yc))\n```", "```py\n>>> tfidfv = TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1, 2), norm='l2')\n>>> Xv = tfidfv.fit_transform(X)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n>>> X_train, X_test, Y_train, Y_test = train_test_split(Xv, Y, test_size=0.25)\n\n>>> rf = RandomForestClassifier(n_estimators=25)\n>>> rf.fit(X_train, Y_train)\n>>> score = rf.score(X_test, Y_test)\n>>> print('Score: %.3f' % score)\nScore: 0.874\n```", "```py\n>>> test_newsline = ['Trading tobacco is reducing the amount of requests for cotton and this has a negative impact on our economy']\n\n>>> yvt = tfidfv.transform(test_newsline)\n>>> category = rf.predict(yvt)\n>>> print('Predicted category: %d' % int(category[0]))\nPredicted category: 1\n```"]