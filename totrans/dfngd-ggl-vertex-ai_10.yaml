- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vertex AI Deployment and Automation Tools – Orchestration through Managed Kubeflow
    Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a typical **machine learning** (**ML**) solution, we often have lots of applications
    and services as part of the end-to-end workflow. If we try to stitch these services
    and applications together using some custom scripts with cron jobs, it becomes
    super tricky to manage the workflows. Thus, it becomes important to make use of
    some orchestration services to carefully manage, scale, and monitor complex workflows.
    Orchestration is the process of stitching multiple applications or services together
    to build an end-to-end solution workflow. Google Cloud provides multiple orchestration
    services, such as Cloud Scheduler, Workflows, and Cloud Composer, to manage complex
    workflows at scale. Cloud Scheduler is ideal for single, repetitive tasks, Workflows
    is more suitable for complex multi-service orchestration, and Cloud Composer is
    ideal for data-driven workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'ML workflows have a lot of steps, from data preparation to model training,
    evaluation, and more. On top of that, monitoring and version tracking become even
    more challenging. In this chapter, we will learn about GCP tooling for orchestrating
    ML workflows effectively. The main topics covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating ML workflows using Vertex AI Pipelines (managed Kubeflow pipelines)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orchestrating ML workflows using Cloud Composer (managed Airflow)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertex AI Pipelines versus Cloud Composer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting predictions on Vertex AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing deployed models on Vertex AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code examples shown in this chapter can be found in the following GitHub
    repo:[https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter10](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter10)
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating ML workflows using Vertex AI Pipelines (managed Kubeflow pipelines)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML solutions are complex and involve lots of steps, including data preparation,
    feature engineering, model selection, model training, testing, evaluation, and
    deployment. On top of these, it is really important to track and version control
    lots of aspects related to the ML model while in production. Vertex AI Pipelines
    on GCP lets us codify our ML workflows in such a way that they are easily composable,
    shareable, and reproducible. Vertex AI Pipelines can run Kubeflow as well as **TensorFlow
    Extended** (**TFX**)-based ML pipelines in a fully managed way. In this section,
    we will learn about developing Kubeflow pipelines for ML development as Vertex
    AI Pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow is a Kubernetes-native solution that simplifies the orchestration of
    ML pipelines and makes experimentation easy and reproducible. Also, the pipelines
    are sharable. It comes with framework support for things such as execution monitoring,
    workflow scheduling, metadata logging, and versioning. A Kubeflow pipeline is
    a description of an ML workflow that combines multiple small components of the
    workflow into a **directed acyclic graph** (**DAG**). Behind the scenes, it runs
    the pipeline components on containers, which provide portability, reproducibility,
    and encapsulation. Each pipeline component is one step in the ML workflow that
    does a specific task. The output of one component may become the input of another
    component and so forth. Each pipeline component is made up of code, packaged as
    a Docker image that performs one step in the pipeline and runs on one or more
    Kubernetes Pods. Kubeflow pipelines can be leveraged for ETL and CI/CD tasks but
    they are more popularly used to run ML workflows.
  prefs: []
  type: TYPE_NORMAL
- en: The Vertex AI SDK lets us create and upload Kubeflow pipelines programmatically
    from within the Jupyter Notebook itself, but we can also use the console UI to
    work on pipelines. The Vertex AI UI lets us visualize the pipeline execution graph.
    It also lets us track, monitor, and compare different pipeline executions.
  prefs: []
  type: TYPE_NORMAL
- en: Developing Vertex AI Pipeline using Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will develop and launch a simple Kubeflow-based Vertex Pipeline
    using the Vertex AI SDK within a Jupyter Notebook. In this example, we will work
    on an open source wine quality dataset. Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter Notebook and install some useful libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In a new cell, import useful libraries for Vertex Pipeline development:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a timestamp variable. It will be useful in creating unique names for
    pipeline objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will set some project-related configurations, such as `project_id`,
    region, staging bucket, and service account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this section we will use the Wine Quality dataset. The Wine Quality dataset
    was created by Cortez,Paulo, Cerdeira,A., Almeida,F., Matos,T., and Reis,J.. (2009).
    You can check it out at OR You can download the dataset from the following link:
    [https://doi.org/10.24432/C56S3T](https://doi.org/10.24432/C56S3T). (UCI Machine
    Learning Repository.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we load and check the wine quality dataset in a notebook cell to understand
    the data and columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The output of this snippet is shown in *Figure 10**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Overview of the wine quality dataset](img/B17792_10_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Overview of the wine quality dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a quick overview of the feature columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`volatile acidity`: The `volatile acidity` column represents the amount of
    gaseous acids'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fixed acidity`: The amount of fixed acids found in wine, which can be tartaric,
    succinic, citric, malic, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`residual sugar`: This column represents the amount of sugar left after the
    fermentation of wine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`citric acid`: The amount of citric acid, which is naturally found in fruits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chlorides`: The amount of salt in the wine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`free sulfur dioxide`: Sulpher dioxide, or SO2, prevents wine oxidation and
    spoilage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total sulfur dioxide`: The total amount of SO2 in a wine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pH`: pH is used for checking acidity in a wine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`density`: Represents the density of the wine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sulphates`: Sulphates help preserve the freshness of wine and also protect
    it from oxidation and bacteria'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alcohol`: The percentage of alcohol present in the wine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea is to predict the wine quality given all the preceding parameters.
    We will convert it into a classification problem and call a wine *best quality*
    if its quality indicator value is >=7.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will define four pipeline components for our task:'
  prefs: []
  type: TYPE_NORMAL
- en: Data loading component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model deploying component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, the first component loads the data and the second component uses that
    data to train a model. The third component evaluates the trained model on the
    test dataset. The fourth component automatically deploys the trained model as
    a Vertex AI endpoint. We will put a condition on automatic model deployment, such
    as if model ROC >= 0.8, then deploy the model, otherwise don’t.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s define these components one by one. The following is the first component
    that loads and splits the data into training and testing partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a Kubeflow component, we can wrap our function with an `@component`
    decorator. Here, we can define the base image, and also the dependencies to install:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In a real project or production pipeline, it is advisable to write package versions
    along with their names to avoid any version realated conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define the function that loads and splits the data into train and
    test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We will keep about 30% of the data for testing and the remaining for training
    and save them as CSV files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To define a component, we can wrap our Python functions with an `@component`
    decorator. It allows us to pass the base image path, packages to install, and
    a YAML file path if we wish to write the component into a file. The YAML file
    definition of a component makes it portable and reusable. We can simply create
    a YAML file with the component definition and load this component anywhere in
    the project. Note that we can use our custom container image with all our custom
    dependencies as well.
  prefs: []
  type: TYPE_NORMAL
- en: The first component essentially loads the wine quality dataset table, creates
    the binary classification output, as discussed previously, drops unnecessary columns,
    and finally divides it into train and test files. Here, the train and test dataset
    files are output artifacts of this component that can be reused by subsequently
    running components.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s define the second component, which trains a random forest classifier
    over the training dataset generated by the first component.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is the decorator, with dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Next, we define our training function, which fits our model on training data
    and saves it as a Pickle file. Here, our output artifact would be a model and
    we can associate it with some metadata as well, as shown in the following function.
    Inside this function, we can associate the model artifact with metadata by putting
    the metadata key and value within `model.metadata` dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This component trains a random forest classifier model on the training dataset
    and saves the model as a Pickle file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s define the third component for model evaluation. We start with
    the `@``component` decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define the actual Python function for model evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a small function that controls the deployment of the model. We only
    deploy a new model if its accuracy is above a certain threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the model outputs, we can calculate accuracy scores and `roc_curve`,
    and log them as metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we check the model accuracy and see whether it satisfies the deployment
    condition. We return the deployment condition flag from here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This component uses the outputs of component 1 (test dataset) and component
    2 (trained model) as input and performs model evaluation. This component performs
    the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Loads the test dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loads the trained model from a Pickle file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs the ROC curve and confusion matrix as an output artifact
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checks whether the model accuracy is greater than the threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we define the model deployment component. This component automatically
    deploys the trained model as a Vertex AI endpoint if the deployment condition
    is `true`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the function that deploys the wine quality model when the deployment
    condition is true. This function will be wrapped around by the previously defined
    `@component` decorator so that we can later use it in the final pipeline definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define a function to create the endpoint for our model so that we
    can use it for inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we import our saved model programmatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we deploy the uploaded model on the desired machine type with the
    desired traffic split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now that the core components of our pipeline are ready, we can go ahead and
    define our Vertex Pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to provide a unique name for our pipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Pipeline definition is the part where we stitch these components together to
    define our ML workflow (or execution graph). Here, we can control which components
    run first and the output of which component should be fed to another component.
    The following scripts define a simple pipeline for our experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `@dsl.pipeline` decorator to define a Kubeflow pipeline. We
    can pass here a `pipeline_root` parameter inside the decorator, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create the execution DAG here and define the order of execution for
    our predefined components. Some components can be dependent, where the output
    of one component is the input for another. Dependent components execute sequentially,
    while independent ones can be executed in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is our condition that decides whether to deploy this model or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, we use the `@dsl.pipeline` decorator to define our pipeline. Note that
    in the preceding definition, the first three components are simple, but the fourth
    component has been defined using `dsl.Condition()`. We only run the model deployment
    component if this condition is satisfied. So, this is how we can control when
    to deploy the model. If our model meets the business criteria, we can choose to
    auto-deploy it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can compile our pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can submit our pipeline job to Vertex AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This script will launch our pipeline in Vertex AI. It will also provide us with
    a console URL to monitor the pipeline job.
  prefs: []
  type: TYPE_NORMAL
- en: We can also locate the pipeline run by going to the **Vertex AI** tab in the
    console and clicking on the **pipelines** tab. *Figure 10**.2* is a screenshot
    of the execution graph present in Vertex AI for our example job.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Execution graph of our example Vertex Pipeline from the Google
    Cloud console UI](img/B17792_10_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Execution graph of our example Vertex Pipeline from the Google
    Cloud console UI
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, this execution graph has all four components defined by us. It
    also has all the artifacts generated by the components. If we click on the **metrics**
    artifact, we can see the output values in the right pane of the console UI. It
    looks something similar to *Figure 10**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Metadata and artifacts related to our pipeline execution](img/B17792_10_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Metadata and artifacts related to our pipeline execution
  prefs: []
  type: TYPE_NORMAL
- en: This is how we can use the Google Cloud console UI to track the execution and
    metrics of our ML-related workflows. Once we have our pipeline ready, we can also
    schedule its execution using services such as the native scheduler for Vertex
    AI Pipelines, Cloud Scheduler (we can define a schedule), Cloud Functions (event-based
    trigger), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have a good understanding of how Kubeflow pipelines can be developed
    on Google Cloud as Vertex AI Pipelines. We should be able to develop and launch
    our custom pipelines from scratch now. In the next section, we will learn about
    Cloud Composer as another solution for workflow orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating ML workflows using Cloud Composer (managed Airflow)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud Composer is a workflow orchestration service on Google Cloud that is built
    upon the open source project of Apache Airflow. The key difference is that Composer
    is fully managed and also integrates with other GCP tooling very easily. With
    Cloud Composer, we can write, execute, schedule, or monitor our workflows that
    are also supported across multi-cloud and hybrid environments. Composer pipelines
    are DAGs that can be easily defined and configured using Python. It comes with
    a rich library of connectors that let us deploy our workflows instantly with one
    click. Graphical representations of workflows on the Google Cloud console make
    monitoring and troubleshooting quite convenient. Automatic synchronization of
    our DAGs ensures that our jobs always stay on schedule.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Composer is commonly used by data scientists and data engineers to build
    complex data pipelines (ETL or ELT pipelines). It can also be used as an orchestrator
    for ML workflows. Cloud Composer is pretty convenient for data-related workflows
    as the Apache project comes with hundreds of operators and sensors that make it
    easy to communicate across multiple cloud environments with very little code.
    It also lets us define failure handling mechanisms such as sending emails or Slack
    notifications on pipeline failure.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s understand how to develop Cloud Composer-based pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Cloud Composer environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can follow these steps to create a Cloud Composer environment using the
    Google Cloud console UI:'
  prefs: []
  type: TYPE_NORMAL
- en: Enable the Cloud Composer API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the left pane of the console, select **Composer** and click on **Create**
    to start creating a Composer environment (see *Figure 10**.4*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![ Figure 10.4 – Creating a Composer environment on the Google Cloud console](img/B17792_10_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Creating a Composer environment on the Google Cloud console
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Create**. It will take about 15–20 minutes to create the environment.
    Once it is complete, the environment page will look like the following (see *Figure
    10**.5*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Ready-to-use Cloud Composer environment](img/B17792_10_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Ready-to-use Cloud Composer environment
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Airflow** to see the Airflow web UI. The Airflow web UI is shown
    in *Figure 10**.6*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![ Figure 10.6 – Airflow web UI with our workflows](img/B17792_10_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – Airflow web UI with our workflows
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding screenshot, there is already one DAG running
    – `airflow_monitoring.py` file. See *Figure 10**.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – GCS location where we can put our Python-based DAGs for execution](img/B17792_10_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – GCS location where we can put our Python-based DAGs for execution
  prefs: []
  type: TYPE_NORMAL
- en: Now that our Composer setup is ready, we can quickly check whether it is working
    as expected. To test things fast, we will use one demo DAG from the Airflow tutorials
    and put it inside the **dags** folder of this bucket. If everything is working
    fine, any DAG that we put inside this bucket should automatically get synced with
    Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code for a demo DAG from the Airflow tutorials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following dictionary with some default arguments will be used when creating
    the operators later. By default, these arguments will be passed to each operator,
    but we can also override some of these arguments in some operators as per the
    requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This is where we define our DAG, with execution steps in the desired or required
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define different tasks that our code will be performing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You can document your task using the following attributes: `doc_md` (Markdown),
    `doc` (plain text), `doc_rst`, `doc_json`, and `doc_yaml`, which gets rendered
    on the UI’s **Task Instance** **Details** page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s define the `t3` task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define the execution order of our tasks. `t1` needs to be executed
    before `t2` and `t3`, but `t2` and `t3` can execute in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As soon as we upload this `.py` file to a GCS bucket inside the dags folder,
    Airflow will automatically sync it. If you refresh the Airflow web UI, it should
    show another DAG, as shown in *Figure 10**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Airflow web UI with all the DAGs that are present in the GCS
    location](img/B17792_10_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – Airflow web UI with all the DAGs that are present in the GCS location
  prefs: []
  type: TYPE_NORMAL
- en: If we are able to see our DAG running in the Airflow UI, it verifies that our
    installation is working fine. Now, let’s open this DAG to check the actual execution
    graph. It should look something similar to what is shown in *Figure 10**.9*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Execution graph of our workflow within the Airflow web UI](img/B17792_10_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – Execution graph of our workflow within the Airflow web UI
  prefs: []
  type: TYPE_NORMAL
- en: Although it is a very simple DAG, it gives an idea of how easy it is to work
    with Airflow using Cloud Composer. The level of logging and monitoring we get
    with Cloud Composer is quite amazing. Cloud Composer makes the lives of data engineers
    really easy so that they can focus on defining complex data pipelines without
    worrying about infrastructure and Airflow management.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a good idea of how Vertex AI Pipelines and Cloud Composer (managed
    Airflow service) can be used as an orchestrator for ML workflows. Now let’s summarize
    some of the similarities and differences between these two.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Pipelines versus Cloud Composer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will talk about some of the key similarities and differences
    between Vertex AI Pipelines and Cloud Composer when it comes to orchestrating
    ML workflows. Based on this comparison, we can choose the best solution for our
    next ML project. The following is a list of points that summarize the important
    aspects of both orchestrators for ML-related tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Both are easy to use and divide the overall ML workflow into smaller execution
    units in terms of tasks (Composer) or containerized components (Vertex AI Pipelines).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passing data between components is similar, and it may require an intermediate
    storage system if the data size is large.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertex AI Pipelines have an extensive list of prebuilt components available
    open source and thus developers can avoid writing a lot of boilerplate code. On
    the other hand, in the case of a Composer-based pipeline, we need to write the
    entire workflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the ease of setting up environments, Vertex AI Pipelines is a little
    bit easier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both run on Kubernetes, but in the case of Vertex AI Pipelines, there is no
    need to worry about clusters, Pods, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Composer is ideal for data-related tasks. We can also implement ML pipelines
    as a data task but we lose a lot of ML-related functionalities, such as lineage
    tracking, metrics, experiment comparisons, and distributed training. These features
    come out of the box with Vertex AI Pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data engineers might feel more comfortable with Composer pipelines, while ML
    engineers might be more comfortable with Vertex AI Pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In many cases, Vertex AI Pipelines can be cheaper to use as here we pay for
    what we use. On the other hand, in the case of Composer, some Pods are always
    running.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If needed, some of the Vertex AI Pipelines capabilities can be used with Composer
    as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with Vertex AI Pipelines requires zero knowledge about Kubernetes, but
    with Cloud Composer, it is important to know common aspects of Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After reading these comparison points, we might find it easy to choose the best
    orchestrator for our next ML use case. Nevertheless, both orchestrators are easy
    to use and are commonly used across organizations to manage their complex data/ML-related
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good understanding of ML orchestration tools on Google Cloud
    with their pros and cons, we are ready to start developing production-grade ML
    pipelines. Next, let’s learn how to get predictions on Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: Getting predictions on Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to get predictions from our ML models on
    Vertex AI. Depending on the use case, prediction requests can be of two types
    – online predictions (real time) and batch predictions. Online predictions are
    synchronous requests made to a model endpoint. Online predictions are needed by
    applications that keep requesting outputs for given inputs in a timely manner
    via an API call in order to update information for end users in near real time.
    For example, the Google Maps API gives us near real-time traffic updates and requires
    online prediction requests. Batch predictions, on the other hand, are asynchronous
    requests. If our use case only requires batch prediction, we might not need to
    deploy the model to an endpoint as the Vertex AI `batchprediciton` service also
    allows us to perform batch prediction from a saved model that is present in a
    GCS location without even needing to create an endpoint. Batch predictions are
    suitable for use cases where the response is not time sensitive and we can afford
    to get a delayed response (for example, an e-commerce company may wish to forecast
    sales for the next six months or so). Using batch predictions, we can make predictions
    of a large amount of data with just a single request.
  prefs: []
  type: TYPE_NORMAL
- en: Getting online predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We must deploy our model to an endpoint before that model can be used to serve
    online prediction requests. Model deployment essentially means keeping the model
    in memory with the required infrastructure (memory and compute) so that it can
    serve predictions with low latency. We can deploy multiple models to a single
    endpoint as well as a single model to multiple endpoints based on the use case
    and scaling requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you deploy a model using the Vertex AI API, you complete the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the endpoint ID.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the model to the endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can use the following Python sample function to create a Vertex AI endpoint.
    This function is taken from official documentation ([https://cloud.google.com/vertex-ai/docs/general/deployment#api](https://cloud.google.com/vertex-ai/docs/general/deployment#api)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The second step is to get the endpoint ID so that we can use it to deploy our
    model. The following shell command will give us a list of all the endpoints within
    our project and location. We can filter it with the endpoint name if we have it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the endpoint ID, we can deploy our model to this endpoint.
    While deploying the model, we can specify parameters for a number of replicas,
    the accelerator count, accelerator types, and so on. The following is a sample
    Python function that can be used to deploy the model to a given endpoint. This
    sample has been taken from the Google Cloud documentation ([https://cloud.google.com/vertex-ai/docs/general/deployment#api](https://cloud.google.com/vertex-ai/docs/general/deployment#api)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we initialize the Vertex AI SDK and deploy our model to an endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Once our model is deployed to an endpoint, it is ready to serve online predictions.
    We can now make online prediction requests to this endpoint. See the following
    sample request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `instances[]` object is required and must contain the list of instances
    to get predictions for. See the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The response body is also similar. It may look something like the following
    example. This example is not related to the earlier model; it is just for understanding
    purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The response when there is an error in processing the input looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We now have a good idea of how to get online predictions using Vertex AI endpoints.
    But not every use case requires on-demand or online predictions. There are times
    when we want to make predictions on a large amount of data but the results are
    not immediately required. In such cases, we can utilize batch predictions. Let’s
    discuss more about getting batch predictions using Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: Getting batch predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed before, batch prediction requests are asynchronous and do not
    require a model to be deployed to an endpoint all the time. To make a batch prediction
    request, we specify an input source and an output location (either Cloud Storage
    or BigQuery), where Vertex AI stores prediction results. The input source location
    must contain our input instances in one of the accepted formats: TFRecord, JSON
    Lines, CSV, BigQuery, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TFRecord input instances may look something like the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Batch prediction can be requested through Vertex AI API programatically or also
    with Google Cloud console UI. As we can pass lots of data to batch prediction
    requests, they may take a long time to complete depending upon the size of data
    and model.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample batch prediction request using the Vertex AI API with Python may look
    something like the following Python function. This sample code has been taken
    from the official documentation ([https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we initialize the Vertex AI SDK and call batch predictions on our deployed
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Once the batch prediction request is complete, the output is saved in the specified
    Cloud Storage or BigQuery location.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `jsonl` output file might look something like the following example output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We now have a fair idea of how online and batch prediction work on Vertex AI.
    The idea of separating batch prediction from online prediction (eliminating the
    need for deployment) saves a lot of resources and costs. Next, let’s discuss some
    important considerations related to deployed models on Google Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: Managing deployed models on Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we deploy an ML model to an endpoint, we associate it with physical resources
    (compute) so that it can serve online predictions at low latency. Depending on
    the requirements, we might want to deploy multiple models to a single endpoint
    or a single model to multiple endpoints as well. Let’s learn about these two scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple models – single endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we already have one model deployed to an endpoint in production and
    we have found some interesting ideas to improve that model. Now, suppose we have
    already trained an improved model that we want to deploy but we also don’t want
    to make any sudden changes to our application. In this situation, we can add our
    latest model to the existing endpoint and start serving a very small percentage
    of traffic with the new model. If everything looks great, we can gradually increase
    the traffic until it is serving the full 100% of the traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Single model – multiple endpoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is useful when we want to deploy our model with different resources for
    different application environments, such as testing and production. Secondly,
    if one of our applications has high-performance needs, we can serve it using an
    endpoint with high-performance machines, while we can serve other applications
    with lower-performance machines to optimize operationalization costs.
  prefs: []
  type: TYPE_NORMAL
- en: Compute resources and scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vertex AI allocates compute nodes to handle online and batch predictions. When
    we deploy our ML model to an endpoint, we can customize the type of virtual machines
    to be used for serving the model. We can choose accelerators such as GPUs or TPUs
    if needed. A machine configuration with more computing resources can serve predictions
    with lower latency, hence handling more prediction requests at the same time.
    But such a machine will cost more than a machine with low compute resources. Thus,
    it is important to choose the best-suited machine depending on the use case and
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: When we deploy a model for online predictions, we can also configure a prediction
    node to automatically scale. But the prediction nodes for batch prediction do
    not automatically scale. By default, if we deploy a model with or without dedicated
    GPU resources, Vertex AI will automatically scale the number of replicas up or
    down so that CPU or GPU usage (whichever is higher) matches the default 60% target
    value. Given these conditions, Vertex AI will scale up, even if this may not have
    been needed to achieve **queries per second** (**QPS**) and latency targets. We
    can monitor the endpoint to track metrics such as CPU and accelerator usage, the
    number of requests, and latency, as well as the current and target number of replicas.
  prefs: []
  type: TYPE_NORMAL
- en: To determine the ideal machine type for a prediction container from a cost perspective,
    we can deploy it to a virtual machine instance and benchmark the instance by making
    prediction requests until the virtual machine hits about 90% of the CPU usage.
    By doing this experiment a few times on different machines, we can identify the
    cost of the prediction service based on the QPS values.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about two popular ML workflow orchestration
    tools – Vertex AI Pipelines (managed Kubeflow) and Cloud Composer (managed Airflow).
    We have also implemented a Vertex Pipeline for an example use case, and similarly,
    we have also developed and executed an example DAG with Cloud Composer. Both Vertex
    AI Pipelines and Cloud Composer are managed services on GCP and make it really
    easy to set up and launch complex ML and data-related workflows. Finally, we have
    learned about getting online and batch predictions on Vertex AI for our custom
    models, including some best practices related to model deployments.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you should have a good understanding of different
    ways of carrying out ML workflow orchestration on GCP and their similarities and
    differences. Now, you should be able to write your own ML workflows and orchestrate
    them on GCP via either Vertex AI Pipelines or Cloud Composer. Finally, you should
    also be confident in getting online and batch predictions using Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good understanding of deploying ML models on GCP, and also
    orchestrating ML workflows, we can start developing production-grade pipelines
    for different use cases. Along similar lines, we will learn about some ML governance
    best practices and tools in the upcoming chapter.
  prefs: []
  type: TYPE_NORMAL
