- en: Deep Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we discussed neural networks and their basic operation.
    Specifically, we discussed the fully connected feedforward neural network, which
    is just one simple topology out of many possible ANN topologies. In this chapter,
    we''re going to focus on two advanced topologies: the **Convolutional Neural Network**
    (**CNN**) and one form of **recurrent neural network** (**RNN**), called the **Long
    Short-Term Memory** (**LSTM**) network. CNNs are used most often for image processing
    tasks, such as object detection and image classification. LSTM networks are often
    used in NLP or language-modeling problems.'
  prefs: []
  type: TYPE_NORMAL
- en: These exotic ANN topologies are considered to be **deep neural networks** (**DNNs**).
    While the term is not well-defined, DNNs are typically understood to be ANNs with
    multiple hidden layers between the input and output layers. Convolutional network
    architectures can become quite deep, with ten or more layers in the network. Recurrent
    architectures can be deep as well, however, much of their depth comes from the
    fact that information can flow either forward or backward through the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to take a look at TensorFlow''s capabilities
    in terms of CNN and RNN architectures. We will discuss TensorFlow''s own examples
    of these topologies and take a look at how they are used in practice. In particular,
    we will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gated recurrent unit networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNN-LSTM networks for advanced applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s get started by taking a look at a classic **machine learning** (**ML**)
    problem: identifying handwritten digits from images.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make the case for CNNs, let's first imagine how we might approach an image
    classification task using a standard feedforward, fully connected ANN. We start
    with an image that's 600 x 600 pixels in size with three color channels. There
    are 1,080,000 pieces of information encoded in such an image (600 x 600 x 3),
    and therefore our input layer would require 1,080,000 neurons. If the next layer
    in the network contains 1,000 neurons, we'd need to maintain one billion weights
    between the first two layers alone. Clearly, the problem is already becoming untenable.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming the ANN in this example can be trained, we'd also run into problems
    with scale and position invariance. If your task is to identify whether or not
    an image contains street signs, the network may have difficulty understanding
    that street signs can be located in any position in the image. The network may
    also have issues with color; if most street signs are green, it may have difficulty
    identifying a blue sign. Such a network would require many training examples to
    get around issues of scale, color, and position variance.
  prefs: []
  type: TYPE_NORMAL
- en: In the past, before CNNs became popular, many researchers viewed this problem
    as a dimensionality reduction problem. One common tactic was to convert all images
    to grayscale, reducing the amount of data by a factor of three. Another tactic
    is to downscale images to something more manageable, such as 100 x 100 pixels,
    or even smaller, depending on the type of processing required. Converting our
    600 x 600 image to grayscale and to 100 x 100 would reduce the number of input
    neurons by a factor of 100, from one million to 10,000, and further reduce the
    number of weights between the input layer and a 1,000-neuron hidden layer down
    from 1 billion to only 10 million.
  prefs: []
  type: TYPE_NORMAL
- en: Even after employing these dimensionality reduction techniques, we would still
    require a very large network with tens of millions of weights. Converting images
    to grayscale before processing avoids issues with color detection, but still does
    not solve scale and position variance problems. We are also still solving a very
    complex problem, since shadows, gradients, and the overall variance of images
    would require us to use a very large training set.
  prefs: []
  type: TYPE_NORMAL
- en: Another common preprocessing tactic employed was to perform various operations
    on images, such as noise reduction, edge detection, and smoothing. By reducing
    shadows and emphasizing edges, the ANN gets clearer signals to learn from. The
    problem with this approach is that preprocessing tasks are typically unintelligent;
    the same edge detection algorithm gets applied to every image in the set, whether
    or not that specific edge detection algorithm is actually effective on a particular
    image.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge, then, is to incorporate the image-preprocessing tasks directly
    in the ANN. If the ANN itself manages the preprocessing tasks, the network can
    learn the best and most efficient ways to preprocess the images in order to optimize
    the network's accuracy. Recall from [Chapter 8](69151615-d71a-4e8f-86c5-90801ffa5393.xhtml), *Artificial
    Neural Network Algorithms* that we can use *any *activation function in a neuron,
    as long as we can differentiate the activation function and employ its gradient
    in the backpropagation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In short, a CNN is an ANN with multiple—perhaps many—preprocessing layers that
    perform transformations on the image before ultimately reaching a final fully
    connected layer or two that performs the actual classification. By incorporating
    the preprocessing tasks into the network, the backpropagation algorithm can tune
    the preprocessing tasks as part of the network training. The network will not
    only learn how to classify images, it will also learn how to preprocess the images
    for your task.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional networks contain several distinct layer types in addition to the
    standard ANN layer types. Both types of network contain an input layer, an output
    layer, and one or more fully connected layers. A CNN, however, also incorporates
    convolution layers, ReLU layers, and pooling layers. Let's take a look at each
    in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions and convolution layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutions are a mathematical tool that combine two functions into a new function;
    specifically, the new function represents the area under the curve created by
    the pointwise multiplication of one function as another function is swept over
    it. If this is difficult to visualize, don't worry; it's easiest to visualize
    as an animation, which unfortunately we can't print in a book. The mathematical
    details of convolutions will not be important in this chapter, but I do encourage
    you to do some additional reading on the topic.
  prefs: []
  type: TYPE_NORMAL
- en: Most image filters—such as blur, sharpen, edge detect, and emboss—can be accomplished
    with convolution operations. In an image context, convolutions are represented
    by a *convolution matrix, *which is typically a small matrix (3 x 3, 5 x 5, or
    something similar). The convolution matrix is much smaller than the image to be
    processed, and the convolution matrix is swept across the image so the output
    of the convolution applied to the entire image builds a new image with the effect
    applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following image of Van Gogh''s *Water Lilies*. Here is the original:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5059b31-b768-4646-8435-813766e67f9e.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'I can use my image editor''s *convolution matrix *filter to create a sharpening
    effect. This has the same effect as the image editor''s *sharpen *filter, except
    that I''m writing the convolution matrix manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ed62c7b-5cdd-480a-be07-16c049351766.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The result is a sharpened version of the original image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b3c3dff-d96b-4f66-b78f-0fe9bf40be49.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'I can also write a convolution matrix that blurs the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/315d900d-e70a-4f56-ae1d-eefed0cd51ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It results in the following image. The effect is subtle, as the oil panting
    itself is a little blurry, but the effect is there:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa8ff56a-c6cc-47f3-b4f6-d9d0d2f89376.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Convolutions can also be used to emboss or detect edges:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb7c1e45-a7da-491f-84d1-9c7d4c000132.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding matrix results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66dd6ff7-6701-4afd-a654-613af40cfb90.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A CNN uses multiple convolving layers, each with multiple convolution filters,
    to build a model of the image. The convolving layers and the convolution filters
    themselves are trained by the backpropagation algorithm, and the network will
    eventually discover the correct filters to use in order to enhance the features
    that the network is trying to identify. As with all learning problems, the types
    of filters the CNN develops may not necessarily be readily understood or interpretable
    by a human, but in many cases, you will find that your network develops a number
    of convolution filters that perform blur, edge detection, color isolation, and
    gradient detection.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to extracting useful features from images, the convolution operations
    in effect provide for spatial and positional independence of features. The convolving
    layers are not fully connected, and therefore are able to inspect specific areas
    of the image. This reduces the dimensionality required of the weights in between
    layers and also helps us avoid reliance on the spatial positioning of features.
  prefs: []
  type: TYPE_NORMAL
- en: There is still a lot of data involved in these operations, so convolving layers
    are typically immediately followed by pooling layers, which essentially downsample
    an image. Most often you will employ something such as *2 x 2 max pooling*, which
    means that for every 2 x 2 area of pixels in the source feature, the pooling layer
    will downsample the 2 x 2 area to a single pixel that has the value of the maximum
    pixel in the source 2 x 2 area. A 2 x 2 pooling layer therefore reduces the image
    size by a factor of four; because the convolution operation (which may also reduce
    dimensionality) has already occurred, this downsampling will typically reduce
    the computation required without the loss of too much information.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, a CNN will employ simple ReLU activation functions immediately
    following the convolution operations and immediately preceding pooling; these
    ReLU functions help avoid oversaturation of the image or the feature maps that
    result from the convolution operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical architecture for a simple CNN would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer, with width x height x color depth neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolving layer, with N convolution filters of an M x M size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max pooling layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second convolving layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second max pooling layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully connected output layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More complex architectures for CNNs typically include several more groups of
    convolving and pooling layers, and may also involve two convolving layers in a
    row before reaching a pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: Each successive convolving layer in the network operates at a higher level than
    the convolving layers before it. The first convolving layer will only be able
    to perform simple convolutions, such as edge detection, smoothing, and blurring.
    The next convolving layer, however, is able to combine the results from previous
    convolutions into higher level features, such as basic shapes or color patterns.
    A third convolving layer can further combine information from previous layers
    to detect complex features, such as wheels, street signs, and handbags. The final
    fully connected layer, or layers, acts much like a standard feedforward ANN, and
    performs the actual classification of the image based on the high-level features
    that the convolving layers have isolated.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now attempt to employ this technique in practice using `TensorFlow.js`
    on the MNIST handwritten digit dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Example – MNIST handwritten digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rather than building an example from first principles, let's instead walk through
    an excellent `TensorFlow.js` MNIST example. The goal of this example is to train
    a CNN to classify images of handwritten digits. More specifically, the goal of
    this example is to achieve a high accuracy in classifications made against the
    MNIST handwritten digit dataset. In this section, we will aim to get an understanding
    of the code and the algorithm by performing experiments on the code and observing
    their results.
  prefs: []
  type: TYPE_NORMAL
- en: The current version of this example may be found on `TensorFlow.js`'s GitHub: [https://github.com/tensorflow/tfjs-examples/tree/master/mnist](https://github.com/tensorflow/tfjs-examples/tree/master/mnist).
    However, as the repository may be updated after this writing, I have also added
    the version that I am using as a Git submodule in this book's example repository.
    If you are using this book's repository and haven't already done so, please run `git
    submodule init`; `git submodule update` from the command line in the repository
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: In the terminal, navigate to `Ch5-CNN`. This path is a symbolic link, so if
    it doesn't work on your system, you may alternately navigate to `tfjs-examples/mnist`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, issue `yarn` from the command line to build the code, and finally issue `yarn
    watch`, which will start a local server and launch your browser to `http://localhost:1234`.
    If you have any other programs using that port, you will have to terminate them
    first.
  prefs: []
  type: TYPE_NORMAL
- en: The page will start by downloading MNIST images from Google's servers. It will
    then train a CNN for 150 epochs, periodically updating two graphs that show the
    loss and the accuracy. Recall that the loss is typically a metric, such as **mean
    square error** (**MSE**), while accuracy is the percentage of correct predictions.
    Finally, the page will display a few example predictions, highlighting correct
    versus incorrect predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'My test run of this page yielded a CNN with an accuracy of around 92%:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8f0de54-12f5-4011-8352-151ec75d87d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Often, the incorrect predictions are understandable. In this example, the digit
    1 does seem to be shaped a bit like a 2\. It is unlikely a human would have made
    this particular error, though I have encountered examples where I would have gotten
    the prediction wrong as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/496b160d-dafe-439a-8353-aa708bcb7765.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Opening `index.js`, we can see the topology of the network toward the top of
    the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This network has two convolving layers with a pooling layer after each, and
    then a single fully connected layer that makes a prediction. Both the convolving
    layers use a `kernelSize` of `5`, which means that the convolution filter is a
    5 x 5 matrix. The first convolving layer uses eight filters, while the second
    uses 16\. This means that the first layer will create and use eight different
    convolution filters, therefore identifying eight separate graphical features of
    the image. These features may be abstract, but in the first layer it is common
    to see features that represent edge detection, blurring or sharpening, or gradient
    identification.
  prefs: []
  type: TYPE_NORMAL
- en: The second convolving layer uses 16 features, which likely will be of a higher
    level than the first layer's features. This layer may try to identify straight
    lines, circles, curves, swoops, and so on. There are more high-level features
    than there are low-level features, so it makes sense that the first layer uses
    fewer filters than the second layer.
  prefs: []
  type: TYPE_NORMAL
- en: The final dense layer is a fully connected layer of 10 neurons, each representing
    a digit. The softmax activation function ensures that the output is normalized
    to 1\. The input to this final layer is a flattened version of the second pooling
    layer. The data needs flattening because convolving and pooling layers are typically
    multidimensional. Convolving and pooling layers use matrices representing height,
    width, and color depth, which themselves are in turn stacked atop one another
    as the result of the convolution filters used. The output of the first convolving
    layer, for example, will be a volume that is [28 x 28 x 1] x 8 in size. The bracketed
    portion is the result of a single convolution operation (that is, a filtered image),
    and eight of them have been generated. When connecting this data to a vector layer,
    such as the standard dense or fully connected layer, it must also be flattened
    into a vector.
  prefs: []
  type: TYPE_NORMAL
- en: The data entering the final dense layer is much smaller than the data coming
    out of the first layer. The max-pooling layers serve to downscale the image. The `poolSize `parameter
    of `[2, 2]` means that a 2 x 2 window of pixels will be reduced to a single value;
    since we are using max-pooling, this will be the largest value (the lightest pixel)
    in the set. The `strides` parameter means that the pooling window will move in
    steps of two pixels at a time. This pooling will reduce both the height and width
    of the image by half, meaning that the image and the data is reduced in area by
    a factor of four. After the first pooling operation, images are reduced to 14
    x 14, and after the second they are 7 x 7\. Because there are 16 filters in the
    second convolving layer, this means that the flattened layer will have *7 * 7
    * 16 = 784* neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see whether we can squeeze some more accuracy out of this model by adding
    another fully connected layer before the output. In the best case scenario, adding
    another layer will give us an improved ability to interpret the interplay of the
    16 features that the convolutions generate.
  prefs: []
  type: TYPE_NORMAL
- en: However, adding another layer will increase the required training time, and
    it also may not improve results. It's perfectly possible that there is no more
    information to be discovered by adding another layer. Always remember that ANNs
    simply build and navigate a mathematical landscape, looking for shapes in the
    data. If the data isn't highly dimensional, adding another dimension to our capabilities
    may simply be unnecessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following line, before the final dense layer in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In context, the code should now look like this, with the new line highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Since you have issued `yarn watch` from the command line, the code should automatically
    rebuild. Refresh the page and observe the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd60951b-94b1-4cfc-bc02-298925b711f3.png)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm is learning at a slower rate than the original version, which
    is expected because we have added a new layer and therefore more complexity to
    the model. Let's increase the training limit a little bit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the `TRAIN_BATCHES` variable and update it to `300`. The line should now
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the file to trigger the rebuild and reload the page. Let''s see whether
    we can beat the baseline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b9bdac3-9b99-4b7f-b002-059567d6e6d8.png)'
  prefs: []
  type: TYPE_IMG
- en: It does seem that we have indeed beaten the baseline score of 92%, however I
    would caution against too much optimism. It is possible we have overtrained and
    overfit the model, and there is a chance it will not perform so well in real life.
    Additionally, because training and validation are stochastic, it is possible that
    the true accuracy of this network is comparable to the baseline's. Indeed, 92%
    is already an excellent result and I would not expect much better from any model.
    However this is still an encouraging result, as the new layer was not too much
    of a burden to add.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, revert your changes so that you are working with the original
    copy of the file. Let's run a different experiment. It would be interesting to
    see how small we can make the network without losing too much accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s reduce the number of convolution filters the second convolving
    layer uses. My reasoning is that numerals use pretty simple shapes: circles, lines,
    and curves. Perhaps we don''t need to capture 16 different features. Maybe eight
    will do. In the second convolving layer, change `filters: 8` to `filters: 2`.
    Your code should now read:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Rerunning the code, we see that we still get decent accuracy, though the variance
    is a little higher than the baseline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9c3f174-2ebb-4800-9601-360c3e9e80c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This supports the overall idea that the shapes and features used are relatively
    few. However, when we look at the test examples, we also find that the mistakes
    are less *understandable* than before. Perhaps we have not lost much accuracy,
    but our model has become more abstract:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a085a4ec-111b-42ce-983d-fa02d3c053f0.png)'
  prefs: []
  type: TYPE_IMG
- en: I encourage you to continue exploring and experimenting with this example, as
    there are many things you can learn by reading the code. One aspect of this example
    I would like to point out in particular is the `data.js` file, which manages the
    handling of the MNIST dataset. In your real-world applications, you will likely
    need to employ an approach similar to this, as your training data will not always
    be on the local machine. This file handles downloading data from a remote source,
    splitting it into testing and validation sets, and maintaining batches to be requested
    by the training algorithm. It is a good, lightweight approach to follow if you
    require training data from a remote source. We will discuss this topic in-depth
    in [Chapter 11](8bb0fe0d-84df-41b9-a955-69a84eb2d8ea.xhtml), *Using Machine Learning
    in Real-Time Applications*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some ideas for experiments you can try:'
  prefs: []
  type: TYPE_NORMAL
- en: Make the network as small as possible while maintaining 90%+ accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make the network as small as possible while maintaining 85%+ accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the model to 90%+ accuracy in fewer than 50 epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discover the fewest number of training examples required to achieve 90%+ accuracy
    (reduce the value of `NUM_TRAIN_ELEMENTS `in `data.js `to use fewer training examples)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will explore series prediction with recurrent neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many cases where memory is required of neural networks. For instance,
    when modeling natural language context is important, that is, the meaning of a
    word late in a sentence is affected by the meaning of words earlier in the sentence.
    Compare this to the approach used by Naive Bayes classifiers, where only the bag
    of words is considered but not their order. Similarly, time series data may require
    some memory in order to make accurate predictions, as a future value may be related
    to current or past values.
  prefs: []
  type: TYPE_NORMAL
- en: RNN are a family of ANN topologies in which the information does not necessarily
    flow in only one direction. In contrast to feedforward neural networks, RNNs allow
    the output of neurons to be fed backward into their input, creating a feedback
    loop. Recurrent networks are almost always time-dependent. The concept of time
    is flexible, however; ordered words in a sentence can be considered time-dependent,
    as one word must follow another. It is not necessary for the time-dependence of
    RNNs to be related to the actual passage of time on a clock.
  prefs: []
  type: TYPE_NORMAL
- en: In the simplest case, all that is required of an RNN is for the output value
    of a neuron to be connected - typically with a weight or decay factor—not just
    to neurons in the next layer, but also back to its own input. If you are familiar
    with **finite impulse response** (**FIR**) filters in digital signal processing,
    this style of neuron can be considered a variant of an FIR filter. This type of
    feedback results in a sort of memory, as the previous activation value is partially
    preserved and used as an input to the neuron's next cycle. You can visualize this
    as an echo created by the neuron, becoming more and more faint until the echo
    is no longer audible. Networks designed in this manner will therefore have a finite
    memory, as ultimately the echo will fade away to nothing.
  prefs: []
  type: TYPE_NORMAL
- en: Another style of RNN is fully recurrent RNNs, in which every neuron is connected
    to every other neuron, whether in the forward or backward direction. In this case,
    it is not just a single neuron that can hear its own echo; every neuron can hear
    the echoes of every other neuron in the network.
  prefs: []
  type: TYPE_NORMAL
- en: While these types of networks are powerful, in many cases a network will need
    memory that persists longer than an echo will last. A very powerful, exotic topology,
    called **LSTM**, was invented to solve the problem of long-term memory. The LSTM
    topology uses an exotic form of neuron called an LSTM unit, which is capable of
    storing all previous input and activation values and recalling them when calculating
    future activation values. When the LSTM network was first introduced, it broke
    an impressive number of records, particularly in speech recognition, language
    modeling, and video processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we will briefly discuss three different types of RNN topologies
    provided by TensorFlow.js: the SimpleRNN (or fully recurrent RNN), the **gated
    recurrent unit** (**GRU**) network, and the LSTM network.'
  prefs: []
  type: TYPE_NORMAL
- en: SimpleRNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first RNN layer provided out-of-the-box by `TensorFlow.js` is the SimpleRNN
    layer type, which is a layer composed of a SimpleRNNCell neuron. This is an exotic
    neuron that can feed its output back to its input. The input to such a neuron
    is a vector of time-dependent values; the activation output of each input value
    is fed back into the input of the next value, and so on. A *dropout *factor between
    0 and 1 may be specified; this value represents the strength of each echo. A neuron
    designed in this manner is similar in many ways to an FIR filter.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, this type of RNN architecture is made possible by earlier work in digital
    signal processing concerning FIR filters. The advantage of this architecture is
    that the mathematics are well-understood. It is possible to *unroll* an RNN, meaning
    that it is possible to create a feedforward ANN of many layers that generates
    the same results as an RNN with fewer layers. This is because the echoes of the
    neurons' feedback are finite. If a neuron is known to echo 20 times, then that
    neuron can be modeled as 21 feedforward neurons (including the source neuron).
    Initial efforts in training these networks were inspired by work on FIR filters,
    as the analysis is much the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following image, created by François Deloche (own work, CC BY-SA
    4.0), which illustrates the unrolling of a recurrent neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89d834d6-7df8-407e-9ec9-cf2a41d56ce5.png)'
  prefs: []
  type: TYPE_IMG
- en: The loop labeled **V** represents the feedback operation of the neuron. As future
    input values (**X**) are given to the neuron, the output from the previous activation
    reaches the input and becomes an input factor. As the graphic illustrates, this
    can be modeled as a linear series of simple neurons.
  prefs: []
  type: TYPE_NORMAL
- en: From TensorFlow's perspective, the operation of recurrent layers are abstracted
    away by the TensorFlow layers API. Let's look at another of TensorFlow.js's examples
    that illustrates the interchangeability of various RNN architectures.
  prefs: []
  type: TYPE_NORMAL
- en: From this book's GitHub repository, navigate to the `Ch9-RNN` directory, which
    is once again a symbolic link to the `tfjs-examples/addition-rnn` directory. (If
    you still have the previous RNN example running, you will need to stop it by pressing
    *Ctrl *+ *C* in the terminal that is running the yarn watch command.) First, issue
    the `yarn` command to build the code, then run `yarn watch` to once again start
    a local server and navigate to `http://localhost:1234`.
  prefs: []
  type: TYPE_NORMAL
- en: This particular example is meant to teach an RNN integer addition by example.
    The training data will be a list of questions, such as `24 + 22` or `14 + 54`,
    represented in string form, and the network will need to be able to decode the
    string, represent it numerically, learn the answers, and be able to extend the
    knowledge to new examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the page loads, you''ll see the following form. Keep the defaults and
    click the **Train Model **button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df4355c3-4336-476c-beed-99d8bf83e2cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You''ll see loss and accuracy graphs similar to the following, which show that
    after 100 epochs of training, the accuracy for this model was 93.8%:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7970408c-f5d0-498f-80be-4a276443ff7d.png)'
  prefs: []
  type: TYPE_IMG
- en: The loss and similarity graph
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll also see test results from a random test input that the model validates
    against:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00c97fbd-a14a-4506-b313-66a262fc8e87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a closer look at how this is working under the hood. Open the `index.js `file
    and find the `createAndCompileModel `function. I will assume that you selected
    the SimpleRNN network type for this example, and omit the switch/case statements
    that handle GRU and LSTM topologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This code builds a model with two recurrent layers, a time-distributed, fully
    connected layer and an output layer. The `vocabularySize `parameter represents
    the total number of unique characters involved, which are the numerals 0-9, the
    plus sign, and the space character. The `maxLen` parameter represents the maximum
    length an input string could be; for two-digit addition problems, `maxLen `will
    be five characters, because the plus sign must be included.
  prefs: []
  type: TYPE_NORMAL
- en: Of particular note in this example is the `timeDistributed` layer type. This
    is a layer wrapper in TensorFlow's API, meant to create a volume of neurons in
    the layer where each slice represents one slice of time. This is similar in spirit
    to the volumes used by CNNs in the previous example, where the depth of the volume
    represented an individual convolution operation. In this example, however, the
    depth of the volume represents a time slice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `timeDistributed `wrapper allows each time slice to be handled by an individual
    dense or fully connected layer, rather than attempting to interpret the time-dependent
    data with only a single vector of neurons, in which case the temporal data may
    be lost. The `timeDistributed `wrapper is required because the previous *simpleRNN*
    layer uses the `returnSequences: true` parameter, which causes the layer to output
    not only the current time step, but all time steps encountered in the layer''s
    history.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's take a look at the GRU topology.
  prefs: []
  type: TYPE_NORMAL
- en: Gated recurrent units
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The GRU topology comprises specialized, exotic neurons that use several internal
    mechanisms to control the memory and feedback of the neuron. The GRU is a recent
    invention, being developed only in 2014 as a simplified version of the LSTM neuron.
    While the GRU is newer than the LSTM, I present it first as it is slightly simpler.
  prefs: []
  type: TYPE_NORMAL
- en: In both GRU and LSTM neurons, the input signal is sent to multiple activation
    functions. Each internal activation function can be considered a standard ANN
    neuron; these internal neurons are combined in order to give the overall neuron
    its memory capabilities. From the outside, GRU and LSTM neurons both look like
    neurons capable of receiving time-dependent inputs. On the inside, these exotic
    neurons use simpler neurons to control how much of the feedback from a previous
    activation is attenuated or amplified, as well as how much of the current signal
    is stored into memory.
  prefs: []
  type: TYPE_NORMAL
- en: GRU and LSTM neurons have two major advantages over simple RNN neurons. First,
    the memories of these neurons do not decay over time like the echoes of a simple
    RNN neuron do. Second, the memory is configurable and self-learning, in the sense
    that the neuron can learn through training how important specific memories are
    to the current activation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following illustration, also by François Deloche (own work, CC
    BY-SA 4.0):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/733ab961-704c-4714-993f-eed6d557c398.png)'
  prefs: []
  type: TYPE_IMG
- en: The flowchart may be a little difficult to interpret at first. The **Z[t]**
    signal is a vector that controls how much of the activation gets stored into memory
    and passed to future values, while the **R[t]** signal controls how much of the
    prior values should be forgotten from memory. Each of these signals are attached
    to standard activation functions, which in turn have their own weights. In a sense,
    the GRU is itself a tiny neural network.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it might be tempting to ask why the memory of neurons can't simply
    be programmed, for example, with a key/value store that the neuron can make lookups
    against. The reason these architectures are used is due to the fact that the backpropagation
    algorithm requires mathematical differentiability. Even exotic topologies like
    RNNs are still trained using mathematical methods such as gradient descent, so
    the entire system must be mathematically representable. For this reason, researchers
    need to use the preceding techniques in order to create a network whose every
    component is mathematically analyzable and differentiable.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the test page at `http://localhost:1234`, change the *RNN Type *parameter
    to GRU while keeping all other parameters the same, and click **Train Model **again.
    The graphs will update and you should see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d85622d8-f6d0-4491-9cf4-a6cc44349762.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the training process has taken longer, but the accuracy has improved
    from 92% to 95% over the SimpleRNN type. The increased training time is not surprising,
    as the GRU architecture essentially triples the number of activation functions
    employed by the network.
  prefs: []
  type: TYPE_NORMAL
- en: While many factors affect the accuracy of the network, two obvious ones stand
    out. First, the GRU topology has long-term memory, unlike the SimpleRNN that will
    eventually forget previous values as their echoes decay. Second, the GRU has more
    precise control over how much of an activation signal is fed into future activations
    as well as how much of the information is retained. These parameters of the network
    are trained by the backpropagation algorithm, so the forgetfulness of neurons
    itself is optimized by the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s take a look at the topology that inspired the GRU and opened up
    entire new fields of research: the LSTM.'
  prefs: []
  type: TYPE_NORMAL
- en: Long Short-Term Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LSTM was introduced in 1997 and made waves throughout the academic ANN community
    due to its impressive accuracy at solving historically difficult problems. In
    particular, the LSTM excelled at many natural language-processing tasks, handwriting
    recognition, and speech recognition. In many cases, LSTM networks beat the previous
    accuracy records by a wide margin. Many systems at the forefront of speech recognition
    and language modeling use LSTM networks. Most likely, systems such as Apple's
    Siri and Google's Assistant, use LSTM networks both in their speech recognition
    and language- parsing models.
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM network gets its name due to the fact that it can retain short-term
    memory (for example, memory of a word used earlier in a sentence) for long periods
    of time. When training, this avoids a problem known as the **disappearing gradient**,
    which is what simple RNNs suffer from as the echoes of previous activations fade
    away.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the GRU, an LSTM neuron is an exotic neuron cell with sophisticated inner
    workings. Specifically, the LSTM neuron has three *gates* it uses internally:
    an *input gate*, which controls how much of a value is allowed to enter the neuron,
    a *forget gate*, which manages the memory of the neuron, and an *output gate*,
    which controls how much of a signal is allowed in the output of the neuron. The
    combination of gates, along with the fact that the neurons are all connected to
    each other, gives the LSTM very fine-grained control over which signals a neuron
    remembers and how they are used. Like the gates in a GRU, the gates in an LSTM
    can also be thought of as individual standard neurons, each with their own weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following graphic by François Deloche (own work, CC BY-SA 4.0):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2dd99f3-47e2-47ce-bbcc-b84d6de2b23a.png)'
  prefs: []
  type: TYPE_IMG
- en: The **I[t ]**signal controls the proportion of the input signal that is allowed
    into the cell. The **O[t]** signal controls how much of the output is allowed
    out of the cell, and the **F[t ]**signal controls how much of the previous value
    is retained by the cell. Keep in mind that these are all vector quantities, so
    that the input, output, and memory can be controlled on a per-element basis.
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM excels at tasks that require memory and knowledge of prior values,
    though the sophisticated inner workings of the cell (there are five distinct activation
    functions involved) lead to much longer training times. Returning to the test
    page in your browser, switch the **RNN Type **to LSTM and click **Train Model:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2791cf8b-a8b2-4d58-a58b-50e8cb9fa041.png)'
  prefs: []
  type: TYPE_IMG
- en: The LSTM has achieved an accuracy of nearly 98%, exceeding both the SimpleRNN
    and GRU RNN topologies. Of course, this network took longer to train than either
    of the others, due to the simple fact that there are more neurons (internal to
    the LSTM cells) that need to be trained.
  prefs: []
  type: TYPE_NORMAL
- en: There are many state-of-the-art uses for LSTM networks. They are very popular
    in audio analysis, such as speech recognition, as audio is heavily time-dependent.
    A single audio sample on its own is meaningless; it is only when many thousands
    of audio samples are taken together in context that an audio clip starts to make
    sense. An LSTM trained to recognize speech would first be trained to decode short
    audio clips (on the order of 0.1-0.25 seconds) into *phonemes*, or textual representations
    of phonetic sounds. Another LSTM layer would then be trained to connect sequences
    of phonemes together in order to determine the most likely phrase that was uttered.
    The first LSTM layer relies on time dependence in order to interpret the raw audio
    signal. The second LSTM layer relies on time dependence to bring context to natural
    language—for instance, using context and grammar to figure out whether the word
    *where* or *we're* was spoken.
  prefs: []
  type: TYPE_NORMAL
- en: Another state-of-the-art use case for LSTM is the CNN-LSTM. This network topology
    combines a CNN with an LSTM; a typical application would be action detection in
    a video clip. The CNN portion of the model analyzes individual video frames (as
    if they were independent images) to identify an object and its position or state.
    The LSTM portion of the model brings the individual frames together and generates
    a time-dependent context around them. Without an LSTM portion, a model would not
    be able to tell whether a baseball is stationary or in motion, for instance. It
    is the memory of the previous states of the object detected by the CNN that provides
    the context for determining the action occurring in the video. The CNN portion
    of the model identifies a baseball, and then the LSTM portion is what understands
    that the ball is moving and likely has been thrown or hit.
  prefs: []
  type: TYPE_NORMAL
- en: Another variation of the CNN-LSTM is used for the automated description of images.
    One can present a CNN-LSTM with an image of a woman standing on a pier by a lake.
    The CNN portion of the model individually identifies the woman, the pier, and
    the lake as objects in the image. The LSTM portion can then generate a natural
    language description of the image based on the information gathered by the CNN;
    it is the LSTM portion that grammatically compiles the description, *woman on
    a pier by a lake*. Remember that natural language descriptions are time-dependent,
    as the order of words matters.
  prefs: []
  type: TYPE_NORMAL
- en: One final note about LSTM networks relates to the *gates* used in the LSTM cell.
    While the input, forget, and output gates are usually standard activation neurons,
    it is also possible to use entire neural networks as gates themselves. In this
    manner, an LSTM can use *other* models as part of their knowledge and memory.
    A typical use case for this approach would be automated language translation.
    Individual LSTMs can be used to model the English and French languages, for instance,
    while an overall LSTM can manage translations between the two.
  prefs: []
  type: TYPE_NORMAL
- en: It is my personal belief that LSTM networks, or some variation thereof, such
    as the GRU topology, will be a key player in the road toward AGI. Having a robust
    memory is essentially a requirement when attempting to emulate general human intelligence,
    and LSTM fits the use case very nicely. These network topologies are at the forefront
    of ANN research, so expect to see major advances over the next couple of years.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed two advanced neural network topologies: the CNN
    and the RNN. We discussed the CNN in the context of image recognition, specifically
    the problem of handwritten digit identification. While exploring the CNN, we also
    discussed the convolution operation itself in the context of image filtering.'
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed how neural networks can be made to retain memory through the
    RNN architecture. We learned that RNNs have many applications, ranging from time-series
    analysis to natural language modeling. We discussed several RNN architecture types,
    such as the simple fully recurrent network and the GRU network. Finally, we discussed
    the state-of-the-art LSTM topology, and how it can be used for language modeling
    and other advanced problems, such as image captioning or video annotation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll take a look at some practical approaches to natural
    language processing, particularly the techniques that are most commonly used in
    conjunction with ML algorithms.
  prefs: []
  type: TYPE_NORMAL
