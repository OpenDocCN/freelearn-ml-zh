- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding the Most Common Machine Learning Attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When getting started with securing your projects, there are many things you
    can use to learn security techniques quickly. The best is the **MITRE ATT&CK framework**.
    As a globally recognized knowledge base, it contains valuable information about
    a range of attack techniques that an adversary can use to attack a system and
    their mitigations. In this chapter, we are going to explore the **MITRE ATLAS**
    framework. It is adapted from the MITRE ATT&CK framework for **machine** **learning**
    (**ML**).
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this chapter is to familiarize ourselves with the different stages
    of an attack and possible attacks on our system. This is essential because, with
    that knowledge, we can understand how an adversary thinks and how to protect our
    system. As there are multiple stages of an attack, you will understand why applying
    the Zero Trust strategy (covered in the previous chapter) is the most effective
    way to protect the system. We must never forget that this is an ongoing process
    as new vulnerabilities and exploits are released daily.
  prefs: []
  type: TYPE_NORMAL
- en: We must always keep up to date with all new information, and the MITRE ATLAS
    framework will help us do exactly that. Finally, after exploring the MITRE ATLAS
    Matrix, we will cover the Azure services related to Azure Machine Learning and
    those most commonly affected by attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the MITRE ATLAS Matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding ML and AI attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Azure services involved in ML attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a better understanding of ML attacks
    and their possible mitigations for ML.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the MITRE ATLAS Matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MITRE ATT&CK framework is a globally recognized knowledge base and framework.
    Security professionals use it to understand and organize adversary behaviors in
    cyber threat environments. **ATT&CK**® (or ATTACK) stands for **Adversarial Tactics,
    Techniques, and Common Knowledge**. It is essentially a catalog of **tactics,
    techniques, and procedures** (**TTPs**) that adversaries use during different
    stages of a cyberattack. It covers many threat vectors, including initial access,
    execution, persistence, privilege escalation, defense evasion, credential access,
    discovery, lateral movement, collection, exfiltration, and impact.
  prefs: []
  type: TYPE_NORMAL
- en: The MITRE ATT&CK framework organizes these techniques into a matrix that classifies
    them based on the various stages of an attack and the platforms on which they
    are applicable (for example, Windows, macOS, or Linux). Each technique in the
    matrix is described in detail in the MITRE knowledge base, including information
    on how adversaries typically employ it and the potential defensive measures we
    can take to detect and prevent it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If this is the first time you are hearing about the MITRE ATT&CK® framework,
    you can explore the MITRE ATT&CK® knowledge base at [https://attack.mitre.org/](https://attack.mitre.org/).
  prefs: []
  type: TYPE_NORMAL
- en: (© 2023 The MITRE Corporation. This work is reproduced and distributed with
    the permission of The MITRE Corporation.)
  prefs: []
  type: TYPE_NORMAL
- en: The framework has become a widely adopted industry standard. It is used by security
    teams, security solutions vendors, and organizations to enhance their threat intelligence,
    develop more effective security controls, and improve incident response capabilities.
    It enables organizations to align their defenses with real-world adversary behaviors,
    helping them proactively detect, respond to, and mitigate cyber threats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although very comprehensive, the MITRE ATT&CK framework might not cover all
    possible known attack methods, but it provides a great starting point. We are
    going to follow the MITRE ATLAS™ framework. **ATLAS** stands for **Adversarial
    Threat Landscape for Artificial-Intelligence Systems**, and it is a knowledge
    base of adversary tactics based on the MITRE ATT&CK framework and contains techniques
    that apply to ML and **artificial intelligence** (**AI**) systems. The ATLAS Matrix
    shows the progression of an attack in stages and the techniques associated with
    each stage. The stages can be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – The MITRE ATLAS stages](img/B21076_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – The MITRE ATLAS stages
  prefs: []
  type: TYPE_NORMAL
- en: While the stages appear in sequence and they usually start from reconnaissance
    and end with impact techniques, not all stages and techniques will be used in
    an attack. It depends on the adversary’s goal and the system architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Let us understand each stage in the sections ahead.
  prefs: []
  type: TYPE_NORMAL
- en: Reconnaissance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reconnaissance** refers to the initial phase of an attack where an adversary
    gathers information about the target ML system. The goal of reconnaissance is
    to gather intelligence that can be used to identify potential vulnerabilities,
    plan the attack, and increase the chances of success. Information can be anything
    from the ML technologies used or research information that can help the adversary
    obtain relevant ML artifacts and tailor attacks to the victim in the next stages
    of the attack.'
  prefs: []
  type: TYPE_NORMAL
- en: Resource development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the initial reconnaissance, the adversary is trying to discover resources
    they can use to support their endgame. This stage is called **resource development**
    and it is usually where the adversary purchases or steals resources to target
    ML artifacts, infrastructure, accounts, or capabilities that can be used later
    in the attack.
  prefs: []
  type: TYPE_NORMAL
- en: Initial access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the **initial access** stage, the adversary attempts to access the ML system.
    That can be anything from networks to devices and platforms. If the adversary
    succeeds in this step, they can get an initial foothold in the system.
  prefs: []
  type: TYPE_NORMAL
- en: ML model access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the adversary gains some form of access to the system, they will try to
    get further by gaining access to the ML model. The techniques used in the **ML
    model access** stage vary as the adversary can try to take advantage of many levels
    of access. They can target the database or technology that houses the data, or
    the endpoints used to train the ML model. The endpoint used for predictions or
    any other product or service that utilizes ML as part of its process is also vulnerable
    to attack.
  prefs: []
  type: TYPE_NORMAL
- en: Execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the **execution** stage, the adversary manages to run or embed malicious
    code or commands on a targeted system to achieve their objective. This tactic
    focuses on the actions taken by an adversary to execute their payloads or explore
    the network to steal more data or gain access to more systems. Remote access tools
    can be run here to run scripts and discover unpatched known vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Persistence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the **persistence** stage, the adversary tries to maintain whatever access
    they have gained in the previous steps. Techniques include but are not limited
    to elevating credentials, cutting off access to other users, and leaving behind
    modified data or models and backdoors so they can regain their access more easily
    if they are discovered.
  prefs: []
  type: TYPE_NORMAL
- en: Defense evasion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of course, no adversary wants to be discovered before accomplishing their goals.
    **Defense evasion** techniques are used by the adversary to avoid detection. Evading
    detection is something that an adversary can accomplish by turning off security
    features or software such as malware detectors.
  prefs: []
  type: TYPE_NORMAL
- en: Discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **discovery** stage is like reconnaissance but from the inside. The adversary
    is trying to work out your ML environment. They are trying to gain knowledge about
    the system and internal network to broaden their goals or get as much information
    as possible before launching an attack. In this stage, the adversary will learn
    what they can or cannot control and what else they need to do based on their objective.
    Here, native operating system tools are often used to collect the information
    needed.
  prefs: []
  type: TYPE_NORMAL
- en: Collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the **collection** stage, all investigation and information-gathering processes
    are finished. The adversary is trying to actively collect data or ML artifacts.
    Suppose their goal is simply to disrupt the service. In that case, the techniques
    in this stage will help them collect everything they need to extract from the
    system before making the service unusable. Extraction is part of the exfiltration
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: ML attack staging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For ML, data extraction or service disruption might not be the only goal of
    the adversary. In AI projects, attacks targeting the ML model can be deployed.
    **ML attack staging** techniques include training proxy models, poisoning the
    target model, and crafting adversarial data to feed the target model. Some of
    them can even be performed offline, so it would be difficult to mitigate in some
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: Exfiltration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **exfiltration** stage would be where data or artifacts will be extracted.
    The adversary is trying to steal (exfiltrate) the ML artifacts or use the information
    for future operations. In this case, the most targeted sources are software repositories,
    container registries, model repositories, and object stores. This is a challenging
    process, as data needs to leave the network, creating traffic that can be detected.
  prefs: []
  type: TYPE_NORMAL
- en: Impact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **impact** stage consists of techniques that disrupt or compromise the integrity
    of the system and possibly manipulate business processes. The adversary can target
    data and corrupt or destroy it. Even worse, data might be slightly changed; not
    enough to trigger suspicion in the system, but just enough to disrupt the services
    in a way that helps the adversary with their endgame or provides cover for a confidentiality
    breach.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, these stages form a logical path an adversary might take to
    attack your system. In reality, that might not be the case as the flow heavily
    depends on their goal. Let us see the techniques used in each stage and examples
    of how they might affect our systems.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding ML and AI attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the stages mentioned in the previous section use multiple techniques to
    achieve each goal. The adversary can use these techniques alone, sequentially,
    or combined. Some attacks can be repeated and used in various stages for different
    purposes. It all depends on the adversary’s goal, which is why by applying Zero
    Trust principles and always verifying all levels of the system, we have a better
    chance of protecting our services or at least detecting an incident before it
    has time to do any extensive damage to the system.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will describe the most common AI and ML attacks per stage. We will
    also talk about attacks from the MITRE ATT&CK framework that, although not ML-specific,
    can be used to access systems that contain ML capabilities, among other things.
    Although we will outline the possible mitigations for each attack, we will go
    through the implementations in more detail in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Let us explore the attack techniques per stage.
  prefs: []
  type: TYPE_NORMAL
- en: Reconnaissance techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are five reconnaissance techniques that aim to gather information about
    the system, as seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Reconnaissance techniques](img/B21076_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Reconnaissance techniques
  prefs: []
  type: TYPE_NORMAL
- en: Let us understand each of these techniques in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Search for the victim’s publicly available information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose the organization uses open source model architectures that they have
    trained on additional proprietary data in production or that are in the research
    stage. In that case, they might publish system details with announcements or press
    releases. Although not technical, these might contain details about their model’s
    development and can help craft more realistic proxy models. To mitigate this,
    limit sharing information about the company’s systems, software stack, or frameworks
    used in developing systems when announcing deals or partnerships.
  prefs: []
  type: TYPE_NORMAL
- en: Search victim-owned websites and application repositories
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Company websites may have a lot of information publicly available, including
    names of departments/divisions, physical locations, and data about key employees
    such as names, roles, and contact information. These sites may also have information
    that shows business operation details and relationships. The same goes for papers
    or technical blogs published by company employees. Employees are also vulnerable
    to social engineering attacks where the adversary poses as another employee to
    get company information. Ensure that you instruct employees not to share information
    about the projects they are working on—even with other employees if they are not
    working on the project—and anonymize the information they share on personal blogs
    or social media.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, ML-enabled applications might be available in mobile stores such as
    Google Play, the iOS App Store, the macOS App Store, and the Microsoft Store.
    The adversary might attempt to scan and analyze the app for ML-related components
    or endpoints. Try to obfuscate the application code where possible and ensure
    you secure endpoints if information gets intercepted.
  prefs: []
  type: TYPE_NORMAL
- en: Search for publicly available adversarial vulnerability analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As soon as the technology is identified, the adversary will research common
    system, model, or algorithm vulnerabilities to see whether they can use existing
    research to stage their attack. Identified vulnerabilities have implementations
    publicly available, making it easier for the adversary to gain initial access
    to the system and plan effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Active scanning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Active scanning is not a simple reconnaissance or information gathering. The
    adversary is actively probing the system to identify entry points or gather more
    actionable information. They can also be trying to determine whether the collected
    information is valid.
  prefs: []
  type: TYPE_NORMAL
- en: Resource development techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are six techniques in this stage that usually exploit information gathered
    from the reconnaissance stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Resource development techniques](img/B21076_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Resource development techniques
  prefs: []
  type: TYPE_NORMAL
- en: Acquire public ML artifacts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As soon as the adversary has identified some of the details of the system, this
    can help them launch attacks such as creating a proxy ML model or directly crafting
    adversarial data, which are attack techniques that we will talk about later in
    this chapter. Artifacts include the software stack used to train the model, algorithms,
    model deployments, and training and testing datasets. The artifacts can also be
    in development or testing environments. Suppose they contain some logic, algorithms,
    or techniques that the production model uses. In that case, they can also compromise
    the production environment. Ensure you protect development environments as well
    as production environments. Access to those artifacts might require access keys
    or authenticated requests and you might think that this is enough, but it is not.
    You need to ensure that you differentiate access methods for different environments
    and rotate access keys so that the adversary cannot use the initial access stage
    techniques to acquire access to multiple environments.
  prefs: []
  type: TYPE_NORMAL
- en: Obtain capabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, the adversary may search for and get software tools to support their operations.
    Software tools can be either malicious or repurposed for malicious intent. Any
    software can be used here, and this tool or software doesn’t need to be ML-enabled.
    For example, the adversary can use a virtual camera to add realistic effects to
    a feed to intercept an actual camera feed going into a system and gain access
    using deep fake technology.
  prefs: []
  type: TYPE_NORMAL
- en: Deep fake technology
  prefs: []
  type: TYPE_NORMAL
- en: Deepfake technology is a way to manipulate or generate videos, images, and audio
    of people using deep learning techniques. The technology uses existing audio,
    video, or images of a person to generate new content in the likeness of that person.
    The technology is so powerful that the generated content is indistinguishable
    from a real recorded video or audio and, as a result, can be used for various
    malicious purposes such as fake news, unauthorized access to systems that use
    biometrics, and financial fraud.
  prefs: []
  type: TYPE_NORMAL
- en: Develop adversarial ML attack capabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As soon as the adversary has access to the system, or at least information about
    it, they may choose to develop their own attacks or implement ideas described
    in public research. Public research papers with existing libraries as a starting
    point are usually well documented and explain which vulnerabilities they exploit.
    You can use this information to protect your system, so it’s imperative not to
    share information publicly so that the adversary has a limited ability to tailor
    the attack on your system.
  prefs: []
  type: TYPE_NORMAL
- en: Acquire infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The adversary might buy or lease infrastructure they will use throughout their
    operation. This infrastructure can include physical or cloud servers’ domains,
    devices, or services. Depending on the implementation, the adversary will make
    it difficult for you to discover their traffic in your network and they will try
    to blend in. They will probably use infrastructure that can be very quickly provisioned
    and then shut down. That means that even if you discover a suspicious endpoint
    and block it, that doesn’t mean you are safe, as the adversary might provision
    new infrastructure and try again. This is why it’s essential to always follow
    the best industry practices about security to prepare for any attack.
  prefs: []
  type: TYPE_NORMAL
- en: Publish poisoned datasets and poison training data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it comes to ML, everything is based on data. Poisoning the training data
    will change the results of the algorithm and the trained model. They can introduce
    vulnerabilities that cannot be easily detectable. The adversary may poison training
    data and publish it in a public location. It can be new data or a different version
    of an open source dataset. Always verify the source of any open source ML artifacts
    you use to train or update your model to protect against poisoned datasets. Staying
    away from public data is not the only thing you can do since data can be introduced
    into your system using the ML supply chain compromise attack, which we will highlight
    later in this chapter. Always validate that the data has not changed in the data
    sources; do not encrypt data or set the data sources as read-only or immutable
    if possible.
  prefs: []
  type: TYPE_NORMAL
- en: Establish accounts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The adversary may create accounts with many services that they can use to target
    your system and gain access to the resources they need. They might also impersonate
    someone from your organization, such as an employee, and use this to gain access
    to your systems. Always verify whom you are talking to about a project and train
    your employees accordingly. For example, if you get a message on LinkedIn from
    a coworker asking you to share access or reset their password for them, always
    verify their identity. Even if they use a company email, if this is not the proper
    process for resetting credentials or requesting access, direct them to follow
    the appropriate process because their account might be compromised. You could
    unknowingly share access information about the ML endpoint or training data with
    a third party.
  prefs: []
  type: TYPE_NORMAL
- en: Initial access techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The adversary can use the following techniques to gain access to the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Initial access techniques](img/B21076_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Initial access techniques
  prefs: []
  type: TYPE_NORMAL
- en: Let us understand each of these techniques in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: ML supply chain compromise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the ML supply chain compromise technique, the adversary is trying to gain
    access by compromising the unique parts of the ML supply chain. It usually includes
    hardware used to train the model, such as GPU hardware, data, parts of the software
    stack, or the model itself. When it comes to hardware, always verify that you
    have the latest updates and patches. When using open source libraries, always
    check the implementation of those algorithms. Any updates to the library should
    be checked for vulnerabilities or malicious code. Data can be poisoned, especially
    public data, and private datasets can be compromised during the labeling phase.
    If you are using a third-party service to label your data, make sure there are
    processes in place to protect against data poisoning. Also, keep versions of your
    datasets to compare any changes and identify issues if possible. If you’re using
    Azure Machine Learning, some capabilities can help you with that. Finally, if
    you are using open source models and fine-tuning them using your own private dataset,
    always verify the source of the model or the libraries, especially when updates
    are released. Every time you incorporate new models or execute unknown code, there
    is the possibility it is infected with traditional malware.
  prefs: []
  type: TYPE_NORMAL
- en: Valid accounts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this case, the adversary may obtain valid credentials from existing accounts
    or API access keys. Especially when using Azure Machine Learning, leaked or stolen
    credentials may provide access to artifacts and allow the adversary to compromise
    them. We should be worried about two levels of access. The first is the user credentials
    and user accounts with access to the trained model or pipelines, and the second
    is the API keys for the inference model or pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Evade ML model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not all attacks are targeted at your ML project. An adversary can launch a craft
    adversarial data attack to prevent an ML model from correctly identifying data
    contents. This technique disrupts any task that relies on ML. Such tasks or processes
    can be, for example, ML-based malware detection software, network scanning software,
    or antivirus software that protects the system from traditional cyberattacks.
    Mitigations for this technique include model hardening to make ML models robust
    to specific inputs, behaviors, or atypical queries or using an ensemble of models
    for inference to increase robustness since an attack can be effective against
    one model type but not another.
  prefs: []
  type: TYPE_NORMAL
- en: Exploit public-facing applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While Azure provides many security features and complies with several industry-standard
    protocols for its services, the responsibility for protecting anything from ML
    to databases, public-facing applications, and internet-accessible endpoints (such
    as web servers) falls on the customer. So, when protecting your ML assets, you
    must think of all the related services using your model. In this book, we will
    talk a lot about security practices that have nothing to do with ML just because
    they apply to related services that use the ML environment, such as networks and
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: ML model access techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are four techniques that can be leveraged to access your ML model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – ML model access techniques](img/B21076_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – ML model access techniques
  prefs: []
  type: TYPE_NORMAL
- en: Let us review each of the ML model access techniques.
  prefs: []
  type: TYPE_NORMAL
- en: ML model inference API access
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ML model inference API access technique refers to legitimate access to the
    inference API. The goal of creating models is to make predictions. More often
    than not, those predictions are leveraged from web applications. The way to accomplish
    this is to publish the model as a web API for use from the service. This API can
    provide an adversary with information about the model type or the data.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, models are retrained to learn based on input. That means if you are
    using the production environment to retrain and improve your model based on actual
    usage, ensure there is an approval process to check the data coming from public
    endpoints. Otherwise, an adversary can introduce false data to the system just
    by using the inference API.
  prefs: []
  type: TYPE_NORMAL
- en: ML-enabled product or service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even if your API is not public, the service using it still contains some information
    about the model and its data. An application that has access to an inference endpoint
    still has to send some data to it, which may reveal details of the ML model in
    logs or metadata. Hijacking the application opens up the ML service, making it
    vulnerable to multiple attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Physical environment access
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attacks are not only digital. Suppose the model or the application interacting
    with the model uses real-world data somehow. In that case, the adversary can influence
    the model by accessing the environment where the data has been collected. For
    example, if you have an application that streams data from sensors or cameras
    by accessing the camera feed and poisoning it, the adversary can influence the
    model. Ensure that when you are using sensors, they are properly secured in their
    communication with any form of system and that there’s not a single point of failure
    in the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Full ML model access
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, although it’s more secure to have the model in a centralized system
    and query it every time you need a prediction in real life, that’s not always
    sustainable or performant. You might be tempted to upload or repackage a mobile
    version of your model and add it to your edge device, such as your sensors or
    mobile device. While this might increase performance and provide faster predictions,
    this increases the attack surface area. If possible, consider uploading your model
    to the cloud with a single access point to reduce the attack surface area.
  prefs: []
  type: TYPE_NORMAL
- en: Execution techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two execution techniques available, both of which rely on specific
    actions or scripts executed by a user or a tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Execution techniques](img/B21076_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – Execution techniques
  prefs: []
  type: TYPE_NORMAL
- en: Let us explore each of these techniques in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: User execution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The adversary will usually rely on specific actions made by a legitimate user
    to gain execution. A system user may unknowingly execute unsafe code introduced
    by the ML supply chain compromise technique or social engineering. To mitigate
    this, train your users not to open suspicious or malicious links or documents
    from unknown sources. Part of the training should also be for checking the ML
    artifacts used throughout the ML process, as those can also be poisoned. Always
    check the checksum of the file or source and verify that it is secure and unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: Command and scripting interpreter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adversaries may use command and script interpreters to execute commands in the
    target system. Those interfaces provide many ways of interacting with the system,
    and it’s a standard feature of different technologies. Depending on the operating
    system, there are tools included. For example, in Windows, the Windows Command
    Shell and PowerShell can be exploited. There are also interpreters for programming
    languages such as Python. Commands and scripts can be embedded in payloads delivered
    to the target system as documents or downloaded from poisoned sources. Remote
    services can also be used.
  prefs: []
  type: TYPE_NORMAL
- en: Persistence techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Two persistent techniques are available, one of which we’ve already discussed
    in the *Resource development* section. The poison training data technique can
    be used to embed vulnerabilities or insert a backdoor trigger. Depending on the
    goal, this can be either a resource development or a persistence technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Persistence techniques](img/B21076_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – Persistence techniques
  prefs: []
  type: TYPE_NORMAL
- en: Backdoor ML model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The adversary can introduce a backdoor into the ML model based on other techniques,
    such as the poison training data technique. A model that includes a back door
    usually works as expected but will produce a different output when triggered by
    an input associated with a specific request from the adversary. This technique
    gives the adversary a persistent artifact on the system. The back door can be
    either a model response tailored to the input from the adversary or the invocation
    of an injected payload that bypasses the model and returns a different set of
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Defense evasion techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There’s only one defense evasion technique we’ve already discussed: the evade
    ML technique. It can also be used as an initial access technique, and after initial
    access has been granted, the adversary is not detected by any other software that
    might be using ML by disrupting its process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Defense evasion techniques](img/B21076_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Defense evasion techniques
  prefs: []
  type: TYPE_NORMAL
- en: Discovery techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three discovery techniques that all target getting more information
    about the model, its ontology, its family, or the artifacts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Discovery techniques](img/B21076_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Discovery techniques
  prefs: []
  type: TYPE_NORMAL
- en: Let us explain these techniques in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Discover ML model ontology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To discover the ontology of the ML model output, the adversary can analyze the
    types of objects the model can take as input. The ontology of the model can also
    be found in either documentation or configuration files. To mitigate against this,
    you can try obfuscating the model’s output and restricting the number of requests
    the user can make to a model. Usually, the adversary would have to create a large
    number of requests for the model to produce multiple outputs and get useful information
    from a range of results.
  prefs: []
  type: TYPE_NORMAL
- en: Discover the ML model family
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The adversary may use examples from the responses and outputs to discover the
    general family of the model. Ensure that the family of the model is not public
    information and cannot be easily guessed by the inputs and outputs of the model.
    Passive ML output obfuscation can be used here to mitigate this and restrict the
    number of requests a user or application can make to the model in a specific amount
    of time. Anything you can do to limit knowledge about the model can make it more
    difficult for the adversary to tailor an attack to your individual technology
    or model family.
  prefs: []
  type: TYPE_NORMAL
- en: Discover ML artifacts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At any point, the adversary will try to discover the private or public artifacts
    you are using in your model. This process usually starts in the resource development
    stage, but in this stage, we focus more on private ML artifacts. To mitigate this,
    collect and secure any artifacts, such as the software stack, testing and training
    data, data management systems, container registries, and software repositories.
    Encrypt sensitive information and systems where possible. Azure provides the management
    of encryption not only in data but also in whole services, such as Azure storage
    accounts.
  prefs: []
  type: TYPE_NORMAL
- en: Collection techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the collection stage, we have three techniques that focus on data collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Collection techniques](img/B21076_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Collection techniques
  prefs: []
  type: TYPE_NORMAL
- en: ML artifact collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The adversary might collect any useful data or company information for exfiltration
    as soon as the artifacts are identified. Suppose the adversary’s goal is not to
    disrupt the service but to gather information, such as proprietary data, by using
    your models and datasets. In that case, the adversary will be interested in getting
    as many ML artifacts as possible. Encryption of sensitive information at rest
    and in transit will help mitigate this to some extent because even if the adversary
    collects data, they won’t be able to read or use it.
  prefs: []
  type: TYPE_NORMAL
- en: Data from information repositories
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data for ML is not always stored in databases or used as datasets. Machine learning
    projects usually require collaboration and planning. Information can also be stored
    in several information repositories and can be mined to get valuable information.
    Information repositories include document-sharing services or project management
    systems such as SharePoint, Confluence, and Jira. To mitigate this technique,
    ensure that the users are trained and informed not to share model endpoints or
    information on the project management software. Documentation about the service
    should also be secured and shared only with people required to possess that information
    using secure channels.
  prefs: []
  type: TYPE_NORMAL
- en: Data from the local system
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Any data stored in any local systems must be secured as well. After gaining
    access to the network, an adversary may search filesystems’ configuration files
    and local datasets to extract data, especially sensitive data such as SSH keys,
    encryption keys, and connection information.
  prefs: []
  type: TYPE_NORMAL
- en: ML attack staging techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After data collection, it makes sense that the adversary will move on to exfiltration
    to get that data out of the system. But for ML, there’s a different stage. We
    also have to consider the ML attack staging. Depending on the adversary’s purpose,
    they might want to leverage their knowledge to disrupt the service by using several
    techniques that target the ML model before they try to extract ML data. Here,
    we can identify four techniques, one of which is the backdoor ML model technique
    that can also be used as a persistent technique. We’ve already discussed it in
    the *Backdoor ML model* section, so here, we will talk about the rest:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – ML attack staging techniques](img/B21076_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – ML attack staging techniques
  prefs: []
  type: TYPE_NORMAL
- en: Create a proxy ML model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The adversary might create an ML model as a proxy for the target model. Proxy
    models can be used in a variety of ways. The adversary might train models from
    similar datasets, use available pre-trained models, or train a proxy model from
    ML artifacts they have gathered in previous stages. The proxy model then can serve
    to replicate the victim’s inference API or to replicate access to target another
    model within the organization.
  prefs: []
  type: TYPE_NORMAL
- en: Verify attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before the launch of the attack, the adversary might need to verify that the
    strategy they have developed works. That means getting an offline or a replicated
    model and trying out the techniques they have planned. This gives them the confidence
    that the attack is effective. Then, they are free to deploy it in the physical
    environment or keep it and use it at a later time. When the adversary has gathered
    enough information and has the capability to verify the attack in a replicated
    system they have built that mirrors the victim organization system, it presents
    a new problem. The actual attack won’t trigger any significant traffic in the
    victim’s systems, making the use of this technique potentially undetectable.
  prefs: []
  type: TYPE_NORMAL
- en: Craft adversarial data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The craft adversarial data technique that was already mentioned in the *Evade
    ML model* section needs information and artifacts collected in multiple stages.
    Typically, the result is data poisoning. Depending on the adversary’s goal, the
    inputs have been modified, which causes effects such as missed predictions, misclassifications,
    or the maximizing of the system’s energy consumption. This attack depends greatly
    on the adversary’s knowledge of the system. You can use many different algorithms
    to develop the adversarial data attack, such as white-box optimization, black-box
    optimization, black-box transfer, or manual modification.
  prefs: []
  type: TYPE_NORMAL
- en: Exfiltration techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The data is usually extracted in the exfiltration stage, which involves two
    techniques that apply to ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Exfiltration techniques](img/B21076_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – Exfiltration techniques
  prefs: []
  type: TYPE_NORMAL
- en: Exfiltration via ML inference API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ML inference API, if vulnerable, can lead to a leak of private information
    about the training, the model itself, or private intellectual property. If the
    model inference API needs to be public, ensure that you secure it as much as possible
    and limit the number of queries a user can do in production so that they cannot
    figure out different ways of getting that information.
  prefs: []
  type: TYPE_NORMAL
- en: Exfiltration via cyber-means
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Of course, the ML project is not the only thing that is vulnerable here. Depending
    on the overall security of the environment and the systems, an adversary might
    choose traditional exfiltration techniques to steal data from your network. Exfiltration
    can be accomplished by just transferring data over the network, transferring data
    over a physical medium such as a removable or cloud drive, or via the internet
    to a web service, a code repository, or directly to cloud storage.
  prefs: []
  type: TYPE_NORMAL
- en: Impact techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are seven impact techniques available where the adversary manipulates
    or interrupts the service for your ML systems or data. We have already covered
    the evade ML model technique. Let us look at the rest of the techniques of this
    stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Impact techniques](img/B21076_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Impact techniques
  prefs: []
  type: TYPE_NORMAL
- en: Denial of ML service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Denial-of-service attacks target ML systems with multiple requests to disrupt
    the service. Since endpoints have finite resources, by using a denial-of-service
    attack, an adversary can create bottlenecks which can be expensive and disrupt
    the service so that it cannot serve other requests, rendering the service useless.
    There are a couple of things you can do to mitigate them that involve deploying
    third-party Azure services, but the first step would probably be to restrict the
    number of ML model requests.
  prefs: []
  type: TYPE_NORMAL
- en: Spam ML systems with chaff data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This technique requires the adversary to know that the system probably uses
    the data for predictions to retrain the service. Spamming the system with many
    requests and data that does not make sense will increase the number of predictions
    or false predictions. It will cause analysts or data scientists working on improving
    the system to waste much time reviewing and correcting those incorrect inferences.
    There are capabilities of Azure Machine Learning that we will talk about to mitigate
    this. However, the obvious choice here is to restrict the number of ML model queries
    or block traffic from suspicious endpoints that make multiple unrelated requests.
  prefs: []
  type: TYPE_NORMAL
- en: Erode ML model integrity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This technique combines spamming the system with data and using adversarial
    data to degrade the model’s performance. This doesn’t have to be a one-time event.
    The attack can be ongoing for quite some time, so the ML system is eroded and
    predictions are inaccurate. This attack might be more difficult to detect since
    it does not have the goal of disrupting the service; it wants to subtly make changes
    to the model that are not detectable over a long period of time.
  prefs: []
  type: TYPE_NORMAL
- en: Harvest cost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Systems have finite resources, and usually, when someone is launching multiple
    requests bombarding the system with so much data, it interrupts the service. With
    cloud computing, infrastructure doesn’t have to be a finite resource. Cloud systems
    can scale to accommodate increased traffic, but at the same time, auto-scaling
    affects the costs of those resources. Restricting the number of queries per application
    or detecting those kinds of attacks can mitigate this technique, which targets
    the operational costs of the victim’s organization.
  prefs: []
  type: TYPE_NORMAL
- en: ML intellectual property theft
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes the target is the model itself. Let’s say you provide ML as a service.
    Someone who has managed to extract that model now has unlimited use of your service
    without paying. That can have a significant impact, as the intellectual property
    is unsafe and can cause economic harm to your organization. Mitigations for this
    technique include controlling access to your models and data at rest, securing
    your models and data in transit, and encrypting services and data where possible.
  prefs: []
  type: TYPE_NORMAL
- en: System misuse for external effect
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the adversary cannot extract the model, they might still attack the system
    and use it for their own purpose. Hijacking the system and using their own data
    to get results or predictions could be an example. By gaining access to a system
    that monitors and protects financial data, the adversary might be able to pass
    invoices that otherwise would be flagged as invalid. As a result, this prohibits
    the system from preventing fraud.
  prefs: []
  type: TYPE_NORMAL
- en: Case studies and examples
  prefs: []
  type: TYPE_NORMAL
- en: For more case studies, you can check the *Further reading* section of this chapter
    or access the complete knowledge base at [https://atlas.mitre.org/](https://atlas.mitre.org/).
  prefs: []
  type: TYPE_NORMAL
- en: After seeing the types of generic techniques used in attacks, let us explore
    the actual services that can be affected in the case of an attack.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Azure services involved in ML attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, attacks are multi-level and primarily based on the adversary’s
    goal. Since we do not know what that is, we can deploy multiple mitigation techniques
    to lessen the impact. As Azure Machine Learning is based on the Azure platform,
    we can deploy numerous tools to detect an incident, and by using automation, the
    platform will deploy mitigation steps before we are even aware that something
    has happened. Although we focused on ML attacks, attacks on related systems, virtual
    machines, and databases are still a concern. Let us look at associated services
    that can be used together with **Azure** **Machine Learning**.
  prefs: []
  type: TYPE_NORMAL
- en: Access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Microsoft Entra ID** usually handles access in Azure. Microsoft Entra ID
    is Microsoft’s cloud-based identity and access management service. It provides
    a range of features and capabilities to manage user identities and secure access
    to various resources in the Azure cloud and other Microsoft services. Besides
    identity and access management, it also provides **Federation**, **single sign-on**
    (**SSO)**, a developer platform, and various features for security and governance.
    Different services might also offer different ways of authentication, such as
    via service credentials, access keys, and shared access signatures. We will focus
    on learning how to secure and mitigate all those services in the following chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML is based on data. Azure Machine Learning supports multiple data sources,
    specifically **Azure Blob** and **File Storage**, **Azure Data Lake**, **Azure
    SQL Database**, **Azure PostgreSQL** database, and **Azure MySQL** database, each
    with individual security and monitoring features. We will be working with their
    security and monitoring features, including encryption at rest and encryption
    in transit, in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although not directly related to Azure Machine Learning, many attacks happen
    by infiltrating the on-premises or cloud network. In the following chapters, we
    will discuss securing the service using network services. These include virtual
    networks, network security groups, **Azure Firewall**, and hybrid solutions such
    as **VPN gateways** and **ExpressRoute**. The **Service Endpoints** and **Private
    Endpoints** features can also be used for better security and isolation.
  prefs: []
  type: TYPE_NORMAL
- en: An Azure **virtual network** (**VNet**) is a fundamental component of Microsoft
    Azure’s networking architecture. It is a logically isolated network environment
    that allows you to securely connect and control Azure resources, including **virtual
    machines** (**VMs**), **Azure App Service**, and databases.
  prefs: []
  type: TYPE_NORMAL
- en: Azure VNets usually work together with **network security groups** (**NSGs**),
    which provide granular network security and act as a basic firewall, allowing
    you to define inbound and outbound traffic rules to filter and control network
    traffic. NSGs do not maintain state but are the first step to securing the network
    traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Firewall is a cloud-based network security service offered by Microsoft
    Azure. It provides centralized, high-level network security and protection for
    VNets. Azure Firewall acts as a fully stateful network traffic filtering and routing
    solution, allowing you to control and monitor both inbound and outbound traffic
    to and from your Azure resources.
  prefs: []
  type: TYPE_NORMAL
- en: An Azure VPN gateway is a networking component that enables secure connectivity
    between on-premises networks and VNets. It provides a way to establish a **virtual
    private network** (**VPN**) tunnel over the public internet, ensuring secure communication
    and extending your on-premises network into the Azure cloud.
  prefs: []
  type: TYPE_NORMAL
- en: If the VPN gateway is not enough, you can use Azure ExpressRoute. This is a
    Microsoft Azure service that enables private and dedicated network connectivity
    between your on-premises network and Azure. It provides a reliable, high-throughput,
    low-latency connection, bypassing the public internet.
  prefs: []
  type: TYPE_NORMAL
- en: '**Service Endpoints** and **Private Endpoints** are two features in Azure that
    provide secure and private connectivity to Azure services. Service Endpoints allows
    you to extend your VNet to the Azure service’s backend, providing secure access
    to that service over the Azure backbone network. Private Endpoints allows you
    to access Azure services privately from your VNet using a private IP address.'
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Applications can be hosted in multiple services inside and outside of Azure.
    Most ML services are intended to be used by an application, which is another component
    we need to consider when we are working on implementing security. In this book,
    we will learn how to secure services that host applications such as Azure App
    Service, VMs, or **container** services, but we will also analyze some best practices
    for developing software applications. Of course, since the implementation of application
    security heavily depends on the programming language and libraries used, we will
    explain the high-level implementation of mitigation techniques such as SQL injection
    or cross-site scripting.
  prefs: []
  type: TYPE_NORMAL
- en: Compute
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML relies heavily on computational resources. You can create multiple compute
    targets for training or hosting inference models in the workspace. Targets can
    be local compute, **Azure Machine Learning compute**, **Azure Databricks**, **HDInsight**,
    **Synapse Spark pools**, **Azure Kubernetes Service** (**AKS**), and Azure VMs.
    These compute targets provide the necessary resources and infrastructure to run
    ML workloads at scale. They must be secured and monitored properly, as they are
    a critical part of ML.
  prefs: []
  type: TYPE_NORMAL
- en: Local compute allows you to use your local machine or on-premises infrastructure
    as the compute target. This is useful for development and experimentation when
    you don’t require large-scale resources.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning compute is a managed compute cluster provided by Azure
    Machine Learning. It dynamically provisions and scales compute resources based
    on your workload requirements. It supports both CPU and GPU instances and is optimized
    for running training jobs at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Databricks is an Apache Spark-based analytics platform that integrates
    with Azure Machine Learning. You can use Azure Databricks clusters as a compute
    target for training and deploying ML models, taking advantage of the distributed
    computing capabilities of Spark. If you still want to use Spark but are not using
    Databricks, you can use Azure Synapse Spark pools.
  prefs: []
  type: TYPE_NORMAL
- en: Azure HDInsight can also be set up as a compute target in Azure Machine Learning.
    Using its distributed processing capabilities allows you to execute ML tasks on
    HDInsight clusters.
  prefs: []
  type: TYPE_NORMAL
- en: AKS is a managed Kubernetes service in Azure. You can deploy your ML workloads
    as containerized applications on AKS and use them as a compute target for training
    and serving models. AKS provides scalability and flexibility for running distributed
    training and inference workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Azure VM instances can also be a compute target. You can provision VMs with
    the required specifications and use them for training and deploying ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All previously mentioned services relate to or can be used with the **Azure
    Machine Learning workspace**. The workspace is the main point of management for
    Azure Machine Learning; however, the workspace itself can be used for security
    or isolation, for example, if multiple workspaces might be needed for different
    scenarios. The workspace provides multiple features for monitoring and organizing
    assets such as model and dataset versioning and data drift. We will also explore
    security features in algorithms, data, and models, such as fairness, data anonymization,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’ve learned about the services related to Azure Machine Learning that
    we need to protect. They might not be the only ones—it depends on your system
    architecture—but they are a great start to your security journey.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many attacks to be prepared for and vulnerabilities are discovered
    daily, so we must follow a framework that helps us keep up to date with current
    vulnerabilities and their mitigations where possible. The MITRE ATLAS framework
    is a great resource to get started as it is adapted to ML. We need to be aware
    of the 12 stages and multiple techniques per stage to protect our ML assets. However,
    as ML assets work with numerous other systems, the implementations we will see
    in the following chapters will include securing Azure Machine Learning and all
    its related services.
  prefs: []
  type: TYPE_NORMAL
- en: But before diving into those implementations, in the next chapter, we will learn
    about the security industry compliance standards we must adhere to and how to
    implement compliance controls together with responsible AI development practices.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural
    Payload Injection: [https://arxiv.org/abs/2101.06896](https://arxiv.org/abs/2101.06896)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Imitation Attacks and Defenses for Black-box Machine Translation Systems: [https://arxiv.org/abs/2004.15015](https://arxiv.org/abs/2004.15015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Explaining and Harnessing Adversarial Examples: https://arxiv.org/abs/1412.6572'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
