<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer131">
			<h1 id="_idParaDest-159"><em class="italic"><a id="_idTextAnchor163"/>Chapter 8</em>: DevOps and MLOps for the Edge</h1>
			<p>The 21st century's flurry of connected devices has transformed the way we live. It can be hard to remember the days without the convenience of smartphones, smartwatches, personal digital assistants (such as Amazon Alexa), connected cars, smart thermostats, or other devices. </p>
			<p>This adoption is not going to slow down anytime soon as the industry forecasts that there will be over 25 billion IoT devices globally in the next few years. With the increased adoption of connected technologies, the new normal is to have <em class="italic">always-on</em> devices. In other words, the <em class="italic">devices should work all the time</em>. Not only that, but we also expect these devices to continuously get smarter and stay secure throughout their life cycles with new features, enhancements, or bug fixes. But how do you make that happen reliably and at scale? Werner Vogels, Amazon's chief technology officer and vice president, often says that "<em class="italic">Everything</em><em class="italic"> </em><em class="italic">fails all the time.</em>" It's challenging to keep any technological solution up and running all the time.</p>
			<p>With <strong class="bold">IoT</strong>, these challenges are elevated and more complicated as the <strong class="bold">edge</strong> devices are deployed in diverse operating conditions, exposed to environmental interferences, and have multiple layers of connectivity, communication, and latency. Thus, it's critical to build an edge-to-cloud continuum mechanism to collect feedback from the deployed fleet of edge devices and act on them quickly. This is where DevOps for IoT helps. <strong class="bold">DevOps</strong> is short for <strong class="bold">development and operations</strong>. It facilitates an agile approach to performing <strong class="bold">continuous integration and continuous deployment</strong> (<strong class="bold">CI/CD</strong>) from the cloud to the edge. </p>
			<p>In this chapter, we will focus on how DevOps capabilities can be leveraged for IoT workloads. We will also expand our discussion to <strong class="bold">MLOps</strong> at the edge, which implies implementing agile practices for <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) workloads. You learned about some of these concepts in the previous chapter when you built an ML pipeline. The focus of this chapter will be on deploying and operating those models efficiently. </p>
			<p>You are already familiar with developing local processes on the edge or deploying components from the cloud in a decoupled way. In this chapter, we will explain how to stitch those pieces together using DevOps principles that will help automate the development, integration, and deployment workflow for a fleet of edge devices. This will allow you to efficiently operate an intelligent distributed architecture on the edge (that is, a Greengrass-enabled device) and help your organization achieve a faster time to market for rolling out different products and features.</p>
			<p>In this chapter, we will be covering the following topics:</p>
			<ul>
				<li>Defining DevOps for IoT workloads</li>
				<li>Performing MLOps at the edge</li>
				<li>Hands-on with deploying containers at the edge</li>
				<li>Checking your knowledge</li>
			</ul>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor164"/>Technical requirements </h1>
			<ul>
				<li>The technical requirements for this chapter are the same as those outlined in <a href="B17595_02_Final_SS_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">Foundations of Edge Workloads</em>. See the full requirements in that chapter.</li>
			</ul>
			<p>Now, let's dive into this chapter.</p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor165"/>Defining DevOps for IoT workloads</h1>
			<p>DevOps has <a id="_idIndexMarker722"/>transformed the way companies do business in <a id="_idIndexMarker723"/>today's world. Companies such as Amazon, Netflix, Google, and Facebook conduct hundreds or more deployments every week to push different features, enhancements, or bug fixes. The deployments themselves are typically transparent to the end customers in that they don't experience any downtime from these constant deployments.</p>
			<p>DevOps is a methodology that brings <em class="italic">developers and operations</em> closer to infer quantifiable technical and business benefits with faster time to market through shorter development <a id="_idIndexMarker724"/>cycles and increased release frequency. A common <a id="_idIndexMarker725"/>misunderstanding is that DevOps is only a set of new technologies to build and deliver software faster. DevOps also represents a cultural shift to promote ownership, collaboration, and cohesiveness across different teams to foster innovation across the organization. DevOps has been adopted by organizations and companies of all sizes for distributed workloads to deliver innovation, enhancements, and operational efficiency faster. The following diagram shows the virtuous cycle of software delivery:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="Images/B17595_08_01.jpg" alt="Figure 8.1 – The virtuous cycle of software delivery&#13;&#10;" width="1234" height="462"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – The virtuous cycle of software delivery</p>
			<p>For the sake of brevity, we are not going to dive deeper into the concepts of DevOps or <strong class="bold">Agile</strong> practices here. Instead, we will focus on introducing the high-level concepts surrounding DevOps and discuss its relevance for IoT workloads.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor166"/>Fundamentals of DevOps</h2>
			<p>DevOps brings <a id="_idIndexMarker726"/>together different tools and best practices, as follows:</p>
			<ul>
				<li><strong class="bold">Shared code repository</strong>: Using <a id="_idIndexMarker727"/>a version control system is a prerequisite and a best practice in the field of code development. All artifacts that are required in the deployment package need to be <a id="_idIndexMarker728"/>stored here. Examples <a id="_idIndexMarker729"/>include <strong class="bold">Bitbucket</strong>, <strong class="bold">Gitlab</strong>, and <strong class="bold">AWS CodeCommit</strong>.</li>
				<li><strong class="bold">Continuous integration</strong> (<strong class="bold">CI</strong>): In this step, developers commit their code changes regularly <a id="_idIndexMarker730"/>in the code repository. Every revision <a id="_idIndexMarker731"/>that is committed will trigger an automated build <a id="_idIndexMarker732"/>process that performs code scanning, code reviews, compilation, and automated unit testing. This allows developers to identify and fix bugs <a id="_idIndexMarker733"/>quickly, allowing them to adhere to the best practices and deliver features faster. The output of this process includes build <a id="_idIndexMarker734"/>artifacts (such as binaries or executable programs) that comply <a id="_idIndexMarker735"/>with the organization's enforced practices. Examples of toolchains <a id="_idIndexMarker736"/>include <strong class="bold">Jenkins</strong>, <strong class="bold">Bamboo</strong>, <strong class="bold">GitLab CI</strong>, and <strong class="bold">AWS CodePipeline</strong>. For IoT <a id="_idIndexMarker737"/>workloads, similar toolchains can be used.</li>
				<li><strong class="bold">Continuous delivery</strong> (<strong class="bold">CD</strong>): This step expands on the previous step of CI and deploys all the <a id="_idIndexMarker738"/>compiled binaries to the staging or test <a id="_idIndexMarker739"/>environment. Once <a id="_idIndexMarker740"/>deployed, automated tests related to integration, functional, or non-functional requirements are executed as <a id="_idIndexMarker741"/>part of the workflow. Examples of toolchains for testing include <strong class="bold">JMeter</strong>, <strong class="bold">Selenium</strong>, <strong class="bold">Jenkins</strong>, and <strong class="bold">Cucumber</strong>. This allows developers to thoroughly test changes and pre-emptively discover issues in the context of the overall application. The final step is deploying the validated code artifacts to the production environment (with or without manual approval). </li>
				<li><strong class="bold">Continuous monitoring</strong> (<strong class="bold">CM</strong>): The core objective for DevOps is to remove silos between <a id="_idIndexMarker742"/>the development and operations teams. Thus, CM is a critical step if you wish to have a continuous feedback loop for observing, alerting, and mitigating issues related to infrastructure or hosted applications, as shown in the following diagram:</li>
			</ul>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="Images/B17595_08_02.jpg" alt="Figure 8.2 – DevOps life cycle &#13;&#10;" width="1627" height="579"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – DevOps life cycle </p>
			<p>Common <a id="_idIndexMarker743"/>toolchains <a id="_idIndexMarker744"/>for monitoring include <strong class="bold">Amazon CloudWatch</strong>, <strong class="bold">Amazon X-Ray</strong>, <strong class="bold">Splunk</strong>, and <strong class="bold">New Relic</strong>.</p>
			<ul>
				<li><strong class="bold">Infrastructure as Code </strong>(<strong class="bold">IaC</strong>): Adhering to the software development practices of CI/CD to expedite shipping code is a great first step, but it's not enough. Teams <a id="_idIndexMarker745"/>can develop and test their code using <a id="_idIndexMarker746"/>agile processes, but the final delivery to production still follows waterfall methods. This is often due to a lack of control regarding provisioning or scaling <a id="_idIndexMarker747"/>the infrastructure dynamically. Traditionally, organizations will have system admins to provision the required infrastructure <a id="_idIndexMarker748"/>resources manually, which can take days, weeks, or months. This is where IaC helps as it allows you to provision and manage the <a id="_idIndexMarker749"/>infrastructure, configurations, and policies using code (or APIs) in an automated fashion without requiring any manual interventions that might be error-prone or time-consuming. Common toolchains include <strong class="bold">Amazon CloudFormation</strong>, <strong class="bold">HashiCorp Terraform</strong>, and <strong class="bold">Ansible</strong>.</li>
			</ul>
			<p>Now that we have covered the the basics of DevOps, let's understand its relevance to IoT and the edge.</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor167"/>Relevance of DevOps for IoT and the edge</h2>
			<p>The evolution <a id="_idIndexMarker750"/>of edge computing from simple radio frequency identification <a id="_idIndexMarker751"/>systems to the microcontrollers and microprocessors of today has opened up different use cases across industry segments that require building a distributed architecture on the edge. For example, the connected HBS hub has a diverse set of functionalities, such as the following:</p>
			<ul>
				<li>A gateway for backend sensors/actuators</li>
				<li>Runtime for local components</li>
				<li>Interface to the cloud</li>
				<li>Message broker</li>
				<li>Datastream processor</li>
				<li>ML inferencing engine</li>
				<li>Container orchestrator</li>
			</ul>
			<p>That's a lot of work on the edge! Thus, the traditional ways of developing and delivering embedded software are not sustainable anymore. So, let's discuss the core activities in the life cycle of an IoT device, as depicted in the following table, to understand the relevance of DevOps:</p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="Images/B17595_08_Table1.jpg" alt="Figure 8.3 – Relevance of DevOps in IoT workloads&#13;&#10;" width="1650" height="1457"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Relevance of DevOps in IoT workloads</p>
			<p>The key components of DevOps such as CI/CD/CM are equally relevant for IoT workloads. This set <a id="_idIndexMarker752"/>of activities is often referred to as <strong class="bold">EdgeOps</strong> and, as we observed <a id="_idIndexMarker753"/>earlier, they are applied differently between the <a id="_idIndexMarker754"/>edge and the cloud. For example, CI is different for the edge because we need to test device software on the same hardware that is deployed in the world. However, because of the higher costs and risks associated with edge deployments, it is common to reduce the frequency of updating devices at the edge. It is also common for <a id="_idIndexMarker755"/>organizations to have different sets of hardware for prototyping <a id="_idIndexMarker756"/>versus production runtimes. </p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor168"/>DevOps challenges with IoT workloads</h2>
			<p>Now that you <a id="_idIndexMarker757"/>understand how to map DevOps phases to different IoT activities, let's expand on those a bit more. The following diagram shows the workflow that's typically involved in the life cycle of a device, from its creation to being decommissioned:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="Images/B17595_08_04.jpg" alt="Figure 8.4 – DevOps workflow for IoT&#13;&#10;" width="1094" height="535"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – DevOps workflow for IoT</p>
			<p>Here, you can see some key differences between an IoT workload and other cloud-hosted workloads. Let's take a look.</p>
			<p><em class="italic">The manufacturing process is involved</em>:</p>
			<p>Distributed workloads such as web apps, databases, and APIs use the underlying infrastructure provided by the cloud platform. Software developers can use IaC practices and integrate them with other CI/CD mechanisms to provision the cloud resources that are automatically required to host their workload. For edge workloads, the product lives beyond the boundaries of any data center. Although it's possible to run edge applications on virtual infrastructure provided by the cloud platform during the testing or prototyping <a id="_idIndexMarker758"/>phases, the real product is always hosted on hardware (such as a <strong class="bold">Raspberry Pi</strong> for this book's project). There is always a dependency on the contract manufacturer (or other vendors) in the supply chain for manufacturing the hardware, as per the required specifications that are followed for programming it with the device firmware. Although the firmware can be developed on the cloud <a id="_idIndexMarker759"/>using DevOps practices, flashing the firmware image is done at manufacturing time only. This hinders the end-to-end automation common in traditional DevOps workflows, where the infrastructure (such as an AWS EC2 instance) is readily imaged and available for application deployments. The following diagram shows the typical life cycle of device manufacturing and distribution:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="Images/B17595_08_05.jpg" alt="Figure 8.5 – IoT device manufacturing process&#13;&#10;" width="1650" height="973"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – IoT device manufacturing process</p>
			<p><em class="italic">Securing the hardware is quintessential</em>:</p>
			<p>Some of the <a id="_idIndexMarker760"/>key vulnerabilities for edge workloads that are listed by <strong class="bold">The Open Web Application Security Project</strong> (<strong class="bold">OWASP</strong>) are as follows: </p>
			<ul>
				<li>Weak, guessable, or hardcoded passwords</li>
				<li>Lack of physical hardening</li>
				<li>Insecure data transfer and storage </li>
				<li>Insecure default settings</li>
				<li>Insecure ecosystem interfaces</li>
			</ul>
			<p>Although distributed workloads may have similar challenges, mitigating them using cloud-native <a id="_idIndexMarker761"/>controls makes them easier to automate than IoT workloads. Using AWS as an example, all communications within AWS infrastructure (such as across data centers) are encrypted in transit by default and require no action. Data at rest can be encrypted with a one-click option (or automation) using the key <a id="_idIndexMarker762"/>management infrastructure provided by AWS (or customers can bring their own). Every service (or hosted workloads) needs <a id="_idIndexMarker763"/>to enable access controls for authentication and authorization through cloud-native <strong class="bold">Identity &amp; Access Management</strong> services, which can be automated as well through IaC implementation. Every service (or hosted workload) can take advantage of observability and traceability through <a id="_idIndexMarker764"/>cloud-native monitoring services (such as <strong class="bold">Amazon CloudTrail</strong> or <strong class="bold">Amazon CloudWatch</strong>).</p>
			<p>On the contrary, for edge workloads, all of the preceding requirements are required to be fulfilled during manufacturing, assembling, and registering the device, thus putting more onus on the supply chain to manually implement these over one-click or automated workflows. For example, as a best practice, edge devices should perform mutual authentication over TLS1.2 with the cloud using credentials such as X.509 certificates compared to using usernames and passwords or symmetric credentials. In addition, the credentials should have least-privileged access implemented using the right set of permissions (through policies). This can help ensure that the devices are implementing the required access controls to protect the device's identity and that the data in transit is fully encrypted. In addition, device credentials (such as X.509 certificates) on the <a id="_idIndexMarker765"/>edge must reside inside a secure element or <strong class="bold">trusted platform module</strong> (<strong class="bold">TPM</strong>) to reduce the risk of unauthorized access and identity compromise. Additionally, secure mechanisms are required to separate the filesystems on the device and encrypt the data at rest using different cryptographic utilities such as <strong class="bold">dm-crypt</strong>, <strong class="bold">GPG</strong>, and <strong class="bold">Bitlocker</strong>. Observability and traceability <a id="_idIndexMarker766"/>implementations <a id="_idIndexMarker767"/>for different edge components <a id="_idIndexMarker768"/>are left to the respective owners.</p>
			<p><em class="italic">Lack of standardized frameworks for the edge</em>:</p>
			<p>Edge components are no longer limited to routers, switches, miniature servers, or workstations. Instead, the <a id="_idIndexMarker769"/>industry is moving toward building distributed architectures on the edge in different ways, as follows:</p>
			<ul>
				<li><strong class="bold">Fog computing</strong>, which lets us shift more intelligence to the edge using a decentralized computing infrastructure of heterogeneous nodes</li>
				<li><strong class="bold">Mobile/Multi-Access Computing (MEC)</strong>, which incorporates next-generation radio spectrums (such as 5G) to enable a new generation of workloads possible for the edge</li>
				<li><strong class="bold">Data center-in-a-box</strong>, which enables resource-intensive computing capabilities at the edge with integrations to the cloud</li>
			</ul>
			<p>The following diagram shows an edge-to-cloud workflow that includes various technology capabilities that are common in distributed architectures:</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="Images/B17595_08_06.jpg" alt="Figure 8.6 – Edge-to-cloud architecture &#13;&#10;" width="452" height="426"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – Edge-to-cloud architecture </p>
			<p>The edge architecture's standards are still evolving. Considering there are different connectivity interfaces, communication protocols, and topologies, there are heterogeneous <a id="_idIndexMarker770"/>ways of solving different use cases. For example, connectivity interfaces may include different short-range (such as <em class="italic">BLE</em>, <em class="italic">Wi-Fi</em>, and <em class="italic">Ethernet</em>) or long-range radio networks (such as <em class="italic">cellular</em>, <em class="italic">NB-IoT</em>, and <em class="italic">LoRa</em>). The connectivity interface that's used needs to be determined during the hardware designing <a id="_idIndexMarker771"/>phase and is implemented as a one-time process. Communication protocols may include different transport layer protocols over TCP (connection-oriented such as <em class="italic">MQTT</em> and <em class="italic">HTTPS</em>) or UDP (connectionless such as <em class="italic">CoAP</em>). Recall the layers of the <strong class="bold">Open System Interconnection</strong> (<strong class="bold">OSI</strong>) model, which we reviewed in <a href="B17595_02_Final_SS_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">Foundations of Edge Workloads</em>. The choice of communication interfaces can be flexible, so long as the underlying Layer 4 protocols are supported on the hardware. For example, if the hardware supports UDP, it can be activated with configuration changes, along with installing additional Layer 7 software (such as a COAP client) as required. Thus, this step can be performed through a cloud-to-edge DevOps workflow (that is, an OTA update). Bringing more intelligence to the edge requires dealing with the challenges of running distributed topologies on a computing infrastructure with low horsepower. Thus, it's necessary <a id="_idIndexMarker772"/>to define standards and design principles to design, deploy, and operate optimized software workloads on the edge (such as brokers, microservices, containers, caches, and lightweight databases). </p>
			<p>Hopefully, this has helped you understand the unique challenges for edge workloads from a DevOps perspective. In the next section, you will understand how AWS IoT Greengrass can help you build and operate distributed workloads on the edge.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor169"/>Understanding the DevOps toolchain for the edge</h1>
			<p>In the previous chapters, you learned how to develop and deploy native processes, data streams, and ML models <a id="_idIndexMarker773"/>on the edge locally and then deployed them at scale using <strong class="bold">Greengrass's</strong> built-in OTA mechanism. We will explain the reverse approach here; that is, building distributed applications on the cloud using DevOps practices and deploying them to the edge. The following <a id="_idIndexMarker774"/>diagram shows the approach to continuously build, test, integrate, and deploy workloads using the <strong class="bold">OTA</strong> update mechanism:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="Images/B17595_08_07.jpg" alt="Figure 8.7 – A CI/CD view for Edge applications" width="1217" height="622"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – A CI/CD view for Edge applications</p>
			<p>The two most common <a id="_idIndexMarker775"/>ways to build a distributed architecture on the edge using AWS IoT Greengrass is by using AWS Lambda services or Docker containers. </p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor170"/>AWS Lambda at the edge </h2>
			<p>I want to make it clear, to avoid any confusion, that the concept of Lambda design, which was <a id="_idIndexMarker776"/>introduced in <a href="B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>, <em class="italic">Ingesting and Streaming Data from the Edge</em>, is an architectural pattern that's used to operate streaming and batch workflows on the edge. <strong class="bold">AWS Lambda</strong>, on the contrary, is a serverless compute service that offers a runtime for executing any type of application with no administration. It allows developers to focus on the business logic, write code in different programming languages (such as <em class="italic">C</em>, <em class="italic">C++</em>, <em class="italic">Java</em>, <em class="italic">Node.js</em>, and <em class="italic">Go</em>), and upload it as a ZIP file. The service takes it from there in provisioning the underlying infrastructure's resources and scales based on incoming requests or events.</p>
			<p>AWS Lambda has been a popular compute choice in designing event-based architectures for real-time <a id="_idIndexMarker777"/>processing, batch, and API-driven workloads. Due to this, AWS has decided to extend the Lambda runtime support for edge processing through <strong class="bold">Amazon IoT Greengrass</strong>. </p>
			<p>So, are you wondering what the value of implementing AWS Lambda at the edge is? </p>
			<p>You are not alone! Considering automated hardware provisioning is not an option for the edge, as explained earlier in this chapter, the value here is around interoperability, consistency, and <a id="_idIndexMarker778"/>continuity from the cloud to the edge. It's very common for IoT workloads to have different code bases for the cloud (<strong class="bold">distributed stack</strong>) and the edge (<strong class="bold">embedded stack</strong>), which leads to additional complexity around code integration, testing, and deployment. This results in additional operational overhead and a delayed time to market. </p>
			<p>AWS Lambda aimed to bridge this gap so that the cloud and embedded developers can use similar technology stacks for software development and have interoperable solutions. Therefore, building a DevOps pipeline from the cloud to the edge using a common toolchain becomes feasible. </p>
			<h4>Benefits of AWS Lambda on AWS IoT Greengrass</h4>
			<p>There are several <a id="_idIndexMarker779"/>benefits of running Lambda functions on the edge, as follows:</p>
			<ul>
				<li>Lambda functions that are deployed locally on the edge devices can <em class="italic">connect to different physical interfaces</em> such as CANBus, Modbus, or Ethernet to access different serial ports or GPIO on the hardware similar to embedded applications. </li>
				<li>Lambda functions can <em class="italic">act as the glue between different edge components</em> (such as Stream Manager) within AWS IoT Greengrass and the cloud resources.</li>
				<li>AWS IoT Greengrass also <em class="italic">makes it easier to deploy different versions of Lambda functions</em> by using an alias or a specific version for the edge. This helps in continuous delivery and is useful for scenarios such as blue/green deployments.</li>
				<li><em class="italic">Granular access control</em>, including specifying configurations (run as root) or permissions (read/write) for different local resources (such as disk volumes, serial ports, or GPIOs), can be managed for Lambda functions.</li>
				<li>Lambda functions can be run in both <strong class="bold">containerized</strong> and <strong class="bold">non-containerized</strong> modes. Non-containerized mode removes the abstraction layer and allows Lambda to run as a regular process on the OS. This is useful for latency-sensitive applications such as ML inferencing. </li>
				<li>Finally, AWS <a id="_idIndexMarker780"/>IoT Greengrass <em class="italic">allows you to manage the hardware resources</em> (RAM) that can be used by the Lambda function on the edge. </li>
			</ul>
			<p>The following diagram shows how an AWS Lambda function that's been deployed on the edge can interact with different components on the physical (such as the filesystem) or abstracted layer (such as stream manager on AWS IoT Greengrass):</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="Images/B17595_08_08.jpg" alt="Figure 8.8 – Lambda interactions on the edge&#13;&#10;" width="1650" height="1166"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – Lambda interactions on the edge</p>
			<p>Here, you can see that Lambda provides some distinct value propositions out of the box that you have to build yourself with native processes.</p>
			<h4>Challenges with Lambda on the edge </h4>
			<p>As you have <a id="_idIndexMarker781"/>understood by now, every solution or architecture has a trade-off. AWS Lambda is not an exception either and can have the following challenges:</p>
			<ul>
				<li><em class="italic">Lambda functions can be resource-intensive</em> compared to native processes. This is because they require additional libraries.</li>
				<li><em class="italic">Lambda functions are AWS only</em>. Thus, if you are looking to develop a cloud-agnostic edge solution (to mitigate vendor lock-in concerns), you may need to stick to native processes or Docker containers. Although Greengrass v2, as an edge software, is open source, AWS Lambda functions are not.</li>
			</ul>
			<p>Now, let's understand containers for the edge.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor171"/>Containers for the edge </h2>
			<p>A <strong class="bold">container</strong> is a unit of software that packages the necessary code with the required dependencies for the <a id="_idIndexMarker782"/>application to run reliably across different computing environments. Essentially, a container provides an abstraction layer to its hosted applications from the underlying <em class="italic">OS</em> (such as Ubuntu, Linux, or Windows) or <em class="italic">architecture</em> (such as x86 or ARM). In addition, since containers are lightweight, a single server or a virtual machine can run multiple containers. For example, you can run a <em class="italic">3-tier architecture</em> (web, app, and a database) on the same server (or VM) using their respective <a id="_idIndexMarker783"/>container images. The two most popular open source frameworks <a id="_idIndexMarker784"/>for container management are <strong class="bold">Docker</strong> and <strong class="bold">Kubernetes</strong>.</p>
			<p>In this section, we will primarily discuss Docker as it's the only option that's supported natively by AWS IoT Greengrass at the time of writing. Similar to Lambda, Docker supports an exhaustive set of programming languages and toolchains for the developers to develop, operate, and deploy their applications in an agile fashion. The following diagram shows the reference architecture for a Docker-based workload deployed on AWS IoT Greengrass:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="Images/B17595_08_09.jpg" alt="Figure 8.9 – Docker abstraction layers&#13;&#10;" width="523" height="385"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Docker abstraction layers</p>
			<p>So, why run containers over Lambda on the edge?</p>
			<p>Containers can bring all of the benefits that Lambda does (and more), along with being heterogeneous (different platforms), open source, and better optimized for edge resources. Containers have <a id="_idIndexMarker785"/>a broader developer community as well. Since containers have an orchestration and abstraction layer, it's not dependent on other runtimes such as AWS IoT Greengrass. So, if your organization decides to move away to another edge solution, containers are more portable than Lambda functions.</p>
			<h3>Benefits of Docker containers on AWS IoT Greengrass</h3>
			<p>Running containers <a id="_idIndexMarker786"/>at the edge using Greengrass has the following benefits:</p>
			<ul>
				<li>Developers can continue <a id="_idIndexMarker787"/>to use their existing CI/CD pipelines and store artifacts (that is, <strong class="bold">Docker images</strong>) in different code repositories <a id="_idIndexMarker788"/>such as <strong class="bold">Amazon Elastic Container Registry</strong> (<strong class="bold">ECR</strong>), the public Docker Hub, the public Docker Trusted Registry, or an S3 bucket.</li>
				<li>Greengrass <a id="_idIndexMarker789"/>simplifies deploying to the edge, with the only dependency being having the <strong class="bold">Docker image URI</strong> in the configuration. In <a id="_idIndexMarker790"/>addition, Greengrass also offers a native Docker application manager component (<strong class="source-inline">aws.greengrass.DockerApplicationManager</strong>) that enables Greengrass to manage credentials and download images from the supported repositories. </li>
				<li>Greengrass offers first-class support for Docker utilities such as <strong class="source-inline">docker-compose</strong>, <strong class="source-inline">docker run</strong>, and <strong class="source-inline">docker load</strong>, all of which can be included as dependencies in the recipe file for the component or can be used separately for testing or monitoring purposes.</li>
				<li>Finally, Greengrass also supports inter-process communication between Docker-based applications and other components. </li>
			</ul>
			<p>The following diagram shows how containerized applications can be developed using a CI/CD approach and be deployed on the edge while running AWS IoT Greengrass:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="Images/B17595_08_10.jpg" alt="Figure 8.10 – CI/CD approach for Docker workloads&#13;&#10;" width="1644" height="556"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.10 – CI/CD approach for Docker workloads</p>
			<p>Next, let's learn <a id="_idIndexMarker791"/>about the challenges with Docker on the edge.</p>
			<h3>Challenges with Docker on the edge </h3>
			<p>Running containers <a id="_idIndexMarker792"/>on the edge has some tradeoffs that need to be considered, as follows:</p>
			<ul>
				<li>Managing containers at scale on the edge brings more operational overhead as it can become complex. Thus, it requires careful designing, planning, and monitoring.</li>
				<li>As you build sophisticated edge applications with private and public Docker images, you are increasing the surface area for attacks as well. Thus, adhering to various operational and security best practices at all times is quintessential.</li>
				<li>In addition to AWS IoT Greengrass-specific updates, you need to have a patching and maintenance routine for Docker-specific utilities as well, which, in turn, increases the operational overhead and network charges.</li>
				<li>An additional layer of abstraction with containers may not be a fit for latency-sensitive use cases. For example, performing ML inferencing on GPUs for time-sensitive actions such as detecting an intrusion in your home through computer vision may run better as a native process over a container.</li>
			</ul>
			<p>In the lab section of this chapter, you will get your hands dirty by deploying a Docker-based application to the edge using AWS IoT Greengrass.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor172"/>Additional toolsets for Greengrass deployments</h2>
			<p>Similar to other AWS services, AWS IoT Greengrass also supports integration with various IaC <a id="_idIndexMarker793"/>solutions such as <strong class="bold">CloudFormation</strong>, <strong class="bold">CDK</strong>, and <strong class="bold">Terraform</strong>. All these tools can help you create cloud-based resources <a id="_idIndexMarker794"/>and integrate <a id="_idIndexMarker795"/>with different CI/CD pipelines for supporting <a id="_idIndexMarker796"/>cloud-to-edge deployments.</p>
			<p>Now that you are familiar with the benefits and tradeoffs of the DevOps toolchain, let's learn how that extends to machine learning.</p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor173"/>MLOps at the edge </h1>
			<p><strong class="bold">Machine Learning Operations</strong> (<strong class="bold">MLOps</strong>) aims to integrate agile methodologies into the end-to-end process <a id="_idIndexMarker797"/>of running machine <a id="_idIndexMarker798"/>learning workloads. MLOps brings together best practices from data science, data engineering, and DevOps to streamline model design, development, and delivery across the <strong class="bold">machine learning development life cycle</strong> (<strong class="bold">MLDLC</strong>).</p>
			<p>As per MLOps <strong class="bold">special interest group</strong> (<strong class="bold">SIG</strong>), MLOps is defined as "<em class="italic">The extension of the DevOps methodology to include machine learning and data science assets as first-class citizens within the DevOps ecology.</em>" MLOps has gained rapid momentum in the last few years from ML practitioners and is a language-, framework-, platform-, and infrastructure-agnostic practice.</p>
			<p>The following diagram shows the virtuous cycle of the MLDLC:</p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="Images/B17595_08_11.jpg" alt="Figure 8.11 – MLOps workflow&#13;&#10;" width="940" height="693"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.11 – MLOps workflow</p>
			<p>The preceding <a id="_idIndexMarker799"/>diagram shows how <strong class="bold">Operations</strong> is a fundamental block of the ML workflow. We introduced some of the concepts of ML design and development in <a href="B17595_07_Final_SS_ePub.xhtml#_idTextAnchor138"><em class="italic">Chapter 7</em></a>, <em class="italic">Machine Learning Workloads at the Edge</em>, so in this section, we will primarily focus on the <strong class="bold">Operations</strong> layer.</p>
			<p>There are <a id="_idIndexMarker800"/>several benefits of MLOps, as follows:</p>
			<ul>
				<li><strong class="bold">Productive</strong>: Data, ML engineers, and data scientists can use self-service environments to iterate faster with curated datasets and integrated ML tools. </li>
				<li><strong class="bold">Repeatable</strong>: Similar to DevOps, bringing automation to all aspects of the ML development life cycle (that is, MLDC) reduces human error and improves efficiency. MLOps helps ensure a repeatable process to help version, build, train, deploy, and operate ML models.</li>
				<li><strong class="bold">Reliable</strong>: Incorporating CI/CD practices into the MLDC adds to the quality and consistency of the deployments.</li>
				<li><strong class="bold">Auditable</strong>: Enabling capabilities such as versioning of all inputs and outputs, ranging from source data to trained models, allows for end-to-end traceability and observability of the ML workload.</li>
				<li><strong class="bold">Governance</strong>: Implementing governance practices to enforce policies helps to guard against model bias and track changes to data lineage and model quality over time.</li>
			</ul>
			<p>So, now that you understand what MLOps is, are you curious to know how it's related to IoT and the edge? Let's take a look.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor174"/>Relevance of MLOps for IoT and the edge</h2>
			<p>As an IoT/edge SME, you will <em class="italic">NOT</em> be owning the MLOps process. Rather, you need to ensure that the <a id="_idIndexMarker801"/>dependencies are met on <a id="_idIndexMarker802"/>the edge (at the hardware and software layer) for the ML engineers to perform their due diligence in setting up and maintaining this workflow. Thus, don't be surprised by the brevity of this section, as our goal is to only introduce you to the fundamental concepts and the associated services available today on AWS for this subject area. We hope to give you a quick ramp-up so that you are adept at having better conversations with ML practitioners in your organization.</p>
			<p>So, with that background, let's consider the scenario where the sensors from the connected HBS hub are reporting various anomalies from different customer installations. This is leading to multiple technician calls and thereby impacting the customer <a id="_idIndexMarker803"/>experience and bottom line. Thus, your <a id="_idIndexMarker804"/>CTO has decided to build a <em class="italic">predictive maintenance solution</em> using ML models to rapidly identify and fix faults through remote operations. The models should be able to identify data drift dynamically and collect additional information around the reported anomalies. Thus, the goal for ML practitioners here is to build an MLOps workflow so that models can be frequently and automatically trained on the collected data, followed by deploying it to the connected HBS hub. </p>
			<p>In addition, it's essential to monitor the performance of the ML models that are deployed on the edge to understand their efficiency; for example, to see how many false positives are being generated. Similar to the DevOps workflow, the ML workflow includes different components such as source control for versioning, a training pipeline for CI/CD, testing for model validation, packaging for deployment, and monitoring for assessing efficiency. If this project is a success, it will help the company add more ML intelligence to the edge and mitigate issues predictively to improve customer experience and reduce costs. The following reference architecture depicts a workflow we can use to implement the predictive maintenance of ML models on AWS IoT Greengrass v2:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="Images/B17595_08_12.jpg" alt="Figure 8.12 – Predictive maintenance of HBS sensors&#13;&#10;" width="1274" height="619"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.12 – Predictive maintenance of HBS sensors</p>
			<p>If we want <a id="_idIndexMarker805"/>to implement the preceding <a id="_idIndexMarker806"/>architecture, we must try to foresee some common challenges.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor175"/>MLOps challenges for the edge</h2>
			<p>Quite <a id="_idIndexMarker807"/>often, the most common questions that are asked by edge and ML practitioners related to MLOps are as follows:</p>
			<ul>
				<li>How do I prepare and deploy ML models to edge devices at scale? </li>
				<li>How do I secure the models (being intellectual property) once they've been deployed at the edge? </li>
				<li>How do I monitor the ML models operating at the edge and retrain them when needed?</li>
				<li>How do I eliminate the need for installing resource-intensive runtimes such as TensorFlow and PyTorch?</li>
				<li>How do I <a id="_idIndexMarker808"/>interface one or more models with my edge applications using a standard interface? </li>
				<li>How do I automate all these tasks so that there is a repeatable, efficient mechanism in place?</li>
			</ul>
			<p>This is not an exhaustive list as it continues to expand with ML being adopted more and more on the edge. The answers to some of those questions are a mix of cultural and technical shifts within an organization. Let's look at some examples:</p>
			<ul>
				<li><strong class="bold">Communication is key</strong>: For MLOps to generate the desired outcomes, communication and collaboration across different stakeholders are key. Considering ML projects involve a different dimension of technology related to algorithms and mathematical models, the ML practitioners often speak a different technical language than traditional IT (or IoT) folks.<p>Thus, becoming an ML organization requires time, training, and co-development exercises to be completed across different teams to produce fruitful results.</p></li>
				<li><strong class="bold">Decoupling and recoupling</strong>: Machine learning models have life cycles that are generally independent of other distributed systems. This decoupling allows ML practitioners to focus on building their applications without being distracted by the rest.<p>At the same time, though, ML workflows have certain dependencies, such as on big data workflows or applications required for inferencing. This means that MLOps is a combination of a traditional CI/CD workflow and another workflow engine. This can often become tricky without a robust pipeline and the required toolsets.</p></li>
				<li><strong class="bold">Deployment can be tricky</strong>: According to Algorithmia's report, <em class="italic">2020 State of Enterprise Machine Learning</em>, "<em class="italic">Bridging the gap between ML model building and practical deployments is a challenging task</em>." There is a fundamental difference between building an ML model in a Jupyter notebook on a laptop or a cloud environment versus deploying that model into a production system that generates value. <p>With IoT, this problem acts as the force multiplier, as it's required to consider various <a id="_idIndexMarker809"/>optimization strategies for different hardware and runtimes before deploying the ML models. For example, in <a href="B17595_07_Final_SS_ePub.xhtml#_idTextAnchor138"><em class="italic">Chapter 7</em></a>, <em class="italic">Machine Learning Workloads at the Edge</em>, you learned how to optimize ML models using <strong class="bold">Amazon SageMaker Neo</strong> so that they can run efficiently in your working environment. </p></li>
				<li><strong class="bold">The environment matters</strong>: The ML models may need to run in offline conditions and thus are more susceptible to data drift due to the high rate of data from changing environments. For example, think of a scenario where your home has a power or water outage due to a natural disaster. Thus, your devices, such as the HVAC or water pumps, act in an anomalous way, leading to data drift for the locally deployed models. Thus, your locally deployed ML models need to be intelligent enough to handle different false positives in unexpected scenarios.</li>
			</ul>
			<p>We have gone through the MLOps challenges for the edge in this section. In the next section, we will understand the MLOps toolchain for the edge.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor176"/>Understanding the MLOps toolchain for the edge</h2>
			<p>In <a href="B17595_07_Final_SS_ePub.xhtml#_idTextAnchor138"><em class="italic">Chapter 7</em></a>, <em class="italic">Machine Learning Workloads at the Edge</em>, you learned how to develop ML models <a id="_idIndexMarker810"/>using Amazon SageMaker, optimize them through SageMaker Neo, and deploy them on the edge using AWS IoT Greengrass v2. In this <a id="_idIndexMarker811"/>chapter, I would like to introduce you to another service in the SageMaker family called <strong class="bold">Edge Manager</strong>, which can help address some of the preceding MLOps challenges and which provides the following capabilities out of the box:</p>
			<ul>
				<li>The ability to <a id="_idIndexMarker812"/>automate the build-train-deploy workflow from cloud to edge devices and trace the life cycle of each model.</li>
				<li>Automatically optimize ML models for deployment on a wide variety of edge devices that are powered by CPUs, GPUs, and embedded ML accelerators. </li>
				<li>Supports model <a id="_idIndexMarker813"/>compilation <a id="_idIndexMarker814"/>from different frameworks <a id="_idIndexMarker815"/>such as <strong class="bold">DarkNet</strong>, <strong class="bold">Keras</strong>, <strong class="bold">MXNet</strong>, <strong class="bold">PyTorch</strong>, <strong class="bold">TensorFlow</strong>, <strong class="bold">TensorFlow-Lite</strong>, <strong class="bold">ONNX</strong>, and <strong class="bold">XGBoost</strong>.</li>
				<li>Supports <a id="_idIndexMarker816"/>multi-modal <a id="_idIndexMarker817"/>hosting of ML models, along <a id="_idIndexMarker818"/>with simple API <a id="_idIndexMarker819"/>interfaces, for performing <a id="_idIndexMarker820"/>common queries such as loading, unloading, and running inferences on the models on a device. </li>
				<li>Supports open <a id="_idIndexMarker821"/>source remote procedure protocols (using <strong class="bold">gRPC</strong>), which <a id="_idIndexMarker822"/>allow you to integrate with existing edge applications <a id="_idIndexMarker823"/>through APIs <a id="_idIndexMarker824"/>in common programming <a id="_idIndexMarker825"/>languages, such <a id="_idIndexMarker826"/>as <strong class="bold">Android Java</strong>, <strong class="bold">C#</strong>/<strong class="bold">.NET</strong>, <strong class="bold">Go</strong>, <strong class="bold">Java</strong>, <strong class="bold">Python</strong>, and <strong class="bold">C</strong>.</li>
				<li>Offers a dashboard to monitor the performance of models running on different devices across the fleet. So, in the scenario explained earlier with a connected HBS hub, if issues related to model drift, model quality, or predictions are identified, these issues can be quickly visualized in a dashboard or reported through configured alerts.</li>
			</ul>
			<p>As you can see, Edge Manager brings robust capabilities to manage required capabilities for MLOps out of the box and brings native integrations with different AWS services, such as AWS IoT Greengrass. The following is a reference architecture for Edge Manager integrating with various other services that you were exposed to earlier in this book, such as SageMaker and S3:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="Images/B17595_08_13.jpg" alt="Figure 8.13 – Edge Manager reference architecture &#13;&#10;" width="1650" height="568"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.13 – Edge Manager reference architecture </p>
			<p class="callout-heading">Note</p>
			<p class="callout">MLOps is still emerging and can be complicated to implement without the involvement of ML practitioners. If you would like to learn more about this subject, please refer to other books that have been published on this subject.</p>
			<p>Now that you have <a id="_idIndexMarker827"/>learned the fundamentals of DevOps and MLOps, let's get our hands dirty so that we can apply some of these practices and operate edge workloads in an agile fashion.</p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor177"/>Hands-on with the DevOps architecture</h1>
			<p>In this section, you will learn how to deploy multiple Docker applications to the edge that have already been <a id="_idIndexMarker828"/>developed using CI/CD best practices in the cloud. These container images are available in a <strong class="bold">Docker repository</strong> called <strong class="bold">Docker Hub</strong>. The following <a id="_idIndexMarker829"/>diagram shows the architecture for this <a id="_idIndexMarker830"/>hands-on exercise, where you will complete <em class="italic">Steps 1</em> and<em class="italic"> 2</em> to integrate the HBS hub with an existing CI/CD pipeline (managed by your DevOps org), configure the Docker containers, and then deploy and validate them so that they can operate at the edge:</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="Images/B17595_08_14.jpg" alt="Figure 8.14 – Hands-on DevOps architecture &#13;&#10;" width="1109" height="352"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.14 – Hands-on DevOps architecture </p>
			<p>The following are <a id="_idIndexMarker831"/>the services you will use in this exercise:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="Images/B17595_08_Table2.jpg" alt="Figure 8.15 – Services for this exercise&#13;&#10;" width="1643" height="245"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.15 – Services for this exercise</p>
			<p>Your objectives for this hands-on section are as follows:</p>
			<ul>
				<li>Deploy container images from Docker Hub to AWS IoT Greengrass.</li>
				<li>Confirm that the containers are running.</li>
			</ul>
			<p>Let's get started.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor178"/>Deploying the container from the cloud to the edge</h2>
			<p>In this section, you will <a id="_idIndexMarker832"/>learn how to deploy a pre-built container from the cloud to the edge:</p>
			<ol>
				<li>Navigate to your device's terminal and confirm that Docker is installed:<p class="source-code"><strong class="bold">cd hbshub/artifacts</strong></p><p class="source-code"><strong class="bold">docker –-version</strong></p><p class="source-code"><strong class="bold">docker-compose –-version</strong></p><p>If Docker Engine and <strong class="source-inline">docker-compose</strong> are not installed, please refer to the documentation from Docker for your respective platform to complete this before proceeding.</p></li>
				<li>Now, let's review the <strong class="source-inline">docker-compose</strong> file. If you have not used <strong class="source-inline">docker-compose</strong> before, then note that it is a utility that's used for defining and running multi-container applications. This tool requires a file called <strong class="source-inline">docker-compose.yaml</strong> that lists the configuration details for application services to be installed and their dependencies.</li>
				<li>Please review the <strong class="source-inline">docker-compose.yaml</strong> file in the <strong class="source-inline">artifacts</strong> folder of this chapter. It includes three container images from Docker Hub corresponding to the web, application, and database tiers:<p class="source-code"><strong class="bold">services:</strong></p><p class="source-code"><strong class="bold">  web:</strong></p><p class="source-code"><strong class="bold">    </strong><strong class="bold">image: "nginx:latest"</strong></p><p class="source-code"><strong class="bold">  app:</strong></p><p class="source-code"><strong class="bold">    image: "hello-world:latest"</strong></p><p class="source-code"><strong class="bold">  db:</strong></p><p class="source-code"><strong class="bold">  image: "redis:latest"</strong></p></li>
				<li>Navigate to the following working directory to review the Greengrass recipe file: <p class="source-code"><strong class="bold">cd ~/hubsub/recipes</strong></p><p class="source-code"><strong class="bold">nano com.hbs.hub.Container-1.0.0.yaml</strong></p></li>
				<li>Note that there is a dependency on the Greengrass-managed Docker application manager <a id="_idIndexMarker833"/>component. This component helps with retrieving container images from the respective repositories and executes Docker-related commands for installing and managing the life cycle of containers on the edge:<p class="source-code">---</p><p class="source-code">RecipeFormatVersion: '2020-01-25'</p><p class="source-code">ComponentName: com.hbs.hub.Container</p><p class="source-code">ComponentVersion: '1.0.0'</p><p class="source-code">ComponentDescription: 'A component that uses Docker Compose to run images from Docker Hub.'</p><p class="source-code">ComponentPublisher: Amazon</p><p class="source-code">ComponentDependencies:</p><p class="source-code">  aws.greengrass.DockerApplicationManager:</p><p class="source-code">    VersionRequirement: ~2.0.0</p><p class="source-code">Manifests:</p><p class="source-code">  - Platform:</p><p class="source-code">      os: all</p><p class="source-code">    Lifecycle:</p><p class="source-code">      Startup: </p><p class="source-code">        RequiresPrivilege: true</p><p class="source-code">        Script: |</p><p class="source-code">          cd {artifacts:path}</p><p class="source-code">          docker-compose up -d</p><p class="source-code">      Shutdown:</p><p class="source-code">        RequiresPrivilege: true</p><p class="source-code">        Script: |</p><p class="source-code">          cd {artifacts:path}</p><p class="source-code">          docker-compose down</p></li>
				<li>Now that we have the updated <strong class="source-inline">docker-compose</strong> file and the Greengrass component recipe, let's create a local deployment:<p class="source-code"><strong class="bold">sudo /greengrass/v2/bin/greengrass-cli deployment create</strong></p><p class="source-code"><strong class="bold">--recipeDir ~/hbshub/recipes --artifactDir</strong></p><p class="source-code"><strong class="bold">~/hbshub/artifacts --merge "com.hbs.hub.Container=1.0.0"</strong></p></li>
				<li>Verify <a id="_idIndexMarker834"/>that the component has been successfully deployed (and is running) using the following command:<p class="source-code"><strong class="bold">sudo /greengrass/v2/bin/greengrass-cli component list</strong></p></li>
				<li>To test which containers are currently running, run the following command:<p class="source-code"><strong class="bold">docker container ls</strong></p><p>You should see the following output:</p></li>
			</ol>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="Images/B17595_08_16.jpg" alt="Figure 8.16 – Running container processes&#13;&#10;" width="1383" height="132"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.16 – Running container processes</p>
			<p>In our example here, the app (<strong class="source-inline">hello-world</strong>) is a one-time process, so it has already been completed. But the remaining two containers are still up and running. If you want to check all the container processes that have run so far, use the following command:</p>
			<p class="source-code"><strong class="bold">docker ps -a</strong></p>
			<p>You should see the following output:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="Images/B17595_08_17.jpg" alt="Figure 8.17 – All container processes&#13;&#10;" width="1375" height="150"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.17 – All container processes</p>
			<p>Congratulations – you now have multiple containers successfully deployed on the edge from <a id="_idIndexMarker835"/>a Docker repository (Docker Hub). In the real world, if you want to run a local web application on the HBS hub, this pattern can be useful. </p>
			<p class="callout-heading">Challenge zone (Optional)</p>
			<p class="callout">Can you figure out how to deploy a Docker image from Amazon ECR or Amazon S3? Although Docker Hub is useful for storing public container images, enterprises will often use a private repository for their home-grown applications. </p>
			<p class="callout">Hint: You need to make changes to <strong class="source-inline">docker-compose</strong> with the appropriate URI for the container images and provide the required permissions to the Greengrass role. </p>
			<p>With that, you've learned how to deploy any number of containers on the edge (so long as the hardware resource permits it) from heterogeneous sources to develop a multi-faceted architecture on the edge. Let's wrap up this chapter with a quick summary and a set of knowledge check questions. </p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor179"/>Summary </h1>
			<p>In this chapter, you were introduced to the DevOps and MLOps concepts that are required to bring operational efficiency and agility to IoT and ML workloads at the edge. You also learned how to deploy containerized applications from the cloud to the edge. This functionality allowed you to build an intelligent, distributed, and heterogeneous architecture on the Greengrass-enabled HBS hub. With this foundation, your organization can continue to innovate with different kinds of workloads, as well as deliver features and functionalities to the end consumers throughout the life cycle of the product. In the next chapter, you will learn about the best practices of scaling IoT operations as your customer base grows from thousands to millions of devices globally. Specifically, you will learn about the different techniques surrounding fleet provisioning and fleet management that are supported by AWS IoT Greengrass.</p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor180"/>Knowledge check</h1>
			<p>Before moving on to the next chapter, test your knowledge by answering these questions. The answers can be found at the end of the book:</p>
			<ol>
				<li value="1">What strategy would you implement to bring agility in developing IoT workloads faster?</li>
				<li>True or false: DevOps is a set of tools to help developers and operations collaborate faster.</li>
				<li>Can you recall at least two challenges of implementing DevOps with IoT workloads?</li>
				<li>What services should you consider when designing a DevOps workflow from the cloud to the edge?</li>
				<li>True or false: Running native containers and AWS Lambda functions on the edge both offer similar benefits.</li>
				<li>Can you think of at least three benefits of using MLOps with IoT workloads?</li>
				<li>What are the different phases of an MLOps workflow?</li>
				<li>True or false: The MLOps toolchain for the edge is limited to a few frameworks and programming languages. </li>
				<li>What service is available from AWS for performing MLOps on the edge?</li>
			</ol>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor181"/>References</h1>
			<p>For more information regarding the topics that were covered in this chapter, take a look at the following resources:</p>
			<ul>
				<li>DevOps and AWS: <a href="https://aws.amazon.com/devops/%0D">https://aws.amazon.com/devops/</a></li>
				<li>Infrastructure as Code with AWS CloudFormation: <a href="https://aws.amazon.com/cloudformation/%0D">https://aws.amazon.com/cloudformation/</a></li>
				<li>Developing an IoT-MLOps workflow on AWS Using Edge Manager: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/edge-greengrass.html%0D">https://docs.aws.amazon.com/sagemaker/latest/dg/edge-greengrass.html</a></li>
				<li>CRISP-ML Methodology with Quality Assurance: <a href="https://arxiv.org/pdf/2003.05155.pdf%0D">https://arxiv.org/pdf/2003.05155.pdf</a></li>
				<li>Machine Learning Operations: <a href="https://ml-ops.org/%0D">https://ml-ops.org/</a></li>
				<li>Overview of Docker: <a href="https://docs.docker.com/get-started/overview/%0D">https://docs.docker.com/get-started/overview/</a></li>
				<li>Different Ways of Running Dockerized Applications on AWS IoT Greengrass: <a href="https://docs.aws.amazon.com/greengrass/v2/developerguide/run-docker-container.html">https://docs.aws.amazon.com/greengrass/v2/developerguide/run-docker-container.html</a></li>
				<li>Special interest group: <a href="https://github.com/cdfoundation/sig-mlops">https://github.com/cdfoundation/sig-mlops</a></li>
			</ul>
		</div>
	</div></body></html>