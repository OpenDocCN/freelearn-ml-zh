<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Feature-Based Object Detection</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Feature-Based Object Detection</h1>
                </header>
            
            <article>
                
<p><span class="author-d-iz88z86z86za0dz67zz78zz78zz74zz68zjz80zz71z9iz90za3mz82z45adyt4z89zz86zz79zblz67zdy11z83zxz88zcz79zfz84zz76z1z85z">In the previous chapter, we understood the importance of and how to model deep layered feature extraction using <strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>). In this chapter, we will learn how to model a CNN to detect where the object in the image is and also classify the object in one of our pre-decided categories.&#160;</span></p>
<p>In this chapter:</p>
<ul>
<li>We will begin with a general discussion on image recognition and what is object detection</li>
<li>A working example of the popular techniques for face detection using OpenCV</li>
<li>Object detection using two-stage models such as Faster-RCNN</li>
<li>Object detection using one-stage model such as SSD</li>
<li>The major part of this chapter will be discussing deep learning-based object detectors and explaining them using a code for the demo</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to object detection</h1>
                </header>
            
            <article>
                
<p>To begin with object detection, we will first see an overview of image recognition as detection is one part of it. In the following figure, an overview of object recognition is described using an image from <kbd>Pascal VOC</kbd> dataset. The input is passes through a model which then produces information in four different styles:</p>
<div class="CDPAlignCenter CDPAlign"><img height="295" width="511" class="alignnone size-full wp-image-346 image-border" src="images/e87720aa-caa0-4632-b7b7-633f61ec8f82.png"/></div>
<p>The model in the previous image performs generic image recognition where we can predict the following&#160; information:</p>
<ul>
<li>A class name for the object in the image</li>
<li>Object center pixel location</li>
<li>A bounding box surrounding the object as output</li>
<li>In instance image where each pixel is classified into a class. The classes are for object as well as background</li>
</ul>
<p>When we say object detection, we are usually referring to the first and third type of image recognition. Our goal is to estimate class names as well as bounding box surrounding target objects. Before we begin our discussion on object detection techniques, in the next section we shall see why detecting objects is a difficult computer vision task.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Challenges in object detection</h1>
                </header>
            
            <article>
                
<p>In the past, several approaches for object detection were proposed. However, these either perform well in a controlled environment or look for special objects in images like a human face. Even in the case of faces, the approaches suffer from issues like low light conditions, a highly occluded face or tiny face size compared to the image size.&#160;</p>
<p><span class=" author-d-iz88z86z86za0dz67zz78zz78zz74zz68zjz80zz71z9iz90za3mz82z45adyt4z89zz86zz79zblz67zdy11z83zxz88zcz79zfz84zz76z1z85z">Following are several challenges that are faced by an object detector in real-world applications:&#160;</span></p>
<ul class="listtype-bullet listindent1 list-bullet1">
<li>
<p><span class=" author-d-iz88z86z86za0dz67zz78zz78zz74zz68zjz80zz71z9iz90za3mz82z45adyt4z89zz86zz79zblz67zdy11z83zxz88zcz79zfz84zz76z1z85z"><strong>Occlusion</strong></span>: <span class=" author-d-iz88z86z86za0dz67zz78zz78zz74zz68zjz80zz71z9iz90za3mz82z45adyt4z89zz86zz79zblz67zdy11z83zxz88zcz79zfz84zz76z1z85z">Objects like dogs or cats can be hidden behind one another, as a result, the features that can be extracted from them are not strong enough to say that they are an object.&#160;</span></p>
</li>
<li>
<p><span class=" author-d-iz88z86z86za0dz67zz78zz78zz74zz68zjz80zz71z9iz90za3mz82z45adyt4z89zz86zz79zblz67zdy11z83zxz88zcz79zfz84zz76z1z85z"><strong>Viewpoint changes</strong></span>: I<span class=" author-d-iz88z86z86za0dz67zz78zz78zz74zz68zjz80zz71z9iz90za3mz82z45adyt4z89zz86zz79zblz67zdy11z83zxz88zcz79zfz84zz76z1z85z">n cases of different viewpoints of an object, the shape may change drastically and hence the features of the object will also change drastically. This causes a detector which is trained to see a given object from one viewpoint to fail on seeing it from other viewpoints. For example, in the case of person detection, if the detector is looking for a head, hands, and legs combination to find a person, will fail if we put the camera overhead to take vertical downward facing images. The only thing that the detector will see are heads and hence the results are drastically reduced.</span></p>
</li>
<li>
<p><span class=" author-d-iz88z86z86za0dz67zz78zz78zz74zz68zjz80zz71z9iz90za3mz82z45adyt4z89zz86zz79zblz67zdy11z83zxz88zcz79zfz84zz76z1z85z"><strong>Variation in sizes</strong></span>: <span class=" author-d-iz88z86z86za0dz67zz78zz78zz74zz68zjz80zz71z9iz90za3mz82z45adyt4z89zz86zz79zblz67zdy11z83zxz88zcz79zfz84zz76z1z85z">The same object can be far from a camera or near. As a result, the size of objects varies. The detector is therefore required to be size invariant as well as rotation invariant.&#160;</span></p>
</li>
<li>
<p><span class=" author-d-iz88z86z86za0dz67zz78zz78zz74zz68zjz80zz71z9iz90za3mz82z45adyt4z89zz86zz79zblz67zdy11z83zxz88zcz79zfz84zz76z1z85z"><strong>Non-rigid objects</strong></span>: <span class=" author-d-iz88z86z86za0dz67zz78zz78zz74zz68zjz80zz71z9iz90za3mz82z45adyt4z89zz86zz79zblz67zdy11z83zxz88zcz79zfz84zz76z1z85z">If the shape of the object splits into parts or there is a fluid object, it becomes even more challenging to describe them using features.&#160;</span></p>
</li>
<li>
<p><span class=" author-d-iz88z86z86za0dz67zz78zz78zz74zz68zjz80zz71z9iz90za3mz82z45adyt4z89zz86zz79zblz67zdy11z83zxz88zcz79zfz84zz76z1z85z"><strong>Motion-blur</strong></span>: <span class=" author-d-iz88z86z86za0dz67zz78zz78zz74zz68zjz80zz71z9iz90za3mz82z45adyt4z89zz86zz79zblz67zdy11z83zxz88zcz79zfz84zz76z1z85z">If we are detecting a moving body like a car, there might be cases where the camera captured image is blurred. This is another challenge for the object detectors, to provide a correct estimation, and making a detector robust is crucial when deployed in moving robots like self-driving cars or drones.</span></p>
</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dataset and libraries used</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be using TensorFlow (v1.4.0) and OpenCV as our main library for detection. We show results on custom images. However, any colored image can be used as input for various models. Wherever required, there are links to pre-trained model files in the sections.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Methods for object&#160;detection</h1>
                </header>
            
            <article>
                
<p>Object detection is the problem<span>&#160;of two steps. First, it should localize an object or multiple objects inside an image. Secondly, it gives out a predicted class for each of the localized objects. T</span>here have been several object detection methods that use a sliding window-based approach. One of the popular detection techniques is face detection approach, developed by Viola and Jones[1]. The paper exploited the fact that the human face has strong descriptive features such as regions near eyes which are darker than near the mouth. So there may be a significant difference between the rectangle area surrounding the eyes with respect to the rectangular area near the nose. Using this as one of the several pre-defined patterns of rectangle pairs, their method computed area difference between rectangles in each pattern.</p>
<p>Detecting faces is a two-step process:</p>
<ul>
<li>First is to create a classifier with parameters for specific object detection. In our case, it is face detection:</li>
</ul>
<pre style="padding-left: 60px"><strong>face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')</strong></pre>
<ul>
<li>In second step, for each image, it face detection is done using previously loaded classifier parameters:</li>
</ul>
<pre style="padding-left: 60px"><strong>faces = face_cascade.detectMultiScale(gray)</strong></pre>
<p>In OpenCV we can code this to detect the face, shown as follows:</p>
<pre><strong>import numpy as np</strong><br/><strong>import cv2</strong><br/><br/><strong># create cascaded classifier with pre-learned weights</strong><br/><strong># For other objects, change the file here</strong><br/><strong>face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')</strong><br/><br/><strong>cap = cv2.VideoCapture(0)</strong><br/><br/><strong>while(True):</strong><br/><strong>    ret, frame = cap.read()</strong><br/><strong>    if not ret:</strong><br/><strong>        print("No frame captured")</strong><br/>    <br/><strong>    # frame = cv2.resize(frame, (640, 480))</strong><br/><strong>    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</strong><br/><br/><strong>    # detect face</strong><br/><strong>    faces = face_cascade.detectMultiScale(gray)</strong><br/><br/><strong>    # plot results</strong><br/><strong>    for (x,y,w,h) in faces:</strong><br/><strong>        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)</strong><br/><br/><strong>    cv2.imshow('img',frame)</strong><br/><strong>    if cv2.waitKey(1) &amp; 0xFF == ord('q'):</strong><br/><strong>        break</strong><br/><br/><strong>cap.release()</strong><br/><strong>cv2.destroyAllWindows()</strong></pre>
<p>Here, we used a file&#160;<kbd>haarcascade_frontalface_default.xml</kbd> which contains classifier parameters available at <a href="https://github.com/opencv/opencv/tree/master/data/haarcascades" target="_blank">https://github.com/opencv/opencv/tree/master/data/haarcascades</a>. We have to download these cascade classifier files in order to run face detection. Also for detecting other objects like eyes, smiles, and so on, we require similar files for use with OpenCV.&#160;</p>
<p>The preceding face detector we saw became popular in several devices ranging from smartphones to digital cameras. However, recent advances in deep learning are creating better face detectors. We will see this in the next few sections on deep learning-based general object detectors.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep learning-based object detection</h1>
                </header>
            
            <article>
                
<p>With recent advancements in CNNs and their performance in image classification, it was becoming intuitive to use the similar model style for&#160;object detection. This has been proven right, as in the last few years there are better object detectors proposed every year which increases overall accuracy on standard benchmarks. Some of the styles of detectors are already in use in smartphones, robot self-driving cars, and so on.&#160;</p>
<p>A generic CNN&#160;outputs class probabilities, as in the case of image recognition. But in order to detect objects, these must be modified to output both the class probability as well as bounding box rectangle coordinates and shape. Early CNN-based object detection, computes possible windows from an input image and then computes features using a CNN model for each window. This output of the CNN feature extractor will then tell us if the chosen window is the target object or not. This is slow due to a large computation of each window through the CNN feature extractor. Intuitively, we would like to extract features from images and use those features for object detection. This not only enhances speed for detection but also filters unwanted noise in the image.&#160;</p>
<p>There have been several methods proposed to tackle such issues of speed and accuracy in object detection. These are in general divided into two major categories:&#160;</p>
<ul>
<li><strong>Two-stage detectors</strong>: Here, the overall process is divided into two major steps, hence the name two-stage detectors. The most popular among these is <strong>Faster R-CNN</strong>. In the next section, we will see a detailed explanation of this method.</li>
<li><strong>One-stage detectors</strong>: While two-stage detectors increased accuracy for detection, they were still hard to train and they were slower for several real-time operations. One-stage detectors rectified these issues by making a network in single architecture which predicts faster. One of the popular models of this style is <strong>Single Shot Multibox Detector</strong> (<strong>SSD</strong>).</li>
</ul>
<p>In the following sections, we will see both of these types of detectors with a demo that shows the quality of results from each.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Two-stage detectors</h1>
                </header>
            
            <article>
                
<p>As CNN show their performance in general image classification, researchers used the same CNNs to do better object detection. The initial approaches using deep learning for object detection can be described as two-stage detectors and one of the popular ones is Faster R-CNN by Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun 2015 <a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank">https://arxiv.org/pdf/1506.01497.pdf</a>.</p>
<p>The method is divided into two stages:</p>
<ol>
<li>In the first stage, the features are extracted from an image and&#160;<strong>Region of Interests</strong> (<strong>ROI</strong>) are proposed. ROIs consists of a possible box where an object might be in the image.&#160;</li>
<li>The second stage uses features and ROIs to compute final bounding boxes and class probabilities for each of the boxes.&#160;These together constitute the final output.&#160;</li>
</ol>
<p>An overview of Faster-RCNN is as shown in the following figure. An input image is used to extract features and a region proposals. These extracted features and proposals are used together to compute predicted bounding boxes and class probabilities for each box:&#160;</p>
<div class="CDPAlignCenter CDPAlign"><img height="214" width="382" src="images/f61d607d-20c7-4394-9ed3-c8ba39a8e4d0.png"/></div>
<p><span>As shown in the previous figure, overall method is considered two-stage because during training the model will first learn to produce ROIs using a sub-model called <strong>Region Proposal Network (RPN)</strong>. It will then learn to produce correct class probabilities and bounding box locations using ROIs and features. An overview of RPN is as shown in the following figure . RPN layer uses feature layer as input creates a proposal for bounding boxes and corresponding probabilities:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="221" width="394" src="images/bcfcf94d-01e0-433a-ad98-d22603384731.png"/></div>
<p><span>The bounding box locations are usually normalized&#160;values for the top left coordinate of the box with width and height values, though this can change depending on the way the model is learnt. During prediction, the model outputs a set of class probabilities, class categories as well as the bounding box location in (x, y, w, h) format. This set is again passed through a threshold to filter out the bounding boxes with confidence scores less than the threshold.</span></p>
<p>The major advantage of using this style of the detector is that it gives better accuracy than one-stage detectors. These usually achieve state-of-the-art detection accuracy. However, they suffer from slower speeds during predictions. If for an application prediction, time plays a crucial role, then it is advised to either provide these networks with a high-performance system or use one-stage detectors. On the other hand, if the requirement is to get the best accuracy, it is highly recommended to use such a method for object detection. An&#160;example output of object detection is as shown in the following figure with the bounding box around detected objects. Each box has a label showing predicted class name and confidence for the box:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/fc24432c-3230-4ddb-a84e-27232070db7f.png"/></div>
<p>The detection in the previous screenshot uses Faster RCNN model and even for small objects, like a person on the right bottom, the model detects with a good confidence score. Overall detected objects are bus, car and person. The model doesn't detect other objects, such as trees, pole, traffic light, and so on because it has not been trained to detect those objects.&#160;&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Demo – Faster R-CNN with ResNet-101</h1>
                </header>
            
            <article>
                
<p>It can be seen from the previous screenshot that even in the case of varying object sizes and also objects with small sizes, the two-stage model of Faster R-CNN predicts accurately. Now, we will show how to run a similar prediction using TensorFlow. Let's begin by cloning a repository, as it will contain most of the required codes:</p>
<pre><strong>git clone https://github.com/tensorflow/models.git</strong><br/><strong>cd models/research</strong></pre>
<p class="mce-root">After we have cloned, we will set up the environment.&#160;We will first download a pre-trained model from TensorFlow <kbd>model-zoo</kbd>:&#160;</p>
<ul>
<li>For macOS X:</li>
</ul>
<pre style="padding-left: 60px"><strong>curl -O http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2017_11_08.tar.gz</strong><br/><strong>tar -xvf faster_rcnn_resnet101_coco_2017_11_08.tar.gz</strong></pre>
<ul>
<li>For Linux:&#160;</li>
</ul>
<pre style="padding-left: 60px"><strong>wget http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2017_11_08.gz</strong><br/><strong>tar -xvf faster_rcnn_resnet101_coco_2017_11_08.tar.gz</strong></pre>
<p>Keep the extracted folder by the name<span>&#160;</span><kbd>faster_rcnn_resnet101_coco_2017_11_08</kbd><span>&#160;</span>in<span>&#160;</span><kbd>models/research/object_detection</kbd><em>.&#160;</em>This completes the downloading of the pre-trained model.&#160;</p>
<p class="mce-root">These two steps have to be performed each time we launch a Terminal shell:</p>
<ul>
<li class="mce-root">At first, we will compile <kbd>protobuf</kbd> files, as TensorFlow uses them to serialize structured data:</li>
</ul>
<pre style="padding-left: 60px"><strong>protoc object_detection/protos/*.proto --python_out=.</strong></pre>
<ul>
<li>Also, run in the research folder:</li>
</ul>
<pre style="padding-left: 60px"><strong>export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim</strong> </pre>
<p class="mce-root">The environment and pre-trained models are set, now we will start with the prediction code. The following code stays and runs inside<span>&#160;</span><kbd>models/research/object_detection</kbd><span>&#160;</span>and the code style is like a Jupyter notebook. As we progress in this section, each of the further code blocks can be run inside a Jupyter notebook cell. If you are not familiar with Jupyter, you can still run complete Python scripts:</p>
<ol>
<li>Let's begin with loading libs that will be used here:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import numpy as np<br/>import os<br/>import sys<br/>import tensorflow as tf<br/>import cv2<br/>from matplotlib import pyplot as plt<br/># inside jupyter uncomment next line <br/># %matplotlib inline<br/>import random<br/>import time<br/>from utils import label_map_util </pre>
<ol start="2">
<li>In order to load a pre-trained model for prediction:</li>
</ol>
<pre style="padding-left: 60px"># load graph <br/>def load_and_create_graph(path_to_pb):<br/>    """<br/>    Loads pre-trained graph from .pb file. <br/>    path_to_pb: path to saved .pb file<br/>    Tensorflow keeps graph global so nothing is returned<br/>    """<br/>    with tf.gfile.FastGFile(path_to_pb, 'rb') as f:<br/>        # initialize graph definition<br/>        graph_def = tf.GraphDef()<br/>        # reads file <br/>        graph_def.ParseFromString(f.read())<br/>        # imports as tf.graph<br/>        _ = tf.import_graph_def(graph_def, name='')</pre>
<ol start="3">
<li>It can be used to load the model Faster R-CNN with the ResNet-101 feature extractor pre-trained on <kbd>MSCOCO</kbd>&#160;dataset:</li>
</ol>
<pre style="padding-left: 60px">load_and_create_graph('faster_rcnn_resnet101_coco_2017_11_08/frozen_inference_graph.pb')</pre>
<ol start="4">
<li>Now, let's set up labels to display in our figure using <kbd>MSCOCO</kbd> labels:</li>
</ol>
<pre style="padding-left: 60px"># load labels for classes output<br/>path_to_labels = os.path.join('data', 'mscoco_label_map.pbtxt')<br/># pre-training was done on 90 categories<br/>nb_classes = 90<br/>label_map = label_map_util.load_labelmap(path_to_labels)<br/>categories = label_map_util.convert_label_map_to_categories(label_map,<br/>                   max_num_classes=nb_classes, use_display_name=True)<br/>category_index = label_map_util.create_category_index(categories)</pre>
<ol start="5">
<li>Before final predictions, we will set up the utility function as:</li>
</ol>
<pre style="padding-left: 60px">def read_cv_image(filename):<br/>    """<br/>    Reads an input color image and converts to RGB order<br/>    Returns image as an array<br/>    """<br/>    img = cv2.imread(filename)<br/>    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/>    return img</pre>
<ol start="6">
<li>Following is utility function to display bounding boxes using <kbd>matplotib</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><br/>def show_mpl_img_with_detections(img, dets, scores, <br/>                                 classes, category_index, <br/>                                 thres=0.6):<br/>    """<br/>    Applies thresholding to each box score and <br/>    plot bbox results on image. <br/>    img: input image as numpy array<br/>    dets: list of K detection outputs for given image.(size:[1,K])<br/>    scores: list of detection score for each detection output(size: [1,K]).<br/>    classes: list of predicted class index(size: [1,K]) <br/>    category_index: dictionary containing mapping from class index to class name. <br/>    thres: threshold to filter detection boxes:(default: 0.6)<br/>    By default K:100 detections<br/>    """<br/>    # plotting utilities from matplotlib<br/>    plt.figure(figsize=(12,8))<br/>    plt.imshow(img)<br/>    height = img.shape[0]<br/>    width = img.shape[1]<br/>    # To use common color of one class and different for different classes<br/>    colors = dict() <br/>    # iterate over all proposed bbox <br/>    # choose whichever is more than a threshold<br/>    for i in range(dets.shape[0]):<br/>        cls_id = int(classes[i])<br/>        # in case of any wrong prediction for class index<br/>        if cls_id &gt;= 0:<br/>            <br/>            score = scores[i]<br/>            # score for a detection is more than a threshold<br/>            if score &gt; thres:<br/>                if cls_id not in colors:<br/>                    colors[cls_id] = (random.random(), <br/>                                      random.random(), <br/>                                      random.random())<br/>                xmin = int(dets[i, 1] * width)<br/>                ymin = int(dets[i, 0] * height)<br/>                xmax = int(dets[i, 3] * width)<br/>                ymax = int(dets[i, 2] * height)<br/>                rect = plt.Rectangle((xmin, ymin), xmax - xmin,<br/>                                     ymax - ymin, fill=False,<br/>                                     edgecolor=colors[cls_id],<br/>                                     linewidth=2.5)<br/>                plt.gca().add_patch(rect)<br/>                # to plot class name and score around each detection box<br/>                class_name = str(category_index[cls_id]['name'])<br/>                <br/>                plt.gca().text(xmin, ymin - 2,<br/>                           '{:s} {:.3f}'.format(class_name, score),<br/>                           bbox=dict(facecolor=colors[cls_id], alpha=0.5),<br/>                           fontsize=8, color='white')<br/>    plt.axis('off')<br/>    plt.show()<br/><br/>    return</pre>
<p>Using this setup, we can do predictions on the input image.&#160;<span>In the following snippet, we are doing predictions on the input image as well as displaying the results. We will launch a <kbd>Tensorflow</kbd> session and run the graph in <kbd>sess.run</kbd> to compute bounding boxes, scores for each box, the class prediction for boxes and number of detections:</span></p>
<pre>image_dir = 'test_images/'<br/># create graph object from previously loaded graph<br/># tensorflow previously loaded graph as default <br/>graph=tf.get_default_graph()<br/><br/># launch a session to run this graph <br/>with tf.Session(graph=graph) as sess:<br/>    # get input node<br/>    image_tensor = graph.get_tensor_by_name('image_tensor:0')<br/>    <br/>    # get output nodes<br/>    detection_boxes = graph.get_tensor_by_name('detection_boxes:0')<br/>    detection_scores = graph.get_tensor_by_name('detection_scores:0')<br/>    detection_classes = graph.get_tensor_by_name('detection_classes:0')<br/>    num_detections = graph.get_tensor_by_name('num_detections:0')<br/>    <br/>    # read image from file and pre-process it for input.<br/>    # Note: we can do this outside session scope too. <br/>    image = read_cv_image(os.path.join(image_dir, 'cars2.png'))<br/>    input_img = image[np.newaxis, :, :, :]<br/>    <br/>    # To compute prediction time <br/>    start = time.time()<br/>    # Run prediction and get 4 outputs<br/>    (boxes, scores, classes, num) = sess.run(<br/>          [detection_boxes, detection_scores, detection_classes, num_detections],<br/>          feed_dict={image_tensor: input_img})<br/>    end = time.time()<br/>    print("Prediction time:",end-start,"secs for ", num[0], "detections")<br/>    # display results<br/>    show_mpl_img_with_detections(image, boxes[0],scores[0], classes[0],category_index, thres=0.6)<br/>    </pre>
<p>Using previous code, an example of prediction is as shown in the following screenshot. Each detected object is displayed with the bounding box. Each bounding box has a name of the predicted class as well as the confidence score for the object inside the box:</p>
<div class="CDPAlignCenter CDPAlign"><img height="240" width="426" src="images/172440dd-3f05-4611-b21c-5edfcd084ecb.png"/></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">One-stage detectors</h1>
                </header>
            
            <article>
                
<p><span>In the previous section</span><span>,&#160;we saw that two-stage detectors suffer from the issue of slower prediction time and harder training by splitting the network into two. In recently proposed networks like <strong>Single Shot Multibox Detectors (SSD)</strong>[3], the prediction time is reduced by removing the intermediate stage and the training is always end-to-end. These networks have shown effectiveness by running on smartphones as well as low-end computation units:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="187" width="332" src="images/2dc74647-a537-43e6-b0cc-d9463a8ccdd1.png"/></div>
<p>&#160;</p>
<p>An abstract view of the network is shown in the preceding figure. The overall output of the network is same as two-stage, the class probability for the object and bounding box coordinates of the form <strong>(x, y, w, h)</strong>, where (x,y) is the top-left corner of the rectangle and (w, h) are the width and height of the box respectively. In order to use multiple resolutions, the model not only uses the final layer of feature extraction but also several intermediate feature layers. An abstract view is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img height="218" width="388" src="images/7db77e68-b1ef-40c9-a317-72247f97d1f3.png"/></div>
<p>To further increase the speed for detection, the model also uses a technique called <strong>non-maximal suppression</strong>. This will suppress all the <strong>Bounding Box</strong> which do not have a maximum score in a given region and for a given category. As a result, the total output boxes from the <strong>MultiBox Layer</strong>&#160;are reduced significantly and thus we have only high scored detections per class in an image.</p>
<p>In the next section, we will see TensorFlow-based SSD object detection. We will use some of the code from the previous section;&#160; Reader does not need to install again if there is already an installation of the previous section.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Demo</h1>
                </header>
            
            <article>
                
<p>In the following codes, we will load a pre-trained model and perform an object detection task on pre-defined 90 categories. Before we begin, check that there is a working TensorFlow (Version = 1.4.0) Python environment.</p>
<p>In this section, our input is as shown in the image with people:&#160;</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="264" width="398" src="images/86cae04e-26c3-4f4d-a0df-4be87d55022c.png"/></div>
<p>We will follow similar instructions as that of two-stage detectors and begin by cloning TensorFlow/models repo:</p>
<pre><strong>git clone https://github.com/tensorflow/models.git</strong><br/><strong>cd models/research</strong></pre>
<p class="mce-root">Let's download a pre-trained model from TensorFlow model-zoo. These are for one-stage detectors:</p>
<ul>
<li>For macOS X:</li>
</ul>
<pre style="padding-left: 60px"><strong>curl -O http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2017_11_17.tar.gz</strong><br/><strong>tar -xvf ssd_inception_v2_coco_2017_11_17.tar.gz</strong></pre>
<ul>
<li>For Linux:</li>
</ul>
<pre style="padding-left: 60px"><strong>wget http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2017_11_17.tar.gz</strong><br/><strong>tar -xvf ssd_inception_v2_coco_2017_11_17.tar.gz</strong></pre>
<p>Similarly, keep the extracted folder by the name<span>&#160;</span><kbd>ssd_inception_v2_coco_2017_11_17 in models/research/object_detection</kbd><em>.</em>&#160;We will set up the environment now.&#160;If this has already been done from the previous section, please skip this:</p>
<ul>
<li class="mce-root">First, we will compile the <kbd>protobuf</kbd> files:</li>
</ul>
<pre style="padding-left: 60px"><strong>protoc object_detection/protos/*.proto --python_out=.</strong></pre>
<ul>
<li>Also, run in the research folder:</li>
</ul>
<pre style="padding-left: 60px"><strong>export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim</strong> </pre>
<p>Let's begin with loading libraries:</p>
<pre class="mce-root"><strong>import numpy as np</strong><br/><strong>import os</strong><br/><strong>import sys</strong><br/><strong>import tensorflow as tf</strong><br/><strong>import cv2</strong><br/><strong>from matplotlib import pyplot as plt</strong><br/><strong># inside jupyter uncomment next line </strong><br/><strong># %matplotlib inline</strong><br/><strong>import random</strong><br/><strong>import time</strong><br/><strong>from utils import label_map_util</strong> </pre>
<ol>
<li>The following code reads pre-trained model. In TensorFlow, these models are usually saved as <kbd>protobuf</kbd> in&#160;<kbd>.pb</kbd> format. Also, note that if there are other formats of pre-trained model files, then we may have to read accordingly:&#160;</li>
</ol>
<pre style="padding-left: 60px"><strong>def load_and_create_graph(path_to_pb):</strong><br/><strong>    """</strong><br/><strong>    Loads pre-trained graph from .pb file. </strong><br/><strong>    path_to_pb: path to saved .pb file</strong><br/><strong>    Tensorflow keeps graph global so nothing is returned</strong><br/><strong>    """</strong><br/><strong>    with tf.gfile.FastGFile(path_to_pb, 'rb') as f:</strong><br/><strong>        # initialize graph definition</strong><br/><strong>        graph_def = tf.GraphDef()</strong><br/><strong>        # reads file </strong><br/><strong>        graph_def.ParseFromString(f.read())</strong><br/><strong>        # imports as tf.graph</strong><br/><strong>        _ = tf.import_graph_def(graph_def, name='')</strong></pre>
<ol start="2">
<li>For using our input image, the following block reads an image from a given path to a file:&#160;</li>
</ol>
<pre style="padding-left: 60px"><strong>def read_cv_image(filename):</strong><br/><strong>    """</strong><br/><strong>    Reads an input color image and converts to RGB order</strong><br/><strong>    Returns image as an array</strong><br/><strong>    """</strong><br/><strong>    img = cv2.imread(filename)</strong><br/><strong>    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</strong><br/><strong>    return img</strong></pre>
<ol start="3">
<li>The last utility function is for the output display of the bounding box around the predicted object with the class name and detection score for each box:</li>
</ol>
<pre style="padding-left: 60px"><strong>def show_mpl_img_with_detections(img, dets, scores, </strong><br/><strong>                                 classes, category_index, </strong><br/><strong>                                 thres=0.6):</strong><br/><strong>    """</strong><br/><strong>    Applies thresholding to each box score and </strong><br/><strong>    plot bbox results on image. </strong><br/><strong>    img: input image as numpy array</strong><br/><strong>    dets: list of K detection outputs for given image. (size:[1,K] )</strong><br/><strong>    scores: list of detection score for each detection output(size: [1,K]).</strong><br/><strong>    classes: list of predicted class index(size: [1,K])    </strong><br/><strong>    category_index: dictionary containing mapping from class index to class name. </strong><br/><strong>    thres: threshold to filter detection boxes:(default: 0.6)</strong><br/><strong>    By default K:100 detections</strong><br/><strong>    """</strong><br/><strong>    # plotting utilities from matplotlib</strong><br/><strong>    plt.figure(figsize=(12,8))</strong><br/><strong>    plt.imshow(img)</strong><br/><strong>    height = img.shape[0]</strong><br/><strong>    width = img.shape[1]</strong><br/><strong>    # To use common color of one class and different for different classes</strong><br/><strong>    colors = dict()    </strong><br/><strong>    # iterate over all proposed bbox </strong><br/><strong>    # choose whichever is more than a threshold</strong><br/><strong>    for i in range(dets.shape[0]):</strong><br/><strong>        cls_id = int(classes[i])</strong><br/><strong>        # in case of any wrong prediction for class index</strong><br/><strong>        if cls_id &gt;= 0:</strong><br/>            <br/><strong>            score = scores[i]</strong><br/><strong>            # score for a detection is more than a threshold</strong><br/><strong>            if score &gt; thres:</strong><br/><strong>                if cls_id not in colors:</strong><br/><strong>                    colors[cls_id] = (random.random(), </strong><br/><strong>                                      random.random(), </strong><br/><strong>                                      random.random())</strong><br/><strong>                xmin = int(dets[i, 1] * width)</strong><br/><strong>                ymin = int(dets[i, 0] * height)</strong><br/><strong>                xmax = int(dets[i, 3] * width)</strong><br/><strong>                ymax = int(dets[i, 2] * height)</strong><br/><strong>                rect = plt.Rectangle((xmin, ymin), xmax - xmin,</strong><br/><strong>                                     ymax - ymin, fill=False,</strong><br/><strong>                                     edgecolor=colors[cls_id],</strong><br/><strong>                                     linewidth=2.5)</strong><br/><strong>                plt.gca().add_patch(rect)</strong><br/><strong>                # to plot class name and score around each detection box</strong><br/><strong>                class_name = str(category_index[cls_id]['name'])</strong><br/>                <br/><strong>                plt.gca().text(xmin, ymin - 2,</strong><br/><strong>                           '{:s} {:.3f}'.format(class_name, score),</strong><br/><strong>                           bbox=dict(facecolor=colors[cls_id], alpha=0.5),</strong><br/><strong>                           fontsize=8, color='white')</strong><br/><strong>    plt.axis('off')</strong><br/><strong>    plt.show()</strong><br/><br/><strong>    return</strong> </pre>
<ol start="4">
<li>We will be using an SSD model for object detection that uses Inception-v2 model for feature extraction. This model is pre-trained on the&#160;<kbd>MSCOCO</kbd> dataset. We saw earlier the code snippet to download the model and also to load. So let's go ahead and read the model:&#160;</li>
</ol>
<pre style="padding-left: 60px"><strong># load pre-trained model</strong><br/><strong>load_and_create_graph('ssd_inception_v2_coco_2017_11_17/frozen_inference_graph.pb')</strong></pre>
<ol start="5">
<li>Before we start using the model to do predictions on the input image, we need our output to make sense. We will create a dictionary map of the class index to pre-defined class names. The following code will read a file <kbd>data/mscoco_label_map.pbtxt</kbd> which contains this index to class name mapping. The final index can be used to read our output as class names:</li>
</ol>
<pre style="padding-left: 60px"><strong># load labels for classes output</strong><br/><strong>path_to_labels = os.path.join('data', 'mscoco_label_map.pbtxt')</strong><br/><strong>nb_classes = 90</strong><br/><strong>label_map = label_map_util.load_labelmap(path_to_labels)</strong><br/><strong>categories = label_map_util.convert_label_map_to_categories(label_map,</strong><br/><strong>                   max_num_classes=nb_classes, use_display_name=True)</strong><br/><strong>category_index = label_map_util.create_category_index(categories)</strong></pre>
<p>We have set up everything necessary for prediction. In TensorFlow, the model is represented as a computational graph and is often referred to as graph in code snippets. This consists of various layers and operation on layers represented as a node the and connection between them is how the data will flow. For performing predictions, we need to know the input node name and output node names. There can be more than one nodes of a type. To start performing the computation, we will first create a session. A graph can only perform computation inside a session and we can create a session as we need it in the program. In the following code snippet, we create a session and get pre-defined input node and output nodes:</p>
<pre><strong>image_dir = 'test_images/'</strong><br/><strong># create graph object from previously loaded graph</strong><br/><strong># tensorflow previously loaded graph as default </strong><br/><strong>graph=tf.get_default_graph()</strong><br/><br/><strong># launch a session to run this graph </strong><br/><strong>with tf.Session(graph=graph) as sess:</strong><br/><strong>    # get input node</strong><br/><strong>    image_tensor = graph.get_tensor_by_name('image_tensor:0')</strong><br/>    <br/><strong>    # get output nodes</strong><br/><strong>    detection_boxes = graph.get_tensor_by_name('detection_boxes:0')</strong><br/><strong>    detection_scores = graph.get_tensor_by_name('detection_scores:0')</strong><br/><strong>    detection_classes = graph.get_tensor_by_name('detection_classes:0')</strong><br/><strong>    num_detections = graph.get_tensor_by_name('num_detections:0')</strong><br/>    <br/><strong>    # read image from file and pre-process it for input.</strong><br/><strong>    # Note: we can do this outside session scope too.  </strong><br/><strong>    image = read_cv_image(os.path.join(image_dir, 'person1.png'))</strong><br/><strong>    # Input Shape : [N, Width,Height,Channels], </strong><br/><strong>    # where N=1, batch size</strong><br/><strong>    input_img = image[np.newaxis, :, :, :] </strong><br/>    <br/><strong>    # To compute prediction time </strong><br/><strong>    start = time.time()</strong><br/><strong>    # Run prediction and get 4 outputs</strong><br/><strong>    (boxes, scores, classes, num) = sess.run(</strong><br/><strong>          [detection_boxes, detection_scores, detection_classes, num_detections],</strong><br/><strong>          feed_dict={image_tensor: input_img})</strong><br/><strong>    end = time.time()</strong><br/><strong>    print("Prediction time:",end-start,"secs for ", num, "detections")</strong><br/>    <br/><strong>    # display results with score threshold of 0.6</strong><br/><strong>    # Since only one image is used , hence we use 0 index for outputs</strong><br/><strong>    show_mpl_img_with_detections(image, boxes[0],scores[0], classes[0], thres=0.6)</strong></pre>
<p>In the previous code, the input node is <kbd>image_tensor:0</kbd> and four output nodes are&#160;<kbd>detection_boxes:0</kbd>, <kbd>detection_scores:0</kbd>, <kbd>detection_classes:0</kbd>, and&#160;<kbd>num_detections:0</kbd>.&#160;</p>
<p>When we run inference on a given image, the inference is as shown in the following figure. Each box color is according to the class, and the predicted class name, as well as the score for class prediction, is displayed in the top-left corner. Ideally, score one shows the model is 100% sure about the category of an object inside the box:</p>
<div class="packt_infobox">This score is not for how correct the box is but only for the confidence for the category of the object inside.</div>
<div class="CDPAlignCenter CDPAlign"><img height="341" width="502" src="images/6a2601de-45f3-43b3-85a3-f6314b0eda24.png"/></div>
<p>Here we used only one image as input. We can use a list of images as input and correspondingly we will get a list of outputs for each image. To display the results, iterate simultaneously on images and outputs as follows:&#160;</p>
<pre><strong>for i in range(nb_inputs):</strong><br/><strong>    show_mpl_img_with_detections(images[i], boxes[i],scores[i], classes[i], thres=0.6)</strong></pre>
<p>To show the comparison with the two-stage detector, for the same input the following are the output prediction with the one-stage detector. We can easily notice that the one-stage detectors such as&#160;SSD is good for large objects but fail to recognize small objects such as people. Also, the prediction scores vary a lot between the two detectors:&#160;</p>
<div class="CDPAlignCenter CDPAlign"><img height="227" width="395" src="images/bbceb912-d626-4585-89b5-293a2796d0e2.png"/></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter gives an overview of object detection and several challenges in modeling a good detector. While there are many methods for detection using deep learning, common categories are one-stage and two-stage detectors. Each of the detectors has its own advantages, such as one-stage detectors are good for real-time applications while two-stage detectors are good for high accuracy output. The difference in accuracy between the models is shown using example figures. We can now understand the choice of object detector and run a pre-trained model using TensorFlow. The various output samples for each show the effectiveness of models in complex images.</p>
<p>In the next chapter, we will learn more about the image recognition problems of segmentation as well as tracking using deep learning methods.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Viola Paul and Michael J. Jones. <em>Robust real-time face detection</em>.&#160;</span>International journal of computer vision<span>&#160;57, no. 2 (2004): 137-154.</span></li>
<li><span>Ren Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. <em>Faster R-CNN: Towards real-time object detection with region proposal networks</em>. In&#160;</span>Advances in neural information processing systems<span>, pp. 91-99. 2015.</span></li>
<li><span>Liu Wei, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C. Berg. S<em>SD: Single Shot Multibox Detector</em>. In&#160;</span>European conference on computer vision<span>, pp. 21-37. Springer, Cham, 2016.</span></li>
<li>&#160;Lin et al., <em>Microsoft COCO: Common Objects in Context</em>,&#160;<a href="https://arxiv.org/pdf/1405.0312.pdf" target="_blank">https://arxiv.org/pdf/1405.0312.pdf</a>.</li>
</ul>
<p>&#160;</p>


            </article>

            
        </section>
    </div>
</body>
</html>