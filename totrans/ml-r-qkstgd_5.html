<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Failures of Banks - Multivariate Analysis</h1>
                </header>
            
            <article>
                
<p>In this chapter, we are going to apply different algorithms with the aim of obtaining a good model using combinations of our predictors. The most common algorithm that's used in credit risk applications, such as credit scoring and rating, is logistic regression. In this chapter, we will see how other algorithms can be applied to solve some of the weaknesses of logistic regression.</p>
<p>In this chapter, we will be covering the following topics:</p>
<ul>
<li>Logistic regression</li>
<li>Regularized methods</li>
<li>Testing a random forest model</li>
<li>Gradient boosting </li>
<li>Deep learning in neural networks</li>
<li>Support vector machines</li>
<li>Ensembles</li>
<li>Automatic machine learning</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression</h1>
                </header>
            
            <article>
                
<p>Mathematically, a binary logistic model has a dependent variable with two categorical values. In our example, these values relate to whether or not a bank is solvent.</p>
<p class="mce-root"/>
<p>In a logistic model, <strong>log odds</strong> refers to the logarithm of the odds for a class, which is a linear combination of one or more independent variables, as follows:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/d8748302-fdf3-43b2-adf6-f6774fcc2739.png" style="width:19.08em;height:3.75em;"/></div>
<p>The coefficients (beta values, <em>β</em>) of the logistic regression algorithm must be estimated using maximum likelihood estimation. Maximum likelihood estimation involves getting values for the regression coefficients that minimize the error in the probabilities that are predicted by the model and the real observed case.</p>
<p>Logistic regression is very sensitive to the presence of outlier values, so high correlations in variables should be avoided. Logistic regression in R can be applied as follows:</p>
<pre>set.seed(1234)<br/>LogisticRegression=glm(train$Default~.,data=train[,2:ncol(train)],family=binomial())<br/><span> ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</span></pre>
<p>The code runs without problems, but a warning message appears. If the variables are highly correlated or collinearity exists, it is expected that the model parameters and the variance are inflated.</p>
<p>The high variance is not due to accurate or good predictors, but is instead due to a misspecified model with redundant predictors. Thus, the maximum likelihood is increased by simply adding more parameters, which results in overfitting.</p>
<p>We can observe the parameters of the model with the <kbd>summary()</kbd> function:</p>
<pre>summary(LogisticRegression)<br/> ## <br/> ## Call:<br/> ## glm(formula = train$Default ~ ., family = binomial(), data =         train[, <br/> ##     2:ncol(train)])<br/> ## <br/> ## Deviance Residuals: <br/> ##     Min       1Q   Median       3Q      Max  <br/> ## -3.9330  -0.0210  -0.0066  -0.0013   4.8724  <br/> ## <br/> ## Coefficients:<br/> ##                   Estimate     Std. Error z value Pr(&gt;|z|)  <br/> ## (Intercept) -11.7599825009   6.9560247460  -1.691   0.0909 .<br/> ## UBPRE395     -0.0575725641   0.0561441397  -1.025   0.3052  <br/> ## UBPRE543      0.0014008963   0.0294470630   0.048   0.9621  <br/> ##         ....                             .....                           ....                            ....             ....<br/> ## UBPRE021     -0.0114148389   0.0057016025  -2.002   0.0453 *<br/> ## UBPRE023      0.4950212919   0.2459506994   2.013   0.0441 *<br/> ## UBPRK447     -0.0210028916   0.0192296299  -1.092   0.2747  <br/> ## ---<br/> ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/> ## <br/> ## (Dispersion parameter for binomial family taken to be 1)<br/> ## <br/> ##     Null deviance: 2687.03  on 7090  degrees of freedom<br/> ## Residual deviance:  284.23  on 6982  degrees of freedom<br/> ## AIC: 502.23<br/> ## <br/> ## Number of Fisher Scoring iterations: 13</pre>
<p>We can see that most of the variables in the last column of the preceding table are insignificant. In cases like this, the number of variables should be reduced in the regression, or another approach should be followed, such as a penalized or regularization method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regularized methods</h1>
                </header>
            
            <article>
                
<p>There are three common approaches to using regularized methods:</p>
<ul>
<li>Lasso</li>
<li>Ridge</li>
<li>Elastic net</li>
</ul>
<p>In this section, we will see how these methods can be implemented in R. For these models, we will use the <kbd>h2o</kbd> package. This provides a predictive analysis platform to be used in machine learning that is open source, based on in-memory parameters, and distributed, fast, and scalable. It helps in creating models that are built on big data and is most suitable for enterprise applications as it enhances production quality.</p>
<div class="packt_infobox">For more information on the <kbd>h2o</kbd> package, please visit its documentation at <a href="https://cran.r-project.org/web/packages/h2o/index.html">https://cran.r-project.org/web/packages/h2o/index.html</a>.<a href="https://cran.r-project.org/web/packages/h2o/index.html"/></div>
<p class="mce-root"/>
<p>This package is very useful because it summarizes <span>several common machine learning algorithms </span><span>in one package. Moreover, these algorithms can be executed in parallel on our own computer, as it is very fast. The package includes generalized linear naïve Bayes, distributed random forest, gradient boosting, and deep learning, among others.</span></p>
<p>It is not necessary to have a high level of programming knowledge, because the package comes with a user interface.</p>
<p>Let's see how the package works. First, the package should be loaded:</p>
<pre>library(h2o)</pre>
<p>Use the <kbd>h2o.init</kbd> <span>method</span><span> </span><span>to initialize H2O. This method accepts other options that can be found in the package documentation:</span></p>
<pre>h2o.init()</pre>
<p>The first step toward building our model involves placing our data in the H2O cluster/Java process. Before this step, we will ensure that our target is considered as a <kbd>factor</kbd> variable:</p>
<pre>train$Default&lt;-as.factor(train$Default)<br/> <br/>test$Default&lt;-as.factor(test$Default)</pre>
<p>Now, let's upload our data to the <kbd>h2o</kbd> cluster:</p>
<pre>as.h2o(train[,2:ncol(train)],destination_frame="train")<br/><br/>as.h2o(test[,2:ncol(test)],destination_frame="test")</pre>
<p>If you close R and restart it later, you will need to upload the datasets <span>again,</span><span> </span><span>as in the preceding code.</span></p>
<p>We can check that the data has been uploaded correctly with the following command:</p>
<pre>h2o.ls()<br/><br/> ##     key<br/> ## 1  test<br/> ## 2 train</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="CDPAlignLeft CDPAlign">The package contains an easy interface that allows us to create different models when we run it in our browser. In general, the interface can be launched by writing the following address in our web browser, <kbd>http://localhost:54321/flow/index.html</kbd>. You will be faced with a page like the one that's shown in the following screenshot. In the <span class="packt_screen">Model</span> tab, we can see a list with all of the available models that are implemented in this package:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-705 image-border" src="assets/4f048b93-f077-4cfa-a381-12ba40c141bf.png" style=""/></div>
<p>First, we are going to develop regularization models. For that, <span class="packt_screen">Generalized Linear Modelling…</span> must be selected. This module includes the following:</p>
<ul>
<li>Gaussian regression</li>
<li>Poisson regression</li>
<li>Binomial regression (classification)</li>
<li>Multinomial classification</li>
<li>Gamma regression</li>
<li>Ordinal regression</li>
</ul>
<p class="mce-root"/>
<p>As shown in the following screenshot, we should fill in the necessary parameters to train our model:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-706 image-border" src="assets/cd371ff1-ed26-4cdd-9a97-a8398f634f8d.png" style=""/></div>
<p>We will fill in the following fields:</p>
<ul>
<li><span class="packt_screen">model_id</span>: Here, we can specify the name that can be used as a reference by the model.</li>
<li><span class="packt_screen">training_frame</span>: The dataset that we wish to use to build and train the model can be mentioned here, as this will be our training dataset.</li>
<li><span class="packt_screen">validation_frame</span>: Here, the dataset that will be used to check the accuracy of the model is mentioned.</li>
<li><span class="packt_screen">nfolds</span>: For validation, we require a certain number of folds to be mentioned here. In our case, the <span class="packt_screen">nfolds</span> value is <kbd>5</kbd>.</li>
<li><span class="packt_screen">seed</span>: This specifies the seed that will be used by the algorithm. We will use a <strong>Random Number Generator</strong> (<strong>RNG</strong>) for the components in the algorithm that require random numbers.</li>
<li><span class="packt_screen">response_column</span>: This is the column to use as the dependent variable. In our case, the column is named <span class="packt_screen">Default</span>.</li>
<li><span class="packt_screen">ignored_columns</span>: In this section, it is possible to ignore variables in the training process. In our case, all of the variables are considered relevant.</li>
<li><span class="packt_screen">ignore_const_cols</span>: This is a flag that indicates that the package should avoid constant variables.</li>
<li><span class="packt_screen">family</span>: This specifies the model type. In our case, we want to train a regression model, so the family should be fixed as <span class="packt_screen">binomial</span>, because our target variable has two possible values.</li>
<li><span class="packt_screen">solver</span>: This specifies the solver to use. We don't change this value because no significant differences have been observed regardless of whether one solver or another is chosen. Hence, we will keep it as the default value.</li>
<li><span class="packt_screen">alpha</span>: Here, you have to choose values for the regularization distribution from L1 to L2. If you select <span class="packt_screen">1</span>, it will be a Lasso regression. If you select <span class="packt_screen">0</span>, it will be a Ridge regression. If any value in between <span class="packt_screen">0</span> and <span class="packt_screen">1</span> is selected, you will have a mixture of both Lasso and Ridge. In our case, we will select <span class="packt_screen">1</span>. One of the main advantages of the Lasso model is in the reduction of the number of variables because the trained models makes the coefficient of non-relevant variables zero, <span>resulting in</span> models that are simple, but accurate at the same time.</li>
<li><span class="packt_screen">lambda_search</span>: This parameter starts a search of the regularization strength.</li>
<li><span class="packt_screen">standardize</span>: If this flag is marked, it means that numeric columns will be transformed to have a zero mean and zero unit variance.</li>
</ul>
<p>Finally, the <span class="packt_screen">Build model</span> button trains the model. Although other options can be selected, the preceding specifications are sufficient:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-707 image-border" src="assets/d64f8f47-f7f4-4f04-aed7-de9bf5fb2b4f.png" style=""/></div>
<p>We can see that the model has been trained quickly. The <span class="packt_screen">View</span> button provides us with some interesting details about the model:</p>
<ul>
<li>Model parameters</li>
<li>Scoring history</li>
<li>The <strong>Receiver Operating Characteristic</strong> (<strong>ROC</strong>) curve for training and validation samples</li>
<li>Standardized coefficient magnitudes</li>
<li>Gains/Lift table for cross-validation, training, and validation samples</li>
<li>Cross-validation models</li>
<li>Metrics</li>
<li>Coefficients</li>
</ul>
<p>Let's see some of the main results:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-708 image-border" src="assets/508719aa-acca-45b9-a847-ee4f09b73369.png" style=""/></div>
<p>As we can see, our Lasso model is trained with <span class="packt_screen">108</span> different variables, but only <span class="packt_screen">56 </span>result in a model that has a coefficient greater than zero.</p>
<p>The model provides an almost perfect classification. In the training sample, the <strong>Area under the curve</strong> (<strong>AUC</strong>) reaches 99.51%. This value is slightly lower in the validation sample, with a value of 98.65%. The standardized variables are also relevant:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-709 image-border" src="assets/95ade899-eea3-4f92-b42c-62fbaf23cd3a.png" style=""/></div>
<p>If a variable is shown in blue, this indicates that the coefficient is positive. If it is negative, the color is orange.</p>
<p>As we can see, <span><span class="packt_screen">UBPRE626</span> looks like an important variable. It shows us how many times the total loans and lease-financing receivables surpassed the actual total of the equity capital. A positive sign here means a higher ratio, which also implies a higher probability that a bank will fail in its operations.</span></p>
<p>The top five relevant variables according to this figure are as follows:</p>
<ol>
<li><span class="packt_screen">UBPRE626</span>: The number of times net loans and lease-financing receivables exceed the total equity capital</li>
<li><span class="packt_screen">UBPRE545</span>: The total of the due and non-accrual loans and leases, divided by the allowance for the loan and lease losses</li>
<li><span class="packt_screen">UBPRE170</span>: The total equity capital</li>
<li><span class="packt_screen">UBPRE394</span>: Other construction and land development loans, divided by the average gross loans and leases</li>
<li><span class="packt_screen">UBPRE672</span>: One quarter of the annualized realized gains (or losses) of the securities, divided by average assets</li>
</ol>
<p>When looking at credit risks, it is important to understand which variables are most significant and the economic relevance of these variables. For example, it would not make sense if the higher the non-performing loans or loans with problems, the higher the solvency of a bank. We aren't concerned about the economic sense of the variables in our model, but <span><span>this</span></span> is a key issue for some models that are developed in financial institutions. If the variables don't have the expected sign, they have to be removed from the model.</p>
<p>In some cases, is necessary to test different combinations of parameters until you obtain the best model. For example, in the recently trained regularized model, we could have tried different values of the <span class="packt_screen">alpha</span> parameter. To test different parameters at the same time, you need to execute the algorithms using code. Let's have a look at how to do this. We will train the regularized models again, but using some code this time. First, we remove all the objects, including the recently created model, from the <kbd>h2o</kbd> system:</p>
<pre>h2o.removeAll()<br/> ## [1] 0</pre>
<p>Then, we upload our training and validation samples again:</p>
<pre>as.h2o(train[,2:ncol(train)],destination_frame="train")<br/>as.h2o(test[,2:ncol(test)],destination_frame="test")</pre>
<p>Let's code our model. A grid of empty parameters is created as follows:</p>
<pre>grid_id &lt;- 'glm_grid'</pre>
<p>Then, we assign different parameters to be tested in this grid:</p>
<pre>hyper_parameters &lt;- list( alpha = c(0, .5, 1) )<br/>stopping_metric &lt;- 'auc'<br/>glm_grid &lt;- h2o.grid(<br/>     algorithm = "glm",<br/>     grid_id = grid_id,<br/>     hyper_params = hyper_parameters,<br/>     training_frame = training,<br/>     nfolds=5,<br/>     x=2:110,<br/>     y=1,<br/>     lambda_search = TRUE,<br/>     family = "binomial", seed=1234)</pre>
<p>As we can see, the parameters are exactly the same as those we used to train the previous model. The only difference is that we now use different alpha values at the same time, which corresponds to a Ridge regression, with a Lasso and an elastic net. The model is trained using the following code:</p>
<pre>results_glm &lt;- h2o.getGrid(<br/>     grid_id = grid_id,<br/>     sort_by = stopping_metric,<br/>     decreasing = TRUE)</pre>
<p>According to the previous code, the different models in the grid should be ordered by the AUC metric. Thus, we are interested in the first model:</p>
<pre>best_GLM &lt;- h2o.getModel(results_glm@model_ids[[1]])</pre>
<p>Let's take a look at some details about this model:</p>
<pre>best_GLM@model$model_summary$regularization<br/> ## [1] "Ridge ( lambda = 0.006918 )"</pre>
<p>The model with the best performance is a Ridge model. The performance of the model can be obtained as follows:</p>
<pre>perf_train&lt;-h2o.performance(model = best_GLM,newdata = training)<br/>perf_train<br/> ## H2OBinomialMetrics: glm<br/> ##<br/> ## MSE:  0.006359316<br/> ## RMSE:  0.07974532<br/> ## LogLoss:  0.02561085<br/> ## Mean Per-Class Error:  0.06116986<br/> ## AUC:  0.9953735<br/> ## Gini:  0.990747<br/> ## R^2:  0.8579102<br/> ## Residual Deviance:  363.213<br/> ## AIC:  581.213<br/> ##<br/> ## Confusion Matrix (vertical: actual; across: predicted) for F1-              optimal threshold:<br/> ##           0   1    Error      Rate<br/> ## 0      6743  15 0.002220  =15/6758<br/> ## 1        40 293 0.120120   =40/333<br/> ## Totals 6783 308 0.007756  =55/7091<br/> ##<br/> ## Maximum Metrics: Maximum metrics at their respective thresholds<br/> ##                         metric threshold    value idx<br/> ## 1                       max f1  0.540987 0.914197 144<br/> ## 2                       max f2  0.157131 0.931659 206<br/> ## 3                 max f0point5  0.617239 0.941021 132<br/> ## 4                 max accuracy  0.547359 0.992244 143<br/> ## 5                max precision  0.999897 1.000000   0<br/> ## 6                   max recall  0.001351 1.000000 383<br/> ## 7              max specificity  0.999897 1.000000   0<br/> ## 8             max absolute_mcc  0.540987 0.910901 144<br/> ## 9   max min_per_class_accuracy  0.056411 0.972973 265<br/> ## 10 max mean_per_class_accuracy  0.087402 0.977216 239<br/> ##<br/><br/></pre>
<p>The <kbd>AUC</kbd> and the <kbd>Gini</kbd> index, which are the main metrics of performance, are only slightly higher than in the Lasso that we trained initially—at least in the training sample.</p>
<p>The performance of the model in the test sample is also high:</p>
<pre>perf_test&lt;-h2o.performance(model = best_GLM,newdata = as.h2o(test))<br/>perf_test<br/> ## H2OBinomialMetrics: glm<br/> ##<br/> ## MSE:  0.01070733<br/> ## RMSE:  0.1034762<br/> ## LogLoss:  0.04052454<br/> ## Mean Per-Class Error:  0.0467923<br/> ## AUC:  0.9875425<br/> ## Gini:  0.975085<br/> ## R^2:  0.7612146<br/> ## Residual Deviance:  246.3081<br/> ## AIC:  464.3081<br/> ##<br/> ## Confusion Matrix (vertical: actual; across: predicted) for F1-            optimal threshold:<br/> ##           0   1    Error      Rate<br/> ## 0      2868  28 0.009669  =28/2896<br/> ## 1        12 131 0.083916   =12/143<br/> ## Totals 2880 159 0.013162  =40/3039<br/> ##<br/> ## Maximum Metrics: Maximum metrics at their respective thresholds<br/> ##                         metric threshold    value idx<br/> ## 1                       max f1  0.174545 0.867550 125<br/> ## 2                       max f2  0.102341 0.904826 138<br/> ## 3                 max f0point5  0.586261 0.885167  89<br/> ## 4                 max accuracy  0.309187 0.987167 107<br/> ## 5                max precision  0.999961 1.000000   0<br/> ## 6                   max recall  0.000386 1.000000 388<br/> ## 7              max specificity  0.999961 1.000000   0<br/> ## 8             max absolute_mcc  0.174545 0.861985 125<br/> ## 9   max min_per_class_accuracy  0.027830 0.955456 210<br/> ## 10 max mean_per_class_accuracy  0.102341 0.965295 138<br/><br/></pre>
<p>Again, the results do not differ significantly in comparison with the Lasso model. Nevertheless, the number of coefficients in the Lasso model is lower, which makes it easier to interpret and more parsimonious.</p>
<p>The total number of coefficients in the Ridge regression is equal to the number of variables in the dataset and the intercept of the model:</p>
<pre>head(best_GLM@model$coefficients)<br/> ##    Intercept     UBPRE395     UBPRE543     UBPRE586     UBPRFB60<br/> ## -8.448270911 -0.004167366 -0.003376142 -0.001531582  0.027969152<br/> ##     UBPRE389<br/> ## -0.004031844</pre>
<p>Now, we will store the predictions of each model in a new data frame. We can combine the results of the different models to obtain an additional model. Initially, our data frame will contain only the ID of each bank and the target variable:</p>
<pre>summary_models_train&lt;-train[,c("ID_RSSD","Default")]<br/>summary_models_test&lt;-test[,c("ID_RSSD","Default")]</pre>
<p>Let's calculate the model predictions and store them in the summary data frame:</p>
<pre>summary_models_train$GLM&lt;-as.vector(h2o.predict(best_GLM,training)[3])<br/>summary_models_test$GLM&lt;-as.vector(h2o.predict(best_GLM,validation)[3])</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>When we run the previous code to calculate the performance of the model, we also obtain a confusion <span><span>matrix</span></span>. For example, in the test sample, we obtain the following:</p>
<pre>perf_test@metrics$cm$table<br/> ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class<br/> ##           0   1  Error         Rate<br/> ## 0      2868  28 0.0097 = 28 / 2,896<br/> ## 1        12 131 0.0839 =   12 / 143<br/> ## Totals 2880 159 0.0132 = 40 / 3,039</pre>
<p>This package classifies a bank as a failed bank if its probability of defaulting is higher than 0.5, and is a successful bank otherwise.</p>
<p>According to this assumption, <kbd>40</kbd> banks are misclassified (<kbd>28</kbd>+<kbd>12</kbd>). Nevertheless, the cutoff of 0.5 is not actually correct, because the proportion of failed versus non-failed banks in the sample is different.</p>
<p>The proportion of failed banks is actually only 4.696%, as shown in the following code :</p>
<pre>mean(as.numeric(as.character(train$Default)))<br/> ## [1] 0.04696094</pre>
<p>Hence, it is more appropriate to consider a bank as failed if the probability of a bank defaulting is higher than this proportion:</p>
<pre>aux&lt;-summary_models_test<br/>aux$pred&lt;-ifelse(summary_models_test$GLM&gt;0.04696094,1,0)</pre>
<p>Thus, the new confusion table for the test sample is as follows:</p>
<pre>table(aux$Default,aux$pred)<br/> ##   <br/> ##        0    1<br/> ##   0 2818   78<br/> ##   1    8  135</pre>
<p>According to this table, the model misclassifies 86 banks (<kbd>78</kbd>+<kbd>8</kbd>). Almost all of the failed banks have been correctly classified. It will be difficult to obtain a better algorithm than this.</p>
<p>The model can be saved locally using <kbd>h2o.saveModel</kbd>:</p>
<pre>h2o.saveModel(object= best_GLM, path=getwd(), force=TRUE)</pre>
<p>We remove the irrelevant objects in the workspace and save it as follows:</p>
<pre>rm(list=setdiff(ls(), c("Model_database","train","test","summary_models_train","summary_models_test","training","validation")))<br/> <br/>save.image("Data13.RData")</pre>
<p>Remember that if you close R and load this workspace again, you should convert your train and test samples into <kbd>h2o</kbd> format <span>again</span><span>:</span></p>
<pre>training&lt;-as.h2o(train[,2:ncol(train)],destination_frame=“train”)<br/>validation&lt;-as.h2o(test[,2:ncol(test)],destination_frame=“test”)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing a random forest model</h1>
                </header>
            
            <article>
                
<p>A random forest is an ensemble of decision trees. In a decision tree, the training sample, which is based on the independent variables, will be split into two or more homogeneous sets. This algorithm deals with both categorical and continuous variables. The best attribute is selected using a recursive selection method and is split to form the leaf nodes. This continues until a criterion that's meant to stop the loop is met. Every tree that's created by the expansion of leaf nodes is considered to be a weak learner. This weak learner is built on top of the rows and columns of the subsets. The higher the number of trees, the lower the variance. Both classification and regression random forests calculate the average prediction of all of the trees to make a final prediction.</p>
<p>When a random forest is trained, some different parameters can be set. Among the most common parameters are the number of trees, the maximum number of variables, the size of terminal nodes, and the depth in each tree. Several tests should be carried out to find a balance between performance and overfitting. For example, the higher the number and the depth of the trees, the better the accuracy on a training set, but this increases the risk of overfitting. To obtain this balance, several parameters and combinations of parameters should be tested on the validation set, and then they should be cross-validated during the training process.</p>
<p>Again, this algorithm is easy to implement in the <kbd>h2o</kbd> package using the visual guide in a browser. The grid of the parameters should be implemented by coding it. The code is almost the same as the one in the previous model. This time, however, the process is more time-consuming:</p>
<pre>grid_space &lt;- list()<br/> grid_space$ntrees &lt;- c(25, 50, 75)<br/> grid_space$max_depth &lt;- c(4, 10, 20)<br/> grid_space$mtries &lt;- c(10, 14, 20)<br/> grid_space$seed &lt;- c(1234)<br/> <br/> grid &lt;- h2o.grid("randomForest", grid_id="RF_grid", x=2:110,y=1,training_frame=training, nfolds=5, hyper_params=grid_space)<br/><br/>results_grid &lt;- h2o.getGrid(grid_id = "RF_grid",<br/>                              sort_by = "auc",<br/>                              decreasing = TRUE)<br/><br/>print(results_grid)<br/> <br/> ## H2O Grid Details<br/> ## ================<br/> ## <br/> ## Grid ID: RF_grid <br/> ## Used hyper parameters: <br/> ##   -  max_depth <br/> ##   -  mtries <br/> ##   -  ntrees <br/> ##   -  seed <br/> ## Number of models: 27 <br/> ## Number of failed models: 0 <br/> ## <br/> ## Hyper-Parameter Search Summary: ordered by decreasing auc<br/> ##   max_depth mtries ntrees seed        model_ids                auc<br/> ## 1        20     20     75 1234 RF_grid_model_26 0.9928546480780869<br/> ## 2        10     10     75 1234 RF_grid_model_19 0.9922021014799943<br/> ## 3        10     10     50 1234 RF_grid_model_10 0.9921534437663471<br/> ## 4        10     20     75 1234 RF_grid_model_25 0.9920343545676484<br/> ## 5        10     20     50 1234 RF_grid_model_16 0.9919039341205663<br/> ## <br/> ## ---<br/> ##    max_depth mtries ntrees seed       model_ids                auc<br/> ## 22        20     20     25 1234 RF_grid_model_8 0.9879017816277361<br/> ## 23        20     10     25 1234 RF_grid_model_2 0.9876307203918924<br/> ## 24        10     20     25 1234 RF_grid_model_7 0.9873765449379537<br/> ## 25        10     14     25 1234 RF_grid_model_4  0.986949956763511<br/> ## 26         4     10     25 1234 RF_grid_model_0  0.984477522802471<br/> ## 27        20     14     25 1234 RF_grid_model_5  0.980687331308817</pre>
<p>As we can see, the combinations in the number of trees (<kbd>ntrees</kbd>), the depth (<kbd>max_depth</kbd>), and the number of variables to be considered in each tree (<kbd>mtries</kbd>) are tested. The resulting models are ordered using the AUC metric.</p>
<p><kbd>27</kbd> different models have been trained according to the preceding specifications. The first model, or the model that has the best accuracy, is selected:</p>
<pre>best_RF &lt;- h2o.getModel(results_grid@model_ids[[1]])</pre>
<p>The performance of this model is obtained for train and test samples:</p>
<pre>h2o.performance(model = best_RF,newdata = training)<br/> ## H2OBinomialMetrics: drf<br/> ## <br/> ## MSE:  0.001317125<br/> ## RMSE:  0.03629222<br/> ## LogLoss:  0.009026859<br/> ## Mean Per-Class Error:  0<br/> ## AUC:  1<br/> ## Gini:  1<br/> ## <br/> ## Confusion Matrix (vertical: actual; across: predicted) for F1-            optimal          threshold:<br/> ##           0   1    Error     Rate<br/> ## 0      6758   0 0.000000  =0/6758<br/> ## 1         0 333 0.000000   =0/333<br/> ## Totals 6758 333 0.000000  =0/7091<br/> ## <br/> ## Maximum Metrics: Maximum metrics at their respective thresholds<br/> ##                         metric threshold    value idx<br/> ## 1                       max f1  0.586667 1.000000  29<br/> ## 2                       max f2  0.586667 1.000000  29<br/> ## 3                 max f0point5  0.586667 1.000000  29<br/> ## 4                 max accuracy  0.586667 1.000000  29<br/> ## 5                max precision  1.000000 1.000000   0<br/> ## 6                   max recall  0.586667 1.000000  29<br/> ## 7              max specificity  1.000000 1.000000   0<br/> ## 8             max absolute_mcc  0.586667 1.000000  29<br/> ## 9   max min_per_class_accuracy  0.586667 1.000000  29<br/> ## 10 max mean_per_class_accuracy  0.586667 1.000000  29<br/><br/></pre>
<p>As you can see, the code is exactly the same as the previous model.</p>
<p>Now, we find the performance of the test or validation sample using the following code:</p>
<pre>h2o.performance(model = best_RF,newdata = validation)<br/> ## H2OBinomialMetrics: drf<br/> ## <br/> ## MSE:  0.00940672<br/> ## RMSE:  0.09698825<br/> ## LogLoss:  0.05488315<br/> ## Mean Per-Class Error:  0.06220299<br/> ## AUC:  0.9882138<br/> ## Gini:  0.9764276<br/> ## <br/> ## Confusion Matrix (vertical: actual; across: predicted) for F1-            optimal              threshold:<br/> ##           0   1    Error      Rate<br/> ## 0      2880  16 0.005525  =16/2896<br/> ## 1        17 126 0.118881   =17/143<br/> ## Totals 2897 142 0.010859  =33/3039<br/> ## <br/> ## Maximum Metrics: Maximum metrics at their respective thresholds<br/> ##                         metric threshold    value idx<br/> ## 1                       max f1  0.346667 0.884211  44<br/> ## 2                       max f2  0.280000 0.897790  49<br/> ## 3                 max f0point5  0.760000 0.897196  18<br/> ## 4                 max accuracy  0.346667 0.989141  44<br/> ## 5                max precision  1.000000 1.000000   0<br/> ## 6                   max recall  0.000000 1.000000  70<br/> ## 7              max specificity  1.000000 1.000000   0<br/> ## 8             max absolute_mcc  0.346667 0.878520  44<br/> ## 9   max min_per_class_accuracy  0.106667 0.965035  62<br/> ## 10 max mean_per_class_accuracy  0.106667 0.968878  62<br/><br/></pre>
<p>The results are almost perfect in both samples. The importance of the variable can be obtained as well:</p>
<pre>var_importance&lt;-data.frame(best_RF@model$variable_importances)<br/> h2o.varimp_plot(best_RF,20)</pre>
<p>The preceding code generates the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-710 image-border" src="assets/2105e9a6-9c14-4225-a3cc-fb03192e5ff4.png" style=""/></div>
<p>Just like Ridge regression, the probability of bankruptcy will be stored for both the train and validation samples:</p>
<pre>summary_models_train$RF&lt;-as.vector(h2o.predict(best_RF,training)[3])<br/>summary_models_test$RF&lt;-as.vector(h2o.predict(best_RF,validation)[3])</pre>
<p>Finally, we can compute the confusion matrix. Remember that the cutoff to classify a bank regarding its probability of bankruptcy is determined based on the observed proportion of bad banks in the total sample:</p>
<pre>aux&lt;-summary_models_test<br/> aux$pred&lt;-ifelse(summary_models_test$RF&gt;0.04696094,1,0)<br/> table(aux$Default,aux$pred)<br/> ##        0    1<br/> ##   0 2753  143<br/> ##   1    5  138</pre>
<p>If the random forest and Ridge regression models are compared, we can see that random forest only misclassifies five failed banks, while there are 12 misclassified banks in the Ridge regression. Nevertheless, the random forest classifies <span>more solvent banks </span><span>as failed banks than Ridge regression, meaning that it has a high level of false positives.</span></p>
<p>The irrelevant objects are, again, removed from the workspace. Moreover, we save a backup of our workspace:</p>
<pre>rm(list=setdiff(ls(), c("Model_database","train","test","summary_models_train","summary_models_test","training","validation")))<br/>save.image("Data14.RData")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gradient boosting</h1>
                </header>
            
            <article>
                
<p><strong>Gradient boosting</strong> means combining weak and average predictors to acquire one strong predictor. This ensures robustness. It is similar to a random forest, which is mainly based on decision trees. The difference is that the sample is not modified from one tree to another; only the weights of the different observations are modified.</p>
<p>Boosting trains trees sequentially by using information from previously trained trees. For this, we first need to create decision trees using the training dataset. Then, we need to create another model that does nothing but rectify the errors that occurred in the training model. This process is repeated sequentially until the specified number of trees, or some other stopping rule, is reached.</p>
<p>More specific details about the algorithm can be found in the documentation of the <kbd>h2o</kbd> package. While training the algorithm, we will need to define parameters such as the number of trees that we will combine and the minimum observation in each node, just like we did for the random forests.</p>
<p><span>The shrinkage parameter, or the rate at which boosting learns, can alter the performance of the model. We will need to consider the results of many experiments to determine the optimal parameters to ensure a high accuracy.</span></p>
<p>Our grid of parameters collects different combinations of the number of trees and the <kbd>max_depth</kbd> parameter:</p>
<pre>grid_space &lt;- list()<br/>grid_space$ntrees &lt;- c(25,75,100)<br/>grid_space$max_depth = c(4,6,8,12,16,20)</pre>
<p>Different models will be trained by executing the following code:</p>
<pre>gbm_grid &lt;- h2o.grid(hyper_params = grid_space,<br/>   algorithm = "gbm",<br/>   grid_id ="Grid1", <br/>   x=2:110,<br/>   y=1,<br/>   training_frame = training,seed=1234)</pre>
<p>The grids are ordered by <kbd>AUC</kbd>. The results are as follows:</p>
<pre>results_gbm &lt;- h2o.getGrid("Grid1", sort_by = "AUC", decreasing = TRUE)    <br/>results_gbm<br/> ## H2O Grid Details<br/> ## ================<br/> ## <br/> ## Grid ID: Grid1 <br/> ## Used hyper parameters: <br/> ##   -  max_depth <br/> ##   -  ntrees <br/> ## Number of models: 18 <br/> ## Number of failed models: 0 <br/> ## <br/> ## Hyper-Parameter Search Summary: ordered by decreasing AUC<br/> ##    max_depth ntrees      model_ids                auc<br/> ## 1         16    100 Grid1_model_16                1.0<br/> ## 2          4    100 Grid1_model_12                1.0<br/> ## 3         16     25  Grid1_model_4                1.0<br/> ## 4         20     75 Grid1_model_11                1.0<br/> ## 5          6     75  Grid1_model_7                1.0<br/> ## 6         20    100 Grid1_model_17                1.0<br/> ## 7          8     75  Grid1_model_8                1.0<br/> ## 8         20     25  Grid1_model_5                1.0<br/> ## 9         12     75  Grid1_model_9                1.0<br/> ## 10        16     75 Grid1_model_10                1.0<br/> ## 11         6    100 Grid1_model_13                1.0<br/> ## 12        12    100 Grid1_model_15                1.0<br/> ## 13         8    100 Grid1_model_14                1.0<br/> ## 14         4     75  Grid1_model_6 0.9999986669119549<br/> ## 15        12     25  Grid1_model_3 0.9999986669119549<br/> ## 16         8     25  Grid1_model_2 0.9999922236530701<br/> ## 17         6     25  Grid1_model_1 0.9998680242835318<br/> ## 18         4     25  Grid1_model_0 0.9977795196794901</pre>
<p>Most models obtain a perfect classification. This might be a sign of overfitting. Let's take a look at the performance of the first model on the validation sample:</p>
<pre>best_GBM &lt;- h2o.getModel(results_gbm@model_ids[[1]])<br/>h2o.performance(model = best_GBM,newdata = as.h2o(test))<br/> ## H2OBinomialMetrics: gbm<br/> ## <br/> ## MSE:  0.01053012<br/> ## RMSE:  0.1026164<br/> ## LogLoss:  0.06001792<br/> ## Mean Per-Class Error:  0.05905179<br/> ## AUC:  0.9876222<br/> ## Gini:  0.9752444<br/> ## <br/> ## Confusion Matrix (vertical: actual; across: predicted) for F1-            optimal          threshold:<br/> ##           0   1    Error      Rate<br/> ## 0      2878  18 0.006215  =18/2896<br/> ## 1        16 127 0.111888   =16/143<br/> ## Totals 2894 145 0.011188  =34/3039<br/> ## <br/> ## Maximum Metrics: Maximum metrics at their respective thresholds<br/> ##                         metric threshold    value idx<br/> ## 1                       max f1  0.076792 0.881944 143<br/> ## 2                       max f2  0.010250 0.892857 154<br/> ## 3                 max f0point5  0.852630 0.906902 118<br/> ## 4                 max accuracy  0.076792 0.988812 143<br/> ## 5                max precision  0.999962 1.000000   0<br/> ## 6                   max recall  0.000006 1.000000 392<br/> ## 7              max specificity  0.999962 1.000000   0<br/> ## 8             max absolute_mcc  0.076792 0.876096 143<br/> ## 9   max min_per_class_accuracy  0.000181 0.958042 246<br/> ## 10 max mean_per_class_accuracy  0.000816 0.963611 203<br/><br/></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The results are very good, even in the test sample. The first model is selected and the predictions are stored, as in the previous models:</p>
<pre>summary_models_train$GBM&lt;-as.vector(h2o.predict(best_GBM,training)[3])<br/>summary_models_test$GBM&lt;-as.vector(h2o.predict(best_GBM,validation)[3])</pre>
<p>Finally, the confusion table in the test sample is calculated:</p>
<pre>aux&lt;-summary_models_test<br/>aux$pred&lt;-ifelse(summary_models_test$GBM&gt;0.04696094,1,0)<br/>table(aux$Default,aux$pred)<br/> ##    <br/> ##        0    1<br/> ##   0 2876   20<br/> ##   1   15  128</pre>
<p>A total of 14 failed banks and 25 non-failed banks are misclassified. Let's save the results:</p>
<pre>rm(list=setdiff(ls(), c("Model_database","train","test","summary_models_train","summary_models_test","training","validation")))<br/>save.image("Data15.RData")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning in neural networks</h1>
                </header>
            
            <article>
                
<p>For machine learning, we need systems that can process nonlinear and unrelated sets of data. This is very important so that we can make predictions for bankruptcy problems, since the relationship between the default and explanatory variables will rarely be linear. Therefore, using neural networks is the best possible solution.</p>
<p><strong>Artificial neural networks</strong> (<strong>ANNs</strong>) have <span>long</span><span> </span><span>since</span><span> </span>been used to solve bankruptcy problems. An ANN is a computer system that has a number of interconnected processors. These processors provide outputs by processing information and by responding dynamically to the inputs that are provided. A prominent and basic example of ANN is the <strong>multilayer perceptron</strong> (<strong>MLP</strong>). <span>An MLP can be represented as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-711 image-border" src="assets/06c35f9e-3a7d-40f6-8910-7f225859bd6b.png" style=""/></div>
<p class="mce-root">Except for the input nodes, each node is a neuron that uses a nonlinear activation function, which was sent in.</p>
<p>As is evident from its diagram, an MLP is nothing but a <strong>feed-forward neural network</strong>. This means that the input information that's provided will only move in a forward direction. This type of network usually consists of one input, one hidden layer, and one output layer. The input layer represents the input data of the model, or the variables. In our case, these are the financial variables. No calculations are made in this layer. Hidden layers are where intermediate processing or computation is done. They perform computation and then transfer the weights (the signals or information) from the input layer to the following layer. Finally, the output layer takes inputs from the hidden layer and calculates the outputs of the network. The input nodes use a nonlinear activation function to send infor<span>mation</span> <span>from one layer to the next. The purpose of the activation function is to transform the input signal into an output signal that models complex nonlinear patterns.  </span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Perceptrons networks learn by modifying the weights after every set of data is processed. These weights specify the number of errors that occur when processing the input, which is obtained by comparing the expected output.</p>
<p>Is deep learning different from MLP? MLP is just one type of deep learning algorithm. In many cases, deep learning is different from an MLP network, but only because of the complexity in the calculations and the number of hidden layers. Deep learning can be considered an MLP with two or more hidden layers. When two or more hidden layers are included, the learning process should be different as well, because the backpropagation learning rule that's used in MLP fails. The perceptron update rule is prone to vanishing and exploding gradients, making it difficult to train networks with more than one or two layers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Designing a neural network</h1>
                </header>
            
            <article>
                
<p>When designing a multilayer network, ensure that you determine the appropriate number of required layers for better accuracy and precision. Often, for many models, just one hidden layer is enough to solve the problem of classification. Nevertheless, the use of many hidden layers has demonstrated its usefulness in areas such as speech recognition or object detection, among others. Another thing to consider is the number of neurons in every hidden layer. This is a very important aspect. Mistakes in estimating these values can lead to problems such as overfitting, when too many neurons are added, and underfitting, when not enough neurons are added.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a neural network</h1>
                </header>
            
            <article>
                
<p>The <kbd>h2o</kbd> package helps us train the neural networks. Deep learning models have many input parameters. In this exercise, the following parameters will be tested:</p>
<pre>hyper_params &lt;- list(<br/>   hidden=list(c(5),c(80,80,80),c(75,75)),<br/>   input_dropout_ratio=c(0.05,0.1,0.15,0.2,0.25),<br/>   rate=c(0.01,0.02,0.10))</pre>
<p>Here, we will test three structures: first, a network containing only a hidden layer with 25 neurons, then a network with three hidden layers with 32 neurons in each layer, and finally, a two hidden layer network with 64 <span><span>neurons</span></span> in each layer.</p>
<p><span>A neural network learns, and neurons progressively specialize in values for specific variables. If neurons are too specialized in the training set, there is a high risk of overfitting. T<span>o avoid overfitting, the </span><kbd>input_dropout_ratio</kbd><span> </span><span>command</span><span> </span><span>is included. </span>The dropout technique is a regularization approach for neural network models to improve the generalization of neural networks.</span></p>
<p>During training, the dropout approach randomly selects neurons and ignores them during training. In practice, at each training step, a different network is created because some of the random units are removed and trained using backpropagation, as usual. This forces the network to learn several independent representations of the patterns with identical input and output, improving the generalization.</p>
<div class="packt_infobox">To obtain more information related to the dropout approach, I recommend the original paper, <em>Dropout: </em><em><em>A Sim</em>ple Way to Prevent Neural Networks from Overfitting</em>, by Nitish Srivastava et al. It is available at <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank">https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a>.</div>
<p>The suggested values for the input layer dropout ratio are 0.1 or 0.2. Finally, with the <kbd>rate</kbd> command, we can specify the learning rate. Remember, if the learning rate is set too high, the model may become less stable, and if it is set too low, then the convergence will be very slow.</p>
<p>Let's write some training code:</p>
<pre>deep_grid &lt;- h2o.grid(<br/>   algorithm="deeplearning",<br/>   grid_id="dl_grid", <br/>   training_frame=training,<br/>   validation_frame=as.h2o(test),<br/>   x=2:110,<br/>   y=1,<br/>   epochs=2,<br/>   stopping_metric="AUC",<br/>   stopping_tolerance=1e-2,<br/>   stopping_rounds=2,<br/>   score_duty_cycle=0.01,  <br/>   l1=1e-5,<br/>   l2=1e-5,<br/>   activation=c("Rectifier"),<br/>   nfolds=5,<br/>   hyper_params=hyper_params,standardize=TRUE,seed=1234)</pre>
<p>The parameters in the preceding code can be described as follows:</p>
<ul>
<li><kbd>epochs</kbd>: The value that's specified here determines the number of times the dataset has to be streamed while learning.</li>
<li><kbd>stopping_metric</kbd>: This specifies the metric to use for early stopping, which in our case is AUC.</li>
<li><kbd>stopping_tolerance</kbd> and <kbd>stopping rounds</kbd>: These determine a tolerance value before the model stops learning, and a stopping value that prevents the model from learning when the <kbd>stopping_metric</kbd> doesn't improve after the number of rounds specified, respectively. When cross-fold validation is specified (as in our case), this option will apply on all cross-validation models. In our case, we set the options of <kbd>stopping_tolerance=1e-2</kbd> and <kbd>stopping_rounds = 2</kbd>, which means that the model won't be trained after 2 rounds of iterations or if there isn't an improvement of at least 2% ( <kbd>1e-2</kbd>).</li>
<li><kbd>score_duty_cycle</kbd>: This indicates how much time to spend scoring versus training. The values are percentages ranging from 0 to 1. Lower values indicate more training. The default value of this option is 0.1, which indicates that 10% of the time should be spent on scoring and the remaining 90% should be spent on training.</li>
<li><kbd>l1</kbd> and <kbd>l2</kbd>: The value that's added here is the regularization index, which ensures better generalization and stability.</li>
<li><kbd>activation</kbd>: Activation functions such as <kbd>tanh</kbd>, <kbd>tanh with dropout</kbd>, <kbd>Maxout</kbd>, and others can be mentioned here.</li>
<li><kbd>nfolds</kbd>: This indicates the number of folds for cross-validation. The training process is very time-consuming because several configurations are tested. The performance of the different configurations can be obtained by running the following code:</li>
</ul>
<pre style="padding-left: 60px">results_deep &lt;- h2o.getGrid("dl_grid",sort_by="auc",decreasing=TRUE)<br/>results_deep<br/> ## H2O Grid Details<br/> ## ================<br/> ## <br/> ## Grid ID: dl_grid <br/> ## Used hyper parameters: <br/> ##   -  hidden <br/> ##   -  input_dropout_ratio <br/> ##   -  rate <br/> ## Number of models: 45 <br/> ## Number of failed models: 0 <br/> ## <br/> ## Hyper-Parameter Search Summary: ordered by decreasing auc<br/> ##         hidden input_dropout_ratio rate        model_ids<br/> ## 1     [75, 75]                0.25 0.01 dl_grid_model_14<br/> ## 2     [75, 75]                0.25  0.1 dl_grid_model_44<br/> ## 3     [75, 75]                 0.2 0.01 dl_grid_model_11<br/> ## 4 [80, 80, 80]                0.25 0.02 dl_grid_model_28<br/> ## 5     [75, 75]                 0.1 0.01  dl_grid_model_5<br/> ##                  auc<br/> ## 1 0.9844357527103902<br/> ## 2 0.9841366966255987<br/> ## 3 0.9831344365969994<br/> ## 4 0.9830902225101693<br/> ## 5 0.9830724480029008<br/> ## <br/> ## ---<br/> ##    hidden input_dropout_ratio rate        model_ids                auc<br/> ## 40    [5]                 0.1  0.1 dl_grid_model_33 0.9603608491593103<br/> ## 41    [5]                 0.1 0.01  dl_grid_model_3 0.9599749201702442<br/> ## 42    [5]                 0.2 0.01  dl_grid_model_9 0.9599749201702442<br/> ## 43    [5]                 0.2 0.02 dl_grid_model_24 0.9591890647676383<br/> ## 44    [5]                0.05 0.02 dl_grid_model_15 0.9587149297862527<br/> ## 45    [5]                0.15  0.1 dl_grid_model_36 0.9575646969846437</pre>
<p>The best model is obtained with three hidden layers of 32 units, a dropout ratio of <kbd>0.25</kbd>, and a learning rate of <kbd>0.02</kbd>.</p>
<p>The best model is selected as follows:</p>
<pre>best_deep &lt;- h2o.getModel(results_deep@model_ids[[1]])</pre>
<p>The performance is obtained for the test sample:</p>
<pre>h2o.performance(model = best_deep,newdata = validation)<br/> ## H2OBinomialMetrics: deeplearning<br/> ## <br/> ## MSE:  0.02464987<br/> ## RMSE:  0.1570028<br/> ## LogLoss:  0.1674725<br/> ## Mean Per-Class Error:  0.1162044<br/> ## AUC:  0.9794568<br/> ## Gini:  0.9589137<br/> ## <br/> ## Confusion Matrix (vertical: actual; across: predicted) for F1-         optimal threshold:<br/> ##           0   1    Error      Rate<br/> ## 0      2871  25 0.008633  =25/2896<br/> ## 1        32 111 0.223776   =32/143<br/> ## Totals 2903 136 0.018756  =57/3039<br/> ## <br/> ## Maximum Metrics: Maximum metrics at their respective thresholds<br/> ##                         metric threshold    value idx<br/> ## 1                       max f1  0.001538 0.795699 135<br/> ## 2                       max f2  0.000682 0.812672 153<br/> ## 3                 max f0point5  0.011028 0.831904 109<br/> ## 4                 max accuracy  0.001538 0.981244 135<br/> ## 5                max precision  0.999998 1.000000   0<br/> ## 6                   max recall  0.000000 1.000000 398<br/> ## 7              max specificity  0.999998 1.000000   0<br/> ## 8             max absolute_mcc  0.001538 0.786148 135<br/> ## 9   max min_per_class_accuracy  0.000017 0.937063 285<br/> ## 10 max mean_per_class_accuracy  0.000009 0.943153 314<br/><br/></pre>
<p>As in the previous models, the predictions on the training and validation samples are stored:</p>
<pre>summary_models_train$deep&lt;-as.vector(h2o.predict(best_deep,training)[3])<br/>summary_models_test$deep&lt;- as.vector(h2o.predict(best_deep,validation)[3])</pre>
<p>The confusion matrix is also obtained for the validation sample:</p>
<pre>aux&lt;-summary_models_test<br/> aux$pred&lt;-ifelse(summary_models_test$deep&gt;0.04696094,1,0)<br/> table(aux$Default,aux$pred)<br/> ##    <br/> ##        0    1<br/> ##   0 2886   10<br/> ##   1   61   82<br/><br/>rm(list=setdiff(ls(), c("Model_database","train","test","summary_models_train","summary_models_test","training","validation")))<br/>save.image("Data16.RData")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support vector machines</h1>
                </header>
            
            <article>
                
<p>The <strong>support vector machine</strong> (<strong>SVM</strong>) algorithm is a supervised learning technique. To understand this algorithm, take a look at the following diagram for the optimal hyperplane and maximum margin:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c68909aa-0f0f-4048-be2f-80e8bc1047f3.png" style=""/></div>
<p>In this classification problem, we only have two classes that exist for many possible solutions to a problem. As shown in the preceding diagram, the SVM classifies these objects by calculating an optimal hyperplane and maximizing the margins between the classes. Both of these things will differentiate the classes to the maximum extent. Samples that are placed closest to the margin are known as <strong>support vectors</strong>. The problem is then treated as an optimization problem and can be solved by optimization techniques, the most common one being the use of Lagrange multipliers.</p>
<p>Even in a separable linear problem, as shown in the preceding diagram, sometimes, it is not always possible to obtain a perfect separation. In these cases, the SVM model is the one that maximizes the margin while minimizing the number of misclassifications. In the real world, the problems are too far apart to be linearly separated, at least without a previous treatment or transformation of data. In the following diagram, the difference between a linear separable problem and a nonlinear separable problem is shown:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/09d1a29f-4e7b-4185-8f50-0290a6af9fcd.png" style=""/></div>
<p>To handle nonlinear problems, a kernel function maps the data to different spaces. This means that data is transformed to a higher-dimensional space. This technique is known as the <strong>kernel trick</strong>, because sometimes it is possible to perform a linear separation between classes, making transformations in the data. </p>
<p>The following are the advantages of the SVM algorithm:</p>
<ul>
<li>SVM is simple</li>
<li>SVM is a combination of statistical and machine learning techniques</li>
<li>SVM can be useful in solving financial problems like our problem statement</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Selecting SVM parameters</h1>
                </header>
            
            <article>
                
<p>Let's discuss some parameters that we might need so that we can use SVM.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The SVM kernel parameter</h1>
                </header>
            
            <article>
                
<p>One of the main difficulties of SVM is selecting the kernel that transforms the data. The following are the most commonly used transformations:</p>
<ul>
<li>Linear</li>
<li>Polynomial</li>
<li>Radial basis</li>
<li>Sigmoid</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The cost parameter</h1>
                </header>
            
            <article>
                
<p>The control on the liquidation and the transaction between the training error and the model complexity will be looked after by the cost parameter (<kbd>C</kbd>). If you have a relatively smaller number for <kbd>C</kbd>, there will be a higher number of training errors. If <kbd>C</kbd> is a bigger number, you could obtain a overfitted model, which means that your trained model has learned all of your training data but it is likely that the model does not work properly on any other sample. You can set the cost to any value between 1.001 and 100.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gamma parameter</h1>
                </header>
            
            <article>
                
<p>The gamma parameter is needed while using a Gaussian kernel. This parameter will calculate the level of influence that each training sample can accomplish. Here, you may consider the lower values to be <em>far</em> and the higher values to be <em>close</em>.</p>
<p>Gamma is actually the opposite of the support vectors that we have seen. Therefore, in an SVM, different values of all three parameters should be tested.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training an SVM model</h1>
                </header>
            
            <article>
                
<p>The SVM algorithm is not available in the <kbd>h2o</kbd> package. To train the <kbd>SVM</kbd> classifier, we are going to use the <kbd>caret</kbd> package. Remember that our target value takes two different values:</p>
<pre>levels(train$Default)<br/> ## [1] "0" "1"</pre>
<p>Although the different values of this variable (<kbd>0</kbd> and <kbd>1</kbd>) do not display problems in other algorithms, in this case, we need to make a little transformation here. The categories in the target variable can only take values like <kbd>X0</kbd> or <kbd>X1</kbd>, so we need to transform them. Let's write some code for this:</p>
<pre>levels(train$Default) &lt;- make.names(levels(factor(train$Default)))<br/>levels(train$Default)<br/> ## [1] "X0" "X1"</pre>
<p>These values are also transformed in the test sample:</p>
<pre>test$Default&lt;-as.factor(test$Default)<br/>levels(test$Default) &lt;- make.names(levels(factor(test$Default)))<br/> levels(test$Default)<br/> ## [1] "X0" "X1"</pre>
<p>We will create a grid with different values of the cost and gamma parameters in a similar way to the <kbd>h2o</kbd> package:</p>
<pre>svmGrid &lt;- expand.grid(sigma= 2^c(-20, -15,-10, -5, 0), C= 2^c(2:5))<br/>print(svmGrid)<br/> ##              sigma  C<br/> ## 1  0.0000009536743  4<br/> ## 2  0.0000305175781  4<br/> ## 3  0.0009765625000  4<br/> ## 4  0.0312500000000  4<br/> ## 5  1.0000000000000  4<br/> ## 6  0.0000009536743  8<br/> ## 7  0.0000305175781  8<br/> ## 8  0.0009765625000  8<br/> ## 9  0.0312500000000  8<br/> ## 10 1.0000000000000  8<br/> ## 11 0.0000009536743 16<br/> ## 12 0.0000305175781 16<br/> ## 13 0.0009765625000 16<br/> ## 14 0.0312500000000 16<br/> ## 15 1.0000000000000 16<br/> ## 16 0.0000009536743 32<br/> ## 17 0.0000305175781 32<br/> ## 18 0.0009765625000 32<br/> ## 19 0.0312500000000 32<br/> ## 20 1.0000000000000 32</pre>
<p>Then, we will run the following code to train the different models:</p>
<pre>library(caret)<br/>set.seed(1234)<br/> <br/>SVM &lt;- train(Default ~ ., data = train[,2:ncol(train)], <br/> method = "svmRadial",<br/> standardize=TRUE,<br/> tuneGrid = svmGrid,<br/> metric = "ROC",<br/> allowParallel=TRUE,<br/> trControl = trainControl(method = "cv", 5, classProbs = TRUE, <br/> summaryFunction=twoClassSummary))</pre>
<p>To train an <kbd>SVM</kbd> classifier, the <kbd>train()</kbd> method should be passed with the <kbd>method</kbd> parameter as <kbd>svmRadial</kbd>, which is our selected kernel. <kbd>TuneGrid</kbd> represents the different combinations of the cost and gamma parameters. The accuracy of models is measured using the <kbd>ROC</kbd> metric. 5-fold cross validation is used.</p>
<p class="mce-root"/>
<p>Once the model has been trained, we can view the results as follows:</p>
<pre>print(SVM)<br/> ## Support Vector Machines with Radial Basis Function Kernel <br/> ## <br/> ## 7091 samples<br/> ##  108 predictor<br/> ##    2 classes: 'X0', 'X1' <br/> ## <br/> ## No pre-processing<br/> ## Resampling: Cross-Validated (5 fold) <br/> ## Summary of sample sizes: 5674, 5673, 5672, 5672, 5673 <br/> ## Resampling results across tuning parameters:<br/> ## <br/> ##   sigma            C   ROC        Sens       Spec     <br/> ##   0.0000009536743   4  0.9879069  0.9899383  0.8710086<br/> ##   0.0000009536743   8  0.9879135  0.9903822  0.8710086<br/> ##   0.0000009536743  16  0.9879092  0.9900863  0.8710086<br/> ##   0.0000009536743  32  0.9880736  0.9909741  0.8679783<br/> ##   0.0000305175781   4  0.9894669  0.9943777  0.8380371<br/> ##   0.0000305175781   8  0.9903574  0.9957094  0.8439168<br/> ##   0.0000305175781  16  0.9903018  0.9958573  0.8499774<br/> ##   0.0000305175781  32  0.9903865  0.9958572  0.8619629<br/> ##   0.0009765625000   4  0.9917597  0.9960052  0.8739937<br/> ##   0.0009765625000   8  0.9913792  0.9963011  0.8590231<br/> ##   0.0009765625000  16  0.9900214  0.9960050  0.8379919<br/> ##   0.0009765625000  32  0.9883768  0.9961529  0.8410222<br/> ##   0.0312500000000   4  0.9824358  0.9789899  0.9159656<br/> ##   0.0312500000000   8  0.9824358  0.9767682  0.8735414<br/> ##   0.0312500000000  16  0.9824358  0.9783977  0.8622343<br/> ##   0.0312500000000  32  0.9824358  0.9755850  0.9189959<br/> ##   1.0000000000000   4  0.4348777  1.0000000  0.0000000<br/> ##   1.0000000000000   8  0.4336278  1.0000000  0.0000000<br/> ##   1.0000000000000  16  0.4273365  1.0000000  0.0000000<br/> ##   1.0000000000000  32  0.4325194  1.0000000  0.0000000<br/> ## <br/> ## ROC was used to select the optimal model using the largest value.<br/> ## The final values used for the model were sigma = 0.0009765625 and C     = 4.</pre>
<p>In summary, the best parameters are the following ones:</p>
<pre>SVM$bestTune<br/> ##          sigma C<br/> ## 9 0.0009765625 4</pre>
<p>Furthermore, we can access the model with the best parameters as follows:</p>
<pre>SVM$finalModel<br/> ## Support Vector Machine object of class "ksvm" <br/> ## <br/> ## SV type: C-svc  (classification) <br/> ##  parameter : cost C = 4 <br/> ## <br/> ## Gaussian Radial Basis kernel function. <br/> ##  Hyperparameter : sigma =  0.0009765625 <br/> ## <br/> ## Number of Support Vectors : 252 <br/> ## <br/> ## Objective Function Value : -619.9088 <br/> ## Training error : 0.007333 <br/> ## Probability model included.</pre>
<p>The performance of the model is not directly obtained, as in the <kbd>h2o</kbd> package. This isn't difficult to do, but we need to use the <kbd>ROCR</kbd> package:</p>
<pre>library(ROCR)<br/>SVM_pred&lt;-as.numeric(unlist(predict(SVM, newdata =test, type = "prob")[2]))<br/>pred2 &lt;- prediction(SVM_pred,test$Default)<br/>pred3 &lt;- performance(pred2,"tpr","fpr")<br/>plot(pred3, lwd=1, colorize=FALSE)<br/>lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);  </pre>
<p>The preceding code generates the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-712 image-border" src="assets/057e4d18-0ae2-43cd-9ea9-ebeecd282edd.png" style=""/></div>
<p>The Gini index can be calculated as <kbd>2*ROC -1</kbd>. We can use the <kbd>Hmisc</kbd> package to calculate the ROC and then calculate the Gini index, as follows:</p>
<pre>library(Hmisc)<br/>print("Gini indicator of SVM in the test sample is:")<br/> ## [1] "Gini indicator of SVM in the test sample is:"<br/><br/>print(abs(as.numeric(2*rcorr.cens(SVM_pred,test[,'Default'])[1]-1)))<br/> ## [1] 0.9766884</pre>
<p>Gini reaches 0.9766 in the test sample. As in the previous models, the confusion matrix is calculated using the validation or test sample. To do this, first, the probabilities are stored for both the train and test samples:</p>
<pre>summary_models_train$SVM&lt;-as.numeric(unlist(predict(SVM, newdata =train, type = "prob")[2]))<br/>summary_models_test$SVM&lt;- as.numeric(unlist(predict(SVM, newdata =test, type = "prob")[2]))</pre>
<p>The confusion table is now calculated on the test sample:</p>
<pre>aux&lt;-summary_models_test<br/>aux$pred&lt;-ifelse(summary_models_test$SVM&gt;0.04696094,1,0)<br/>table(aux$Default,aux$pred)<br/> ##    <br/> ##        0    1<br/> ##   0 2828   68<br/> ##   1    8  135</pre>
<p>SVM does a good job of classifying banks. Only 76 banks (<kbd>68</kbd>+<kbd>8</kbd>) are misclassified on the test sample. Now, a new backup of the workspace is created:</p>
<pre>rm(list=setdiff(ls(), c("Model_database","train","test","summary_models_train","summary_models_test","train_woe","test_woe")))<br/>save.image("~/Data17.RData")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensembles</h1>
                </header>
            
            <article>
                
<p>At this point, we have trained five different models. The predictions are stored in two data frames, one for training and the other for the validation samples:</p>
<pre>head(summary_models_train)<br/> ##    ID_RSSD Default          GLM RF            GBM              deep<br/> ## 4       37       0 0.0013554364  0 0.000005755001 0.000000018217172<br/> ## 21     242       0 0.0006967876  0 0.000005755001 0.000000002088871<br/> ## 38     279       0 0.0028306028  0 0.000005240935 0.000003555978680<br/> ## 52     354       0 0.0013898732  0 0.000005707480 0.000000782777042<br/> ## 78     457       0 0.0021731695  0 0.000005755001 0.000000012535539<br/> ## 81     505       0 0.0011344433  0 0.000005461855 0.000000012267744<br/> ##             SVM<br/> ## 4  0.0006227083<br/> ## 21 0.0002813123<br/> ## 38 0.0010763298<br/> ## 52 0.0009740568<br/> ## 78 0.0021555739<br/> ## 81 0.0005557417</pre>
<p>Let's summarize the accuracy of the previously trained models. First, the predictive power of each classifier will be calculated using the Gini index. With the following code, the Gini index for the training and validation samples is calculated:</p>
<pre>gini_models&lt;-as.data.frame(names(summary_models_train[,3:ncol(summary_models_train)]))<br/>colnames(gini_models)&lt;-"Char"<br/><br/>for (i in 3:ncol(summary_models_train))<br/>{<br/> <br/>   gini_models$Gini_train[i-2]&lt;-(abs(as.numeric(2*rcorr.cens(summary_models_train[,i],summary_models_train$Default)[1]-1)))<br/> <br/>   gini_models$Gini_test[i-2]&lt;-(abs(as.numeric(2*rcorr.cens(summary_models_test[,i],summary_models_test$Default)[1]-1)))<br/> <br/>}</pre>
<p>The results are stored in a data frame called <kbd>gini_models</kbd>. The variation in the predictive power between the train and test samples is also calculated:</p>
<pre>gini_models$var_train_test&lt;-(gini_models$Gini_train-gini_models$Gini_test)/gini_models$Gini_train<br/>print(gini_models)<br/><br/> ##   Char Gini_train Gini_test var_train_test<br/> ## 1  GLM  0.9906977 0.9748967     0.01594943<br/> ## 2   RF  1.0000000 0.9764276     0.02357242<br/> ## 3  GBM  1.0000000 0.9754665     0.02453348<br/> ## 4 deep  0.9855324 0.9589837     0.02693848<br/> ## 5  SVM  0.9920815 0.9766884     0.01551595</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>There are not really many significant differences between the models. The SVM is the model with the highest predictive power in the test sample. On the other hand, the deep learning model obtains the worst results.</p>
<p>These results indicate that it is not very difficult to find banks that will fail in less than a year from the current financial statement, which is how we defined our target variable.</p>
<p>We can also see the predictive power of each model, depending on the number of banks that are correctly classified:</p>
<pre>decisions_train &lt;- summary_models_train<br/> <br/>decisions_test &lt;- summary_models_test</pre>
<p>Now, let's create new data frames where the banks are classified as solvent or non-solvent banks, depending on the predicted probabilities, as we have done for each model:</p>
<pre>for (m in 3:ncol(decisions_train))<br/>{<br/>   <br/>   decisions_train[,m]&lt;-ifelse(decisions_train[,m]&gt;0.04696094,1,0)<br/>   <br/>   decisions_test[,m]&lt;-ifelse(decisions_test[,m]&gt;0.04696094,1,0)<br/>   <br/> }</pre>
<p>Now, a function that counts the number of banks as correctly and non-correctly classified is created:</p>
<pre>accuracy_function &lt;- function(dataframe, observed, predicted)<br/>{<br/> bads&lt;-sum(as.numeric(as.character(dataframe[,observed])))<br/>  goods&lt;-nrow(dataframe)-bads<br/>   y &lt;- as.vector(table(dataframe[,predicted], dataframe[,observed]))<br/>   names(y) &lt;- c("TN", "FP", "FN", "TP")<br/>  return(y)<br/> }</pre>
<p>By running the preceding function, we will see a summary of the performance of each model. First, the function is applied on the training sample:</p>
<pre>print("Accuracy GLM model:")<br/> ## [1] "Accuracy GLM model:"<br/>accuracy_function(decisions_train,"Default","GLM")<br/> ##   TN   FP   FN   TP <br/> ## 6584  174    9  324<br/><br/>print("Accuracy RF model:")<br/> ## [1] "Accuracy RF model:"<br/>accuracy_function(decisions_train,"Default","RF")<br/> ##   TN   FP   FN   TP <br/> ## 6608  150    0  333<br/><br/>print("Accuracy GBM model:")<br/> ## [1] "Accuracy GBM model:"<br/>accuracy_function(decisions_train,"Default","GBM")<br/> ##   TN   FP   FN   TP <br/> ## 6758    0    0  333<br/><br/>print("Accuracy deep model:")<br/> ## [1] "Accuracy deep model:"<br/>accuracy_function(decisions_train,"Default","deep")<br/> ##   TN   FP   FN   TP <br/> ## 6747   11  104  229<br/><br/>print("Accuracy SVM model:")<br/> ## [1] "Accuracy SVM model:"<br/>accuracy_function(decisions_train,"Default","SVM")<br/> ##   TN   FP   FN   TP <br/> ## 6614  144    7  326</pre>
<p>Then, we can see the results of the different models in our test sample:</p>
<pre>print("Accuracy GLM model:")<br/> ## [1] "Accuracy GLM model:"<br/>accuracy_function(decisions_test,"Default","GLM")<br/> ##   TN   FP   FN   TP<br/> ## 2818   78    8  135<br/><br/>print("Accuracy RF model:")<br/> ## [1] "Accuracy RF model:"<br/>accuracy_function(decisions_test,"Default","RF")<br/> ##   TN   FP   FN   TP<br/> ## 2753  143    5  138<br/><br/>print("Accuracy GBM model:")<br/> ## [1] "Accuracy GBM model:"<br/>accuracy_function(decisions_test,"Default","GBM")<br/> ##   TN   FP   FN   TP<br/> ## 2876   20   15  128<br/><br/>print("Accuracy deep model:")<br/> ## [1] "Accuracy deep model:"<br/>accuracy_function(decisions_test,"Default","deep")<br/> ##   TN   FP   FN   TP<br/> ## 2886   10   61   82<br/><br/>print("Accuracy SVM model:")<br/> ## [1] "Accuracy SVM model:"<br/>accuracy_function(decisions_test,"Default","SVM")<br/> ##   TN   FP   FN   TP<br/> ## 2828   68    8  135</pre>
<p>According to the table that was measured on the test sample, <kbd>RF</kbd> is the most accurate classifier of the failed banks, but this also misclassifies <kbd>138</kbd> solvent banks as failed, providing false alerts.</p>
<p>The results of the different models are correlated:</p>
<pre>correlations&lt;-cor(summary_models_train[,3:ncol(summary_models_train)], use="pairwise", method="pearson")<br/> <br/>print(correlations)<br/> ##            GLM        RF       GBM      deep       SVM<br/> ## GLM  1.0000000 0.9616688 0.9270350 0.8010252 0.9910695<br/> ## RF   0.9616688 1.0000000 0.9876728 0.7603979 0.9719735<br/> ## GBM  0.9270350 0.9876728 1.0000000 0.7283464 0.9457436<br/> ## deep 0.8010252 0.7603979 0.7283464 1.0000000 0.7879191<br/> ## SVM  0.9910695 0.9719735 0.9457436 0.7879191 1.0000000</pre>
<p>It might be interesting to combine the results of the different models to obtain a better model. Here, the concept of ensembles comes in handy. <strong>Ensemble</strong> is a technique that's used to combine different algorithms to make a more robust model. This combined model incorporates the predictions from all the base learners. The resulting model will have a higher level of accuracy than the accuracy that would be attained if the models were run separately. <span>In fact, some of the previous models that we've developed are ensemble models, for example; the random forest or</span><span> </span><strong><span>Gradient Boosting Machine</span></strong><span> </span><span>(</span><strong><span>GBM</span></strong><span>). </span>There are many options when creating an ensemble. In this section, we will look at different alternatives, from the simplest to those that are more complex.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Average model</h1>
                </header>
            
            <article>
                
<p>This is simply defined as taking the average of the predictions from the models:</p>
<pre>summary_models_test$avg&lt;-(summary_models_test$GLM + summary_models_test$RF + summary_models_test$GBM + summary_models_test$deep + summary_models_test$SVM)/5</pre>
<p>Thus, the final probability of the failure of a bank will be calculated as the simple average of the probabilities of failure of the previous five models.</p>
<p>The predictive power of this simple ensemble is as follows:</p>
<pre>abs(as.numeric(2*rcorr.cens(summary_models_test[,"avg"],summary_models_test$Default)[1]-1))<br/> ## [1] 0.9771665</pre>
<p>We can create a confusion matrix as follows:</p>
<pre>aux&lt;-summary_models_test<br/>aux$pred&lt;-ifelse(summary_models_test$avg&gt;0.04696094,1,0)<br/>table(aux$Default,aux$pred) <br/> ##        0    1<br/> ##   0 2834   62<br/> ##   1    7  136</pre>
<p>This combined model only misclassifies <kbd>7</kbd> failed banks and <kbd>62</kbd> non-failed banks. Apparently, this averaged model is better than the performance of all of the individual models.</p>
<p>To add some degree of conservatism, we might think that a better approach would be to assign the highest probability of failure from the different models. Nevertheless, this approach is not likely to be successful, because we have observed before that random forest creates false alarms for some banks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Majority vote</h1>
                </header>
            
            <article>
                
<p>This is defined as taking the prediction with the maximum vote while predicting the outcome of a classification problem. First, we need to assign a vote for each model. This step is already done in the <kbd>decisions_test</kbd> data frame. A bank will be classified as non-solvent if three of the five models classify it as such. Let's see the results of this approach:</p>
<pre>decisions_test$votes&lt;-rowSums(decisions_test[,3:7])<br/>decisions_test$majority_vote&lt;-ifelse(decisions_test$votes&gt;2,1,0) <br/>    <br/>table(decisions_test$Default,decisions_test$majority_vote)<br/> ##        0    1<br/> ##   0 2844   52<br/> ##   1    8  135</pre>
<p>The results seem not to be as good as the individual models or the ensemble that considered the average probabilities:</p>
<pre>rm(list=setdiff(ls(), c("Model_database","train","test","summary_models_train","summary_models_test","train_woe","test_woe","decisions_train","decisions_test")))<br/>save.image("~/Data18.RData")<br/>rm(list=ls())</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model of models</h1>
                </header>
            
            <article>
                
<p>This involves combining the individual output of the models (such as random forest or SVM) using another machine learning model (such as Lasso, GBM, or random forest). The top layer of the ensemble model can be any model, even if the same technique (such as random forest) is used in the bottom layer. The most complex algorithms (such as random forest, gradient boosting, SVM, and others) do not always show better performance than the simpler ones (such as trees or logistic regression).</p>
<p>In this case, no additional examples and algorithms will be trained, as the previous results will be sufficient.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automatic machine learning</h1>
                </header>
            
            <article>
                
<p>Now that we have learned how to develop a powerful model to predict bank failures, we will test a final option to develop different models. Specifically, we will try out <strong>automatic machine learning</strong> (<strong>autoML</strong>), which is included in the <kbd>h2o</kbd> package. The process that we have carried out to build many models and find the best one without any prior knowledge is done automatically by the <kbd>autoML</kbd> function. This function trains different models by trying different grids of parameters. Moreover, stacked ensembles or models based on previously trained models are trained to find more accurate or predictive models.</p>
<p>In my opinion, using this function before launching any model is highly recommended to get an initial idea of a reference starting point. Using an automatic approach, we can assess the most reliable algorithms, the most important potential variables to be used, or a reference of the accuracy we could obtain.</p>
<p>To test this function, we will load a previous workspace:</p>
<pre>load("~/Data12.RData")</pre>
<p><kbd>Data12.RData</kbd> contains train and test samples before launching any model.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We need to load the <kbd>h2o</kbd> package as well. Moreover, all of the objects that were created in the <kbd>h2o</kbd> space will be removed:</p>
<pre>library(h2o)<br/>h2o.init()<br/>h2o.removeAll()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Standardizing variables</h1>
                </header>
            
            <article>
                
<p>In the previous models, we fixed a parameter that standardized the data. However, this option is not available in the <kbd>autoML</kbd> function. Thus, the variables will be standardized first. The columns will have zero mean and unit variance. We need to standardize the variables because otherwise the results will have dominating variables that seem to have a higher variance compared to other attributes as a consequence of their scale.</p>
<p>Standardization is done using the <kbd>caret</kbd> package. First, we choose the name of numeric columns to standardize:</p>
<pre>library(caret)<br/>features &lt;- setdiff(names(train), c("ID_RSSD","Default"))</pre>
<p>The variables are transformed with the <kbd>preProcess</kbd> function:</p>
<pre>pre_process &lt;- preProcess(x = train[, features], <br/>                method = c( "center", "scale"))</pre>
<p>The previous function stores the parameters that are needed to make the standardization on any dataset. With the <kbd>predict</kbd> function, we can actually apply this transformation. Both the train and test samples must be transformed:</p>
<pre># apply to both training &amp; test<br/>train &lt;- cbind(train[,"Default"],predict(pre_process, train[, features]))<br/>test &lt;- cbind(test[,"Default"],predict(pre_process, test[, features]))<br/> <br/>colnames(train)[1]&lt;-"Default"<br/>colnames(test)[1]&lt;-"Default"</pre>
<p>Now, we are ready to create the different models. The train and test samples are converted into <kbd>h2o</kbd> tables:</p>
<pre>train &lt;- as.h2o(train)<br/>test &lt;- as.h2o(test)</pre>
<p>We need the name of both the target and the predictors:</p>
<pre>y &lt;- "Default"<br/>x &lt;- setdiff(names(train), y)</pre>
<p>After all of these basic preprocessing steps, it is time to create a model. The <kbd>h2o.automl</kbd> f<span>unction</span><span> </span><span>implements the autoML method. The following parameters are needed:</span></p>
<ul>
<li><kbd>x</kbd>: The names of the predictors</li>
<li><kbd>y</kbd>: The target column name</li>
<li><kbd>training_frame</kbd>: The training dataset that is to be used for creating the model</li>
<li><kbd>leaderboard_frame</kbd>: The validation dataset that's used by <kbd>h2o</kbd> to ensure that the model doesn't overfit the data</li>
</ul>
<p>There are more parameters, but the preceding list contains the minimum requirements. It is also possible to exclude some algorithms, for example. In our case, we will fix the maximum number of models to be trained and the AUC as the stopping metric criteria.</p>
<p>Let's train some models:</p>
<pre>AML_models &lt;- h2o.automl(y = y, x = x,<br/>                   training_frame = train,<br/>                   max_models = 10,stopping_metric ="AUC",<br/>                   seed = 1234,sort_metric ="AUC")</pre>
<p>We can access the <kbd>Leaderboard</kbd> of the trained models as follows:</p>
<pre>Leaderboard &lt;- AML_models@leaderboard<br/>print(Leaderboard)<br/> ##                                                model_id       auc<br/> ## 1             GBM_grid_0_AutoML_20190105_000223_model_4 0.9945125<br/> ## 2 StackedEnsemble_BestOfFamily_0_AutoML_20190105_000223 0.9943324<br/> ## 3    StackedEnsemble_AllModels_0_AutoML_20190105_000223 0.9942727<br/> ## 4             GLM_grid_0_AutoML_20190105_000223_model_0 0.9941941<br/> ## 5             GBM_grid_0_AutoML_20190105_000223_model_1 0.9930208<br/> ## 6             GBM_grid_0_AutoML_20190105_000223_model_5 0.9926648<br/> ##      logloss mean_per_class_error       rmse         mse<br/> ## 1 0.03801166           0.04984862 0.09966934 0.009933978<br/> ## 2 0.03566530           0.03747844 0.09228175 0.008515921<br/> ## 3 0.03589846           0.03929486 0.09251204 0.008558478<br/> ## 4 0.03026294           0.05200978 0.08904775 0.007929502<br/> ## 5 0.03664414           0.06546054 0.09659713 0.009331005<br/> ## 6 0.13078645           0.08500441 0.18747430 0.035146615<br/> ## <br/> ## [12 rows x 6 columns]</pre>
<p>According to the <kbd>Leaderboard</kbd>, a gradient boosting model is the best if accuracy is to be considered. <span>Let's obtain the prediction of this boosting model:</span></p>
<pre>leader_model &lt;- AML_models@leader<span>pred_test &lt;- as.data.frame(h2o.predict(object = leader_model, newdata = test))</span></pre>
<p class="mce-root">It is possible to print the complete details of the model with the following code (the results are not printed in this book because of their length):</p>
<pre class="mce-root">print(leader_model)</pre>
<p>It is possible to analyze the importance of individual models in a stacked model, as well. Let's see the accuracy of the best model on the test sample:</p>
<pre class="mce-root">head(pred_test)<br/> ##   predict        p0          p1<br/> ## 1       0 0.9977300 0.002270014<br/> ## 2       0 0.9977240 0.002275971<br/> ## 3       0 0.9819248 0.018075249<br/> ## 4       0 0.9975793 0.002420683<br/> ## 5       0 0.9977238 0.002276235<br/> ## 6       0 0.9977240 0.002276009</pre>
<p>It is important to remember that the predict column is the predicted category according to the model, but we need to take into consideration 50% as a threshold in the predicted probability of bankruptcy.</p>
<p>Like in the previous algorithms, we should define the observed default rate in our sample:</p>
<pre>pred_test$predict&lt;-ifelse(pred_test$p1&gt;0.04696094,1,0)</pre>
<p>Now, we will add the observed class, and then the accuracy table will be calculated:</p>
<pre>pred_test&lt;-cbind(as.data.frame(test[,"Default"]),pred_test)<br/>table(pred_test$Default,pred_test$predict)<br/> ##        0    1<br/> ##   0 2810   86<br/> ##   1    6  137</pre>
<p>The automatic model obtains very good performance, but it's a little worse than the SVM model. Like the previous models in the <kbd>h2o</kbd> package, the model can be saved for future use:</p>
<pre>h2o.saveModel(leader_model, path = "AML_model")</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary </h1>
                </header>
            
            <article>
                
<p>In this chapter, we used different models and algorithms to try and optimize our model. All of the algorithms obtained good results. This would not have been the case in other problems. You can try using different algorithms in your problems and test the best combinations of parameters to solve your specific problem. A combination of different algorithms or ensembles might be a good option as well. </p>
<p>In the next chapter, we will continue by looking at other real problems—specifically, data visualization of economic imbalances in European countries. </p>


            </article>

            
        </section>
    </body></html>