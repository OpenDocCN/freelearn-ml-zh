- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Visualizing Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化卷积神经网络
- en: Up to this point, we have only dealt with tabular data and, briefly, text data,
    in *Chapter 5*, *Local Model-Agnostic Interpretation Methods*. This chapter will
    exclusively explore interpretation methods that work with images and, in particular,
    with the **Convolutional Neural Network** (**CNN**) models that train image classifiers.
    Typically, deep learning models are regarded as the epitome of black box models.
    However, one of the benefits of a CNN is how easily it lends itself to visualization,
    so we can not only visualize outcomes but also every step of the learning process
    with **activations**. The possibility of interpreting these steps is rare among
    so-called black box models. Once we have grasped how CNNs learn, we will study
    how to use state-of-the-art gradient-based attribution methods, such as *saliency
    maps* and *Grad-CAM* to debug class attribution. Lastly, we will extend our attribution
    debugging know-how with perturbation-based attribution methods such as *occlusion
    sensitivity* and `KernelSHAP`.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只处理了表格数据，以及在*第五章*，*局部模型无关解释方法*中简要提到的文本数据。本章将专门探讨适用于图像的解释方法，特别是训练图像分类器的**卷积神经网络**（**CNN**）模型。通常，深度学习模型被视为黑盒模型的典范。然而，CNN的一个优点是它很容易进行可视化，因此我们不仅可以可视化结果，还可以通过**激活**来可视化学习过程中的每一步。在所谓的黑盒模型中，解释这些步骤的可能性是罕见的。一旦我们掌握了CNN的学习方式，我们将研究如何使用最先进的基于梯度的属性方法，如*显著性图*和*Grad-CAM*来调试类别属性。最后，我们将通过基于扰动的属性方法，如*遮挡敏感性*和`KernelSHAP`来扩展我们的属性调试知识。
- en: 'These are the main topics we are going to cover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们将要讨论的主要主题：
- en: Assessing the CNN classifier with traditional interpretation methods
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用传统解释方法评估CNN分类器
- en: Visualizing the learning process with an activation-based method
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于激活的方法可视化学习过程
- en: Evaluating misclassifications with gradient-based attribution methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于梯度的属性方法评估误分类
- en: Understanding classifications with perturbation-based attribution methods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于扰动的属性方法理解分类
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter’s example uses the `mldatasets`, `pandas`, `numpy`, `sklearn`,
    `tqdm`, `torch`, `torchvision`, `pytorch-lightning`, `efficientnet-pytorch`, `torchinfo`,
    `matplotlib`, `seaborn`, and `captum` libraries. Instructions on how to install
    all of these libraries are in the *Preface*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例使用了`mldatasets`、`pandas`、`numpy`、`sklearn`、`tqdm`、`torch`、`torchvision`、`pytorch-lightning`、`efficientnet-pytorch`、`torchinfo`、`matplotlib`、`seaborn`和`captum`库。如何安装所有这些库的说明在*前言*中。
- en: 'The code for this chapter is located here: [https://packt.link/qzUvD](https://packt.link/qzUvD).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于此处：[https://packt.link/qzUvD](https://packt.link/qzUvD)。
- en: The mission
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: Over two billion tons of waste is produced annually globally, and it’s expected
    to grow to over 3.5 billion tons by 2050\. The alarming rise in global waste production
    and the need for effective waste management systems have become increasingly critical
    in recent years. Over half of all household trash in high-income countries is
    recyclable, with 20% in lower-income countries and rising. Currently, most waste
    ends up in landfills or incinerated, contributing to environmental pollution and
    climate change. This is avoidable, considering that, globally, a significant portion
    of all recyclable materials is not recycled.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 全球每年产生超过20亿吨垃圾，预计到2050年将增长到超过35亿吨。近年来，全球垃圾产量急剧上升和有效废物管理系统需求日益迫切。在高收入国家，超过一半的家庭垃圾是可回收的，在低收入国家为20%，并且还在上升。目前，大多数垃圾最终都堆放在垃圾填埋场或焚烧，导致环境污染和气候变化。考虑到全球范围内，很大一部分可回收材料没有得到回收，这是可以避免的。
- en: 'Assuming recyclable waste is collected, it can still be hard and costly to
    sort it. Previously, waste classification technologies included:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设可回收垃圾被收集，但仍可能很难且成本高昂地进行分类。以前，废物分类技术包括：
- en: Separating materials by size with rotating cylindrical screens with holes (“trommel
    screens”)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过旋转圆柱形筛网（“摇筛”）按尺寸分离材料
- en: Separating ferrous and non-ferrous metals with magnetic forces and magnetic
    fields (“eddy current separators”)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过磁力和磁场分离铁和非铁金属（“涡流分离器”）
- en: Separating by weight with air
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过空气按重量分离
- en: Separating by density with water (“sink-float separation”)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过水按密度分离（“沉浮分离”）
- en: Manual sorting performed by humans
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由人工执行的手动分类
- en: Implementing all of these techniques effectively can be challenging, even for
    a large, wealthy, urban municipality. To tackle this challenge, **smart recycling
    systems** have emerged, leveraging computer vision and AI to classify waste efficiently
    and accurately.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于大型、富裕的城市市政府，有效地实施所有这些技术也可能具有挑战性。为了应对这一挑战，**智能回收系统**应运而生，利用计算机视觉和人工智能高效、准确地分类废物。
- en: The development of smart recycling systems can be traced back to the early 2010s
    when researchers and innovators started exploring the potential of computer vision
    and AI to improve waste management processes. They first developed basic image
    recognition algorithms, utilizing features such as color, shape, and texture to
    identify waste materials. These systems were primarily used in research settings
    with limited commercial applications. As machine learning and AI became more advanced,
    smart recycling systems underwent significant improvements. CNNs and other deep
    learning techniques enabled these systems to learn from vast amounts of data and
    improve their waste classification accuracy. Additionally, the integration of
    AI-driven robotics allowed for automated sorting and handling of waste materials,
    increasing efficiency in recycling plants.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 智能回收系统的发展可以追溯到2010年代初，当时研究人员和革新者开始探索计算机视觉和人工智能改善废物管理流程的潜力。他们首先开发了基本的图像识别算法，利用颜色、形状和纹理等特征来识别废物材料。这些系统主要用于研究环境，商业应用有限。随着机器学习和人工智能的进步，智能回收系统经历了显著的改进。卷积神经网络（CNN）和其他深度学习技术使这些系统能够从大量数据中学习并提高其废物分类的准确性。此外，人工智能驱动的机器人集成使得废物材料的自动化分拣和处理成为可能，从而提高了回收工厂的效率。
- en: Costs are significantly lower than a decade ago for cameras, robots, and even
    chips that run deep learning models in low-latency, high-volume scenarios, making
    state-of-the-art smart recycling systems accessible to even smaller and poorer
    municipal waste management departments. One of these municipalities in Brazil
    is looking to revamp their 20-year-old recycling plant made up of a patchwork
    of machines with a collective sorting accuracy of only 70%. Human sorting can
    only partially compensate for the difference, leading to inevitable pollution
    and contamination issues. The Brazilian municipality want to replace the current
    system with a single conveyor belt that sorts waste efficiently from 12 different
    categories into bins with a series of robots.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 摄像头、机器人和用于低延迟、高容量场景运行深度学习模型的芯片等成本与十年前相比显著降低，这使得最先进的智能回收系统对甚至更小、更贫穷的城市废物管理部门也变得可负担。巴西的一个城市正在考虑翻新他们20年前建成的一个由各种机器拼凑而成的回收厂，这些机器的集体分拣准确率仅为70%。人工分拣只能部分弥补这一差距，导致不可避免的污染和污染问题。该巴西市政府希望用一条单条传送带替换当前系统，这条传送带由一系列机器人高效地将12种不同类别的废物分拣到垃圾桶中。
- en: They purchased the conveyor belt, industrial robots, and cameras. Then, they
    paid an AI consultancy company to develop a model to classify the recyclables.
    Still, they wanted models of different sizes because they weren’t sure how quickly
    these would run on the hardware they had.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 他们购买了传送带、工业机器人和摄像头。然后，他们支付了一家人工智能咨询公司开发一个用于分类可回收物的模型。然而，他们想要不同大小的模型，因为他们不确定这些模型在他们的硬件上运行的速度有多快。
- en: 'As requested, the consultancy returned with models of various sizes between
    4 and 64 million parameters. The largest model (b7) is over six times slower than
    the smallest one (b0). Still, the largest model has a significantly higher validation
    F1 score at 96% (F1 val), as opposed to approximately 90% for the smallest one:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如请求，咨询公司带回了4到6400万参数之间各种大小的模型。最大的模型（b7）比最小的模型（b0）慢六倍以上。然而，最大的模型在验证F1分数上显著更高，达到96%（F1
    val），而最小的模型大约为90%：
- en: '![Chart, line chart  Description automatically generated](img/B18406_07_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成描述](img/B18406_07_01.png)'
- en: 'Figure 7.1: F1 scores for models delivered by the AI consultancy company'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：由人工智能咨询公司提供的模型F1分数
- en: The municipal leadership was delighted with the results but also surprised because
    the consultants asked for no domain knowledge or data to train the models, which
    made them very skeptical. They asked their recycling plant workers to test the
    models with a batch of recyclables. They got a 25% misclassification rate with
    that one batch.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 市政领导对结果感到非常满意，但也感到惊讶，因为顾问们要求不要提供任何领域知识或用于训练模型的数据，这使得他们非常怀疑。他们要求回收厂的工人用一批可回收物测试这些模型。他们用这一批次的模型得到了25%的错误分类率。
- en: To seek a second opinion and an honest evaluation of the model, the municipality
    has approached another AI consultancy firm – yours!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The first order of business was to assemble a test dataset that was more realistic
    of the edge cases that the recycling plant workers found among the misclassifications.
    Your colleague obtained F1 scores with the test dataset between 62% and 66% (F1
    test). Next, they have asked you to understand what’s causing those misclassifications.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: The approach
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'No single interpretation method is perfect, and even the best scenario can
    only tell you one part of the story. Therefore, you have decided to, first, assess
    the model’s predictive performance using traditional interpretation methods, including
    the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: ROC curves and ROC-AUC
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confusion matrices and some metrics derived from them, such as accuracy, precision,
    recall, and F1
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, you’ll examine the model using an activation-based method:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate activation
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is followed by evaluating decisions with three gradient-based methods:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Saliency maps
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grad-CAM
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrated gradients
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And a backpropagation-based method:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: DeepLIFT
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is followed by three perturbation-based methods:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Occlusion sensitivity
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature ablation
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shapley value sampling
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I hope that you understand why the model is not performing as it should and
    how to fix it by the end of this process. You can also leverage the many plots
    and visualizations you will produce to communicate this story to the municipality’s
    executives.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Preparations
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will find most of the code for this example here: [https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/07/GarbageClassifier.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/07/GarbageClassifier.ipynb)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Loading the libraries
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run this example, you need to install the following libraries:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '`torchvision` to load the dataset'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mldatasets`, `pandas`, `numpy`, and `sklearn` (scikit-learn) to manipulate
    the dataset'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch`, `pytorch-lightning`, `efficientnet-pytorch`, and `torchinfo` to predict
    with the models and show info about the models'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib`, `seaborn`, `cv2`, `tqdm`, and `captum` to make and visualize
    the interpretations'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You should load all of them first:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we will load and prepare the data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and preparing the data
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data used to train the model is publicly available at Kaggle ([https://www.kaggle.com/datasets/mostafaabla/garbage-classification](https://www.kaggle.com/datasets/mostafaabla/garbage-classification)).
    It’s called “Garbage Classification” and is a compilation of several different
    online sources, including web scraping. It has already been split into training
    and test datasets and also comes with an additional smaller test dataset taken
    from Wikimedia Commons that your colleague used to test the models. These test
    images come in a slightly higher resolution too.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'We download the data from a ZIP file like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It will also extract the ZIP file into four folders corresponding to the three
    datasets and the larger resolution test dataset. Please note that `garbage_dataset_sample`
    has only a fraction of the training and validation datasets. If you want to download
    the full datasets, then use `dataset_file = "garbage_dataset"`. It won’t impact
    the size of the test dataset either way. Next, we can initialize the transformation
    and loading of the datasets like this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它还会将ZIP文件提取到四个文件夹中，分别对应三个数据集和更高分辨率的测试数据集。请注意，`garbage_dataset_sample`只包含训练和验证数据集的一小部分。如果你想下载完整的数据集，请使用`dataset_file
    = "garbage_dataset"`。无论哪种方式，都不会影响测试数据集的大小。接下来，我们可以这样初始化数据集的转换和加载：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'What the above code does is compose a series of standard transforms such as
    normalization and converting images to tensors. Then, it instantiates PyTorch
    datasets corresponding to each folder – that is, one for the training, validation,
    and test datasets, as well as the larger resolution test dataset (`test_400_data`).
    These datasets also include transforms. That way, each time an image is loaded
    from one of the datasets, it is automatically transformed. We can verify that
    the shapes of the datasets match our expectations with the following code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码所做的就是组合一系列标准转换，如归一化和将图像转换为张量。然后，它实例化与每个文件夹对应的PyTorch数据集——即一个用于训练、验证和测试数据集，以及更高分辨率的测试数据集（`test_400_data`）。这些数据集也包括转换。这样，每次从数据集中加载图像时，它都会自动进行转换。我们可以使用以下代码来验证数据集的形状是否符合我们的预期：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code outputs the number of images in each dataset and the dimensions
    of the images in the datasets. You can tell that there are over 3,700 training
    images, 900 validation images, and 120 test images of 3 x 224 x 224 dimensions.
    The first number corresponds to the channels (red, green, and blue) and the following
    two to the width and height in pixels, which is what the model uses for inference.
    The Test 400 dataset is the same as the Test dataset, the except images have a
    larger height and width. We won’t need the Test 400 dataset for inference, so
    it’s Okay that it doesn’t meet the model’s dimension requirements:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出了每个数据集中的图像数量和图像的维度。你可以看出，有超过3,700张训练图像，900张验证图像和120张测试图像，它们的维度为3 x 224
    x 224。第一个数字对应于通道（红色、绿色和蓝色），接下来的两个数字对应于像素的宽度和高度，这是模型用于推理的。Test 400数据集与Test数据集相同，只是图像的高度和宽度更大。我们不需要Test
    400数据集进行推理，所以它不符合模型的维度要求也是可以的：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Data preparation
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'If you print`(test_data[0])`, you’ll notice that it will first output a tensor
    with the image and then a single integer, which we call a scalar. This integer
    is a number between 0 and 11, which corresponds to the labels used. For quick
    reference, these are the 12 labels:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打印`(test_data[0])`，你会注意到它首先会输出一个包含图像的张量，然后是一个单独的整数，我们称之为标量。这个整数是一个介于0到11之间的数字，对应于使用的标签。为了快速参考，以下是12个标签：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Interpreting often involves taking single samples and extracting them from
    the dataset to later perform inference with the model. To that end, it’s important
    to get familiar with extracting any image from the dataset, say the very first
    sample from the test dataset:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 解释通常涉及从数据集中提取单个样本，以便稍后使用模型进行推理。为此，熟悉从数据集中提取任何图像，比如测试数据集的第一个样本是很重要的：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![A picture containing diagram  Description automatically generated](img/B18406_07_02.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![包含图表的图片 自动生成描述](img/B18406_07_02.png)'
- en: 'Figure 7.2: A test sample for a recyclable alkaline battery'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：一个可回收碱性电池的测试样本
- en: 'Another preprocessing step we will need to perform is the **One-Hot Encoding**
    (**OHE**) of the `y` labels because we will need the OHE form to evaluate the
    model’s predictive performance. Once we initialize the `OneHotEncoder`, we will
    need to `fit` it to the test labels (`y_test`) in array format. But first, we
    will need to put the test labels into a list (`y_test`). We can do the same with
    the validation labels because these will also be useful for easy evaluation:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要执行的一个预处理步骤是对`y`标签进行**独热编码**（**OHE**），因为我们需要OHE形式来评估模型的预测性能。一旦我们初始化了`OneHotEncoder`，我们需要将其`fit`到测试标签（`y_test`）的数组格式中。但首先，我们需要将测试标签放入一个列表（`y_test`）。我们也可以用同样的方法处理验证标签，因为这些标签也便于评估：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Also, for the sake of reproducibility, always initialize your random seeds
    like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了确保可重复性，始终这样初始化你的随机种子：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It is acknowledged that determinism is very difficult with deep learning and
    is often session-, platform-, and architecture-dependent. If you are using an
    **NVIDIA GPU**, you can attempt to use PyTorch to avoid nondeterministic algorithms
    with the command `torch.use_deterministic_algorithms(True)`. It’s not a guarantee,
    but it will throw an error when the operation that you are attempting can’t be
    accomplished deterministically. If it succeeds, it will be much slower. It’s only
    worth it if you need to make model outcomes identical – for instance, for scientific
    research or regulatory compliance. For further details about reproducibility and
    PyTorch, look here: [https://pytorch.org/docs/stable/notes/randomness.html](https://pytorch.org/docs/stable/notes/randomness.html).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中确定性的实现非常困难，并且通常依赖于会话、平台和架构。如果你使用**NVIDIA GPU**，你可以尝试使用PyTorch通过命令`torch.use_deterministic_algorithms(True)`来避免非确定性算法。这并不保证，但如果尝试的操作无法以确定性完成，它将引发错误。如果成功，它将运行得慢得多。只有在你需要使模型结果一致时才值得这样做——例如，用于科学研究或合规性。有关可重复性和PyTorch的更多详细信息，请查看此处：[https://pytorch.org/docs/stable/notes/randomness.html](https://pytorch.org/docs/stable/notes/randomness.html)。
- en: Inspect data
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查数据
- en: 'Now, let’s take a peek at what images are in our datasets. We know that the
    training and validation datasets are very similar, so we will start with the validation
    dataset. We can iterate every class in `labels_l` and randomly select a single
    one from the validation dataset with `np.random.choice`. We place each image on
    a 4 × 3 grid with the class label above it:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的数据集中有哪些图像。我们知道训练和验证数据集非常相似，所以我们从验证数据集开始。我们可以迭代`labels_l`中的每个类别，并使用`np.random.choice`从验证数据集中随机选择一个。我们将每个图像放置在一个4×3的网格中，类别标签位于其上方：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code generates *Figure 7.3*. You can tell that there is significant
    pixelation around the edges of the items; some items appear much darker than others,
    and some of the pictures are from odd angles:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码生成了*图7.3*。你可以看出，物品的边缘存在明显的像素化；有些物品比其他物品暗得多，而且有些图片是从奇怪的角度拍摄的：
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B18406_07_03.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 描述自动生成，置信度中等](img/B18406_07_03.png)'
- en: 'Figure 7.3: A random sample of the validation dataset'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：验证数据集的随机样本
- en: 'Let’s now do the same for the test dataset to compare it to the validation/training
    datasets. We can use the same code as before, except we replace `y_val` with `y_test`,
    and `val_data` with `test_data.` The resulting code generates *Figure 7.4*. You
    can tell that the test set has less pixelated and more consistently lit items,
    mostly from the top- and side-facing angles:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对测试数据集做同样的处理，以便与验证/训练数据集进行比较。我们可以使用之前的相同代码，只需将`y_val`替换为`y_test`，将`val_data`替换为`test_data`。生成的代码生成了*图7.4*。你可以看出，测试集的像素化较少，物品的照明更一致，主要是从正面和侧面角度拍摄的：
- en: '![A picture containing text, different, various  Description automatically
    generated](img/B18406_07_04.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本的图片，不同，各种 描述自动生成](img/B18406_07_04.png)'
- en: 'Figure 7.4: A random sample of the test dataset'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：测试数据集的随机样本
- en: We won’t need to train a CNN in this chapter. Thankfully, it has been provided
    to us by the client.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不需要训练CNN。幸运的是，客户已经为我们提供了它。
- en: The CNN models
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CNN模型
- en: The models trained by the other consultancy company are fine-tuned EfficientNet
    models. In other words, the AI consultancy company took a previously trained model
    with the EfficientNet architecture and trained it further with the garbage classification
    dataset. This technique is called **transfer learning** because it allows a model
    to utilize previously learned knowledge from a large dataset (in this case, a
    million images from the ImageNet database) and apply it to new tasks with smaller
    datasets. The advantage is it significantly reduces training time and computational
    resources while maintaining high performance because it has already learned to
    extract useful features from images, which can be a valuable starting point for
    a new task and only needs to adapt to the specific task at hand.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其他咨询公司训练的模型是微调后的EfficientNet模型。换句话说，AI咨询公司使用EfficientNet架构的先前训练模型，并使用垃圾分类数据集进一步训练它。这种技术被称为**迁移学习**，因为它允许模型利用从大型数据集（在这种情况下，来自ImageNet数据库的百万张图片）中学习到的先前知识，并将其应用于具有较小数据集的新任务。其优势是显著减少了训练时间和计算资源，同时保持高性能，因为它已经学会了从图像中提取有用的特征，这可以成为新任务的宝贵起点，并且只需要适应手头的特定任务。
- en: It makes sense that they chose EfficientNet. After all, EfficientNet is a family
    of CNNs introduced by Google AI researchers in 2019\. The key innovation of EfficientNet
    is its compound scaling method, which enables the model to achieve higher accuracy
    and efficiency than other CNNs. In addition, it is based on the observation that
    different dimensions of the model, such as width, depth, and resolution, contribute
    to the overall performance in a balanced way. The EfficientNet architecture is
    built upon a baseline model called EfficientNet-B0\. A compound scaling method
    is employed to create larger and more powerful versions of the baseline model,
    which simultaneously scales up the width, depth, and resolution of the network.
    This results in a series of models, EfficientNet-B1 to EfficientNet-B7, with increasing
    capacity and performance. The largest model, EfficientNet-B7, has achieved state-of-the-art
    performance on several benchmarks, such as ImageNet.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 EfficientNet 是有道理的。毕竟，EfficientNet 是由 Google AI 研究人员在 2019 年引入的一组 CNN。EfficientNet
    的关键创新是其复合缩放方法，这使得模型能够比其他 CNN 实现更高的准确性和效率。此外，它基于这样的观察：模型的各个维度，如宽度、深度和分辨率，以平衡的方式对整体性能做出贡献。EfficientNet
    架构建立在称为 EfficientNet-B0 的基线模型之上。采用复合缩放方法创建基线模型更大、更强大的版本，同时提高网络的宽度、深度和分辨率。这产生了一系列模型，从
    EfficientNet-B1 到 EfficientNet-B7，容量和性能逐渐提高。最大的模型 EfficientNet-B7 在多个基准测试中实现了最先进的性能，例如
    ImageNet。
- en: Load the CNN model
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载 CNN 模型
- en: 'Before we can load the model, we must define the class for EfficientLite –
    a class that inherits from PyTorch Lightning’s `pl.LightningModule`. This class
    is designed to create a custom model based on the EfficientNet architecture, train
    it, and perform inference. We only need it for the latter, which is why we have
    also adapted it to include a `predict()` function – much like scikit-learn models
    do for the convenience of being able to use similar evaluation functions to would
    with these models:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够加载模型之前，我们必须定义 EfficientLite 类的类——一个继承自 PyTorch Lightning 的 `pl.LightningModule`
    的类。这个类旨在创建基于 EfficientNet 架构的定制模型，对其进行训练并执行推理。我们只需要它来执行后者，这就是为什么我们还将其修改为包含一个 `predict()`
    函数——类似于 scikit-learn 模型，以便能够使用类似的评估函数：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You will notice that the class has three functions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到这个类有三个函数：
- en: '`__init__`: This is the constructor for the `EfficientLite` class. It initializes
    the model by loading a pretrained EfficientNet model using the `efficientnet_pytorch.EfficientNet.from_pretrained()`
    method. It then replaces the last fully connected layer (`_fc`) with a new `torch.nn.Linear`
    layer that has the same number of input features but a different number of output
    features equal to the number of classes (`num_class`).'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__init__`: 这是 `EfficientLite` 类的构造函数。它通过使用 `efficientnet_pytorch.EfficientNet.from_pretrained()`
    方法加载预训练的 EfficientNet 模型来初始化模型。然后，它将最后一个全连接层 (`_fc`) 替换为一个新创建的 `torch.nn.Linear`
    层，该层具有相同数量的输入特征，但输出特征的数量不同，等于类别的数量 (`num_class`)。'
- en: '`forward`: This method defines the forward pass of the model. It takes an input
    tensor `x` and passes it through the model, returning the output.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward`: 此方法定义了模型的正向传播。它接收一个输入张量 `x` 并将其通过模型传递，返回输出。'
- en: '`predict`: This method takes a dataset and performs inference using the trained
    model. It first sets the model to evaluation mode (`self.model.eval()`). The input
    dataset is converted into a DataLoader object with a batch size of 32\. The method
    iterates over the DataLoader, processing each batch of data and computing probabilities
    using the softmax function. The `clear_gpu_cache()` function is called after each
    iteration to release unused GPU memory. Finally, the method returns the computed
    probabilities as a `numpy` array.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict`: 此方法接收一个数据集并使用训练好的模型进行推理。它首先将模型设置为评估模式 (`self.model.eval()`)。输入数据集被转换为具有
    32 个批次的 DataLoader 对象。该方法遍历 DataLoader，处理每个数据批次，并使用 softmax 函数计算概率。在每个迭代之后调用 `clear_gpu_cache()`
    函数以释放未使用的 GPU 内存。最后，该方法返回计算出的概率作为 `numpy` 数组。'
- en: 'If you are using a CUDA-enabled GPU, there’s a utility function called `clear_gpu_cache()`,
    which is run every time there’s a GPU-intensive operation. Depending on how powerful
    your GPU is, you may need to run it more often. Feel free to use another convenience
    function, `print_gpu_mem_used()`, to check how much GPU memory is utilized at
    any given moment or to print the entire summary with `print(torch.cuda.memory_summary())`.
    The next code downloads the pre-trained EfficientNet model, loads the model weights
    to EfficientLite, and prepares the model for inference. Lastly, it prints a summary:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用支持CUDA的GPU，有一个名为`clear_gpu_cache()`的实用函数，每次进行GPU密集型操作时都会运行。根据你的GPU性能如何，你可能需要更频繁地运行它。你可以自由地使用另一个便利函数`print_gpu_mem_used()`来检查在任何给定时刻GPU内存的使用情况，或者使用`print(torch.cuda.memory_summary())`来打印整个摘要。接下来的代码下载预训练的EfficientNet模型，将模型权重加载到EfficientLite中，并准备模型进行推理。最后，它打印了一个摘要：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The code is pretty straightforward but what’s important to note is that we
    are choosing the b4 model for this chapter, which is in between b0 and b7 in terms
    of size, speed, and accuracy. You can change the last digit according to your
    hardware’s abilities, but it might change some of the outcomes of this chapter’s
    code. The preceding snippet outputs the following summary:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 代码相当直接，但重要的是要注意，我们在这个章节选择了b4模型，它在大小、速度和准确性方面介于b0和b7之间。你可以根据你的硬件能力更改最后一位数字，但这可能会改变本章代码的一些结果。前面的代码片段输出了以下摘要：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: It has pretty much everything we need to know about the model. It has two custom
    convolutional layers (`Conv2dStaticSamePadding`), each followed by a batch normalization
    layer (`BatchNorm2d`) and 32 `MBConvBlock` modules.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它几乎包含了我们需要的关于模型的所有信息。它有两个自定义卷积层（`Conv2dStaticSamePadding`），每个卷积层后面跟着一个批归一化层（`BatchNorm2d`）和32个`MBConvBlock`模块。
- en: The network also has a memory-efficient implementation of the Swish activation
    function (`MemoryEfficientSwish`), which, like all activation functions, introduces
    non-linearity into the model. It’s smooth and non-monotonic, which helps it converge
    more quickly while learning more complex and nuanced patterns. It also has a global
    average pooling operation (`AdaptiveAvgPool2d`), which reduces the spatial dimensions
    of the feature maps. It then has a first `Dropout` layer for regularization, followed
    by a fully connected layer (`Linear`) that takes it from 1792 nodes to 12\. Dropout
    prevents overfitting by making a fraction of the neurons inactive in each update
    cycle. If you want to see more details of how the output shape of each layer gets
    reduced between one layer and another, enter the `input_size` into the summary
    – like `summary(garbage_mdl, input_size=(64, 3, 224, 224))` – because the network
    was designed with a batch size of 64 in mind. Don’t worry if none of these terms
    sound familiar to you. We will revisit them later.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 网络还有一个Swish激活函数的内存高效实现（`MemoryEfficientSwish`），就像所有激活函数一样，它将非线性引入模型。它是平滑且非单调的，有助于它更快地收敛，同时学习更复杂和细微的模式。它还有一个全局平均池化操作（`AdaptiveAvgPool2d`），它减少了特征图的空间维度。然后有一个用于正则化的第一个`Dropout`层，后面跟着一个将节点数从1792减少到12的完全连接层（`Linear`）。Dropout通过在每个更新周期中使一部分神经元不活跃来防止过拟合。如果你想知道每个层之间的输出形状是如何减少的，可以将`input_size`输入到摘要中——例如`summary(garbage_mdl,
    input_size=(64, 3, 224, 224))`——因为网络是针对64个批次的尺寸设计的。如果你对这些术语不熟悉，不要担心。我们稍后会重新讨论它们。
- en: Assessing the CNN classifier with traditional interpretation methods
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用传统解释方法评估CNN分类器
- en: 'We will first evaluate the model using the validation dataset with the `evaluate_multiclass_mdl`
    function. The arguments include the model (`garbage_mdl`), our validation data
    (`val`_`data`), as well as the class names (`labels_l`) and the encoder (`ohe`).
    Lastly, we won’t plot the ROC curves (`plot_roc=False`). This function returns
    the predicted labels and probabilities, which we can store in variables for later
    use:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用`evaluate_multiclass_mdl`函数和验证数据集来评估模型。参数包括模型（`garbage_mdl`）、我们的验证数据（`val_data`）、类别名称（`labels_l`）以及编码器（`ohe`）。最后，我们不会绘制ROC曲线（`plot_roc=False`）。此函数返回预测标签和概率，我们可以将它们存储在变量中以供以后使用：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code generates both *Figure 7.5* with a confusion matrix and
    *Figure 7.6* with performance metrics for each class:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了带有混淆矩阵的*图7.5*和每个类别的性能指标的*图7.6*：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18406_07_05.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面、文本、应用程序、电子邮件  自动生成的描述](img/B18406_07_05.png)'
- en: 'Figure 7.5: The confusion matrix for the validation dataset'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：验证数据集的混淆矩阵
- en: 'Even though the confusion matrix in *Figure 7.5* seems to suggest a perfect
    classification, once you see the precision and recall breakdown in *Figure 7.6*,
    you can tell that the model had issues with metal, plastic, and white glass:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管*图7.5*中的混淆矩阵似乎表明分类完美，但一旦你看到*图7.6*中的精确率和召回率分解，你就可以知道模型在金属、塑料和白色玻璃方面存在问题：
- en: '![A picture containing text, receipt  Description automatically generated](img/B18406_07_06.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本的图片，收据  自动生成的描述](img/B18406_07_06.png)'
- en: 'Figure 7.6: The classification report for the validation dataset'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：验证数据集的分类报告
- en: You can expect a model to always reach `100%` training accuracy if you train
    it for enough epochs using optimal hyperparameters. A near-perfect validation
    accuracy is harder to achieve, depending on how different these two are. We know
    that the validation dataset is simply a sample of images from the same collection,
    so it’s not particularly surprising that 94.7% was achieved.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用最优的超参数对模型进行足够的轮次训练，你可以期望模型总是达到`100%`的训练准确率。接近完美的验证准确率更难实现，这取决于这两个值之间的差异。我们知道验证数据集只是来自同一集合的图像样本，所以达到94.7%并不特别令人惊讶。
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Chart, line chart  Description automatically generated](img/B18406_07_07.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B18406_07_07.png)'
- en: 'Figure 7.7: The ROC curve for the test dataset'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：测试数据集的ROC曲线
- en: The test ROC plot (*Figure 7.7*) shows the macro-average and micro-average ROC
    curves. The difference in both of these is in how they are calculated. Macro metrics
    are computed for each class independently and then averaged, treating each differently,
    whereas micro-averages factor in the contribution or representation of each class;
    generally, micro-averages are more reliable.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 测试ROC图(*图7.7*)显示了宏平均和微平均的ROC曲线。这两者的区别在于它们的计算方式。宏度量是独立地对每个类别进行计算然后平均，对待每个类别不同，而微平均则考虑了每个类别的贡献或代表性；一般来说，微平均更可靠。
- en: '![Chart, scatter chart  Description automatically generated](img/B18406_07_08.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成的描述](img/B18406_07_08.png)'
- en: 'Figure 7.8: The confusion matrix for the test dataset'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：测试数据集的混淆矩阵
- en: If we take a look at the confusion matrix in *Figure 7.8*, we can tell that
    only biological, green glass, and shoes are getting 10-out-of-10 classifications.
    However, a lot of items are being misclassified as biologicals and shoes. On the
    other hand, many items are more often than not misclassified, such as metal, paper,
    and plastic. Many of them are similar in shape or color, so you could understand
    how that would happen, but how does a piece of metal get confused with white glass,
    or paper with a battery?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看*图7.8*中的混淆矩阵，我们可以看出只有生物、绿色玻璃和鞋子得到了10/10的分类。然而，很多物品被错误地分类为生物和鞋子。另一方面，很多物品经常被错误分类，比如金属、纸张和塑料。许多物品在形状或颜色上相似，所以你可以理解为什么会这样，但金属怎么会和白色玻璃混淆，或者纸张会和电池混淆呢？
- en: '![Table  Description automatically generated](img/B18406_07_09.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![表格  自动生成的描述](img/B18406_07_09.png)'
- en: 'Figure 7.9: The predictive performance metrics for the test dataset'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9：测试数据集的预测性能指标
- en: 'When classification models are discussed in a business setting, stakeholders
    are often only interested in one number: accuracy. It’s easy to let this drive
    the discussion, but there’s much more nuance to it. For instance, the disappointing
    test accuracy (68.3%) could mean many things. It could mean that six classes are
    getting perfect classification, and all others are not, or that 12 classes are
    getting only half misclassified. There are many possibilities of what could be
    going on.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业环境中讨论分类模型时，利益相关者通常只对一个数字感兴趣：准确率。很容易让这个数字驱动讨论，但其中有很多细微差别。例如，令人失望的测试准确率（68.3%）可能意味着很多事情。这可能意味着六个类别得到了完美的分类，而其他所有类别都没有，或者12个类别只有一半被错误分类。可能发生的事情有很多。
- en: In any case, when dealing with a multiclass classification problem, even an
    accuracy below 50% might not be as bad as it seems. Consider that the **no information
    rate** represents the accuracy that can be achieved by a naive model that always
    predicts the most frequent class in the dataset. It serves as a benchmark to ensure
    that the developed model is providing insights beyond this simplistic approach.
    And with 12 evenly split, the **no information rate** is likely to be around 8.33%
    (100%/12 classes), so 68% is still orders of magnitude higher than that. In fact,
    there is less of a leap to 100%! To a machine learning practitioner, this means
    that if we judge solely based on test accuracy results, the model is still learning
    something of value that can be improved upon.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: In any case, the predictive performance metrics in *Figure 7.9* for the test
    dataset are consistent with what we saw in the confusion matrix. Biological gets
    high recall but low precision and metal, paper, plastic, and trash are low for
    both.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Determining what misclassifications to focus on
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have already noticed some exciting misclassifications we can focus on:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '**Metal false positives**: 16 out of the 120 samples in the test dataset were
    misclassified as metal. That’s 42% of all misclassifications! What is it about
    metal that renders it so easily confused with other garbage according to the model?'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plastic false negatives**: 70% of all true plastic samples were misclassified.
    Thus, plastics had the lowest recall of any material besides trash. It’s easy
    to tell why trash would be so difficult to classify because it’s exceedingly diverse
    but not plastic.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should also examine some true positives to contrast these misclassifications.
    Namely, batteries because they get many false positives as metals and plastic,
    and white glass because it gets false negatives as metals 30% of the time. Since
    there are so many metal false positives, we should narrow them down to just those
    that are for batteries.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize the tasks ahead, we can create a DataFrame (`preds_df`) with the
    true labels (`y_true`) in one column and predicted labels in another (`y_pred`).
    And to understand how certain the models are of these predictions, we can create
    another DataFrame with the probabilities (`probs_df`). We can generate column
    totals for these probabilities to sort the columns according to which category
    the model is most certain about across all samples. Then, we can concatenate our
    predictions DataFrame with the first 12 columns from our probabilities DataFrame:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s now output the DataFrame with color coding for the prediction instances
    we are interested in assessing. On one hand, we have the metal false positives
    and, on the other, the plastic false negatives. But we also have the true positives
    for battery and white glass. Lastly, we have bolded all probabilities over 50%
    and hidden all probabilities of 0% so that it’s easier to spot any predictions
    with high probabilities:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Table  Description automatically generated](img/B18406_07_10.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: Table with all 38 misclassifications in the test dataset, selected
    true positives, and their true and predicted labels, as well as their predicted
    probabilities'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily store the indexes for these instances in lists with the following
    code. That way, for future reference, we can iterate through these lists to assess
    individual predictions or subset arrays with them to perform interpretation tasks
    for the entire group. As you can tell, we have lists for all four groups:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now that we have all our data preprocessed, the model is fully loaded and lists
    the groups of predictions to debug. Now we can move forward. Let the interpretation
    begin!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the learning process with activation-based methods
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get into discussing activations, layers, filters, neurons, gradients,
    convolutions, kernels, and all the fantastic elements that make up a CNN, let’s
    first briefly revisit the mechanics of a CNN and one in particular.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The convolution layer is the essential building block of a CNN, which is a sequential
    neural network. It convolves the input with **learnable filters**, which are relatively
    small but are applied across the entire width, height, and depth at specific distances
    or **strides**. Each filter produces a two-dimensional **activation map** (also
    known as a **feature map**). It’s called an activation map because it denotes
    positions of activations in the images – in other words, where specific “features”
    are located. In this context, a feature is an abstract spatial representation
    that, downstream in the process, is reflected in the learned weights of fully
    connected (**linear**) layers. For instance, in the garbage CNN case, the first
    convolutional layer has 48 filters with a 3 × 3 kernel, a 2 × 2 stride, and static
    padding, which ensure that the output maps maintain the same size as the inputs.
    Filters are template matching because they end up activating areas of the activation
    map when certain patterns are found in the input image.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: But before we get to our fully connected layers, we have to reduce the dimensions
    of our filters until they have a workable size. For instance, if we flattened
    the output of our first convolution (48 × 112 × 112), we would have over 602,000
    features. I think we can all agree that that would be too much to feed into a
    fully connected layer. Even if we used enough neurons to handle this workload,
    we probably wouldn’t have captured enough spatial representations for the neural
    network to make sense of the images. For this reason, convolutional layers are
    often paired with pooling layers, which downsample the input – in other words,
    they reduce the dimensionality of the data. In this case, there’s an adaptive
    average pooling layer (`AdaptiveAvgPool2d`) that performs an average across all
    the channels as well as many pooling layers within the **Mobile Inverted Bottleneck
    Convolution Blocks** (`MBConvBlock`).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Incidentally, `MBConvBlock`, `Conv2dStaticSamePadding`, and `BatchNorm2d` are
    the building blocks of the EfficientNet architecture. These components work together
    to create a highly efficient and accurate convolutional neural network:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，`MBConvBlock`、`Conv2dStaticSamePadding`和`BatchNorm2d`是EfficientNet架构的构建块。这些组件共同工作，创建了一个高度高效且准确的卷积神经网络：
- en: '`MBConvBlock`: Mobile inverted bottleneck convolution blocks that form the
    core of the EfficientNet architecture. In traditional convolutional layers, filters
    are applied across all input channels simultaneously, resulting in a high number
    of computations, but `MBConvBlocks` divide the process into two steps: first,
    they apply depthwise convolutions that handle each input channel separately, and
    then use pointwise (1 x 1) convolutions to combine the information from different
    channels. For this reason, inside the `MBConvBlock` modules for B0, there are
    three convolutional layers: a depthwise convolution, a pointwise (1 x 1) convolution
    (called project convolution), and another pointwise (1 x 1) convolution (called
    expand convolution) in some blocks. However, the first block only contains two
    convolutional layers (depthwise and project convolutions) because it doesn’t have
    an expand convolution. For B4, the architecture is similar except more convolutions
    are stacked in each block and there are twice as many `MBConvBlocks`. Naturally,
    B7 has many more blocks and convolutional layers. For B4, there are a total of
    158 convolutional operations between the 32 `MBConvBlocks`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MBConvBlock`: 形成EfficientNet架构核心的移动倒置瓶颈卷积块。在传统的卷积层中，过滤器同时应用于所有输入通道，导致计算量很大，但`MBConvBlocks`将这个过程分为两个步骤：首先，它们应用深度卷积，分别处理每个输入通道，然后使用点卷积（1
    x 1）来结合来自不同通道的信息。因此，在B0的`MBConvBlock`模块中，有三个卷积层：一个深度卷积，一个点卷积（称为项目卷积），以及在某些块中的另一个点卷积（称为扩展卷积）。然而，第一个块只包含两个卷积层（深度卷积和项目卷积），因为它没有扩展卷积。对于B4，架构类似，但每个块中堆叠的卷积更多，`MBConvBlocks`的数量也翻倍。自然地，B7有更多的块和卷积层。对于B4，总共有158次卷积操作分布在32个`MBConvBlocks`之间。'
- en: '`Conv2dStaticSamePadding`: Unlike traditional convolutional layers (such as
    `Conv2d`), these don’t reduce the dimensions. It ensures the input and output
    feature maps have the same spatial dimensions.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Conv2dStaticSamePadding`: 与传统的卷积层（如`Conv2d`）不同，这些层不会减少维度。它确保输入和输出特征图具有相同的空间维度。'
- en: '`BatchNorm2d`: Batch normalization layers that help stabilize and accelerate
    training by normalizing the input features, which helps keep the distribution
    of the input features consistent during training.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BatchNorm2d`: 批标准化层，通过归一化输入特征来帮助稳定和加速训练，这有助于在训练过程中保持输入特征的分布一致性。'
- en: 'Once the over 230 convolutional and pooling operations are performed, we are
    left with a flattened output of a more workable size: 1,792 features, which the
    fully connected layer converts into 12, which, leveraging **softmax** activation,
    outputs probabilities between 0 and 1 for each of the classes. In the garbage
    CNN, there is a **dropout** layer involved to help regularize the training. We
    can ignore this entirely because, for inference, they are ignored.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行了超过230次的卷积和池化操作，我们得到一个更易于处理的扁平化输出：1,792个特征，全连接层将这些特征转换为12个，利用**softmax**激活函数，为每个类别输出介于0和1之间的概率。在垃圾CNN中，有一个**dropout**层用于帮助正则化训练。我们可以完全忽略这一点，因为在推理过程中，它们是被忽略的。
- en: If this wasn’t entirely clear, don’t fret! The sections that follow will demonstrate
    visually through activations, gradients, and perturbations how the network probably
    learned or did not learn image representations.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这还不够清晰，不要担心！接下来的部分将通过激活、梯度和扰动直观地展示网络可能学习或未学习的图像表示方式。
- en: Intermediate activations
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 中间激活
- en: For inference, the image goes through the network’s input and the prediction
    comes out through the output traversing every single layer. However, one of the
    advantages of having a sequential and layered architecture is that we can extract
    any layer’s output and not just the final layer. The **intermediate activations**
    are simply the outputs of any of the convolution or pooling layers. They are activation
    maps because, after an activation function has been applied, the brighter spots
    map to the image’s features. In this case, the model used ReLU on all convolutional
    layers, so that is what activates the spots. We are only interested in the convolutional
    layers’ intermediate activations because the pooling layers are simply downsampled
    versions of these ones. Why not see the higher-resolution version instead?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: As the filters become smaller in width and height, the learned representations
    will be larger. In other words, the first convolutional layer may be about details
    such as texture, the following one about edges, and the last one about shapes.
    We must then flatten the convolutional layers’ output to feed it to the multilayer
    perceptron that takes over from then on.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'What we will do now is extract activations for some of the convolutional layers.
    In B4, there are 158, so we can’t do all of them! To this end, we will obtain
    the first level of layers with `model.children()`, and iterate across them. We
    will append the two `Conv2dStaticSamePadding` layers from this top level into
    a `conv_layers` list. But we will also go deeper, appending the first convolutional
    layer for the first six `MBConvBlock` layers in the `ModuleList` layers. In the
    end, we should have eight convolutional layers – the six in the middle belonging
    to Mobile Inverted Bottleneck Convolution blocks:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Before we iterate across all of them producing activation maps for each convolutional
    layer, let’s do it for a single filter and layer:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s visualize the first filter, but before we do that, we must decide what
    colormap to use. A colormap will determine what colors to assign to different
    numbers as a gradient. For instance, the following colormap has white for `0`
    (`#ffffff` in hexadecimal), a medium gray for `0.25`, and black (`#000000` in
    hexadecimal) for `1` with a gradient between these colors:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You can also use any of the named colormaps from [https://matplotlib.org/stable/tutorials/colors/colormaps.html](https://matplotlib.org/stable/tutorials/colors/colormaps.html),
    rather than using your own. Next, let’s plot the attribution for the first filter
    like this:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Diagram  Description automatically generated with low confidence](img/B18406_07_11.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: Intermediate activation map for the first filter for the first
    convolutional layer for the first true positive battery sample'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: As you can tell in *Figure 7.11*, it seems like the intermediate activations
    for the first filter are finding the edges of the battery and the most prominent
    text.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will iterate across all computational layers and every battery and
    visualize attributions for each one. Now, some of these attribution operations
    can be computationally expensive, so it’s important to clear the GPU cache (`clear_gpu_cache()`)
    in between them:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As you can tell from *Figure 7.12*, the first convolutional layer seems to
    be picking up on the battery’s letters as well as its contours:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated with low confidence](img/B18406_07_12.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12: Intermediate activations for the first convolutional layer for
    battery #4'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'However, *Figure 7.13* shows how, by the fourth convolutional layer, the network
    understands a battery’s contours better:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B18406_07_13.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13: Intermediate activations for the fourth convolutional layer for
    battery #4'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'The last convolutional layer in *Figure 7.14* is impossible to interpret because
    there are 1,792 filters that are 7 pixels wide and high, but rest assured, there
    are some very high-level features encoded in those tiny maps:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text  Description automatically generated](img/B18406_07_14.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: Intermediate activations for the last convolutional layer for
    battery #4'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Extracting intermediate activations can provide you with some insight on a
    sample-by-sample basis. In other words, it’s a **local model interpretation method**.
    It’s by no means the only layerwise-attribution method. Captum has more than ten
    layer attribution methods: [https://github.com/pytorch/captum#about-captum](https://github.com/pytorch/captum#about-captum).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating misclassifications with gradient-based attribution methods
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Gradient-based methods** calculate **attribution maps** for each classification
    with both forward and background passes through the CNN. As the name suggests,
    these methods leverage the gradients in the backward pass to compute the attribution
    maps. All of these methods are local interpretation methods because they only
    derive a single interpretation per sample. Incidentally, attributions in this
    context mean that we are attributing the predicted labels to areas of an image.
    They are often called **sensitivity maps** in academic literature, too.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, we will first need to create an array with all of our misclassification
    samples (`X_misclass`) from the test dataset (`test_data`) using the combined
    indexes for all of our misclassifications of interest (`misclass_idxs`). Since
    there aren’t that many misclassifications, we are loading a single batch of them
    (`next`):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The next step is to create a utility function we can reuse to obtain the attribution
    maps for any method. Optionally, we can smooth the map with a method called `NoiseTunnel`
    ([https://github.com/pytorch/captum#getting-started](https://github.com/pytorch/captum#getting-started)).
    We will cover this method in more detail later:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The preceding code can create attribution maps for any Captum method for a given
    model and device. To that end, it takes tensors for the images, `X`, and their
    corresponding labels, `y`. The labels are optional and only needed if the attribution
    method is targeted – most methods are. Most attribution methods (`attr_method`)
    are initialized with only the model, but some require some additional arguments
    (`init_args`). Where they tend to have the most arguments is when the attribution
    is generated with the `attribute` function, which is why we have the `**kwargs`
    collect additional arguments in the `get_attribution_maps` function and place
    them in this call.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: One important thing to note is that, in this function, we iterate across all
    the samples in the `X` tensor and create the attribute maps for each one independently.
    This is often unnecessary because the attribute methods are all equipped to process
    a batch at once. However, there’s a risk the hardware can’t handle an entire batch,
    and at the time of this writing, very few methods come with an `internal_batch_size`
    argument, which can limit how many samples are processed at a time. What we are
    doing here is essentially equivalent to setting this number to `1` every single
    time in an effort to ensure that we don’t run into memory issues. However, if
    you have powerful hardware, you can rewrite the function to process the `X` and
    `y` tensors directly.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will perform our first gradient-based attribution method.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Saliency maps
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Saliency maps** rely on the absolute value of gradients. The intuition is
    that it will find the pixels in the image that can be perturbed the least so that
    the output changes the most with these values. It doesn’t perform perturbations,
    so it doesn’t validate the hypothesis, and the use of absolute values prevents
    it from finding other evidence to the contrary.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: This first saliency map method was groundbreaking at the time and has inspired
    a bunch of different methods. It’s typically nicknamed “vanilla” to distinguish
    it from other saliency maps.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating saliency maps for all of our misclassified samples is relatively
    simple with our `get_attribution_maps` function. All you need is the Captum attribution
    method (`attr.Saliency`), model (`garbage_mdl`), device, and the tensors for the
    misclassified samples (`X_misclass` and `y_misclass`):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can plot the output of one of these saliency maps, the fifth one, side by
    side with the sample image to provide context. Matplotlib can do this easily with
    a `subplots` grid. We will make a 1 × 3 grid and place the sample image in the
    first spot, its saliency heatmap in the second, and one overlayed over the other
    in the third. As we have done with previous attribution maps, we can use `tensor_to_img`
    to convert the images to `numpy` arrays while also applying a colormap to the
    attribution. It uses the jet colormap (`cmap=''jet''`) by default to make the
    salient areas appear more striking:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding code generates the plot in *Figure 7.15*:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B18406_07_15.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.15: Saliency maps for plastic misclassified as biological waste'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: The sample image in *Figure 7.15* appears to be shredded plastic, but the prediction
    is for biological waste. The vanilla saliency map attributes that prediction mostly
    to the smoother duller areas of the plastic. It appears that the lack of specular
    highlights has thrown the model off, but typically, older broken pieces of plastic
    lose their shine.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Specular highlights are bright spots of light that appear on the surface of
    an object when it reflects light. They are often the direct reflections of a light
    source and are more pronounced on shiny or glossy surfaces, such as metal, glass,
    or water.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Guided Grad-CAM
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To discuss **guided Grad-CAM**, we first ought to discuss **CAM**, which stands
    for **Class Activation Map**. The way CAM works is that it removes all but the
    last fully connected layers, and it replaces the last **MaxPooling** layer with
    a **Global Average Pooling** (**GAP**) layer. A GAP layer calculates the average
    value of each feature map, reducing it to a single value per map, while a MaxPooling
    layer downsizes feature maps by selecting the maximum value from a set of values
    in a local region of the map. For instance, in this case:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: The last convolutional layer outputs a tensor that is `1792` × `7` × `7`.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GAP reduces dimensions by merely averaging the last two dimensions of this tensor,
    producing a `1792` × `1` × `1` tensor.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then feeds this to a fully connected layer with 12 neurons corresponding
    to each class.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you retrain a CAM model and pass a sample image through the CAM model,
    it takes the weights from the last layer (a `1792` × `12` tensor) and extracts
    the values corresponding to the predicted class (a `1792` × `1` tensor).
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, you calculate the dot product of the last convolutional layer’s output
    (`1792` × `7` × `7`) with the weight tensor (`1792` x `1`).
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This weighted sum will end with a `1` × `7` × `7` tensor.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With bilinear interpolation to stretch it out to `1` × `224` × `224`, this becomes
    an upsampled class activation map. When you upsample data, you increase its dimensions.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The intuition behind CAM is that CNNs inherently retain spatial details in convolutional
    layers but they are, sadly, lost in fully connected layers. In fact, each filter
    in the last convolutional layer represents visual patterns at different spatial
    locations. Once weighted, they represent the most salient regions in the entire
    image. However, to apply CAM, you must radically modify a model and retrain it,
    and some models don’t lend themselves easily to this.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, Grad-CAM is a similar concept but lacks the modifying
    and retraining hassle, and uses gradients instead – specifically, those of the
    class score (prior to softmax) concerning the convolutional layer’s activation
    maps. GAP is performed on these gradients to obtain **neuron importance weights**.
    Then, we compute a weighted linear combination of activation maps with these weights,
    followed by a ReLU. The ReLU is very important because it ensures locating features
    that only positively influence the outcome. Like CAM, it is upsampled with bilinear
    interpolation to match the dimensions of the image.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Grad-CAM does have some shortcomings too, such as failing to identify multiple
    occurrences or the entirety of the object represented by the predicted class.
    Like CAM, the resolution of the activation maps may be limited by the final convolutional
    layer’s dimensions, hence the upsampling.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons, we are using **guided Grad-CAM** instead. Guided Grad-CAM
    is a combination of Grad-CAM and guided backpropagation. Guided backpropagation
    is another visualization method that computes the gradients of the target class
    with respect to the input image, but it modifies the backpropagation process to
    only propagate positive gradients for positive activations. This results in a
    higher-resolution, more detailed visualization. This is achieved by performing
    an element-wise multiplication of the Grad-CAM heatmap (upsampled to the input
    image resolution) with the guided backpropagation result. The output is a visualization
    that emphasizes the most relevant features in the image for the given class, with
    higher spatial detail than Grad-CAM alone.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating Grad-CAM attribution maps for all of our misclassified samples can
    be done with our `get_attribution_maps` function. All you need is the Captum attribution
    method (`attr.GuidedGradCam`), model (`garbage_mdl`), device, and the tensors
    for the misclassified samples (`X_misclass` and `y_misclass`), and, within the
    method initialization arguments, a layer for which Grad-CAM attributions are computed:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Notice that we aren’t using the last layer (which can be indexed with `7` or
    `-1`) but the fourth one (`3`). This is just to keep things interesting, but we
    can change it. Next, let’s plot the attributions just as we have before. Nearly
    the same code is used except `saliency_maps` is replaced by `gradcam_maps`. The
    output is depicted in *Figure 7.16*.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B18406_07_16.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.16: Guided Grad-CAM heatmaps for plastic misclassified as biological
    waste'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: As you can observe in *Figure 7.16*, similar smooth matte areas are highlighted
    as with the saliency attribution maps, that except guided Grad-CAM yields a few
    bright areas and edges.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Take all of this with a grain of salt. There is still a lot of ongoing debate
    in the CNN interpretation domain. And researchers are still coming up with new
    and better methods, and even techniques that are nearly perfect for most use cases
    still have flaws. Regarding CAM-like methods, there are many newer ones, such
    as **Score-CAM**, **Ablation-CAM**, and **Eigen-CAM**, which provide similar functionality
    but don’t rely on gradients, which can be unstable and, therefore, occasionally
    unreliable. We won’t discuss them here because, of course, they aren’t gradient-based!
    But it’s essential to note that it doesn’t hurt to try different methods to see
    what works for your use case.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Integrated gradients
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Integrated gradients** (**IG**), also known as **path-integrated gradients**,
    is a technique that is not exclusive to CNNs. You can apply it to any neural network
    architecture because it computes the gradients of the output with respect to the
    inputs averaged all along a path between a **baseline** and the actual input.
    It is agnostic to the presence of convolutional layers. However, it requires the
    definition of a baseline, which is supposed to convey a lack of signal, like a
    uniformly colored image. In practice, for CNNs in particular, this is what a zero
    baseline represents, which, for every pixel, would usually mean a completely black
    image. Also, although the name suggests the use of **path integrals**, integrals
    aren’t computed but approximated, with summation in sufficiently small intervals
    for a certain number of steps. For a CNN, this means it makes variations of the
    input image progressively darker or lighter until it becomes the baseline corresponding
    to the predefined number of steps. It then feeds these variations to the CNN,
    computes the gradients for each one, and averages them. The IG is the dot product
    of the image times the gradient averages.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Like Shapley values, IG is grounded in solid mathematical theory. In this case,
    it’s the **fundamental theorem of calculus for line integrals**. The mathematical
    proof of the IG method ensures that the attributions of all the features add up
    to the difference between the model’s prediction on the input data and its prediction
    on the baseline input. In addition to this property, which they call **completeness**,
    there is linearity preservation, symmetry preservation, and sensitivity. We won’t
    describe each of these properties here. However, it’s important to note that some
    interpretation methods satisfy notable mathematical properties, while others demonstrate
    their effectiveness in practical terms.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: In addition to IG, we will also leverage `NoiseTunnel` to perform small random
    perturbations on the sample image – in other words, to add noise. It creates different
    noisy versions of the same sample image multiple times and then computes the attribution
    method for each. It then averages these attributions, potentially making the attribution
    maps much smoother, which is why this method is called **SmoothGrad**.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'But wait, you may ask: Shouldn’t it be a perturbation-based method then?! We’ve
    already dealt with several perturbation-based methods before in this book, from
    SHAP to anchors, and something they have in common is that they perturb the input
    to measure the effect on the output. SmoothGrad doesn’t measure the impact on
    the outputs. It only helps yield a more robust attribution map because the mean
    attribution of perturbed inputs should make for more trustworthy attribution maps.
    We perform cross-validation to evaluate machine learning models for the same reason:
    the average metrics performed on different test datasets with slightly different
    distributions make for better metrics.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'For IG, we will use very similar code as we did for Saliency, except we will
    add several arguments related to `NoiseTunnel`, such as the type of noise tunnel
    (`nt_type=''smoothgrad''`), the sample variations to produce (`nt_samples=20`),
    and an amount of random noise to add to each one in standard deviations (`stdevs=0.2`).
    We will find that the more permuted samples to generate, the better, up to a point,
    and then it doesn’t have much effect. However, there is such a thing as too much
    noise, and if you use too little, there won’t be any effect:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can also optionally define the number of steps for the IG (`n_steps`). It’s
    set to `50` by default, and we can also modify the baselines, which is a tensor
    of zeros by default. As we’ve done with Grad-CAM, we can plot our first sample
    image side by side with the IG map, but this time, we will modify the code to
    plot the SmoothGrad integrated gradients (`smooth_ig_maps`) in the third position,
    like this:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![Chart  Description automatically generated](img/B18406_07_17.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.17: Integrated gradient heatmaps for plastic misclassified as biological
    waste'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The areas in the IG heatmap in *Figure 7.17* coincide with many of the regions
    spotted by the saliency and guided Grad-CAM maps. However, there are more clusters
    of strong attributions in the bright yellow areas as well as in brownish shadowed
    areas, which is consistent with how some foods look when they are disposed of
    (like banana peels and rotten leafy greens). On the other hand, the bright orange
    and green areas aren’t.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: As for the SmoothGrad IG heatmap, it is striking how different this map is compared
    to the non-smooth IG heatmap. This is not always the case; often, it’s just a
    smoother version. What likely happened was that the `0.2` noise distorted the
    attributions a bit too much, or that 20 perturbed samples weren’t enough. However,
    it’s tough to tell because it’s also possible that SmoothGrad more accurately
    depicts the real story.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: We won’t do this now, but you can visually “tune” the `stdevs` and `nt_samples`
    parameters. You can try it with less noise and more samples, using a series of
    combinations, such as `0.1` and `80`, and `0.15` and `40`, trying to figure out
    whether you see a commonality between them. The one you go with is the one that
    most clearly depicts this consistent story. One of the shortcomings of SmoothGrad
    is having to define optimal parameters. Incidentally, IG also has the same issue
    with defining the baselines and number of steps (`n_steps`). The default baseline
    won’t work in cases where the input image is too large or small, so it must be
    changed, and the authors of the IG paper suggest that 20-300 steps will approximate
    the integral within 5%.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Bonus method: DeepLIFT'
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IG has its detractors, who have made similar methods that avoid using gradients,
    such as **DeepLIFT**. IG can be sensitive to zero-valued gradients and discontinuities
    with gradients, which can lead to misleading attributions. But these point to
    general disadvantages shared by all gradient-based methods. For this reason, we
    are introducing the **Deep Learning Important FeaTures** algorithm (**DeepLIFT**).
    It’s neither a gradient-based nor a perturbation-based method. It’s a backpropagation-based
    approach!
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will contrast it with IG. Like IG and Shapley values, DeepLIFT
    was designed for **completeness**, and as such, complies with remarkable mathematical
    properties. In addition to that, like IG, DeepLIFT can also be applied to various
    deep learning architectures, including CNNs and **recurrent neural networks**
    (**RNNs**), making it versatile for different use cases.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: DeepLIFT works by decomposing the output prediction of the model into contributions
    from each input feature, using the concept of “difference-from-reference.” It
    backpropagates these contributions through the network layers to assign an importance
    score to each input feature.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, like IG, it uses a baseline that represents no specific information
    about any class. However, it then calculates the difference in the activations
    of each neuron between the input and the baseline, and it backpropagates these
    differences through the network, calculating each neuron’s contribution to the
    output prediction. Then we sum the contributions for each input feature to obtain
    its importance score (attribution).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s advantages over IG are as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '**Reference-based**: Unlike gradient-based methods such as IG, DeepLIFT explicitly
    compares the input to a reference input, making the attributions more interpretable
    and meaningful.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-linear interactions**: DeepLIFT considers the non-linear interactions
    between neurons when computing attributions. It captures these interactions by
    considering the multipliers (the change in output due to the change in input)
    in each layer of the neural network.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stability**: DeepLIFT is more stable than gradient-based methods, as it is
    less sensitive to small changes in the input, providing more consistent attributions.
    So, using a SmoothGrad is unnecessary on DeepLIFT attributions although highly
    recommended for gradient-based methods.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, DeepLIFT provides a more interpretable, stable, and comprehensive approach
    to attributions, making it a valuable tool for understanding and explaining deep
    learning models.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will create DeepLIFT attribution maps in a similar fashion as we have
    done the others:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: To plot an attribution map, nearly the same code as with Grad-CAM is used, except
    `gradcam_maps` is replaced by `deeplift_maps`. The output is depicted in *Figure
    7.18*.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B18406_07_18.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.18: DeepLIFT heatmaps for plastic misclassified as biological waste'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The attributions of *Figure 7.18* are not as noisy as in IG. But they also seem
    to cluster around some dull yellows and dark areas in the shadows; it also points
    toward dull greens near the top-right corner.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Tying it all together
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we will take everything that we have learned about gradient-based attribution
    methods and use it to understand the reasons for all the chosen misclassifications
    (the plastic false negatives and metal false positives). As we did with intermediate
    activation maps, we can leverage the `compare_img_pred_viz` function to place
    the higher-resolution sample image side by side with four attribution maps: saliency,
    Grad-CAM, SmoothGrad IG, and DeepLift. To this end, we first have to iterate all
    the misclassifications’ positions and indexes and extract all the maps. Note that
    we are using `overlay_bg` in the `tensor_to_img` function to produce a new image
    overlaying the original image with the heatmap for each. Lastly, we concatenate
    the four attribution outputs into a single image (`viz_img`). Just as we have
    done before, we extract the actual label (`y_true`), predicted label (`y_pred`),
    and `pandas` series with the probabilities (`probs_s`) to add some context to
    the plot we will produce. The `for` loop will produce six plots but, for brevity’s
    sake, we are only going to discuss three of them:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding code generates *Figures 7.19* to *7.21*. It’s important to note
    that in all generated plots, we can observe saliency attributions at the top left,
    SmoothGrad IG at the top right, guided Grad-CAM at the bottom left, and DeepLIFT
    at the bottom right:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B18406_07_19.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.19: Gradient-based attributions for battery as metal misclassification
    #8'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7.19*, there’s a lack of consistency between all four attribution
    methods. The saliency attribution maps show that all the center parts of the batteries
    are seen as metal surfaces, in addition to the white parts of the cardboard container.
    On the other hand, SmoothGrad IG zeros in on the white cardboard and Grad-CAM
    on the blue cardboard almost exclusively. Lastly, DeepLIFT is much more sparse,
    only pointing to some parts of the white cardboard.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7.20*, the attributions are much more consistent than in *Figure
    7.19*. Matte white areas are clearly confusing the model. This makes sense considering
    that the plastic in the training data was mostly single pieces of empty plastic
    containers – including white milk jugs. However, people do recycle toys, plastic
    tools like spatulas, and other plastic objects. Interestingly enough, although
    all attribution methods were salient around white and light-yellow surfaces, SmoothGrad
    IG also highlights some edges, like one of the ducks’ hats and another one’s collar:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_07_20.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.20: Gradient-based attributions for plastic misclassification #86'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'To continue with the recycling toys theme, how do LEGO bricks get misclassified
    as batteries? See *Figure 7.21* for an interpretation:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B18406_07_21.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.21: Gradient-based attributions for plastic misclassification #89'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.21* shows how, among all the attribution methods, it’s mostly the
    yellow and green bricks (and to a lesser degree, light blue) that are to blame
    for the misclassification because these are popular colors among battery manufacturers,
    as attested by the training data. Also, the flat surface in between the studs
    got the most attributions since these resemble the contacts in batteries and,
    more specifically, 9-volt square batteries. As with the other examples, saliency
    was the most noisy method. However, this time, guided Grad-CAM was the least noisy.
    It was also more salient on the edges than on surfaces, unlike the others.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: We will next try to discover what the model learned about batteries (in addition
    to white glass) through perturbation-based attribution methods performed on true
    positives.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Understanding classifications with perturbation-based attribution methods
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perturbation-based methods have already been covered to a great extent in this
    book so far. So many of the methods we have covered, including SHAP, LIME, anchors,
    and even permutation feature importance, employ perturbation-based strategies.
    The intuition behind them is that if you remove, alter, or mask features in your
    input data and then make predictions with them, you’ll be able to attribute the
    difference between the new predictions and the original predictions to the changes
    you made in the input. These strategies can be leveraged in both global and local
    interpretation methods.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now do the same as we did with the misclassification samples, but to
    the chosen true positives, and gather four of each class in a single tensor (`X_correctcls`):'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'One of the more complicated aspects of performing permutation methods on images
    is that there are not just a few dozen features but many thousands to permute.
    Picture this: 224 x 224 equals 50,176 pixels, and if we want to measure how a
    change in each pixel independently affects the outcome, we’ll need to make at
    least 20 permuted samples for each pixel. So, over a million! For this reason,
    several permutation methods accept masks to determine which blocks of pixels to
    permute at once. If we group them in blocks of 32 x 32 pixels, this means we’ll
    have only 49 blocks in total to permute. However, although it will speed up the
    attribution methods, we’ll miss out on the effects on smaller sets of pixels the
    larger the block.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use many methods to create masks, such as using a segmentation algorithm
    to break up the images into intuitive blocks based on surfaces and edges. Segmentation
    is done per image, so the number and placement of segments will vary on an image-to-image
    basis. There are many methods with scikit-learn’s image segmentation library (`skimage.segmentation`):
    [https://scikit-image.org/docs/stable/api/skimage.segmentation.html](https://scikit-image.org/docs/stable/api/skimage.segmentation.html).
    However, we are going to keep things simple and create one mask for all 224 x
    224 images with the following code:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: What the preceding code does is initialize a tensor of zeros the size of the
    model’s input. It’s easier to conceptualize this tensor as an empty image. Then
    it moves across strides that are 16 pixels wide and high, from the top-left corner
    of the image to the bottom right. As it moves across, it sets the values with
    consecutive numbers with the `counter`. What you end up with is a tensor with
    all values filled with numbers between 0 and 195, and, if you visualized it as
    an image, it would be a diagonal gradient from black at the top left to light
    gray at the bottom right. What’s important to note is that each block with the
    same value is treated as if it were the same pixel by the attribution method.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Before we move forward, let’s discuss baselines. In Captum attribution methods,
    as in other libraries for that matter, the default baseline is a tensor of zeros,
    which is usually equivalent to a black image when images are made up of floating-point
    numbers between 0 and 1\. However, in our case, we are standardizing our input
    tensors so the model doesn’t see tensors with a minimum of 0 but a mean of 0!
    Therefore, for our garbage model, a tensor of zeros corresponds to a medium gray
    image, not a black image. For gradient-based methods, there’s nothing inherently
    wrong with a gray image baseline because there are likely a number of steps between
    it and the input image. However, perturbation-based methods can be particularly
    sensitive to having baselines that are too close to the input image because if
    you replace parts of the input image with the baseline, the model won’t tell the
    difference!
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'For our garbage model’s case, a black image is made up of tensors of `-2.1179`
    because one of the transformations performed to standardize the input tensors
    was `(x-0.485)/0.229`, which happens to equal approximately `-2.1179`, when `x=0`.
    You can also calculate the tensors when `x=1`; it converts to `2.64` for a white
    image. That being said, there’s no harm in assuming that somewhere in our true
    positive samples, there’s at least one pixel that has the lowest value and another
    with the highest, so we will just use `max()` and `min()` to create both light
    and dark baselines:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We will use only one baseline for all but one perturbation method but feel free
    to switch them around. Now, on to creating attribution maps for each method!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Feature ablation
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Feature ablation** is a relatively simple method. What it does is occlude
    portions of the sample input image by replacing it with the baseline, which is,
    by default, zero. The goal is to understand the importance of each input feature
    (or feature group) in making a prediction by observing the effect of altering
    it.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how feature ablation works:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '**Obtain the original prediction**: First, the model’s prediction for the original
    input is obtained. This serves as a baseline for comparing the effect of perturbing
    the input features.'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Perturb the input feature**: Next, for each input feature (or feature group
    as set by the feature mask), it is replaced with the baselines value. This creates
    an “ablated” version of the input.'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Obtain the prediction for the perturbed input**: The model’s prediction is
    calculated for the ablated input.'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Compute the attribution**: The difference in the model’s predictions between
    the original input and the ablated input is calculated. This difference is attributed
    to the altered feature, indicating its importance in the prediction.'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature ablation is a simple and intuitive approach to understanding the importance
    of input features in a model’s prediction. However, it has some limitations. It
    assumes that features are independent and may not accurately capture the effects
    of interactions between features. Additionally, it can be computationally expensive
    for models with a large number of input features or complex input structures.
    Despite these limitations, feature ablation is a valuable tool for understanding
    and interpreting model behavior.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate the attribution maps, we will use the `get_attribution_maps` function
    as we have before, and enter the additional arguments for the `feature_mask` and
    `baselines`:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To plot an example of the attribution map, you can copy the same code that
    we used for saliency, except `saliency_maps` is replaced by `ablation_maps`, and
    we are using the second image in the `occlusion_maps` array, like this:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B18406_07_22.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.22: Feature ablation maps for a white glass true positive from the
    test dataset'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7.22*, the feature groups in the bottom of the bowl of the wine glass
    appear to be most important because their absence makes the biggest difference
    in the outcome, but other portions of the glass are also salient to a lesser degree,
    except the stem of the wine glass. It makes sense because a wine glass without
    a stem is still a glass-like container.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss a similar method that will be able to show us attributions
    with greater detail.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Occlusion sensitivity
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Occlusion sensitivity** is very similar to feature ablation because it also
    replaces portions of the image with a baseline. However, unlike feature ablation,
    it doesn’t use a feature mask to group pixels together. Instead, it groups contiguous
    features automatically with a sliding window and strides. In this process, it
    creates many overlapping regions. When this happens, it averages the output differences
    to compute the attribution for each pixel.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, besides overlapping regions and their corresponding averages,
    occlusion sensitivity and feature ablation are identical. In fact, if we used
    both sliding windows and strides of 3 x 16 x 16, there wouldn’t be any overlapping
    areas and the feature grouping would be identical to those defined by our `feature_mask`
    made up of 16 x 16 blocks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: So, you may wonder, what’s the point of being familiar with both methods? The
    point is occlusion sensitivity is only suitable for use when a fixed grouping
    of contiguous features matter, like with images and perhaps other spatial data.
    And because of its use of strides, it can capture local dependencies and spatial
    relationships between features. However, although we used contiguous blocks of
    features, feature ablation doesn’t have to because the `feature_mask` can be arranged
    in whichever way it makes most sense for your inputs to be segmented. This small
    detail makes it very versatile to other data types. Therefore, feature ablation
    is a more general approach that can handle various input types and model architectures,
    while occlusion sensitivity is specifically tailored to image data and convolutional
    neural networks, with a focus on spatial relationships between features.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate the attribution maps for occlusion, we will do as before, and enter
    the additional arguments for the `baselines`, `sliding_window_shapes`, and `strides`:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Please note that we are creating ample overlapping regions by setting the strides
    to be only 8 pixels while the sliding windows are 16 pixels. To plot an attribution,
    you can copy the same code that we used for feature ablation, except `ablation_maps`
    is replaced by `occlusion_maps`. The output is depicted in *Figure 7.23*:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B18406_07_23.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.23: Occlusion sensitivity maps for a white glass true positive from
    the test dataset'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: With *Figure 7.23*, we can tell that occlusion’s attributions are eerily similar
    to the ablation’s attribution, except with more resolution. This resemblance shouldn’t
    be surprising considering how the feature mask of the former aligns with the sliding
    window of the latter.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Whether we use blocks of non-overlapping 16 x 16 pixels or overlapping 8 x 8,
    the impact of their absence is measured independently to create the attributions.
    Therefore, both ablation and occlusion methods aren’t equipped to measure interactions
    between non-contiguous feature groups. This can prove to be a problem when the
    absence of two non-contiguous feature groups is what causes a classification to
    change. For instance, can a wine glass without a stem or a base still be considered
    a wine glass? It can certainly be considered glass, one would hope, but perhaps
    the model has learned the wrong relationships.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'Speaking of relationships, next, we will revisit an old friend: Shapley!'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Shapley value sampling
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you recall from *Chapter 4*, *Global Model-Agnostic Interpretation Methods*,
    Shapley has provided a method that is very good at measuring and attributing the
    impact of coalitions of features to the outcome. Shapley does this by permuting
    entire coalitions of features at a time rather than permuting one feature at a
    time, like the two previous methods. That way, it can tease out how more than
    one feature or feature group interacts with one another.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to create the attribution maps should be very familiar by now. This
    method uses the `feature_mask` and `baselines` but also the number of feature
    permutations tested (`n_samples`). This last attribute has a huge impact on the
    fidelity of the method. However, it can make it notoriously computationally expensive,
    so we are not going to run it with the default 25 samples per permutation. Instead,
    we will use 5 samples to make things more manageable. However, feel free to tweak
    it should your hardware be able to handle it:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/B18406_07_24.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.24: Shapley value sampling maps for a white glass true positive from
    the test dataset'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.24* shows some consistent attributions, such as the most salient
    area being in the bottom-left corner of the wine glass bowl. Also, the base seems
    to be more important than the occlusion and ablation methods suggested.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: However, the attributions are a lot more noisy than the previous ones. This
    is partially because we didn’t use a sufficient number of samples to cover all
    the combinations of features and interactions, and partially because of the messy
    nature of interactions. It makes sense that attributions for a single independent
    feature are concentrated in a few areas, such as the bowl of a wine glass. However,
    interactions can rely on several parts of the image, such as the base and the
    rim of the wine glass. They may become important only when they appear together.
    More interesting is the effect of a background. For instance, if you remove portions
    of the background, does the wine glass no longer look like a wine glass? Perhaps
    the background is more important than you think, especially when dealing with
    a translucent material.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: KernelSHAP
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we are on the topic of Shapley values, let’s try `KernelSHAP` from *Chapter
    4*, *Global Model-Agnostic Interpretation Methods*. It leverages LIME to compute
    Shapley values more efficiently. The Captum implementation is similar to the SHAP
    one except it uses linear regression and not Lasso, and it computes the kernel
    differently. Also, for the LIME image explainer, it is best to use meaningful
    feature groups (called superpixels) rather than the contiguous blocks we have
    used in the feature mask. The same advice persists for `KernelSHAP`. However,
    we will keep this simple for this exercise, and also consistent for comparing
    with the other three permutation-based methods.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now create the attribution maps but, this time, we will do one with
    light baselines and another with dark ones. Because `KernelSHAP` is an approximation
    to Shapley sampling values and not as computationally expensive, we can set `n_samples=300`.
    However, this won’t necessarily guarantee high fidelity because it takes a high
    amount of samples in `KernelSHAP` to approximate what a relatively low amount
    of samples can do exhaustively with Shapley:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B18406_07_25.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.25: KernelSHAP maps for a white glass true positive from the test
    dataset'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The two attribution maps in *Figure 7.25* are mostly not consistent with each
    other, but more importantly, not with the previous attributions. Sometimes, some
    methods have a harder time than others, or it takes some tweaking to get them
    to work how we expect them to.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Tying it all together
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we will take everything that we have learned about perturbation-based
    attribution methods and use it to understand the reasons for all the chosen true
    positive classifications (for both white glass and batteries). As we did before,
    we can leverage the `compare_img_pred_viz` function to place the higher-resolution
    sample image side by side with the four attribution maps: feature ablation, occlusion
    sensitivity, Shapley, and `KernelSHAP`. First, we have to iterate all the classifications’
    positions and indexes and extract all the maps. Note that we are using `overlay_bg`
    to produce a new image overlaying the original image with the heatmap for every
    attribution, as we did for the gradient-based section. Lastly, we concatenate
    the four attribution outputs into a single image (`viz_img`). Just as we have
    done before, we extract the actual label (`y_true`), predicted label (`y_pred`),
    and `pandas` series with the probabilities (`probs_s`) to add some context to
    the plot we will produce. The `for` loop will produce six plots, but we will only
    discuss two of them:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Overall, you can tell that ablation and occlusion are very consistent, while
    much less so with Shapley and `KernelSHAP`. However, what Shapley and `KernelSHAP`
    have in common is that the attributions are more spread out.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7.26*, all attribution methods have text highlighted, as well as,
    at least, the left contact of the battery. This is similar to *Figure 7.28*, where
    the text is abundantly highlighted as well as the top contact. This suggests that,
    for batteries, the model has learned that text and a contact matter. As for white
    glass, it is less clear. All the attribution methods in *Figure 7.27* point to
    some of the edges of the broken vase, but not always the same edges (except for
    ablation and occlusion, which are consistent):'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_07_26.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.26: Perturbation-based attributions for battery classification #1'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'White glass is the hardest glass to classify of the three, and it’s not hard
    to tell why:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B18406_07_27.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.27: Perturbation-based attributions for white glass classification
    #113'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: As noted in *Figure 7.27*, and others in the test example, it’s hard for the
    model to distinguish white glass from the light backgrounds. It manages to classify
    it correctly with these examples. However, this doesn’t mean it will generalize
    well in other examples where glass is in shards and not as well-lit. As long as
    the attributions show significant influence from the background, it’s hard to
    trust that it can recognize glass for its specular highlights, texture, and edges
    alone.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B18406_07_28.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.28: Perturbation-based attributions for battery classification #2'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: For *Figure 7.28*, the background is also highlighted significantly in all attribution
    maps. But perhaps this is because the baseline was dark and so is the object in
    its entirety. If you replace an area just outside the edge of the battery with
    a black square, it makes sense that the model would be confused. For this reason,
    with permutation-based methods, it’s important to choose an appropriate baseline.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Mission accomplished
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The mission was to provide an objective evaluation of the garbage classification
    model for the municipal recycling plant. The predictive performance on out-of-sample
    validation images was dismal! You could have stopped there, but then you would
    not have known how to make a better model.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the predictive performance evaluation was instrumental in deriving
    specific misclassifications, as well as correct classifications, to assess using
    other interpretation methods. To this end, you ran a comprehensive suite of interpretation
    methods, including activation, gradient, perturbation, and backpropagation-based
    methods. The consensus between all the methods was that the model was having the
    following issues:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Differentiating between the background and the objects
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding that different objects share similar color hues
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confounding lighting conditions, such as specular highlights as specific material
    characteristics, like with the wine glasses
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An inability to separate unique features of each object, such as plastic studs
    in LEGO bricks from battery contacts
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being confused by objects with multiple materials, such as batteries contained
    in plastic and even cardboard packaging
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To address these problems, the model needed to be trained with a more varied
    dataset – hopefully, one that reflects the real-world conditions of the recycling
    plant; for instance, the expected background (on a conveyor belt), different lighting
    conditions, and even objects partially occluded by hands, gloves, bags, and so
    on. Also, they ought to add a category for miscellaneous objects that are made
    up of multiple materials.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this dataset has been compiled, it is essential to leverage data augmentation
    to make the model even more robust to all sorts of variations: angle, brightness,
    contrast, saturation, and hue variants. And they won’t have to retrain the model
    from scratch! They can even fine-tune EfficientNet!'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading this chapter, you should understand how to leverage traditional
    interpretation methods to more thoroughly assess predictive performance on a CNN
    classifier and visualize the learning process of CNNs with activation-based methods.
    You should also understand how to compare and contrast misclassifications and
    true positives with gradient-based and perturbation-based attribution methods.
    In the next chapter, we will study interpretation methods for NLP transformers.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Smilkov, D., Thorat, N., Kim, B., Viégas, F., and Wattenberg, M., 2017, *SmoothGrad:
    Removing noise by adding noise*. ArXiv, abs/1706.03825: [https://arxiv.org/abs/1706.03825](https://arxiv.org/abs/1706.03825)'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sundararajan, M., Taly, A., and Yan, Q., 2017, *Axiomatic Attribution for Deep
    Networks*. Proceedings of Machine Learning Research, pp. 3319–3328, International
    Convention Centre, Sydney, Australia: [https://arxiv.org/abs/1703.01365](https://arxiv.org/abs/1703.01365)'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeiler, M.D., and Fergus, R., 2014, *Visualizing and Understanding Convolutional
    Networks*. In European conference on computer vision, pp. 818–833: [https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901)'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shrikumar, A., Greenside, P., and Kundaje, A., 2017, *Learning Important Features
    Through Propagating Activation Differences*: [https://arxiv.org/abs/1704.02685](https://arxiv.org/abs/1704.02685)'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn more on Discord
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/inml](Chapter_7.xhtml)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code107161072033138125.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
