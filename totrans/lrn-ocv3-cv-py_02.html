<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0;Handling Files, Cameras, and GUIs"><div class="titlepage" id="aid-I3QM2"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Handling Files, Cameras, and GUIs</h1></div></div></div><p>Installing OpenCV and running samples is fun, but at this stage, we want to try it out ourselves. This chapter introduces OpenCV's I/O functionality. We also discuss the concept of a project and the beginnings of an object-oriented design for this project, which we will flesh out in subsequent chapters.</p><p>By starting with a look at the I/O capabilities and design patterns, we will build our project in the same way we would make a sandwich: from the outside in. Bread slices and spread, or endpoints and glue, come before fillings or algorithms. We choose this approach because computer vision is mostly extroverted—it contemplates the real world outside our computer—and we want to apply all our subsequent algorithmic work to the real world through a common interface.</p><div class="section" title="Basic I/O scripts"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec13"/>Basic I/O scripts</h1></div></div></div><p>Most CV <a id="id46" class="indexterm"/>applications need to get images as input. Most also produce images as output. An interactive CV application might require a camera as an input source and a window as an output destination. However, other possible sources and destinations include image files, video files, and raw bytes. For example, raw bytes might be transmitted via a network connection, or they might be generated by an algorithm if we incorporate procedural graphics into our application. Let's look at each of these possibilities.</p><div class="section" title="Reading/writing an image file"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec11"/>Reading/writing an image file</h2></div></div></div><p>OpenCV <a id="id47" class="indexterm"/>provides the <code class="literal">imread()</code> and <code class="literal">imwrite()</code> functions that <a id="id48" class="indexterm"/>support various file formats for still images. The <a id="id49" class="indexterm"/>supported formats vary by system but should always include the <a id="id50" class="indexterm"/>BMP format. Typically, PNG, JPEG, and TIFF should be among the supported formats too.</p><p>Let's explore the anatomy of the representation of an image in Python and NumPy.</p><p>No matter the format, each pixel has a value, but the difference is in how the pixel is represented. For example, we can create a black square image from scratch by simply creating a 2D NumPy array:</p><div class="informalexample"><pre class="programlisting">img = numpy.zeros((3,3), dtype=numpy.uint8)</pre></div><p>If we print this image to a console, we obtain the following result:</p><div class="informalexample"><pre class="programlisting">array([[0, 0, 0],
       [0, 0, 0],
       [0, 0, 0]], dtype=uint8)</pre></div><p>Each pixel is <a id="id51" class="indexterm"/>represented by a single 8-bit integer, which means <a id="id52" class="indexterm"/>that the values for each pixel are in the 0-255 <a id="id53" class="indexterm"/>range.</p><p>Let's now <a id="id54" class="indexterm"/>convert this image into <span class="strong"><strong>Blue-green-red</strong></span> (<span class="strong"><strong>BGR</strong></span>) using <a id="id55" class="indexterm"/>
<code class="literal">cv2.cvtColor</code>:</p><div class="informalexample"><pre class="programlisting">img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)</pre></div><p>Let's observe how the image has changed:</p><div class="informalexample"><pre class="programlisting">array([[[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]],

       [[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]],

       [[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]]], dtype=uint8)</pre></div><p>As you can see, each pixel is now represented by a three-element array, with each integer representing the B, G, and R channels, respectively. Other color spaces, such as HSV, will be represented in the same way, albeit with different value ranges (for example, the hue value of the HSV color space has a range of 0-180) and different numbers of channels.</p><p>You can check the structure of an image by inspecting the <code class="literal">shape</code> property, which returns rows, columns, and the number of channels (if there is more than one).</p><p>Consider this example:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; img = numpy.zeros((3,3), dtype=numpy.uint8)
&gt;&gt;&gt; img.shape</pre></div><p>The preceding code will print <code class="literal">(3,3)</code>. If you then converted the image to BGR, the shape would be <code class="literal">(3,3,3)</code>, which indicates the presence of three channels per pixel.</p><p>Images can be loaded from one file format and saved to another. For example, let's convert an image from PNG to JPEG:</p><div class="informalexample"><pre class="programlisting">import cv2

image = cv2.imread('MyPic.png')
cv2.imwrite('MyPic.jpg', image)</pre></div><div class="note" title="Note"><h3 class="title"><a id="note09"/>Note</h3><p>Most of the <a id="id56" class="indexterm"/>OpenCV functionalities that we use are in the <code class="literal">cv2</code> <a id="id57" class="indexterm"/>module. You might come across other OpenCV guides that instead rely on the <code class="literal">cv</code> or <code class="literal">cv2.cv</code> modules, which are legacy versions. The reason why the Python module is called <code class="literal">cv2</code> is not because it is a Python binding module for OpenCV 2.x.x, but because it has introduced a better API, which leverages object-oriented programming as opposed to the previous <code class="literal">cv</code> module, which adhered to a more procedural style of programming.</p></div><p>By default, <code class="literal">imread()</code> returns an image in the BGR color format even if the file uses a grayscale format. BGR <a id="id58" class="indexterm"/>represents the same color space as <span class="strong"><strong>red-green-blue</strong></span> (<span class="strong"><strong>RGB</strong></span>), but the byte order is reversed.</p><p>Optionally, we may <a id="id59" class="indexterm"/>specify the mode of <code class="literal">imread()</code> to be <a id="id60" class="indexterm"/>one of the following enumerators:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">IMREAD_ANYCOLOR = 4</code></li><li class="listitem"><code class="literal">IMREAD_ANYDEPTH = 2</code></li><li class="listitem"><code class="literal">IMREAD_COLOR = 1</code></li><li class="listitem"><code class="literal">IMREAD_GRAYSCALE = 0</code></li><li class="listitem"><code class="literal">IMREAD_LOAD_GDAL = 8</code></li><li class="listitem"><code class="literal">IMREAD_UNCHANGED = -1</code></li></ul></div><p>For example, let's load a PNG file as a grayscale image (losing any color information in the process), and then, save it as a grayscale PNG image:</p><div class="informalexample"><pre class="programlisting">import cv2

grayImage = cv2.imread('MyPic.png', cv2.IMREAD_GRAYSCALE)
cv2.imwrite('MyPicGray.png', grayImage)</pre></div><p>To avoid unnecessary headaches, use absolute paths to your images (for example, <code class="literal">C:\Users\Joe\Pictures\MyPic.png</code> on Windows or <code class="literal">/home/joe/pictures/MyPic.png</code> on Unix) at least while you're familiarizing yourself with OpenCV's API. The path of an image, unless absolute, is relative to the folder that contains the Python script, so in the preceding example, <code class="literal">MyPic.png</code> would have to be in the same folder as your Python script or the image won't be found.</p><p>Regardless of the mode, <code class="literal">imread()</code> discards any alpha channel (transparency). The <code class="literal">imwrite()</code> function <a id="id61" class="indexterm"/>requires an image to be in the BGR or grayscale <a id="id62" class="indexterm"/>format with a certain number of bits per <a id="id63" class="indexterm"/>channel that the output format can support. For example, <code class="literal">bmp</code> <a id="id64" class="indexterm"/>requires 8 bits per channel, while PNG allows either 8 or 16 bits per channel.</p></div><div class="section" title="Converting between an image and raw bytes"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec12"/>Converting between an image and raw bytes</h2></div></div></div><p>Conceptually, a byte is an integer ranging from 0 to 255. In all real-time graphic applications <a id="id65" class="indexterm"/>today, a pixel is typically represented by one byte per channel, though other representations are also possible.</p><p>An OpenCV image is a 2D or 3D array of the <code class="literal">.array</code> type. An 8-bit grayscale image is a 2D array containing byte values. A 24-bit BGR image is a 3D array, which also contains byte values. We may access these values by using an expression, such as <code class="literal">image[0, 0]</code> or <code class="literal">image[0, 0, 0]</code>. The first index is the pixel's <span class="emphasis"><em>y</em></span> coordinate or row, <code class="literal">0</code> being the top. The second index is the pixel's <span class="emphasis"><em>x</em></span> coordinate or column, <code class="literal">0</code> being the leftmost. The third index (if applicable) represents a color channel.</p><p>For example, in an <a id="id66" class="indexterm"/>8-bit grayscale image with a white pixel in the upper-left corner, <code class="literal">image[0, 0]</code> is <code class="literal">255</code>. For a 24-bit BGR image with a blue pixel in the upper-left corner, <code class="literal">image[0, 0]</code> is <code class="literal">[255, 0, 0]</code>.</p><div class="note" title="Note"><h3 class="title"><a id="note10"/>Note</h3><p>As an <a id="id67" class="indexterm"/>alternative to using an expression, such as <code class="literal">image[0, 0]</code> or <code class="literal">image[0, 0] = 128</code>, we may use an expression, such as <code class="literal">image.item((0, 0))</code> or <code class="literal">image.setitem((0, 0), 128)</code>. The latter expressions are more efficient for single-pixel operations. However, as we will see in subsequent chapters, we usually want to perform operations on large slices of an image rather than on single pixels.</p></div><p>Provided that an image has 8 bits per channel, we can cast it to a standard Python <code class="literal">bytearray</code>, which is one-dimensional:</p><div class="informalexample"><pre class="programlisting">byteArray = bytearray(image)</pre></div><p>Conversely, provided that <code class="literal">bytearray</code> contains bytes in an appropriate order, we can cast and then reshape it to get a <code class="literal">numpy.array</code> type that is an image:</p><div class="informalexample"><pre class="programlisting">grayImage = <span class="strong"><strong>numpy</strong></span>.array(grayByteArray).reshape(height, width)
bgrImage = <span class="strong"><strong>numpy</strong></span>.array(bgrByteArray).reshape(height, width, 3)</pre></div><p>As a more complete example, let's convert <code class="literal">bytearray</code>, which contains random bytes to a grayscale image and a BGR image:</p><div class="informalexample"><pre class="programlisting">import cv2
import <span class="strong"><strong>numpy</strong></span>
import os

# Make an array of 120,000 random bytes.
randomByteArray = bytearray(os.urandom(120000))
flatNumpyArray = numpy.array(randomByteArray)

# Convert the array to make a 400x300 grayscale image.
grayImage = flatNumpyArray.reshape(300, 400)
cv2.imwrite('RandomGray.png', grayImage)

# Convert the array to make a 400x100 color image.
bgrImage = flatNumpyArray.reshape(100, 400, 3)
cv2.imwrite('RandomColor.png', bgrImage)</pre></div><p>After running this <a id="id68" class="indexterm"/>script, we should have a pair of <a id="id69" class="indexterm"/>randomly generated images, <code class="literal">RandomGray.png</code> and <code class="literal">RandomColor.png</code>, in <a id="id70" class="indexterm"/>the script's directory.</p><div class="note" title="Note"><h3 class="title"><a id="note11"/>Note</h3><p>Here, we use Python's standard <code class="literal">os.urandom()</code> function to generate random raw bytes, which we will then convert to a NumPy array. Note that it is also possible to generate a random NumPy array directly (and more efficiently) using a statement, such as <code class="literal">numpy.random.randint(0, 256, 120000).reshape(300, 400)</code>. The only reason we use <code class="literal">os.urandom()</code> is to help demonstrate a conversion from raw bytes.</p></div></div><div class="section" title="Accessing image data with numpy.array"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec13"/>Accessing image data with numpy.array</h2></div></div></div><p>Now <a id="id71" class="indexterm"/>that you have a better understanding <a id="id72" class="indexterm"/>of how an image is formed, we <a id="id73" class="indexterm"/>can start performing basic operations on it. We know that the easiest (and most common) way to load an image in OpenCV is to use the <code class="literal">imread</code> function. We also know that this will return an image, which is really an array (either a 2D or 3D one, depending on the parameters you passed to <code class="literal">imread()</code>).</p><p>The <code class="literal">y.array</code> structure is well optimized for array operations, and it allows certain kinds of bulk manipulations that are not available in a plain Python list. These kinds of <code class="literal">.array</code> type-specific operations come in handy for image manipulations in OpenCV. Let's explore image manipulations from the start and step by step though, with a basic example: say you want to manipulate a pixel at the coordinates, (0, 0), of a BGR image and turn it into a white pixel.</p><div class="informalexample"><pre class="programlisting">import cv

import <span class="strong"><strong>numpy</strong></span> as np
img = cv.imread('MyPic.png')
img[0,0] = [255, 255, 255]</pre></div><p>If you then showed the image with a standard <code class="literal">imshow()</code> call, you will see a white dot in the top-left corner of the image. Naturally, this isn't very useful, but it shows what can be accomplished. Let's now leverage the ability of <code class="literal">numpy.array</code> to operate transformations to an array much faster than a plain Python array.</p><p>Let's say that you want to change the blue value of a particular pixel, for example, the pixel at coordinates, (150, 120). The <code class="literal">numpy.array</code> type provides a very handy method, <code class="literal">item()</code>, which takes <a id="id74" class="indexterm"/>three parameters: the x (or left) position, y (or top), and the index within the array at (x, y) position (remember <a id="id75" class="indexterm"/>that in a BGR image, the data at <a id="id76" class="indexterm"/>a certain position is a three-element array containing the B, G, and R values in this order) and returns the value at the index position. Another <code class="literal">itemset()</code> method sets the value of a particular channel of a particular pixel to a specified value (<code class="literal">itemset()</code> takes two arguments: a three-element tuple (x, y, and index) and the new value).</p><p>In this example, we will change the value of blue at (150, 120) from its current value (127) to an arbitrary 255:</p><div class="informalexample"><pre class="programlisting">import cv
import <span class="strong"><strong>numpy</strong></span> as  np
img = cv.imread('MyPic.png')
print img.item(150, 120, 0)  // prints the current value of B for that pixel
img.itemset( (150, 120, 0), 255)
print img.item(150, 120, 0)  // prints 255</pre></div><p>Remember that we do this with <code class="literal">numpy.array</code> for two reasons: <code class="literal">numpy.array</code> is an extremely optimized library for these kind of operations, and because we obtain more readable code through NumPy's elegant methods rather than the raw index access of the first example.</p><p>This particular code doesn't do much in itself, but it does open a world of possibilities. It is, however, advisable that you utilize built-in filters and methods to manipulate an entire image; the above approach is only suitable for small regions of interest.</p><p>Now, let's take a look at a very common operation, namely, manipulating channels. Sometimes, you'll want to zero-out all the values of a particular channel (B, G, or R).</p><div class="note" title="Note"><h3 class="title"><a id="tip04"/>Tip</h3><p>Using loops to manipulate the Python arrays is very costly in terms of runtime and should be avoided at all costs. Using array indexing allows for efficient manipulation of pixels. This is a costly and slow operation, especially if you manipulate videos, you'll find yourself with a jittery output. Then a feature called indexing comes to the rescue. Setting all G (green) values of an image to <code class="literal">0</code> is as simple as using this code:</p><div class="informalexample"><pre class="programlisting">import cv
import  as  np
img = cv.imread('MyPic.png')
img[:, :, 1] = 0</pre></div></div><p>This is a fairly impressive piece of code and easy to understand. The relevant line is the last one, which basically instructs the program to take all pixels from all rows and columns and set the resulting value at index one of the three-element array, representing the color of the pixel to <code class="literal">0</code>. If you display this image, you will notice a complete absence of green.</p><p>There are a <a id="id77" class="indexterm"/>number of interesting things we can do by accessing raw pixels with NumPy's array indexing; one of them <a id="id78" class="indexterm"/>is defining <span class="strong"><strong>regions of interests</strong></span> (<span class="strong"><strong>ROI</strong></span>). Once the region is <a id="id79" class="indexterm"/>defined, we can perform a number of <a id="id80" class="indexterm"/>operations, namely, binding this region to a variable, and then even defining a second region and assigning it the value of the first one (visually copying a portion of the image over to another position in the image):</p><div class="informalexample"><pre class="programlisting">import cv
import numpy as  np
img = cv.imread('MyPic.png')
my_roi = img[0:100, 0:100]
img[300:400, 300:400] = my_roi</pre></div><p>It's important to make sure that the two regions correspond in terms of size. If not, NumPy will (rightly) complain that the two shapes mismatch.</p><p>Finally, there are a few interesting details we can obtain from <code class="literal">numpy.array</code>, such as the image properties using this code:</p><div class="informalexample"><pre class="programlisting">import cv
import numpy  as  np
img = cv.imread('MyPic.png')
print img.shape
print img.size
print img.dtype</pre></div><p>These three properties are in this order:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Shape</strong></span>: NumPy returns a tuple containing the width, height, and—if the image is in color—the number of channels. This is useful to debug a type of image; if the image is monochromatic or grayscale, it will not contain a channel's value.</li><li class="listitem"><span class="strong"><strong>Size</strong></span>: This property refers to the size of an image in pixels.</li><li class="listitem"><span class="strong"><strong>Datatype</strong></span>: This property refers to the datatype used for an image (normally a variation of an unsigned integer type and the bits supported by this type, that is, <code class="literal">uint8</code>).</li></ul></div><p>All in all, it is strongly advisable that you familiarize yourself with NumPy in general and <code class="literal">numpy.array</code> in <a id="id81" class="indexterm"/>particular when working with <a id="id82" class="indexterm"/>OpenCV, as it is the foundation of <a id="id83" class="indexterm"/>an image processing done with Python.</p></div><div class="section" title="Reading/writing a video file"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec14"/>Reading/writing a video file</h2></div></div></div><p>OpenCV <a id="id84" class="indexterm"/>provides the <code class="literal">VideoCapture</code> and <code class="literal">VideoWriter</code> <a id="id85" class="indexterm"/>classes that support various video file formats. The <a id="id86" class="indexterm"/>supported formats vary by system but should always include an AVI. Via its <code class="literal">read()</code> method, a <code class="literal">VideoCapture</code> class may be polled for new frames until it <a id="id87" class="indexterm"/>reaches the end of its video file. Each frame is an image in a BGR format.</p><p>Conversely, an image may be passed to the <code class="literal">write()</code> method of the <code class="literal">VideoWriter</code> class, which appends the image to a file in <code class="literal">VideoWriter</code>. Let's look at an example that reads frames from one AVI file and writes them to another with a YUV encoding:</p><div class="informalexample"><pre class="programlisting">import cv2

videoCapture = cv2.VideoCapture('MyInputVid.avi')
fps = videoCapture.get(cv2.CAP_PROP_FPS)
size = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)),
        int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)))
videoWriter = cv2.VideoWriter(
    'MyOutputVid.avi', cv2.VideoWriter_fourcc('I','4','2','0'), fps, size)

success, frame = videoCapture.read()
while success: # Loop until there are no more frames.
    videoWriter.write(frame)
    success, frame = videoCapture.read()</pre></div><p>The arguments to the <code class="literal">VideoWriter</code> class constructor deserve special attention. A video's filename must be specified. Any preexisting file with this name is overwritten. A video codec must also be specified. The available codecs may vary from system to system. These are the options that are included:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">cv2.VideoWriter_fourcc('I','4','2','0')</code>: This option is an uncompressed YUV encoding, 4:2:0 chroma subsampled. This encoding is widely compatible but produces large files. The file extension should be <code class="literal">.avi</code>.</li><li class="listitem"><code class="literal">cv2.VideoWriter_fourcc('P','I','M','1')</code>: This option is MPEG-1. The file extension should be <code class="literal">.avi</code>.</li><li class="listitem"><code class="literal">cv2.VideoWriter_fourcc('X','V','I','D')</code>: This option is MPEG-4 and a preferred option if you want the resulting video size to be average. The file extension should be <code class="literal">.avi</code>.</li><li class="listitem"><code class="literal">cv2.VideoWriter_fourcc('T','H','E','O')</code>: This option is Ogg Vorbis. The file extension should be <code class="literal">.ogv</code>.</li><li class="listitem"><code class="literal">cv2.VideoWriter_fourcc('F','L','V','1')</code>: This option is a Flash video. The file extension should be <code class="literal">.flv</code>.</li></ul></div><p>A frame rate and <a id="id88" class="indexterm"/>frame size must be specified too. Since we are copying video frames <a id="id89" class="indexterm"/>from another video, these properties can <a id="id90" class="indexterm"/>be read from the <code class="literal">get()</code> method of the <code class="literal">VideoCapture</code> <a id="id91" class="indexterm"/>class.</p></div><div class="section" title="Capturing camera frames"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec15"/>Capturing camera frames</h2></div></div></div><p>A stream <a id="id92" class="indexterm"/>of camera frames is represented by the <code class="literal">VideoCapture</code> class too. However, for a camera, we construct a <code class="literal">VideoCapture</code> class by passing the <a id="id93" class="indexterm"/>camera's device index instead of a video's filename. Let's consider an example that captures 10 seconds of video from a camera and writes it to an AVI file:</p><div class="informalexample"><pre class="programlisting">import cv2

cameraCapture = cv2.VideoCapture(0)
fps = 30 # an assumption
size = (int(cameraCapture.get(cv2.CAP_PROP_FRAME_WIDTH)),
        int(cameraCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)))
videoWriter = cv2.VideoWriter(
    'MyOutputVid.avi', cv2.VideoWriter_fourcc('I','4','2','0'), fps, size)

success, frame = cameraCapture.read()
numFramesRemaining = 10 * fps - 1
while success and numFramesRemaining &gt; 0:
    videoWriter.write(frame)
    success, frame = cameraCapture.read()
    numFramesRemaining -= 1
cameraCapture.release()</pre></div><p>Unfortunately, the <code class="literal">get()</code> method of a <code class="literal">VideoCapture</code> class does not return an accurate value for the camera's frame rate; it always returns <code class="literal">0</code>. The official documentation at <a class="ulink" href="http://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html">http://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html</a> reads:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"When querying a property that is not supported by the backend used by the <code class="literal">VideoCapture</code> class, value <code class="literal">0</code> is returned."</em></span></p></blockquote></div><p>This occurs most commonly on systems where the driver only supports basic functionalities.</p><p>For the purpose of creating an appropriate <code class="literal">VideoWriter</code> class for the camera, we have to either make an assumption about the frame rate (as we did in the code previously) or measure it using a timer. The latter approach is better and we will cover it later in this chapter.</p><p>The number of <a id="id94" class="indexterm"/>cameras and their order is of course system-dependent. Unfortunately, OpenCV does not provide any means of querying the number <a id="id95" class="indexterm"/>of cameras or their properties. If an invalid index is used to construct a <code class="literal">VideoCapture</code> class, the <code class="literal">VideoCapture</code> class will not yield any frames; its <code class="literal">read()</code> method will return <code class="literal">(false, None)</code>. A good way to prevent it from trying to retrieve frames from <code class="literal">VideoCapture</code> that were not opened correctly is to use the <code class="literal">VideoCapture.isOpened</code> method, which returns a Boolean.</p><p>The <code class="literal">read()</code> method is inappropriate when we need to synchronize a set of cameras or a multihead camera (such as a stereo camera or Kinect). Then, we use the <code class="literal">grab()</code> and <code class="literal">retrieve()</code> methods instead. For a set of cameras, we use this code:</p><div class="informalexample"><pre class="programlisting">success0 = cameraCapture0.grab()
success1 = cameraCapture1.grab()
if success0 and success1:
    frame0 = cameraCapture0.retrieve()
    frame1 = cameraCapture1.retrieve()</pre></div></div><div class="section" title="Displaying images in a window"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec16"/>Displaying images in a window</h2></div></div></div><p>One of the <a id="id96" class="indexterm"/>most basic operations in OpenCV is <a id="id97" class="indexterm"/>displaying an image. This can be done with the <code class="literal">imshow()</code> function. If you come from any other GUI framework background, you would think it sufficient to call <code class="literal">imshow()</code> to display an image. This is only partially true: the image will be displayed, and will disappear immediately. This is by design, to enable the constant refreshing of a window frame when working with videos. Here's a very simple example code to display an image:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('my-image.png')
cv2.imshow('my image', img)
cv2.waitKey()
cv2.destroyAllWindows()</pre></div><p>The <code class="literal">imshow()</code> function takes two parameters: the name of the frame in which we want to display the image, and the image itself. We'll talk about <code class="literal">waitKey()</code> in more detail when we explore the displaying of frames in a window.</p><p>The aptly <a id="id98" class="indexterm"/>named <code class="literal">destroyAllWindows()</code> function <a id="id99" class="indexterm"/>disposes of all the windows created by OpenCV.</p></div><div class="section" title="Displaying camera frames in a window"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec17"/>Displaying camera frames in a window</h2></div></div></div><p>OpenCV allows <a id="id100" class="indexterm"/>named windows <a id="id101" class="indexterm"/>to be created, redrawn, and destroyed using the <code class="literal">namedWindow()</code>, <code class="literal">imshow()</code>, and <code class="literal">destroyWindow()</code> functions. Also, any window may capture keyboard input via the <code class="literal">waitKey()</code> function and mouse input via the <code class="literal">setMouseCallback()</code> function. Let's look at an example where we show the frames of a live camera input:</p><div class="informalexample"><pre class="programlisting">import cv2

clicked = False
def onMouse(event, x, y, flags, param):
    global clicked
    if event == cv2.EVENT_LBUTTONUP:
        clicked = True

cameraCapture = cv2.VideoCapture(0)
cv2.namedWindow('MyWindow')
cv2.setMouseCallback('MyWindow', onMouse)

print 'Showing camera feed. Click window or press any key to stop.'
success, frame = cameraCapture.read()
while success and cv2.waitKey(1) == -1 and not clicked:
    cv2.imshow('MyWindow', frame)
    success, frame = cameraCapture.read()

cv2.destroyWindow('MyWindow')
cameraCapture.release()</pre></div><p>The argument for <code class="literal">waitKey()</code> is a number of milliseconds to wait for keyboard input. The return value is either <code class="literal">-1</code> (meaning that no key has been pressed) or an ASCII keycode, such as <code class="literal">27</code> for <span class="emphasis"><em>Esc</em></span>. For a list of ASCII keycodes, see <a class="ulink" href="http://www.asciitable.com/">http://www.asciitable.com/</a>. Also, note that Python provides a standard function, <code class="literal">ord()</code>, which can convert a character to its ASCII keycode. For example, <code class="literal">ord('a')</code> returns <code class="literal">97</code>.</p><div class="note" title="Note"><h3 class="title"><a id="tip05"/>Tip</h3><p>On some systems, <code class="literal">waitKey()</code> may return a value that encodes more than just the ASCII keycode. (A bug is known to occur on Linux when OpenCV uses GTK as its backend GUI library.) On all systems, we can ensure that we extract just the ASCII keycode by reading the last byte from the return value like this:</p><div class="informalexample"><pre class="programlisting">keycode = cv2.waitKey(1)
if keycode != -1:
    keycode &amp;= 0xFF</pre></div></div><p>OpenCV's window functions and <code class="literal">waitKey()</code> are interdependent. OpenCV windows are only updated when <code class="literal">waitKey()</code> is called, and <code class="literal">waitKey()</code> only captures input when an OpenCV window has focus.</p><p>The mouse <a id="id102" class="indexterm"/>callback passed to <code class="literal">setMouseCallback()</code> should take five arguments, as seen in our code sample. The callback's <a id="id103" class="indexterm"/>
<code class="literal">param</code> argument is set as an optional third argument to <code class="literal">setMouseCallback()</code>. By default, it is <code class="literal">0</code>. The callback's event argument is one of the following actions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">cv2.EVENT_MOUSEMOVE</code>: This event refers to mouse movement</li><li class="listitem"><code class="literal">cv2.EVENT_LBUTTONDOWN</code>: This event refers to the left button down</li><li class="listitem"><code class="literal">cv2.EVENT_RBUTTONDOWN</code>: This refers to the right button down</li><li class="listitem"><code class="literal">cv2.EVENT_MBUTTONDOWN</code>: This refers to the middle button down</li><li class="listitem"><code class="literal">cv2.EVENT_LBUTTONUP</code>: This refers to the left button up</li><li class="listitem"><code class="literal">cv2.EVENT_RBUTTONUP</code>: This event refers to the right button up</li><li class="listitem"><code class="literal">cv2.EVENT_MBUTTONUP</code>: This event refers to the middle button up</li><li class="listitem"><code class="literal">cv2.EVENT_LBUTTONDBLCLK</code>: This event refers to the left button being double-clicked</li><li class="listitem"><code class="literal">cv2.EVENT_RBUTTONDBLCLK</code>: This refers to the right button being double-clicked</li><li class="listitem"><code class="literal">cv2.EVENT_MBUTTONDBLCLK</code>: This refers to the middle button being double-clicked</li></ul></div><p>The mouse callback's flags argument may be some bitwise combination of the following events:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">cv2.EVENT_FLAG_LBUTTON</code>: This event refers to the left button being pressed</li><li class="listitem"><code class="literal">cv2.EVENT_FLAG_RBUTTON</code>: This event refers to the right button being pressed</li><li class="listitem"><code class="literal">cv2.EVENT_FLAG_MBUTTON</code>: This event refers to the middle button being pressed</li><li class="listitem"><code class="literal">cv2.EVENT_FLAG_CTRLKEY</code>: This event refers to the <span class="emphasis"><em>Ctrl</em></span> key being pressed</li><li class="listitem"><code class="literal">cv2.EVENT_FLAG_SHIFTKEY</code>: This event refers to the <span class="emphasis"><em>Shift</em></span> key being pressed</li><li class="listitem"><code class="literal">cv2.EVENT_FLAG_ALTKEY</code>: This event refers to the <span class="emphasis"><em>Alt</em></span> key being pressed</li></ul></div><p>Unfortunately, OpenCV does not provide any means of handling window events. For example, we cannot stop our application when a window's close button is clicked. Due to OpenCV's limited event handling and GUI capabilities, many developers prefer to integrate it with <a id="id104" class="indexterm"/>other application <a id="id105" class="indexterm"/>frameworks. Later in this chapter, we will design an abstraction layer to help integrate OpenCV into any application framework.</p></div></div></div>
<div class="section" title="Project Cameo (face tracking and image manipulation)" id="aid-J2B81"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec14"/>Project Cameo (face tracking and image manipulation)</h1></div></div></div><p>OpenCV is <a id="id106" class="indexterm"/>often studied through a cookbook approach that covers a lot of algorithms but nothing about high-level application development. To an extent, this approach is understandable because OpenCV's potential applications are so diverse. OpenCV is used in a wide variety of applications: photo/video editors, motion-controlled games, a robot's AI, or psychology experiments where we log participants' eye movements. Across such different use cases, can we truly study a useful set of abstractions?</p><p>I believe we can and the sooner we start creating abstractions, the better. We will structure our study of OpenCV around a single application, but, at each step, we will design a component of this application to be extensible and reusable.</p><p>We will develop an interactive application that performs face tracking and image manipulations on camera input in real time. This type of application covers a broad range of OpenCV's functionality and challenges us to create an efficient, effective implementation.</p><p>Specifically, our application will perform real-time facial merging. Given two streams of camera input (or, optionally, prerecorded video input), the application will superimpose faces from one stream onto faces in the other. Filters and distortions will be applied to give this blended scene a unified look and feel. Users should have the experience of being engaged in a live performance where they enter another environment and persona. This type of user experience is popular in amusement parks such as Disneyland.</p><p>In such an <a id="id107" class="indexterm"/>application, users would immediately notice flaws, such as a low frame rate or inaccurate tracking. To get the best results, we will try several approaches using conventional imaging and depth imaging.</p><p>We will call our application Cameo. A cameo is (in jewelry) a small portrait of a person or (in film) a very brief role played by a celebrity.</p></div>
<div class="section" title="Cameo &#x2013; an object-oriented design"><div class="titlepage" id="aid-K0RQ2"><div><div><h1 class="title"><a id="ch02lvl1sec15"/>Cameo – an object-oriented design</h1></div></div></div><p>Python <a id="id108" class="indexterm"/>applications can be written in a purely procedural style. This is often done with small applications, such as our basic I/O scripts, discussed previously. However, from now on, we will use an object-oriented style because it promotes modularity and extensibility.</p><p>From our overview of OpenCV's I/O functionality, we know that all images are similar, regardless of their source or destination. No matter how we obtain a stream of images or where we send it as output, we can apply the same application-specific logic to each frame in this stream. Separation of I/O code and application code becomes especially convenient in an application, such as Cameo, which uses multiple I/O streams.</p><p>We will create classes called <code class="literal">CaptureManager</code> and <code class="literal">WindowManager</code> as high-level interfaces to I/O streams. Our application code may use <code class="literal">CaptureManager</code> to read new frames and, optionally, to dispatch each frame to one or more outputs, including a still image file, a video file, and a window (via a <code class="literal">WindowManager</code> class). A <code class="literal">WindowManager</code> class lets our application code handle a window and events in an object-oriented style.</p><p>Both <code class="literal">CaptureManager</code> and <code class="literal">WindowManager</code> are extensible. We could make implementations that do not rely on OpenCV for I/O. Indeed, <span class="emphasis"><em>Appendix A</em></span>, <span class="emphasis"><em>Integrating with Pygame</em></span>, <span class="emphasis"><em>OpenCV Computer Vision with Python</em></span>, uses a <code class="literal">WindowManager</code> subclass.</p><div class="section" title="Abstracting a video stream with managers.CaptureManager"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec18"/>Abstracting a video stream with managers.CaptureManager</h2></div></div></div><p>As we <a id="id109" class="indexterm"/>have seen, OpenCV can capture, show, and record a stream of images from either a video file or a <a id="id110" class="indexterm"/>camera, but there are some special considerations in each case. Our <code class="literal">CaptureManager</code> class abstracts some of the differences and provides a higher-level interface to dispatch images from the capture stream to one or more outputs—a still image file, video file, or a window.</p><p>A <code class="literal">CaptureManager</code> class is initialized with a <code class="literal">VideoCapture</code> class and has the <code class="literal">enterFrame()</code> and <code class="literal">exitFrame()</code> methods that should typically be called on every iteration of an application's main loop. Between a call to <code class="literal">enterFrame()</code> and <code class="literal">exitFrame()</code>, the application may (any number of times) set a <code class="literal">channel</code> property and get a <code class="literal">frame</code> property. The <code class="literal">channel</code> property is initially <code class="literal">0</code> and only multihead cameras use other values. The <code class="literal">frame</code> property is an image corresponding to the current channel's state when <code class="literal">enterFrame()</code> was called.</p><p>A <code class="literal">CaptureManager</code> class also has the <code class="literal">writeImage()</code>, <code class="literal">startWritingVideo()</code>, and <code class="literal">stopWritingVideo()</code> methods that may be called at any time. Actual file writing is postponed until <code class="literal">exitFrame()</code>. Also, during the <code class="literal">exitFrame()</code> method, the <code class="literal">frame</code> property may be shown in a window, depending on whether the application code provides a <code class="literal">WindowManager</code> class either as an argument to the constructor of <code class="literal">CaptureManager</code> or by setting a <code class="literal">previewWindowManager</code> property.</p><p>If the application code manipulates <code class="literal">frame</code>, the manipulations are reflected in recorded files and in the window. A <code class="literal">CaptureManager</code> class has a constructor argument and property called <code class="literal">shouldMirrorPreview</code>, which should be <code class="literal">True</code> if we want <code class="literal">frame</code> to be mirrored (horizontally flipped) in the window but not in recorded files. Typically, when facing a camera, users prefer live camera feed to be mirrored.</p><p>Recall that a <code class="literal">VideoWriter</code> class needs a frame rate, but OpenCV does not provide any way to get an accurate frame rate for a camera. The <code class="literal">CaptureManager</code> class works around this limitation by using a frame counter and Python's standard <code class="literal">time.time()</code> function to estimate the frame rate if necessary. This approach is not foolproof. Depending on frame rate fluctuations and the system-dependent implementation of <code class="literal">time.time()</code>, the accuracy of the estimate might still be poor in some cases. However, if we deploy to unknown hardware, it is better than just assuming that the user's camera has a particular frame rate.</p><p>Let's create <a id="id111" class="indexterm"/>a file called <a id="id112" class="indexterm"/>
<code class="literal">managers.py</code>, which will contain our implementation of <code class="literal">CaptureManager</code>. The implementation turns out to be quite long. So, we will look at it in several pieces. First, let's add imports, a constructor, and properties, as follows:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy
import time

class CaptureManager(object):
    
    
    def __init__(self, capture, previewWindowManager = None,
                 shouldMirrorPreview = False):
        
        
        self.previewWindowManager = previewWindowManager
        self.shouldMirrorPreview = shouldMirrorPreview
        
        
        self._capture = capture
        self._channel = 0
        self._enteredFrame = False
        self._frame = None
        self._imageFilename = None
        self._videoFilename = None
        self._videoEncoding = None
        self._videoWriter = None
        
        self._startTime = None
        self._framesElapsed = long(0)
        self._fpsEstimate = None
    
    @property
    def channel(self):
        return self._channel
    
    @channel.setter
    def channel(self, value):
        if self._channel != value:
            self._channel = value
            self._frame = None
    
    @property
    def frame(self):
        if self._enteredFrame and self._frame is None:
            _, self._frame = self._capture.retrieve()
        return self._frame
    
    @property
    def isWritingImage (self):

        return self._imageFilename is not None
    
    @property
    def isWritingVideo(self):
        return self._videoFilename is not None</pre></div><p>Note that most of the <code class="literal">member</code> variables are non-public, as denoted by the underscore prefix <a id="id113" class="indexterm"/>in variable names, such as <code class="literal">self._enteredFrame</code>. These nonpublic variables relate to the state of the current frame and any file-writing operations. As discussed previously, the application code only needs to configure a few things, which are implemented as constructor arguments and settable public properties: the camera channel, window manager, and the option to mirror the camera preview.</p><p>This book <a id="id114" class="indexterm"/>assumes a certain level of familiarity with Python; however, if you are getting confused by those <code class="literal">@</code> annotations (for example, <code class="literal">@property</code>), refer to the Python documentation about <code class="literal">decorators</code>, a built-in feature of the language that allows the wrapping of a function by another function, normally used to apply a user-defined behavior in several places of an application (refer to <a class="ulink" href="https://docs.python.org/2/reference/compound_stmts.html#grammar-token-decorator">https://docs.python.org/2/reference/compound_stmts.html#grammar-token-decorator</a>).</p><div class="note" title="Note"><h3 class="title"><a id="note12"/>Note</h3><p>Python does not have the concept of private member variables and the single/double underscore prefix (<code class="literal">_</code>) is only a convention.</p><p>By this convention, in Python, variables that are prefixed with a single underscore should be treated as protected (accessed only within the class and its subclasses), while variables that are prefixed with a double underscore should be treated as private (accessed only within the class).</p></div><p>Continuing <a id="id115" class="indexterm"/>with our <a id="id116" class="indexterm"/>implementation, let's add the <code class="literal">enterFrame()</code> and <code class="literal">exitFrame()</code> methods to <code class="literal">managers.py</code>:</p><div class="informalexample"><pre class="programlisting">    def enterFrame(self):
        """Capture the next frame, if any."""
        
        # But first, check that any previous frame was exited.
        assert not self._enteredFrame, \
            'previous enterFrame() had no matching exitFrame()'
        
        if self._capture is not None:
            self._enteredFrame = self._capture.grab()
    
    def exitFrame (self):
        """Draw to the window. Write to files. Release the frame."""
        
        # Check whether any grabbed frame is retrievable.
        # The getter may retrieve and cache the frame.
        if self.frame is None:
            self._enteredFrame = False
            return
        
        # Update the FPS estimate and related variables.
        if self._framesElapsed == 0:
            self._startTime = time.time()
        else:
            timeElapsed = time.time() - self._startTime
            self._fpsEstimate =  self._framesElapsed / timeElapsed
        self._framesElapsed += 1
        
        # Draw to the window, if any.
        if self.previewWindowManager is not None:
            if self.shouldMirrorPreview:
                mirroredFrame = numpy.fliplr(self._frame).copy()
                self.previewWindowManager.show(mirroredFrame)
            else:
                self.previewWindowManager.show(self._frame)
        
        # Write to the image file, if any.
        if self.isWritingImage:
            cv2.imwrite(self._imageFilename, self._frame)
            self._imageFilename = None
        
        # Write to the video file, if any.
        self._writeVideoFrame()
        
        # Release the frame.
        self._frame = None
        self._enteredFrame = False</pre></div><p>Note that <a id="id117" class="indexterm"/>the implementation of <code class="literal">enterFrame()</code> only grabs (synchronizes) a frame, whereas actual retrieval <a id="id118" class="indexterm"/>from a channel is postponed to a subsequent reading of the <code class="literal">frame</code> variable. The implementation of <code class="literal">exitFrame()</code> takes the image from the current channel, estimates a frame rate, shows the image via the window manager (if any), and fulfills any pending requests to write the image to files.</p><p>Several other methods also pertain to file writing. To finish our class implementation, let's add the remaining file-writing methods to <code class="literal">managers.py</code>:</p><div class="informalexample"><pre class="programlisting">    def writeImage(self, filename):
        """Write the next exited frame to an image file."""
        self._imageFilename = filename
    
    def startWritingVideo(
            self, filename,
            encoding = cv2.VideoWriter_fourcc('I','4','2','0')):
        """Start writing exited frames to a video file."""
        self._videoFilename = filename
        self._videoEncoding = encoding
    
    def stopWritingVideo (self):
        """Stop writing exited frames to a video file."""
        self._videoFilename = None
        self._videoEncoding = None
        self._videoWriter = None
    
    
def _writeVideoFrame(self):
        
        if not self.isWritingVideo:
            return
        
        if self._videoWriter is None:
            fps = self._capture.get(cv2.CAP_PROP_FPS)
            if fps == 0.0:
                # The capture's FPS is unknown so use an estimate.
                if self._framesElapsed &lt; 20:
                    # Wait until more frames elapse so that the
                    # estimate is more stable.
                    return
                else:
                    fps = self._fpsEstimate
            size = (int(self._capture.get(
                        cv2.CAP_PROP_FRAME_WIDTH)),
                    int(self._capture.get(
                        cv2.CAP_PROP_FRAME_HEIGHT)))
            self._videoWriter = cv2.VideoWriter(
                self._videoFilename, self._videoEncoding,
                fps, size)
        
        self._videoWriter.write(self._frame)</pre></div><p>The <code class="literal">writeImage()</code>, <code class="literal">startWritingVideo()</code>, and <code class="literal">stopWritingVideo()</code> public methods simply record the parameters for file-writing operations, whereas the actual writing operations are postponed to the next call of <code class="literal">exitFrame()</code>. The <code class="literal">_writeVideoFrame()</code> nonpublic method creates or appends a video file in a manner that should be familiar from our earlier scripts. (See the <span class="emphasis"><em>Reading/writing a video file</em></span> section.) However, in situations where the frame rate is unknown, we skip some frames at the start of the capture session so that we have time to build up an estimate of the frame rate.</p><p>Although our <a id="id119" class="indexterm"/>current implementation of <code class="literal">CaptureManager</code> relies on <code class="literal">VideoCapture</code>, we could make other implementations that do not use OpenCV for input. For example, we could make a subclass that <a id="id120" class="indexterm"/>is instantiated with a socket connection, whose byte stream could be parsed as a stream of images. We could also make a subclass that uses a third-party camera library with different hardware support than what OpenCV provides. However, for Cameo, our current implementation is sufficient.</p></div><div class="section" title="Abstracting a window and keyboard with managers.WindowManager"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec19"/>Abstracting a window and keyboard with managers.WindowManager</h2></div></div></div><p>As <a id="id121" class="indexterm"/>we have seen, OpenCV <a id="id122" class="indexterm"/>provides functions <a id="id123" class="indexterm"/>that cause a window <a id="id124" class="indexterm"/>to be created, destroyed, show an image, and process events. Rather than being methods of a window class, these functions require a window's name to pass as an argument. Since this interface is not object-oriented, it is inconsistent with OpenCV's general style. Also, it is unlikely to be compatible with other window or event handling interfaces that we might eventually want to use instead of OpenCV's.</p><p>For the sake of object orientation and adaptability, we abstract this functionality into a <code class="literal">WindowManager</code> class with the <code class="literal">createWindow()</code>, <code class="literal">destroyWindow()</code>, <code class="literal">show()</code>, and <code class="literal">processEvents()</code> methods. As a property, a <code class="literal">WindowManager</code> class has a function object called <code class="literal">keypressCallback</code>, which (if not <code class="literal">None</code>) is called from <code class="literal">processEvents()</code> in response to any key press. The <code class="literal">keypressCallback</code> object must take a single argument, such as an ASCII keycode.</p><p>Let's add <a id="id125" class="indexterm"/>the following implementation of <code class="literal">WindowManager</code> to <code class="literal">managers.py</code>:</p><div class="informalexample"><pre class="programlisting">class WindowManager(object):
    
    
    def __init__(self, windowName, keypressCallback = None):
        self.keypressCallback = keypressCallback
        
        self._windowName = windowName
        self._isWindowCreated = False

    
    @property
    def isWindowCreated(self):
        return self._isWindowCreated
    
    def createWindow (self):
        cv2.namedWindow(self._windowName)
        self._isWindowCreated = True
    
    def show(self, frame):
        cv2.imshow(self._windowName, frame)
    
    def destroyWindow (self):
        cv2.destroyWindow(self._windowName)
        self._isWindowCreated = False
    
    def processEvents (self):
        keycode = cv2.waitKey(1)
        if self.keypressCallback is not None and keycode != -1:
            # Discard any non-ASCII info encoded by GTK.
            keycode &amp;= 0xFF
            self.keypressCallback(keycode)</pre></div><p>Our current implementation only supports keyboard events, which will be sufficient for Cameo. However, we could modify <code class="literal">WindowManager</code> to support mouse events too. For example, the class's interface could be expanded to include a <code class="literal">mouseCallback</code> property (and optional constructor argument), but could otherwise remain the same. With some event framework other than OpenCV's, we could support additional event types in the same way by adding callback properties.</p><p>
<span class="emphasis"><em>Appendix A</em></span>, <span class="emphasis"><em>Integrating with Pygame</em></span>, <span class="emphasis"><em>OpenCV Computer Vision with Python</em></span>, shows a <code class="literal">WindowManager</code> subclass that is implemented <a id="id126" class="indexterm"/>with Pygame's <a id="id127" class="indexterm"/>window <a id="id128" class="indexterm"/>handling and event framework instead of OpenCV's. This implementation improves on the base <code class="literal">WindowManager</code> class by properly handling quit events—for example, when a user clicks on a window's close button. Potentially, many other event types can be handled via Pygame too.</p></div><div class="section" title="Applying everything with cameo.Cameo"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec20"/>Applying everything with cameo.Cameo</h2></div></div></div><p>Our <a id="id129" class="indexterm"/>application is represented by a <code class="literal">Cameo</code> class with two methods: <code class="literal">run()</code> and <code class="literal">onKeypress()</code>. On initialization, a <code class="literal">Cameo</code> class creates a <code class="literal">WindowManager</code> class with <code class="literal">onKeypress()</code> as a callback, as well as a <code class="literal">CaptureManager</code> class using a camera and the <code class="literal">WindowManager</code> class. When <code class="literal">run()</code> is called, the application executes a main loop in which frames and events are processed. As a result of event processing, <code class="literal">onKeypress()</code> may be called. The spacebar causes a screenshot to be taken, <span class="emphasis"><em>Tab</em></span> causes a screencast (a video recording) to start/stop, and <span class="emphasis"><em>Esc</em></span> causes the application to quit.</p><p>In the same <a id="id130" class="indexterm"/>directory as <code class="literal">managers.py</code>, let's create a file called <code class="literal">cameo.py</code> containing the following implementation of <code class="literal">Cameo</code>:</p><div class="informalexample"><pre class="programlisting">import cv2
from managers import WindowManager, CaptureManager

class Cameo(object):
    
    def __init__(self):
        self._windowManager = WindowManager('Cameo',
                                            self.onKeypress)
        self._captureManager = CaptureManager(
            cv2.VideoCapture(0), self._windowManager, True)
    
    def run(self):
        """Run the main loop."""
        self._windowManager.createWindow()
        while self._windowManager.isWindowCreated:
            self._captureManager.enterFrame()
            frame = self._captureManager.frame
            
            # TODO: Filter the frame (Chapter 3).
            
            self._captureManager.exitFrame()
            self._windowManager.processEvents()
    
    def onKeypress (self, keycode):
        """Handle a keypress.
        
        space  -&gt; Take a screenshot.
        tab    -&gt; Start/stop recording a screencast.
        escape -&gt; Quit.
        
        """
        if keycode == 32: # space
            self._captureManager.writeImage('screenshot.png')
        elif keycode == 9: # tab
            if not self._captureManager.isWritingVideo:
                self._captureManager.startWritingVideo(
                    'screencast.avi')
            else:
                self._captureManager.stopWritingVideo()
        elif keycode == 27: # escape
            self._windowManager.destroyWindow()

if __name__=="__main__":
    Cameo().run()</pre></div><p>When running the application, note that the live camera feed is mirrored, while screenshots and screencasts are not. This is the intended behavior, as we pass <code class="literal">True</code> for <code class="literal">shouldMirrorPreview</code> when initializing the <code class="literal">CaptureManager</code> class.</p><p>So far, we do not <a id="id131" class="indexterm"/>manipulate the frames in any way except to mirror them for preview. We will start to add more interesting effects in <a class="link" title="Chapter 3. Processing Images with OpenCV 3" href="part0023.xhtml#aid-LTSU1">Chapter 3</a>, <span class="emphasis"><em>Filtering Images</em></span>.</p></div></div>
<div class="section" title="Summary" id="aid-KVCC1"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Summary</h1></div></div></div><p>By now, we should have an application that displays a camera feed, listens for keyboard input, and (on command) records a screenshot or screencast. We are ready to extend the application by inserting some image-filtering code (<a class="link" title="Chapter 3. Processing Images with OpenCV 3" href="part0023.xhtml#aid-LTSU1">Chapter 3</a>, <span class="emphasis"><em>Filtering Images</em></span>) between the start and end of each frame. Optionally, we are also ready to integrate other camera drivers or application frameworks (<span class="emphasis"><em>Appendix A</em></span>, <span class="emphasis"><em>Integrating with Pygame</em></span>, <span class="emphasis"><em>OpenCV Computer Vision with Python</em></span>) besides the ones supported by OpenCV.</p><p>We also now have the knowledge to process images and understand the principle of image manipulation through the NumPy arrays. This forms the perfect foundation to understand the next topic, filtering images.</p></div></body></html>