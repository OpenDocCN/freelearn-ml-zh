- en: Chapter 2. Unsupervised Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。无监督机器学习
- en: As we have seen in the [Chapter 1](text00015.html#ch01 "Chapter 1. Introduction
    to Practical Machine Learning Using Python") , *Introduction to Practical Machine
    Learning Using Python* , unsupervised learning is designed to provide insightful
    information on data unlabeled date. In many cases, a large dataset (both in terms
    of number of points and number of features) is unstructured and does not present
    any information at first sight, so these techniques are used to highlight hidden
    structures on data (clustering) or to reduce its complexity without losing relevant
    information (dimensionality reduction). This chapter will focus on the main clustering
    algorithms (the first part of the chapter) and dimensionality reduction methods
    (the second part of the chapter). The differences and advantages of the methods
    will be highlighted by providing a practical example using Python libraries. All
    of the code will be available on the author's GitHub profile, in the [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/)
    folder. We will now start describing clustering algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第一章](text00015.html#ch01 "第一章。使用Python的实用机器学习介绍")中所见，*使用Python的实用机器学习介绍*，无监督学习旨在提供关于未标记数据的洞察性信息。在许多情况下，大数据集（无论是点的数量还是特征的数量）都是非结构化的，并且一开始并不呈现任何信息，因此这些技术被用来突出数据中的隐藏结构（聚类）或在不丢失相关信息的情况下降低其复杂性（降维）。本章将重点关注主要的聚类算法（本章的第一部分）和降维方法（本章的第二部分）。通过提供使用Python库的实用示例来突出显示这些方法的差异和优势。所有代码都将可在作者的GitHub个人资料中找到，在[https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/)文件夹中。我们现在将开始描述聚类算法。
- en: Clustering algorithms
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法
- en: Clustering algorithms are employed to restructure data in somehow ordered subsets
    so that a meaningful structure can be inferred. A cluster can be defined as a
    group of data points with some similar features. The way to quantify the similarity
    of data points is what determines the different categories of clustering.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法被用于以某种有序的方式重新组织数据，以便可以推断出有意义的结构。一个簇可以被定义为具有某些相似特征的数据点集合。量化数据点相似度的方法决定了聚类的不同类别。
- en: Clustering algorithms can be divided into different categories based on different
    metrics or assumptions in which data has been manipulated. We are going to discuss
    the most relevant categories used nowadays, which are distribution methods, centroid
    methods, density methods, and hierarchical methods. For each category, a particular
    algorithm is going to be presented in detail, and we will begin by discussing
    distribution methods. An example to compare the different algorithms will be discussed,
    and both the IPython notebook and script are available in the my GitHub book folder
    at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/)
    .
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法可以根据数据操作的不同度量或假设被划分为不同的类别。我们将讨论目前最相关的类别，这些类别包括分布方法、质心方法、密度方法和层次方法。对于每个类别，我们将详细介绍一个特定的算法，并且我们将首先讨论分布方法。将讨论一个示例来比较不同的算法，并且IPython笔记本和脚本都可在我的GitHub书籍文件夹中找到，链接为[https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/)。
- en: Distribution methods
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布方法
- en: These methods assume that the data comes from a certain distribution, and the
    expectation maximization algorithm is used to find the optimal values for the
    distribution parameters. Expectation maximization and the mixture of **Gaussian**
    clustering are discussed hereafter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法假设数据来自某种分布，并且使用期望最大化算法来找到分布参数的最优值。接下来将讨论期望最大化和高斯聚类的混合。
- en: Expectation maximization
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 期望最大化
- en: 'This algorithm is used to find the maximum likelihood estimates of parameter
    distribution models that depend on hidden (unobserved) variables. Expectation
    maximization consists of iterating two steps: the **E-step** , which creates the
    **log-likelihood** function evaluated using the current values of the parameters,
    and the **M-step** , where the new parameters'' values are computed, maximizing
    the log-likelihood of the E-step.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法用于寻找依赖于隐藏（未观测）变量的参数分布模型的最大似然估计。期望最大化包括迭代两个步骤：**E步骤**，它创建使用当前参数值评估的**对数似然**函数，以及**M步骤**，在此步骤中计算新的参数值，以最大化E步骤的对数似然。
- en: 'Consider a set of *N* elements, *{x^((i)) }i = 1,…,N* , and a log-likelihood
    on the data as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含 *N* 个元素，*{x^((i)) }i = 1,…,N*，和一个数据集上的对数似然如下：
- en: '![Expectation maximization](img/Image00046.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![期望最大化](img/Image00046.jpg)'
- en: Here, *θ* represents the set of parameters and *z^((i))* are the so-called hidden
    variables.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*θ* 代表参数集，而 *z^((i))* 是所谓的隐藏变量。
- en: 'We want to find the values of the parameters that maximize the log-likelihood
    without knowing the values of the *z^((i))* (unobserved variables). Consider a
    distribution over the *z^((i))* , and *Q(z^((i)) )* , such as ![Expectation maximization](img/Image00047.jpg)
    . Therefore:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望找到最大化对数似然参数值，而不了解 *z^((i))*（未观测变量）的值。考虑一个关于 *z^((i))* 的分布，以及 *Q(z^((i))
    )*，例如![期望最大化](img/Image00047.jpg)。因此：
- en: '![Expectation maximization](img/Image00048.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![期望最大化](img/Image00048.jpg)'
- en: 'This means *Q(z^((i)) )* is the posterior distribution of the hidden variable,
    *z^((i))* , given *x^((i))* parameterized by *θ* . The expectation maximization
    algorithm comes from the use of Jensen''s inequality and it ensures that carrying
    out these two steps:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 *Q(z^((i)) )* 是给定 *x^((i))* 参数化的 *θ* 的隐藏变量 *z^((i))* 的后验分布。期望最大化算法来自Jensen不等式的使用，并确保执行以下两个步骤：
- en: '![Expectation maximization](img/Image00049.jpg)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![期望最大化](img/Image00049.jpg)'
- en: '![Expectation maximization](img/Image00050.jpg)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![期望最大化](img/Image00050.jpg)'
- en: The log-likelihood converges to the maximum, and so the associated *θ* values
    are found.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然收敛到最大值，因此找到了相关的 *θ* 值。
- en: Mixture of Gaussians
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高斯混合
- en: 'This method models the entire dataset using a mixture of Gaussian distributions.
    Therefore, the number of clusters will be given as the number of Gaussians considered
    in the mixture. Given a dataset of N elements, *{x^((i)) }i = 1,…,N* , where each
    ![Mixture of Gaussians](img/Image00051.jpg) is a vector of d-features modeled
    by a mixture of Gaussian such as the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法使用高斯分布的混合来模拟整个数据集。因此，簇的数量将作为混合中考虑的高斯数量给出。给定一个包含N个元素的数集，*{x^((i)) }i = 1,…,N*，其中每个![高斯混合](img/Image00051.jpg)是一个由高斯混合建模的d特征向量，如下所示：
- en: '![Mixture of Gaussians](img/Image00052.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![高斯混合](img/Image00052.jpg)'
- en: 'Where:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![Mixture of Gaussians](img/Image00053.jpg) is a hidden variable that represents
    the Gaussian component each *x^((i)) * is generated from'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![高斯混合](img/Image00053.jpg) 是一个隐藏变量，表示每个 *x^((i)) * 生成的来自高斯成分'
- en: '![Mixture of Gaussians](img/Image00054.jpg) represents the set of mean parameters
    of the Gaussian components'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![高斯混合](img/Image00054.jpg) 代表高斯成分的均值参数集'
- en: '![Mixture of Gaussians](img/Image00055.jpg) represents the set of variance
    parameters of the Gaussian components'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![高斯混合](img/Image00055.jpg) 代表高斯成分的方差参数集'
- en: '![Mixture of Gaussians](img/Image00056.jpg) is the mixture weight, representing
    the probability that a randomly selected *x^((i)) * was generated by the Gaussian
    component *k* , where ![Mixture of Gaussians](img/Image00057.jpg) , and ![Mixture
    of Gaussians](img/Image00058.jpg) is the set of weights'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![高斯混合](img/Image00056.jpg) 是混合权重，表示随机选择的 *x^((i)) * 被高斯成分 *k* 生成的概率，其中![高斯混合](img/Image00057.jpg)，而![高斯混合](img/Image00058.jpg)是权重集'
- en: '![Mixture of Gaussians](img/Image00059.jpg) is the Gaussian component *k* with
    parameters ![Mixture of Gaussians](img/Image00060.jpg) associated with the point
    *x^((i))*'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![高斯混合](img/Image00059.jpg) 是与点 *x^((i))* 相关的参数![高斯混合](img/Image00060.jpg)的关联高斯成分
    *k*'
- en: 'The parameters of our model are thus φ, µ and ∑. To estimate them, we can write
    down the log-likelihood of the dataset:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们模型的参数是φ，µ和∑。为了估计它们，我们可以写下数据集的对数似然：
- en: '![Mixture of Gaussians](img/Image00061.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![高斯混合](img/Image00061.jpg)'
- en: In order to find the values of the parameters, we apply the expectation maximization
    algorithm explained in the previous section where ![Mixture of Gaussians](img/Image00062.jpg)
    and ![Mixture of Gaussians](img/Image00063.jpg) .
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到参数的值，我们应用了前一小节中解释的期望最大化算法，其中包含![高斯混合](img/Image00062.jpg)和![高斯混合](img/Image00063.jpg)。
- en: 'After choosing a first guess of the parameters, we iterate the following steps
    until convergence:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择参数的第一个猜测之后，我们重复以下步骤直到收敛：
- en: '**E- step** : The weights ![Mixture of Gaussians](img/Image00064.jpg) are updated
    by following the rule obtained by applying Bayes'' theorem:![Mixture of Gaussians](img/Image00065.jpg)'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**E步骤**：通过应用贝叶斯定理得到的规则更新权重 ![高斯混合](img/Image00064.jpg)：![高斯混合](img/Image00065.jpg)'
- en: '**M-step** : The parameters are updated to the following (these formulas come
    from solving the maximization problem, which means setting the derivatives of
    the log-likelihood to zero):![Mixture of Gaussians](img/Image00066.jpg)![Mixture
    of Gaussians](img/Image00067.jpg)![Mixture of Gaussians](img/Image00068.jpg)'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**M步骤**：参数更新为以下形式（这些公式来源于解决最大化问题，即设置对数似然函数的导数为零）：![高斯混合](img/Image00066.jpg)![高斯混合](img/Image00067.jpg)![高斯混合](img/Image00068.jpg)'
- en: Note that the expectation maximization algorithm is needed because the hidden
    variables *z^((i))* are unknown. Otherwise, it would have been a supervised learning
    problem, where *z^((i))* is the label of each point of the training set (and the
    supervised algorithm used would be the Gaussian discriminant analysis). Therefore,
    this is an unsupervised algorithm and the goal is also to find *z^((i))* , that
    is, which of the *K* Gaussian components each point *x^((i))* is associated with.
    In fact, by calculating the posterior probability ![Mixture of Gaussians](img/Image00069.jpg)
    for each of the *K* classes, it is possible to assign each *x(i)* to the class
    *k* with the highest posterior probability. There are several cases in which this
    algorithm can be successfully used to cluster (label) the data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，期望最大化算法是必需的，因为隐藏变量 *z^((i))* 是未知的。否则，它将是一个监督学习问题，其中 *z^((i))* 是训练集中每个点的标签（并且使用的监督算法将是高斯判别分析）。因此，这是一个无监督算法，目标也是找到
    *z^((i))* ，即每个点 *x^((i))* 与哪个 *K* 个高斯成分相关联。实际上，通过计算每个 *K* 个类别的后验概率 ![高斯混合](img/Image00069.jpg)，可以将每个
    *x(i)* 分配给后验概率最高的类 *k*。在几种情况下，这个算法可以成功地用于聚类（标记）数据。
- en: A possible practical example is the case of a professor with student grades
    for two different classes but not labeled per class. He wants to split the grades
    into the original two classes assuming that the distribution of grades in each
    class is Gaussian. Another example solvable with the mixture of the Gaussian approach
    is to determine the country of each person based on a set of people's height values
    coming from two different countries and assuming that the distribution of height
    in each country follows Gaussian distribution.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能的实际例子是一位教授，他有两个不同班级的学生成绩，但没有按班级标记。他希望将成绩分为原始的两个班级，假设每个班级的成绩分布是高斯分布。另一个可以用高斯混合方法解决的问题是根据来自两个不同国家的一组人的身高值来确定每个人的国家，假设每个国家的身高分布遵循高斯分布。
- en: Centroid methods
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类方法
- en: This class collects all the techniques that find the centers of the clusters,
    assigning the data points to the nearest cluster center and minimizing the distances
    between the centers and the assigned points. This is an optimization problem and
    the final centers are vectors; they may not be the points in the original dataset.
    The number of clusters is a parameter to be specified a priori and the generated
    clusters tend to have similar sizes so that the border lines are not necessarily
    well defined. This optimization problem may lead to a local optimal solution,
    which means that different initialization values can result in slightly different
    clusters. The most common method is known as **k-means** ( **Lloyd's algorithm**
    ), in which the distance minimized is the **Euclidean norm** . Other techniques
    find the centers as the medians of the clusters ( **k-medians clustering** ) or
    impose the center's values to be the actual data points. Furthermore, other variations
    of these methods differ in the choice that the initial centers are defined ( **k-means++**
    or **fuzzy c-means** ).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此类收集了所有寻找聚类中心的技术，将数据点分配给最近的聚类中心，并最小化中心与分配给点的距离。这是一个优化问题，最终的中心是向量；它们可能不是原始数据集中的点。聚类数是一个需要事先指定的参数，生成的聚类倾向于具有相似的大小，因此边界线不一定定义良好。此优化问题可能导致局部最优解，这意味着不同的初始化值可能导致略微不同的聚类。最常见的方法是称为
    **k-means**（**Lloyd 算法**），其中最小化的距离是 **欧几里得范数**。其他技术将中心寻找为聚类的中位数（**k-medians 聚类**）或强制中心值是实际数据点。此外，这些方法的变体在定义初始中心的选择上有所不同（**k-means++**
    或 **模糊 c-均值**）。
- en: k-means
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-means
- en: 'This algorithm tries to find the center of each cluster as the mean of all
    the members that minimize the distance between the center and the assigned points
    themselves. It can be associated with the k-nearest-neighbor algorithm in classification
    problems, and the resulting set of clusters can be represented as a **Voronoi
    diagram** (a method of partitioning the space in regions based on the distance
    from a set of points, in this case, the clusters'' centers). Consider the usual
    dataset, ![k-means](img/Image00070.jpg) . The algorithm prescribes to choose a
    number of centers *K* , assign the initial mean cluster centers ![k-means](img/Image00071.jpg)
    to random values, and then iterate the following steps until convergence:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法试图将每个聚类的中心作为所有成员的均值，以最小化中心与分配给点的距离。它可以与分类问题中的 k-最近邻算法相关联，并且生成的聚类集可以表示为 **Voronoi
    图**（一种基于从一组点（在这种情况下，为聚类中心）的距离来划分空间区域的方法）。考虑常用的数据集，![k-means](img/Image00070.jpg)。算法规定选择一个中心数
    *K*，将初始均值聚类中心![k-means](img/Image00071.jpg)分配给随机值，然后迭代以下步骤直到收敛：
- en: 'For each data point *i* , calculate the Euclidean square distances between
    each point *i* and each center *j* and find the center index *d[i]* , which corresponds
    to the minimum of these distances: ![k-means](img/Image00072.jpg) .'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个数据点 *i*，计算每个点 *i* 与每个中心 *j* 之间的欧几里得平方距离，并找到对应于这些距离最小值的中心索引 *d[i]*：![k-means](img/Image00072.jpg)。
- en: For each center *j* , recalculate its mean value as the mean of the points that
    have *d_i j* equal to *j* (that is, points belonging to the cluster with mean
    ![k-means](img/Image00073.jpg) ):![k-means](img/Image00074.jpg)
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个中心 *j*，重新计算其均值，作为具有 *d_ij* 等于 *j* 的点的均值（即属于均值![k-means](img/Image00073.jpg)的聚类中的点）：![k-means](img/Image00074.jpg)。
- en: 'It is possible to show that this algorithm converges with respect to the function
    given by the following function:'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以证明，此算法相对于以下函数收敛：
- en: '![k-means](img/Image00075.jpg)'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![k-means](img/Image00075.jpg)'
- en: It decreases monotonically with the number of iterations. Since *F* is a nonconvex
    function, it is not guaranteed that the final minimum will be the global minimum.
    In order to avoid the problem of a clustering result associated with the local
    minima, the k-means algorithm is usually run multiple times with different random
    initial center's means. Then the result associated with the lower *F* value is
    chosen as the optimal clustering solution.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 随着迭代次数的增加，它单调递减。由于 *F* 是一个非凸函数，不能保证最终的最小值是全球最小值。为了避免与局部最小值相关的聚类结果问题，k-means
    算法通常多次运行，每次使用不同的随机初始中心均值。然后选择与较低 *F* 值相关的结果作为最优聚类解决方案。
- en: Density methods
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 密度方法
- en: These methods are based on the idea that sparse areas have to be considered
    borders (or noise) and high-density zones should be related to the cluster's centers.
    The common method is called **density-based spatial clustering of applications
    with noise** ( **DBSCAN** ), which defines the connection between two points through
    a certain distance threshold (for this reason, it is similar to hierarchical algorithms;
    see [Chapter 3](text00024.html#page "Chapter 3. Supervised Machine Learning")
    , *Supervised Machine Learning* ). Two points are considered linked (belonging
    to the same cluster) only if a certain density criterion is satisfied—the number
    of neighboring points has to be higher than a threshold value within a certain
    radius. Another popular method is mean-shift, in which each data point is assigned
    to the cluster that has the highest density in its neighborhood. Due to the time-consuming
    calculations of the density through a kernel density estimation, mean-shift is
    usually slower than DBSCAN or centroid methods. The main advantages of this class
    of algorithms are the ability to define clusters with arbitrary shapes and the
    ability to determine the best number of clusters instead of setting this number
    a priori as a parameter, making these methods suitable to cluster datasets in
    which it is not known.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法基于这样的想法，稀疏区域必须被认为是边界（或噪声），而高密度区域应该与聚类的中心相关联。常见的方法被称为**基于密度的空间聚类应用噪声**（**DBSCAN**），它通过一定的距离阈值定义两点之间的连接（因此，它与层次算法类似；参见[第3章](text00024.html#page
    "第3章. 监督机器学习")，*监督机器学习*）。只有当满足一定的密度标准时，两点才被认为是相连的（属于同一个聚类）——在某个半径内，邻近点的数量必须高于一个阈值。另一种流行的方法是均值漂移，其中每个数据点被分配到其邻域密度最高的聚类。由于通过核密度估计计算密度耗时，均值漂移通常比DBSCAN或质心方法慢。这类算法的主要优点是能够定义任意形状的聚类，并且能够确定最佳聚类数量，而不是预先将此数量作为参数设置，这使得这些方法适合于聚类未知数量的数据集。
- en: Mean – shift
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 均值漂移
- en: 'Mean-shift is nonparametric algorithm that finds the positions of the local
    maxima in a density kernel function defined on a dataset. The local maxima found
    can be considered the centers of clusters in a dataset ![Mean – shift](img/Image00076.jpg)
    , and the number of maxima is the number of clusters. In order to be applied as
    a clustering algorithm, each point ![Mean – shift](img/Image00077.jpg) has to
    be associated with the density of its neighborhood:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 均值漂移是一种非参数算法，它在一个数据集上定义的密度核函数中找到局部最大值的位置。找到的局部最大值可以被认为是数据集 ![均值漂移](img/Image00076.jpg)
    中聚类的中心，最大值的数量是聚类数量。为了作为聚类算法应用，每个点 ![均值漂移](img/Image00077.jpg) 必须与其邻域的密度相关联：
- en: '![Mean – shift](img/Image00078.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![均值漂移](img/Image00078.jpg)'
- en: 'Here, *h* is the so-called bandwidth; it estimates the radius of the neighborhood
    in which the points affect the density value *f(x^((l)) )* (that is, the other
    points have negligible effect on ![Mean – shift](img/Image00079.jpg) ). *K* is
    the kernel function that satisfies these conditions:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*h* 是所谓的带宽；它估计了影响密度值 *f(x^((l)))* 的点的邻域半径（即，其他点对 ![均值漂移](img/Image00079.jpg)
    的影响可以忽略不计）。*K* 是满足这些条件的核函数：
- en: '![Mean – shift](img/Image00080.jpg)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![均值漂移](img/Image00080.jpg)'
- en: '![Mean – shift](img/Image00081.jpg)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![均值漂移](img/Image00081.jpg)'
- en: 'Typical examples of *K(x^((i)) )* are the following functions:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*K(x^((i)))* 的典型例子包括以下函数：'
- en: '![Mean – shift](img/Image00082.jpg) : Gaussian kernel'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![均值漂移](img/Image00082.jpg) : 高斯核'
- en: '![Mean – shift](img/Image00083.jpg) : Epanechnikov kernel'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![均值漂移](img/Image00083.jpg) : Epanechnikov核'
- en: 'The mean-shift algorithm imposes the maximization of *f(x^((l)) )* , which
    translates into the mathematical equation (remember that in function analysis,
    the maximum is found by imposing the derivative to *0* ):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 均值漂移算法强制最大化 *f(x^((l)))*，这转化为数学方程（记住在函数分析中，最大值是通过将导数设为 *0* 来找到的）：
- en: '![Mean – shift](img/Image00084.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![均值漂移](img/Image00084.jpg)'
- en: Here, *K'* is derivative of the kernel density function *K* .
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*K'* 是核密度函数 *K* 的导数。
- en: 'Therefore, to find the local maxima position associated with the feature vector
    *x^((l))* , the following iterative equation can be computed:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了找到与特征向量 *x^((l))* 相关的局部最大值位置，可以计算以下迭代方程：
- en: '![Mean – shift](img/Image00085.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![均值漂移](img/Image00085.jpg)'
- en: Here, ![Mean – shift](img/Image00086.jpg) is called the mean-shift vector. The
    algorithm will converge when at iteration *t=a* , the condition ![Mean – shift](img/Image00087.jpg)
    is satisfied.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![均值漂移](img/Image00086.jpg) 被称为均值漂移向量。当迭代 *t=a* 时，算法将收敛，满足条件 ![均值漂移](img/Image00087.jpg)。
- en: 'Supported by the equation, we can now explain the algorithm with the help of
    the following figure. At the first iteration *t=0* , the original points ![Mean
    – shift](img/Image00088.jpg) (red) are spread on the data space, the mean shift
    vector ![Mean – shift](img/Image00089.jpg) is calculated, and the same points
    are marked with a cross ( *x* ) to track their evolution with the algorithm. At
    iteration *1* , the dataset will be obtained using the aforementioned equation,
    and the resulting points ![Mean – shift](img/Image00090.jpg) are shown in the
    following figure with the ( *+* ) symbol:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程的支持下，我们可以借助以下图解来解释算法。在第一次迭代 *t=0* 时，原始点 ![均值漂移](img/Image00088.jpg)（红色）散布在数据空间中，计算了均值漂移向量
    ![均值漂移](img/Image00089.jpg)，并且用交叉（ *x* ）标记相同的点以跟踪它们随算法的演变。在迭代 *1* 时，将使用上述方程获得数据集，并在以下图中用（
    *+* ）符号显示结果点 ![均值漂移](img/Image00090.jpg)：
- en: '![Mean – shift](img/Image00091.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![均值漂移](img/Image00091.jpg)'
- en: Sketch of the mean-shift evolution through iterations
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 均值漂移迭代过程中的演变草图
- en: In the preceding figure, at iteration *0* the original points are shown in red
    (cross), at iteration *1* and *K* the sample points (symbols *+* and *** respectively)
    move towards the local density maxima indicated by blue squares.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个图中，迭代 *0* 时原始点用红色（交叉）表示，迭代 *1* 和 *K* 时样本点（符号 *+* 和 *** 分别）向由蓝色方块指示的局部密度最大值移动。
- en: Again, at iteration *K* , the new data points ![Mean – shift](img/Image00092.jpg)
    are computed and they are shown with the *** symbol in the preceding figure. The
    density function values ![Mean – shift](img/Image00093.jpg) associated with ![Mean
    – shift](img/Image00094.jpg) are larger than the values in the previous iterations
    since the algorithm aims to maximize them. The original dataset is now clearly
    associated with points ![Mean – shift](img/Image00092.jpg) , and they converge
    to the locations plotted in blue squares in the preceding figure. The feature
    vectors ![Mean – shift](img/Image00088.jpg) are now collapsing to two different
    local maxima, which represent the centers of the two clusters.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，在迭代 *K* 时，计算了新的数据点 ![均值漂移](img/Image00092.jpg)，并在前一个图中用 *** 符号表示。与之前迭代相比，与
    ![均值漂移](img/Image00094.jpg) 相关的密度函数值 ![均值漂移](img/Image00093.jpg) 更大，因为算法旨在最大化这些值。现在原始数据集与点
    ![均值漂移](img/Image00092.jpg) 明确关联，并且它们收敛到前一个图中用蓝色方块标记的位置。特征向量 ![均值漂移](img/Image00088.jpg)
    现在正塌缩到两个不同的局部最大值，这代表了两个簇的中心。
- en: In order to properly use the method, some considerations are necessary.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确使用该方法，需要考虑一些因素。
- en: The only parameter required, the bandwidth *h* , needs to be tuned cleverly
    to achieve good results. In fact, too low value of *h* may result in a large number
    of clusters, while a large value of *h* may merge multiple distinct clusters.
    Note also that if the number *d* of feature vector dimensions is large, the mean-shift
    method may lead to poor results. This is because in a very-high-dimensional space,
    the number of local maxima is accordingly large and the iterative equation can
    easily converge too soon.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一需要的参数，带宽 *h*，需要巧妙地调整以达到良好的结果。实际上，*h* 的值太低可能会导致大量簇的出现，而 *h* 的值太大可能会合并多个不同的簇。注意，如果特征向量维度数
    *d* 很大，均值漂移方法可能会导致结果不佳。这是因为在一个非常高维的空间中，局部最大值的数量相应很大，迭代方程很容易过早收敛。
- en: Hierarchical methods
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次方法
- en: 'The class of hierarchical methods, also called connectivity-based clustering,
    forms clusters by collecting elements on a similarity criteria based on a distance
    metric: close elements gather in the same partition while far elements are separated
    into different clusters. This category of algorithms is divided in two types:
    **divisive clustering** and **agglomerative clustering** . The divisive approach
    starts by assigning the entire dataset to a cluster, which is then divided in
    two less similar (distant) clusters. Each partition is further divided until each
    data point is itself a cluster. The agglomerative method, which is the most often
    employed method, starts from the data points, each of them representing a cluster.
    Then these clusters are merged by similarity until a single cluster containing
    all the data points remains. These methods are called **hierarchical** because
    both categories create a hierarchy of clusters iteratively, as the following figure
    shows. This hierarchical representation is called a **dendrogram** . On the horizontal
    axis, there are the elements of the dataset, and on the vertical axis, the distance
    values are plotted. Each horizontal line represents a cluster and the vertical
    axis indicates which element/cluster forms the related cluster:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类方法，也称为基于连接性的聚类，通过基于距离度量的相似性标准收集元素来形成簇：接近的元素聚集在同一个分区中，而远离的元素被分离到不同的簇中。这类算法分为两种类型：**划分聚类**和**聚类聚类**。划分方法首先将整个数据集分配到一个簇，然后将其划分为两个不太相似（距离较远）的簇。每个分区进一步划分，直到每个数据点本身就是一个簇。聚类方法，这是最常用的方法，从数据点开始，每个数据点代表一个簇。然后通过相似性将这些簇合并，直到只剩下一个包含所有数据点的单一簇。这些方法被称为**层次聚类**，因为这两个类别都是通过迭代创建簇的层次结构，如图所示。这种层次表示称为**树状图**。在水平轴上，有数据集的元素，在垂直轴上绘制距离值。每条水平线代表一个簇，垂直轴指示哪个元素/簇形成了相关的簇：
- en: '![Hierarchical methods](img/Image00095.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![层次聚类方法](img/Image00095.jpg)'
- en: In the preceding figure, agglomerative clustering starts from many clusters
    as dataset points and ends up with a single cluster that contains the entire dataset.
    Vice versa, the divisive method starts from a single cluster and finishes when
    all clusters contain a single data point each.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，层次聚类从许多簇（数据点）开始，最终形成一个包含整个数据集的单一簇。相反，划分方法从单一簇开始，直到每个簇都包含一个单独的数据点时结束。
- en: The final clusters are then formed by applying criteria to stop the agglomeration/division
    strategy. The distance criteria sets the maximum distance above which two clusters
    are too far away to be merged, and the *number of clusters* criteria sets the
    maximum number of clusters to stop the hierarchy from continuing to merge or split
    the partitions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用标准来停止聚类/划分策略，最终形成簇。距离标准设定了两个簇合并的最大距离，超过这个距离的两个簇被认为太远而不能合并；簇数量标准设定了簇的最大数量，以防止层次结构继续合并或分割分区。
- en: 'An example of agglomeration is given by the following algorithm:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下算法给出了聚类的示例：
- en: Assign each element *i* of the dataset ![Hierarchical methods](img/Image00096.jpg)
    to a different cluster.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集 ![层次聚类方法](img/Image00096.jpg) 中的每个元素 *i* 分配到不同的簇。
- en: Calculate the distances between each pair of clusters and merge the closest
    pair into a single cluster, reducing the total number of clusters by *1* .
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每对簇之间的距离，并将最近的这对簇合并成一个单一簇，从而减少簇的总数 *1* 。
- en: Calculate the distances of the new cluster from the others.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算新簇与其他簇的距离。
- en: Repeat steps 2 and 3 until only a single cluster remains with all *N* elements.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2 和 3，直到只剩下一个包含所有 *N* 个元素的单一簇。
- en: 'Since the distance *d(C1,C2)* between two clusters *C1* , *C2* , is computed
    by definition between two points ![Hierarchical methods](img/Image00097.jpg) and
    each cluster contains multiple points, a criteria to decide which elements have
    to be considered to calculate the distance is necessary (linkage criteria). The
    common linkage criteria of two clusters *C1* and *C2* are as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于两个簇 *C1* 和 *C2* 之间的距离 *d(C1,C2)* 是根据定义在两个点 ![层次聚类方法](img/Image00097.jpg) 之间计算的，并且每个簇包含多个点，因此需要一项标准来决定在计算距离时必须考虑哪些元素（链接标准）。两个簇
    *C1* 和 *C2* 的常见链接标准如下：
- en: '**Single linkage** : The minimum distance among the distances between any element
    of *C1* and any element of *C2* is given by the following:![Hierarchical methods](img/Image00098.jpg)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单链接**：任何元素属于*C1*与任何元素属于*C2*之间的距离的最小值如下所示：![层次方法](img/Image00098.jpg)'
- en: '**Complete linkage** : The maximum distance among the distances between any
    element of *C1* and any element of *C2* is given by the following:![Hierarchical
    methods](img/Image00099.jpg)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全链接**：任何元素属于*C1*与任何元素属于*C2*之间的距离的最大值如下所示：![层次方法](img/Image00099.jpg)'
- en: '**Unweighted pair group method with arithmetic mean (UPGMA) or average linkage**
    : The average distance among the distances between any element of *C1* and any
    element of *C2* is ![Hierarchical methods](img/Image00100.jpg) , where ![Hierarchical
    methods](img/Image00101.jpg) are the numbers of elements of *C1* and *C2* , respectively.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无权成对组平均法（UPGMA）或平均链接**：任何元素属于*C1*与任何元素属于*C2*之间的距离的平均值如下所示：![层次方法](img/Image00100.jpg)，其中![层次方法](img/Image00101.jpg)分别是*C1*和*C2*的元素数量。'
- en: '**Ward algorithm** : This merges partitions that do not increase a certain
    measure of heterogeneity. It aims to join two clusters *C1* and *C2* that have
    the minimum increase of a variation measure, called the merging cost ![Hierarchical
    methods](img/Image00102.jpg) , due to their combination. The distance in this
    case is replaced by the merging cost, which is given by the following formula:![Hierarchical
    methods](img/Image00103.jpg)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ward算法**：这种算法合并不会增加某种异质性度量的分区。它的目标是连接两个聚类*C1*和*C2*，这两个聚类由于组合而具有最小增加的变异度量，称为合并成本![层次方法](img/Image00102.jpg)。在这种情况下，距离被替换为合并成本，其公式如下所示：![层次方法](img/Image00103.jpg)'
- en: Here, ![Hierarchical methods](img/Image00101.jpg) are the numbers of elements
    of C1 and C2, respectively.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，![层次方法](img/Image00101.jpg)分别是C1和C2的元素数量。
- en: 'There are different metrics *d(c1,c2)* that can be chosen to implement a hierarchical
    algorithm. The most common is the Euclidean distance:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的度量*d(c1,c2)*可以选择来实现层次算法。最常见的是欧几里得距离：
- en: '![Hierarchical methods](img/Image00104.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![层次方法](img/Image00104.jpg)'
- en: Note that this class of method is not particularly time-efficient, so it is
    not suitable for clustering large datasets. It is also not very robust towards
    erroneously clustered data points (outliers), which may lead to incorrect merging
    of clusters.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这类方法在时间效率上并不特别高，因此不适合聚类大型数据集。它对错误聚类的数据点（异常值）也不够鲁棒，这可能导致聚类合并错误。
- en: Training and comparison of the clustering methods
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类方法的训练和比较
- en: To compare the clustering methods just presented, we need to generate a dataset.
    We choose the two dataset classes given by the two two-dimensional multivariate
    normal distributions with means and covariance equal to ![Training and comparison
    of the clustering methods](img/Image00105.jpg) and ![Training and comparison of
    the clustering methods](img/Image00106.jpg) , respectively.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较刚刚介绍的聚类方法，我们需要生成一个数据集。我们选择了由两个二维多元正态分布给出的两个数据集类别，其均值和协方差分别等于![聚类方法的训练和比较](img/Image00105.jpg)和![聚类方法的训练和比较](img/Image00106.jpg)。
- en: 'The data points are generated using the NumPy library and plotted with matplotlib:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点使用NumPy库生成，并用matplotlib进行绘图：
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A normally distributed noise has been added to both classes to make the example
    more realistic. The result is shown in the following figure:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使示例更真实，已向两个类别添加了正态分布的噪声。结果如下所示：
- en: '![Training and comparison of the clustering methods](img/Image00107.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![聚类方法的训练和比较](img/Image00107.jpg)'
- en: Two multivariate normal classes with noise
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 带噪声的两个多元正态类别
- en: 'The clustering methods have been implemented using the `sklearn` and `scipy`
    libraries and again plotted with matplotlib:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类方法使用`sklearn`和`scipy`库实现，并再次用matplotlib进行绘图：
- en: '[PRE1]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The k-means function and Gaussian mixture model have a specified number of
    clusters (`n_clusters =2,n_components=2` ), while the mean-shift algorithm has
    the bandwidth value `bandwidth=7` . The hierarchical algorithm is implemented
    using the ward linkage and the maximum (ward) distance, `max_d` , is set to `110`
    to stop the hierarchy. The `fcluster` function is used to obtain the predicted
    class for each data point. The predicted classes for the k-means and the mean-shift
    method are accessed using the `labels_` attribute, while the Gaussian mixture
    model needs to employ the `predict` function. The k -means, mean-shift, and Gaussian
    mixture methods have been trained using the `fit` function, while the hierarchical
    method has been trained using the `linkage` function. The output of the preceding
    code is shown in the following figure:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 函数和高斯混合模型具有指定的聚类数（`n_clusters =2,n_components=2`），而 mean-shift 算法具有带宽值
    `bandwidth=7`。层次算法使用 ward 连接和最大（ward）距离 `max_d` 实现，将 `max_d` 设置为 `110` 以停止层次结构。使用
    `fcluster` 函数获取每个数据点的预测类别。k-means 和 mean-shift 方法的预测类别通过 `labels_` 属性访问，而高斯混合模型需要使用
    `predict` 函数。k-means、mean-shift 和高斯混合方法使用 `fit` 函数进行训练，而层次方法使用 `linkage` 函数进行训练。前述代码的输出显示在以下图中：
- en: '![Training and comparison of the clustering methods](img/Image00108.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![聚类方法的训练和比较](img/Image00108.jpg)'
- en: IClustering of the two multivariate classes using k-means, mean-shift, Gaussian
    mixture model, and hierarchical ward method
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 k-means、mean-shift、高斯混合模型和层次 ward 方法对两个多元类别进行聚类
- en: 'The mean-shift and hierarchical methods show two classes, so the choice of
    parameters (bandwidth and maximum distance) is appropriate. Note that the maximum
    distance value for the hierarchical method has been chosen looking at the dendrogram
    (the following figure) generated by the following code:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: mean-shift 和层次方法显示了两个类别，因此参数（带宽和最大距离）的选择是合适的。注意，层次方法的最大距离值是根据以下代码生成的树状图（以下图）选择的：
- en: '[PRE2]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `truncate_mode=''lastp''` flag allows us to specify the number of last
    merges to show in the plot (in this case, `p=12` ). The preceding figure clearly
    shows that when the distance is between `100` and `135` , there are only two clusters
    left:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`truncate_mode=''lastp''` 标志允许我们指定在图中显示的最后合并的数量（在这种情况下，`p=12`）。前述图清楚地显示，当距离在
    `100` 和 `135` 之间时，只剩下两个聚类：'
- en: '![Training and comparison of the clustering methods](img/Image00109.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![聚类方法的训练和比较](img/Image00109.jpg)'
- en: IHierarchical clustering dendrogram for the last 12 merges
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: IHierarchical clustering dendrogram for the last 12 merges
- en: In the preceding figure on the horizontal axis, the number of data points belonging
    to each cluster before the last *12* merges is shown in brackets ().
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述图的水平轴上，显示在最后 *12* 合并之前属于每个聚类的数据点数量，用括号（）表示。
- en: 'Apart from the Gaussian mixture model, the other three algorithms misclassify
    some data points, especially k-means and hierarchical methods. This result proves
    that the Gaussian mixture model is the most robust method, as expected, since
    the dataset comes from the same distribution assumption. To evaluate the quality
    of the clustering, scikit-learn provides methods to quantify the correctness of
    the partitions: v-measure, completeness, and homogeneity. These methods require
    the real value of the class for each data point, so they are referred to as external
    validation procedures. This is because they require additional information not
    used while applying the clustering methods. Homogeneity, *h* , is a score between
    *0* and *1* that measures whether each cluster contains only elements of a single
    class. Completeness, *c* , quantifies with a score between *0* and *1* whether
    all the elements of a class are assigned to the same cluster. Consider a clustering
    that assigns each data point to a different cluster. In this way, each cluster
    will contains only one class and the homogeneity is *1* , but unless each class
    contains only one element, the completeness is very low because the class elements
    are spread around many clusters. Vice versa, if a clustering results in assigning
    all the data points of multiple classes to the same cluster, certainly the completeness
    is *1* but homogeneity is poor. These two scores have a similar formula, as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了高斯混合模型之外，其他三种算法错误地将一些数据点分类，尤其是k-means和层次方法。这一结果证明，正如预期的那样，高斯混合模型是最鲁棒的方法，因为数据集来自相同的分布假设。为了评估聚类的质量，scikit-learn提供了量化分区正确性的方法：v-measure、完整性和同质性。这些方法需要每个数据点的真实类别值，因此它们被称为外部验证过程。这是因为它们需要应用聚类方法时未使用的额外信息。同质性，*h*，是一个介于*0*和*1*之间的分数，用于衡量每个簇是否只包含单个类别的元素。完整性，*c*，用一个介于*0*和*1*之间的分数量化，是否所有类别的元素都被分配到同一个簇中。考虑一个将每个数据点分配到不同簇的聚类。这样，每个簇将只包含一个类别的元素，同质性为*1*，但除非每个类别只包含一个元素，否则完整性非常低，因为类别元素分布在许多簇中。反之，如果一个聚类导致将多个类别的所有数据点分配到同一个簇，那么完整性一定是*1*，但同质性较差。这两个分数具有类似的公式，如下所示：
- en: '![Training and comparison of the clustering methods](img/Image00110.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![聚类方法的训练和比较](img/Image00110.jpg)'
- en: 'Here:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '![Training and comparison of the clustering methods](img/Image00111.jpg) is
    the conditional entropy of the classes *C^l * , given the cluster assignments
    ![Training and comparison of the clustering methods](img/Image00112.jpg)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![聚类方法的训练和比较](img/Image00111.jpg)是给定簇分配的类别 *C^l * 的条件熵'
- en: '![Training and comparison of the clustering methods](img/Image00113.jpg) is
    the conditional entropy of the clusters, given the class membership ![Training
    and comparison of the clustering methods](img/Image00114.jpg)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![聚类方法的训练和比较](img/Image00113.jpg)是给定类别成员资格的簇的条件熵 ![聚类方法的训练和比较](img/Image00114.jpg)'
- en: '*H(C[l] )* is the entropy of the classes: ![Training and comparison of the
    clustering methods](img/Image00115.jpg)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*H(C[l] )* 是类别的熵：![聚类方法的训练和比较](img/Image00115.jpg)'
- en: '*H(C)* is the entropy of the clusters: ![Training and comparison of the clustering
    methods](img/Image00116.jpg)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*H(C)* 是簇的熵：![聚类方法的训练和比较](img/Image00116.jpg)'
- en: '*N[pc]* is the number of elements of class *p* in cluster *c* , *N[p]* is the
    number of elements of class *p* , and *N[c]* is the number of elements of cluster
    *c*'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N[pc]* 是簇 *c* 中类别 *p* 的元素数量，*N[p]* 是类别 *p* 的元素数量，*N[c]* 是簇 *c* 的元素数量'
- en: 'The v-measure is simply the harmonic mean of the homogeneity and the completeness:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: v-measure是同质性和完整性的调和平均值：
- en: '![Training and comparison of the clustering methods](img/Image00117.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![聚类方法的训练和比较](img/Image00117.jpg)'
- en: 'These measures require the true labels to evaluate the quality of the clustering,
    and often this is not real-case scenario. Another method only employs data from
    the clustering itself, called **silhouette** , which calculates the similarities
    of each data point with the members of the cluster it belongs to and with the
    members of the other clusters. If on average each point is more similar to the
    points of its own cluster than the rest of the points, then the clusters are well
    defined and the score is close to *1* (it is close to *-1* , otherwise). For the
    formula, consider each point *i* and the following quantities:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '*d[s] (i)* is the average distance of the point *i* from the points of the
    same cluster'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*d[rest] (i)* is the minimum distance of point *i* from the rest of the points
    in all other clusters'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The silhouette can be defined as
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![Training and comparison of the clustering methods](img/Image00118.jpg) ,
    and the silhouette score is the average of *s(i)* for all data points.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'The four clustering algorithms we covered are associated with the following
    values of these four measures calculated using `sklearn` (scikit-learn):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As expected from the analysis of the preceding figure, the Gaussian mixture
    model has the best values of the homogeneity, completeness, and v-measure measures
    (close to *1* ); mean-shift has reasonable values (around *0.5* ); while k-means
    and hierarchical methods result in poor values (around *0.3* ). The silhouette
    score instead is decent for all the methods (between *0.35* and *0.41* ), meaning
    that the clusters are reasonably well defined.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Dimensionality reduction** , which is also called feature extraction, refers
    to the operation to transform a data space given by a large number of dimensions
    to a subspace of fewer dimensions. The resulting subspace should contain only
    the most relevant information of the initial data, and the techniques to perform
    this operation are categorized as linear or non-linear. Dimensionality reduction
    is a broad class of techniques that is useful for extracting the most relevant
    information from a large dataset, decreasing its complexity but keeping the relevant
    information.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The most famous algorithm, **Principal Component Analysis** ( **PCA** ), is
    a linear mapping of the original data into a subspace of uncorrelated dimensions,
    and it will be discussed hereafter. The code shown in this paragraph is available
    in IPython notebook and script versions at the author's GitHub book folder at
    [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/)
    .
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA)
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The principal component analysis algorithm aims to identify the subspace where
    the relevant information of a dataset lies. In fact, since the data points can
    be correlated in some data dimensions, PCA will find the few uncorrelated dimensions
    in which the data varies. For example, a car trajectory can be described by a
    series of variables such as velocity in km/h or m/s, position in latitude and
    longitude, position in meters from a chosen point, and position in miles from
    a chosen point. Clearly, the dimensions can be reduced because the velocity variables
    and the position variables give the same information (correlated variables), so
    the relevant subspace can be composed of two uncorrelated dimensions (a velocity
    variable and a position variable). PCA finds not only the uncorrelated set of
    variables but also the dimensions where the variance is maximized. That is, between
    the velocity in km/h and miles/h, the algorithm will select the variable with
    the highest variance, which is trivially represented by the line between the two
    axes given by the function *velocity[km/h]=3.6*velocity[m/s]* (typically closer
    to the km/h axis because *1 km/h = 3.6 m/s* and the velocity projections are more
    spread along the km/h axis than the m/s axis):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析算法旨在识别数据集相关信息的子空间。实际上，由于数据点在某些数据维度上可能存在相关性，PCA 将找到数据变化的少数不相关维度。例如，汽车轨迹可以用一系列变量来描述，如速度（km/h
    或 m/s）、经纬度位置、从选定点的米数位置以及从选定点的英里位置。显然，维度可以减少，因为速度变量和位置变量提供了相同的信息（相关变量），因此相关子空间可以由两个不相关维度组成（一个速度变量和一个位置变量）。PCA
    不仅找到不相关的一组变量，还找到方差最大的维度。也就是说，在 km/h 和英里/小时的速度之间，算法将选择具有最高方差的变量，这可以通过函数 *速度[km/h]=3.6*速度[m/s]*
    之间的线简单地表示（通常更接近 km/h 轴，因为 *1 km/h = 3.6 m/s* ，速度投影在 km/h 轴上比在 m/s 轴上更分散）：
- en: '![Principal Component Analysis (PCA)](img/Image00119.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析 (PCA)](img/Image00119.jpg)'
- en: The linear function between the velocity in m/s and km/h
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: m/s 和 km/h 之间的线性函数
- en: The preceding figure represents the linear function between the velocity in
    m/s and km/h. The projections of the points along the km/h axis have a large variance,
    while the projections on the m/s axis have a lower variance. The variance along
    the linear function *velocity[km/h]=3.6*velocity[m/s]* is larger than both axes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表示了 m/s 和 km/h 之间的线性函数。沿 km/h 轴的点投影具有较大的方差，而沿 m/s 轴的投影具有较低的方差。线性函数 *速度[km/h]=3.6*速度[m/s]*
    沿着轴的方差大于两个轴。
- en: 'Now we are ready to discuss the method and its features in detail. It is possible
    to show that finding the uncorrelated dimensions in which the variance is maximized
    is equivalent to computing the following steps. As usual, we consider the feature
    vectors ![Principal Component Analysis (PCA)](img/Image00096.jpg) :'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们准备详细讨论该方法及其特性。可以证明，找到方差最大的不相关维度等同于计算以下步骤。像往常一样，我们考虑特征向量 ![主成分分析 (PCA)](img/Image00096.jpg)
    :'
- en: 'The average of the dataset: ![Principal Component Analysis (PCA)](img/Image00120.jpg)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的平均值：![主成分分析 (PCA)](img/Image00120.jpg)
- en: 'The mean shifted dataset: ![Principal Component Analysis (PCA)](img/Image00121.jpg)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值平移后的数据集：![主成分分析 (PCA)](img/Image00121.jpg)
- en: The rescaled dataset, in which each feature vector component ![Principal Component
    Analysis (PCA)](img/Image00122.jpg) has been divided by the standard deviation,
    ![Principal Component Analysis (PCA)](img/Image00123.jpg) , where ![Principal
    Component Analysis (PCA)](img/Image00124.jpg)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整比例后的数据集，其中每个特征向量分量 ![主成分分析 (PCA)](img/Image00122.jpg) 已除以标准差 ![主成分分析 (PCA)](img/Image00123.jpg)
    ，其中 ![主成分分析 (PCA)](img/Image00124.jpg)
- en: 'The sample covariance matrix: ![Principal Component Analysis (PCA)](img/Image00125.jpg)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本协方差矩阵：![主成分分析 (PCA)](img/Image00125.jpg)
- en: The *k* largest eigenvalues, ![Principal Component Analysis (PCA)](img/Image00126.jpg)
    , and their associated eigenvectors, ![Principal Component Analysis (PCA)](img/Image00127.jpg)
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k* 个最大的特征值 ![主成分分析 (PCA)](img/Image00126.jpg) 及其相关的特征向量 ![主成分分析 (PCA)](img/Image00127.jpg)'
- en: Projected feature vectors on the subspace of the *k* eigenvectors ![Principal
    Component Analysis (PCA)](img/Image00128.jpg) , where ![Principal Component Analysis
    (PCA)](img/Image00129.jpg) is the matrix of the eigenvectors with *N* rows and
    *k* columns
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投影到 *k* 个特征向量子空间上的特征向量 ![主成分分析 (PCA)](img/Image00128.jpg) ，其中 ![主成分分析 (PCA)](img/Image00129.jpg)
    是具有 *N* 行和 *k* 列的特征向量矩阵
- en: The final feature's vectors (principal components), ![Principal Component Analysis
    (PCA)](img/Image00130.jpg) lie on a subspace *R^k * , which still retain the maximum
    variance (and information) of the original vectors.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的特征向量（主成分），![主成分分析（PCA）](img/Image00130.jpg)位于子空间*R^k*上，仍然保留了原始向量的最大方差（和信息）。
- en: Note that this technique is particularly useful when dealing with high-dimensional
    datasets, such as in face recognition. In this field, an input image has to be
    compared with a database of other images to find the right person. The PCA application
    is called **Eigenfaces** , and it exploits the fact that a large number of pixels
    (variables) in each image are correlated. For instance, the background pixels
    are all correlated (the same), so a dimensionality reduction can be applied, and
    comparing images in a smaller subspace is a faster approach that gives accurate
    results. An example of implementation of Eigenfaces can be found on the author's
    GitHub profile at [https://github.com/ai2010/eigenfaces](https://github.com/ai2010/eigenfaces)
    .
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种技术在处理高维数据集时特别有用，例如在人脸识别中。在这个领域，必须将输入图像与图像数据库中的其他图像进行比较，以找到正确的人。PCA的应用被称为**主成分面**，它利用了这样一个事实：每张图像中的大量像素（变量）是相关的。例如，背景像素都是相关的（相同的），因此可以应用降维，并在较小的子空间中比较图像是一种更快的方法，可以给出准确的结果。Eigenfaces的实现示例可以在作者的GitHub个人资料[https://github.com/ai2010/eigenfaces](https://github.com/ai2010/eigenfaces)上找到。
- en: PCA example
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PCA示例
- en: 'As an example of the usage of PCA as well as the NumPy library discussed in
    [Chapter 1](text00015.html#ch01 "Chapter 1. Introduction to Practical Machine
    Learning Using Python") , *Introduction to Practical Machine Learning using Python*
    we are going to determine the principal component of a two-dimensional dataset
    distributed along the line *y=2x* , with random (normally distributed) noise.
    The dataset and the corresponding figure (see the following figure) have been
    generated using the following code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 作为PCA的使用示例以及[第1章](text00015.html#ch01 "第1章。使用Python的实用机器学习介绍")中讨论的NumPy库的示例，*使用Python的实用机器学习介绍*，我们将确定沿直线*y=2x*分布的二维数据集的主成分，该数据集具有随机（正态分布）噪声。数据集和相应的图（见下图）已使用以下代码生成：
- en: '[PRE4]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The following figure shows the resulting dataset. Clearly there is a direction
    in which the data is distributed and it corresponds to the principal component
    ![PCA example](img/Image00131.jpg) that we are going to extract from the data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了结果数据集。很明显，存在一个数据分布的方向，它对应于我们将从数据中提取的主成分![PCA示例](img/Image00131.jpg)。
- en: '![PCA example](img/Image00132.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![PCA示例](img/Image00132.jpg)'
- en: A two-dimensional dataset. The principal component direction v1 is indicated
    by an arrow.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一个二维数据集。主成分方向v1由箭头指示。
- en: 'The algorithm calculates the mean of the two-dimensional dataset and the mean
    shifted dataset, and then rescales with the corresponding standard deviation:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 算法计算二维数据集和均值偏移数据集的平均值，然后使用相应的标准差进行缩放：
- en: '[PRE5]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To extract the principal component, we have to calculate the eigenvalues and
    eigenvectors and select the eigenvector associated with the largest eigenvalue:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取主成分，我们必须计算特征值和特征向量，并选择与最大特征值相关的特征向量：
- en: '[PRE6]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To check whether the principal component lies along the line as expected, we
    need to rescale back its coordinates:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查主成分是否如预期地沿着直线分布，我们需要重新缩放其坐标：
- en: '[PRE7]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting slope is approximately `2` , which agrees with the value chosen
    at the beginning. The `scikit-learn` library provides a possible ready-to-use
    implementation of the PCA algorithm without applying any rescaling or mean shifting.
    To use the `sklearn` module, we need to transform the rescaled data into a matrix
    structure in which each row is a data point with *x* , *y* coordinates:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的斜率大约为`2`，这与开始时选择的价值一致。`scikit-learn`库提供了一个可能的无需应用任何缩放或均值偏移即可使用的PCA算法实现。要使用`sklearn`模块，我们需要将缩放后的数据转换成一个矩阵结构，其中每一行是一个具有*x*，*y*坐标的数据点：
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The PCA module can be started now, specifying the number of principal components
    we want (`1` in this case):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以启动PCA模块，指定我们想要的 主成分数量（在这种情况下为`1`）：
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The principal component is exactly the same as the one obtained using the step-by-step
    approach,`[ 0.70710678 0.70710678]` , so the slope will also be the same. The
    dataset can now be transformed into the new one-dimensional space with both approaches:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分与使用逐步方法获得的结果完全相同，`[ 0.70710678 0.70710678]`，因此斜率也将相同。现在可以使用两种方法将数据集转换成新的一个维空间：
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The assert exception is not thrown, so the results show a perfect agreement
    between the two methods.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 断言异常没有被抛出，所以结果显示两种方法之间完全一致。
- en: Singular value decomposition
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: 'This method is based on a theorem that states that a matrix *X d x N* can be
    decomposed as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法基于一个定理，该定理表明一个 *d x N* 矩阵 *X* 可以分解如下：
- en: '![Singular value decomposition](img/Image00133.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![奇异值分解](img/Image00133.jpg)'
- en: 'Here:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '*U* is a *d x d* unitary matrix'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*U* 是一个 *d x d* 的单位矩阵'
- en: ∑ is a *d x N* diagonal matrix where the diagonal entries s *i* are called singular
    values
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ∑ 是一个 *d x N* 的对角矩阵，其中对角线上的元素 s *i* 被称为奇异值
- en: '*V* is an *N x N* unitary matrix'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V* 是一个 *N x N* 的单位矩阵'
- en: 'In our case, *X* can be composed by the feature''s vectors ![Singular value
    decomposition](img/Image00134.jpg) , where each ![Singular value decomposition](img/Image00135.jpg)
    is a column. We can reduce the number of dimensions of each feature vector *d*
    , approximating the singular value decomposition. In practice, we consider only
    the largest singular values ![Singular value decomposition](img/Image00136.jpg)
    so that:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，*X* 可以由特征向量 ![奇异值分解](img/Image00134.jpg) 组成，其中每个 ![奇异值分解](img/Image00135.jpg)
    是一列。我们可以通过近似奇异值分解来减少每个特征向量 *d* 的维度。在实践中，我们只考虑最大的奇异值 ![奇异值分解](img/Image00136.jpg)，因此：
- en: '![Singular value decomposition](img/Image00137.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![奇异值分解](img/Image00137.jpg)'
- en: '*t* represents the dimension of the new reduced space where the feature vectors
    are projected. *A* vector *x^((i))* is transformed in the new space using the
    following formula:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*t* 代表新的降维空间，其中特征向量被投影。向量 *x^((i))* 在新空间中使用以下公式进行转换：'
- en: '![Singular value decomposition](img/Image00138.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![奇异值分解](img/Image00138.jpg)'
- en: This means that the matrix ![Singular value decomposition](img/Image00139.jpg)
    (not ![Singular value decomposition](img/Image00140.jpg) ) represents the feature
    vectors in the *t* dimensional space.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着矩阵 ![奇异值分解](img/Image00139.jpg)（不是 ![奇异值分解](img/Image00140.jpg)）代表了 *t*
    维空间中的特征向量。
- en: Note that it is possible to show that this method is very similar to the PCA;
    in fact, the `scikit-learn` library uses SVD to implement PCA.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，可以证明这种方法与PCA非常相似；事实上，`scikit-learn` 库使用SVD来实现PCA。
- en: Summary
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, the main clustering algorithms were discussed in detail. We
    implemented them (using scikit-learn) and compared the results. Also, the most
    relevant dimensionality reduction technique, principal component analysis, was
    presented and implemented. You should now have the knowledge to use the main unsupervised
    learning techniques in real scenarios using Python and its libraries.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，详细讨论了主要的聚类算法。我们实现了它们（使用scikit-learn）并比较了结果。此外，还介绍了最相关的降维技术——主成分分析，并对其进行了实现。现在，你应该有了使用Python及其库在真实场景中应用主要无监督学习技术的知识。
- en: In the next chapter, the supervised learning algorithms will be discussed, for
    both classification and regression problems.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，将讨论监督学习算法，包括分类和回归问题。
