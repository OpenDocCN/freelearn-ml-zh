- en: Chapter 2. Unsupervised Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen in the [Chapter 1](text00015.html#ch01 "Chapter 1. Introduction
    to Practical Machine Learning Using Python") , *Introduction to Practical Machine
    Learning Using Python* , unsupervised learning is designed to provide insightful
    information on data unlabeled date. In many cases, a large dataset (both in terms
    of number of points and number of features) is unstructured and does not present
    any information at first sight, so these techniques are used to highlight hidden
    structures on data (clustering) or to reduce its complexity without losing relevant
    information (dimensionality reduction). This chapter will focus on the main clustering
    algorithms (the first part of the chapter) and dimensionality reduction methods
    (the second part of the chapter). The differences and advantages of the methods
    will be highlighted by providing a practical example using Python libraries. All
    of the code will be available on the author's GitHub profile, in the [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/)
    folder. We will now start describing clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering algorithms are employed to restructure data in somehow ordered subsets
    so that a meaningful structure can be inferred. A cluster can be defined as a
    group of data points with some similar features. The way to quantify the similarity
    of data points is what determines the different categories of clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms can be divided into different categories based on different
    metrics or assumptions in which data has been manipulated. We are going to discuss
    the most relevant categories used nowadays, which are distribution methods, centroid
    methods, density methods, and hierarchical methods. For each category, a particular
    algorithm is going to be presented in detail, and we will begin by discussing
    distribution methods. An example to compare the different algorithms will be discussed,
    and both the IPython notebook and script are available in the my GitHub book folder
    at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Distribution methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These methods assume that the data comes from a certain distribution, and the
    expectation maximization algorithm is used to find the optimal values for the
    distribution parameters. Expectation maximization and the mixture of **Gaussian**
    clustering are discussed hereafter.
  prefs: []
  type: TYPE_NORMAL
- en: Expectation maximization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This algorithm is used to find the maximum likelihood estimates of parameter
    distribution models that depend on hidden (unobserved) variables. Expectation
    maximization consists of iterating two steps: the **E-step** , which creates the
    **log-likelihood** function evaluated using the current values of the parameters,
    and the **M-step** , where the new parameters'' values are computed, maximizing
    the log-likelihood of the E-step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a set of *N* elements, *{x^((i)) }i = 1,…,N* , and a log-likelihood
    on the data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Expectation maximization](img/Image00046.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *θ* represents the set of parameters and *z^((i))* are the so-called hidden
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to find the values of the parameters that maximize the log-likelihood
    without knowing the values of the *z^((i))* (unobserved variables). Consider a
    distribution over the *z^((i))* , and *Q(z^((i)) )* , such as ![Expectation maximization](img/Image00047.jpg)
    . Therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Expectation maximization](img/Image00048.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This means *Q(z^((i)) )* is the posterior distribution of the hidden variable,
    *z^((i))* , given *x^((i))* parameterized by *θ* . The expectation maximization
    algorithm comes from the use of Jensen''s inequality and it ensures that carrying
    out these two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Expectation maximization](img/Image00049.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: '![Expectation maximization](img/Image00050.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: The log-likelihood converges to the maximum, and so the associated *θ* values
    are found.
  prefs: []
  type: TYPE_NORMAL
- en: Mixture of Gaussians
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This method models the entire dataset using a mixture of Gaussian distributions.
    Therefore, the number of clusters will be given as the number of Gaussians considered
    in the mixture. Given a dataset of N elements, *{x^((i)) }i = 1,…,N* , where each
    ![Mixture of Gaussians](img/Image00051.jpg) is a vector of d-features modeled
    by a mixture of Gaussian such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mixture of Gaussians](img/Image00052.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mixture of Gaussians](img/Image00053.jpg) is a hidden variable that represents
    the Gaussian component each *x^((i)) * is generated from'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Mixture of Gaussians](img/Image00054.jpg) represents the set of mean parameters
    of the Gaussian components'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Mixture of Gaussians](img/Image00055.jpg) represents the set of variance
    parameters of the Gaussian components'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Mixture of Gaussians](img/Image00056.jpg) is the mixture weight, representing
    the probability that a randomly selected *x^((i)) * was generated by the Gaussian
    component *k* , where ![Mixture of Gaussians](img/Image00057.jpg) , and ![Mixture
    of Gaussians](img/Image00058.jpg) is the set of weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Mixture of Gaussians](img/Image00059.jpg) is the Gaussian component *k* with
    parameters ![Mixture of Gaussians](img/Image00060.jpg) associated with the point
    *x^((i))*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The parameters of our model are thus φ, µ and ∑. To estimate them, we can write
    down the log-likelihood of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mixture of Gaussians](img/Image00061.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In order to find the values of the parameters, we apply the expectation maximization
    algorithm explained in the previous section where ![Mixture of Gaussians](img/Image00062.jpg)
    and ![Mixture of Gaussians](img/Image00063.jpg) .
  prefs: []
  type: TYPE_NORMAL
- en: 'After choosing a first guess of the parameters, we iterate the following steps
    until convergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '**E- step** : The weights ![Mixture of Gaussians](img/Image00064.jpg) are updated
    by following the rule obtained by applying Bayes'' theorem:![Mixture of Gaussians](img/Image00065.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**M-step** : The parameters are updated to the following (these formulas come
    from solving the maximization problem, which means setting the derivatives of
    the log-likelihood to zero):![Mixture of Gaussians](img/Image00066.jpg)![Mixture
    of Gaussians](img/Image00067.jpg)![Mixture of Gaussians](img/Image00068.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the expectation maximization algorithm is needed because the hidden
    variables *z^((i))* are unknown. Otherwise, it would have been a supervised learning
    problem, where *z^((i))* is the label of each point of the training set (and the
    supervised algorithm used would be the Gaussian discriminant analysis). Therefore,
    this is an unsupervised algorithm and the goal is also to find *z^((i))* , that
    is, which of the *K* Gaussian components each point *x^((i))* is associated with.
    In fact, by calculating the posterior probability ![Mixture of Gaussians](img/Image00069.jpg)
    for each of the *K* classes, it is possible to assign each *x(i)* to the class
    *k* with the highest posterior probability. There are several cases in which this
    algorithm can be successfully used to cluster (label) the data.
  prefs: []
  type: TYPE_NORMAL
- en: A possible practical example is the case of a professor with student grades
    for two different classes but not labeled per class. He wants to split the grades
    into the original two classes assuming that the distribution of grades in each
    class is Gaussian. Another example solvable with the mixture of the Gaussian approach
    is to determine the country of each person based on a set of people's height values
    coming from two different countries and assuming that the distribution of height
    in each country follows Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Centroid methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This class collects all the techniques that find the centers of the clusters,
    assigning the data points to the nearest cluster center and minimizing the distances
    between the centers and the assigned points. This is an optimization problem and
    the final centers are vectors; they may not be the points in the original dataset.
    The number of clusters is a parameter to be specified a priori and the generated
    clusters tend to have similar sizes so that the border lines are not necessarily
    well defined. This optimization problem may lead to a local optimal solution,
    which means that different initialization values can result in slightly different
    clusters. The most common method is known as **k-means** ( **Lloyd's algorithm**
    ), in which the distance minimized is the **Euclidean norm** . Other techniques
    find the centers as the medians of the clusters ( **k-medians clustering** ) or
    impose the center's values to be the actual data points. Furthermore, other variations
    of these methods differ in the choice that the initial centers are defined ( **k-means++**
    or **fuzzy c-means** ).
  prefs: []
  type: TYPE_NORMAL
- en: k-means
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This algorithm tries to find the center of each cluster as the mean of all
    the members that minimize the distance between the center and the assigned points
    themselves. It can be associated with the k-nearest-neighbor algorithm in classification
    problems, and the resulting set of clusters can be represented as a **Voronoi
    diagram** (a method of partitioning the space in regions based on the distance
    from a set of points, in this case, the clusters'' centers). Consider the usual
    dataset, ![k-means](img/Image00070.jpg) . The algorithm prescribes to choose a
    number of centers *K* , assign the initial mean cluster centers ![k-means](img/Image00071.jpg)
    to random values, and then iterate the following steps until convergence:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each data point *i* , calculate the Euclidean square distances between
    each point *i* and each center *j* and find the center index *d[i]* , which corresponds
    to the minimum of these distances: ![k-means](img/Image00072.jpg) .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each center *j* , recalculate its mean value as the mean of the points that
    have *d_i j* equal to *j* (that is, points belonging to the cluster with mean
    ![k-means](img/Image00073.jpg) ):![k-means](img/Image00074.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It is possible to show that this algorithm converges with respect to the function
    given by the following function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![k-means](img/Image00075.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: It decreases monotonically with the number of iterations. Since *F* is a nonconvex
    function, it is not guaranteed that the final minimum will be the global minimum.
    In order to avoid the problem of a clustering result associated with the local
    minima, the k-means algorithm is usually run multiple times with different random
    initial center's means. Then the result associated with the lower *F* value is
    chosen as the optimal clustering solution.
  prefs: []
  type: TYPE_NORMAL
- en: Density methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These methods are based on the idea that sparse areas have to be considered
    borders (or noise) and high-density zones should be related to the cluster's centers.
    The common method is called **density-based spatial clustering of applications
    with noise** ( **DBSCAN** ), which defines the connection between two points through
    a certain distance threshold (for this reason, it is similar to hierarchical algorithms;
    see [Chapter 3](text00024.html#page "Chapter 3. Supervised Machine Learning")
    , *Supervised Machine Learning* ). Two points are considered linked (belonging
    to the same cluster) only if a certain density criterion is satisfied—the number
    of neighboring points has to be higher than a threshold value within a certain
    radius. Another popular method is mean-shift, in which each data point is assigned
    to the cluster that has the highest density in its neighborhood. Due to the time-consuming
    calculations of the density through a kernel density estimation, mean-shift is
    usually slower than DBSCAN or centroid methods. The main advantages of this class
    of algorithms are the ability to define clusters with arbitrary shapes and the
    ability to determine the best number of clusters instead of setting this number
    a priori as a parameter, making these methods suitable to cluster datasets in
    which it is not known.
  prefs: []
  type: TYPE_NORMAL
- en: Mean – shift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Mean-shift is nonparametric algorithm that finds the positions of the local
    maxima in a density kernel function defined on a dataset. The local maxima found
    can be considered the centers of clusters in a dataset ![Mean – shift](img/Image00076.jpg)
    , and the number of maxima is the number of clusters. In order to be applied as
    a clustering algorithm, each point ![Mean – shift](img/Image00077.jpg) has to
    be associated with the density of its neighborhood:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mean – shift](img/Image00078.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *h* is the so-called bandwidth; it estimates the radius of the neighborhood
    in which the points affect the density value *f(x^((l)) )* (that is, the other
    points have negligible effect on ![Mean – shift](img/Image00079.jpg) ). *K* is
    the kernel function that satisfies these conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mean – shift](img/Image00080.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Mean – shift](img/Image00081.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'Typical examples of *K(x^((i)) )* are the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mean – shift](img/Image00082.jpg) : Gaussian kernel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Mean – shift](img/Image00083.jpg) : Epanechnikov kernel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The mean-shift algorithm imposes the maximization of *f(x^((l)) )* , which
    translates into the mathematical equation (remember that in function analysis,
    the maximum is found by imposing the derivative to *0* ):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mean – shift](img/Image00084.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *K'* is derivative of the kernel density function *K* .
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, to find the local maxima position associated with the feature vector
    *x^((l))* , the following iterative equation can be computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mean – shift](img/Image00085.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Mean – shift](img/Image00086.jpg) is called the mean-shift vector. The
    algorithm will converge when at iteration *t=a* , the condition ![Mean – shift](img/Image00087.jpg)
    is satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: 'Supported by the equation, we can now explain the algorithm with the help of
    the following figure. At the first iteration *t=0* , the original points ![Mean
    – shift](img/Image00088.jpg) (red) are spread on the data space, the mean shift
    vector ![Mean – shift](img/Image00089.jpg) is calculated, and the same points
    are marked with a cross ( *x* ) to track their evolution with the algorithm. At
    iteration *1* , the dataset will be obtained using the aforementioned equation,
    and the resulting points ![Mean – shift](img/Image00090.jpg) are shown in the
    following figure with the ( *+* ) symbol:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mean – shift](img/Image00091.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Sketch of the mean-shift evolution through iterations
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, at iteration *0* the original points are shown in red
    (cross), at iteration *1* and *K* the sample points (symbols *+* and *** respectively)
    move towards the local density maxima indicated by blue squares.
  prefs: []
  type: TYPE_NORMAL
- en: Again, at iteration *K* , the new data points ![Mean – shift](img/Image00092.jpg)
    are computed and they are shown with the *** symbol in the preceding figure. The
    density function values ![Mean – shift](img/Image00093.jpg) associated with ![Mean
    – shift](img/Image00094.jpg) are larger than the values in the previous iterations
    since the algorithm aims to maximize them. The original dataset is now clearly
    associated with points ![Mean – shift](img/Image00092.jpg) , and they converge
    to the locations plotted in blue squares in the preceding figure. The feature
    vectors ![Mean – shift](img/Image00088.jpg) are now collapsing to two different
    local maxima, which represent the centers of the two clusters.
  prefs: []
  type: TYPE_NORMAL
- en: In order to properly use the method, some considerations are necessary.
  prefs: []
  type: TYPE_NORMAL
- en: The only parameter required, the bandwidth *h* , needs to be tuned cleverly
    to achieve good results. In fact, too low value of *h* may result in a large number
    of clusters, while a large value of *h* may merge multiple distinct clusters.
    Note also that if the number *d* of feature vector dimensions is large, the mean-shift
    method may lead to poor results. This is because in a very-high-dimensional space,
    the number of local maxima is accordingly large and the iterative equation can
    easily converge too soon.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The class of hierarchical methods, also called connectivity-based clustering,
    forms clusters by collecting elements on a similarity criteria based on a distance
    metric: close elements gather in the same partition while far elements are separated
    into different clusters. This category of algorithms is divided in two types:
    **divisive clustering** and **agglomerative clustering** . The divisive approach
    starts by assigning the entire dataset to a cluster, which is then divided in
    two less similar (distant) clusters. Each partition is further divided until each
    data point is itself a cluster. The agglomerative method, which is the most often
    employed method, starts from the data points, each of them representing a cluster.
    Then these clusters are merged by similarity until a single cluster containing
    all the data points remains. These methods are called **hierarchical** because
    both categories create a hierarchy of clusters iteratively, as the following figure
    shows. This hierarchical representation is called a **dendrogram** . On the horizontal
    axis, there are the elements of the dataset, and on the vertical axis, the distance
    values are plotted. Each horizontal line represents a cluster and the vertical
    axis indicates which element/cluster forms the related cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical methods](img/Image00095.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding figure, agglomerative clustering starts from many clusters
    as dataset points and ends up with a single cluster that contains the entire dataset.
    Vice versa, the divisive method starts from a single cluster and finishes when
    all clusters contain a single data point each.
  prefs: []
  type: TYPE_NORMAL
- en: The final clusters are then formed by applying criteria to stop the agglomeration/division
    strategy. The distance criteria sets the maximum distance above which two clusters
    are too far away to be merged, and the *number of clusters* criteria sets the
    maximum number of clusters to stop the hierarchy from continuing to merge or split
    the partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of agglomeration is given by the following algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Assign each element *i* of the dataset ![Hierarchical methods](img/Image00096.jpg)
    to a different cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the distances between each pair of clusters and merge the closest
    pair into a single cluster, reducing the total number of clusters by *1* .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the distances of the new cluster from the others.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2 and 3 until only a single cluster remains with all *N* elements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since the distance *d(C1,C2)* between two clusters *C1* , *C2* , is computed
    by definition between two points ![Hierarchical methods](img/Image00097.jpg) and
    each cluster contains multiple points, a criteria to decide which elements have
    to be considered to calculate the distance is necessary (linkage criteria). The
    common linkage criteria of two clusters *C1* and *C2* are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single linkage** : The minimum distance among the distances between any element
    of *C1* and any element of *C2* is given by the following:![Hierarchical methods](img/Image00098.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete linkage** : The maximum distance among the distances between any
    element of *C1* and any element of *C2* is given by the following:![Hierarchical
    methods](img/Image00099.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unweighted pair group method with arithmetic mean (UPGMA) or average linkage**
    : The average distance among the distances between any element of *C1* and any
    element of *C2* is ![Hierarchical methods](img/Image00100.jpg) , where ![Hierarchical
    methods](img/Image00101.jpg) are the numbers of elements of *C1* and *C2* , respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ward algorithm** : This merges partitions that do not increase a certain
    measure of heterogeneity. It aims to join two clusters *C1* and *C2* that have
    the minimum increase of a variation measure, called the merging cost ![Hierarchical
    methods](img/Image00102.jpg) , due to their combination. The distance in this
    case is replaced by the merging cost, which is given by the following formula:![Hierarchical
    methods](img/Image00103.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, ![Hierarchical methods](img/Image00101.jpg) are the numbers of elements
    of C1 and C2, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There are different metrics *d(c1,c2)* that can be chosen to implement a hierarchical
    algorithm. The most common is the Euclidean distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical methods](img/Image00104.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that this class of method is not particularly time-efficient, so it is
    not suitable for clustering large datasets. It is also not very robust towards
    erroneously clustered data points (outliers), which may lead to incorrect merging
    of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Training and comparison of the clustering methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To compare the clustering methods just presented, we need to generate a dataset.
    We choose the two dataset classes given by the two two-dimensional multivariate
    normal distributions with means and covariance equal to ![Training and comparison
    of the clustering methods](img/Image00105.jpg) and ![Training and comparison of
    the clustering methods](img/Image00106.jpg) , respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data points are generated using the NumPy library and plotted with matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'A normally distributed noise has been added to both classes to make the example
    more realistic. The result is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training and comparison of the clustering methods](img/Image00107.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Two multivariate normal classes with noise
  prefs: []
  type: TYPE_NORMAL
- en: 'The clustering methods have been implemented using the `sklearn` and `scipy`
    libraries and again plotted with matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The k-means function and Gaussian mixture model have a specified number of
    clusters (`n_clusters =2,n_components=2` ), while the mean-shift algorithm has
    the bandwidth value `bandwidth=7` . The hierarchical algorithm is implemented
    using the ward linkage and the maximum (ward) distance, `max_d` , is set to `110`
    to stop the hierarchy. The `fcluster` function is used to obtain the predicted
    class for each data point. The predicted classes for the k-means and the mean-shift
    method are accessed using the `labels_` attribute, while the Gaussian mixture
    model needs to employ the `predict` function. The k -means, mean-shift, and Gaussian
    mixture methods have been trained using the `fit` function, while the hierarchical
    method has been trained using the `linkage` function. The output of the preceding
    code is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training and comparison of the clustering methods](img/Image00108.jpg)'
  prefs: []
  type: TYPE_IMG
- en: IClustering of the two multivariate classes using k-means, mean-shift, Gaussian
    mixture model, and hierarchical ward method
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean-shift and hierarchical methods show two classes, so the choice of
    parameters (bandwidth and maximum distance) is appropriate. Note that the maximum
    distance value for the hierarchical method has been chosen looking at the dendrogram
    (the following figure) generated by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `truncate_mode=''lastp''` flag allows us to specify the number of last
    merges to show in the plot (in this case, `p=12` ). The preceding figure clearly
    shows that when the distance is between `100` and `135` , there are only two clusters
    left:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training and comparison of the clustering methods](img/Image00109.jpg)'
  prefs: []
  type: TYPE_IMG
- en: IHierarchical clustering dendrogram for the last 12 merges
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure on the horizontal axis, the number of data points belonging
    to each cluster before the last *12* merges is shown in brackets ().
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the Gaussian mixture model, the other three algorithms misclassify
    some data points, especially k-means and hierarchical methods. This result proves
    that the Gaussian mixture model is the most robust method, as expected, since
    the dataset comes from the same distribution assumption. To evaluate the quality
    of the clustering, scikit-learn provides methods to quantify the correctness of
    the partitions: v-measure, completeness, and homogeneity. These methods require
    the real value of the class for each data point, so they are referred to as external
    validation procedures. This is because they require additional information not
    used while applying the clustering methods. Homogeneity, *h* , is a score between
    *0* and *1* that measures whether each cluster contains only elements of a single
    class. Completeness, *c* , quantifies with a score between *0* and *1* whether
    all the elements of a class are assigned to the same cluster. Consider a clustering
    that assigns each data point to a different cluster. In this way, each cluster
    will contains only one class and the homogeneity is *1* , but unless each class
    contains only one element, the completeness is very low because the class elements
    are spread around many clusters. Vice versa, if a clustering results in assigning
    all the data points of multiple classes to the same cluster, certainly the completeness
    is *1* but homogeneity is poor. These two scores have a similar formula, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training and comparison of the clustering methods](img/Image00110.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training and comparison of the clustering methods](img/Image00111.jpg) is
    the conditional entropy of the classes *C^l * , given the cluster assignments
    ![Training and comparison of the clustering methods](img/Image00112.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Training and comparison of the clustering methods](img/Image00113.jpg) is
    the conditional entropy of the clusters, given the class membership ![Training
    and comparison of the clustering methods](img/Image00114.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '*H(C[l] )* is the entropy of the classes: ![Training and comparison of the
    clustering methods](img/Image00115.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*H(C)* is the entropy of the clusters: ![Training and comparison of the clustering
    methods](img/Image00116.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N[pc]* is the number of elements of class *p* in cluster *c* , *N[p]* is the
    number of elements of class *p* , and *N[c]* is the number of elements of cluster
    *c*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The v-measure is simply the harmonic mean of the homogeneity and the completeness:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training and comparison of the clustering methods](img/Image00117.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These measures require the true labels to evaluate the quality of the clustering,
    and often this is not real-case scenario. Another method only employs data from
    the clustering itself, called **silhouette** , which calculates the similarities
    of each data point with the members of the cluster it belongs to and with the
    members of the other clusters. If on average each point is more similar to the
    points of its own cluster than the rest of the points, then the clusters are well
    defined and the score is close to *1* (it is close to *-1* , otherwise). For the
    formula, consider each point *i* and the following quantities:'
  prefs: []
  type: TYPE_NORMAL
- en: '*d[s] (i)* is the average distance of the point *i* from the points of the
    same cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*d[rest] (i)* is the minimum distance of point *i* from the rest of the points
    in all other clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The silhouette can be defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![Training and comparison of the clustering methods](img/Image00118.jpg) ,
    and the silhouette score is the average of *s(i)* for all data points.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The four clustering algorithms we covered are associated with the following
    values of these four measures calculated using `sklearn` (scikit-learn):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As expected from the analysis of the preceding figure, the Gaussian mixture
    model has the best values of the homogeneity, completeness, and v-measure measures
    (close to *1* ); mean-shift has reasonable values (around *0.5* ); while k-means
    and hierarchical methods result in poor values (around *0.3* ). The silhouette
    score instead is decent for all the methods (between *0.35* and *0.41* ), meaning
    that the clusters are reasonably well defined.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Dimensionality reduction** , which is also called feature extraction, refers
    to the operation to transform a data space given by a large number of dimensions
    to a subspace of fewer dimensions. The resulting subspace should contain only
    the most relevant information of the initial data, and the techniques to perform
    this operation are categorized as linear or non-linear. Dimensionality reduction
    is a broad class of techniques that is useful for extracting the most relevant
    information from a large dataset, decreasing its complexity but keeping the relevant
    information.'
  prefs: []
  type: TYPE_NORMAL
- en: The most famous algorithm, **Principal Component Analysis** ( **PCA** ), is
    a linear mapping of the original data into a subspace of uncorrelated dimensions,
    and it will be discussed hereafter. The code shown in this paragraph is available
    in IPython notebook and script versions at the author's GitHub book folder at
    [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The principal component analysis algorithm aims to identify the subspace where
    the relevant information of a dataset lies. In fact, since the data points can
    be correlated in some data dimensions, PCA will find the few uncorrelated dimensions
    in which the data varies. For example, a car trajectory can be described by a
    series of variables such as velocity in km/h or m/s, position in latitude and
    longitude, position in meters from a chosen point, and position in miles from
    a chosen point. Clearly, the dimensions can be reduced because the velocity variables
    and the position variables give the same information (correlated variables), so
    the relevant subspace can be composed of two uncorrelated dimensions (a velocity
    variable and a position variable). PCA finds not only the uncorrelated set of
    variables but also the dimensions where the variance is maximized. That is, between
    the velocity in km/h and miles/h, the algorithm will select the variable with
    the highest variance, which is trivially represented by the line between the two
    axes given by the function *velocity[km/h]=3.6*velocity[m/s]* (typically closer
    to the km/h axis because *1 km/h = 3.6 m/s* and the velocity projections are more
    spread along the km/h axis than the m/s axis):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal Component Analysis (PCA)](img/Image00119.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The linear function between the velocity in m/s and km/h
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure represents the linear function between the velocity in
    m/s and km/h. The projections of the points along the km/h axis have a large variance,
    while the projections on the m/s axis have a lower variance. The variance along
    the linear function *velocity[km/h]=3.6*velocity[m/s]* is larger than both axes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to discuss the method and its features in detail. It is possible
    to show that finding the uncorrelated dimensions in which the variance is maximized
    is equivalent to computing the following steps. As usual, we consider the feature
    vectors ![Principal Component Analysis (PCA)](img/Image00096.jpg) :'
  prefs: []
  type: TYPE_NORMAL
- en: 'The average of the dataset: ![Principal Component Analysis (PCA)](img/Image00120.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The mean shifted dataset: ![Principal Component Analysis (PCA)](img/Image00121.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rescaled dataset, in which each feature vector component ![Principal Component
    Analysis (PCA)](img/Image00122.jpg) has been divided by the standard deviation,
    ![Principal Component Analysis (PCA)](img/Image00123.jpg) , where ![Principal
    Component Analysis (PCA)](img/Image00124.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The sample covariance matrix: ![Principal Component Analysis (PCA)](img/Image00125.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *k* largest eigenvalues, ![Principal Component Analysis (PCA)](img/Image00126.jpg)
    , and their associated eigenvectors, ![Principal Component Analysis (PCA)](img/Image00127.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Projected feature vectors on the subspace of the *k* eigenvectors ![Principal
    Component Analysis (PCA)](img/Image00128.jpg) , where ![Principal Component Analysis
    (PCA)](img/Image00129.jpg) is the matrix of the eigenvectors with *N* rows and
    *k* columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final feature's vectors (principal components), ![Principal Component Analysis
    (PCA)](img/Image00130.jpg) lie on a subspace *R^k * , which still retain the maximum
    variance (and information) of the original vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this technique is particularly useful when dealing with high-dimensional
    datasets, such as in face recognition. In this field, an input image has to be
    compared with a database of other images to find the right person. The PCA application
    is called **Eigenfaces** , and it exploits the fact that a large number of pixels
    (variables) in each image are correlated. For instance, the background pixels
    are all correlated (the same), so a dimensionality reduction can be applied, and
    comparing images in a smaller subspace is a faster approach that gives accurate
    results. An example of implementation of Eigenfaces can be found on the author's
    GitHub profile at [https://github.com/ai2010/eigenfaces](https://github.com/ai2010/eigenfaces)
    .
  prefs: []
  type: TYPE_NORMAL
- en: PCA example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As an example of the usage of PCA as well as the NumPy library discussed in
    [Chapter 1](text00015.html#ch01 "Chapter 1. Introduction to Practical Machine
    Learning Using Python") , *Introduction to Practical Machine Learning using Python*
    we are going to determine the principal component of a two-dimensional dataset
    distributed along the line *y=2x* , with random (normally distributed) noise.
    The dataset and the corresponding figure (see the following figure) have been
    generated using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The following figure shows the resulting dataset. Clearly there is a direction
    in which the data is distributed and it corresponds to the principal component
    ![PCA example](img/Image00131.jpg) that we are going to extract from the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![PCA example](img/Image00132.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A two-dimensional dataset. The principal component direction v1 is indicated
    by an arrow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm calculates the mean of the two-dimensional dataset and the mean
    shifted dataset, and then rescales with the corresponding standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To extract the principal component, we have to calculate the eigenvalues and
    eigenvectors and select the eigenvector associated with the largest eigenvalue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To check whether the principal component lies along the line as expected, we
    need to rescale back its coordinates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting slope is approximately `2` , which agrees with the value chosen
    at the beginning. The `scikit-learn` library provides a possible ready-to-use
    implementation of the PCA algorithm without applying any rescaling or mean shifting.
    To use the `sklearn` module, we need to transform the rescaled data into a matrix
    structure in which each row is a data point with *x* , *y* coordinates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The PCA module can be started now, specifying the number of principal components
    we want (`1` in this case):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The principal component is exactly the same as the one obtained using the step-by-step
    approach,`[ 0.70710678 0.70710678]` , so the slope will also be the same. The
    dataset can now be transformed into the new one-dimensional space with both approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The assert exception is not thrown, so the results show a perfect agreement
    between the two methods.
  prefs: []
  type: TYPE_NORMAL
- en: Singular value decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This method is based on a theorem that states that a matrix *X d x N* can be
    decomposed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Singular value decomposition](img/Image00133.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*U* is a *d x d* unitary matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∑ is a *d x N* diagonal matrix where the diagonal entries s *i* are called singular
    values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*V* is an *N x N* unitary matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our case, *X* can be composed by the feature''s vectors ![Singular value
    decomposition](img/Image00134.jpg) , where each ![Singular value decomposition](img/Image00135.jpg)
    is a column. We can reduce the number of dimensions of each feature vector *d*
    , approximating the singular value decomposition. In practice, we consider only
    the largest singular values ![Singular value decomposition](img/Image00136.jpg)
    so that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Singular value decomposition](img/Image00137.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*t* represents the dimension of the new reduced space where the feature vectors
    are projected. *A* vector *x^((i))* is transformed in the new space using the
    following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Singular value decomposition](img/Image00138.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This means that the matrix ![Singular value decomposition](img/Image00139.jpg)
    (not ![Singular value decomposition](img/Image00140.jpg) ) represents the feature
    vectors in the *t* dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it is possible to show that this method is very similar to the PCA;
    in fact, the `scikit-learn` library uses SVD to implement PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the main clustering algorithms were discussed in detail. We
    implemented them (using scikit-learn) and compared the results. Also, the most
    relevant dimensionality reduction technique, principal component analysis, was
    presented and implemented. You should now have the knowledge to use the main unsupervised
    learning techniques in real scenarios using Python and its libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, the supervised learning algorithms will be discussed, for
    both classification and regression problems.
  prefs: []
  type: TYPE_NORMAL
