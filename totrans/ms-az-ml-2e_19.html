<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer271">
			<h1 id="_idParaDest-239"><em class="italic"><a id="_idTextAnchor238"/>Chapter 15</em>: Model Interoperability, Hardware Optimization, and Integrations</h1>
			<p>In the previous chapter, we discovered how to deploy our machine learning scoring either as a batch or real-time scorer, what endpoints are and how we can deploy them, and finally, we had a look at how we can monitor our deployed solutions. In this chapter, we will dive deeper into additional deployment scenarios for ML inferencing, possible other hardware infrastructure we can utilize, and how we can integrate our models and endpoints with other Azure services. </p>
			<p>In the first section, we will have a look at how to provide model interoperability by converting ML models into a standardized model format and an inference-optimized scoring framework. <strong class="bold">Open Neural Network Exchange</strong> (<strong class="bold">ONNX</strong>) is a standardized format to serialize and store ML models and acyclic computational graphs and operations efficiently. We will learn what the ONNX framework is, how we can convert ML models from popular ML frameworks to ONNX, and how we can score ONNX models on multiple platforms using ONNX Runtime.</p>
			<p>Following that, we will take a look at alternative hardware targets, such as <strong class="bold">field-programmable gate arrays</strong> (<strong class="bold">FPGAs</strong>). We will understand how they work internally and how they can lead to higher performance and better efficiency compared to standard hardware or even GPUs.</p>
			<p>Finally, we will have a look at how we can integrate ML models and endpoints into other services. We will get a deeper understanding of the process to deploy ML to edge devices, and we will integrate one of our previously set up endpoints with Power BI.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Model interoperability with ONNX</li>
				<li>Hardware optimization with FPGAs</li>
				<li>Integrating ML models and endpoints with Azure services</li>
			</ul>
			<h1 id="_idParaDest-240"><a id="_idTextAnchor239"/>Technical requirements</h1>
			<p>In this chapter, you will require access to a Microsoft Power BI account. You can get one either through your place of work or by creating a trial account here: <a href="https://app.powerbi.com/signupredirect?pbi_source=web">https://app.powerbi.com/signupredirect?pbi_source=web</a>. </p>
			<p>All code examples in this chapter can be found in the GitHub repository for this book: <a href="https://github.com/PacktPublishing/Masthttps://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter15">https://github.com/PacktPublishing/Masthttps://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter15</a>. </p>
			<h1 id="_idParaDest-241"><a id="_idTextAnchor240"/>Model interoperability with ONNX</h1>
			<p>In the previous chapter, we learned how <a id="_idIndexMarker1687"/>to deploy ML models as web services for online and batch scoring. However, many real-world use cases require you to embed a trained ML model directly into an application without the use of a separate scoring service. The target service is likely written in a different language than the language used for training the ML model. A <a id="_idIndexMarker1688"/>common example is that a simple model trained in Python using scikit-learn needs to be embedded into a Java application.</p>
			<p>Model interoperability gives you the flexibility to train your model with your language and framework of choice, export it to a common format, and then score it in a different language and platform using the shared format. In some cases, using a native runtime optimized for scoring on the target environment even achieves a better scoring performance than running the original model.</p>
			<p>First, we will take a look at the ONNX initiative, consisting of the specification, runtime, and ecosystem, and how it helps to achieve model interoperability across a large set of support languages, frameworks, operations, and target platforms.</p>
			<p>Then, we will look into converting ML models from popular frameworks to ONNX (called ONNX frontends) and executing ONNX models in a native inferencing runtime using ONNX Runtime, one of the multiple ONNX backends. Let's delve into it.</p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor241"/>What is model interoperability and how can ONNX help?</h2>
			<p>As an IT organization grows, so<a id="_idIndexMarker1689"/> does the amount of tooling, development, and deployment platforms and choices. In ML, this problem is even more present as there are multiple ML frameworks as well as model serialization formats. Therefore, once the organization grows, it becomes a near-impossible challenge to align every scientist and engineer on the same tooling, frameworks, and model formats that also need to support all your target environments. Does your XGBoost model run on iOS? Does your PyTorch model work in Java? Can your scikit-learn model be loaded in a browser-based JavaScript application? One way to solve this problem of model interoperability is to ensure that trained ML models can be ported to a standardized format that can be executed natively across all target platforms. This is exactly what ONNX is about.</p>
			<p>ONNX is a joint initiative<a id="_idIndexMarker1690"/> from major IT companies such as Microsoft, Facebook, Amazon, ARM, and Intel to facilitate ML model interoperability. It allows organizations to choose different languages, frameworks, and environments for ML training, as well as different languages, environments, and devices for inferencing. As an example, ONNX enables an organization to<a id="_idIndexMarker1691"/> train deep learning models using PyTorch and TensorFlow and traditional ML models using LightGBM and XGBoost, and deploy these models to a Java-based web service, an Objective-C-based iOS application, and a browser-based JavaScript application. This interoperability is enabled through three key ingredients:</p>
			<ul>
				<li><strong class="bold">ONNX specification</strong>: A data format for <em class="italic">efficient serialization and deserialization</em> for model definitions and model weights using <strong class="bold">Protocol Buffers</strong> (<strong class="bold">Protobuf</strong>). To represent a <a id="_idIndexMarker1692"/>wide range of ML models, the ONNX specification is comprised of a definition of an extensible computation graph model, as well as definitions of standard data types and built-in operators. With the ONNX specification, many ML models consisting of a variety of supported architectures, building blocks, operations, and data types can be efficiently represented in a single file, which we call the <em class="italic">ONNX model</em>.</li>
				<li><strong class="bold">ONNX Runtime</strong>: An <a id="_idIndexMarker1693"/>efficient <em class="italic">native inferencing engine</em> with bindings to many higher-level languages, such as C#, Python, JavaScript, Java/Kotlin (Android), and Objective-C (iOS). This means that with the ONNX Runtime bindings for one of these<a id="_idIndexMarker1694"/> languages, we can load, score, and even train ONNX models. It also provides built-in GPU<a id="_idIndexMarker1695"/> acceleration using DirectML, TensorRT, <strong class="bold">Deep Neural Network Library</strong> (<strong class="bold">DNNL</strong>), nGraph, CUDA, and the <strong class="bold">Microsoft Linear Algebra Subprograms</strong> (<strong class="bold">MLAS</strong>) library, and <a id="_idIndexMarker1696"/>weight quantization and graph optimization to run efficiently on various compute targets, such as Cloud Compute, Jupyter kernels, mobile phones, and web browsers.</li>
				<li><strong class="bold">ONNX ecosystem</strong>: A <em class="italic">collection of libraries</em> that facilitate conversion from and to ONNX. ONNX libraries <a id="_idIndexMarker1697"/>can be broadly categorized into ONNX frontends (<em class="italic">to ONNX</em>) and ONNX backends (<em class="italic">from ONNX</em>). While <em class="italic">ONNX frontend</em> libraries help to convert arbitrary computations into ONNX models (models following the ONNX specification), <em class="italic">ONNX backend</em> libraries provide support to execute ONNX models or to convert ONNX models into a specific framework runtime. ONNX is widely used within Microsoft as well as other large companies and, therefore, supports a wide range of frameworks and languages. Many popular libraries are officially supported frontends, such as traditional ML algorithms, scikit-learn, LightGBM, XGBoost, and CatBoost, as well as modern DL frameworks, such as TensorFlow, Keras, PyTorch, Caffe 2, and CoreML.</li>
			</ul>
			<p>ONNX is a great choice<a id="_idIndexMarker1698"/> for providing model interoperability to allow an organization to decouple model training, model serialization, and model inferencing. Let's learn about popular ONNX frontends and backends in action in the next section.</p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor242"/>Converting models to ONNX format with ONNX frontends</h2>
			<p>ONNX frontends are <a id="_idIndexMarker1699"/>packages, tools, or libraries that can convert existing ML models or numeric computations into ONNX models. While popular ML frameworks used to implement ONNX export out of the box (similar to the PyTorch <strong class="source-inline">torch.onnx</strong> module), most frameworks today support ONNX through a separate conversion library. The most popular ONNX frontends at the time of writing are as follows:</p>
			<ul>
				<li><strong class="source-inline">skl2onnx</strong>: Converts scikit-learn models to ONNX</li>
				<li><strong class="source-inline">tf2onnx</strong>: Converts TensorFlow models to ONNX</li>
				<li><strong class="source-inline">onnxmltools</strong>: Converts XGBoost, LightGBM, CatBoost, H2O, libsvm, and CoreML models to ONNX</li>
				<li><strong class="source-inline">torch.onnx</strong>: Converts PyTorch models to ONNX</li>
			</ul>
			<p>Once, the ONNX frontend libraries are installed, the conversion to ONNX specification is often simply done by running a single command. Let's see this in action with TensorFlow as an example:</p>
			<ol>
				<li>First, we will save a Keras model using the TensorFlow <strong class="source-inline">SaveModel</strong> format. We can achieve this by calling <strong class="source-inline">model.save()</strong> and providing the path to serialize the <strong class="source-inline">SaveModel</strong> model to disk:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">train.py</p>
			<p class="source-code">model = create_model()</p>
			<p class="source-code">model.fit(X_train, y_train)</p>
			<p class="source-code"><strong class="bold">model.save('tf_model')</strong></p>
			<ol>
				<li value="2">We can then use the <strong class="source-inline">tf2onnx</strong> library to convert the <strong class="source-inline">SaveModel</strong> model into an ONNX model, as shown in the following snippet:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">convert.sh</p>
			<p class="source-code">python -m <strong class="bold">tf2onnx.convert</strong> \</p>
			<p class="source-code">    --saved-model tf_model \</p>
			<p class="source-code">    --output model.onnx</p>
			<p>As we see in the preceding <a id="_idIndexMarker1700"/>example, all we need is a single command to convert TensorFlow models into ONNX models. Once we have an ONNX model, we can use ONNX backends to score them, as shown in the following section.</p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor243"/>Native scoring of ONNX models with ONNX backends</h2>
			<p>Once a model is exported as <a id="_idIndexMarker1701"/>an ONNX model, we can load it using an ONNX-compatible backend. The reference implementation for the<a id="_idIndexMarker1702"/> ONNX backend is called <strong class="bold">ONNX Runtime</strong>, and is a native implementation <a id="_idIndexMarker1703"/>with bindings in many high-level languages.</p>
			<p>First, we can load, analyze, and check an ONNX model using the <strong class="source-inline">onnx</strong> library, as shown in the following example:</p>
			<p class="source-code">import onnx</p>
			<p class="source-code">model = <strong class="bold">onnx.load("model.onnx")</strong></p>
			<p class="source-code">onnx.checker.check_model(model)</p>
			<p>However, if we want to score the model, we need to use the <strong class="source-inline">onnxruntime</strong> backend library. First, we need to load the model for an inferencing session; this means we can load the optimized model and don't need to allocate any buffers for storing gradients. In the next step, we can score the model by executing <strong class="source-inline">run(output_names, input_feed, run_options=None)</strong>. The <strong class="source-inline">output_names</strong> argument refers to the named output layer we want to return from the model, whereas <strong class="source-inline">input_feed</strong> represents the data we want to pass to the model. The scoring properties, such as the log level, can be configured through the <strong class="source-inline">run_options</strong> argument. The following example shows how to score the model and return the last layer's output from an ONNX model:</p>
			<p class="source-code">import onnxruntime as rt</p>
			<p class="source-code"><strong class="bold">session = rt.InferenceSession("model.onnx")</strong></p>
			<p class="source-code">outputs = session.run(None, {'input': X.values})</p>
			<p>In the preceding code, we load the ONNX model optimized for inferencing, pass data to the model's <strong class="source-inline">input</strong> parameter, and return the last layer's output using the ONNX Runtime Python API. You can access the layer information, as well as names of inputs and outputs, using the helper method, <strong class="source-inline">session.get_modelmeta()</strong>.</p>
			<p>In this section, we learned about ONNX, how to create an ONNX model from trained ML models using ONNX frontends, and how to score an ONNX model using ONNX Runtime, the reference implementation for an ONNX backend. While we looked only at the Python API of ONNX Runtime, many <a id="_idIndexMarker1704"/>other high-level bindings are available.</p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor244"/>Hardware optimization with FPGAs</h1>
			<p>In the previous section, we <a id="_idIndexMarker1705"/>exported a model to ONNX to take advantage of an inference-optimized and hardware-accelerated runtime to improve the scoring performance. In this section, we will take this approach one step further to deploy on even faster inferencing hardware: FPGAs.</p>
			<p>But, before we talk about how to deploy a model to an FPGA, let's first understand what an FPGA is and why we would choose one as a target for DL inference instead of a GPU.</p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor245"/>Understanding FPGAs</h2>
			<p>Most people typically come across a specific<a id="_idIndexMarker1706"/> variety of <strong class="bold">integrated circuit</strong> (<strong class="bold">IC</strong>), called an <strong class="bold">application-specific integrated circuit</strong> (<strong class="bold">ASIC</strong>). ASICs are purpose-built ICs, such <a id="_idIndexMarker1707"/>as the processor in your laptop, the GPU cores on your graphics card, or the microcontroller in your <a id="_idIndexMarker1708"/>washing machine. These chips share the fact that they have a fixed hardware footprint optimized to support a specific task. Often, like any <a id="_idIndexMarker1709"/>general processor, they operate with a specific <strong class="bold">instruction set</strong>, allowing certain commands to be run. When you program something with a higher-level language, such as Java, C++, or Python, the compiler or interpreter will translate this high-level code into machine code, which is the set of commands the processor understands and is able to run.</p>
			<p>The strength of an ASIC is that the underlying chip architecture can be optimized for the specific workload, resulting in the most optimal design for the hardware in terms of the area it requires. The weakness of an ASIC is that it is only good for performing the specific task it was designed for, and its design is fixed, as the underlying hardware cannot be altered. </p>
			<p>Even though we can run any task on a standard processor, for something very specific, such as the computation and backtracking for thousands of nodes in a neural network, they might not be optimal. Therefore, a lot of these calculations are now run on a GPU instead, as its chip architecture leans more toward running the same calculations in parallel, which leans more toward the ingrained structure of a neural network algorithm than a standard CPU would.</p>
			<p>FPGAs are defined by a different concept than their ASIC counterparts. FPGAs trade in the most optimal design, especially when it comes to the used area on a chip, for the freedom of <em class="italic">re-programmability</em>. This main feature allows a user to purchase an FPGA and then build themselves their own processor, a hardware switch, a network router, or anything else, and change the <a id="_idIndexMarker1710"/>underlying hardware design any time they feel like it. </p>
			<p>As hardware in the end is something physical made up of some form of binary logic gates, registers, and wires, this capability of FPGAs might sound like magic. Then again, we are using flash drives daily that can store data and can erase data again. For example, modern <strong class="bold">NAND flash drives</strong> are erased<a id="_idIndexMarker1711"/> through a process called <strong class="bold">field electron emission</strong>, which<a id="_idIndexMarker1712"/> allows a charge to move through a thin layer of insulation to <em class="italic">reset</em> the setting of bits or, to be more precise, blocks of bits.</p>
			<p>Remembering this, let's have <a id="_idIndexMarker1713"/>a look at the basic building blocks of an FPGA, called <strong class="bold">logic elements</strong>. <em class="italic">Figure 15.1</em> shows the general concept of these building blocks. Different manufacturers tweak different aspects of these, but the base concept remains the same:</p>
			<div>
				<div id="_idContainer264" class="IMG---Figure">
					<img src="image/B17928_15_01.jpg" alt="Figure 15.1 – Structure of a logic element in an FPGA " width="1382" height="426"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.1 – Structure of a logic element in an FPGA</p>
			<p>A logic element is typically made up of the following components:</p>
			<ul>
				<li><strong class="bold">Input/output</strong> (<strong class="bold">I/O</strong>): Denotes the<a id="_idIndexMarker1714"/> interconnection with other logical elements or with external I/O (think of Ethernet and USB, for example).</li>
				<li><strong class="bold">Lookup table</strong> (<strong class="bold">LUT</strong>): Holds the main<a id="_idIndexMarker1715"/> logical function performed in this logic element. Any logic in a digital circuit can be broken down to a <strong class="bold">Boolean function</strong> that maps a certain number of binary inputs to a certain number of binary outputs. </li>
				<li><strong class="bold">D-FlipFlop (Register)</strong>: Stores the<a id="_idIndexMarker1716"/> input value of the current <strong class="bold">clock cycle</strong> for the next <a id="_idIndexMarker1717"/>clock cycle, the length of which is the inverse of the <strong class="bold">frequency</strong> of the running circuit. The idea to store something for the next round is the basic principle of all digital hardware and a necessity to be able to do hardware pipelining. The maximum processing time between any adjacent registers in the circuit defines the maximum frequency the circuit can run at.</li>
				<li><strong class="bold">Multiplexer</strong> (<strong class="bold">MUX</strong>): Chooses which of its<a id="_idIndexMarker1718"/> inputs are shown as the output. In this case, it either shows the current result from the Boolean function, or the one from the previous clock cycle.</li>
			</ul>
			<p>Through the LUT, any Boolean<a id="_idIndexMarker1719"/> function (and through a register, any multi-layered hardware logic) can be realized. In addition, the LUT can be erased and reset, which enables the reprogrammable nature of FPGAs.</p>
			<p>The full schematic structure of an FPGA is shown in <em class="italic">Figure 15.2</em>. Just understand that a normal-sized FPGA will have upward of 500,000 logic elements:</p>
			<div>
				<div id="_idContainer265" class="IMG---Figure">
					<img src="image/B17928_15_02.jpg" alt="Figure 15.2 – Schematic structure of an FPGA " width="1650" height="1013"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.2 – Schematic structure of an FPGA</p>
			<p>In addition to <a id="_idIndexMarker1720"/>logic elements, <em class="italic">Figure 15.2</em> shows <strong class="bold">switch matrices</strong> and <strong class="bold">I/O blocks</strong>. Switch matrices are the last piece of the puzzle and allow the setting and resetting of the required connections <a id="_idIndexMarker1721"/>among logic elements, and between them and the I/O blocks. With their help, it is possible<a id="_idIndexMarker1722"/> to fully reprogram the circuit structure on an FPGA.</p>
			<p>Finally, to facilitate the <a id="_idIndexMarker1723"/>programming of an FPGA, a so-called <strong class="bold">hardware description language</strong> (<strong class="bold">HDL</strong>) is used. There are two major languages used for hardware design (be it for FPGAs or ASICs), <strong class="bold">SystemVerilog</strong> and <strong class="bold">VHDL</strong>. When you see code written in these languages, it might look like a high-level programming language, but in reality, you are not programming anything; you are instead <em class="italic">describing</em> the desired hardware architecture. In a sense, you give the machine a picture of a circuit in the form of code, and it tries to map this onto the given elements on the FPGA. This step is called <strong class="bold">synthesis</strong>. After this<a id="_idIndexMarker1724"/> step, a binary is sent to the FPGA that populates the required logic elements with the correct Boolean functions and sets all the interconnections accordingly.</p>
			<p>Besides this logical structure, you will find a lot of other integrated systems in modern FPGAs, combining the strength of ASICs and FPGAs. You might even find a processor such as an <strong class="bold">ARM Cortex</strong> on the IC itself. The<a id="_idIndexMarker1725"/> idea is to let anything that would be extremely time-consuming to build from scratch on the FPGA fabric run on the processor instead while using the FPGA to host your custom hardware designs. For example, it would take a lot of time to build the lower layers of the Ethernet protocol on an FPGA, as TCP requires a highly sophisticated hardware circuit. Therefore, outsourcing this part into a processor can speed up development time immensely. </p>
			<p>Now that we have a<a id="_idIndexMarker1726"/> general idea of what an FPGA is and how it works, let's discuss why they might be more useful for DL than GPUs.</p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor246"/>Comparing GPUs and FPGAs for deep neural networks</h2>
			<p>As we discussed in the previous section, the<a id="_idIndexMarker1727"/> underlying hardware structure of a GPU supports deep neural networks for training and inference. The reason for this is that they are designed with 3D image rendering in mind and, therefore, have a lot of logic on board to facilitate matrix multiplications, a<a id="_idIndexMarker1728"/> task that is extremely time-consuming on CPUs and crucial for DNNs. Through GPUs, the processing time can typically be lowered from days to mere hours. The same can be said for FPGAs, as we can basically build any specialized circuit we require to optimize the speed and power consumption of any tasks we want to perform.</p>
			<p>Therefore, both are options that are far superior for DNNs than general CPUs. But, which one should we choose and why? Let's now go through a list of aspects to consider and how each of these two options fares in both cases:</p>
			<ul>
				<li><strong class="bold">Complexity to implement</strong>: GPUs typically offer a software-level language (for example, CUDA) to disconnect the programmer from the underlying hardware. For FPGAs, the programmer must understand the hardware domain and how to design for it. Therefore, building the correct circuit for an FPGA is far more complicated than just using another library in a high-level programming language. But, there is work being done to abstract this layer as much as possible with specialized tooling and converters.</li>
				<li><strong class="bold">Power consumption</strong>: GPUs produce a lot of heat and require a lot of cooling and electricity. This is because of the additional complexity of the hardware design in order to facilitate software programmability, in turn supporting the base hardware stack of RAM, CPU, and GPU. FPGAs, on the other hand, do not require this stack to operate and, therefore, in most cases, have a low to medium power output, through which they are 4 to 10 times more power-efficient than GPUs.</li>
				<li><strong class="bold">Hardware stack</strong>: GPUs are dependent on the whole memory management of the standard hardware stack (CPU cache, RAM, and GPU memory), and require an external system to control them. This leads to an inefficient but required hardware design for GPUs to facilitate the connection layers to the standard hardware stack, which makes it less performant. FPGAs, on the other hand, have all the required elements (such as high-speed memory) on board the IC and, therefore, can run completely <em class="italic">autonomously</em> without pulling any data from system memory or any other place. </li>
				<li><strong class="bold">Latency and interconnectability</strong>: While GPUs are connected to a standard hardware stack and only have a <a id="_idIndexMarker1729"/>few actual hardware ports at the back of it (HDMI and DisplayPort), which are often only outputs, an FPGA can connect to anything. This means it can <a id="_idIndexMarker1730"/>support vastly different input and output standards at the same time, making it extremely flexible and adaptable to any given situation. In addition, it can process data with very low latency, as no data needs to pass through the system memory, CPU, or SW layer, making it far superior for applications such as real-time video processing. </li>
				<li><strong class="bold">Flexibility</strong>: Even though GPUs have a parallel hardware architecture, you might not be able to use it effectively. The specific DNN algorithm must be mapped to the underlying hardware, and this might be neither perfect nor even feasible. It falls into the same problem class as distributing processes among CPU cores. In addition, GPUs are designed to handle 32-bit or 64-bit standard data types. If you are using a very specialized data type or a custom one, you might not be able to run it on a GPU at all. FPGAs, on the other hand, allow you to define whatever data size or data type you want to work with and, on top of that, allow even a so-called <em class="italic">partial reconfiguration</em> during<a id="_idIndexMarker1731"/> runtime, which it uses to reprogram parts of the logic during runtime.</li>
				<li><strong class="bold">Industry readiness</strong>: In a typical industrial scenario, be it defense, manufacturing, smart cities, or any other, the hardware deployed must be compact, must have a long lifespan, should have low power consumption, should survive the environment it is positioned in (dust, heat, humidity), and in some scenarios, needs to have <em class="italic">functional safety</em>, which means it must follow certain compliance <a id="_idIndexMarker1732"/>standards and protocols. A GPU is a bad choice for any of these circumstances, as it is very power-hungry, has a lifespan of 2 to 5 years, requires massive amounts of cooling, does not survive hostile environments, and does not have<a id="_idIndexMarker1733"/> functional safety. FPGAs were designed with industrial settings in mind and, therefore, are typically built for long life (10 to 30 years) and safety, while having a low footprint on power and required space.</li>
				<li><strong class="bold">Costs</strong>: If you've ever bought a GPU for your PC, you might have an idea of the cost of such an extension card. FPGAs, on the other hand, can be expensive but are typically cheaper to obtain for comparable setup requirements. </li>
			</ul>
			<p>Taking all these points into consideration, FPGAs are technically superior in most ways and often cheaper, but have the major problem that they require developers to understand hardware design. This problem led to the creation of toolkits helping bridge the gap between hardware and ML development, some of which are as follows:</p>
			<ul>
				<li><strong class="bold">Vitis AI for Xilinx FPGAs</strong>: A development kit for<a id="_idIndexMarker1734"/> ML inferencing utilizing pre-designed <strong class="bold">Deep Learning Processor Units</strong> (<strong class="bold">DLUs</strong>). More<a id="_idIndexMarker1735"/> information can be found here: <a href="https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html">https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html</a>. In addition, you can find some information on how to use this with the NP VM series in Azure here: <a href="https://github.com/Xilinx/Vitis-AI/tree/master/docs/azure">https://github.com/Xilinx/Vitis-AI/tree/master/docs/azure</a>.</li>
				<li><strong class="bold">OpenVINO for Intel FPGAs</strong>: A development<a id="_idIndexMarker1736"/> kit for DL and ML inferencing. More information can be found here: <a href="https://www.intel.com/content/www/us/en/artificial-intelligence/programmable/solutions.html">https://www.intel.com/content/www/us/en/artificial-intelligence/programmable/solutions.html</a>. </li>
				<li><strong class="bold">Microsoft Project Brainwave</strong>: A development<a id="_idIndexMarker1737"/> platform for DL and ML inferencing for computer vision and NLP. More information can be found here: <a href="https://www.microsoft.com/en-us/research/project/project-brainwave">https://www.microsoft.com/en-us/research/project/project-brainwave</a>.</li>
			</ul>
			<p>These are just a <a id="_idIndexMarker1738"/>few options to support the<a id="_idIndexMarker1739"/> deployment and acceleration of ML models through FPGAs. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">FPGAs are a very exceptional technology, but they require an ample understanding of hardware design to be used efficiently and successfully in any project, or a very sophisticated toolkit for abstracting the hardware layer.</p>
			<p>Now that we know why we might prefer to take an FPGA for DNNs, let's have a brief look at how FPGAs can be utilized in that regard with Azure Machine Learning.</p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor247"/>Running DNN inferencing on Intel FPGAs with Azure</h2>
			<p>As discussed in<a id="_idIndexMarker1740"/> the previous section, building a hardware design for an FPGA is not an easy task. You could certainly do this from scratch utilizing one of the Azure VMs sporting an FPGA (<a href="https://docs.microsoft.com/en-us/azure/virtual-machines/np-series">https://docs.microsoft.com/en-us/azure/virtual-machines/np-series</a>), or with your own FPGA development kit. Another option is to use the hardware-accelerated Python package that is available in the Azure Machine Learning Python SDK. This package gives you an abstraction layer through a generic hardware design supporting a subset of models and options to use, specifically ones for DNN inferencing. Through this, you have access to the <strong class="bold">Azure PBS VM family</strong>, which has an<a id="_idIndexMarker1741"/> Intel FPGA attached and is only available through Azure Machine Learning. This machine type is deployable in East US, Southeast Asia, West Europe, and West US 2.</p>
			<p>The general approach is very similar to ONNX; you take a trained model and convert it to a specific format that can be executed on FPGAs. In this case, your model must be either ResNet, DenseNet, VGG, or SSD-VGG, and must be written in TensorFlow in order to fit the underlying hardware design. Furthermore, we will use quantized 16-bit float model weights converted to ONNX models, which will be run on the FPGA. For these models, FPGAs give you the best inference performance in the cloud. </p>
			<p>To enable hardware acceleration through FPGAs, we require a few extra steps compared to the ONNX example. The following list shows what steps need to be performed:</p>
			<ol>
				<li value="1">Pick a supported model featurizer.</li>
				<li>Train the supported model with a custom classifier.</li>
				<li>Quantize the model featurizer's weights to 16-bit precision.</li>
				<li>Convert the model to an ONNX format.</li>
				<li>(Optional) Register the model.</li>
				<li>Create a compute target (preferably Azure Kubernetes Service) with PBS nodes.</li>
				<li>Deploy the model.<p class="callout-heading">Important Note</p><p class="callout">As the code is cluttered and hard to interpret, we will skip the code examples in this section. However, you can find detailed examples of FPGA model training, conversion, and deployments on Azure's GitHub repository at <a href="https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/deployment/accelerated-models">https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/deployment/accelerated-models</a>.</p></li>
			</ol>
			<p>Let's discuss these <a id="_idIndexMarker1742"/>steps in some more detail. </p>
			<p>From the DNN layers we discussed in <a href="B17928_10_ePub.xhtml#_idTextAnchor165"><em class="italic">Chapter 10</em></a>, <em class="italic">Training Deep Neural Networks on Azure</em>, only the feature <a id="_idIndexMarker1743"/>extractor layers (<strong class="bold">featurizers</strong>) will be hardware-accelerated for inferencing. In order to run a model on an FPGA, you need to pick a supported model from the <strong class="source-inline">azureml.accel.models</strong> package (<a href="https://docs.microsoft.com/en-us/python/api/azureml-accel-models/azureml.accel.models">https://docs.microsoft.com/en-us/python/api/azureml-accel-models/azureml.accel.models</a>). You can attach any classification or regression head (or both) on top using TensorFlow or Keras, but they will not be hardware-accelerated, similar to running only certain operations on GPUs. The designers opted here to deploy only the most time-consuming parts onto the FPGA.</p>
			<p>In the next step, you can train the model, consisting of a predefined feature extractor and a custom classification head, using your own data and weights, or by fine-tuning, for example, provided ImageNet weights. This should happen with 32-bit precision, as convergence will be faster during training.</p>
			<p>Once the training is finished, you need to quantize the weights of the featurizer into half-precision floats, using the quantized models provided in the <strong class="source-inline">azureml.accel.models</strong> package. This step needs to be done because the designers opted here for a fixed data size of 16-bit in order to make the hardware design as generic and reusable as possible.</p>
			<p>For the next step, you convert the whole model into an ONNX model, using the <strong class="source-inline">AccelOnnxConverter</strong> method from the same Azure package. In addition, the <strong class="source-inline">AccelContainerImage</strong> class helps you to define <strong class="source-inline">InferenceConfig</strong> for the FPGA-based compute targets.</p>
			<p>Finally, you can<a id="_idIndexMarker1744"/> register your model using the Azure Machine Learning model registry, and you can create an AKS cluster using the <strong class="source-inline">Standard_PB6s</strong> nodes. Once the cluster is up and running, you use your <strong class="source-inline">Webservice.deploy_from_image</strong> method to deploy the web service.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can find a detailed example of the deployment steps in the Azure Machine Learning documentation here: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-fpga-web-service">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-fpga-web-service</a>. </p>
			<p>The workflow to deploy a model through Azure Machine Learning to an FPGA-based compute target is a bit different from simply deploying ONNX models, as you have to consider the limited supported selection of models right from the beginning. Another difference is that, while you choose a predefined supported model for FPGA deployment, you can only accelerate the feature extractor part of the model. This means you have to attach an additional classification or regression head—a step that is not immediately obvious. Once you understand this, it will make more sense that you only quantize the feature extractor to half-precision floats after training.</p>
			<p>While this process seems a bit difficult and customized, the performance and latency gain, especially when dealing with predictions on image data, is huge. But, you should take advantage of this optimization only if you are ready to adapt your training processes and pipelines to this specific environment, as shown throughout the section.</p>
			<p>Now that we have a good understanding of what FPGAs are and how we can utilize them through Azure Machine Learning, let's have a look in the next section at what other Azure services we can<a id="_idIndexMarker1745"/> integrate with our models.</p>
			<h1 id="_idParaDest-249"><a id="_idTextAnchor248"/>Integrating ML models and endpoints with Azure services</h1>
			<p>Relying on the Azure Machine Learning service either<a id="_idIndexMarker1746"/> for experimentation, performing end-to-end training, or simply registering your trained models and environments brings you a ton of value. In <a href="B17928_14_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 14</em></a>, <em class="italic">Model Deployment, Endpoints, and Operations</em>, we covered two main scenarios, a real-time scoring web service through automated deployments and batch scoring through a deployed pipeline. While these two use cases are quite different in requirement and deployment types, they show what is possible once you have a trained model and packaged environment stored in Azure Machine Learning. In this section, we will discuss how to use and integrate these models or their endpoints in other Azure services.</p>
			<p>In many scenarios, abstracting your batch-scoring pipeline from the actual data processing pipeline to separate concerns and responsibilities makes a lot of sense. However, sometimes your scoring should happen directly during the data processing or querying time and in the same system. Once your ML model is registered and versioned with Azure Machine Learning, you can pull out a specific version of the model anywhere using the Azure ML SDK, either in Python, C#, the command line, or any other language that can make a call to a REST service.</p>
			<p>This makes it possible to pull trained and converted ONNX models from a desktop application, either during build time or at runtime. You can load models while running a Spark job, for example, on Azure Databricks or Azure Synapse. Through that, you can avoid transferring TBs of data to a separate scoring service. </p>
			<p>Other services, such as Azure Data Explorer, allow you to call models directly from the service through a Python extension (<a href="https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/pythonplugin">https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/pythonplugin</a>). Azure Data Explorer is an exciting managed service for storing and querying large amounts of telemetry data efficiently. It is used internally at Azure to power Azure Log Analytics, Azure Application Insights, and Time Series Insights. It has a powerful Python runtime with many popular packages available, and so provides the perfect service for performing anomaly detection or time-series analysis based on your custom models. In addition, it allows you to access its time-series data during ML modeling through a<a id="_idIndexMarker1747"/> Python<a id="_idIndexMarker1748"/> extension called <strong class="bold">Kqlmagic</strong> (<a href="https://docs.microsoft.com/en-us/azure/data-explorer/kqlmagic">https://docs.microsoft.com/en-us/azure/data-explorer/kqlmagic</a>).</p>
			<p class="callout-heading">Important Note </p>
			<p class="callout">When using Azure Machine Learning for model deployments, you can take advantage of all the Azure ecosystem and can expect to see model or endpoint integration with more and more Azure services over time.</p>
			<p>Closing this chapter, we will dive deeper into two other integration options in the upcoming sections. We will have a look at <a id="_idIndexMarker1749"/>deploying ML models through <strong class="bold">Azure IoT Edge</strong> to a gateway or device in the field, and we will look at how to utilize ML endpoints for data augmentation in <strong class="bold">Power BI</strong>.</p>
			<h2 id="_idParaDest-250"><a id="_idTextAnchor249"/>Integrating with Azure IoT Edge</h2>
			<p>So far, we have discussed <a id="_idIndexMarker1750"/>different ways to make our models run on systems in the cloud, be it on machines with CPUs, GPUs, or FPGAs, either as a batch-scoring process or as a real-time endpoint. Now, let's discuss another interesting deployment scenario, deploying real-time scorers to one to up to hundreds of thousands of devices in the field. The control of such devices and the processing of gathered telemetry and events fall under the<a id="_idIndexMarker1751"/> topic of the so-called <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>), which enables us to react in near real time to changes and critical problems in any sort of environment. </p>
			<p>In these scenarios, the integration of ML allows us to distribute a model to a multitude of systems and devices simultaneously, allowing<a id="_idIndexMarker1752"/> these so-called <strong class="bold">edge devices</strong> to execute the model on the local runtime in order to react to the result of the ML processing accordingly. This could be a local camera system that performs ML-powered image processing to react to intruders and send out alarms or any other scenario you might imagine.</p>
			<p>To get a base<a id="_idIndexMarker1753"/> understanding of how to achieve this utilizing the Azure platform, let's first have a look at how IoT scenarios are realized through the help of <strong class="bold">Azure IoT Hub</strong> and other services, and then discuss how this can be integrated with Azure Machine Learning and our trained models.</p>
			<h3>Understanding IoT solutions on Azure</h3>
			<p>The basis for any IoT<a id="_idIndexMarker1754"/> architecture in Azure is Azure IoT Hub. This serves as a cloud gateway to communicate with devices and other gateways in the field and offers the ability to control them to a certain extent. On the one hand, it runs Azure Event Hubs underneath to be able to handle a huge amount of incoming telemetry through a distributed structure, not too different from Apache Kafka. On the other hand, it serves as a control instrument serving the following functions:</p>
			<ul>
				<li><strong class="bold">Device cataloging</strong>: The ledger of all <a id="_idIndexMarker1755"/>devices registered to Azure IoT Hub. Any device connected receives its own device name and connection configuration, defining how the direct connection between hub and device is secured, which happens using either a rotating key or a device certificate. </li>
				<li><strong class="bold">Device provisioning</strong>: A service that allows devices to automatically register themselves to IoT Hub to obtain either a connection string with a key or a certificate. Useful if more than a handful of devices must be registered.</li>
				<li><strong class="bold">Device twin</strong>: A configuration file that defines important properties for the device, which can be set or requested. In between the stream of telemetry, the device is asked to send this file sporadically, updating the state of the device in the cloud gateway. Therefore, the device twin always holds the most recent state of the device. This functionality is automatically implemented when using the <strong class="bold">Azure IoT device SDK</strong> on the device.</li>
				<li><strong class="bold">Command and control</strong>: This is enabled through the <strong class="bold">Azure IoT Service SDK</strong>. Commands from a console or an external <a id="_idIndexMarker1756"/>application can be used to either send new desired properties to single devices, define configurations for a group of devices, or <a id="_idIndexMarker1757"/>send a predefined command that the device needs to understand and implement. This could be a request to restart the device or flash its firmware. </li>
				<li><strong class="bold">Monitoring and diagnostics</strong>: A diagnostic view on any incoming and outgoing messaging from and to IoT Hub. It can be used to understand the throughput of incoming telemetry, understand any control plane information exchanged, and warn if a device is unreachable and malfunctioning.</li>
			</ul>
			<p>In addition to this cloud gateway, Azure offers a device runtime on the edge called Azure IoT Edge, which can be installed on a device or gateway. It is powered by the Moby Docker<a id="_idIndexMarker1758"/> runtime (<a href="https://mobyproject.org/">https://mobyproject.org/</a>), which allows users to deploy Docker containers to a device in the field. The setup of any solution operating in this runtime is defined by a <strong class="bold">deployment manifest</strong> that is set up for an edge device through a device twin configuration file in IoT Hub. This manifest defines the following components:</p>
			<ul>
				<li><strong class="bold">IoT Edge agent</strong>: Verifies and<a id="_idIndexMarker1759"/> instantiates modules, checks<a id="_idIndexMarker1760"/> their state during runtime, and reports back any configuration or runtime problem utilizing the device twin configuration file. It is the main module of the runtime and is <em class="italic">required</em>.</li>
				<li><strong class="bold">IoT Edge hub</strong>: Enables the IoT Edge runtime to mimic IoT Hub for additional devices connecting to this local edge <a id="_idIndexMarker1761"/>device. This enables any form of complex hierarchy, while devices can use the same protocol communicating with an IoT Edge device as they would with IoT Hub. This module is <em class="italic">required</em>.</li>
				<li><strong class="bold">Container modules</strong>: Defines the <a id="_idIndexMarker1762"/>container images to be copied to the edge runtime. This is done by defining a link to the source files stored in Azure Container Registry. Besides any user-defined container that can be deployed in this manner, there are also a bunch of containerized versions of Azure services that can be sent to the runtime. This list includes Blob storage, an Azure Function app, certain Cognitive Services, and even a small, optimized version of a SQL server <a id="_idIndexMarker1763"/>called <strong class="bold">SQL Edge</strong>.</li>
				<li><strong class="bold">Local communication via routing</strong>: Defines the <a id="_idIndexMarker1764"/>first option to connect modules together by setting direct connections between inputs and outputs of the various modules defined before.</li>
				<li><strong class="bold">Local communication via an MQTT broker</strong>: Defines the second option to connect modules together. Instead of<a id="_idIndexMarker1765"/> setting direct connections, a broker is used to which modules can subscribe. This broker also offers connections to external devices that understand how to talk to an MQTT broker.</li>
			</ul>
			<p>These are the main components and<a id="_idIndexMarker1766"/> options to consider when defining the deployment manifest. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The greatest strength that Azure IoT Edge brings to the table is the ability to define, manage, and version containers in the cloud, and deploy them to thousands of devices. With the help of device configurations, we can group devices and only target a certain group for a new test update, thus enabling best practices for DevOps in an IoT setting. </p>
			<p>Now, let's briefly have a look at an example. <em class="italic">Figure 15.3</em> shows a simple setup for scoring a containerized ML model on incoming telemetry through Azure IoT Edge and its connection with Azure IoT Hub:</p>
			<div>
				<div id="_idContainer266" class="IMG---Figure">
					<img src="image/B17928_15_03.jpg" alt="Figure 15.3 – Azure IoT Hub connecting to the edge runtime " width="860" height="376"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.3 – Azure IoT Hub connecting to the edge runtime</p>
			<p>The connections in <em class="italic">Figure 15.3</em> show the internal routing between containers, including actioning that takes place locally, while any insights from the ML scoring and any initial telemetry are sent additionally to the <a id="_idIndexMarker1767"/>cloud for further analysis. This is the typical scenario for any ML model operating on the edge.</p>
			<p>With this knowledge in mind, let's now have a look at how to integrate Azure Machine Learning in such an IoT architecture.</p>
			<h3>Integrating Azure Machine Learning</h3>
			<p>In <a href="B17928_03_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 3</em></a>, <em class="italic">Preparing the Azure Machine Learning Workspace</em>, we learned that every Azure Machine Learning workspace comes with its own Azure Container Registry. We can now use this registry to <a id="_idIndexMarker1768"/>achieve our goal. <em class="italic">Figure 15.4</em> shows an example of an end-to-end solution for ML on the edge:</p>
			<div>
				<div id="_idContainer267" class="IMG---Figure">
					<img src="image/B17928_15_04.jpg" alt="Figure 15.4 – End-to-end ML on Azure IoT Edge scenario " width="757" height="327"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.4 – End-to-end ML on Azure IoT Edge scenario</p>
			<p>It depicts the following steps:</p>
			<ol>
				<li value="1">Collecting telemetry in a storage account, either through routing single messages from IoT Hub or through a batch upload from the Blob storage on the edge to the storage account in the cloud</li>
				<li>Training an ML model on the captured data as we learned previously</li>
				<li>Registering a container including the trained model and dependencies in the existing Azure Container Registry of the Azure Machine Learning workspace</li>
				<li>Creating an IoT Edge <a id="_idIndexMarker1769"/>deployment manifest defining an ML module sourced fromAzure Container Registry</li>
				<li>Deploying the created configuration through Azure IoT Hub to the edge device</li>
			</ol>
			<p>Through this setup, we are now able to deploy and control an ML model on the edge, enabling vast scenarios for running low-latency ML solutions on external devices.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">If you are interested to try this out, feel free to follow the tutorial for setting up an example ML model on Azure IoT Edge, found here: <a href="https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-machine-learning-edge-01-intro">https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-machine-learning-edge-01-intro</a>.</p>
			<p>Finally, if you are<a id="_idIndexMarker1770"/> interested in further options for ML solutions on the edge, have a look at one of the newest additions to the Azure IoT space, called <strong class="bold">Azure Percept</strong> (<a href="https://azure.microsoft.com/en-us/services/azure-percept/">https://azure.microsoft.com/en-us/services/azure-percept/</a>). It offers a<a id="_idIndexMarker1771"/> ready-made hardware development kit for video and audio inferencing that works together with Azure IoT Hub and Azure Machine Learning.</p>
			<p>Now that we've had a glimpse into the world of IoT and scenarios for ML on the edge, let's have a look at how to utilize real-time ML endpoints with Power BI.</p>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor250"/>Integrating with Power BI</h2>
			<p>One of the most interesting<a id="_idIndexMarker1772"/> integrations from an enterprise perspective is the Azure Machine Learning integration with Power BI. It allows us to utilize our ML endpoints to apply our models to data columns from the comfort of the built-in <strong class="bold">Power Query editor</strong>. Think for a second<a id="_idIndexMarker1773"/> how powerful this concept of rolling out ML models to be used by data analysts in their BI tools is.</p>
			<p>Let's try this out by utilizing the <strong class="source-inline">sentiment-analysis-pbi</strong> endpoint we created in <a href="B17928_14_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 14</em></a>, <em class="italic">Model Deployment, Endpoints, and Operations</em>, by following these steps:</p>
			<ol>
				<li value="1">If you haven't done so already, download the Power BI Desktop application (<a href="https://powerbi.microsoft.com/en-gb/desktop/">https://powerbi.microsoft.com/en-gb/desktop/</a>) to your machine, run it, and log in.</li>
				<li>Download the <strong class="source-inline">sentiment_examples.csv</strong> file from the chapter repository, and select <strong class="bold">Get Data</strong> | <strong class="bold">Text/CSV</strong> to load the content of this local file into an in-memory dataset in Power BI.</li>
				<li>The Power Query editor will open and will show you an icon of the file with the name and size. Right-click on that, and select <strong class="bold">Text</strong>.</li>
				<li>You should be greeted by a table with one column. Rename the column <strong class="source-inline">Phrases</strong>, as shown in <em class="italic">Figure 15.5</em>:</li>
			</ol>
			<div>
				<div id="_idContainer268" class="IMG---Figure">
					<img src="image/B17928_15_05.jpg" alt="Figure 15.5 – Sample phrases for sentiment analysis " width="524" height="286"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.5 – Sample phrases for sentiment analysis</p>
			<ol>
				<li value="5">The editor gives you a lot of<a id="_idIndexMarker1774"/> possibilities to apply transformations to this data. Looking at the menu, you should see a button on the far-right side called <strong class="bold">Azure Machine Learning</strong>. Click on it.</li>
				<li>If you are logged in correctly, you should see all available endpoints in all the Azure Machine Learning workspaces you have access to. Select our previously created endpoint, <strong class="source-inline">AzureML.sentiment-analysis-pbi</strong>. In the <strong class="bold">query</strong> field, select the <strong class="source-inline">Phrases</strong> column. This will be the input for our ML endpoint. <em class="italic">Figure 15.6</em> shows what this should look like:</li>
			</ol>
			<div>
				<div id="_idContainer269" class="IMG---Figure">
					<img src="image/B17928_15_06.jpg" alt="Figure 15.6 – Choosing the right ML endpoint in Power BI " width="654" height="272"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.6 – Choosing the right ML endpoint in Power BI</p>
			<ol>
				<li value="7">Click on <strong class="bold">OK</strong>. Power BI will now start sending the request to the endpoint. Please be aware that you might get a warning in one of the Power BI windows concerning data <a id="_idIndexMarker1775"/>privacy, as we are sending potentially private data to another service. Please accept this by selecting the first checkbox, so the action can be performed.</li>
				<li>As a result, you should now see a new column called <strong class="source-inline">AzureML.sentiment-analysis-pbi</strong>, with a lot of fields denoted as <strong class="source-inline">Record</strong>. As our endpoints send more than one output, we receive a record. You can now click on each record individually, or you can click on the small button showing two arrows next to the column header name. This allows you to expand this <strong class="source-inline">Record</strong> column into multiple ones. Select all column names and press <strong class="bold">OK</strong>. <em class="italic">Figure 15.7</em> shows the result you should see:</li>
			</ol>
			<div>
				<div id="_idContainer270" class="IMG---Figure">
					<img src="image/B17928_15_07.jpg" alt="Figure 15.7 – Power BI sentiment results " width="897" height="282"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.7 – Power BI sentiment results</p>
			<p>As we can see, the model gives a label for each sentence (<strong class="source-inline">NEGATIVE</strong> or <strong class="source-inline">POSITIVE</strong>) and a confidence value score, denoting how sure the ML model is about the label given. The results are reasonably accurate, except perhaps for the fourth phrase.</p>
			<ol>
				<li value="9">You can now click <strong class="bold">Close &amp; Apply</strong> in the upper left-hand corner, which will result in <a id="_idIndexMarker1776"/>Power BI creating an ML-enhanced dataset, with which you could now build visuals in a report and eventually publish a report to the Power BI service in the cloud.</li>
			</ol>
			<p>As you can see for yourself, integrating with Power BI is a quick and easy way to empower everyone to utilize your deployed ML endpoints with their business data, while not understanding much about the inner workings of the ML services.</p>
			<p>Feel free to add some of your own phrases to play around with.</p>
			<h1 id="_idParaDest-252"><a id="_idTextAnchor251"/>Summary</h1>
			<p>In this chapter, we learned how to convert ML models into a portable and executable format with ONNX, what an FPGA is, and how we can deploy a DNN featurizer to an FPGA VM through Azure Machine Learning. In addition, we learned how to integrate our ML models into various Azure services, such as Azure IoT Edge and Power BI.</p>
			<p>This concludes our discussion through the previous two chapters on the various options to deploy ML models for batch or real-time inferencing. </p>
			<p>In the next chapter, we will bring everything we learned so far together to understand and build an end-to-end MLOps pipeline, enabling us to create an enterprise-ready and automated environment for any kind of process that requires the addition of ML.</p>
		</div>
	</div>
</div>
</body></html>