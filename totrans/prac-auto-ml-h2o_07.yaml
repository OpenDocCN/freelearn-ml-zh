- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Working with Model Explainability
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用模型可解释性
- en: The justification of model selection and performance is just as important as
    model training. You can have *N* trained models using different algorithms, and
    all of them will be able to make good enough predictions for real-world problems.
    So, how do you select one of them to be used in your production services, and
    how do you justify to your stakeholders that your chosen model is better than
    the others, even though all the other models were also able to make accurate predictions
    to some degree? One answer is performance metrics, but as we saw in the previous
    chapter, there are plenty of performance metrics and all of them measure different
    types of performance. Choosing the correct performance metric boils down to the
    context of your ML problem. What else can we use that will help us choose the
    right model and also further help us in justifying this selection?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择和性能的合理性与模型训练同样重要。您可以使用*N*个使用不同算法训练的模型，并且所有这些模型都将能够对现实世界问题做出足够好的预测。那么，您如何从它们中选择一个用于生产服务，以及您如何向您的利益相关者证明您选择的模型比其他模型更好，尽管其他模型也能在一定程度上做出准确的预测？一个答案是性能指标，但正如我们在上一章中看到的，有大量的性能指标，并且它们衡量不同类型的性能。选择正确的性能指标归结于您的机器学习问题的上下文。我们还能使用什么来帮助我们选择正确的模型，并进一步帮助我们证明这一选择是合理的？
- en: The answer to that is visual graphs. Human beings are visual creatures and,
    as such, a picture speaks a thousand words. A good graph can explain more about
    a model than any metric number. The versatility of graphs can be very useful in
    explaining the model’s behavior and how it fits as a solution to our ML problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是可视化图表。人类是视觉生物，因此，一张图片胜过千言万语。一个好的图表可以比任何指标数字更好地解释一个模型。图表的多样性在解释模型的行为以及它如何作为我们机器学习问题的解决方案方面非常有用。
- en: H2O’s explainability interface is a unique feature that wraps over various explainability
    features and visuals that H2O auto-computes for a model or list of models, including
    the H2O AutoML object.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: H2O的可解释性界面是一个独特功能，它封装了H2O为模型或模型列表自动计算的各种可解释性功能和可视化，包括H2O AutoML对象。
- en: In this chapter, we shall explore the H2O explainability interface and how it
    works with the H2O AutoML object. We shall also implement a practical example
    to understand how to use the explainability interface in Python and R. Finally,
    we shall go through and understand all the various explainability features that
    we get as outputs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨H2O可解释性界面及其与H2O AutoML对象的工作方式。我们还将实现一个实际示例，以了解如何在Python和R中使用可解释性界面。最后，我们将了解并理解我们作为输出获得的所有各种可解释性功能。
- en: 'In this chapter, we are going to cover the following main topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主要主题：
- en: Working with the model explainability interface
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型可解释性界面
- en: Exploring the various explainability features
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索各种可解释性功能
- en: By the end of this chapter, you should have a good idea of how to interpret
    model performance by looking at the various performance metrics described by the
    model explainability interface.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您应该对如何通过查看模型可解释性界面描述的各种性能指标来解释模型性能有一个很好的了解。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will require the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您需要以下内容：
- en: The latest version of your preferred web browser.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您首选网络浏览器的最新版本。
- en: An **Integrated Development Environment** (**IDE**) of your choice or a Terminal.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您选择的**集成开发环境**（**IDE**）或终端。
- en: All the experiments conducted in this chapter are performed on a Terminal. You
    are free to follow along using the same setup or you can perform the same experiments
    using any IDE of your choice.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章中进行的所有实验都是在终端上进行的。您可以根据相同的设置自由跟随，或者您可以使用您选择的任何IDE执行相同的实验。
- en: All the code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%207](https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%207).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码示例都可以在GitHub上找到，网址为[https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%207](https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%207)。
- en: So, let’s begin by understanding how the model explainability interface works.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们首先了解模型可解释性界面是如何工作的。
- en: Working with the model explainability interface
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用模型可解释性界面
- en: 'The **model explainability interface** is a simple function that incorporates
    various graphs and information about the model and its workings. There are two
    main functions for model explainability in H2O:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型可解释性接口** 是一个简单的函数，它包含有关模型及其工作方式的多种图表和信息。H2O 中有两个主要的模型可解释性函数：'
- en: The `h2o.explain()` function, which is used to explain the model’s behavior
    on the entire test dataset. This is also called **global explanation**.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h2o.explain()` 函数，用于解释整个测试数据集中模型的行为。这也被称为 **全局解释**。'
- en: The `h2o.explain_row()` function, which is used to explain the model’s behavior
    on an individual row in the test dataset. This is also called **local explanation**.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h2o.explain_row()` 函数，用于解释测试数据集中单个行的模型行为。这也被称为 **局部解释**。'
- en: Both these functions work on either a single H2O model object, a list of H2O
    model objects, or the H2O AutoML object. These functions generate a list of results
    that consists of various graphical plots such as a **variable importance graph**,
    **partial dependency graph**, and a **leaderboard** if used on multiple models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数可以在单个 H2O 模型对象、H2O 模型对象列表或 H2O AutoML 对象上工作。这些函数生成一个包含各种图形图表的结果列表，例如 **变量重要性图**、**部分依赖图**和
    **排行榜**（如果用于多个模型）。
- en: 'For graphs and other visual results, the `explain` object relies on visualization
    engines to render the graphics:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图表和其他视觉结果，`explain` 对象依赖于可视化引擎来渲染图形：
- en: For the R interface, H2O uses the `ggplot2` package for rendering.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 R 接口，H2O 使用 `ggplot2` 包进行渲染。
- en: For the Python interface, H2O uses the `matplotlib` package for rendering.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Python 接口，H2O 使用 `matplotlib` 包进行渲染。
- en: With this in mind, we need to make sure that whenever we are using the explainability
    interface to get visual graphs, we run it in an environment that supports graph
    rendering. This interface won’t be of much use in Terminals and other non-graphical
    command-line interfaces. The examples in this chapter have been run on **Jupyter
    Notebook**, but any environment that supports plot rendering should work fine.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我们需要确保每次我们使用可解释性接口获取视觉图表时，都在支持图形渲染的环境中运行。此接口在终端和其他非图形命令行界面中用处不大。本章中的示例是在
    **Jupyter Notebook** 中运行的，但任何支持绘图渲染的环境都应该可以正常工作。
- en: 'The explainability function has the following parameters:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性函数具有以下参数：
- en: '`newdata`/`frame`: This parameter is used to specify the H2O test DataFrame
    needed to compute some of the explainability features such as the `newdata`, while
    the same in the Python explainability interface is `frame`.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`newdata`/`frame`: 此参数用于指定计算某些可解释性特征（如 `newdata`）所需的 H2O 测试 DataFrame，而在 Python
    可解释性接口中则是 `frame`。'
- en: '`columns`: This parameter is used to specify the columns to be considered in
    column-based explanations such as **individual conditional expectation plots**
    or **partial dependency plots**.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`columns`: 此参数用于指定在基于列的解释中要考虑的列，例如 **个体条件期望图** 或 **部分依赖图**。'
- en: '`top_n_features`: This parameter is used to specify the number of columns to
    be considered based on the feature importance ranking for column-based explanations.
    The default value is `5`.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_n_features`: 此参数用于指定基于特征重要性排名进行基于列的解释时需要考虑的列数。默认值为 `5`。'
- en: Either the `columns` parameter or the `top_n_features` parameter will be considered
    by the explainability function. Preference is given to the `columns` parameter,
    so if both the parameters are passed with values, then `top_n_features` will be
    ignored.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性函数将考虑 `columns` 参数或 `top_n_features` 参数。优先考虑 `columns` 参数，因此如果两个参数都传递了值，则
    `top_n_features` 将被忽略。
- en: '`include_explanations`: This parameter is used to specify the explanations
    that you want from the explainability function’s output.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_explanations`: 此参数用于指定您希望从可解释性函数输出中获取的解释。'
- en: '`exclude_explanations`: This parameter is used to specify the explanations
    that you do not want from the explainability function’s output. `include_explanations`
    and `exclude_explanations` are mutually exclusive parameters. The available values
    for both parameters are as follows:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exclude_explanations`: 此参数用于指定您不希望从可解释性函数输出中获取的解释。`include_explanations` 和
    `exclude_explanations` 是互斥参数。这两个参数的有效值如下：'
- en: '`leaderboard`: This value is only valid for the list of models or the AutoML
    object.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`leaderboard`: 此值仅适用于模型列表或 AutoML 对象。'
- en: '`residual_analysis`: This value is only valid for regression models.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`residual_analysis`: 此值仅适用于回归模型。'
- en: '`confusion_matrix`: This value is only valid for classification models.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`confusion_matrix`：这个值仅对分类模型有效。'
- en: '`varimp`: This value stands for variable importance and is only valid for base
    models, not for stacked ensemble models.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`varimp`：这个值代表变量重要性，仅对基础模型有效，对堆叠集成模型无效。'
- en: '`varimp_heatmap`: This value stands for heatmap of variable importance.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`varimp_heatmap`：这个值代表变量重要性的热图。'
- en: '`model_correlation_heatmap`: This value stands for heatmap of model correlation.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_correlation_heatmap`：这个值代表模型相关性的热图。'
- en: '`shap_summary`: This value stands for Shapley additive explanations.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shap_summary`：这个值代表Shapley加性解释。'
- en: '`pdp`: This value stands for partial dependency plots.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pdp`：这个值代表部分依赖图。'
- en: '`ice`: This value stands for individual conditional expectation plots.'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ice`：这个值代表单个条件期望图。'
- en: '`plot_overrides`: This parameter is used to override the values for individual
    explanation plots. This parameter is useful if you want the top 10 features to
    be considered for one plot but specific columns for another:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plot_overrides`：此参数用于覆盖单个解释图的值。如果您想在一个图中考虑前10个特征，但在另一个图中考虑特定列，则此参数很有用：'
- en: '[PRE0]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`object`: This parameter is used to specify the H2O models or the H2O AutoML
    object, which we will cover shortly. This parameter is specific to the R explainability
    interface.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`object`：此参数用于指定H2O模型或H2O AutoML对象，我们将在稍后介绍。此参数是R可解释性接口特有的。'
- en: Now that we know how the explainability interface works and what its various
    parameters are, let’s understand it better with an implementation example.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了可解释性接口的工作原理及其各种参数，让我们通过一个实现示例来更好地理解它。
- en: We shall use **Fisher’s Iris flower dataset**, which we used in [*Chapter 1*](B17298_01.xhtml#_idTextAnchor017),
    *Understanding H2O AutoML Basics*, to train models using AutoML. We will then
    use the explainability interface on the AutoML object to display all the explainability
    features it has to provide.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**费舍尔的鸢尾花数据集**，我们在[*第1章*](B17298_01.xhtml#_idTextAnchor017)，“理解H2O AutoML基础知识”中使用它来使用AutoML训练模型。然后我们将使用AutoML对象上的可解释性接口来显示它提供的所有可解释性特征。
- en: So, let’s start by implementing it in Python.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从在Python中实现它开始。
- en: Implementing the model explainability interface in Python
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中实现模型可解释性界面
- en: 'To implement the model explainability function in Python, follow these steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python中实现模型可解释性函数，请按照以下步骤操作：
- en: 'Import the `h2o` library and spin up a local H2O server:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`h2o`库并启动本地H2O服务器：
- en: '[PRE1]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The explainability interface performs heavy computations behind the scenes to
    calculate the data needed to plot the graphs. To speed up processing, it is recommended
    to initialize the H2O server with as much memory as you can allocate.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性界面在后台进行大量计算，以计算绘制图表所需的数据。为了加快处理速度，建议使用尽可能多的内存初始化H2O服务器。
- en: 'Import the dataset using `h2o.importFile(“Dataset/iris.data”)`:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`h2o.importFile(“Dataset/iris.data”)`导入数据集：
- en: '[PRE2]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Set which columns are the features and which columns are the labels:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置哪些列是特征列，哪些列是标签列：
- en: '[PRE3]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Remove the label from among the features:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从特征中移除标签：
- en: '[PRE4]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Split the DataFrame into training and testing DataFrames:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将DataFrame拆分为训练集和测试集：
- en: '[PRE5]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Initialize the H2O AutoML object:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化H2O AutoML对象：
- en: '[PRE6]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Trigger the H2O AutoML object so that it starts auto-training the models:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 触发H2O AutoML对象，使其开始自动训练模型：
- en: '[PRE7]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once training has finished, we can use the H2O explainability interface, `h2o.explain()`,
    on the now-trained `aml` object:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们就可以使用H2O可解释性接口`h2o.explain()`对现在训练好的`aml`对象进行操作：
- en: '[PRE8]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `explain` function will take some time to finish computing. Once it does,
    you should see a big output that lists all the explainability features. The output
    should look as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`explain`函数将花费一些时间来完成计算。一旦完成，您应该看到一个列出所有可解释性特征的输出。输出应该如下所示：'
- en: '![Figure 7.1 – Model explainability interface output ](img/B17298_07_001.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 模型可解释性界面输出](img/B17298_07_001.jpg)'
- en: Figure 7.1 – Model explainability interface output
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 模型可解释性界面输出
- en: 'You can also use the `h2o.explain_row()` interface to display the model explainability
    features for a single row of the dataset:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以使用`h2o.explain_row()`接口来显示数据集单行的模型可解释性特征：
- en: '[PRE9]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output of this should give you a leaderboard of the models making predictions
    on the first row of the dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出应为您提供对数据集第一行进行预测的模型的排行榜。
- en: 'To get additional information about the model from an explainability point
    of view, you can further extend the explainability interface by using the `explain_row()`
    function on the leader model, as follows:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要从可解释性角度获取关于模型的更多信息，你可以通过在领先模型上使用 `explain_row()` 函数进一步扩展可解释性界面，如下所示：
- en: '[PRE10]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The output of this should give you all the applicable graphical model explainability
    features for that model based on its predictions on that row.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出应该会给出基于该行预测的所有适用的图形模型可解释性功能。
- en: Now that we know how to use the model explainability interface in Python, let’s
    see how we can use this interface in the R Language.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何在 Python 中使用模型可解释性界面，让我们看看我们如何在 R 语言中使用这个界面。
- en: Implementing the model explainability interface in R
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 R 中实现模型可解释性界面
- en: Similar to how we implemented the explainability interface in Python, H2O has
    provisions to use the explainability interface in the R programming language as
    well.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在 Python 中实现可解释性界面类似，H2O 也提供了在 R 编程语言中使用可解释性界面的方法。
- en: 'To implement the model explainability function in R, follow these steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 R 中实现模型可解释性功能，请按照以下步骤操作：
- en: 'Import the `h2o` library and spin up a local H2O server:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `h2o` 库并启动本地 H2O 服务器：
- en: '[PRE11]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Import the dataset using `h2o.importFile(“Dataset/iris.data”)`:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `h2o.importFile(“Dataset/iris.data”)` 导入数据集：
- en: '[PRE12]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Set the `C5` column as the label:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `C5` 列设置为标签：
- en: '[PRE13]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Split the DataFrame into training and testing DataFrames and assign them to
    the appropriate variables:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 DataFrame 分割为训练和测试 DataFrame，并将它们分配给适当的变量：
- en: '[PRE14]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Run the H2O AutoML training:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 H2O AutoML 训练：
- en: '[PRE15]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Use the H2O explainability interface on the now-trained `aml` object:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在现在已训练的 `aml` 对象上使用 H2O 可解释性界面：
- en: '[PRE16]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Once the explainability object finishes its computation, you should see a big
    output that lists all the explainability features.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦可解释性对象完成其计算，你应该会看到一个列出所有可解释性功能的大输出。
- en: 'Just like Python, you can also extend the model explainability interface function
    so that it can be run on a single row using the `h2o.explain_row()` function,
    as follows:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像 Python 一样，你还可以扩展模型可解释性界面函数，使其可以通过使用 `h2o.explain_row()` 函数在单行上运行，如下所示：
- en: '[PRE17]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This will give you the leaderboard of models making predictions on the first
    row of the dataset.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出在数据集的第一行上进行预测的模型的排行榜。
- en: 'Similarly, you can expand this explainability interface by using the `h2o.explain_row()`
    function on the leader model to get more advanced information about the leader
    model:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，你可以通过在领先模型上使用 `h2o.explain_row()` 函数来扩展这个可解释性界面，以获取关于领先模型的更高级信息：
- en: '[PRE18]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In these examples, we have used the Iris flower dataset to solve a multinomial
    classification problem. Similarly, we can use the explainability interface on
    trained regression models. Some of the explainability features are only available
    depending on whether the trained model is a regression model or a classification
    model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些示例中，我们使用了 Iris 花数据集来解决多项分类问题。同样，我们可以在训练好的回归模型上使用可解释性界面。一些可解释性功能仅根据训练模型是回归模型还是分类模型而可用。
- en: Now that we know how to implement the model explainability interface in Python
    and R, let’s look deeper into the output of the interface and try to understand
    the various explainability features that H2O computed.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何在 Python 和 R 中实现模型可解释性界面，让我们更深入地查看界面的输出，并尝试理解 H2O 计算的各种可解释性功能。
- en: Exploring the various explainability features
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索各种可解释性功能
- en: The output of the explainability interface is an `H2OExplanation` object. The
    `H2OExplanation` object is nothing but a simple dictionary with the explainability
    features’ names as keys. You can retrieve individual explainability features by
    using a feature’s key name as a `dict` key on the explainability object.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性界面的输出是一个 `H2OExplanation` 对象。`H2OExplanation` 对象不过是带有可解释性功能名称作为键的简单字典。你可以通过使用特征的关键名称作为可解释性对象的
    `dict` 键来检索单个可解释性功能。
- en: If you scroll down the output of the explainability interface for the H2O AutoML
    object, you will notice that there are plenty of headings with explanations. Below
    these headings, there’s a brief description of what the explainability feature
    is. Some have graphical diagrams, while others may have tables.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你向下滚动 H2O AutoML 对象的可解释性界面输出，你会注意到有很多带有解释的标题。在这些标题下面，是对可解释性功能的简要描述。一些包含图形图表，而其他可能包含表格。
- en: 'The various explainability features are as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 各种可解释性功能如下：
- en: '**Leaderboard**: This feature is a leaderboard comprising all trained models
    and their basic metrics ranked from best performing to worst. This feature is
    computed only if the explainability interface is run on the H2O AutoML object
    or list of H2O models.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排行榜**：这是一个排行榜，包括所有训练模型及其从最佳性能到最差性能的基本指标。此功能仅在解释性界面在H2O AutoML对象或H2O模型列表上运行时计算。'
- en: '**Confusion Matrix**: This feature is a performance metric that generates a
    matrix that keeps track of correct and incorrect predictions of a classification
    model. It is only available for classification models. For multiple models, the
    confusion matrix is only calculated for the leader model.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混淆矩阵**：这是一个性能指标，它生成一个矩阵，跟踪分类模型的正确和错误预测。它仅适用于分类模型。对于多个模型，混淆矩阵仅针对领先模型计算。'
- en: '**Residual Analysis**: This feature plots the predicted values against the
    residuals on the test dataset used in the explainability interface. It only analyzes
    the leader model based on the model ranking on the leaderboard. It is only available
    for regression models. For multiple models, residual analysis is performed on
    the leader model.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差分析**：此功能在用于解释性界面的测试数据集上绘制预测值与残差的关系。它仅分析基于排行榜模型排名的领先模型。它仅适用于回归模型。对于多个模型，残差分析仅在领先模型上执行。'
- en: '**Variable Importance**: This feature plots the importance of variables in
    the dataset. It is available for all models except for stacked models. For multiple
    models, it is only performed on the leader model, which is not a stacked model.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变量重要性**：此功能绘制了数据集中变量的重要性。它适用于所有模型，除了堆叠模型。对于多个模型，它仅在领先模型上执行，该模型不是堆叠模型。'
- en: '**Variable Importance Heatmap**: This feature plots a heatmap of variable importance
    across all the models. It is available for comparing all models except stacked
    models.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变量重要性热图**：此功能绘制了所有模型中变量重要性的热图。它适用于比较所有模型，除了堆叠模型。'
- en: '**Model Correlation Heatmap**: This feature plots the correlation between the
    predicted values of different models. This helps group together models with similar
    performance. It is only available for multiple model explanations.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型相关性热图**：此功能绘制了不同模型预测值之间的相关性。这有助于将具有相似性能的模型分组在一起。它仅适用于多个模型解释。'
- en: '**SHAP Summary of Top Tree-Based Model**: This feature plots the importance
    of variables in contributing to the decision-making that’s done by complex tree-based
    models such as Random Forest and neural networks. This feature computes this plot
    for the top-ranking tree-based model in the leaderboard.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顶级树模型SHAP摘要**：此功能绘制了变量在复杂树模型（如随机森林和神经网络）的决策制定中的重要性。此功能为排行榜中排名最高的树模型计算此图。'
- en: '**Partial Dependence Multi Plots**: This feature plots the dependency between
    the target feature and a certain set of features in the dataset that we consider
    important.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部分依赖多图**：此功能绘制了目标特征与数据集中我们认为重要的某些特征集之间的依赖关系。'
- en: '**Individual Conditional Expectation** (**ICE**) **Plots**: This feature plots
    the dependency between the target feature and a certain set of features in the
    dataset that we consider important for each instance separately.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个体条件期望**（**ICE**）**图**：此功能绘制了目标特征与数据集中我们认为对每个实例单独重要的某些特征集之间的依赖关系。'
- en: Comparing this to the output you got from the model explainability interface
    in the experiment we performed in the *Working with the model explainability interface*
    section, you will notice that some of the explainability features are missing
    from the output. This is because some of these features are only available to
    the type of model trained. For example, residual analysis is only available for
    regression models, while the experiment conducted in the *Working with the model
    explainability interface* section is a classification problem that trained a classification
    model. Hence, you won’t find residual analysis in the model’s explainability output.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与我们在“使用模型解释性界面”部分进行的实验中从模型解释性界面获得的输出进行比较，您会注意到输出中缺少一些解释性功能。这是因为其中一些功能仅适用于训练的模型类型。例如，残差分析仅适用于回归模型，而我们在“使用模型解释性界面”部分进行的实验是一个分类问题，训练了一个分类模型。因此，您在模型的解释性输出中找不到残差分析。
- en: You can perform the same experiment using a regression problem; the model explainability
    interface will output regression-supported explainability features.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用回归问题进行相同的实验；模型可解释性界面将输出回归支持的可解释性特征。
- en: Now that we know about the different explainability features that are available
    in the explanation interface, let’s dive deep into them one by one to get an in-depth
    understanding of what they mean. We shall go through the output we got from our
    implementation of the explainability interface in Python and R.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了在解释界面中可用的不同可解释性特征，那么让我们逐一深入探讨它们，以深入了解它们的含义。我们将通过我们在Python和R中实现的可解释性界面的输出进行说明。
- en: 'In the previous chapters, we understood what the leaderboard and confusion
    matrix are. So, let’s start with the next explanation feature: residual analysis.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们了解了排行榜和混淆矩阵是什么。那么，让我们从下一个解释特征：剩余分析开始。
- en: Understanding residual analysis
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解剩余分析
- en: '**Residual analysis** is performed for **regression models**. As described
    in [*Chapter 5*](B17298_05.xhtml#_idTextAnchor109), *Understanding AutoML Algorithms*,
    in the *Understanding generalized linear models* and *Introduction to linear regression*
    sections, **residuals** are the difference between the values predicted by the
    regression model and the actual values for that same row of data. Analyzing these
    residual values is a great way of diagnosing any problems in your model.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**剩余分析**是对**回归模型**进行的。如[*第五章*](B17298_05.xhtml#_idTextAnchor109)中所述，“理解AutoML算法”部分，在“理解广义线性模型”和“线性回归简介”部分，**残差**是回归模型预测的值与同一行数据的实际值之间的差异。分析这些残差值是诊断模型中任何问题的绝佳方式。'
- en: A residual analysis plot is a graph where you plot the **residual values** against
    the **predicted values**. Another thing we learned in [*Chapter 5*](B17298_05.xhtml#_idTextAnchor109),
    *Understanding AutoML Algorithms*, in the *Understanding generalized linear models*
    and *Understanding the assumptions of linear regression* sections, is that one
    of the primary assumptions in **linear regression** is that the distribution of
    residuals is **normally distributed**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余分析图是你在其中绘制**残差值**与**预测值**的图表。我们在[*第五章*](B17298_05.xhtml#_idTextAnchor109)中学习到，在“理解广义线性模型”和“理解线性回归的假设”部分，线性回归的一个主要假设是残差的分布是**正态分布**。
- en: So, accordingly, we expect our residual plot to be an amorphous collection of
    points. There should not be any patterns between the residual values and the predicted
    values.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，相应地，我们期望我们的残差图是一个无定形的点集。残差值和预测值之间不应有任何模式。
- en: Residual analysis can highlight the presence of **heteroscedasticity** in a
    trained model. Heteroscedasticity is said to have occurred if the standard deviation
    of the predicted values changes over different values of the features.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余分析可以突出训练模型中**异方差性**的存在。如果预测值的标准差在不同特征值上发生变化，则称为发生了异方差性。
- en: 'Consider the following diagram:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下图表：
- en: '![Figure 7.2 – Regression graph for a homoscedastic dataset ](img/B17298_07_002.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 同方差数据集的回归图](img/B17298_07_002.jpg)'
- en: Figure 7.2 – Regression graph for a homoscedastic dataset
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 同方差数据集的回归图
- en: The preceding diagram shows a regression plot where we had some sample data
    that maps the relationship between *X* and *Y*. Let’s fit a straight line through
    this data, which represents our linear model. If we calculate the residuals for
    every point as we go from left to right on the *X*-axis, we will notice that the
    error rate remains fairly constant throughout all the values of *X*. This means
    that all the error values lie between the parallel blue lines. Such a situation
    where the distribution of errors or residuals is constant throughout the independent
    variables is called homoscedasticity.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了一个回归图，其中我们有一些样本数据，这些数据映射了*X*和*Y*之间的关系。让我们通过这些数据拟合一条直线，这代表我们的线性模型。如果我们从*X*轴的左侧到右侧计算每个点的残差，我们会注意到误差率在整个*X*值的范围内保持相对恒定。这意味着所有误差值都位于平行蓝色线之间。在整个独立变量中，误差或残差的分布保持恒定的情况称为同方差性。
- en: 'The opposite of homoscedasticity is **heteroscedasticity**. This is where the
    error rate varies over the change in the value of *X*. Refer to the following
    diagram:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**同方差性**的相反面是**异方差性**。这是指误差率随着*X*值的变化而变化。参考以下图表：'
- en: '![Figure 7.3 – Regression graph for a heteroscedastic dataset ](img/B17298_07_003.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 异方差数据集的回归图](img/B17298_07_003.jpg)'
- en: Figure 7.3 – Regression graph for a heteroscedastic dataset
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 异方差数据集的回归图
- en: As you can see, the magnitude of the errors made by the linear model increases
    with an increase in *X*. If you plot the blue error lines encompassing all the
    errors, then you will notice that they gradually fan out and are not parallel.
    This situation where the distribution of errors or residuals is not constant throughout
    the independent variables is called heteroscedasticity.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，线性模型所犯错误的幅度随着X的增加而增加。如果你绘制包含所有错误的蓝色误差线，那么你会注意到它们逐渐扩散开来，并且不是平行的。这种在整个自变量中误差或残差分布不恒定的情况称为异方差性。
- en: What heteroscedasticity tells us is that there is some sort of information that
    the model has not been able to capture and learn from. Heteroscedasticity also
    violates linear regressions’ basic assumption. Thus, it can help you identify
    that you may need to add the missing information to your dataset to correctly
    train your linear model or that you may need to implement some non-linear regression
    algorithm to get a better-performing model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 异方差性告诉我们，模型未能捕捉和学习的某些信息。异方差性也违反了线性回归的基本假设。因此，它可以帮助你确定你可能需要将缺失信息添加到你的数据集中以正确训练你的线性模型，或者你可能需要实现一些非线性回归算法以获得性能更好的模型。
- en: Since residual analysis is a regression-specific model explainability feature,
    we cannot use the Iris dataset classification experiment that we performed in
    the *Working with the model explainability interface* section. Instead, we need
    to train a regression model and then use the model explainability interface on
    that model to get the residual analysis output. So, let’s look at a regression
    problem using the Red Wine Quality dataset. You can find this dataset at [https://archive.ics.uci.edu/ml/datasets/wine+quality](https://archive.ics.uci.edu/ml/datasets/wine+quality).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于残差分析是回归特定的模型可解释性特征，我们不能使用我们在“使用模型可解释性界面”部分进行的Iris数据集分类实验。相反，我们需要训练一个回归模型，然后在该模型上使用模型可解释性界面来获取残差分析输出。因此，让我们看看使用红葡萄酒质量数据集的回归问题。您可以在[https://archive.ics.uci.edu/ml/datasets/wine+quality](https://archive.ics.uci.edu/ml/datasets/wine+quality)找到这个数据集。
- en: 'This dataset consists of the following features:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包括以下特征：
- en: '**fixed acidity**: This feature explains the amount of acidity that is non-volatile,
    meaning it does not evaporate over some time.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**固定酸度**: 这个特征解释了非挥发性酸度的含量，意味着它不会在一段时间内蒸发。'
- en: '**volatile acidity**: This feature explains the amount of acidity that is volatile,
    meaning it will evaporate over some time.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**挥发性酸度**: 这个特征解释了挥发性酸度的含量，意味着它会在一段时间内蒸发。'
- en: '**citric acid**: This feature explains the amount of citric acid present in
    the wine.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**柠檬酸**: 这个特征解释了葡萄酒中存在的柠檬酸含量。'
- en: '**residual sugar**: This feature explains the amount of residual sugar present
    in the wine.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残余糖分**: 这个特征解释了葡萄酒中存在的残余糖分含量。'
- en: '**chlorides**: This feature explains the number of chlorides present in the
    wine.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**氯化物**: 这个特征解释了葡萄酒中存在的氯化物数量。'
- en: '**free sulfur dioxide**: This feature explains the amount of free sulfur dioxide
    present in the wine.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**游离二氧化硫**: 这个特征解释了葡萄酒中存在的游离二氧化硫含量。'
- en: '**total sulfur dioxide**: This feature explains the amount of total sulfur
    dioxide present in the wine.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总二氧化硫**: 这个特征解释了葡萄酒中存在的总二氧化硫含量。'
- en: '**density**: This feature explains the density of the wine.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密度**: 这个特征解释了葡萄酒的密度。'
- en: '**pH**: This feature explains the pH value of the wine, with 0 being the most
    acidic and 14 being the most basic.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pH值**: 这个特征解释了葡萄酒的pH值，其中0表示最酸，14表示最碱。'
- en: '**sulphates**: This feature explains the number of sulfates present in the
    wine.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硫酸盐**: 这个特征解释了葡萄酒中存在的硫酸盐数量。'
- en: '**alcohol**: This feature explains the amount of alcohol present in the wine.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**酒精含量**: 这个特征解释了葡萄酒中存在的酒精含量。'
- en: '**quality**: This is the response column that notes the quality of the wine.
    0 indicates that the wine is very bad, while 10 indicates that the wine is excellent.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量**: 这是响应列，表示葡萄酒的质量。0表示葡萄酒非常差，而10表示葡萄酒非常好。'
- en: We will run our basic H2O AutoML process of training the model and then use
    the model explainability interface on the trained AutoML object to get the residual
    analysis plot.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行我们的基本H2O AutoML流程来训练模型，然后使用训练好的AutoML对象上的模型可解释性界面来获取残差分析图。
- en: 'Now, let’s observe the residual analysis plot that we get from this implementation
    and then see how we can retrieve the required information from the graph. Refer
    to the following diagram:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们观察从这个实现中得到的残差分析图，然后看看我们如何从图中检索所需的信息。参考以下图表：
- en: '![Figure 7.4 – Residual analysis graph plot for the Red Wine Quality dataset
    ](img/B17298_07_004.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 红酒质量数据集的残差分析图](img/B17298_07_004.jpg)'
- en: Figure 7.4 – Residual analysis graph plot for the Red Wine Quality dataset
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 红酒质量数据集的残差分析图
- en: Here, you can see the residual analysis for the stacked ensemble model, which
    is the leader of the AutoML trained models. On the *X*-axis, you have **Fitted**,
    also called predicted values, while on the *Y*-axis, you have **Residuals**.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到堆叠集成模型的残差分析，这是AutoML训练模型的领导者。在*X*轴上，你有**拟合值**，也称为预测值，而在*Y*轴上，你有**残差**。
- en: On the left border of the *Y*-axis and below the *X*-axis you will see a **grayscale**
    column and row, respectively. These help you observe the distribution of those
    residuals across the *X* and *Y* axes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Y*轴的左侧和*X*轴下方，你会看到一个**灰度**列和一个行，分别。这些帮助你观察这些残差在*X*和*Y*轴上的分布。
- en: To ensure that the distribution of residuals is normal and that the data is
    not heteroskedastic, you need to observe this grayscale on the *Y*-axis. A normal
    distribution would ideally give you a grayscale that is the darkest at the center
    and lightens as it moves away.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保残差的分布是正态的，并且数据不是异方差性的，你需要观察*Y*轴上的这个灰度。理想的正态分布会给你一个在中心最暗，向外逐渐变亮的灰度。
- en: 'Now that you understood how to interpret the residual analysis graph, let’s
    learn more about the next explainability feature: variable importance.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何解读残差分析图，让我们进一步学习下一个可解释性特征：变量重要性。
- en: Understanding variable importance
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解变量重要性
- en: '**Variable importance**, also called **feature importance**, as the name suggests,
    explains the importance of the different variables/features in the dataset in
    making predictions. In any ML problem, your dataset will often have multiple variables
    that contribute to the characteristics of your prediction column. However, in
    most cases, you will often have some features that contribute more compared to
    others.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**变量重要性**，也称为**特征重要性**，正如其名，解释了数据集中不同变量/特征在预测中的重要性。在任何机器学习问题中，你的数据集通常会有多个变量，这些变量会影响你的预测列的特征。然而，在大多数情况下，你通常会有一些特征比其他特征贡献更多。'
- en: This understanding can help scientists and engineers remove any unwanted features
    that introduce noise from the dataset. This can further improve the quality of
    the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这种理解可以帮助科学家和工程师从数据集中移除任何引入噪声的不需要的特征。这可以进一步提高模型的质量。
- en: H2O calculates variable importance differently for different types of algorithms.
    First, let’s understand how variable importance is calculated for **tree-based
    algorithms**.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: H2O为不同类型的算法计算变量重要性不同。首先，让我们了解变量重要性是如何在**基于树的算法**中计算的。
- en: 'Variable importance in tree-based algorithms is calculated based on two criteria:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的算法中的变量重要性是根据两个标准计算的：
- en: Selection of the variable for deciding on the decision tree
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树变量选择
- en: Improvement in the squared error over the whole tree because of the selection
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于选择而导致的整个树上的平方误差改进
- en: Whenever H2O is building a decision tree as a part of training a tree-based
    model, it will use one of the features as a node to further split the tree. As
    we studied in [*Chapter 5*](B17298_05.xhtml#_idTextAnchor109), *Understanding
    AutoML Algorithms*, in the *Understanding the Distributed Random Forest algorithm*
    section, we know that every node split in the decision tree aims to reduce the
    overall squared error. This deducted value is nothing but the difference between
    the squared errors of the parent node against the children node.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 每当H2O在训练基于树的模型时构建决策树，它将使用一个特征作为节点来进一步分割树。正如我们在[*第五章*](B17298_05.xhtml#_idTextAnchor109)“理解AutoML算法”部分所研究的，在“理解分布式随机森林算法”部分，我们知道决策树中的每个节点分割都旨在减少整体平方误差。这个扣除的值就是父节点与子节点平方误差之间的差异。
- en: H2O considers this reduction in squared error in calculating the feature importance.
    The squared error for every node in the tree-based model leads to the variance
    of the response value for that node being lowered. The squared error for every
    node in the tree-based model leads to the variance of the response value for that
    node being lowered.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: H2O在计算特征重要性时考虑了平方误差的减少。基于树的模型中每个节点的平方误差导致该节点的响应值方差降低。
- en: 'Thus, accordingly, the equation for calculating the squared error of the tree
    is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，相应地，计算树平方误差的方程如下：
- en: '![](img/Formula_B17298_07_001.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![img/Formula_B17298_07_001.jpg](img/Formula_B17298_07_001.jpg)'
- en: 'Here, we have the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: '*MSE* means mean squared error'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MSE* 表示均方误差'
- en: '*N* indicates the total number of observations'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N* 表示观测值的总数'
- en: '*VAR* means variance'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VAR* 表示方差'
- en: 'The equation for calculating variance is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 计算方差的公式如下：
- en: '![](img/Formula_B17298_07_002.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![img/Formula_B17298_07_002.jpg](img/Formula_B17298_07_002.jpg)'
- en: 'Here, we have the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: '![](img/Formula_B17298_07_003.png) indicates the value of the observation'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![img/Formula_B17298_07_003.png](img/Formula_B17298_07_003.png) 表示观测值的数值'
- en: '![](img/Formula_B17298_07_004.png) indicates the mean of all the observations'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![img/Formula_B17298_07_004.png](img/Formula_B17298_07_004.png) 表示所有观测值的平均值'
- en: '*N* indicates the total number of observations'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N* 表示观测值的总数'
- en: For tree-based ensemble algorithms such as the **Gradient Boosting Algorithm**
    (**GBM**), the decision trees are trained sequentially. Every tree is built on
    top of the previous tree’s errors. So, the feature importance calculation is the
    same as how we do it for individual nodes in single decision trees.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于树的集成算法，如 **梯度提升算法**（**GBM**），决策树是顺序训练的。每一棵树都是建立在上一棵树的错误之上的。因此，特征重要性计算与我们在单个决策树中的单个节点计算相同。
- en: For **Distributed Random Forest** (**DRF**), the decision trees are trained
    in parallel, so H2O just averages the results to calculate the feature importance.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **分布式随机森林**（**DRF**），决策树是并行训练的，因此H2O只是平均结果来计算特征重要性。
- en: For **XGBoost**, H2O calculates the feature importance from the gains of the
    loss function for individual features when building the tree.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **XGBoost**，H2O在构建树时，从单个特征的损失函数增益中计算特征重要性。
- en: For **deep learning**, H2O calculates the feature importance using a special
    method called the **Gedeon method**.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **深度学习**，H2O使用一种称为 **Gedeon方法** 的特殊方法来计算特征重要性。
- en: For **Generalized Linear Models** (**GLMs**), variable importance is the same
    as the predictor weights, also called the coefficient magnitudes. If, during training,
    you decide to standardize the data, then the standardized coefficients are returned.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **广义线性模型**（**GLMs**），变量重要性与预测权重相同，也称为系数幅度。如果在训练过程中，你决定标准化数据，则返回标准化系数。
- en: 'The following diagram shows the feature importance that was calculated for
    our experiment on the Iris flower dataset:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了我们对爱丽丝花数据集进行的实验中计算的特征重要性：
- en: '![Figure 7.5 – Variable importance graph for the Iris flower dataset ](img/B17298_07_005.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 7.5 – 爱丽丝花数据集的变量重要性图](img/B17298_07_005.jpg)'
- en: Figure 7.5 – Variable importance graph for the Iris flower dataset
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 爱丽丝花数据集的变量重要性图
- en: The preceding diagram shows the variable importance map for a deep learning
    model. If you compare it with your leaderboard, you will see that the variable
    importance graph is plotted for the most leading model, which is not a stacked
    ensemble model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了深度学习模型的变量重要性图。如果你与你的排行榜进行比较，你会看到变量重要性图是为最领先的模型绘制的，这并不是一个堆叠集成模型。
- en: On the *Y*-axis of the graph, you have the feature names – in our case, the
    **C1**, **C2**, **C3**, and **C4** columns of the Iris flower dataset. On the
    *X*-axis, you have the importance of these variables. It is possible to get the
    raw metric value of feature importance, but H2O displays the importance values
    by scaling them down between **0** and **1**, where **1** indicates the most important
    variable while **0** indicates the least important variable.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表的 *Y* 轴上，你有特征名称 – 在我们的案例中，是爱丽丝花数据集的 **C1**、**C2**、**C3** 和 **C4** 列。在 *X*
    轴上，你有这些变量的重要性。可以得到特征重要性的原始指标值，但H2O通过将其缩放到 **0** 到 **1** 之间来显示重要性值，其中 **1** 表示最重要的变量，而
    **0** 表示最不重要的变量。
- en: 'Since variable importance is available for both classification and regression
    models, you will also get a variable importance graph as an explainability feature
    of the Red Wine Quality regression model. The graph should look as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 由于变量重要性对分类和回归模型都可用，你也会在红酒质量回归模型的可解释性功能中获得一个变量重要性图。图表应该如下所示：
- en: '![Figure 7.6 – Variable importance graph for the Red Wine Quality dataset  ](img/B17298_07_006.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 红酒质量数据集的变量重要性图](img/B17298_07_006.jpg)'
- en: Figure 7.6 – Variable importance graph for the Red Wine Quality dataset
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 红酒质量数据集的变量重要性图
- en: Now that you know how to interpret a feature importance graph, let’s understand
    feature importance heatmaps.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何解释特征重要性图，让我们来理解特征重要性热图。
- en: Understanding feature importance heatmaps
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解特征重要性热图
- en: When displaying feature importance for a specific model, it is fairly easy to
    represent it as a histogram or bar graph. However, we often need to compare the
    feature importance of various models so that we can understand which feature is
    deemed important by which model and how we can use this information to compare
    model performance. H2O AutoML will inherently train multiple models with different
    ML algorithms. Therefore, a comparative study of the model performance is a must
    and a graphical representation of feature importance can be of great help to scientists
    and engineers.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当显示特定模型的特征重要性时，将其表示为直方图或条形图相对容易。然而，我们经常需要比较不同模型的特征重要性，以便了解哪个模型认为哪些特征是重要的，以及我们如何利用这些信息来比较模型性能。H2O
    AutoML会自动训练多个具有不同机器学习算法的模型。因此，对模型性能的比较是必不可少的，而特征重要性的图形表示对科学家和工程师来说非常有帮助。
- en: To represent the feature importance of all the models trained by H2O AutoML
    in a single graph, H2O generates a heatmap of feature importance.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在单个图表中表示H2O AutoML训练的所有模型的特征重要性，H2O生成一个特征重要性热图。
- en: A heatmap is a data visualization graph where the color of the graph is affected
    by the amount of density or magnitude of a specific value.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 热图是一种数据可视化图表，其中图表的颜色受特定值的密度或大小的影响。
- en: Some H2O models compute the variable importance on encoded versions of categorical
    columns. Different models also have different ways of encoding categorical values.
    So, comparing the variable importance of these categorical columns across all
    the models can be tricky. H2O does this comparison by summarizing the variable
    importance across all the features and returning a single variable importance
    value that represents the original categorical feature.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一些H2O模型会在分类列的编码版本上计算变量重要性。不同的模型也有不同的编码分类值的方式。因此，比较所有模型中这些分类列的变量重要性可能会很棘手。H2O通过汇总所有特征上的变量重要性，并返回一个代表原始分类特征的单一变量重要性值来完成这个比较。
- en: 'The following is the feature importance heatmap for the Iris flower dataset
    experiment:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为鸢尾花数据集实验生成的特征重要性热图：
- en: '![Figure 7.7 – Variable importance heatmap ](img/B17298_07_007.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – 变量重要性热图](img/B17298_07_007.jpg)'
- en: Figure 7.7 – Variable importance heatmap
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 变量重要性热图
- en: Here, we can see the top 10 models on the leaderboard.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到排行榜上排名前10的模型。
- en: The heatmap has the **C1**, **C2**, **C3**, and **C4** features on the *Y*-axis
    and the model IDs on the *X*-axis. The color of the plots indicates how important
    the model considers the feature during its prediction. More importance equals
    more value, which, in turn, turns the respective plot red. The lower the importance,
    the lower the importance value of the feature will be; the color will become cooler
    and become blue.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 热图在Y轴上具有**C1**、**C2**、**C3**和**C4**特征，而在X轴上则是模型ID。图表的颜色表示模型在预测过程中认为特征的重要程度。重要性越高，价值越大，相应的图表就会变成红色。重要性越低，特征的重要性值就越低；颜色会变得更冷，变成蓝色。
- en: Now that you know how to interpret feature importance heatmaps, let’s learn
    about model correlation heatmaps.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何解释特征重要性热图，让我们来学习模型相关性热图。
- en: Understanding model correlation heatmaps
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解模型相关性热图
- en: Another important comparison between multiple models is **model correlation**.
    Model correlation can be interpreted as how similar the models are in terms of
    performance when you compare their prediction values.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 多个模型之间另一个重要的比较是**模型相关性**。模型相关性可以理解为当你比较它们的预测值时，模型在性能上的相似程度。
- en: Different models trained using the same or different ML algorithms are said
    to be highly correlated if the predictions made by one model are the same or similar
    to the predictions made by the other.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型做出的预测与另一个模型做出的预测相同或相似，那么使用相同或不同机器学习算法训练的不同模型被认为是高度相关的。
- en: In a model correlation heatmap, H2O compares the prediction values of all the
    models that it trains and compares them to one another.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型相关性热图中，H2O 比较了它所训练的所有模型的预测值，并将它们相互比较。
- en: 'The following is the model correlation heatmap graph we got from our experiment
    on the Iris flower dataset:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们从鸢尾花数据集实验中得到的模型相关性热图：
- en: '![Figure 7.8 – Model correlation heatmap for the Iris flower dataset ](img/B17298_07_008.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 鸢尾花数据集的模型相关性热图](img/B17298_07_008.jpg)'
- en: Figure 7.8 – Model correlation heatmap for the Iris flower dataset
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 鸢尾花数据集的模型相关性热图
- en: Tip
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: To understand this explainability feature graph, kindly refer to the *Model
    Correlation* section in the output you got after executing the `explain()` function
    in your code.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解这个可解释性特征图，请参考您在代码中执行 `explain()` 函数后得到的输出中的 *模型相关性* 部分。
- en: On the *X* and *Y* axes, we have the model IDs. Their cross-section on the graph
    indicates the correlation value between them. You will notice that the heat points
    on the graph within the *X* and *Y* axes have the same model ID, which will always
    be 1; therefore, the plot will always be red. This is correct as, technically,
    it’s the same model and when you compare the prediction values of a model with
    itself, there will be 100% correlation.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *X* 和 *Y* 轴上，我们有模型 ID。它们在图上的截面表示它们之间的相关性值。您会注意到图上 *X* 和 *Y* 轴之间的热点具有相同的模型
    ID，这始终是 1；因此，图表始终是红色的。这是正确的，因为技术上它们是同一个模型，当您比较一个模型的预测值与自身的预测值时，会有 100% 的相关性。
- en: To get a better idea of the correlation between different models, you can refer
    to these heat values. Dark red points indicate high correlation, while those with
    cool blue values indicate low correlation. Models highlighted in red are interpretable
    models such as GLMs.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地了解不同模型之间的相关性，您可以参考这些热值。深红色点表示高度相关，而那些具有凉爽蓝色值的点表示低相关性。用红色突出显示的模型是可解释模型，如
    GLM。
- en: You may notice that since the model correlation heatmap supports stacked ensemble
    models and feature importance heatmaps don’t, if you ignore the stacked ensemble
    models in the model correlation heatmap (*Figure 7.8*), the rest of the models
    are the same as the ones in the feature importance heatmap (*Figure 7.7*).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到，由于模型相关性热图支持堆叠集成模型，而特征重要性热图不支持，如果您忽略模型相关性热图（*图 7.8*）中的堆叠集成模型，其余的模型与特征重要性热图（*图
    7.7*）中的模型相同。
- en: Now that you know how to interpret model correlation heatmaps, let’s learn more
    about partial dependency plots.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经知道了如何解释模型相关性热图，让我们再来了解部分依赖图。
- en: Understanding partial dependency plots
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解部分依赖图
- en: A **Partial Dependence Plot** (**PDP**) is a graph diagram that shows you the
    dependency between the predicted values and the set of input features that we
    are interested in while marginalizing the values of features in which we are not
    interested.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**部分依赖图**（**PDP**）是一个图表，展示了在边际化我们不感兴趣的输入特征值的情况下，预测值与我们感兴趣的输入特征集之间的依赖关系。'
- en: Another way of understanding a PDP is that it represents a function of input
    features that we are interested in that gives us the expected predicted values
    as output.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种理解 PDP 的方法是，它表示了一个输入特征的函数，该函数给出了预期的预测值作为输出。
- en: A PDP is a very interesting graph that is useful in showing and explaining the
    model training results to members of the organization that are not so skilled
    in the domain of data science.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: PDP 是一个非常有趣的图表，有助于向那些不太擅长数据科学领域的组织成员展示和解释模型训练结果。
- en: 'First, let’s understand how to interpret a DPD before learning how it is calculated.
    The following diagram shows the PDP we got for our experiment when using the Iris
    flower dataset:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习如何计算它之前，让我们先了解如何解释一个 DPD。以下是我们使用鸢尾花数据集进行实验时得到的 PDP 图：
- en: '![Figure 7.9 – PDP for the C1 column with Iris-setosa as the target ](img/B17298_07_009.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 以 Iris-setosa 为目标的 C1 列的 PDP](img/B17298_07_009.jpg)'
- en: Figure 7.9 – PDP for the C1 column with Iris-setosa as the target
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 以 Iris-setosa 为目标的 C1 列的 PDP
- en: Tip
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: To understand this explainability feature graph, kindly refer to the *Partial
    Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这个可解释性特征图，请参考您在代码中执行`explain()`函数后得到的输出中的*部分依赖图*部分。
- en: The PDP plot is a graph that shows you the marginal effects of a feature on
    the response values. On the *X*-axis of the graph, you have the selected feature
    and its range of values. On the *Y*-axis, you have the mean response values for
    the target value. The PDP plot aims to tell the viewer what the mean response
    value predicted by the model for a given value of the selected feature is.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: PDP图是一个显示特征对响应值边际效应的图表。在图表的*X*轴上，您有选定的特征及其值范围。在*Y*轴上，您有目标值的平均响应值。PDP图旨在告诉观众模型对选定特征的给定值预测的平均响应值。
- en: In *Figure 7.9*, the PDP graph is plotted for the **C1** column for the target
    value, which is **Iris-setosa**. On the *X*-axis, we have the **C1** column, which
    stands for the sepal length of the flower in centimeters. The range of these values
    ranges from the minimum value present in the dataset to the maximum value. On
    the *Y*-axis, we have the mean response values. For this experiment, the mean
    response values are probabilities that the flower is an Iris-setosa, which is
    the selected target value of the plot. The colorful lines on the graph indicate
    the mean response values predicted by the different models trained by H2O AutoML
    for the range of **C1** values.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.9*中，PDP图是为目标值**Iris-setosa**的**C1**列绘制的。在*X*轴上，我们有**C1**列，代表花的萼片长度（以厘米为单位）。这些值的范围从数据集中的最小值到最大值。在*Y*轴上，我们有平均响应值。对于这个实验，平均响应值是花是Iris-setosa的概率，这是图表选定的目标值。图上的彩色线条表示H2O
    AutoML为**C1**值范围预测的不同模型预测的平均响应值。
- en: Looking at this graph gives us a good idea of how the response value is dependent
    on the single feature, **C1**, for every individual model. We can see that so
    long as the sepal length lies between 4.5 to 6.5 centimeters, most of the models
    show an approximate probability that there is a 35% chance that the flower is
    of the Iris-setosa class.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这个图表能让我们很好地了解响应值如何依赖于每个模型的单个特征，**C1**。我们可以看到，只要花瓣的萼片长度在4.5到6.5厘米之间，大多数模型显示的近似概率是花属于Iris-setosa类的可能性有35%。
- en: 'Similarly, in the following graph, we have plotted the PDP graph for the **C1**
    column, only this time the target response column is **Iris-versicolor**:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在下面的图表中，我们绘制了**C1**列的PDP图，但这次的目标响应列是**Iris-versicolor**：
- en: '![Figure 7.10 – PDP for the C1 column with Iris-versicolor as the target ](img/B17298_07_010.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 以Iris-versicolor为目标列的PDP图](img/B17298_07_010.jpg)'
- en: Figure 7.10 – PDP for the C1 column with Iris-versicolor as the target
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 以Iris-versicolor为目标列的PDP图
- en: Tip
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: To understand this explainability feature graph, kindly refer to the *Partial
    Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这个可解释性特征图，请参考您在代码中执行`explain()`函数后得到的输出中的*部分依赖图*部分。
- en: 'Here, we can see that so long the values of **C1** are between 4.5 to 6.5,
    there is around a 27% to 40% chance that the flower is of the Iris-versicolor
    class. Now, let’s look at the following PDP plot for **C1** for the third target
    value, **Iris-virginica**:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，只要**C1**的值在4.5到6.5之间，花属于Iris-versicolor类的可能性大约在27%到40%之间。现在，让我们看看以下针对**C1**的第三个目标值，**Iris-virginica**的PDP图：
- en: '![Figure 7.11 – PDP for the C1 column with Iris-virginica as the target ](img/B17298_07_011.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11 – 以Iris-virginica为目标列的PDP图](img/B17298_07_011.jpg)'
- en: Figure 7.11 – PDP for the C1 column with Iris-virginica as the target
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 以Iris-virginica为目标列的PDP图
- en: Tip
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: To better understand this explainability feature graph, kindly refer to the
    *Partial Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个可解释性特征图，请参考您在代码中执行`explain()`函数后得到的输出中的*部分依赖图*部分。
- en: You will notice that, for **Iris-virginica**, all the models predict differently
    for the same values of **C1**. This could mean that the **Iris-virginica** class
    is not strongly dependent on the sepal length of the flower – that is, the **C1**
    value.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，对于**Iris-virginica**，所有模型对相同的**C1**值预测不同。这可能意味着**Iris-virginica**类并不强烈依赖于花的萼片长度，即**C1**值。
- en: Another case where PDP might be useful is in model selection. Let’s assume you
    are certain that a specific feature in your dataset will be contributing greatly
    to the response value and you train multiple models on it. Then, you can choose
    the model that best suits this relationship as that model will make the most realistically
    accurate predictions.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: PDP可能还有助于模型选择。假设你确信你的数据集中某个特定特征将对响应值产生很大贡献，并且你针对它训练了多个模型。然后，你可以选择最适合这种关系的模型，因为这个模型将做出最真实准确的预测。
- en: Now, let’s try to understand how the PDP plot is generated and how H2O computes
    these plot values.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试理解PDP图是如何生成的，以及H2O是如何计算这些绘图值的。
- en: 'The PDP plot data can be calculated as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: PDP图数据可以按以下方式计算：
- en: Choose a feature and target value to plot the dependency.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个特征和目标值来绘制依赖关系。
- en: Bootstrap a dataset from the validation dataset, where the value of the selected
    feature is set to the minimum value present in the validation dataset for all
    rows.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从验证数据集中自助生成一个数据集，其中所选特征的值设置为验证数据集中所有行的最小值。
- en: Pass this bootstrapped dataset to one of the models trained by H2O AutoML and
    calculate the mean of the prediction values it got for all rows.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此自助数据集传递给由H2O AutoML训练的其中一个模型，并计算它对所有行的预测值的平均值。
- en: Plot this value on the PDP graph for that model.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此值绘制在该模型的PDP图上。
- en: Repeat *steps 3* and *4* for the remaining models.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对剩余的模型重复*步骤3*和*步骤4*。
- en: Repeat *step 2*, but this time, increment the value of the selected feature
    to the next value present in the validation dataset. Then, repeat the remaining
    steps.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤2*，但这次，将所选特征的值增加到验证数据集中存在的下一个值。然后，重复剩余的步骤。
- en: You will do this for all the feature values present in the validation dataset
    and plot them on the results for all the models on the same PDP graph.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 你将对验证数据集中所有特征值进行此操作，并将它们绘制在所有模型的同一PDP图上。
- en: Once finished, you will repeat the same process for different combinations of
    the feature and target response values.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，你将对特征和目标响应值的组合进行不同的操作。
- en: 'H2O will make multiple PDP plots for all the combinations of features and response
    values. The following is a PDP plot where the selected feature is **C2** and the
    selected target value is **Iris-setosa**:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: H2O将为所有特征和响应值的组合创建多个PDP图。以下是一个PDP图，其中所选特征是**C2**，所选目标值是**Iris-setosa**：
- en: '![Figure 7.12 – PDP for the C2 column with Iris-setosa as the target ](img/B17298_07_012.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图7.12 – 以Iris-setosa为目标列的C2列PDP图](img/B17298_07_012.jpg)'
- en: Figure 7.12 – PDP for the C2 column with Iris-setosa as the target
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 – 以Iris-setosa为目标列的C2列PDP图
- en: Tip
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: To better understand this explainability feature graph, kindly refer to the
    *Partial Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个可解释性特征图，请参考你在代码中执行`explain()`函数后得到的输出中的*部分依赖性图*部分。
- en: 'Similarly, it created different combinations of the PDP plot for the **C3**
    and **C4** features. The following is a PDP plot where the selected feature is
    **C3** and the selected target value is **Iris-versicolor**:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，它为**C3**和**C4**特征创建了不同的PDP图组合。以下是一个PDP图，其中所选特征是**C3**，所选目标值是**Iris-versicolor**：
- en: '![Figure 7.13 – PDP for the C3 column with Iris-versicolor as the target ](img/B17298_07_013.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图7.13 – 以Iris-versicolor为目标列的PDP图](img/B17298_07_013.jpg)'
- en: Figure 7.13 – PDP for the C3 column with Iris-versicolor as the target
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 – 以Iris-versicolor为目标列的PDP图
- en: Tip
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: To better understand this explainability feature graph, kindly refer to the
    *Partial Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个可解释性特征图，请参考你在代码中执行`explain()`函数后得到的输出中的*部分依赖性图*部分。
- en: Now that you know how to interpret feature importance heatmaps, let’s learn
    about SHAP summary plots.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道如何解释特征重要性热图，让我们来了解SHAP摘要图。
- en: Understanding SHAP summary plots
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解SHAP摘要图
- en: For sophisticated problems, tree-based models can become difficult to understand.
    Complex tree models can be very large and complicated to understand. The **SHAP
    summary plot** is a simplified graph of the tree-based model that gives you a
    summarized view of the model’s complexity and how it behaves.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂的问题，基于树的模型可能难以理解。复杂的树模型可能非常大且难以理解。**SHAP摘要图**是基于树的模型的简化图，它为你提供了一个模型复杂性和行为的概述视图。
- en: '**SHAP** stands for **Shapley Additive Explanations**. SHAP is a model explainability
    feature that takes an approach from game theory to explain the output of an ML
    model. The SHAP summary plot shows you the contribution of the features toward
    predicting values, similar to PDPs.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**SHAP**代表**Shapley Additive Explanations**。SHAP是一种模型可解释性特征，它采用博弈论的方法来解释机器学习模型的输出。SHAP摘要图显示了特征对预测值的贡献，类似于PDPs。'
- en: 'Let’s try to interpret a SHAP value from an example. The following is the SHAP
    summary we get from the Red Wine Quality dataset:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试从一个例子中解释SHAP值。以下是从红葡萄酒质量数据集中得到的SHAP摘要：
- en: '![Figure 7.14 – SHAP summary plot for the Red Wine Quality dataset ](img/B17298_07_014.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图7.14 – 红葡萄酒质量数据集的SHAP摘要图](img/B17298_07_014.jpg)'
- en: Figure 7.14 – SHAP summary plot for the Red Wine Quality dataset
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 – 红葡萄酒质量数据集的SHAP摘要图
- en: Tip
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: To better understand this explainability feature graph, kindly refer to the
    *SHAP Summary* section in the output you get after executing the `explain()` function
    on your regression model.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个可解释性特征图，请参考执行回归模型的`explain()`函数后得到的输出中的*SHAP摘要*部分。
- en: On the right-hand side, you can see a bluish-red bar. This bar represents the
    normalized value of the wine quality in color. The redder the color, the better
    the quality; the bluer the color, the poorer the wine quality. In binomial problems,
    the color will be a stark contrast between red and blue. However, in regression
    problems, like in our example, we can have a whole spectrum of colors, indicating
    the range of possible numerical values.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧，你可以看到一个蓝红色条形。这个条形表示葡萄酒质量的归一化值，用颜色表示。颜色越红，质量越好；颜色越蓝，葡萄酒质量越差。在二项问题中，颜色将在红色和蓝色之间形成鲜明对比。然而，在我们的例子中，回归问题，我们可以有一个完整的颜色光谱，表示可能的数值范围。
- en: On the *Y*-axis, you have the features from the dataset. They are in descending
    order from top to bottom based on the feature’s importance. In our example, the
    alcohol content is the most important feature in the dataset; it contributes more
    to the final prediction value.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Y*轴上，你有数据集中的特征。它们根据特征的重要性从上到下按降序排列。在我们的例子中，酒精含量是数据集中最重要的特征；它对最终预测值的影响更大。
- en: On the *X*-axis, you have the **SHAP value**. The SHAP value denotes how the
    feature helps the model toward the expected outcome. The more positive the SHAP
    value, the more the feature contributes to the outcome.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在*X*轴上，你有**SHAP值**。SHAP值表示特征如何帮助模型达到预期的结果。SHAP值越正，该特征对结果的影响就越大。
- en: Let’s take the example of alcohol from the SHAP summary. Based on this, we can
    see that alcohol has the highest SHAP value among the rest of the features. Thus,
    alcohol contributes greatly to the model’s prediction. Also, the points on the
    graph for alcohol with the highest SHAP value are in red. This also indicates
    that high alcohol content contributes to a positive outcome. Keeping this in mind,
    what we can extract from this graph is that the feature alcohol content plays
    an important part in the prediction of the quality of the wine and that the higher
    the content of the alcohol, the better the quality of the wine.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以SHAP摘要中的酒精为例。基于此，我们可以看到酒精在所有其他特征中具有最高的SHAP值。因此，酒精对模型的预测贡献很大。此外，具有最高SHAP值的酒精在图中的点用红色表示。这也表明高酒精含量有助于产生积极的结果。考虑到这一点，我们可以从这张图中提取的信息是，特征酒精含量在预测葡萄酒质量方面起着重要作用，而且酒精含量越高，葡萄酒的质量越好。
- en: Similarly, you can interpret the same knowledge from the other features. This
    can help you compare and understand which features are important and how they
    contribute to the final prediction of the model.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你也可以从其他特征中解释相同的知识。这可以帮助你比较和理解哪些特征是重要的，以及它们如何对模型的最终预测做出贡献。
- en: One interesting question on the SHAP summary and the PDP is, what is the difference
    between them? Well, the prime difference between the two is that PDP explains
    the effect of replacing only one feature at a time on the output, while the SHAP
    summary considers the overall interaction of that feature with other features
    in the dataset. So, PDP works on the assumption that your features are independent
    of one another, while SHAP takes into account the combined contributions of different
    features and their combined effects on the overall prediction.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SHAP摘要和PDP的一个有趣的问题是，它们之间有什么区别？嗯，这两个之间的主要区别是，PDP解释了仅替换一个特征对输出的影响，而SHAP摘要考虑了该特征与数据集中其他特征的总体交互。因此，PDP基于你的特征相互独立的假设，而SHAP则考虑了不同特征的组合贡献及其对总体预测的综合影响。
- en: Calculating **SHAP values** is a complex process that is derived from game theory.
    If you are interested in expanding your knowledge of game theory and how SHAP
    values are calculated, feel free to explore them at your own pace. A good starting
    point for understanding SHAP is to follow the explanations at https://shap.readthedocs.io/en/latest/index.xhtml.
    At the time of writing, H2O acts as a wrapper for the SHAP library and internally
    uses this library to calculate the SHAP values.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 计算SHAP值是一个复杂的过程，它源于博弈论。如果你对扩展你的博弈论知识以及SHAP值是如何计算的感兴趣，请随时以自己的节奏探索。理解SHAP的一个好起点是遵循https://shap.readthedocs.io/en/latest/index.xhtml上的解释。在撰写本文时，H2O充当SHAP库的包装器，并在内部使用此库来计算SHAP值。
- en: Now that we know how to interpret a SHAP summary plot, let’s learn about explainability
    feature, **Individual Conditional Expectation** (**ICE**) plots.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了如何解释SHAP摘要图，让我们来了解可解释性特征，**单个条件期望**（**ICE**）图。
- en: Understanding individual conditional expectation plots
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解单个条件期望图
- en: An **ICE** plot is a graph that displays a line for every instance of an observation
    that shows how the prediction for the given observation changes when the value
    of a feature changes.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**ICE**图是一个显示每个观察实例的线图的图表，它显示了当特征值变化时给定观察的预测如何变化。'
- en: ICE plots are similar to PDP graphs. PDP focuses on the overall average effect
    of a change in a feature on the prediction outcome, while ICE plots focus on the
    dependency of the outcome on individual instances of the feature value. If you
    average the ICE plot values, you should get a PDP.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ICE图类似于PDP图。PDP关注特征变化对预测结果的整体平均影响，而ICE图关注结果对特征值单个实例的依赖性。如果你平均ICE图值，你应该得到一个PDP。
- en: 'The way to compute ICE plots is very simple, as shown in the following screenshot:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 计算ICE图的方法非常简单，如下截图所示：
- en: '![Figure 7.15 – Sample dataset for ICE graph plots highlighting Observation
    1 ](img/B17298_07_015.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![图7.15 – ICE图绘制示例数据集突出显示观察到1](img/B17298_07_015.jpg)'
- en: Figure 7.15 – Sample dataset for ICE graph plots highlighting Observation 1
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 – ICE图绘制示例数据集突出显示观察到1
- en: 'Once your model has been trained, you must perform the following steps to calculate
    the ICE plots:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的模型已经训练好，你必须执行以下步骤来计算ICE图：
- en: Consider the first observation – in our example, **Observation 1** – and plot
    the relationship between **Feature 1** and the respective **Target** value.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑第一个观察 – 在我们的例子中，**观察到1** – 并绘制**特征1**与相应的**目标**值之间的关系。
- en: Keeping the values in **Feature 1** constant, create a bootstrapped dataset
    while replacing all the other feature values with those seen in **Observation
    1** in the original dataset; mark all other observations as **Observation 1**.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持**特征1**的值不变，创建一个引导数据集，同时将所有其他特征值替换为原始数据集中**观察到1**中看到的值；将所有其他观察标记为**观察到1**。
- en: Calculate the **Target** value of the observations using your trained model.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你的训练模型计算观察的**目标**值。
- en: 'Refer to the following screenshot for the bootstrapped dataset:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下截图以获取引导数据集：
- en: '![Figure 7.16 – Bootstrapped dataset for Observation 1 for Feature 1 ](img/B17298_07_016.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图7.16 – 观察到1的1号特征的引导数据集](img/B17298_07_016.jpg)'
- en: Figure 7.16 – Bootstrapped dataset for Observation 1 for Feature 1
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 – 观察到1的1号特征的引导数据集
- en: 'Repeat the same for the next observation. Consider the second observation –
    in our example, **Observation 2** – and plot the relationship between **Feature
    1** and the respective **Target** value:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对下一个观察重复相同的操作。考虑第二个观察 – 在我们的例子中，**观察到2** – 并绘制**特征1**与相应的**目标**值之间的关系：
- en: '![Figure 7.17 – Sample dataset for an ICE plot highlighting Observation 2 ](img/B17298_07_017.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图7.17 – 强调观察2的样本数据集的ICE图](img/B17298_07_017.jpg)'
- en: Figure 7.17 – Sample dataset for an ICE plot highlighting Observation 2
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 – 强调观察2的样本数据集的ICE图
- en: 'Keep the values in **Feature 1** constant and create a bootstrapped dataset;
    then, calculate the **Target** values using the trained model. Refer to the following
    resultant bootstrapped dataset:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持**特征1**的值不变，创建一个自举数据集；然后，使用训练好的模型计算**目标**值。参考以下生成的自举数据集：
- en: '![Figure 7.18 – Bootstrapped dataset for Observation 2 for Feature 1 ](img/B17298_07_018.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图7.18 – 特征1的观察2的自举数据集](img/B17298_07_018.jpg)'
- en: Figure 7.18 – Bootstrapped dataset for Observation 2 for Feature 1
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 – 特征1的观察2的自举数据集
- en: We repeat this process for all observations against all features.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对所有观察结果和所有特征重复此过程。
- en: The results that are observed from these bootstrapped datasets are plotted on
    the individual ICE plots per feature.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这些自举数据集中观察到的结果绘制在每个特征的单独ICE图上。
- en: 'Let’s see how we can interpret the ICE plot and extract observable information
    out of the graph. Refer to the following screenshot, which shows the ICE plot
    we get after running the model explainability interface on the AutoML object that
    was trained on the Red Wine Quality dataset:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何解释ICE图并从中提取可观察的信息。参考以下截图，它显示了在训练在红酒质量数据集上的AutoML对象上运行模型可解释性界面后得到的ICE图：
- en: '![Figure 7.19 – ICE plot for the Red Wine Quality dataset ](img/B17298_07_019.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![图7.19 – 红酒质量数据集的ICE图](img/B17298_07_019.jpg)'
- en: Figure 7.19 – ICE plot for the Red Wine Quality dataset
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 – 红酒质量数据集的ICE图
- en: 'As the heading states, this is an ICE plot on the alcohol feature column of
    the dataset for a stacked ensemble model trained by H2O AutoML. Keep in mind that
    this model is the leader of the list of models trained by AutoML. ICE plots are
    only plotted for the leader of the dataset. You can also observe the ICE plots
    of the other models by extracting the models using their model IDs and then running
    the `ice_plot()` function on them. Refer to the following code example:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如标题所述，这是一个H2O AutoML训练的堆叠集成模型的酒精特征列的ICE图。请注意，这个模型是AutoML训练的模型列表中的领导者。ICE图只为数据集的领导者绘制。您还可以通过提取具有其模型ID的其他模型并对其运行`ice_plot()`函数来观察其他模型的ICE图。参考以下代码示例：
- en: '[PRE19]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: On the *X*-axis of the graph, you have the range of values for the alcohol feature.
    On the *Y*-axis, you have the range of values of the predicted outcomes – that
    is, the quality of the wine.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表的*X*轴上，有酒精特征的值范围。在*Y*轴上，有预测结果的值范围——即葡萄酒的质量。
- en: On the left-hand side of the graph, you can see the legends stating different
    types of lines and their percentiles. The ICE plot plots the effects for each
    decile. So, technically, when plotting the ICE plot, you compute a line for every
    observation. However, in a dataset that contains thousands if not millions of
    rows of data, you will end up with an equal number of lines on the plot. This
    will make the ICE plot messy. That is why to better observe this data, you must
    aggregate the lines together to the nearest decile and plot a single line for
    every percentile partition.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表的左侧，您可以看到说明不同类型线条及其百分位数的图例。ICE图绘制了每个十分位的效果。所以，从技术上讲，在绘制ICE图时，您为每个观察结果计算一条线。然而，在一个包含数千行甚至数百万行数据的数据集中，您将在图表上得到相等数量的线条。这将使ICE图变得混乱。这就是为什么为了更好地观察这些数据，您必须将线条聚合到最近的十分位，并为每个百分位数分区绘制一条单独的线条。
- en: The dotted black line is the average of all these other percentile lines and
    is nothing but the PDP line for that feature.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 点状黑色线是所有这些其他百分位数线的平均值，也就是该特征的PDP线。
- en: Now that you know how to interpret an ICE plot, let’s look at learning curve
    plots.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经知道如何解释ICE图，让我们看看学习曲线图。
- en: Understanding learning curve plots
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解学习曲线图
- en: The **learning curve plot** is one of the most used plots by data scientists
    to observe the learning rate of a model. The **learning curve** shows how your
    model learns from the dataset and the efficiency with which it does the learning.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习曲线图**是数据科学家观察模型学习率最常用的图表之一。**学习曲线**显示了模型如何从数据集中学习以及它学习的效率。'
- en: When working on an ML problem, an important question that often needs to be
    answered is, *how much data do we need to train the most accurate model?* A learning
    curve plot can help you understand how increasing the dataset affects your overall
    model performance.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理一个机器学习问题时，一个经常需要回答的重要问题是，“我们需要多少数据来训练最准确的模型？”学习曲线图可以帮助您了解增加数据集如何影响您的整体模型性能。
- en: Using this information, you can decide if increasing the size of the dataset
    can result in better model performance or if you need to work on your model training
    to improve your model’s performance.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些信息，您可以决定增加数据集的大小是否会导致更好的模型性能，或者您是否需要改进模型训练以提高模型性能。
- en: 'Let’s observe the learning curve plot we got from our experiment on the Red
    Wine Quality dataset for the XRT model trained by AutoML:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们观察我们从红葡萄酒质量数据集上的XRT模型训练实验中获得的学习曲线图：
- en: '![Figure 7.20 – Learning curve plot for the XRT model on the Red Wine Quality
    dataset ](img/B17298_07_020.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![图7.20 – 在红葡萄酒质量数据集上的XRT模型学习曲线图](img/B17298_07_020.jpg)'
- en: Figure 7.20 – Learning curve plot for the XRT model on the Red Wine Quality
    dataset
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20 – 在红葡萄酒质量数据集上的XRT模型学习曲线图
- en: On the *X*-axis of the graph, you have the number of trees created by the XRT
    algorithm. As you can see, the algorithm created around 40 to 50 trees in total.
    On the *Y*-axis, you have the performance metric, RMSE, which is calculated at
    every stage during the model training as the algorithm creates the trees.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表的X轴上，您有XRT算法创建的树的数量。如您所见，该算法总共创建了大约40到50棵树。在Y轴上，您有性能指标RMSE，这是在算法创建树的过程中在每个阶段计算的。
- en: As shown in the preceding screenshot, the RMSE metric decreases as the algorithm
    creates more trees. Eventually, the rate at which the RMSE lowers decreases over
    a certain number of trees created. Any trees created over this number do not contribute
    to the overall improvement in the model’s performance. Thus, the learning rate
    eventually decreases over the increase in several trees.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一张截图所示，随着算法创建更多的树，RMSE指标逐渐降低。最终，在创建了一定数量的树之后，RMSE降低的速度会逐渐减慢。超过这个数量创建的树不会对模型性能的整体提升做出贡献。因此，随着树的数量增加，学习率最终会降低。
- en: The lines on the graph depict the various datasets that were used by the algorithm
    during training and the respective RMSE during every instance of creating the
    trees.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图表上的线条描述了算法在训练过程中使用的各种数据集以及在每个创建树实例中的相应RMSE。
- en: 'At the time of writing, as of H2O version *3.36.1*, the learning curve plot
    is not part of the default model explainability interface. To plot the learning
    curve, you must plot it using the following function on the respective model object:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，截至H2O版本*3.36.1*，学习曲线图不是默认模型可解释性界面的一部分。要绘制学习曲线，您必须使用以下函数在相应的模型对象上绘制：
- en: '[PRE20]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The learning curve plot is different for different algorithms. The following
    screenshot shows a learning plot for a GLM model trained by AutoML on the same
    dataset:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 不同算法的学习曲线图是不同的。以下截图显示了在相同数据集上由AutoML训练的GLM模型的学习图：
- en: '![Figure 7.21 – Learning curve plot for the GLM model on the Red Wine Quality
    dataset ](img/B17298_07_021.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![图7.21 – 在红葡萄酒质量数据集上的GLM模型学习曲线图](img/B17298_07_021.jpg)'
- en: Figure 7.21 – Learning curve plot for the GLM model on the Red Wine Quality
    dataset
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21 – 在红葡萄酒质量数据集上的GLM模型学习曲线图
- en: As you can see, instead of the number of trees on the *X*-axis, we now have
    iterations. The number of trees is relevant for tree-based algorithms such as
    XRT and DRF, but linear models such as GLM running on the linear algorithm makes
    more sense to aid with learning. On the *Y*-axis, you have deviance instead of
    RMSE as deviance is more suitable for measuring the performance of a linear model.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，现在在X轴上不再是树的数量，而是迭代次数。树的数量对于基于树的算法（如XRT和DRF）是相关的，但运行在线性算法上的线性模型（如GLM）更有助于学习。在Y轴上，您有偏差而不是RMSE，因为偏差更适合衡量线性模型的表现。
- en: The learning curve is different for different types of algorithms, including
    stacked ensemble models. Feel free to explore the different variations of the
    learning curve for different algorithms. H2O already takes care of selecting the
    appropriate performance metric and the steps in learning, depending on the algorithms,
    so you don’t have to worry about whether you chose the right metric to measure
    the learning rate or not.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线对于不同类型的算法是不同的，包括堆叠集成模型。您可以自由探索不同算法的学习曲线的不同变体。H2O已经根据算法选择合适的性能指标和学习步骤，因此您不必担心是否选择了正确的指标来衡量学习率。
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we focused on understanding the model explainability interface
    provided by H2O. First, we understood how the explainability interface provides
    different explainability features that help users get detailed information about
    the models trained. Then, we learned how to implement this functionality on models
    trained by H2O’s AutoML in both Python and R.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于理解H2O提供的模型可解释性界面。首先，我们了解了可解释性界面如何提供不同的可解释性功能，帮助用户获取关于训练模型的详细信息。然后，我们学习了如何在H2O的AutoML训练的模型上实现这一功能，无论是使用Python还是R。
- en: Once we were comfortable with its implementation, we started exploring and understanding
    the various explainability graphs displayed by the explainability interface’s
    output, starting with residual analysis. We observed how residual analysis helps
    highlight heteroscedasticity in the dataset and how it helps you identify if there
    is any missing information in your dataset.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对其实施感到满意，我们就开始探索和理解可解释性界面输出显示的各种可解释性图表，从残差分析开始。我们观察到残差分析如何帮助突出数据集中的异方差性，以及它是如何帮助您识别数据集中是否存在任何缺失信息的。
- en: Then, we explored variable importance and how it helps you identify important
    features in the dataset. Building on top of this, we learned how feature importance
    heatmaps can help you observe feature importance among all the models trained
    by AutoML.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们探讨了变量重要性以及它是如何帮助您识别数据集中的重要特征的。在此基础上，我们学习了特征重要性热图如何帮助您观察AutoML训练的所有模型中的特征重要性。
- en: Then, we discovered how model correlation heatmaps can be interpreted and how
    they help us identify models with similar prediction behavior from a list of models.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们发现如何解释模型相关性热图，以及它们如何帮助我们识别从模型列表中具有相似预测行为的模型。
- en: Later, we learned about PDP graphs and how they express the dependency of the
    overall outcome over the individual features of the dataset. With this knowledge
    in mind, we explored the SHAP summary and ICE plots, where we understood the two
    graphs and how each focuses on different aspects of outcome dependency on individual
    features.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们学习了PDP图以及它们如何表达整体结果对数据集个别特征的依赖性。带着这些知识，我们探索了SHAP摘要和ICE图，我们了解了这两张图以及它们如何分别关注结果对个别特征的依赖性的不同方面。
- en: Finally, we explored what a learning plot is and how it helps us understand
    how the model improves in performance, also called learning, over the number of
    observations, iterations, or trees, depending on the type of algorithms used to
    train the model.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨了学习曲线是什么以及它是如何帮助我们理解模型在观察次数、迭代或树的数量上的性能改进，也称为学习，这取决于训练模型所使用的算法类型。
- en: In the next chapter, we shall use all the knowledge we’ve learned from the last
    few chapters and explore the other advanced parameters that are available when
    using H2O’s AutoML feature.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将运用从前几章学到的所有知识，探索在使用H2O的AutoML功能时可以使用的其他高级参数。
- en: Part 3 H2O AutoML Advanced Implementation and Productization
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分 H2O AutoML 高级实现与产品化
- en: This part will help you understand H2O AutoML’s advanced features and parameters
    used to customize certain characteristics of AutoML to suit specialized needs.
    This will help you get the desired personalized results that generalized machine
    learning fails to provide. It will also explain the various ways that H2O AutoML
    can be used with different types of technologies, and you will understand how
    you can deploy your machine learning models into production, and commercially
    use them to meet business needs.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分将帮助您了解H2O AutoML的高级特性和参数，这些参数用于定制AutoML的某些特性以适应特定需求。这将帮助您获得通用机器学习无法提供的个性化结果。它还将解释H2O
    AutoML如何与不同类型的技术一起使用，您将了解如何将您的机器学习模型部署到生产环境中，并商业性地使用它们来满足业务需求。
- en: 'This section comprises the following chapters:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包括以下章节：
- en: '[*Chapter 8*](B17298_08.xhtml#_idTextAnchor169), *Exploring Optional Parameters
    for H2O AutoML*'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B17298_08.xhtml#_idTextAnchor169), *探索H2O AutoML的可选参数*'
- en: '[*Chapter 9*](B17298_09.xhtml#_idTextAnchor186), [*Exploring Miscellaneous
    Features in H2O AutoML*](https://epic.packtpub.com/index.php?module=oss_Chapters&action=DetailView&record=3a065625-7e22-e0bf-231f-61a9d1f3e976)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B17298_09.xhtml#_idTextAnchor186), [*探索H2O AutoML中的其他功能*](https://epic.packtpub.com/index.php?module=oss_Chapters&action=DetailView&record=3a065625-7e22-e0bf-231f-61a9d1f3e976)'
- en: '[*Chapter 10*](B17298_10.xhtml#_idTextAnchor196), [*Working with Plain Old
    Java Objects (POJOs)*](https://epic.packtpub.com/index.php?module=oss_Chapters&action=DetailView&record=bb77c8ca-d15c-48c5-2b00-61a9d1abce98)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B17298_10.xhtml#_idTextAnchor196), [*与普通Java对象（POJOs）一起工作*](https://epic.packtpub.com/index.php?module=oss_Chapters&action=DetailView&record=bb77c8ca-d15c-48c5-2b00-61a9d1abce98)'
- en: '[*Chapter 11*](B17298_11.xhtml#_idTextAnchor210), *Working with Model Object
    Optimized (MOJO)*'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B17298_11.xhtml#_idTextAnchor210), *与模型对象优化（MOJO）一起工作*'
- en: '[*Chapter 12*](B17298_12.xhtml#_idTextAnchor225), [*Working with H2O AutoML
    and Apache Spark*](https://epic.packtpub.com/index.php?module=oss_Chapters&action=DetailView&record=854f151d-1690-0982-b488-61a9d16f9b67)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第12章*](B17298_12.xhtml#_idTextAnchor225), [*使用H2O AutoML和Apache Spark*](https://epic.packtpub.com/index.php?module=oss_Chapters&action=DetailView&record=854f151d-1690-0982-b488-61a9d16f9b67)'
- en: '[*Chapter 13*](B17298_13.xhtml#_idTextAnchor239), *Using H2O AutoML with Other
    Technologies*'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第13章*](B17298_13.xhtml#_idTextAnchor239), *使用H2O AutoML与其他技术*'
