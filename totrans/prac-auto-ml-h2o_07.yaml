- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with Model Explainability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The justification of model selection and performance is just as important as
    model training. You can have *N* trained models using different algorithms, and
    all of them will be able to make good enough predictions for real-world problems.
    So, how do you select one of them to be used in your production services, and
    how do you justify to your stakeholders that your chosen model is better than
    the others, even though all the other models were also able to make accurate predictions
    to some degree? One answer is performance metrics, but as we saw in the previous
    chapter, there are plenty of performance metrics and all of them measure different
    types of performance. Choosing the correct performance metric boils down to the
    context of your ML problem. What else can we use that will help us choose the
    right model and also further help us in justifying this selection?
  prefs: []
  type: TYPE_NORMAL
- en: The answer to that is visual graphs. Human beings are visual creatures and,
    as such, a picture speaks a thousand words. A good graph can explain more about
    a model than any metric number. The versatility of graphs can be very useful in
    explaining the model’s behavior and how it fits as a solution to our ML problem.
  prefs: []
  type: TYPE_NORMAL
- en: H2O’s explainability interface is a unique feature that wraps over various explainability
    features and visuals that H2O auto-computes for a model or list of models, including
    the H2O AutoML object.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we shall explore the H2O explainability interface and how it
    works with the H2O AutoML object. We shall also implement a practical example
    to understand how to use the explainability interface in Python and R. Finally,
    we shall go through and understand all the various explainability features that
    we get as outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with the model explainability interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the various explainability features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you should have a good idea of how to interpret
    model performance by looking at the various performance metrics described by the
    model explainability interface.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will require the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of your preferred web browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **Integrated Development Environment** (**IDE**) of your choice or a Terminal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the experiments conducted in this chapter are performed on a Terminal. You
    are free to follow along using the same setup or you can perform the same experiments
    using any IDE of your choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%207](https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%207).
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s begin by understanding how the model explainability interface works.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the model explainability interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **model explainability interface** is a simple function that incorporates
    various graphs and information about the model and its workings. There are two
    main functions for model explainability in H2O:'
  prefs: []
  type: TYPE_NORMAL
- en: The `h2o.explain()` function, which is used to explain the model’s behavior
    on the entire test dataset. This is also called **global explanation**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `h2o.explain_row()` function, which is used to explain the model’s behavior
    on an individual row in the test dataset. This is also called **local explanation**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both these functions work on either a single H2O model object, a list of H2O
    model objects, or the H2O AutoML object. These functions generate a list of results
    that consists of various graphical plots such as a **variable importance graph**,
    **partial dependency graph**, and a **leaderboard** if used on multiple models.
  prefs: []
  type: TYPE_NORMAL
- en: 'For graphs and other visual results, the `explain` object relies on visualization
    engines to render the graphics:'
  prefs: []
  type: TYPE_NORMAL
- en: For the R interface, H2O uses the `ggplot2` package for rendering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the Python interface, H2O uses the `matplotlib` package for rendering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this in mind, we need to make sure that whenever we are using the explainability
    interface to get visual graphs, we run it in an environment that supports graph
    rendering. This interface won’t be of much use in Terminals and other non-graphical
    command-line interfaces. The examples in this chapter have been run on **Jupyter
    Notebook**, but any environment that supports plot rendering should work fine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The explainability function has the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`newdata`/`frame`: This parameter is used to specify the H2O test DataFrame
    needed to compute some of the explainability features such as the `newdata`, while
    the same in the Python explainability interface is `frame`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns`: This parameter is used to specify the columns to be considered in
    column-based explanations such as **individual conditional expectation plots**
    or **partial dependency plots**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_n_features`: This parameter is used to specify the number of columns to
    be considered based on the feature importance ranking for column-based explanations.
    The default value is `5`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Either the `columns` parameter or the `top_n_features` parameter will be considered
    by the explainability function. Preference is given to the `columns` parameter,
    so if both the parameters are passed with values, then `top_n_features` will be
    ignored.
  prefs: []
  type: TYPE_NORMAL
- en: '`include_explanations`: This parameter is used to specify the explanations
    that you want from the explainability function’s output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exclude_explanations`: This parameter is used to specify the explanations
    that you do not want from the explainability function’s output. `include_explanations`
    and `exclude_explanations` are mutually exclusive parameters. The available values
    for both parameters are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`leaderboard`: This value is only valid for the list of models or the AutoML
    object.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`residual_analysis`: This value is only valid for regression models.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`confusion_matrix`: This value is only valid for classification models.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`varimp`: This value stands for variable importance and is only valid for base
    models, not for stacked ensemble models.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`varimp_heatmap`: This value stands for heatmap of variable importance.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_correlation_heatmap`: This value stands for heatmap of model correlation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shap_summary`: This value stands for Shapley additive explanations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pdp`: This value stands for partial dependency plots.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ice`: This value stands for individual conditional expectation plots.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plot_overrides`: This parameter is used to override the values for individual
    explanation plots. This parameter is useful if you want the top 10 features to
    be considered for one plot but specific columns for another:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`object`: This parameter is used to specify the H2O models or the H2O AutoML
    object, which we will cover shortly. This parameter is specific to the R explainability
    interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know how the explainability interface works and what its various
    parameters are, let’s understand it better with an implementation example.
  prefs: []
  type: TYPE_NORMAL
- en: We shall use **Fisher’s Iris flower dataset**, which we used in [*Chapter 1*](B17298_01.xhtml#_idTextAnchor017),
    *Understanding H2O AutoML Basics*, to train models using AutoML. We will then
    use the explainability interface on the AutoML object to display all the explainability
    features it has to provide.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s start by implementing it in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the model explainability interface in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To implement the model explainability function in Python, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `h2o` library and spin up a local H2O server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The explainability interface performs heavy computations behind the scenes to
    calculate the data needed to plot the graphs. To speed up processing, it is recommended
    to initialize the H2O server with as much memory as you can allocate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the dataset using `h2o.importFile(“Dataset/iris.data”)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set which columns are the features and which columns are the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remove the label from among the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the DataFrame into training and testing DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the H2O AutoML object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Trigger the H2O AutoML object so that it starts auto-training the models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once training has finished, we can use the H2O explainability interface, `h2o.explain()`,
    on the now-trained `aml` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `explain` function will take some time to finish computing. Once it does,
    you should see a big output that lists all the explainability features. The output
    should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Model explainability interface output ](img/B17298_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Model explainability interface output
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the `h2o.explain_row()` interface to display the model explainability
    features for a single row of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of this should give you a leaderboard of the models making predictions
    on the first row of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get additional information about the model from an explainability point
    of view, you can further extend the explainability interface by using the `explain_row()`
    function on the leader model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of this should give you all the applicable graphical model explainability
    features for that model based on its predictions on that row.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to use the model explainability interface in Python, let’s
    see how we can use this interface in the R Language.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the model explainability interface in R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to how we implemented the explainability interface in Python, H2O has
    provisions to use the explainability interface in the R programming language as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement the model explainability function in R, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `h2o` library and spin up a local H2O server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the dataset using `h2o.importFile(“Dataset/iris.data”)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the `C5` column as the label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the DataFrame into training and testing DataFrames and assign them to
    the appropriate variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the H2O AutoML training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the H2O explainability interface on the now-trained `aml` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the explainability object finishes its computation, you should see a big
    output that lists all the explainability features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like Python, you can also extend the model explainability interface function
    so that it can be run on a single row using the `h2o.explain_row()` function,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will give you the leaderboard of models making predictions on the first
    row of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, you can expand this explainability interface by using the `h2o.explain_row()`
    function on the leader model to get more advanced information about the leader
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In these examples, we have used the Iris flower dataset to solve a multinomial
    classification problem. Similarly, we can use the explainability interface on
    trained regression models. Some of the explainability features are only available
    depending on whether the trained model is a regression model or a classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to implement the model explainability interface in Python
    and R, let’s look deeper into the output of the interface and try to understand
    the various explainability features that H2O computed.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the various explainability features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The output of the explainability interface is an `H2OExplanation` object. The
    `H2OExplanation` object is nothing but a simple dictionary with the explainability
    features’ names as keys. You can retrieve individual explainability features by
    using a feature’s key name as a `dict` key on the explainability object.
  prefs: []
  type: TYPE_NORMAL
- en: If you scroll down the output of the explainability interface for the H2O AutoML
    object, you will notice that there are plenty of headings with explanations. Below
    these headings, there’s a brief description of what the explainability feature
    is. Some have graphical diagrams, while others may have tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The various explainability features are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Leaderboard**: This feature is a leaderboard comprising all trained models
    and their basic metrics ranked from best performing to worst. This feature is
    computed only if the explainability interface is run on the H2O AutoML object
    or list of H2O models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confusion Matrix**: This feature is a performance metric that generates a
    matrix that keeps track of correct and incorrect predictions of a classification
    model. It is only available for classification models. For multiple models, the
    confusion matrix is only calculated for the leader model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Residual Analysis**: This feature plots the predicted values against the
    residuals on the test dataset used in the explainability interface. It only analyzes
    the leader model based on the model ranking on the leaderboard. It is only available
    for regression models. For multiple models, residual analysis is performed on
    the leader model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variable Importance**: This feature plots the importance of variables in
    the dataset. It is available for all models except for stacked models. For multiple
    models, it is only performed on the leader model, which is not a stacked model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variable Importance Heatmap**: This feature plots a heatmap of variable importance
    across all the models. It is available for comparing all models except stacked
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Correlation Heatmap**: This feature plots the correlation between the
    predicted values of different models. This helps group together models with similar
    performance. It is only available for multiple model explanations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SHAP Summary of Top Tree-Based Model**: This feature plots the importance
    of variables in contributing to the decision-making that’s done by complex tree-based
    models such as Random Forest and neural networks. This feature computes this plot
    for the top-ranking tree-based model in the leaderboard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partial Dependence Multi Plots**: This feature plots the dependency between
    the target feature and a certain set of features in the dataset that we consider
    important.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Individual Conditional Expectation** (**ICE**) **Plots**: This feature plots
    the dependency between the target feature and a certain set of features in the
    dataset that we consider important for each instance separately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing this to the output you got from the model explainability interface
    in the experiment we performed in the *Working with the model explainability interface*
    section, you will notice that some of the explainability features are missing
    from the output. This is because some of these features are only available to
    the type of model trained. For example, residual analysis is only available for
    regression models, while the experiment conducted in the *Working with the model
    explainability interface* section is a classification problem that trained a classification
    model. Hence, you won’t find residual analysis in the model’s explainability output.
  prefs: []
  type: TYPE_NORMAL
- en: You can perform the same experiment using a regression problem; the model explainability
    interface will output regression-supported explainability features.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know about the different explainability features that are available
    in the explanation interface, let’s dive deep into them one by one to get an in-depth
    understanding of what they mean. We shall go through the output we got from our
    implementation of the explainability interface in Python and R.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapters, we understood what the leaderboard and confusion
    matrix are. So, let’s start with the next explanation feature: residual analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding residual analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Residual analysis** is performed for **regression models**. As described
    in [*Chapter 5*](B17298_05.xhtml#_idTextAnchor109), *Understanding AutoML Algorithms*,
    in the *Understanding generalized linear models* and *Introduction to linear regression*
    sections, **residuals** are the difference between the values predicted by the
    regression model and the actual values for that same row of data. Analyzing these
    residual values is a great way of diagnosing any problems in your model.'
  prefs: []
  type: TYPE_NORMAL
- en: A residual analysis plot is a graph where you plot the **residual values** against
    the **predicted values**. Another thing we learned in [*Chapter 5*](B17298_05.xhtml#_idTextAnchor109),
    *Understanding AutoML Algorithms*, in the *Understanding generalized linear models*
    and *Understanding the assumptions of linear regression* sections, is that one
    of the primary assumptions in **linear regression** is that the distribution of
    residuals is **normally distributed**.
  prefs: []
  type: TYPE_NORMAL
- en: So, accordingly, we expect our residual plot to be an amorphous collection of
    points. There should not be any patterns between the residual values and the predicted
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Residual analysis can highlight the presence of **heteroscedasticity** in a
    trained model. Heteroscedasticity is said to have occurred if the standard deviation
    of the predicted values changes over different values of the features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Regression graph for a homoscedastic dataset ](img/B17298_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Regression graph for a homoscedastic dataset
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows a regression plot where we had some sample data
    that maps the relationship between *X* and *Y*. Let’s fit a straight line through
    this data, which represents our linear model. If we calculate the residuals for
    every point as we go from left to right on the *X*-axis, we will notice that the
    error rate remains fairly constant throughout all the values of *X*. This means
    that all the error values lie between the parallel blue lines. Such a situation
    where the distribution of errors or residuals is constant throughout the independent
    variables is called homoscedasticity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The opposite of homoscedasticity is **heteroscedasticity**. This is where the
    error rate varies over the change in the value of *X*. Refer to the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Regression graph for a heteroscedastic dataset ](img/B17298_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Regression graph for a heteroscedastic dataset
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the magnitude of the errors made by the linear model increases
    with an increase in *X*. If you plot the blue error lines encompassing all the
    errors, then you will notice that they gradually fan out and are not parallel.
    This situation where the distribution of errors or residuals is not constant throughout
    the independent variables is called heteroscedasticity.
  prefs: []
  type: TYPE_NORMAL
- en: What heteroscedasticity tells us is that there is some sort of information that
    the model has not been able to capture and learn from. Heteroscedasticity also
    violates linear regressions’ basic assumption. Thus, it can help you identify
    that you may need to add the missing information to your dataset to correctly
    train your linear model or that you may need to implement some non-linear regression
    algorithm to get a better-performing model.
  prefs: []
  type: TYPE_NORMAL
- en: Since residual analysis is a regression-specific model explainability feature,
    we cannot use the Iris dataset classification experiment that we performed in
    the *Working with the model explainability interface* section. Instead, we need
    to train a regression model and then use the model explainability interface on
    that model to get the residual analysis output. So, let’s look at a regression
    problem using the Red Wine Quality dataset. You can find this dataset at [https://archive.ics.uci.edu/ml/datasets/wine+quality](https://archive.ics.uci.edu/ml/datasets/wine+quality).
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset consists of the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**fixed acidity**: This feature explains the amount of acidity that is non-volatile,
    meaning it does not evaporate over some time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**volatile acidity**: This feature explains the amount of acidity that is volatile,
    meaning it will evaporate over some time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**citric acid**: This feature explains the amount of citric acid present in
    the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**residual sugar**: This feature explains the amount of residual sugar present
    in the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**chlorides**: This feature explains the number of chlorides present in the
    wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**free sulfur dioxide**: This feature explains the amount of free sulfur dioxide
    present in the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**total sulfur dioxide**: This feature explains the amount of total sulfur
    dioxide present in the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**density**: This feature explains the density of the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pH**: This feature explains the pH value of the wine, with 0 being the most
    acidic and 14 being the most basic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sulphates**: This feature explains the number of sulfates present in the
    wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**alcohol**: This feature explains the amount of alcohol present in the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**quality**: This is the response column that notes the quality of the wine.
    0 indicates that the wine is very bad, while 10 indicates that the wine is excellent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will run our basic H2O AutoML process of training the model and then use
    the model explainability interface on the trained AutoML object to get the residual
    analysis plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s observe the residual analysis plot that we get from this implementation
    and then see how we can retrieve the required information from the graph. Refer
    to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Residual analysis graph plot for the Red Wine Quality dataset
    ](img/B17298_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Residual analysis graph plot for the Red Wine Quality dataset
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can see the residual analysis for the stacked ensemble model, which
    is the leader of the AutoML trained models. On the *X*-axis, you have **Fitted**,
    also called predicted values, while on the *Y*-axis, you have **Residuals**.
  prefs: []
  type: TYPE_NORMAL
- en: On the left border of the *Y*-axis and below the *X*-axis you will see a **grayscale**
    column and row, respectively. These help you observe the distribution of those
    residuals across the *X* and *Y* axes.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that the distribution of residuals is normal and that the data is
    not heteroskedastic, you need to observe this grayscale on the *Y*-axis. A normal
    distribution would ideally give you a grayscale that is the darkest at the center
    and lightens as it moves away.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you understood how to interpret the residual analysis graph, let’s
    learn more about the next explainability feature: variable importance.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding variable importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Variable importance**, also called **feature importance**, as the name suggests,
    explains the importance of the different variables/features in the dataset in
    making predictions. In any ML problem, your dataset will often have multiple variables
    that contribute to the characteristics of your prediction column. However, in
    most cases, you will often have some features that contribute more compared to
    others.'
  prefs: []
  type: TYPE_NORMAL
- en: This understanding can help scientists and engineers remove any unwanted features
    that introduce noise from the dataset. This can further improve the quality of
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: H2O calculates variable importance differently for different types of algorithms.
    First, let’s understand how variable importance is calculated for **tree-based
    algorithms**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Variable importance in tree-based algorithms is calculated based on two criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: Selection of the variable for deciding on the decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improvement in the squared error over the whole tree because of the selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whenever H2O is building a decision tree as a part of training a tree-based
    model, it will use one of the features as a node to further split the tree. As
    we studied in [*Chapter 5*](B17298_05.xhtml#_idTextAnchor109), *Understanding
    AutoML Algorithms*, in the *Understanding the Distributed Random Forest algorithm*
    section, we know that every node split in the decision tree aims to reduce the
    overall squared error. This deducted value is nothing but the difference between
    the squared errors of the parent node against the children node.
  prefs: []
  type: TYPE_NORMAL
- en: H2O considers this reduction in squared error in calculating the feature importance.
    The squared error for every node in the tree-based model leads to the variance
    of the response value for that node being lowered. The squared error for every
    node in the tree-based model leads to the variance of the response value for that
    node being lowered.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, accordingly, the equation for calculating the squared error of the tree
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17298_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*MSE* means mean squared error'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N* indicates the total number of observations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*VAR* means variance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The equation for calculating variance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17298_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17298_07_003.png) indicates the value of the observation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B17298_07_004.png) indicates the mean of all the observations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N* indicates the total number of observations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For tree-based ensemble algorithms such as the **Gradient Boosting Algorithm**
    (**GBM**), the decision trees are trained sequentially. Every tree is built on
    top of the previous tree’s errors. So, the feature importance calculation is the
    same as how we do it for individual nodes in single decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: For **Distributed Random Forest** (**DRF**), the decision trees are trained
    in parallel, so H2O just averages the results to calculate the feature importance.
  prefs: []
  type: TYPE_NORMAL
- en: For **XGBoost**, H2O calculates the feature importance from the gains of the
    loss function for individual features when building the tree.
  prefs: []
  type: TYPE_NORMAL
- en: For **deep learning**, H2O calculates the feature importance using a special
    method called the **Gedeon method**.
  prefs: []
  type: TYPE_NORMAL
- en: For **Generalized Linear Models** (**GLMs**), variable importance is the same
    as the predictor weights, also called the coefficient magnitudes. If, during training,
    you decide to standardize the data, then the standardized coefficients are returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the feature importance that was calculated for
    our experiment on the Iris flower dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Variable importance graph for the Iris flower dataset ](img/B17298_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Variable importance graph for the Iris flower dataset
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows the variable importance map for a deep learning
    model. If you compare it with your leaderboard, you will see that the variable
    importance graph is plotted for the most leading model, which is not a stacked
    ensemble model.
  prefs: []
  type: TYPE_NORMAL
- en: On the *Y*-axis of the graph, you have the feature names – in our case, the
    **C1**, **C2**, **C3**, and **C4** columns of the Iris flower dataset. On the
    *X*-axis, you have the importance of these variables. It is possible to get the
    raw metric value of feature importance, but H2O displays the importance values
    by scaling them down between **0** and **1**, where **1** indicates the most important
    variable while **0** indicates the least important variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since variable importance is available for both classification and regression
    models, you will also get a variable importance graph as an explainability feature
    of the Red Wine Quality regression model. The graph should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Variable importance graph for the Red Wine Quality dataset  ](img/B17298_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Variable importance graph for the Red Wine Quality dataset
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to interpret a feature importance graph, let’s understand
    feature importance heatmaps.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding feature importance heatmaps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When displaying feature importance for a specific model, it is fairly easy to
    represent it as a histogram or bar graph. However, we often need to compare the
    feature importance of various models so that we can understand which feature is
    deemed important by which model and how we can use this information to compare
    model performance. H2O AutoML will inherently train multiple models with different
    ML algorithms. Therefore, a comparative study of the model performance is a must
    and a graphical representation of feature importance can be of great help to scientists
    and engineers.
  prefs: []
  type: TYPE_NORMAL
- en: To represent the feature importance of all the models trained by H2O AutoML
    in a single graph, H2O generates a heatmap of feature importance.
  prefs: []
  type: TYPE_NORMAL
- en: A heatmap is a data visualization graph where the color of the graph is affected
    by the amount of density or magnitude of a specific value.
  prefs: []
  type: TYPE_NORMAL
- en: Some H2O models compute the variable importance on encoded versions of categorical
    columns. Different models also have different ways of encoding categorical values.
    So, comparing the variable importance of these categorical columns across all
    the models can be tricky. H2O does this comparison by summarizing the variable
    importance across all the features and returning a single variable importance
    value that represents the original categorical feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the feature importance heatmap for the Iris flower dataset
    experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Variable importance heatmap ](img/B17298_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Variable importance heatmap
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the top 10 models on the leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: The heatmap has the **C1**, **C2**, **C3**, and **C4** features on the *Y*-axis
    and the model IDs on the *X*-axis. The color of the plots indicates how important
    the model considers the feature during its prediction. More importance equals
    more value, which, in turn, turns the respective plot red. The lower the importance,
    the lower the importance value of the feature will be; the color will become cooler
    and become blue.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to interpret feature importance heatmaps, let’s learn
    about model correlation heatmaps.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model correlation heatmaps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important comparison between multiple models is **model correlation**.
    Model correlation can be interpreted as how similar the models are in terms of
    performance when you compare their prediction values.
  prefs: []
  type: TYPE_NORMAL
- en: Different models trained using the same or different ML algorithms are said
    to be highly correlated if the predictions made by one model are the same or similar
    to the predictions made by the other.
  prefs: []
  type: TYPE_NORMAL
- en: In a model correlation heatmap, H2O compares the prediction values of all the
    models that it trains and compares them to one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the model correlation heatmap graph we got from our experiment
    on the Iris flower dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Model correlation heatmap for the Iris flower dataset ](img/B17298_07_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Model correlation heatmap for the Iris flower dataset
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To understand this explainability feature graph, kindly refer to the *Model
    Correlation* section in the output you got after executing the `explain()` function
    in your code.
  prefs: []
  type: TYPE_NORMAL
- en: On the *X* and *Y* axes, we have the model IDs. Their cross-section on the graph
    indicates the correlation value between them. You will notice that the heat points
    on the graph within the *X* and *Y* axes have the same model ID, which will always
    be 1; therefore, the plot will always be red. This is correct as, technically,
    it’s the same model and when you compare the prediction values of a model with
    itself, there will be 100% correlation.
  prefs: []
  type: TYPE_NORMAL
- en: To get a better idea of the correlation between different models, you can refer
    to these heat values. Dark red points indicate high correlation, while those with
    cool blue values indicate low correlation. Models highlighted in red are interpretable
    models such as GLMs.
  prefs: []
  type: TYPE_NORMAL
- en: You may notice that since the model correlation heatmap supports stacked ensemble
    models and feature importance heatmaps don’t, if you ignore the stacked ensemble
    models in the model correlation heatmap (*Figure 7.8*), the rest of the models
    are the same as the ones in the feature importance heatmap (*Figure 7.7*).
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to interpret model correlation heatmaps, let’s learn more
    about partial dependency plots.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding partial dependency plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **Partial Dependence Plot** (**PDP**) is a graph diagram that shows you the
    dependency between the predicted values and the set of input features that we
    are interested in while marginalizing the values of features in which we are not
    interested.
  prefs: []
  type: TYPE_NORMAL
- en: Another way of understanding a PDP is that it represents a function of input
    features that we are interested in that gives us the expected predicted values
    as output.
  prefs: []
  type: TYPE_NORMAL
- en: A PDP is a very interesting graph that is useful in showing and explaining the
    model training results to members of the organization that are not so skilled
    in the domain of data science.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s understand how to interpret a DPD before learning how it is calculated.
    The following diagram shows the PDP we got for our experiment when using the Iris
    flower dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – PDP for the C1 column with Iris-setosa as the target ](img/B17298_07_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – PDP for the C1 column with Iris-setosa as the target
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To understand this explainability feature graph, kindly refer to the *Partial
    Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  prefs: []
  type: TYPE_NORMAL
- en: The PDP plot is a graph that shows you the marginal effects of a feature on
    the response values. On the *X*-axis of the graph, you have the selected feature
    and its range of values. On the *Y*-axis, you have the mean response values for
    the target value. The PDP plot aims to tell the viewer what the mean response
    value predicted by the model for a given value of the selected feature is.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7.9*, the PDP graph is plotted for the **C1** column for the target
    value, which is **Iris-setosa**. On the *X*-axis, we have the **C1** column, which
    stands for the sepal length of the flower in centimeters. The range of these values
    ranges from the minimum value present in the dataset to the maximum value. On
    the *Y*-axis, we have the mean response values. For this experiment, the mean
    response values are probabilities that the flower is an Iris-setosa, which is
    the selected target value of the plot. The colorful lines on the graph indicate
    the mean response values predicted by the different models trained by H2O AutoML
    for the range of **C1** values.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at this graph gives us a good idea of how the response value is dependent
    on the single feature, **C1**, for every individual model. We can see that so
    long as the sepal length lies between 4.5 to 6.5 centimeters, most of the models
    show an approximate probability that there is a 35% chance that the flower is
    of the Iris-setosa class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, in the following graph, we have plotted the PDP graph for the **C1**
    column, only this time the target response column is **Iris-versicolor**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – PDP for the C1 column with Iris-versicolor as the target ](img/B17298_07_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – PDP for the C1 column with Iris-versicolor as the target
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To understand this explainability feature graph, kindly refer to the *Partial
    Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that so long the values of **C1** are between 4.5 to 6.5,
    there is around a 27% to 40% chance that the flower is of the Iris-versicolor
    class. Now, let’s look at the following PDP plot for **C1** for the third target
    value, **Iris-virginica**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – PDP for the C1 column with Iris-virginica as the target ](img/B17298_07_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – PDP for the C1 column with Iris-virginica as the target
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To better understand this explainability feature graph, kindly refer to the
    *Partial Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that, for **Iris-virginica**, all the models predict differently
    for the same values of **C1**. This could mean that the **Iris-virginica** class
    is not strongly dependent on the sepal length of the flower – that is, the **C1**
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Another case where PDP might be useful is in model selection. Let’s assume you
    are certain that a specific feature in your dataset will be contributing greatly
    to the response value and you train multiple models on it. Then, you can choose
    the model that best suits this relationship as that model will make the most realistically
    accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s try to understand how the PDP plot is generated and how H2O computes
    these plot values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PDP plot data can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a feature and target value to plot the dependency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bootstrap a dataset from the validation dataset, where the value of the selected
    feature is set to the minimum value present in the validation dataset for all
    rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass this bootstrapped dataset to one of the models trained by H2O AutoML and
    calculate the mean of the prediction values it got for all rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot this value on the PDP graph for that model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 3* and *4* for the remaining models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *step 2*, but this time, increment the value of the selected feature
    to the next value present in the validation dataset. Then, repeat the remaining
    steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will do this for all the feature values present in the validation dataset
    and plot them on the results for all the models on the same PDP graph.
  prefs: []
  type: TYPE_NORMAL
- en: Once finished, you will repeat the same process for different combinations of
    the feature and target response values.
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O will make multiple PDP plots for all the combinations of features and response
    values. The following is a PDP plot where the selected feature is **C2** and the
    selected target value is **Iris-setosa**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – PDP for the C2 column with Iris-setosa as the target ](img/B17298_07_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – PDP for the C2 column with Iris-setosa as the target
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To better understand this explainability feature graph, kindly refer to the
    *Partial Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, it created different combinations of the PDP plot for the **C3**
    and **C4** features. The following is a PDP plot where the selected feature is
    **C3** and the selected target value is **Iris-versicolor**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – PDP for the C3 column with Iris-versicolor as the target ](img/B17298_07_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – PDP for the C3 column with Iris-versicolor as the target
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To better understand this explainability feature graph, kindly refer to the
    *Partial Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to interpret feature importance heatmaps, let’s learn
    about SHAP summary plots.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding SHAP summary plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For sophisticated problems, tree-based models can become difficult to understand.
    Complex tree models can be very large and complicated to understand. The **SHAP
    summary plot** is a simplified graph of the tree-based model that gives you a
    summarized view of the model’s complexity and how it behaves.
  prefs: []
  type: TYPE_NORMAL
- en: '**SHAP** stands for **Shapley Additive Explanations**. SHAP is a model explainability
    feature that takes an approach from game theory to explain the output of an ML
    model. The SHAP summary plot shows you the contribution of the features toward
    predicting values, similar to PDPs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to interpret a SHAP value from an example. The following is the SHAP
    summary we get from the Red Wine Quality dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – SHAP summary plot for the Red Wine Quality dataset ](img/B17298_07_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – SHAP summary plot for the Red Wine Quality dataset
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To better understand this explainability feature graph, kindly refer to the
    *SHAP Summary* section in the output you get after executing the `explain()` function
    on your regression model.
  prefs: []
  type: TYPE_NORMAL
- en: On the right-hand side, you can see a bluish-red bar. This bar represents the
    normalized value of the wine quality in color. The redder the color, the better
    the quality; the bluer the color, the poorer the wine quality. In binomial problems,
    the color will be a stark contrast between red and blue. However, in regression
    problems, like in our example, we can have a whole spectrum of colors, indicating
    the range of possible numerical values.
  prefs: []
  type: TYPE_NORMAL
- en: On the *Y*-axis, you have the features from the dataset. They are in descending
    order from top to bottom based on the feature’s importance. In our example, the
    alcohol content is the most important feature in the dataset; it contributes more
    to the final prediction value.
  prefs: []
  type: TYPE_NORMAL
- en: On the *X*-axis, you have the **SHAP value**. The SHAP value denotes how the
    feature helps the model toward the expected outcome. The more positive the SHAP
    value, the more the feature contributes to the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the example of alcohol from the SHAP summary. Based on this, we can
    see that alcohol has the highest SHAP value among the rest of the features. Thus,
    alcohol contributes greatly to the model’s prediction. Also, the points on the
    graph for alcohol with the highest SHAP value are in red. This also indicates
    that high alcohol content contributes to a positive outcome. Keeping this in mind,
    what we can extract from this graph is that the feature alcohol content plays
    an important part in the prediction of the quality of the wine and that the higher
    the content of the alcohol, the better the quality of the wine.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you can interpret the same knowledge from the other features. This
    can help you compare and understand which features are important and how they
    contribute to the final prediction of the model.
  prefs: []
  type: TYPE_NORMAL
- en: One interesting question on the SHAP summary and the PDP is, what is the difference
    between them? Well, the prime difference between the two is that PDP explains
    the effect of replacing only one feature at a time on the output, while the SHAP
    summary considers the overall interaction of that feature with other features
    in the dataset. So, PDP works on the assumption that your features are independent
    of one another, while SHAP takes into account the combined contributions of different
    features and their combined effects on the overall prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating **SHAP values** is a complex process that is derived from game theory.
    If you are interested in expanding your knowledge of game theory and how SHAP
    values are calculated, feel free to explore them at your own pace. A good starting
    point for understanding SHAP is to follow the explanations at https://shap.readthedocs.io/en/latest/index.xhtml.
    At the time of writing, H2O acts as a wrapper for the SHAP library and internally
    uses this library to calculate the SHAP values.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to interpret a SHAP summary plot, let’s learn about explainability
    feature, **Individual Conditional Expectation** (**ICE**) plots.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding individual conditional expectation plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An **ICE** plot is a graph that displays a line for every instance of an observation
    that shows how the prediction for the given observation changes when the value
    of a feature changes.
  prefs: []
  type: TYPE_NORMAL
- en: ICE plots are similar to PDP graphs. PDP focuses on the overall average effect
    of a change in a feature on the prediction outcome, while ICE plots focus on the
    dependency of the outcome on individual instances of the feature value. If you
    average the ICE plot values, you should get a PDP.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way to compute ICE plots is very simple, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Sample dataset for ICE graph plots highlighting Observation
    1 ](img/B17298_07_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Sample dataset for ICE graph plots highlighting Observation 1
  prefs: []
  type: TYPE_NORMAL
- en: 'Once your model has been trained, you must perform the following steps to calculate
    the ICE plots:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the first observation – in our example, **Observation 1** – and plot
    the relationship between **Feature 1** and the respective **Target** value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keeping the values in **Feature 1** constant, create a bootstrapped dataset
    while replacing all the other feature values with those seen in **Observation
    1** in the original dataset; mark all other observations as **Observation 1**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the **Target** value of the observations using your trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Refer to the following screenshot for the bootstrapped dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Bootstrapped dataset for Observation 1 for Feature 1 ](img/B17298_07_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Bootstrapped dataset for Observation 1 for Feature 1
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeat the same for the next observation. Consider the second observation –
    in our example, **Observation 2** – and plot the relationship between **Feature
    1** and the respective **Target** value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Sample dataset for an ICE plot highlighting Observation 2 ](img/B17298_07_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – Sample dataset for an ICE plot highlighting Observation 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep the values in **Feature 1** constant and create a bootstrapped dataset;
    then, calculate the **Target** values using the trained model. Refer to the following
    resultant bootstrapped dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Bootstrapped dataset for Observation 2 for Feature 1 ](img/B17298_07_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Bootstrapped dataset for Observation 2 for Feature 1
  prefs: []
  type: TYPE_NORMAL
- en: We repeat this process for all observations against all features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The results that are observed from these bootstrapped datasets are plotted on
    the individual ICE plots per feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s see how we can interpret the ICE plot and extract observable information
    out of the graph. Refer to the following screenshot, which shows the ICE plot
    we get after running the model explainability interface on the AutoML object that
    was trained on the Red Wine Quality dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – ICE plot for the Red Wine Quality dataset ](img/B17298_07_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – ICE plot for the Red Wine Quality dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'As the heading states, this is an ICE plot on the alcohol feature column of
    the dataset for a stacked ensemble model trained by H2O AutoML. Keep in mind that
    this model is the leader of the list of models trained by AutoML. ICE plots are
    only plotted for the leader of the dataset. You can also observe the ICE plots
    of the other models by extracting the models using their model IDs and then running
    the `ice_plot()` function on them. Refer to the following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: On the *X*-axis of the graph, you have the range of values for the alcohol feature.
    On the *Y*-axis, you have the range of values of the predicted outcomes – that
    is, the quality of the wine.
  prefs: []
  type: TYPE_NORMAL
- en: On the left-hand side of the graph, you can see the legends stating different
    types of lines and their percentiles. The ICE plot plots the effects for each
    decile. So, technically, when plotting the ICE plot, you compute a line for every
    observation. However, in a dataset that contains thousands if not millions of
    rows of data, you will end up with an equal number of lines on the plot. This
    will make the ICE plot messy. That is why to better observe this data, you must
    aggregate the lines together to the nearest decile and plot a single line for
    every percentile partition.
  prefs: []
  type: TYPE_NORMAL
- en: The dotted black line is the average of all these other percentile lines and
    is nothing but the PDP line for that feature.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to interpret an ICE plot, let’s look at learning curve
    plots.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding learning curve plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **learning curve plot** is one of the most used plots by data scientists
    to observe the learning rate of a model. The **learning curve** shows how your
    model learns from the dataset and the efficiency with which it does the learning.
  prefs: []
  type: TYPE_NORMAL
- en: When working on an ML problem, an important question that often needs to be
    answered is, *how much data do we need to train the most accurate model?* A learning
    curve plot can help you understand how increasing the dataset affects your overall
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Using this information, you can decide if increasing the size of the dataset
    can result in better model performance or if you need to work on your model training
    to improve your model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s observe the learning curve plot we got from our experiment on the Red
    Wine Quality dataset for the XRT model trained by AutoML:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Learning curve plot for the XRT model on the Red Wine Quality
    dataset ](img/B17298_07_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – Learning curve plot for the XRT model on the Red Wine Quality
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: On the *X*-axis of the graph, you have the number of trees created by the XRT
    algorithm. As you can see, the algorithm created around 40 to 50 trees in total.
    On the *Y*-axis, you have the performance metric, RMSE, which is calculated at
    every stage during the model training as the algorithm creates the trees.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding screenshot, the RMSE metric decreases as the algorithm
    creates more trees. Eventually, the rate at which the RMSE lowers decreases over
    a certain number of trees created. Any trees created over this number do not contribute
    to the overall improvement in the model’s performance. Thus, the learning rate
    eventually decreases over the increase in several trees.
  prefs: []
  type: TYPE_NORMAL
- en: The lines on the graph depict the various datasets that were used by the algorithm
    during training and the respective RMSE during every instance of creating the
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, as of H2O version *3.36.1*, the learning curve plot
    is not part of the default model explainability interface. To plot the learning
    curve, you must plot it using the following function on the respective model object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The learning curve plot is different for different algorithms. The following
    screenshot shows a learning plot for a GLM model trained by AutoML on the same
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Learning curve plot for the GLM model on the Red Wine Quality
    dataset ](img/B17298_07_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – Learning curve plot for the GLM model on the Red Wine Quality
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, instead of the number of trees on the *X*-axis, we now have
    iterations. The number of trees is relevant for tree-based algorithms such as
    XRT and DRF, but linear models such as GLM running on the linear algorithm makes
    more sense to aid with learning. On the *Y*-axis, you have deviance instead of
    RMSE as deviance is more suitable for measuring the performance of a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: The learning curve is different for different types of algorithms, including
    stacked ensemble models. Feel free to explore the different variations of the
    learning curve for different algorithms. H2O already takes care of selecting the
    appropriate performance metric and the steps in learning, depending on the algorithms,
    so you don’t have to worry about whether you chose the right metric to measure
    the learning rate or not.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on understanding the model explainability interface
    provided by H2O. First, we understood how the explainability interface provides
    different explainability features that help users get detailed information about
    the models trained. Then, we learned how to implement this functionality on models
    trained by H2O’s AutoML in both Python and R.
  prefs: []
  type: TYPE_NORMAL
- en: Once we were comfortable with its implementation, we started exploring and understanding
    the various explainability graphs displayed by the explainability interface’s
    output, starting with residual analysis. We observed how residual analysis helps
    highlight heteroscedasticity in the dataset and how it helps you identify if there
    is any missing information in your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we explored variable importance and how it helps you identify important
    features in the dataset. Building on top of this, we learned how feature importance
    heatmaps can help you observe feature importance among all the models trained
    by AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we discovered how model correlation heatmaps can be interpreted and how
    they help us identify models with similar prediction behavior from a list of models.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we learned about PDP graphs and how they express the dependency of the
    overall outcome over the individual features of the dataset. With this knowledge
    in mind, we explored the SHAP summary and ICE plots, where we understood the two
    graphs and how each focuses on different aspects of outcome dependency on individual
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we explored what a learning plot is and how it helps us understand
    how the model improves in performance, also called learning, over the number of
    observations, iterations, or trees, depending on the type of algorithms used to
    train the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we shall use all the knowledge we’ve learned from the last
    few chapters and explore the other advanced parameters that are available when
    using H2O’s AutoML feature.
  prefs: []
  type: TYPE_NORMAL
- en: Part 3 H2O AutoML Advanced Implementation and Productization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part will help you understand H2O AutoML’s advanced features and parameters
    used to customize certain characteristics of AutoML to suit specialized needs.
    This will help you get the desired personalized results that generalized machine
    learning fails to provide. It will also explain the various ways that H2O AutoML
    can be used with different types of technologies, and you will understand how
    you can deploy your machine learning models into production, and commercially
    use them to meet business needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B17298_08.xhtml#_idTextAnchor169), *Exploring Optional Parameters
    for H2O AutoML*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B17298_09.xhtml#_idTextAnchor186), [*Exploring Miscellaneous
    Features in H2O AutoML*](https://epic.packtpub.com/index.php?module=oss_Chapters&action=DetailView&record=3a065625-7e22-e0bf-231f-61a9d1f3e976)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B17298_10.xhtml#_idTextAnchor196), [*Working with Plain Old
    Java Objects (POJOs)*](https://epic.packtpub.com/index.php?module=oss_Chapters&action=DetailView&record=bb77c8ca-d15c-48c5-2b00-61a9d1abce98)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B17298_11.xhtml#_idTextAnchor210), *Working with Model Object
    Optimized (MOJO)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B17298_12.xhtml#_idTextAnchor225), [*Working with H2O AutoML
    and Apache Spark*](https://epic.packtpub.com/index.php?module=oss_Chapters&action=DetailView&record=854f151d-1690-0982-b488-61a9d16f9b67)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B17298_13.xhtml#_idTextAnchor239), *Using H2O AutoML with Other
    Technologies*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
