- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with Model Explainability
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The justification of model selection and performance is just as important as
    model training. You can have *N* trained models using different algorithms, and
    all of them will be able to make good enough predictions for real-world problems.
    So, how do you select one of them to be used in your production services, and
    how do you justify to your stakeholders that your chosen model is better than
    the others, even though all the other models were also able to make accurate predictions
    to some degree? One answer is performance metrics, but as we saw in the previous
    chapter, there are plenty of performance metrics and all of them measure different
    types of performance. Choosing the correct performance metric boils down to the
    context of your ML problem. What else can we use that will help us choose the
    right model and also further help us in justifying this selection?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The answer to that is visual graphs. Human beings are visual creatures and,
    as such, a picture speaks a thousand words. A good graph can explain more about
    a model than any metric number. The versatility of graphs can be very useful in
    explaining the model’s behavior and how it fits as a solution to our ML problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: H2O’s explainability interface is a unique feature that wraps over various explainability
    features and visuals that H2O auto-computes for a model or list of models, including
    the H2O AutoML object.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we shall explore the H2O explainability interface and how it
    works with the H2O AutoML object. We shall also implement a practical example
    to understand how to use the explainability interface in Python and R. Finally,
    we shall go through and understand all the various explainability features that
    we get as outputs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Working with the model explainability interface
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the various explainability features
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you should have a good idea of how to interpret
    model performance by looking at the various performance metrics described by the
    model explainability interface.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will require the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of your preferred web browser.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **Integrated Development Environment** (**IDE**) of your choice or a Terminal.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the experiments conducted in this chapter are performed on a Terminal. You
    are free to follow along using the same setup or you can perform the same experiments
    using any IDE of your choice.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%207](https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%207).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s begin by understanding how the model explainability interface works.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Working with the model explainability interface
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **model explainability interface** is a simple function that incorporates
    various graphs and information about the model and its workings. There are two
    main functions for model explainability in H2O:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The `h2o.explain()` function, which is used to explain the model’s behavior
    on the entire test dataset. This is also called **global explanation**.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `h2o.explain_row()` function, which is used to explain the model’s behavior
    on an individual row in the test dataset. This is also called **local explanation**.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both these functions work on either a single H2O model object, a list of H2O
    model objects, or the H2O AutoML object. These functions generate a list of results
    that consists of various graphical plots such as a **variable importance graph**,
    **partial dependency graph**, and a **leaderboard** if used on multiple models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'For graphs and other visual results, the `explain` object relies on visualization
    engines to render the graphics:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: For the R interface, H2O uses the `ggplot2` package for rendering.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the Python interface, H2O uses the `matplotlib` package for rendering.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this in mind, we need to make sure that whenever we are using the explainability
    interface to get visual graphs, we run it in an environment that supports graph
    rendering. This interface won’t be of much use in Terminals and other non-graphical
    command-line interfaces. The examples in this chapter have been run on **Jupyter
    Notebook**, but any environment that supports plot rendering should work fine.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'The explainability function has the following parameters:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '`newdata`/`frame`: This parameter is used to specify the H2O test DataFrame
    needed to compute some of the explainability features such as the `newdata`, while
    the same in the Python explainability interface is `frame`.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns`: This parameter is used to specify the columns to be considered in
    column-based explanations such as **individual conditional expectation plots**
    or **partial dependency plots**.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_n_features`: This parameter is used to specify the number of columns to
    be considered based on the feature importance ranking for column-based explanations.
    The default value is `5`.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Either the `columns` parameter or the `top_n_features` parameter will be considered
    by the explainability function. Preference is given to the `columns` parameter,
    so if both the parameters are passed with values, then `top_n_features` will be
    ignored.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '`include_explanations`: This parameter is used to specify the explanations
    that you want from the explainability function’s output.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exclude_explanations`: This parameter is used to specify the explanations
    that you do not want from the explainability function’s output. `include_explanations`
    and `exclude_explanations` are mutually exclusive parameters. The available values
    for both parameters are as follows:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`leaderboard`: This value is only valid for the list of models or the AutoML
    object.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`residual_analysis`: This value is only valid for regression models.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`confusion_matrix`: This value is only valid for classification models.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`varimp`: This value stands for variable importance and is only valid for base
    models, not for stacked ensemble models.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`varimp_heatmap`: This value stands for heatmap of variable importance.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_correlation_heatmap`: This value stands for heatmap of model correlation.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shap_summary`: This value stands for Shapley additive explanations.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pdp`: This value stands for partial dependency plots.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ice`: This value stands for individual conditional expectation plots.'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plot_overrides`: This parameter is used to override the values for individual
    explanation plots. This parameter is useful if you want the top 10 features to
    be considered for one plot but specific columns for another:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`object`: This parameter is used to specify the H2O models or the H2O AutoML
    object, which we will cover shortly. This parameter is specific to the R explainability
    interface.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know how the explainability interface works and what its various
    parameters are, let’s understand it better with an implementation example.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: We shall use **Fisher’s Iris flower dataset**, which we used in [*Chapter 1*](B17298_01.xhtml#_idTextAnchor017),
    *Understanding H2O AutoML Basics*, to train models using AutoML. We will then
    use the explainability interface on the AutoML object to display all the explainability
    features it has to provide.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s start by implementing it in Python.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the model explainability interface in Python
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To implement the model explainability function in Python, follow these steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `h2o` library and spin up a local H2O server:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The explainability interface performs heavy computations behind the scenes to
    calculate the data needed to plot the graphs. To speed up processing, it is recommended
    to initialize the H2O server with as much memory as you can allocate.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the dataset using `h2o.importFile(“Dataset/iris.data”)`:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Set which columns are the features and which columns are the labels:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Remove the label from among the features:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Split the DataFrame into training and testing DataFrames:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Initialize the H2O AutoML object:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Trigger the H2O AutoML object so that it starts auto-training the models:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once training has finished, we can use the H2O explainability interface, `h2o.explain()`,
    on the now-trained `aml` object:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `explain` function will take some time to finish computing. Once it does,
    you should see a big output that lists all the explainability features. The output
    should look as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Model explainability interface output ](img/B17298_07_001.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Model explainability interface output
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the `h2o.explain_row()` interface to display the model explainability
    features for a single row of the dataset:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output of this should give you a leaderboard of the models making predictions
    on the first row of the dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'To get additional information about the model from an explainability point
    of view, you can further extend the explainability interface by using the `explain_row()`
    function on the leader model, as follows:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The output of this should give you all the applicable graphical model explainability
    features for that model based on its predictions on that row.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to use the model explainability interface in Python, let’s
    see how we can use this interface in the R Language.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the model explainability interface in R
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to how we implemented the explainability interface in Python, H2O has
    provisions to use the explainability interface in the R programming language as
    well.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement the model explainability function in R, follow these steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `h2o` library and spin up a local H2O server:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Import the dataset using `h2o.importFile(“Dataset/iris.data”)`:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Set the `C5` column as the label:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Split the DataFrame into training and testing DataFrames and assign them to
    the appropriate variables:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Run the H2O AutoML training:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Use the H2O explainability interface on the now-trained `aml` object:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Once the explainability object finishes its computation, you should see a big
    output that lists all the explainability features.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like Python, you can also extend the model explainability interface function
    so that it can be run on a single row using the `h2o.explain_row()` function,
    as follows:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This will give you the leaderboard of models making predictions on the first
    row of the dataset.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, you can expand this explainability interface by using the `h2o.explain_row()`
    function on the leader model to get more advanced information about the leader
    model:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In these examples, we have used the Iris flower dataset to solve a multinomial
    classification problem. Similarly, we can use the explainability interface on
    trained regression models. Some of the explainability features are only available
    depending on whether the trained model is a regression model or a classification
    model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to implement the model explainability interface in Python
    and R, let’s look deeper into the output of the interface and try to understand
    the various explainability features that H2O computed.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the various explainability features
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The output of the explainability interface is an `H2OExplanation` object. The
    `H2OExplanation` object is nothing but a simple dictionary with the explainability
    features’ names as keys. You can retrieve individual explainability features by
    using a feature’s key name as a `dict` key on the explainability object.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: If you scroll down the output of the explainability interface for the H2O AutoML
    object, you will notice that there are plenty of headings with explanations. Below
    these headings, there’s a brief description of what the explainability feature
    is. Some have graphical diagrams, while others may have tables.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'The various explainability features are as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '**Leaderboard**: This feature is a leaderboard comprising all trained models
    and their basic metrics ranked from best performing to worst. This feature is
    computed only if the explainability interface is run on the H2O AutoML object
    or list of H2O models.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排行榜**：这是一个排行榜，包括所有训练模型及其从最佳性能到最差性能的基本指标。此功能仅在解释性界面在H2O AutoML对象或H2O模型列表上运行时计算。'
- en: '**Confusion Matrix**: This feature is a performance metric that generates a
    matrix that keeps track of correct and incorrect predictions of a classification
    model. It is only available for classification models. For multiple models, the
    confusion matrix is only calculated for the leader model.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混淆矩阵**：这是一个性能指标，它生成一个矩阵，跟踪分类模型的正确和错误预测。它仅适用于分类模型。对于多个模型，混淆矩阵仅针对领先模型计算。'
- en: '**Residual Analysis**: This feature plots the predicted values against the
    residuals on the test dataset used in the explainability interface. It only analyzes
    the leader model based on the model ranking on the leaderboard. It is only available
    for regression models. For multiple models, residual analysis is performed on
    the leader model.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差分析**：此功能在用于解释性界面的测试数据集上绘制预测值与残差的关系。它仅分析基于排行榜模型排名的领先模型。它仅适用于回归模型。对于多个模型，残差分析仅在领先模型上执行。'
- en: '**Variable Importance**: This feature plots the importance of variables in
    the dataset. It is available for all models except for stacked models. For multiple
    models, it is only performed on the leader model, which is not a stacked model.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变量重要性**：此功能绘制了数据集中变量的重要性。它适用于所有模型，除了堆叠模型。对于多个模型，它仅在领先模型上执行，该模型不是堆叠模型。'
- en: '**Variable Importance Heatmap**: This feature plots a heatmap of variable importance
    across all the models. It is available for comparing all models except stacked
    models.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变量重要性热图**：此功能绘制了所有模型中变量重要性的热图。它适用于比较所有模型，除了堆叠模型。'
- en: '**Model Correlation Heatmap**: This feature plots the correlation between the
    predicted values of different models. This helps group together models with similar
    performance. It is only available for multiple model explanations.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型相关性热图**：此功能绘制了不同模型预测值之间的相关性。这有助于将具有相似性能的模型分组在一起。它仅适用于多个模型解释。'
- en: '**SHAP Summary of Top Tree-Based Model**: This feature plots the importance
    of variables in contributing to the decision-making that’s done by complex tree-based
    models such as Random Forest and neural networks. This feature computes this plot
    for the top-ranking tree-based model in the leaderboard.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顶级树模型SHAP摘要**：此功能绘制了变量在复杂树模型（如随机森林和神经网络）的决策制定中的重要性。此功能为排行榜中排名最高的树模型计算此图。'
- en: '**Partial Dependence Multi Plots**: This feature plots the dependency between
    the target feature and a certain set of features in the dataset that we consider
    important.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部分依赖多图**：此功能绘制了目标特征与数据集中我们认为重要的某些特征集之间的依赖关系。'
- en: '**Individual Conditional Expectation** (**ICE**) **Plots**: This feature plots
    the dependency between the target feature and a certain set of features in the
    dataset that we consider important for each instance separately.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个体条件期望**（**ICE**）**图**：此功能绘制了目标特征与数据集中我们认为对每个实例单独重要的某些特征集之间的依赖关系。'
- en: Comparing this to the output you got from the model explainability interface
    in the experiment we performed in the *Working with the model explainability interface*
    section, you will notice that some of the explainability features are missing
    from the output. This is because some of these features are only available to
    the type of model trained. For example, residual analysis is only available for
    regression models, while the experiment conducted in the *Working with the model
    explainability interface* section is a classification problem that trained a classification
    model. Hence, you won’t find residual analysis in the model’s explainability output.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与我们在“使用模型解释性界面”部分进行的实验中从模型解释性界面获得的输出进行比较，您会注意到输出中缺少一些解释性功能。这是因为其中一些功能仅适用于训练的模型类型。例如，残差分析仅适用于回归模型，而我们在“使用模型解释性界面”部分进行的实验是一个分类问题，训练了一个分类模型。因此，您在模型的解释性输出中找不到残差分析。
- en: You can perform the same experiment using a regression problem; the model explainability
    interface will output regression-supported explainability features.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know about the different explainability features that are available
    in the explanation interface, let’s dive deep into them one by one to get an in-depth
    understanding of what they mean. We shall go through the output we got from our
    implementation of the explainability interface in Python and R.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapters, we understood what the leaderboard and confusion
    matrix are. So, let’s start with the next explanation feature: residual analysis.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Understanding residual analysis
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Residual analysis** is performed for **regression models**. As described
    in [*Chapter 5*](B17298_05.xhtml#_idTextAnchor109), *Understanding AutoML Algorithms*,
    in the *Understanding generalized linear models* and *Introduction to linear regression*
    sections, **residuals** are the difference between the values predicted by the
    regression model and the actual values for that same row of data. Analyzing these
    residual values is a great way of diagnosing any problems in your model.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: A residual analysis plot is a graph where you plot the **residual values** against
    the **predicted values**. Another thing we learned in [*Chapter 5*](B17298_05.xhtml#_idTextAnchor109),
    *Understanding AutoML Algorithms*, in the *Understanding generalized linear models*
    and *Understanding the assumptions of linear regression* sections, is that one
    of the primary assumptions in **linear regression** is that the distribution of
    residuals is **normally distributed**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: So, accordingly, we expect our residual plot to be an amorphous collection of
    points. There should not be any patterns between the residual values and the predicted
    values.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Residual analysis can highlight the presence of **heteroscedasticity** in a
    trained model. Heteroscedasticity is said to have occurred if the standard deviation
    of the predicted values changes over different values of the features.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following diagram:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Regression graph for a homoscedastic dataset ](img/B17298_07_002.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Regression graph for a homoscedastic dataset
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows a regression plot where we had some sample data
    that maps the relationship between *X* and *Y*. Let’s fit a straight line through
    this data, which represents our linear model. If we calculate the residuals for
    every point as we go from left to right on the *X*-axis, we will notice that the
    error rate remains fairly constant throughout all the values of *X*. This means
    that all the error values lie between the parallel blue lines. Such a situation
    where the distribution of errors or residuals is constant throughout the independent
    variables is called homoscedasticity.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'The opposite of homoscedasticity is **heteroscedasticity**. This is where the
    error rate varies over the change in the value of *X*. Refer to the following
    diagram:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Regression graph for a heteroscedastic dataset ](img/B17298_07_003.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Regression graph for a heteroscedastic dataset
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the magnitude of the errors made by the linear model increases
    with an increase in *X*. If you plot the blue error lines encompassing all the
    errors, then you will notice that they gradually fan out and are not parallel.
    This situation where the distribution of errors or residuals is not constant throughout
    the independent variables is called heteroscedasticity.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: What heteroscedasticity tells us is that there is some sort of information that
    the model has not been able to capture and learn from. Heteroscedasticity also
    violates linear regressions’ basic assumption. Thus, it can help you identify
    that you may need to add the missing information to your dataset to correctly
    train your linear model or that you may need to implement some non-linear regression
    algorithm to get a better-performing model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Since residual analysis is a regression-specific model explainability feature,
    we cannot use the Iris dataset classification experiment that we performed in
    the *Working with the model explainability interface* section. Instead, we need
    to train a regression model and then use the model explainability interface on
    that model to get the residual analysis output. So, let’s look at a regression
    problem using the Red Wine Quality dataset. You can find this dataset at [https://archive.ics.uci.edu/ml/datasets/wine+quality](https://archive.ics.uci.edu/ml/datasets/wine+quality).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset consists of the following features:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '**fixed acidity**: This feature explains the amount of acidity that is non-volatile,
    meaning it does not evaporate over some time.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**volatile acidity**: This feature explains the amount of acidity that is volatile,
    meaning it will evaporate over some time.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**citric acid**: This feature explains the amount of citric acid present in
    the wine.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**residual sugar**: This feature explains the amount of residual sugar present
    in the wine.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**chlorides**: This feature explains the number of chlorides present in the
    wine.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**free sulfur dioxide**: This feature explains the amount of free sulfur dioxide
    present in the wine.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**total sulfur dioxide**: This feature explains the amount of total sulfur
    dioxide present in the wine.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**density**: This feature explains the density of the wine.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pH**: This feature explains the pH value of the wine, with 0 being the most
    acidic and 14 being the most basic.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sulphates**: This feature explains the number of sulfates present in the
    wine.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**alcohol**: This feature explains the amount of alcohol present in the wine.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**quality**: This is the response column that notes the quality of the wine.
    0 indicates that the wine is very bad, while 10 indicates that the wine is excellent.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will run our basic H2O AutoML process of training the model and then use
    the model explainability interface on the trained AutoML object to get the residual
    analysis plot.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s observe the residual analysis plot that we get from this implementation
    and then see how we can retrieve the required information from the graph. Refer
    to the following diagram:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Residual analysis graph plot for the Red Wine Quality dataset
    ](img/B17298_07_004.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Residual analysis graph plot for the Red Wine Quality dataset
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can see the residual analysis for the stacked ensemble model, which
    is the leader of the AutoML trained models. On the *X*-axis, you have **Fitted**,
    also called predicted values, while on the *Y*-axis, you have **Residuals**.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: On the left border of the *Y*-axis and below the *X*-axis you will see a **grayscale**
    column and row, respectively. These help you observe the distribution of those
    residuals across the *X* and *Y* axes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that the distribution of residuals is normal and that the data is
    not heteroskedastic, you need to observe this grayscale on the *Y*-axis. A normal
    distribution would ideally give you a grayscale that is the darkest at the center
    and lightens as it moves away.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you understood how to interpret the residual analysis graph, let’s
    learn more about the next explainability feature: variable importance.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Understanding variable importance
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Variable importance**, also called **feature importance**, as the name suggests,
    explains the importance of the different variables/features in the dataset in
    making predictions. In any ML problem, your dataset will often have multiple variables
    that contribute to the characteristics of your prediction column. However, in
    most cases, you will often have some features that contribute more compared to
    others.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: This understanding can help scientists and engineers remove any unwanted features
    that introduce noise from the dataset. This can further improve the quality of
    the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: H2O calculates variable importance differently for different types of algorithms.
    First, let’s understand how variable importance is calculated for **tree-based
    algorithms**.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Variable importance in tree-based algorithms is calculated based on two criteria:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Selection of the variable for deciding on the decision tree
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improvement in the squared error over the whole tree because of the selection
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whenever H2O is building a decision tree as a part of training a tree-based
    model, it will use one of the features as a node to further split the tree. As
    we studied in [*Chapter 5*](B17298_05.xhtml#_idTextAnchor109), *Understanding
    AutoML Algorithms*, in the *Understanding the Distributed Random Forest algorithm*
    section, we know that every node split in the decision tree aims to reduce the
    overall squared error. This deducted value is nothing but the difference between
    the squared errors of the parent node against the children node.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: H2O considers this reduction in squared error in calculating the feature importance.
    The squared error for every node in the tree-based model leads to the variance
    of the response value for that node being lowered. The squared error for every
    node in the tree-based model leads to the variance of the response value for that
    node being lowered.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, accordingly, the equation for calculating the squared error of the tree
    is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17298_07_001.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '*MSE* means mean squared error'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N* indicates the total number of observations'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*VAR* means variance'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The equation for calculating variance is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17298_07_002.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17298_07_003.png) indicates the value of the observation'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B17298_07_004.png) indicates the mean of all the observations'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N* indicates the total number of observations'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For tree-based ensemble algorithms such as the **Gradient Boosting Algorithm**
    (**GBM**), the decision trees are trained sequentially. Every tree is built on
    top of the previous tree’s errors. So, the feature importance calculation is the
    same as how we do it for individual nodes in single decision trees.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: For **Distributed Random Forest** (**DRF**), the decision trees are trained
    in parallel, so H2O just averages the results to calculate the feature importance.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: For **XGBoost**, H2O calculates the feature importance from the gains of the
    loss function for individual features when building the tree.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: For **deep learning**, H2O calculates the feature importance using a special
    method called the **Gedeon method**.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: For **Generalized Linear Models** (**GLMs**), variable importance is the same
    as the predictor weights, also called the coefficient magnitudes. If, during training,
    you decide to standardize the data, then the standardized coefficients are returned.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the feature importance that was calculated for
    our experiment on the Iris flower dataset:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Variable importance graph for the Iris flower dataset ](img/B17298_07_005.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Variable importance graph for the Iris flower dataset
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows the variable importance map for a deep learning
    model. If you compare it with your leaderboard, you will see that the variable
    importance graph is plotted for the most leading model, which is not a stacked
    ensemble model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: On the *Y*-axis of the graph, you have the feature names – in our case, the
    **C1**, **C2**, **C3**, and **C4** columns of the Iris flower dataset. On the
    *X*-axis, you have the importance of these variables. It is possible to get the
    raw metric value of feature importance, but H2O displays the importance values
    by scaling them down between **0** and **1**, where **1** indicates the most important
    variable while **0** indicates the least important variable.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Since variable importance is available for both classification and regression
    models, you will also get a variable importance graph as an explainability feature
    of the Red Wine Quality regression model. The graph should look as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Variable importance graph for the Red Wine Quality dataset  ](img/B17298_07_006.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Variable importance graph for the Red Wine Quality dataset
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to interpret a feature importance graph, let’s understand
    feature importance heatmaps.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Understanding feature importance heatmaps
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When displaying feature importance for a specific model, it is fairly easy to
    represent it as a histogram or bar graph. However, we often need to compare the
    feature importance of various models so that we can understand which feature is
    deemed important by which model and how we can use this information to compare
    model performance. H2O AutoML will inherently train multiple models with different
    ML algorithms. Therefore, a comparative study of the model performance is a must
    and a graphical representation of feature importance can be of great help to scientists
    and engineers.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: To represent the feature importance of all the models trained by H2O AutoML
    in a single graph, H2O generates a heatmap of feature importance.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: A heatmap is a data visualization graph where the color of the graph is affected
    by the amount of density or magnitude of a specific value.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Some H2O models compute the variable importance on encoded versions of categorical
    columns. Different models also have different ways of encoding categorical values.
    So, comparing the variable importance of these categorical columns across all
    the models can be tricky. H2O does this comparison by summarizing the variable
    importance across all the features and returning a single variable importance
    value that represents the original categorical feature.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the feature importance heatmap for the Iris flower dataset
    experiment:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Variable importance heatmap ](img/B17298_07_007.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Variable importance heatmap
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the top 10 models on the leaderboard.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: The heatmap has the **C1**, **C2**, **C3**, and **C4** features on the *Y*-axis
    and the model IDs on the *X*-axis. The color of the plots indicates how important
    the model considers the feature during its prediction. More importance equals
    more value, which, in turn, turns the respective plot red. The lower the importance,
    the lower the importance value of the feature will be; the color will become cooler
    and become blue.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to interpret feature importance heatmaps, let’s learn
    about model correlation heatmaps.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model correlation heatmaps
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important comparison between multiple models is **model correlation**.
    Model correlation can be interpreted as how similar the models are in terms of
    performance when you compare their prediction values.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Different models trained using the same or different ML algorithms are said
    to be highly correlated if the predictions made by one model are the same or similar
    to the predictions made by the other.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: In a model correlation heatmap, H2O compares the prediction values of all the
    models that it trains and compares them to one another.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the model correlation heatmap graph we got from our experiment
    on the Iris flower dataset:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Model correlation heatmap for the Iris flower dataset ](img/B17298_07_008.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Model correlation heatmap for the Iris flower dataset
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: To understand this explainability feature graph, kindly refer to the *Model
    Correlation* section in the output you got after executing the `explain()` function
    in your code.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: On the *X* and *Y* axes, we have the model IDs. Their cross-section on the graph
    indicates the correlation value between them. You will notice that the heat points
    on the graph within the *X* and *Y* axes have the same model ID, which will always
    be 1; therefore, the plot will always be red. This is correct as, technically,
    it’s the same model and when you compare the prediction values of a model with
    itself, there will be 100% correlation.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: To get a better idea of the correlation between different models, you can refer
    to these heat values. Dark red points indicate high correlation, while those with
    cool blue values indicate low correlation. Models highlighted in red are interpretable
    models such as GLMs.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: You may notice that since the model correlation heatmap supports stacked ensemble
    models and feature importance heatmaps don’t, if you ignore the stacked ensemble
    models in the model correlation heatmap (*Figure 7.8*), the rest of the models
    are the same as the ones in the feature importance heatmap (*Figure 7.7*).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to interpret model correlation heatmaps, let’s learn more
    about partial dependency plots.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Understanding partial dependency plots
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **Partial Dependence Plot** (**PDP**) is a graph diagram that shows you the
    dependency between the predicted values and the set of input features that we
    are interested in while marginalizing the values of features in which we are not
    interested.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Another way of understanding a PDP is that it represents a function of input
    features that we are interested in that gives us the expected predicted values
    as output.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: A PDP is a very interesting graph that is useful in showing and explaining the
    model training results to members of the organization that are not so skilled
    in the domain of data science.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s understand how to interpret a DPD before learning how it is calculated.
    The following diagram shows the PDP we got for our experiment when using the Iris
    flower dataset:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – PDP for the C1 column with Iris-setosa as the target ](img/B17298_07_009.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – PDP for the C1 column with Iris-setosa as the target
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: To understand this explainability feature graph, kindly refer to the *Partial
    Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: The PDP plot is a graph that shows you the marginal effects of a feature on
    the response values. On the *X*-axis of the graph, you have the selected feature
    and its range of values. On the *Y*-axis, you have the mean response values for
    the target value. The PDP plot aims to tell the viewer what the mean response
    value predicted by the model for a given value of the selected feature is.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7.9*, the PDP graph is plotted for the **C1** column for the target
    value, which is **Iris-setosa**. On the *X*-axis, we have the **C1** column, which
    stands for the sepal length of the flower in centimeters. The range of these values
    ranges from the minimum value present in the dataset to the maximum value. On
    the *Y*-axis, we have the mean response values. For this experiment, the mean
    response values are probabilities that the flower is an Iris-setosa, which is
    the selected target value of the plot. The colorful lines on the graph indicate
    the mean response values predicted by the different models trained by H2O AutoML
    for the range of **C1** values.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Looking at this graph gives us a good idea of how the response value is dependent
    on the single feature, **C1**, for every individual model. We can see that so
    long as the sepal length lies between 4.5 to 6.5 centimeters, most of the models
    show an approximate probability that there is a 35% chance that the flower is
    of the Iris-setosa class.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, in the following graph, we have plotted the PDP graph for the **C1**
    column, only this time the target response column is **Iris-versicolor**:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – PDP for the C1 column with Iris-versicolor as the target ](img/B17298_07_010.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – PDP for the C1 column with Iris-versicolor as the target
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: To understand this explainability feature graph, kindly refer to the *Partial
    Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that so long the values of **C1** are between 4.5 to 6.5,
    there is around a 27% to 40% chance that the flower is of the Iris-versicolor
    class. Now, let’s look at the following PDP plot for **C1** for the third target
    value, **Iris-virginica**:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – PDP for the C1 column with Iris-virginica as the target ](img/B17298_07_011.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – PDP for the C1 column with Iris-virginica as the target
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: To better understand this explainability feature graph, kindly refer to the
    *Partial Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that, for **Iris-virginica**, all the models predict differently
    for the same values of **C1**. This could mean that the **Iris-virginica** class
    is not strongly dependent on the sepal length of the flower – that is, the **C1**
    value.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Another case where PDP might be useful is in model selection. Let’s assume you
    are certain that a specific feature in your dataset will be contributing greatly
    to the response value and you train multiple models on it. Then, you can choose
    the model that best suits this relationship as that model will make the most realistically
    accurate predictions.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s try to understand how the PDP plot is generated and how H2O computes
    these plot values.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'The PDP plot data can be calculated as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Choose a feature and target value to plot the dependency.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bootstrap a dataset from the validation dataset, where the value of the selected
    feature is set to the minimum value present in the validation dataset for all
    rows.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass this bootstrapped dataset to one of the models trained by H2O AutoML and
    calculate the mean of the prediction values it got for all rows.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot this value on the PDP graph for that model.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 3* and *4* for the remaining models.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *step 2*, but this time, increment the value of the selected feature
    to the next value present in the validation dataset. Then, repeat the remaining
    steps.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will do this for all the feature values present in the validation dataset
    and plot them on the results for all the models on the same PDP graph.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Once finished, you will repeat the same process for different combinations of
    the feature and target response values.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O will make multiple PDP plots for all the combinations of features and response
    values. The following is a PDP plot where the selected feature is **C2** and the
    selected target value is **Iris-setosa**:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – PDP for the C2 column with Iris-setosa as the target ](img/B17298_07_012.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – PDP for the C2 column with Iris-setosa as the target
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: To better understand this explainability feature graph, kindly refer to the
    *Partial Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, it created different combinations of the PDP plot for the **C3**
    and **C4** features. The following is a PDP plot where the selected feature is
    **C3** and the selected target value is **Iris-versicolor**:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – PDP for the C3 column with Iris-versicolor as the target ](img/B17298_07_013.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – PDP for the C3 column with Iris-versicolor as the target
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: To better understand this explainability feature graph, kindly refer to the
    *Partial Dependency Plots* section in the output you got after executing the `explain()`
    function in your code.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to interpret feature importance heatmaps, let’s learn
    about SHAP summary plots.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Understanding SHAP summary plots
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For sophisticated problems, tree-based models can become difficult to understand.
    Complex tree models can be very large and complicated to understand. The **SHAP
    summary plot** is a simplified graph of the tree-based model that gives you a
    summarized view of the model’s complexity and how it behaves.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '**SHAP** stands for **Shapley Additive Explanations**. SHAP is a model explainability
    feature that takes an approach from game theory to explain the output of an ML
    model. The SHAP summary plot shows you the contribution of the features toward
    predicting values, similar to PDPs.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to interpret a SHAP value from an example. The following is the SHAP
    summary we get from the Red Wine Quality dataset:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – SHAP summary plot for the Red Wine Quality dataset ](img/B17298_07_014.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – SHAP summary plot for the Red Wine Quality dataset
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: To better understand this explainability feature graph, kindly refer to the
    *SHAP Summary* section in the output you get after executing the `explain()` function
    on your regression model.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: On the right-hand side, you can see a bluish-red bar. This bar represents the
    normalized value of the wine quality in color. The redder the color, the better
    the quality; the bluer the color, the poorer the wine quality. In binomial problems,
    the color will be a stark contrast between red and blue. However, in regression
    problems, like in our example, we can have a whole spectrum of colors, indicating
    the range of possible numerical values.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: On the *Y*-axis, you have the features from the dataset. They are in descending
    order from top to bottom based on the feature’s importance. In our example, the
    alcohol content is the most important feature in the dataset; it contributes more
    to the final prediction value.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: On the *X*-axis, you have the **SHAP value**. The SHAP value denotes how the
    feature helps the model toward the expected outcome. The more positive the SHAP
    value, the more the feature contributes to the outcome.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the example of alcohol from the SHAP summary. Based on this, we can
    see that alcohol has the highest SHAP value among the rest of the features. Thus,
    alcohol contributes greatly to the model’s prediction. Also, the points on the
    graph for alcohol with the highest SHAP value are in red. This also indicates
    that high alcohol content contributes to a positive outcome. Keeping this in mind,
    what we can extract from this graph is that the feature alcohol content plays
    an important part in the prediction of the quality of the wine and that the higher
    the content of the alcohol, the better the quality of the wine.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you can interpret the same knowledge from the other features. This
    can help you compare and understand which features are important and how they
    contribute to the final prediction of the model.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: One interesting question on the SHAP summary and the PDP is, what is the difference
    between them? Well, the prime difference between the two is that PDP explains
    the effect of replacing only one feature at a time on the output, while the SHAP
    summary considers the overall interaction of that feature with other features
    in the dataset. So, PDP works on the assumption that your features are independent
    of one another, while SHAP takes into account the combined contributions of different
    features and their combined effects on the overall prediction.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Calculating **SHAP values** is a complex process that is derived from game theory.
    If you are interested in expanding your knowledge of game theory and how SHAP
    values are calculated, feel free to explore them at your own pace. A good starting
    point for understanding SHAP is to follow the explanations at https://shap.readthedocs.io/en/latest/index.xhtml.
    At the time of writing, H2O acts as a wrapper for the SHAP library and internally
    uses this library to calculate the SHAP values.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to interpret a SHAP summary plot, let’s learn about explainability
    feature, **Individual Conditional Expectation** (**ICE**) plots.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Understanding individual conditional expectation plots
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An **ICE** plot is a graph that displays a line for every instance of an observation
    that shows how the prediction for the given observation changes when the value
    of a feature changes.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: ICE plots are similar to PDP graphs. PDP focuses on the overall average effect
    of a change in a feature on the prediction outcome, while ICE plots focus on the
    dependency of the outcome on individual instances of the feature value. If you
    average the ICE plot values, you should get a PDP.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'The way to compute ICE plots is very simple, as shown in the following screenshot:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Sample dataset for ICE graph plots highlighting Observation
    1 ](img/B17298_07_015.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Sample dataset for ICE graph plots highlighting Observation 1
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Once your model has been trained, you must perform the following steps to calculate
    the ICE plots:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Consider the first observation – in our example, **Observation 1** – and plot
    the relationship between **Feature 1** and the respective **Target** value.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keeping the values in **Feature 1** constant, create a bootstrapped dataset
    while replacing all the other feature values with those seen in **Observation
    1** in the original dataset; mark all other observations as **Observation 1**.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the **Target** value of the observations using your trained model.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Refer to the following screenshot for the bootstrapped dataset:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Bootstrapped dataset for Observation 1 for Feature 1 ](img/B17298_07_016.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Bootstrapped dataset for Observation 1 for Feature 1
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeat the same for the next observation. Consider the second observation –
    in our example, **Observation 2** – and plot the relationship between **Feature
    1** and the respective **Target** value:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Sample dataset for an ICE plot highlighting Observation 2 ](img/B17298_07_017.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – Sample dataset for an ICE plot highlighting Observation 2
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep the values in **Feature 1** constant and create a bootstrapped dataset;
    then, calculate the **Target** values using the trained model. Refer to the following
    resultant bootstrapped dataset:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Bootstrapped dataset for Observation 2 for Feature 1 ](img/B17298_07_018.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Bootstrapped dataset for Observation 2 for Feature 1
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: We repeat this process for all observations against all features.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The results that are observed from these bootstrapped datasets are plotted on
    the individual ICE plots per feature.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s see how we can interpret the ICE plot and extract observable information
    out of the graph. Refer to the following screenshot, which shows the ICE plot
    we get after running the model explainability interface on the AutoML object that
    was trained on the Red Wine Quality dataset:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – ICE plot for the Red Wine Quality dataset ](img/B17298_07_019.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – ICE plot for the Red Wine Quality dataset
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'As the heading states, this is an ICE plot on the alcohol feature column of
    the dataset for a stacked ensemble model trained by H2O AutoML. Keep in mind that
    this model is the leader of the list of models trained by AutoML. ICE plots are
    only plotted for the leader of the dataset. You can also observe the ICE plots
    of the other models by extracting the models using their model IDs and then running
    the `ice_plot()` function on them. Refer to the following code example:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: On the *X*-axis of the graph, you have the range of values for the alcohol feature.
    On the *Y*-axis, you have the range of values of the predicted outcomes – that
    is, the quality of the wine.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: On the left-hand side of the graph, you can see the legends stating different
    types of lines and their percentiles. The ICE plot plots the effects for each
    decile. So, technically, when plotting the ICE plot, you compute a line for every
    observation. However, in a dataset that contains thousands if not millions of
    rows of data, you will end up with an equal number of lines on the plot. This
    will make the ICE plot messy. That is why to better observe this data, you must
    aggregate the lines together to the nearest decile and plot a single line for
    every percentile partition.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: The dotted black line is the average of all these other percentile lines and
    is nothing but the PDP line for that feature.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to interpret an ICE plot, let’s look at learning curve
    plots.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Understanding learning curve plots
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **learning curve plot** is one of the most used plots by data scientists
    to observe the learning rate of a model. The **learning curve** shows how your
    model learns from the dataset and the efficiency with which it does the learning.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: When working on an ML problem, an important question that often needs to be
    answered is, *how much data do we need to train the most accurate model?* A learning
    curve plot can help you understand how increasing the dataset affects your overall
    model performance.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Using this information, you can decide if increasing the size of the dataset
    can result in better model performance or if you need to work on your model training
    to improve your model’s performance.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s observe the learning curve plot we got from our experiment on the Red
    Wine Quality dataset for the XRT model trained by AutoML:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Learning curve plot for the XRT model on the Red Wine Quality
    dataset ](img/B17298_07_020.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – Learning curve plot for the XRT model on the Red Wine Quality
    dataset
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: On the *X*-axis of the graph, you have the number of trees created by the XRT
    algorithm. As you can see, the algorithm created around 40 to 50 trees in total.
    On the *Y*-axis, you have the performance metric, RMSE, which is calculated at
    every stage during the model training as the algorithm creates the trees.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding screenshot, the RMSE metric decreases as the algorithm
    creates more trees. Eventually, the rate at which the RMSE lowers decreases over
    a certain number of trees created. Any trees created over this number do not contribute
    to the overall improvement in the model’s performance. Thus, the learning rate
    eventually decreases over the increase in several trees.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: The lines on the graph depict the various datasets that were used by the algorithm
    during training and the respective RMSE during every instance of creating the
    trees.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, as of H2O version *3.36.1*, the learning curve plot
    is not part of the default model explainability interface. To plot the learning
    curve, you must plot it using the following function on the respective model object:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The learning curve plot is different for different algorithms. The following
    screenshot shows a learning plot for a GLM model trained by AutoML on the same
    dataset:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Learning curve plot for the GLM model on the Red Wine Quality
    dataset ](img/B17298_07_021.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – Learning curve plot for the GLM model on the Red Wine Quality
    dataset
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, instead of the number of trees on the *X*-axis, we now have
    iterations. The number of trees is relevant for tree-based algorithms such as
    XRT and DRF, but linear models such as GLM running on the linear algorithm makes
    more sense to aid with learning. On the *Y*-axis, you have deviance instead of
    RMSE as deviance is more suitable for measuring the performance of a linear model.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The learning curve is different for different types of algorithms, including
    stacked ensemble models. Feel free to explore the different variations of the
    learning curve for different algorithms. H2O already takes care of selecting the
    appropriate performance metric and the steps in learning, depending on the algorithms,
    so you don’t have to worry about whether you chose the right metric to measure
    the learning rate or not.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on understanding the model explainability interface
    provided by H2O. First, we understood how the explainability interface provides
    different explainability features that help users get detailed information about
    the models trained. Then, we learned how to implement this functionality on models
    trained by H2O’s AutoML in both Python and R.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Once we were comfortable with its implementation, we started exploring and understanding
    the various explainability graphs displayed by the explainability interface’s
    output, starting with residual analysis. We observed how residual analysis helps
    highlight heteroscedasticity in the dataset and how it helps you identify if there
    is any missing information in your dataset.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Then, we explored variable importance and how it helps you identify important
    features in the dataset. Building on top of this, we learned how feature importance
    heatmaps can help you observe feature importance among all the models trained
    by AutoML.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Then, we discovered how model correlation heatmaps can be interpreted and how
    they help us identify models with similar prediction behavior from a list of models.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Later, we learned about PDP graphs and how they express the dependency of the
    overall outcome over the individual features of the dataset. With this knowledge
    in mind, we explored the SHAP summary and ICE plots, where we understood the two
    graphs and how each focuses on different aspects of outcome dependency on individual
    features.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we explored what a learning plot is and how it helps us understand
    how the model improves in performance, also called learning, over the number of
    observations, iterations, or trees, depending on the type of algorithms used to
    train the model.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we shall use all the knowledge we’ve learned from the last
    few chapters and explore the other advanced parameters that are available when
    using H2O’s AutoML feature.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Part 3 H2O AutoML Advanced Implementation and Productization
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part will help you understand H2O AutoML’s advanced features and parameters
    used to customize certain characteristics of AutoML to suit specialized needs.
    This will help you get the desired personalized results that generalized machine
    learning fails to provide. It will also explain the various ways that H2O AutoML
    can be used with different types of technologies, and you will understand how
    you can deploy your machine learning models into production, and commercially
    use them to meet business needs.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'This section comprises the following chapters:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B17298_08.xhtml#_idTextAnchor169), *Exploring Optional Parameters
    for H2O AutoML*'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B17298_09.xhtml#_idTextAnchor186), [*Exploring Miscellaneous
    Features in H2O AutoML*](https://epic.packtpub.com/index.php?module=oss_Chapters&action=DetailView&record=3a065625-7e22-e0bf-231f-61a9d1f3e976)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B17298_10.xhtml#_idTextAnchor196), [*Working with Plain Old
    Java Objects (POJOs)*](https://epic.packtpub.com/index.php?module=oss_Chapters&action=DetailView&record=bb77c8ca-d15c-48c5-2b00-61a9d1abce98)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B17298_11.xhtml#_idTextAnchor210), *Working with Model Object
    Optimized (MOJO)*'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B17298_12.xhtml#_idTextAnchor225), [*Working with H2O AutoML
    and Apache Spark*](https://epic.packtpub.com/index.php?module=oss_Chapters&action=DetailView&record=854f151d-1690-0982-b488-61a9d16f9b67)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B17298_13.xhtml#_idTextAnchor239), *Using H2O AutoML with Other
    Technologies*'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
