<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer084">
			<h1 id="_idParaDest-93" class="chapter-number"><a id="_idTextAnchor093"/>7</h1>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor094"/>Training Fully Custom ML Models with Vertex AI</h1>
			<p>In the previous chapters, we learned about training no-code (Auto-ML) as well as low-code (BQML) <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) models with <a id="_idIndexMarker387"/>minimum technical expertise required. These solutions are really handy when it comes to solving common ML problems. However, sometimes the problem or data itself is so complex that it requires the development of custom <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>) models, in <a id="_idIndexMarker388"/>most cases large deep learning-based models. Working on custom models requires a significant level of technical expertise in the fields of ML, deep learning, and AI. Sometimes, even with this expertise, it becomes really difficult to manage training and experiments of large-scale custom deep learning models due to a lack of resources, compute, and proper metadata <span class="No-Break">tracking mechanisms.</span></p>
			<p>To make the lives of ML developers easier, Vertex AI provides a managed environment for launching large-scale custom training jobs. Vertex AI-managed jobs let us track useful metadata, monitor jobs through the Google Cloud console UI, and launch large-scale batch inference jobs without the need to actively monitor them. In this chapter, we will learn how to work with custom deep learning-based models on Google Vertex AI. Specifically, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Building a basic deep learning model <span class="No-Break">with TensorFlow</span></li>
				<li>Packaging a model to submit to Vertex AI as a <span class="No-Break">training job</span></li>
				<li>Monitoring model <span class="No-Break">training progress</span></li>
				<li>Evaluating <span class="No-Break">trained models</span></li>
			</ul>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor095"/>Technical requirements</h1>
			<p>This chapter requires basic-level knowledge of the deep learning framework TensorFlow and neural networks. Code artifacts can be found in the following GitHub repo – <a href="https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter07"><span class="No-Break">https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter07</span></a></p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor096"/>Building a basic deep learning model with TensorFlow</h1>
			<p><strong class="bold">TensorFlow</strong>, or <strong class="bold">TF</strong> for <a id="_idIndexMarker389"/>short, is an end-to-end platform for building ML models. The main focus of the TensorFlow framework is to simplify the development, training, evaluation, and deployment of deep neural networks. When it comes to working with <a id="_idIndexMarker390"/>unstructured data (such as images, videos, audio, etc.), neural network-based solutions have achieved significantly <a id="_idIndexMarker391"/>better results than traditional ML approaches that mostly rely on handcrafted features. Deep neural networks are good at understanding complex patterns from high-dimensional data points (for example, an image with millions of pixels). In this section, we will develop a basic neural network-based model using TensorFlow. In the next few sections, we will see how Vertex AI can help with setting up scalable and systemic training/tuning of such <span class="No-Break">custom models.</span></p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">It is important to note that TensorFlow is not the only ML framework that Vertex AI supports. Vertex AI supports many different ML frameworks and open-source projects including Pytorch, Spark and XGBoost. Pytorch is one of the fastest growing ML frameworks and with Vertex AI’s Pytorch integrations, we can easily train, deploy and orchestrate PyTorch models in production. Vertex AI provides prebuilt training and serving containers and also supports optimized distributed training of PyTorch models. Similarly, Vertex AI provides prebuilt training, serving and explainability features for multiple ML frameworks including XGBoost, TensorFlow, Pytorch <span class="No-Break">and Scikit-learn.</span></p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor097"/>Experiment – converting black-and-white images into color images</h2>
			<p>In<a id="_idIndexMarker392"/> this experiment, we will develop a<a id="_idIndexMarker393"/> TensorFlow-based deep learning model that takes black-and-white images as input and converts them into color images. As this exercise, requires developing a custom model, we will start our initial development work on a Jupyter Notebook. The first step is to create a user-managed Jupyter Notebook inside Vertex AI Workbench using a preconfigured TensorFlow image. More details on how to successfully create a Vertex AI Workbench notebook instance can be found in <a href="B17792_04.xhtml#_idTextAnchor056"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Vertex AI Workbench</em>. Next, let’s launch one Jupyter Notebook from the JupyterLab application. We are now all set to start working on <span class="No-Break">our experiment.</span></p>
			<p>We will start with importing useful libraries (prebuilt Python packages) in the first cell of our notebook. In this experiment, we will be using the following Python libraries – <strong class="source-inline">numpy</strong> for multi-dimensional array manipulation, TensorFlow for developing a deep learning model, OpenCV (or <strong class="source-inline">cv2</strong>) for image manipulation, and <strong class="source-inline">matplotlib</strong> for plotting images <span class="No-Break">or graphs:</span></p>
			<pre class="source-code">
import numpy as np
import tensorflow
import glob
import cv2
import matplotlib.pyplot as plt
%matplotlib inline</pre>			<p>We will <a id="_idIndexMarker394"/>need an image dataset with at least a few thousand images to train and test our model. In this experiment, we will work with the <strong class="bold">Oxford-IIIT Pet</strong> dataset, which<a id="_idIndexMarker395"/> is a public and free-to-use dataset. This dataset consists of around 7k pet images from more than <a id="_idIndexMarker396"/>30 different annotated categories. The dataset can be downloaded from the following website: <a href="https://www.robots.ox.ac.uk/~vgg/data/pets/">https://www.robots.ox.ac.uk/~vgg/data/pets/</a>. </p>
			<p>We can also download this dataset using the following commands in <span class="No-Break">our terminal:</span></p>
			<pre class="source-code">
wget <a href="https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz">https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz</a>
wget <a href="https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz">https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz</a></pre>			<p>Once the downloads are complete, put the zipped files in the same directory as our notebook. Now, let’s create a <strong class="bold">data</strong> folder, extract all images into it, and extract the annotations into another folder. We can do this in a notebook cell using the following command (the <strong class="source-inline">!</strong> sign in a notebook cell lets us run terminal commands from within <span class="No-Break">Jupyter Notebook):</span></p>
			<pre class="source-code">
!mkdir data
!tar -xf images.tar.gz -C data
!mkdir labels
!tar -xf annotations.tar.gz -C labels</pre>			<p>As our current experiment is concerned with converting black-and-white images into colored versions, we will not be using annotations. Now, let’s quickly verify in a new cell whether we have all the images successfully copied into the <span class="No-Break"><strong class="bold">data</strong></span><span class="No-Break"> folder:</span></p>
			<pre class="source-code">
all_image_files = glob.glob("data/images/*.jpg")
print("Total number of image files : ", \
    len(all_image_files))</pre>			<p>Here, the <strong class="source-inline">glob</strong> module helps us by listing all the <strong class="source-inline">.jpg</strong> image paths inside the data directory. The length of this list will be equal to the number of images. The preceding code should print the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
Total number of image files :  7390</pre>			<p>Now that we<a id="_idIndexMarker397"/> have successfully downloaded<a id="_idIndexMarker398"/> and extracted data, let’s check a few images to be sure everything is fine. The following code block will plot a few random images with <span class="No-Break">their annotations:</span></p>
			<pre class="source-code">
for i in range(3):
    plt.figure(figsize=(13, 13))
    for j in range(6):
        img_path = np.random.choice(all_image_files)
        img = cv2.imread(img_path)
        img_class = img_path.split("/")[-1].split("_")[0]
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        plt.subplot(660 + 1 + j)
        plt.imshow(img)
        plt.axis('off')
        plt.title(img_class)
    plt.show()</pre>			<p>Here, we are extracting the image class (or annotation) from the image path name itself as all images have the pet category in their filenames. The output of the preceding code should look something like shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B17792_07_1.jpg" alt="Figure 7.1 – A few samples from the pet dataset" width="1174" height="597"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – A few samples from the pet dataset</p>
			<p>Now that we <a id="_idIndexMarker399"/>have verified our dataset, let’s split <a id="_idIndexMarker400"/>these images into three sets – train, validation, and test – as we usually do for training/validating and testing ML models. We will keep 60% of the images for training, 20% for validation, and the remaining 20% for testing. One simple way to do these splits is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
train_files = all_image_files[:int( \
    len(all_image_files)*0.6)]
validation_files = all_image_files[int( \
    len(all_image_files)*0.6):int(len(all_image_files)*0.8)]
test_files = all_image_files[int( \
    len(all_image_files)*0.8):]
print(len(train_files), len(validation_files), \
    len(test_files))</pre>			<p>The main focus of this experiment is to develop a deep learning model that converts black-and-white images into color images. To learn this mapping, the model will require pairs of black-and-white and corresponding color versions in order to learn this mapping. Our dataset already has color images. We will utilize the OpenCV library to convert them into grayscale (black-and-white) images and use them as input in our model. We will compare the output with their color versions. Another important thing to keep in mind is that our deep learning model will take fixed-size images as inputs, so we also need to bring all input images into a common resolution. In our experiment, we will change all of our images to have an 80x80 resolution. We already have training, validation, and test splits of the image paths. We can now read those files and prepare data for training, validation, and testing purposes. The following code blocks can be used to prepare the dataset as <span class="No-Break">described previously.</span></p>
			<p>Let’s first define<a id="_idIndexMarker401"/> empty lists for storing <a id="_idIndexMarker402"/>training, validation, and test <span class="No-Break">data, respectively:</span></p>
			<pre class="source-code">
train_x = []
train_y = []
val_x = []
val_y = []
test_x = []
test_y = []</pre>			<p>Here, we read training images, resize them to the required size, create a black-and-white version of each of them, and store them as <span class="No-Break">target images:</span></p>
			<pre class="source-code">
for file in train_files:
    try:
        img = cv2.imread(file)
        img = cv2.resize(img, (80,80))
        color_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        black_n_white_img = cv2.cvtColor(color_img, \
            cv2.COLOR_RGB2GRAY)
    except:
        continue
    train_x.append((black_n_white_img-127.5)/127.5)
    train_y.append((color_img-127.5)/127.5)</pre>			<p>Similarly, we repeat the same process for <span class="No-Break">validation files:</span></p>
			<pre class="source-code">
for file in validation_files:
    try:
        img = cv2.imread(file)
        img = cv2.resize(img, (80,80))
        color_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        black_n_white_img = cv2.cvtColor(color_img, \
            cv2.COLOR_RGB2GRAY)
    except:
        continue
    val_x.append((black_n_white_img-127.5)/127.5)
    val_y.append((color_img-127.5)/127.5)</pre>			<p>Now, prepare<a id="_idIndexMarker403"/> test files as well in a <span class="No-Break">similar </span><span class="No-Break"><a id="_idIndexMarker404"/></span><span class="No-Break">way:</span></p>
			<pre class="source-code">
for file in test_files:
    try:
        img = cv2.imread(file)
        img = cv2.resize(img, (80,80))
        color_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        black_n_white_img = cv2.cvtColor(color_img, \
            cv2.COLOR_RGB2GRAY)
    except:
        continue
    test_x.append((black_n_white_img-127.5)/127.5)
    test_y.append((color_img-127.5)/127.5)</pre>			<p>Note that images are represented with pixel values ranging from 0 to 255. We are normalizing pixel values and bringing them into the range [-1, 1] by subtracting and dividing by 127.5. Data normalization makes the optimization of deep learning models smoother and <span class="No-Break">more stable.</span></p>
			<p>Now that we have successfully prepared our dataset with train, validation, and test set splits, let’s check a few samples to confirm that the data has been prepared correctly. The following code block chooses some random training set images and <span class="No-Break">plots them.</span></p>
			<p>Let’s plot<a id="_idIndexMarker405"/> some input images to get a <a id="_idIndexMarker406"/>sense of <span class="No-Break">the data:</span></p>
			<pre class="source-code">
# input images
indexes = np.random.choice(range(0,4000), size=3)
print("Input Samples (black and white): ")
plt.figure(figsize=(7,7))
for i in range(3):
    plt.subplot(330+1+i)
    plt.imshow((train_x[indexes[i]]+1.0)/2.0, cmap='gray')
    plt.axis('off')
plt.show()</pre>			<p>We will also be plotting the output versions (colored versions) of these randomly <span class="No-Break">chosen images:</span></p>
			<pre class="source-code">
# corresponding output images
print("Output Samples (colored): ")
plt.figure(figsize=(7,7))
for i in range(3):
    plt.subplot(330+1+i)
    plt.imshow((train_y[indexes[i]]+1.0)/2.0)
    plt.axis('off')
plt.show()</pre>			<p>If everything is correct, we should see input-output pair images very similar to as in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B17792_07_2.jpg" alt="" role="presentation" width="788" height="559"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Sample input-output pairs for data verification</p>
			<p>As we are<a id="_idIndexMarker407"/> dealing with image data here, we<a id="_idIndexMarker408"/> will be working with a <strong class="bold">Convolutional Neural Network</strong> (<strong class="bold">CNN</strong>)-based model<a id="_idIndexMarker409"/> so that we can extract useful features from image data. The current research shows that CNNs can be very useful in extracting features and other useful information from image data. As we will be working with CNNs here, we need to convert our image dataset into NumPy arrays, and also add one channel dimension to each black-and-white input image (CNNs accept image input as a three-dimensional array, one dimension each for width, height, and channels). A colored image will already have three channels, one for each color value – R, G, and B. The following code block prepares our final dataset as per the steps <span class="No-Break">described previously:</span></p>
			<pre class="source-code">
train_x = np.expand_dims(np.array(train_x),-1)
val_x = np.expand_dims(np.array(val_x),-1)
test_x = np.expand_dims(np.array(test_x),-1)
train_y = np.array(train_y)
val_y = np.array(val_y)
test_y = np.array(test_y)</pre>			<p>Now, once again, let’s check the dimensions of our <span class="No-Break">dataset splits:</span></p>
			<pre class="source-code">
print(train_x.shape, train_y.shape, val_x.shape, \
    val_y.shape, test_x.shape, test_y.shape)</pre>			<p>It should print something <span class="No-Break">like this:</span></p>
			<pre class="source-code">
(4430, 80, 80, 1) (4430, 80, 80, 3) (1478, 80, 80, 1) (1478, 80, 80, 3) (1476, 80, 80, 1) (1476, 80, 80, 3)</pre>			<p>Everything <a id="_idIndexMarker410"/>looks great from a data perspective. Let’s<a id="_idIndexMarker411"/> jump into defining our neural <span class="No-Break">network architecture.</span></p>
			<p>In this experiment, we will define a TensorFlow-based CNN that takes black-and-white images as input and predicts their colored variants as output. The model architecture can be broadly divided into two<a id="_idIndexMarker412"/> parts – <strong class="bold">encoder</strong> and <strong class="bold">decoder</strong>. The encoder part of the model takes a black-and-white image as input and<a id="_idIndexMarker413"/> extracts useful features from it by passing it through four down-sampling convolutional layers. Each convolutional layer is followed by layers <a id="_idIndexMarker414"/>of <strong class="bold">LeakyReLU</strong> activation<a id="_idIndexMarker415"/> and <strong class="bold">batch normalization</strong> except for the last layer, which has a <strong class="bold">dropout</strong> layer<a id="_idIndexMarker416"/> in place of batch normalization. After passing through the encoder model, an input image with the dimensions (80, 80, 1) changes into a feature vector with the dimensions (5, <span class="No-Break">5, 256).</span></p>
			<p>The second part of the model is called the decoder. The decoder part takes the feature vector from the encoder output and converts it back into a colored version of the corresponding input image. The decoder is made up of four transpose-convolutional or up-sampling layers. Each decoder layer is followed by layers of ReLU activation and batch normalization except for the last layer, which has tanh activation and does not have a normalization layer. tanh activation restricts final output vector values into the range [-1,1], which is desired for our <span class="No-Break">output image.</span></p>
			<p>The following code blocks define the <span class="No-Break">TensorFlow model:</span></p>
			<pre class="source-code">
def tf_model():
    black_n_white_input = tensorflow.keras.layers.Input( \
        shape=(80, 80, 1))
    enc = black_n_white_input</pre>			<p>The <a id="_idIndexMarker417"/>encoder part starts from here, within the <span class="No-Break">same </span><span class="No-Break"><a id="_idIndexMarker418"/></span><span class="No-Break">function:</span></p>
			<pre class="source-code">
    #Encoder part
    enc = tensorflow.keras.layers.Conv2D(32, kernel_size=3, \
        strides=2, padding='same')(enc)
    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)
    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)
    enc = tensorflow.keras.layers.Conv2D(64, kernel_size=3, \
        strides=2, padding='same')(enc)
    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)
    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)
    enc = tensorflow.keras.layers.Conv2D(128, \
        kernel_size=3, strides=2, padding='same')(enc)
    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)
    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)
    enc = tensorflow.keras.layers.Conv2D(256, \
        kernel_size=1, strides=2, padding='same')(enc)
    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)
    enc = tensorflow.keras.layers.Dropout(0.5)(enc)</pre>			<p>The <a id="_idIndexMarker419"/>decoder part starts from here, within the <span class="No-Break">same </span><span class="No-Break"><a id="_idIndexMarker420"/></span><span class="No-Break">function:</span></p>
			<pre class="source-code">
    #Decoder part
    dec = enc
    dec = tensorflow.keras.layers.Conv2DTranspose(256, \
        kernel_size=3, strides=2, padding='same')(dec)
    dec = tensorflow.keras.layers.Activation('relu')(dec)
    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)
    dec = tensorflow.keras.layers.Conv2DTranspose(128, \
        kernel_size=3, strides=2, padding='same')(dec)
    dec = tensorflow.keras.layers.Activation('relu')(dec)
    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)
    dec = tensorflow.keras.layers.Conv2DTranspose(64, \
        kernel_size=3, strides=2, padding='same')(dec)
    dec = tensorflow.keras.layers.Activation('relu')(dec)
    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)
    dec = tensorflow.keras.layers.Conv2DTranspose(32, \
        kernel_size=3, strides=2, padding='same')(dec)
    dec = tensorflow.keras.layers.Activation('relu')(dec)
    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)
    dec = tensorflow.keras.layers.Conv2D(3, kernel_size=3,\
        padding='same')(dec)</pre>			<p>Finally, add<a id="_idIndexMarker421"/> tanh activation to get the required output<a id="_idIndexMarker422"/> image in <span class="No-Break">colored format:</span></p>
			<pre class="source-code">
    color_image = tensorflow.keras.layers.Activation('tanh')(dec)
    return black_n_white_input, color_image</pre>			<p>Now, let’s create a TensorFlow model object and print the summary of <span class="No-Break">our model:</span></p>
			<pre class="source-code">
black_n_white_input, color_image = tf_model()
model = tensorflow.keras.models.Model( \
    inputs=black_n_white_input, outputs=color_image)
model.summary()</pre>			<p>This should <a id="_idIndexMarker423"/>print the model summary as shown<a id="_idIndexMarker424"/> in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B17792_07_3.jpg" alt="Figure 7.3 – TensorFlow model summary (see full summary on Github)" width="675" height="935"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – TensorFlow model summary (see full summary on GitHub)</p>
			<p>As we can <a id="_idIndexMarker425"/>see from the summary, our model<a id="_idIndexMarker426"/> has roughly 1.1 million trainable parameters. The next step is to compile the <span class="No-Break">TensorFlow model:</span></p>
			<pre class="source-code">
_optimizer = tensorflow.keras.optimizers.Adam(\
    learning_rate=0.0002, beta_1=0.5)
model.compile(loss='mse', optimizer=_optimizer)</pre>			<p>We are using the Adam optimizer with a learning rate of 0.0002 and the <strong class="source-inline">beta_1</strong> parameter with a value of 0.5. Here, <strong class="source-inline">beta_1</strong> represents the value for the exponential decay rate for the first-moment estimates and the learning rate tells the optimizer the rate of updating the model parameter values during training. The rest of the parameter values are kept as the default. The idea is to pass a black-and-white image and reconstruct its colored version, so we will be using the <strong class="bold">Mean Squared Error</strong> (<strong class="bold">MSE</strong>) loss function<a id="_idIndexMarker427"/> as a reconstruction loss on the <span class="No-Break">pixel level.</span></p>
			<p>We are all set to start<a id="_idIndexMarker428"/> the training now. We will train <a id="_idIndexMarker429"/>our model for about 100 epochs, with a batch size of 128 for this experiment, and check the results. The following code snippet starts <span class="No-Break">the training:</span></p>
			<pre class="source-code">
history = model.fit(
    train_x,
    train_y,
    batch_size=128,
    epochs=100,
    validation_data=(val_x, val_y),
)</pre>			<p>The output logs should look something like <span class="No-Break">the following:</span></p>
			<pre class="source-code">
Epoch 1/100
35/35 [==============================] - 25s 659ms/step - loss: 0.2940 - val_loss: 0.1192
Epoch 2/100
35/35 [==============================] - 20s 585ms/step - loss: 0.1117 - val_loss: 0.0917
Epoch 3/100
35/35 [==============================] - 20s 580ms/step - loss: 0.0929 - val_loss: 0.0784
Epoch 4/100
35/35 [==============================] - 20s 577ms/step - loss: 0.0832 - val_loss: 0.0739
Epoch 5/100
35/35 [==============================] - 20s 573ms/step - loss: 0.0778 - val_loss: 0.0698
. . . . .
. . . . .
. . . . .
. . . . .
Epoch 100/100
35/35 [==============================] - 20s 559ms/step - loss: 0.0494 - val_loss: 0.0453</pre>			<p>To<a id="_idIndexMarker430"/> check whether our training went<a id="_idIndexMarker431"/> smoothly, we can have a look at the loss charts from the <span class="No-Break"><strong class="source-inline">history</strong></span><span class="No-Break"> variable:</span></p>
			<pre class="source-code">
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.show()</pre>			<p>The preceding snippet will plot the training and validation loss as a line chart for all the training epochs. The output graphs should look something like <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em>. As we can see, training<a id="_idIndexMarker432"/> and validation loss are consistently<a id="_idIndexMarker433"/> decreasing as training progresses. It is reassuring that our training is going in the <span class="No-Break">right direction.</span></p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B17792_07_4.jpg" alt="Figure 7.4 – Training and validation loss" width="818" height="630"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Training and validation loss</p>
			<p>The final step is now to check the results on an unseen test dataset. The following code chooses some random samples from <strong class="source-inline">test_set</strong> and generates model outputs for them. We’ve also plotted input images, model-generated colored images, and actual colored images for <span class="No-Break">understanding purposes:</span></p>
			<pre class="source-code">
samples = np.random.choice(range(0, len(test_files)), size=5)</pre>			<p>Plot a few test images to verify the <span class="No-Break">model outputs:</span></p>
			<pre class="source-code">
# show input images
print("Black-n-White Input images!")
plt.figure(figsize=(8,8))
for i in range(5):
    plt.subplot(550+1+i)
    plt.imshow((test_x[samples[i]]+1.0)/2.0, cmap='gray')
    plt.axis('off')
plt.show()</pre>			<p>Here, the <a id="_idIndexMarker434"/>model generates a <span class="No-Break">colored </span><span class="No-Break"><a id="_idIndexMarker435"/></span><span class="No-Break">version:</span></p>
			<pre class="source-code">
# generate color images from model
print("Model Outputs!")
plt.figure(figsize=(8,8))
for i in range(5):
    plt.subplot(550+1+i)
    model_input = test_x[samples[i]]
    output = model.predict(np.array([model_input]))
    plt.imshow((output[0]+1.0)/2.0)
    plt.axis('off')
plt.show()</pre>			<p>Also, plot the real colored version <span class="No-Break">for reference:</span></p>
			<pre class="source-code">
# show real color output images
print("Real Color Version!")
plt.figure(figsize=(8,8))
for i in range(5):
    plt.subplot(550+1+i)
    plt.imshow((test_y[samples[i]]+1.0)/2.0)
    plt.axis('off')
plt.show()</pre>			<p>When <a id="_idIndexMarker436"/>plotting model outputs or <a id="_idIndexMarker437"/>input images, we add 1.0 to the image array and divide by 2.0. We are doing this because during data preprocessing, we normalized image pixel values into the range [-1,1]. But ideally, image pixel values can’t be negative, so we need to inverse our transformation for image plotting purposes. So, adding 1.0 and dividing by 2.0 brings pixel values into the range [0,1], which is supported by Matplotlib for plotting. See <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B17792_07_5.jpg" alt="Figure 7.5 – Black-and-white to color model output" width="744" height="506"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Black-and-white to color model output</p>
			<p>As we can see from the preceding outputs, our model is learning some kind of colorization, which is, of course, not ideal but still looks pretty good. An interesting thing to notice is that it is not filling the color gradient randomly; we can clearly spot the main objects as they have a different contrast from the background. Given that we had a very small model and small training dataset, this performance is <span class="No-Break">quite promising.</span></p>
			<p>However, this is<a id="_idIndexMarker438"/> not the best model<a id="_idIndexMarker439"/> architecture for solving the image colorization problem. Nowadays, generative models such as <strong class="bold">Generative Adversarial Networks </strong>(<strong class="bold">GANs</strong>) provide the best results for such problems. We will study GANs<a id="_idIndexMarker440"/> later in this book, but for now, let’s stick to this simple experiment. Next, we will work with other Vertex AI tools that will make our lives easier when it comes <span class="No-Break">to experimentation.</span></p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor098"/>Packaging a model to submit it to Vertex AI as a 
training job</h1>
			<p>The <a id="_idIndexMarker441"/>previous section demonstrated a small image colorization experiment on a Vertex AI Workbench notebook. Notebooks are great for small-scale and quick experiments, but when it comes to large-scale experiments (with more compute and/or memory requirements), it is advised to launch them as a Vertex AI job and specify desired machine specifications (accelerators such as GPU or TPU if needed) for optimal experimentation. Vertex AI jobs also let us execute tons of experiments in parallel without waiting for the results of a single experiment. Experiment tracking is also quite easy with Vertex AI jobs, so it becomes easier to compare your latest experiments with past experiments with the help of saved metadata and the Vertex AI UI. Now, let’s use our model experimentation setup from the previous section and launch it as a Vertex AI <span class="No-Break">training job.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Vertex AI jobs run in a containerized environment, so in order to launch an experiment, we must package our entire code (including reading data, preprocessing, model building, training, and evaluation) into a single script to be launched within the container. Google Cloud provides tons of prebuilt container images for training and evaluation (with dependencies pre-installed for desired frameworks such as TensorFlow, PyTorch, etc.). Plus, we also have the flexibility of defining our own custom container with any kind of dependencies that we <span class="No-Break">may need.</span></p>
			<p>For the experiment from the previous section, as we downloaded open source data into our Jupyter environment, this data is not yet present in <strong class="bold">Google Cloud Storage</strong> (<strong class="bold">GCS</strong>) (i.e., a GCS bucket or BigQuery). So, first, we need to store this data somewhere such that our Vertex AI training job can read it from within the training container. To make things easier for us, we will upload our pre-processed  data into a storage bucket. This will<a id="_idIndexMarker442"/> save us the effort of preparing data again within the job container. We can use the following script to save our prepared data into a <span class="No-Break">GCS bucket:</span></p>
			<pre class="source-code">
from io import BytesIO
import numpy as np
from tensorflow.python.lib.io import file_io
dest = 'gs://data-bucket-417812395597/' # Destination to save in GCS
## saving training data
np.save(file_io.FileIO(dest+'train_x', 'w'), train_x)
np.save(file_io.FileIO(dest+'train_y', 'w'), train_y)
## saving validation data
np.save(file_io.FileIO(dest+'val_x', 'w'), val_x)
np.save(file_io.FileIO(dest+'val_y', 'w'), val_y)
## saving test data
np.save(file_io.FileIO(dest+'test_x', 'w'), test_x)
np.save(file_io.FileIO(dest+'test_y', 'w'), test_y)</pre>			<p>Note that before executing this code, we must create a bucket where we want to store these NumPy arrays. In this case, we have already created one bucket with the <span class="No-Break">name </span><span class="No-Break"><strong class="source-inline">data-bucket-417812395597</strong></span><span class="No-Break">.</span></p>
			<p>We can read these NumPy arrays in any number of training jobs/experiments using the <span class="No-Break">following script:</span></p>
			<pre class="source-code">
train_x = np.load(BytesIO(file_io.read_file_to_string( \
    dest+'train_x', binary_mode=True)))
train_y = np.load(BytesIO(file_io.read_file_to_string( \
    dest+'train_y', binary_mode=True)))
val_x = np.load(BytesIO(file_io.read_file_to_string( \
    dest+'val_x', binary_mode=True)))
val_y = np.load(BytesIO(file_io.read_file_to_string( \
    dest+'val_y', binary_mode=True)))
test_x = np.load(BytesIO(file_io.read_file_to_string( \
    dest+'test_x', binary_mode=True)))
test_y = np.load(BytesIO(file_io.read_file_to_string( \
    dest+'test_y', binary_mode=True)))</pre>			<p>Our data requirements are now all set. Next, let’s work on setting up our Vertex AI <span class="No-Break">training job.</span></p>
			<p>First, we will install some useful packages required to define and launch Vertex <span class="No-Break">AI jobs:</span></p>
			<pre class="source-code">
# Install the packages
! pip3 install --upgrade google-cloud-aiplatform \
                        google-cloud-storage \
                        pillow</pre>			<p>Once <a id="_idIndexMarker443"/>package installation is done, we will move to a new notebook and import <span class="No-Break">useful libraries:</span></p>
			<pre class="source-code">
import numpy as np
import glob
import matplotlib.pyplot as plt
import os
from google.cloud import aiplatform
%matplotlib inline</pre>			<p>Next, we will define our <span class="No-Break">project configurations:</span></p>
			<pre class="source-code">
PROJECT_ID='41xxxxxxxxx7'
REGION='us-west2'
BUCKET_URI='gs://my-training-artifacts'</pre>			<p>Note that<a id="_idIndexMarker444"/> we have created a bucket with the name <strong class="source-inline">my-training-artifacts</strong> to store all the intermediate metadata and artifacts as a result of our Vertex <span class="No-Break">AI job.</span></p>
			<p>Next, let’s initialize the Vertex AI SDK with our <span class="No-Break">project configurations:</span></p>
			<pre class="source-code">
aiplatform.init(project=PROJECT_ID, location=REGION, \
    staging_bucket=BUCKET_URI)</pre>			<p>For our experimentations, we will be using prebuilt TensorFlow images as our model is also based on TensorFlow. Let’s define the images to <span class="No-Break">be used:</span></p>
			<pre class="source-code">
TRAIN_VERSION = "tf-cpu.2-9"
DEPLOY_VERSION = "tf2-cpu.2-9"
TRAIN_IMAGE = "us-docker.pkg.dev/vertex-ai/training/{}:latest".format(TRAIN_VERSION)
DEPLOY_IMAGE = "us-docker.pkg.dev/vertex-ai/prediction/{}:latest".format(DEPLOY_VERSION)</pre>			<p>In this section, we will just launch a simple training job. In the next sections, we will also deploy and test our <span class="No-Break">trained models.</span></p>
			<p>Next, let’s define some command-line arguments for our training (these can be modified on a <span class="No-Break">per-need basis):</span></p>
			<pre class="source-code">
JOB_NAME = "vertex_custom_training"
MODEL_DIR = "{}/{}".format(BUCKET_URI, JOB_NAME)
TRAIN_STRATEGY = "single"
EPOCHS = 20
STEPS = 100
CMDARGS = [
    "--epochs=" + str(EPOCHS),
    "--steps=" + str(STEPS),
    "--distribute=" + TRAIN_STRATEGY,
]</pre>			<p>We should also provide a meaningful job name; it will help us distinguish our experiment from other experiments running <span class="No-Break">in parallel.</span></p>
			<p>The next <a id="_idIndexMarker445"/>step is to write down our entire training script – starting from reading data, defining the model, training, and saving the model into a single file. We will write down our entire code from the previous section into a file with the name <strong class="source-inline">task.py</strong>. The following are the contents of our <span class="No-Break"><strong class="source-inline">task.py</strong></span><span class="No-Break"> file:</span></p>
			<pre class="source-code">
%%writefile task.py
# Single, Mirror and Multi-Machine Distributed Training
import tensorflow as tf
import tensorflow
from tensorflow.python.client import device_lib
import argparse
import os
import sys
from io import BytesIO
import numpy as np
from tensorflow.python.lib.io import file_io</pre>			<p>The following part of the file parses <span class="No-Break">command-line arguments:</span></p>
			<pre class="source-code">
# parse required arguments
parser = argparse.ArgumentParser()
parser.add_argument('--lr', dest='lr', \
                    default=0.001, type=float, \
                    help='Learning rate.')
parser.add_argument('--epochs', dest='epochs', \
                    default=10, type=int, \
                    help='Number of epochs.')
parser.add_argument('--steps', dest='steps', \
                    default=35, type=int, \
                    help='Number of steps per epoch.')
parser.add_argument('--distribute', dest='distribute', \
                    type=str, default='single', \
                    help='distributed training strategy')
args = parser.parse_args()</pre>			<p>Here, we <a id="_idIndexMarker446"/>print some version and environment configurations to keep track of <span class="No-Break">current settings:</span></p>
			<pre class="source-code">
print('Python Version = {}'.format(sys.version))
print('TensorFlow Version = {}'.format(tf.__version__))
print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', \
    'Not found')))
print('DEVICES', device_lib.list_local_devices())</pre>			<p>Here, we define a <span class="No-Break">training strategy:</span></p>
			<pre class="source-code">
# Single Machine, single compute device
if args.distribute == 'single':
    if tf.test.is_gpu_available():
        strategy = tf.distribute.OneDeviceStrategy(device="/gpu:0")
    else:
        strategy = tf.distribute.OneDeviceStrategy(device="/cpu:0")
# Single Machine, multiple compute device
elif args.distribute == 'mirror':
    strategy = tf.distribute.MirroredStrategy()
# Multiple Machine, multiple compute device
elif args.distribute == 'multi':
    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
# Multi-worker configuration
print('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))</pre>			<p>Now, we <a id="_idIndexMarker447"/>prepare the dataset for training, validation, <span class="No-Break">and testing:</span></p>
			<pre class="source-code">
# Preparing dataset
BUFFER_SIZE = 10000
BATCH_SIZE = 128
def make_datasets_unbatched():
    # Load train, validation and test sets
    dest = 'gs://data-bucket-417812395597/'
    train_x = np.load(BytesIO(
        file_io.read_file_to_string(dest+'train_x', \
            binary_mode=True)
    ))
    train_y = np.load(BytesIO(
        file_io.read_file_to_string(dest+'train_y', \
            binary_mode=True)
    ))
    val_x = np.load(BytesIO(
        file_io.read_file_to_string(dest+'val_x', \
            binary_mode=True)
    ))
    val_y = np.load(BytesIO(
        file_io.read_file_to_string(dest+'val_y', \
            binary_mode=True)
    ))
    test_x = np.load(BytesIO(
        file_io.read_file_to_string(dest+'test_x', \
            binary_mode=True)
    ))
    test_y = np.load(BytesIO(
        file_io.read_file_to_string(dest+'test_y', \
            binary_mode=True)
    ))
    return train_x, train_y, val_x, val_y, test_x, test_y</pre>			<p>Now, we define our TensorFlow model as <span class="No-Break">discussed before:</span></p>
			<pre class="source-code">
def tf_model():
    black_n_white_input = tensorflow.keras.layers.Input(shape=(80, 80, 1))
    enc = black_n_white_input</pre>			<p>Here is the<a id="_idIndexMarker448"/> definition of Encoder part of the <span class="No-Break">TF model:</span></p>
			<pre class="source-code">
    #Encoder part
    enc = tensorflow.keras.layers.Conv2D(
        32, kernel_size=3, strides=2, padding='same'
    )(enc)
    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)
    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)
    enc = tensorflow.keras.layers.Conv2D(
        64, kernel_size=3, strides=2, padding='same'
    )(enc)
    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)
    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)
    enc = tensorflow.keras.layers.Conv2D(
        128, kernel_size=3, strides=2, padding='same'
    )(enc)
    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)
    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)
    enc = tensorflow.keras.layers.Conv2D(
        256, kernel_size=1, strides=2, padding='same'
    )(enc)
    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)
    enc = tensorflow.keras.layers.Dropout(0.5)(enc)</pre>			<p>The Encoder part is<a id="_idIndexMarker449"/> now done. Next we define the decoder part of the model within the <span class="No-Break">same function:</span></p>
			<pre class="source-code">
    #Decoder part
    dec = enc
    dec = tensorflow.keras.layers.Conv2DTranspose(
        256, kernel_size=3, strides=2, padding='same'
    )(dec)
    dec = tensorflow.keras.layers.Activation('relu')(dec)
    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)
    dec = tensorflow.keras.layers.Conv2DTranspose(
        128, kernel_size=3, strides=2, padding='same'
    )(dec)
    dec = tensorflow.keras.layers.Activation('relu')(dec)
    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)
    dec = tensorflow.keras.layers.Conv2DTranspose(
        64, kernel_size=3, strides=2, padding='same'
    )(dec)
    dec = tensorflow.keras.layers.Activation('relu')(dec)
    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)
    dec = tensorflow.keras.layers.Conv2DTranspose(
        32, kernel_size=3, strides=2, padding='same'
    )(dec)
    dec = tensorflow.keras.layers.Activation('relu')(dec)
    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)
    dec = tensorflow.keras.layers.Conv2D(
        3, kernel_size=3, padding='same'
    )(dec)
Here, we apply tanh activation function to get the colored output image -
    color_image = tensorflow.keras.layers.Activation('tanh')(dec)
    return black_n_white_input, color_image</pre>			<p>Now, we are <a id="_idIndexMarker450"/>ready to build and compile our <span class="No-Break">TensorFlow model:</span></p>
			<pre class="source-code">
# Build the and compile TF model
def build_and_compile_tf_model():
    black_n_white_input, color_image = tf_model()
    model = tensorflow.keras.models.Model(
        inputs=black_n_white_input,
        outputs=color_image
    )
    _optimizer = tensorflow.keras.optimizers.Adam(
        learning_rate=0.0002,
        beta_1=0.5
    )
    model.compile(
        loss='mse',
        optimizer=_optimizer
    )
    return model</pre>			<p>The following block launches training with the defined settings and saves the <span class="No-Break">trained model:</span></p>
			<pre class="source-code">
# Train the model
NUM_WORKERS = strategy.num_replicas_in_sync
# Here the batch size scales up by number of workers since
# `tf.data.Dataset.batch` expects the global batch size.
GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS
MODEL_DIR = os.getenv("AIP_MODEL_DIR")
train_x, train_y, _, _, _, _ = make_datasets_unbatched()
with strategy.scope():
    # Creation of dataset, and model building/compiling need to be within
    # `strategy.scope()`.
    model = build_and_compile_tf_model()
model.fit(
    train_x,
    train_y,
    epochs=args.epochs,
    steps_per_epoch=args.steps
)
model.save(MODEL_DIR)</pre>			<p>Now that we have all the configurations set up and our training script, <strong class="source-inline">task.py</strong>, is ready, we are all set to define and launch our custom training job on <span class="No-Break">Vertex AI.</span></p>
			<p>Let’s define our<a id="_idIndexMarker451"/> custom Vertex AI <span class="No-Break">training job:</span></p>
			<pre class="source-code">
job = aiplatform.CustomTrainingJob(
    display_name=JOB_NAME,
    script_path="task.py",
    container_uri=TRAIN_IMAGE,
    requirements=[],
    model_serving_container_image_uri=DEPLOY_IMAGE,
)</pre>			<p>The final<a id="_idIndexMarker452"/> step is to launch <span class="No-Break">the job:</span></p>
			<pre class="source-code">
MODEL_DISPLAY_NAME = "tf_bnw_to_color"
# Start the training job
model = job.run(
    model_display_name=MODEL_DISPLAY_NAME,
    args=CMDARGS,
    machine_type = "n1-standard-16",
    replica_count=1,
)</pre>			<p>This setup launches a Vertex AI custom training job on an <strong class="source-inline">n1-standard-16</strong> machine as defined as a parameter in the preceding <strong class="source-inline">job.run</strong> method. When we launch the job in a notebook cell, it gives us a URL to the Google Cloud console UI. By clicking on it, we can monitor our job logs within the Vertex <span class="No-Break">AI UI.</span></p>
			<p>A Vertex AI training job looks something like <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.6</em> in the Google Cloud console UI. Here, we can re-verify the configurations and parameters of our job that we had defined at the time <span class="No-Break">of launch:</span></p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B17792_07_6.jpg" alt="Figure 7.6 – Vertex AI training job" width="754" height="940"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Vertex AI training job</p>
			<p>The <a id="_idIndexMarker453"/>Vertex AI UI lets us monitor near real-time logs of all the training/custom jobs. We can monitor our training within the UI, and it looks something like <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B17792_07_7.jpg" alt="Figure 7.7 – Real-time logs for Vertex AI training job on Google Cloud console" width="1209" height="298"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – Real-time logs for Vertex AI training job on Google Cloud console</p>
			<p>Going through the logs may not be the best way to monitor training progress as we may want to track a few parameters, such as loss and accuracy. In the next section, we will learn about how to set up TensorBoard-based live monitoring of training progress. However, these logs can be really handy for debugging purposes; if our pipeline fails in between before completing the execution successfully, we can always check these logs to identify the <span class="No-Break">root cause.</span></p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor099"/>Monitoring model training progress</h1>
			<p>In the previous section, we <a id="_idIndexMarker454"/>saw how easy it is to launch a Vertex AI custom training job with desired configurations and machine types. These Vertex AI training jobs are really useful for running large-scale experiments where training uses high compute (multiple GPUs or TPUs) and also may run for a few days. Such long-running experiments are not very feasible to run in a Jupyter Notebook-based environment. Another great thing about launching Vertex AI jobs is that all the metadata and lineage are tracked in a systematic way so that we can come back later and look into our past experiments and compare them with the latest ones in an easy and <span class="No-Break">accurate way.</span></p>
			<p>Another important aspect is monitoring the live progress of training jobs (including metrics such as loss and accuracy). For this purpose, we can easily set up Vertex AI TensorBoard within our Vertex AI job and track the progress in a near real-time fashion. In this section, we will set up a TensorBoard instance for our <span class="No-Break">previous experiment.</span></p>
			<p>Most of the code/scripts will be similar to the previous section. Here, we will just examine the modifications needed to set up <span class="No-Break">TensorBoard monitoring.</span></p>
			<p>Firstly, we need to make small changes in the <strong class="source-inline">task.py</strong> file to account for TensorFlow callbacks as we want to monitor training loss. To keep things clean, we will modify a copy of the <strong class="source-inline">task.py</strong> file that we have renamed to <strong class="source-inline">task2.py</strong>. The following are the changes in the <span class="No-Break"><strong class="source-inline">model.fit</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
### Create a TensorBoard callback and write to the gcs path provided by AIP_TENSORBOARD_LOG_DIR
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'],
    histogram_freq=1)
model.fit(
    train_x,
    train_y,
    epochs=args.epochs,
    steps_per_epoch=args.steps,
    callbacks=[tensorboard_callback],
)</pre>			<p>In the preceding script, we have just defined a TensorFlow callback object and also passed it into the <span class="No-Break"><strong class="source-inline">model.fit</strong></span><span class="No-Break"> function.</span></p>
			<p>Working with<a id="_idIndexMarker455"/> TensorBoard requires a service account to be in place (instead of individual user accounts). If we already don’t have a service account set up, we can use the following script to quickly set up a service account. A service account is used to grant permissions to services, VMs, and other tooling on <span class="No-Break">Google Cloud:</span></p>
			<pre class="source-code">
SERVICE_ACCOUNT="dummy-sa"
IS_COLAB=False
if (
    SERVICE_ACCOUNT == ""
    or SERVICE_ACCOUNT is None
    or SERVICE_ACCOUNT == "dummy-sa"
):
    # Get your service account from gcloud
    if not IS_COLAB:
        shell_output = ! gcloud auth list 2&gt;/dev/null
        SERVICE_ACCOUNT = shell_output[2].replace("*", \
            "").strip()</pre>			<p>If we are working <a id="_idIndexMarker456"/>with colab, the following code snippet will create a service <span class="No-Break">account accordingly:</span></p>
			<pre class="source-code">
    else:  # IS_COLAB:
        shell_output = ! gcloud projects describe  $PROJECT_ID
        project_number = shell_output[-1].split(":")[1].strip().replace("'", "")
        SERVICE_ACCOUNT = f"{project_number}-compute@developer.gserviceaccount.com"
    print("Service Account:", SERVICE_ACCOUNT)</pre>			<p>The next step is to create a Vertex AI TensorBoard instance that we will use for monitoring <span class="No-Break">our training.</span></p>
			<p>Set up the <span class="No-Break">TensorBoard instance:</span></p>
			<pre class="source-code">
TENSORBOARD_NAME = "training-monitoring"  # @param {type:"string"}
if (
    TENSORBOARD_NAME == ""
    or TENSORBOARD_NAME is None
    or TENSORBOARD_NAME == "training-monitoring"
):
    TENSORBOARD_NAME = PROJECT_ID + "-tb-" + TIMESTAMP
tensorboard = aiplatform.Tensorboard.create(
    display_name=TENSORBOARD_NAME, project=PROJECT_ID, \
        location=REGION
)
Let's verify if the TensorBoard instance was successfully created or not - TENSORBOARD_RESOURCE_NAME = tensorboard.gca_resource.name
print("TensorBoard resource name:", TENSORBOARD_RESOURCE_NAME)</pre>			<p>We need a<a id="_idIndexMarker457"/> staging bucket for our Vertex AI job so that it can write event logs into <span class="No-Break">that location:</span></p>
			<pre class="source-code">
BUCKET_URI = "gs://tensorboard-staging"  # @param {type:"string"}
if BUCKET_URI == "" or BUCKET_URI is None or BUCKET_URI == "gs://[your-bucket-name]":
    BUCKET_URI = "gs://" + PROJECT_ID + "aip-" + TIMESTAMP
! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}
GCS_BUCKET_OUTPUT = BUCKET_URI + "/output/"</pre>			<p>We are now all <a id="_idIndexMarker458"/>set to define our custom <span class="No-Break">training job:</span></p>
			<pre class="source-code">
JOB_NAME = "tensorboard-example-job-{}".format(TIMESTAMP)
BASE_OUTPUT_DIR = "{}{}".format(GCS_BUCKET_OUTPUT, JOB_NAME)
job = aiplatform.CustomTrainingJob(
    display_name=JOB_NAME,
    script_path="task2.py",
    container_uri=TRAIN_IMAGE,
    requirements=[],
    model_serving_container_image_uri=DEPLOY_IMAGE,
    staging_bucket=BASE_OUTPUT_DIR,
)</pre>			<p>We can now launch the Vertex AI job using the following script. Here, we can choose the machine type and also specify the <strong class="source-inline">replica_count</strong> parameter, which controls the number of replicas to run for the <span class="No-Break">current job:</span></p>
			<pre class="source-code">
MODEL_DISPLAY_NAME = "tf_bnw_to_color_tb"
# Start the training job
model = job.run(
    model_display_name=MODEL_DISPLAY_NAME,
    service_account=SERVICE_ACCOUNT,
    tensorboard=TENSORBOARD_RESOURCE_NAME,
    args=CMDARGS,
    machine_type = "n1-standard-8",
    replica_count=1,
)</pre>			<p>Once we<a id="_idIndexMarker459"/> launch the job, it will give us the URL for locating the Vertex AI job in the Google Cloud console UI like in the previous section; but this time, it will also give us a URL to the Vertex TensorBoard UI. Using this URL, we will be able to monitor our training in a near <span class="No-Break">real-time fashion.</span></p>
			<p>This is how it looks for our little experiment (see <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">):</span></p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B17792_07_8.jpg" alt="Figure 7.8 – Vertex TensorBoard for real-time monitoring of experiments" width="891" height="674"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Vertex TensorBoard for real-time monitoring of experiments</p>
			<p>We can<a id="_idIndexMarker460"/> configure it to show more desired metrics for our experiments. Now that we are able to launch Vertex AI training, monitor it, and also save our TensorFlow-trained model, let’s move on to the model <span class="No-Break">evaluation part.</span></p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor100"/>Evaluating trained models</h1>
			<p>In this section, we will <a id="_idIndexMarker461"/>take the already trained model from the previous section and launch a batch inference job on the test data. The first step here will be to load our test data into a <span class="No-Break">Jupyter Notebook:</span></p>
			<pre class="source-code">
from io import BytesIO
import numpy as np
from tensorflow.python.lib.io import file_io
dest = 'gs://data-bucket-417812395597/'
test_x = np.load(BytesIO(file_io.read_file_to_string(dest+'test_x',\
    binary_mode=True)))
test_y = np.load(BytesIO(file_io.read_file_to_string(dest+'test_y',\
    binary_mode=True)))
print(test_x.shape, test_y.shape)</pre>			<p>The next step is to create a JSON payload of instances from our test data and save it in a cloud storage location. The batch inference module will be able to read these instances and <span class="No-Break">perform inference:</span></p>
			<pre class="source-code">
import json
BATCH_PREDICTION_INSTANCES_FILE = "batch_prediction_instances.jsonl"
BATCH_PREDICTION_GCS_SOURCE = (
    BUCKET_URI + "/batch_prediction_instances/" + BATCH_PREDICTION_INSTANCES_FILE
)</pre>			<p>Here we convert the input images to a serializable format so that the prediction service can accept input as a <span class="No-Break">JSON file:</span></p>
			<pre class="source-code">
# converting to serializable format
x_test = [(image).astype(np.float32).tolist() for image in test_x]
# Write instances at JSONL
with open(BATCH_PREDICTION_INSTANCES_FILE, "w") as f:
    for x in x_test:
        f.write(json.dumps(x) + "\n")
# Upload to Cloud Storage bucket
! gsutil cp batch_prediction_instances.jsonl BATCH_PREDICTION_GCS_SOURCE
print("Uploaded instances to: ", BATCH_PREDICTION_GCS_SOURCE)</pre>			<p>Now our<a id="_idIndexMarker462"/> test dataset instances are ready in a cloud storage bucket. We can launch batch prediction over them, and the batch inference module will save the output results into a new folder inside the <span class="No-Break">same bucket:</span></p>
			<pre class="source-code">
MIN_NODES = 1
MAX_NODES = 1
# The name of the job
BATCH_PREDICTION_JOB_NAME = "bnw_to_color_batch_prediction"
# Folder in the bucket to write results to
DESTINATION_FOLDER = "batch_prediction_results"
# The Cloud Storage bucket to upload results to
BATCH_PREDICTION_GCS_DEST_PREFIX = BUCKET_URI + "/" + DESTINATION_FOLDER</pre>			<p>Here, we call the <a id="_idIndexMarker463"/>batch prediction service using <span class="No-Break">the SDK:</span></p>
			<pre class="source-code">
# Make SDK batch_predict method call
batch_prediction_job = model.batch_predict(
    instances_format="jsonl",
    predictions_format="jsonl",
    job_display_name=BATCH_PREDICTION_JOB_NAME,
    gcs_source=BATCH_PREDICTION_GCS_SOURCE,
    gcs_destination_prefix = BATCH_PREDICTION_GCS_DEST_PREFIX,
    model_parameters=None,
    starting_replica_count=MIN_NODES,
    max_replica_count=MAX_NODES,
    machine_type="n1-standard-4",
    sync=True,
)</pre>			<p>We can also monitor the progress of the batch prediction job within the Google Cloud console UI if needed. Once this job finishes, we can check the outputs inside the defined <span class="No-Break">destination folder.</span></p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor101"/>Summary</h1>
			<p>In the chapter, we learned how to work with a Vertex AI-based managed training environment and launch custom training jobs. Launching custom training jobs on Vertex AI comes with a number of advantages, such as managed metadata tracking, no need to actively monitor jobs, and the ability to launch any number of experiments in parallel, choose your desired machine specifications to run your experiments, monitor training progress and results in near-real time fashion using the Cloud console UI, and run managed batch inference jobs on a saved model. It is also tighly integrated with other <span class="No-Break">GCP products.</span></p>
			<p>After reading this chapter, you should be able to develop and run custom deep learning models (using frameworks such as TensorFlow) on Vertex AI Workbench notebooks. Secondly, you should be able to launch long-running Vertex AI custom training jobs and also understand the advantages of the managed Vertex AI training framework. The managed Google Cloud console interface and TensorBoard make it easy to monitor and evaluate various Vertex AI <span class="No-Break">training jobs.</span></p>
			<p>Now that we have a good understanding of training models using Vertex AI on GCP, we will learn about model explainability in the <span class="No-Break">next chapter.</span></p>
		</div>
	</div>
</div>
</body></html>