- en: Credit Card Fraud Detection Using Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fraud management has been known to be a very painful problem for banking and
    finance firms. Card-related frauds have proven to be especially difficult for
    firms to combat. Technologies such as chip and PIN are available and are already
    used by most credit card system vendors, such as Visa and MasterCard. However,
    the available technology is unable to curtail 100% of credit card fraud. Unfortunately,
    scammers come up with newer ways of phishing to obtain passwords from credit card
    users. Also, devices such as skimmers make stealing credit card data a cake walk!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Despite the availability of some technical abilities to combat credit card fraud,
    *The Nilson Report*, a leading publication covering payment systems worldwide,
    estimated that credit card fraud is going to soar to $32 billion in 2020 ([https://nilsonreport.com/upload/content_promo/The_Nilson_Report_10-17-2017.pdf](https://nilsonreport.com/upload/content_promo/The_Nilson_Report_10-17-2017.pdf)).
    To get a perspective on the estimated loss, it is more than the recent profits
    posted by companies such as Coca-Cola ($2 billion), Warren Buffet’s Berkshire
    Hathaway ($24 billion), and JP Morgan Chase ($23.5 billion)!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'While credit card chip technology-providing companies have been investing hugely
    to advance the technology to counter credit card fraud, in this chapter, we are
    going to examine whether and how far machine learning can help deal with the credit
    card fraud problem. We will cover the following topics as we progress through
    this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning in credit card fraud detection
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders and the various types
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The credit card fraud dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building AEs with the H2O library in R
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of auto encoder for credit card fraud detection
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning in credit card fraud detection
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The task of fraud detection often boils down to outlier detection, in which
    a dataset is verified to find potential anomalies in the data. Traditionally,
    this task was deemed a manual task, where risk experts checked all transactions
    manually. Even though there is a technical layer, it is purely based on a rules
    base that scans through each transaction, and then those shortlisted as suspicious
    are sent through for a manual review to make a final decision on the transaction.
    However, there are some major drawbacks to this system:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Organizations need substantial fraud management budgets for manual review staff.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extensive training is required to train the employees working as manual review
    staff.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the personnel to manually review transactions is time consuming and
    expensive.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even the most highly trained manual review staff carry certain biases, therefore
    making the whole review system inaccurate.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual reviews increase the time required to fulfill a transaction. The customers
    might get frustrated with the long wait times required to pass a credit card transaction.
    This may impact the loyalty of customers.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual reviews may yield false positives. A false positive not only affects
    the sale in the process but also lifetime value generated from the customer.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工审查可能会导致假阳性。假阳性不仅会影响过程中的销售，还会影响客户产生的终身价值。
- en: Fortunately, with the rise of **machine learning** (**ML**), **artificial intelligence**
    (**AI**), and deep learning, it became feasible to automate the manual credit
    card transaction review process to a large extent. This not only saves an intensive
    amount of labor but also yields better detection of credit card fraud, which otherwise
    is impacted due to biases that human reviewers carry.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，随着**机器学习**（**ML**）、**人工智能**（**AI**）和深度学习的兴起，在很大程度上自动化了人工信用卡交易审查过程成为可能。这不仅节省了大量劳动力，而且还能更好地检测信用卡欺诈，否则由于人工审查者携带的偏见，欺诈检测可能会受到影响。
- en: ML-based fraud detection strategies generally can be accomplished using both
    supervised ML and unsupervised ML techniques.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于机器学习的欺诈检测策略通常可以使用监督机器学习和无监督机器学习技术来完成。
- en: Supervised ML models are generally used when large amounts of transaction data
    tagged as **genuine **or **fraud **are available. A model is trained on the labeled
    dataset and the resultant model is then used for classifying any new credit card
    transactions into one of the two possible classes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当有大量标记为**真实**或**欺诈**的交易数据可用时，通常使用监督机器学习模型。模型在标记的数据集上训练，然后使用得到的模型将任何新的信用卡交易分类到两个可能的类别之一。
- en: With most organizations, the problem is that labeled data is unavailable, or
    very little labeled data is available. This makes supervised learning models less
    feasible. This is where unsupervised models come into play. They are designed
    to spot anomalous behavior in transactions and they do not need explicit pre-labeled
    data to identify the anomalous behavior. The general idea in unsupervised fraud
    detection is to detect behavior anomalies by identifying transactions that do
    not conform to the majority.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数组织中，问题在于标记的数据不可用，或者可用的标记数据非常少。这使得监督学习模型不太可行。这就是无监督模型发挥作用的地方。它们被设计用来在交易中识别异常行为，并且它们不需要显式的预标记数据来识别异常行为。在无监督欺诈检测中的基本思想是通过识别不符合大多数交易的交易来检测行为异常。
- en: Another thing to keep in mind is that fraud events are rare, and are not as
    common as genuine transactions. Due to the rarity of fraud, severe class imbalance
    problem may be seen in datasets related to credit card fraud. In other words,
    one would observe that 95% or more of the data in the dataset is of genuine transactions,
    and less than 5% of the data belongs to fraudulent transactions. Also, even if
    you learn about a fraudulent transaction today, the model is likely to face an
    anomaly tomorrow with different features. So, the problem space of genuine transactions
    is well known and it is pretty much stagnant; however, the problem space for fraudulent
    transactions is not well known and it is not constant. Due to these reasons, it
    make sense to deal with the fraud detection problem with unsupervised learning
    rather than supervised learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点需要记住的是，欺诈事件是罕见的，并不像真实交易那样普遍。由于欺诈的罕见性，数据集中可能会出现严重的类别不平衡问题。换句话说，人们会观察到数据集中95%或更多的数据是真实交易，而少于5%的数据属于欺诈交易。此外，即使你今天了解到了一个欺诈交易，模型明天可能也会因为不同的特征而面临异常。因此，真实交易的问题空间是众所周知的，并且几乎是停滞不前的；然而，欺诈交易的问题空间并不为人所知，并且不是恒定的。由于这些原因，用无监督学习而不是监督学习来处理欺诈检测问题是有意义的。
- en: 'Anomaly detection is an unsupervised learning algorithm that is also termed
    a **one-class classification** algorithm. It distinguishes between **normal** and
    **anomalous** observations. The key principle on which the algorithm is built
    is that anomalous observations do not conform to the expected pattern of other
    common observations in a dataset. It is called a one-class classification as it
    learns the pattern of genuine transactions, and anything that shows non-conformance
    to this pattern is termed as an **anomaly**, and therefore as a fraudulent **transaction**.
    The following figure is an illustration showing anomaly detection in a two-dimensional
    space:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测是一种无监督学习算法，也被称为**单类分类**算法。它区分**正常**和**异常**观察。算法建立的关键原则是异常观察不符合数据集中其他常见观察的预期模式。因为它学习真实交易的模式，任何不符合这一模式的都被称为**异常**，因此也被视为**欺诈交易**。以下图示展示了在二维空间中的异常检测：
- en: '![](img/a74dd4da-4def-42ae-aa44-3d2cf811cb62.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Anomaly detection illustrated in 2D space
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple example of an anomaly is the identification of data points that are
    too far from the mean (standard deviation) in a time series. The following figure
    is an illustration displaying the data points that are identified as anomalies
    in a time series:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a73590b-e9ae-403e-86b8-b13ce39d2a66.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: Anomaly in time series—identified through standard deviation
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus our efforts on a type of unsupervised deep learning
    application known as **AEs**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders explained
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Autoencoders** (**AEs**) are neural networks that are of a feedforward and
    non-recurrent type. They aim to copy the given inputs to the outputs. An AE works
    by compressing the input into a lower dimensional summary. This summary is often
    referred as latent space representation. An AE attempts to reconstruct the output
    from the latent space representation. An **Encoder**, a **Latent Space Representation**,
    and a **Decoder** are the three parts that make up the AEs. The following figure
    is an illustration showing the application of an AE on a sample picked from the MNIST
    dataset:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ec6a118-78cf-4d2c-9747-e56c5f0a31fb.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Application of AE on MNIST dataset sample
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder and decoder components of AEs are fully-connected feedforward networks.
    The number of neurons in a latent space representation is a hyperparameter that
    needs to be passed as part of building the AE. The number of neurons or nodes
    that is decided in the latent semantic space dictates the amount of compression
    that is attained while compressing the actual input image into a latent space
    representation. The general architecture of an AE is shown in the following figure:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfd126a4-2ff5-4855-860e-3fbf58cf59e0.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: General architecture of a AE
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The given input first passes through an **Encoder**, which is a fully-connected
    **artificial neural network** (**ANN**). The **Encoder** acts upon the **Input**
    and reduces its dimensions, as specified in the hyperparameter. The **Decoder**
    is another fully-connected ANN that picks up this reduced **Input** (latent space
    representation) and then reconstructs the **Output**. The goal is to get the **Output**
    identical to that of the **Input**. In general, the architectures of the **Encoder**
    and the **Decoder** are mirror images. Although there is no such requirement that
    mandates that the **Encoder** and **Decoder** architectures should be the same,
    it is generally practiced that way. In fact, the only requirement of the AE is
    to obtain identical output from that of the given input. Anything in between can
    be customized to the whims and fancies of the individual building the AE.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the encoder can be represented as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/f0867ffa-839a-4d32-b873-572ef7961eb2.png)]'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'where *x* is the input and *h* is the function that acts on the input to represent
    it in a concise summary format. A decoder, on the other hand, can be represented
    as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/f8c2a45f-0bf3-4960-bc5b-5d66df7ad3a2.png).]'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: While the expectation is to obtain ![](img/7eed3d3e-b794-4999-a444-e13c99a809a8.png),
    this is not always the case as the reconstruction is done from a compact summary
    representation; therefore, there is occurrence of certain error. The error *e*
    is computed from the original input *x* and reconstructed output *r*, ![](img/71b0ef36-ab8e-44a8-9243-cfb52997e5e1.png)
    .
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: The AE network then learns by reducing the **Mean Squared Error** (**MSE**),
    and the error is propagated back to the hidden layers for adjustment. The weights
    of the decoder and encoder are transposes of each other, which makes it faster
    to learn training parameters. The mirrored architectures of the encoder and decoder
    make it possible to learn the training parameters faster. In different architectures,
    the weights cannot be simply transposed; therefore, the computation time will
    increase. This is the reason for keeping the mirrored architectures for the encoder
    and decoder.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Types of AEs based on hidden layers
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on the size of the hidden layer, AEs can be classified into two types, **undercomplete
    AEs** and **overcomplete AEs**:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '**Undercomplete AE**: If the AE simply learns to copy the input to the output,
    then it is not useful. The idea is to produce a concise representation as the
    output of the encoder, and this concise representation should consist of the most
    useful features of the input. The amount of conciseness achieved by the input
    layer is governed by the number of neurons or nodes that we use in the latent
    space representation. This can be set as a parameter while building the AE. If
    the number of neurons is set to fewer dimensions than that of the input features,
    then the AE is forced to learn most of the key features of the input data. The
    architecture where the number of neurons in latent space is less than that of
    input dimensions is called an undercomplete AE.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overcomplete AE**: It is possible to represent the number of neurons in latent
    space as equal to or more than that of the input dimensions. This kind of architecture
    is termed an overcomplete AE.  In this case, the AE does not learn anything and
    simply copies the input to the latent space, which in turn is propagated through
    to the decoder.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apart from the number of neurons in the latent space, the following are some
    of the other parameters that can be used in an AE architecture:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of layers in the encoder and decoder**: The depth of the encoder and
    decoder can be set to any number. Generally, in a mirrored architecture of encoder
    and decoder, the number of layers is set as the same number. The last figure is
    an illustration showing the AE with two layers, excluding the input and output,
    in both the encoder and decoder.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of neurons per layer in encoder and decoder**: The number of neurons
    decreases with each layer in an encoder and it increases with each layer in a
    decoder. The neurons in layers of encoders and decoders are symmetric.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function**: Loss functions such as MSE or cross-entropy are used by
    AEs to learn the weights during backpropagation. If the input is in the range
    of (0,1), then cross-entropy is used as metric, otherwise MSE is used.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of AEs based on restrictions
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on the restrictions imposed on the loss, AEs can be grouped into the
    following types:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '**Plain Vanilla AEs**: This is the simplest AE architecture possible, with
    a fully-connected neural layer as the encoder and decoder.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sparse AEs**: Sparse AEs are an alternative method for introducing an information
    bottleneck, without requiring a reduction in the number of nodes in our hidden
    layers. Rather than preferring an undercomplete AE, the loss function is constructed
    in a way that it penalizes the activations within a layer. For any given observation,
    the network is encouraged to learn encoding and decoding, which only relies on
    activating a small number of neurons.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Denoising AEs**: This is a type of overcomplete AE that experiences the risk
    of learning the **identity function** or **null function**. Essentially, the AE
    learns the output that is equal to the input, therefore making the AE useless. Denoising
    AEs avoid this problem of learning the identity function by randomly initializing
    some of the inputs to 0\. During the computation of the loss function, the noise-induced
    input is not considered; therefore, the network still learns the correct weights
    without the risk of learning the identity function. At the same time, the AE is
    trained to learn to reconstruct the output, even from the corrupted input.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure is a example of denoising AEs on sample images from the MNIST
    dataset:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7a07b41-c689-4e55-a083-f06fe8654b83.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Application of denoising AEs on MNIST samples
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional AEs**: When dealing with images as inputs, one can use convolutional
    layers as part of the encoder and decoder networks. Such kinds of AEs that use
    convolutional layers are termed **convolutional AEs**. The following figure is
    an illustration showing the use of convolutions in AEs:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/cad4e496-083e-4442-9e06-9dfba7e17d0c.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: Convolutional AEs
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '**Stacked AEs**: Stacked AEs are ones that have multiple layers in the encoder
    as well as the decoder. You can refer to the general architecture of an AE as
    an example illustration of a stacked AE architecture, with the encoder and decoder
    having two layers (excluding the input and output layers).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variational AEs**: A **variational AE** (**VAE**), rather than building an
    encoder that outputs a singl'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'e value to describe each latent state attribute, describes a probability distribution
    for each latent attribute. This makes it possible to design complex generative
    models of data and also generate fictional celebrity images and digital artwork.
    The following figure is an illustration depicting the representation of data in
    VAEs:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f0cf2cf0-7185-44d1-a669-4a7522e025b3.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: In a VAE, the encoder model is sometimes referred to as the recognition model,
    whereas the decoder model is sometimes referred to as the generative model. The
    encoder outputs a range of statistical distributions for the latent features.
    These features are randomly sampled and used by the decoder to reconstruct the
    input. For any sampling of the latent distributions, the decoder is expected to
    be able to accurately reconstruct the input. Thus, values that are nearby to one
    another in latent space should correspond with very similar reconstructions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Applications of AEs
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some of the practical applications where AEs may be used:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '**Image coloring**: Given a grayscale image as input, AEs can auto color the
    image and return the colored image as output.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise removal**: Denoising AEs are able to remove noise from images and reconstruct
    images without noise. Tasks such as watermark removal from videos and images can
    be accomplished.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: AEs represent the input data in a compressed
    form, but with a focus on key features alone. Therefore, things like images can
    be represented with reduced pixels, without much loss of information during image
    reconstruction.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image search**: This is used to identify similar images based on a given
    input.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information retrieval**: When retrieving information from a corpus, AEs may
    be used to group together all the documents that belong to a given input.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic modeling**: Variational AEs are used to approximate the posterior distribution,
    and it has become a promising alternative for inferring latent topic distributions
    of text documents.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have covered the fundamentals that are needed for us to understand AEs and
    their applications. Let us understand, at a high level, the solution we are going
    to employ using AEs on the credit card fraud detection problem.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: The credit card fraud dataset
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally in a fraud dataset, we have sufficient data for the negative class
    (non-fraud/genuine transactions) and very few or no data for the positive class
    (fraudulent transactions). This is termed a **class imbalance problem** in the ML
    world. We train an AE on the non-fraud data and learn features using the encoder.
    The decoder is then used to compute the reconstruction error on the training set
    to find a threshold. This threshold will be used on the unseen data (test dataset
    or otherwise). We use the threshold to identify those test instances whose values
    are greater than the threshold as fraud instances.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: For the project in this chapter, we will be using a dataset that is sourced
    from this URL: [https://essentials.togaware.com/data/](https://essentials.togaware.com/data/).
    This is a public dataset of credit card transactions. This dataset is originally
    made available through the research paper *Calibrating Probability with Undersampling
    for Unbalanced Classification*, A. Dal Pozzolo, O. Caelen, R. A Johnson and G.
    Bontempi, IEEE **Symposium Series on Computational Intelligence** (**SSCI**),
    Cape Town, South Africa, 2015\. The dataset is also available at this URL: [http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata](http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata).
    The dataset was collected and analyzed during a research collaboration of Worldline
    and the Machine Learning Group ([http://mlg.ulb.ac.be](http://mlg.ulb.ac.be/))
    of ULB (Université Libre de Bruxelles) on big data mining and fraud detection.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the characteristics of the dataset:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: The paper made the dataset available as an Rdata file. There is a CSV converted
    version of this dataset available on Kaggle as well as other sites.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It contains transactions made by credit cards in September 2013 by European
    cardholders.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transactions occurred on two days are recorded and is presented as the dataset.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a total of 284,807 transactions in the dataset.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset suffers from a severe class imbalance problem. Only 0.172% of all
    transactions are fraudulent transactions (492 fraudulent transactions).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a total thirty features in the dataset, namely `V1`, `V2`, ...,`V28`,
    `Time`, and `Amount`.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variables `V1`, `V2`, ...,`V28` are the principal components obtained with
    PCA from the original set of variables.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to confidentiality, the original set of variables that yielded the principal
    components are not revealed.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Time` feature contains the seconds elapsed between each transaction and
    the first transaction in the dataset.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Amount` feature is the transaction amount.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dependent variable is named `Class`. The fraudulent transactions are represented
    as 1 in the class and genuine transactions are represented as 0.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now jump into using AEs for the credit card fraud detection.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Building AEs with the H2O library in R
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using the AE implementation available in H2O for our project. H2O
    is a fully open source, distributed, in-memory ML platform with linear scalability.
    It offers parallelized implementations of some of the most widely used ML algorithms.
    It supports an easy to use, unsupervised, and non-linear AE as part of its deep
    learning model. The DL AE of H2O is based on the multilayer neural net architecture,
    where the entire network is trained together, instead of being stacked layer by
    layer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'The `h2o` package can be installed in R with the following command:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Additional details on the installation and dependencies of H2O in R are available
    at this URL: [https://cran.r-project.org/web/packages/h2o/index.html](https://cran.r-project.org/web/packages/h2o/index.html).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the package is installed successfully, the functions offered by the `h2o`
    package, including the AE, can simply be used by including the following line
    in R code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is all we need to do prior to coding our credit card fraud detection system
    with the AE. Without waiting any longer, let's start building our code to explore
    and prepare our dataset, as well as to implement the AE that captures fraudulent
    credit card transactions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoder code implementation for credit card fraud detection
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As usual, like all other projects, let''s first load the data into an R dataframe
    and then perform EDA to understand the dataset better. Please note the inclusion
    of `h2o` as well as the `doParallel` library in the code. These inclusions enable
    us to use the AE that is part of the `h2o` library, as well as to utilize the
    multiple CPU cores that are present in the laptop/desktop as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Initializing the H2O cluster in localhost under the port `54321`. The `nthreads`
    defines the number of thread pools to be used, this is close to the number of
    cpus to be used. In our case, we are saying use all CPUs, we are also specifying
    the maximum memory to use by H2O cluster as `8G`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will get a similar output to that shown in the following code block:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, to set the working directory of the data file location, load Rdata and
    read it into the dataframe, and view the dataframe using the following code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The will give the following output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e302731-40c7-43f1-9350-8c3cb93e9608.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now print the dataframe structure using the following code:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will give the following output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, to view the class distribution, use the following code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You will get the following output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To view the relationship between the `V1` and `Class` variables, use the following
    code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will give the following output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75981835-0fdf-4ffd-807e-6cceab13ec32.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: 'To visualize the distribution of transaction amounts with respect to class,
    use the following code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will give the following output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95ac410b-3790-435f-a980-d0f89f27af11.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: 'To visualize the distribution of transaction times with respect to class, use
    the following code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will give the following output:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc9a994e-1e30-440c-9f0e-80c79e7ae4ba.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: 'Use the following code to visualize the `V2` variable with respect to `Class`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You will get the following as the output:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c891040-a5e6-4568-a669-bfa9307f150e.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: 'Use the following code to visualize `V3` with respect to `Class`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following graph is the resultant output:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/084bee5f-c39d-4cd7-a4ef-f3654ca44992.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: 'To visualize the `V3` variable with respect to `Class`, use the following code:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following graph is the resultant output:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/312d60a0-e11b-4ec9-ac1a-5b5a2fb77cc3.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: 'Use the following code to visualize the `V6` variable with respect to `Class`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following graph is the resultant output:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ec1dc19-0116-4bae-b1f4-72f520de15f8.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'Use the following code to visualize the `V7` variable with respect to `Class`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following graph is the resultant output:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b97b5a5-1b06-4151-adde-1b85b632f657.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'Use the following code to visualize the `V8` variable with respect to `Class`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following graph is the resultant output:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c471bfc-a6ff-4867-96cc-30da7a810d47.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'To visualize the `V9` variable with respect to `Class`, use the following code:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following graph is the resultant output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b82a52f6-7fe5-4983-b56f-9c9d92dfeed7.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: 'To visualize the `V10` variable with respect to `Class`, use the following
    code:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following graph is the resultant output:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aeee5f93-f562-4bd6-944f-e2cfb1936b0c.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: 'From all the visualizations related to variables with respect to class, we
    can infer that most of the principal components are centered on `0`. Now, to plot
    the distribution of classes in the data, use the following code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following bar graph is the resultant output:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ab03c79-8eea-4c65-94a7-ba90deedae22.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: 'We observe that the distribution of classes is very imbalanced. The representation
    of the major class (non-fraudulent transactions, represented by `0`) in the dataset
    is too heavy when compared to the minority class (fraudulent transactions: `1`).
    In the traditional supervised ML way of dealing with this kind of problem, we
    would have treated the class imbalance problem with techniques such as **Synthetic
    Minority Over-Sampling Technique** (**SMORT**). However, with AEs, we do not treat
    the class imbalance during data preprocessing; rather, we feed the data as is
    to the AE for learning. In fact, the AE is learning the thresholds and the characteristics
    of the data from the majority class; this is the reason we call it a one-class
    classification problem.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to do some feature engineering prior to training our AE. Let''s
    first focus on the `Time` variable in the data. Currently, it is in the seconds
    format, but we may better represent it as days. Run the following code to see
    the current form of time in the dataset:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You will get the following output:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We know that there are 86,400 seconds in a given day (60 seconds per minute
    * 60 minutes per hour * 24 hours per day). We will convert the `Time` variable
    into `Day` by considering the value in `Time` and representing it as `day1` if
    the number of seconds is less than or equal to 86,400, and anything over 86,400
    becomes `day2.` There are only two days possible, as we can see from the summary
    that the maximum value represented by the time variable is `172792` seconds:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following is the resultant output of the first six rows after the conversion:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a810fdad-2a9b-4045-a5ba-92635cd881ac.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'Now, use the following code to view the last six rows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following is the resultant output of the last six rows after the conversion:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5506a84a-a119-421a-be3c-1284e371c32a.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s print the distribution of transactions by the day in which the
    transaction falls, using the following code:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You will get the following as the output:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s create a new variable, `Time_day`, based on the seconds represented
    in the `Time` variable, and summarize the `Time_day` variable with respect to
    `Day` using the following code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We get the following as the resultant output:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Use the following code the convert all character variables in the dataset to
    factors:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can further fine-tune the `Time_day` variable by converting the variable
    into a factor. The factors represents the time of day at which the transaction
    happened, for example, `morning`, `afternoon`, `evening`, and `night`. We can
    create a new variable called `Time_Group`, based on the various buckets of the
    day, using the following code:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following is the resultant output of the first six rows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26eb4e24-39aa-4069-8b8b-b62b16d185af.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'Use the following code to view and confirm the last six rows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This will give the following output, and we see that we have successfully converted
    the data which represent the various time of the day:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bf537f3-165f-47c8-a6b2-b04cd963ba4e.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following code:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code will generate the following output:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c44ad84-e7fe-4871-a9b6-b0a84d54b8d1.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: We can infer from the visualization that there is no difference in the count
    of transactions that happened on day 1 and day 2\. Both remain close to 150,000
    transactions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will convert the `Class` variable as a factor and then visualize the
    data by `Time_Group` variable using the following code:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This will generate the following output:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e186689d-e77c-4cde-b52d-5ed9c1d5707c.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: The inference obtained from this visualization is that the number of non-fraudulent
    transactions remains almost the same across all time periods of the day, whereas
    we see a huge rise in the number of fraudulent transactions during the morning
    `Time` group.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do a last bit of exploration of the transaction amount with respect
    to class:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The preceding code will generate the following output:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: One interesting insight from the summary is that the mean amount in fraudulent
    transactions is higher compared to genuine transactions. However, the maximum
    transaction amount that we see in fraudulent transactions is much lower than the
    genuine transactions. It can also be seen that genuine transactions have a higher
    median amount.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s convert our R dataframe to an H2O dataframe to apply the AE to
    it. This is a requirement in order to use the functions from the `h2o` library:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `tanh` activation function is a rescaled and shifted logistic function.
    Other functions, such as ReLu and Maxout, are also provided by the `h2o` library
    and they can also be used.  In the first AE model, let's use the `tanh` activation
    function. This choice is arbitrary and other activation functions may also be
    tried as desired.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'The `h2o.deeplearning` function has a parameter AE and this should be set to
    `TRUE` to train a AE model. Let''s build our AE model now:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code generates the following output:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We will save the model so we do not have to retrain t again and again. Then
    load the model that is persisted on the disk and print the model to verify the
    AE learning using the following code:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This will generate the following output:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We will now make predictions on test dataset using the AE model that is built,
    using the following code:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This will generate the following output:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'It is possible to visualize the encoder representing the data in a conscious
    manner in the inner layers through the `h2o.deepfeatures` function. Let''s try
    visualizing the reduced data in a second layer:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The preceding code will generate the following output:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let us now plot the data of `DF.L2.C1` with respect to `DF.L2.C2` to verify
    if the encoder has detected the fraudulent transactions, using the following code:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This will generate the following output:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d57f16e-1bf6-47eb-94e4-83557dbb69eb.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: 'Again we plot the data of `DF.L2.C3` with respect to `DF.L2.C4` to verify the
    if the encoder have detected any fraud transaction, using the following code:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The preceding code will generate the following output:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bcd6f63-b386-415a-a650-87cf67b96e22.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: 'We see from the two visualizations that the fraudulent transactions are indeed
    detected by the dimensionality reduction approach with our AE model. Those few
    scattered dots (represented by `1`) depicts the fraud transactions that are detected.
    We can also train a new model with the other hidden layers, using our first model.
    This results in 10 columns, since the third layer has 10 nodes. We are just attempting
    to slice out one layer where some level of reduction was done and use that to
    build a new model:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The preceding code will generate the following output:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As we can see, the training models and data are successfully created. We will
    now go ahead and train the new model, save it and the print it. First, we will
    get the feature names from the sliced encoder layer:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then we will training a new model:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We will then save the model to avoid retraining again, then retrieve the model
    and print it using the following code:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This will generate the following output:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'For measuring model performance on test data, we need to convert the test data
    to the same reduced dimensions as the training data:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The preceding code will generate the following output:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We see, the data has been converted successfully. Now, to make predictions
    on the test dataset with `model_two`, we will use the following code:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'This will generate the following output:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'As we can see, from the output, predictions has been successfully completed
    and now let us visualize the predictions using the following code:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This will generate the following output:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We see that our AE is able to correctly predict non-fraudulent transactions
    with 98% accuracy, which is good. However, it is yielding only 58% accuracy when
    predicting fraudulent transactions. This is definitely something to focus on.
    Our model needs some improvement, and this can be accomplished through the following
    options:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Using other layers' latent space representations as input to build `model_two`
    (recollect that we currently use the layer 3 representation)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ReLu or Maxout activation functions instead of `Tanh`
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking the misclassified instances through the `h2o.anomaly` function and
    increasing or decreasing the cutoff threshold MSE values, which separates the
    fraudulent transactions from non-fraudulent transactions
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trying out a more complex architecture in the encoder and decoder
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are not going to be attempting these options in this chapter as they are
    experimental in nature. However, interested readers may try and improve the accuracy
    of the model by trying these options.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, one best practice is to explicitly shut down the `h2o` cluster. This
    can be accomplished with the following command:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Summary
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about an unsupervised deep learning technique called
    AEs. We covered the definition, working principle, types, and applications of
    AEs. H2O, an open source library that enables us to create deep learning models,
    including AEs, was explored. We then discussed a credit card fraud open dataset
    and implemented a project with an AE to detect fraudulent credit card transactions.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Can deep neural networks help with creative tasks such as prose generation,
    story writing, caption generation for images, and poem writing? Not sure?! Let's
    explore RNNs, in the next chapter, a special type of deep neural network that
    enables us to accomplish creative tasks. Turn the page to explore the world of
    RNNs for prose generation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
