<html><head></head><body>
  <div id="_idContainer208" class="Basic-Text-Frame">
    <h1 class="chapterNumber">6</h1>
    <h1 id="_idParaDest-94" class="chapterTitle">Designing Good Validation</h1>
    <p class="normal">In a Kaggle competition, in the heat of modeling and submitting results, it may seem enough to take at face value the results you get back from the leaderboard. In the end, you may think that what counts in a competition is your ranking. This is a common error that is made repeatedly in competitions. In actual fact, you won’t know what the actual leaderboard (the private one) looks like until after the competition has closed, and trusting the public part of it is not advisable because it is quite often misleading.</p>
    <p class="normal">In this chapter, we will introduce you to the importance of <strong class="keyWord">validation</strong> in data competitions. You will learn about:</p>
    <ul>
      <li class="bulletList">What overfitting is and how a public leaderboard can be misleading</li>
      <li class="bulletList">The dreadful shake-ups</li>
      <li class="bulletList">The different kinds of validation strategies</li>
      <li class="bulletList">Adversarial validation</li>
      <li class="bulletList">How to spot and leverage leakages</li>
      <li class="bulletList">What your strategies should be when choosing your final submissions</li>
    </ul>
    <p class="normal">Monitoring your performances when modeling and distinguishing when overfitting happens is a key competency not only in data science competitions but in all data science projects. Validating your models properly is one of the most important skills that you can learn from a Kaggle competition and that you can resell in the professional world.</p>
    <h1 id="_idParaDest-95" class="heading-1">Snooping on the leaderboard</h1>
    <p class="normal">As we previously described, in each competition, Kaggle divides the test set into a <strong class="keyWord">public part</strong>, which is visualized <a id="_idIndexMarker412"/>on the ongoing leaderboard, and a <strong class="keyWord">private part</strong>, which will be used to calculate the final scores. These test parts are usually randomly determined (although in time series competitions, they are determined based on time) and the entire test set is released without any distinction made between public and private.</p>
    <div class="note">
      <p class="normal">Recently, in order to avoid the scrutinizing of test data in certain competitions, Kaggle has even held back the test data, providing only some examples of it and replacing them with the real test set when the submission is made. These are called <strong class="keyWord">Code</strong> competitions because you are not actually providing the predictions themselves, but a Notebook containing the code to generate them.</p>
    </div>
    <p class="normal">Therefore, a submission derived from a model will cover the entire test set, but only the public part will immediately be scored, leaving the scoring of the private part until after the competition has closed.</p>
    <p class="normal">Given this, three considerations arise:</p>
    <ul>
      <li class="bulletList">In order for a competition to work properly, training data and test data should be from <strong class="keyWord">the same distribution</strong>. Moreover, the private and public parts of the test data should resemble each other in terms of distribution.</li>
      <li class="bulletList">Even if the training and test data are apparently from the same distribution, the <strong class="keyWord">lack of sufficient examples</strong> in either set could make it difficult to obtain aligned results between the training data and the public and private test data.</li>
      <li class="bulletList">The public test data should be regarded as a holdout test in a data science project: to be used only for final validation. Hence, it should not be queried much in order to avoid what is called <strong class="keyWord">adaptive overfitting</strong>, which implies a model that works well <a id="_idIndexMarker413"/>on a specific test set but underperforms on others.</li>
    </ul>
    <p class="normal">Keeping in mind these three considerations is paramount to understanding the dynamics of a competition. In most competitions, there are always quite a few questions in the discussion forums about how the training, public, and private test data relate to each other, and it is quite common to see submissions of hundreds of solutions that have only been evaluated based on their efficacy on the public leaderboard.</p>
    <p class="normal">It is also common to hear discussions about <strong class="keyWord">shake-ups</strong> that revolutionize the rankings. They are, in fact, a rearranging of <a id="_idIndexMarker414"/>the final rankings that can disappoint many who previously held better positions on the public leaderboard. Anecdotally, shake-ups are commonly attributed to <a id="_idIndexMarker415"/>differences between the training and test set or between the private and public parts of the test data. They are measured <em class="italic">ex ante</em> based on how competitors have seen their expected local scores correlate with the leaderboard feedback and <em class="italic">ex post</em> by a series of analyses based on two figures:</p>
    <ul>
      <li class="bulletList">A general shake-up figure based on <code class="inlineCode">mean(abs(private_rank-public_rank)/number_of_teams)</code></li>
      <li class="bulletList">A top leaderboard shake-up figure, taking into account only the top 10% of public ranks</li>
    </ul>
    <div class="note">
      <p class="normal">These <em class="italic">ex post</em> figures were first devised by <em class="italic">Steve Donoho</em> (<a href="https://www.kaggle.com/breakfastpirate"><span class="url">https://www.kaggle.com/breakfastpirate</span></a>) who compiled a ranking of the worst Kaggle shake-ups (see <a href="https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49106#278831"><span class="url">https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49106#278831</span></a>). They are nowadays easily available, recreated by many Notebooks based on the Meta Kaggle dataset we discussed in <em class="chapterRef">Chapter 5</em>, <em class="italic">Competition Tasks and Metrics</em> (see <a href="https://www.kaggle.com/jtrotman/meta-kaggle-competition-shake-up"><span class="url">https://www.kaggle.com/jtrotman/meta-kaggle-competition-shake-up</span></a>). For instance, by consulting these figures, you may find out how dreadful the <em class="italic">RSNA Intracranial Hemorrhage Detection</em> competition was for many because of its shake-ups, especially in the top positions.</p>
    </div>
    <p class="normal">However, aside from an <em class="italic">ex post</em> evaluation, there are quite a few lessons that we can get from previous shake-ups that can help you in your Kaggle competitions. A few researchers from UC Berkeley think so too. In their paper presented at NIPS 2019, Roelofs, Fridovich-Keil et al. study in detail a few thousand Kaggle competitions to gain insight into the public-private leaderboard dynamics in Kaggle competitions. Although they focus on a limited subset of competitions (120, above a certain number of participants, focused on binary classification), they obtained some interesting findings:</p>
    <ul>
      <li class="bulletList">There is little adaptive overfitting; in other words, public standings usually do hold in the unveiled private leaderboard.</li>
      <li class="bulletList">Most shake-ups are due to random fluctuations and overcrowded rankings where competitors are too near to each other, and any slight change in the performance in the private test sets causes major changes in the rankings.</li>
      <li class="bulletList">Shake-ups happen <a id="_idIndexMarker416"/>when the training set is very small or the training data is not <strong class="keyWord">independent and identically distributed </strong>(<strong class="keyWord">i.i.d.</strong>).</li>
    </ul>
    <div class="note">
      <p class="normal">The full paper, Roelofs, R., Fridovich-Keil, S. et al. <em class="italic">A meta-analysis of overfitting in machine learning</em>. Proceedings of the 33<sup class="superscript">rd</sup> International Conference on Neural Information Processing Systems. 2019, can be found at this link: <a href="https://papers.nips.cc/paper/2019/file/ee39e503b6bedf0c98c388b7e8589aca-Paper.pdf"><span class="url">https://papers.nips.cc/paper/2019/file/ee39e503b6bedf0c98c388b7e8589aca-Paper.pdf</span></a>.</p>
    </div>
    <p class="normal">In our long experience of Kaggle competitions, however, we have seen quite a lot of problems with adaptive overfitting since the beginning. For instance, you can read <em class="italic">Greg Park</em>’s analysis of one of the first competitions we ever took part in: <a href="http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/"><span class="url">http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/</span></a>. Since this is quite a common and persistent problem for many Kagglers, we suggest a strategy that is a bit more sophisticated than <a id="_idIndexMarker417"/>simply following what happens on the public leaderboard:</p>
    <ul>
      <li class="bulletList">Always build reliable cross-validation systems for local scoring.</li>
      <li class="bulletList">Always try to control non-i.i.d distributions using the best validation scheme dictated by the situation. Unless clearly stated in the description of the competition, it is not an easy task to spot non-i.i.d. distributions, but you can get hints from discussion or by experimenting using stratified validation schemes (when stratifying according to a certain feature, the results improve decisively, for instance).</li>
      <li class="bulletList">Correlate local scoring with the public leaderboard in order to figure out whether or not they go in the same direction.</li>
      <li class="bulletList">Test using adversarial validation, revealing whether or not the test distribution is similar to the training data.</li>
      <li class="bulletList">Make your solutions more robust using ensembling, especially if you are working with small datasets.</li>
    </ul>
    <p class="normal">In the following sections, we are going to explore each of these ideas (except for ensembling, which is the topic of a future chapter) and provide you with all the best tools and strategies to obtain the best results, especially on the private dataset.</p>
    <h1 id="_idParaDest-96" class="heading-1">The importance of validation in competitions</h1>
    <p class="normal">If you think about a competition carefully, you can imagine it as a huge system of experiments. Whoever <a id="_idIndexMarker418"/>can create the most systematic and efficient way to run these experiments wins.</p>
    <p class="normal">In fact, in spite of all your theoretical knowledge, you will be in competition with the hundreds or thousands of data professionals who have more or less the same competencies as you. </p>
    <p class="normal">In addition, they will be using exactly the same data as you and roughly the same tools for learning from the data (TensorFlow, PyTorch, Scikit-learn, and so on). Some will surely have better access to computational resources, although the availability of Kaggle Notebooks and generally decreasing cloud computing prices mean the gap is no longer so wide. Consequently, if you look at differences in knowledge, data, models, and available computers, you won’t find many discriminating factors between you and the other competitors that could explain huge performance differences in a competition. Yet, some participants consistently outperform others, implying there is some underlying success factor.</p>
    <p class="normal">In interviews and meet-ups, some Kagglers describe this success factor as “grit,” some others as “trying everything,” some others again as a “willingness to put everything you have into a competition.” These may sound a bit obscure and magic. Instead, we call it <strong class="keyWord">systematic experimentation</strong>. In our opinion, the key to successful participation resides in the number of experiments you <a id="_idIndexMarker419"/>conduct and the way you run all of them. The more experiments you undertake, the more chances you will have to crack the problem better than other participants. This number certainly depends on a few factors, such as the time you have available, your computing resources (the faster the better, but as we previously mentioned, this is not such a strong differentiator <em class="italic">per se</em>), your team size, and their involvement in the task. This aligns with the commonly reported grit and engagement as keys for success.</p>
    <p class="normal">However, these are not the only factors affecting the result. You have to take into account that the way you run your experiments also has an impact. <em class="italic">Fail fast and learn from it</em> is an important factor in a competition. Of course, you need to reflect carefully both when you fail and when you succeed in order to learn something from your experiences, or your competition will just turn into a random sequence of attempts in the hope of picking the right solution.</p>
    <p class="normal">Therefore, <em class="italic">ceteris paribus</em>, having a proper <strong class="keyWord">validation strategy</strong> is the great discriminator between successful Kaggle competitors <a id="_idIndexMarker420"/>and those who just overfit the leaderboard and end up in lower-than-expected rankings after a competition.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Validation</strong> is the method you use to correctly evaluate the errors that your model produces and to <a id="_idIndexMarker421"/>measure how its performance improves or decreases based on your experiments.</p>
    </div>
    <p class="normal">Generally, the impact of choosing proper validation is too often overlooked in favor of more quantitative factors, such as having the latest, most powerful GPU or a larger team producing submissions. </p>
    <p class="normal">Nevertheless, if you count only on the firepower of experiments and their results on the <a id="_idIndexMarker422"/>leaderboard, it will be like “throwing mud at the wall and hoping something will stick” (see <a href="http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/"><span class="url">http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/</span></a>). Sometimes such a strategy will work, but most often it won’t, because you will miss important opportunities to experiment in the right direction, and you won’t even be able to see the shining gem you managed to produce in the middle of all that mud. For instance, if you concentrate too much on trying your luck on the public leaderboard using a random, unsystematic strategy, even if you produce great solutions, you may end up not choosing your final submission correctly and missing the best scoring one on the private leaderboard.</p>
    <p class="normal">Having a proper validation strategy can help you decide which of your models should be submitted for ranking on the private test set. Though the temptation to submit your top public leaderboard models may be high, <em class="italic">always consider your own validation scores</em>. For your final submissions, depending on the situation and whether or not you trust the leaderboard, choose your best model based on the leaderboard and your best based on your local validation results. If you don’t trust the leaderboard (especially when the training sample is small or the examples are non-i.i.d.), submit models that have two of the best validation scores, picking two very different models or ensembles. In this way, you will reduce the risk of choosing solutions that won’t perform on the private test set.</p>
    <p class="normal">Having pointed out the importance of having a method of experimenting, what is left is all a matter of the practicalities of validation. In fact, when you model a solution, you take a series of interrelated decisions:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">How to process your data</li>
      <li class="numberedList">What model to apply</li>
      <li class="numberedList">How to change the model’s architecture (especially true for deep learning models)</li>
      <li class="numberedList">How to set the model’s hyperparameters</li>
      <li class="numberedList">How to post-process the predictions</li>
    </ol>
    <p class="normal">Even if the public leaderboard is perfectly correlated with the private one, the limited number of daily submissions (a limitation present in all competitions) prevents you from even scratching <a id="_idIndexMarker423"/>the surface of possible tests that you could do in all the aforementioned areas. Having a proper validation system tells you beforehand if what you are doing could work on the leaderboard.</p>
    <div class="interviewBox">
      <div class="intervieweePhoto">
        <img src="../Images/Dmitry_Larko.png" alt=""/>
      </div>
      <p class="intervieweeName">Dmitry Larko</p>
      <p class="normal"><a href="https://www.kaggle.com/dmitrylarko"><span class="url">https://www.kaggle.com/dmitrylarko</span></a></p>
      <p class="normal"><strong class="keyWord">Dmitry Larko</strong> is a Kaggle Competition Grandmaster and the chief data scientist at H2O.ai. He has over a decade of experience in ML and data science. He discovered Kaggle in December 2012 and participated in his first competition a <a id="_idIndexMarker424"/>few months later. He is a strong advocate of validation in Kaggle competitions, as he told us in his interview.</p>
      <p class="interviewHeader">What’s your favorite kind of competition and why? In terms of techniques and solving approaches, what is your specialty on Kaggle?</p>
      <p class="normal"><em class="italic">I have mostly participated in competitions for tabular datasets but also enjoy competitions for computer vision.</em></p>
      <p class="interviewHeader">How do you approach a Kaggle competition? How different is this approach to what you do in your day-to-day work?</p>
      <p class="normal"><em class="italic">I always try to start simple and build a submission pipeline for smaller/simpler models first. A major step here is to create a proper validation scheme so you can validate your ideas in a robust way. Also, it is always a good idea to spend as much time as you can looking at the data and analyzing it.</em></p>
      <p class="normal"><em class="italic">In my day-to-day work, I am building an AutoML platform, so a lot of things I try on Kaggle end up being implemented as a part of this platform.</em></p>
      <p class="interviewHeader">Tell us about a particularly challenging competition you entered, and what insights you used to tackle the task.</p>
      <p class="normal"><em class="italic">Nothing comes to my mind, and it doesn’t matter, because what is technically challenging for me could be a piece of cake for somebody else. Technical challenges are not that important; what’s important is to remember that a competition is somewhat like a marathon, not a sprint. Or you can see it as a marathon of sprints if you like. So, it is important not to get exhausted, sleep well, exercise, and take a walk in a park to regenerate your brain for new ideas. To win a Kaggle competition, you will need all your creativity and expertise and sometimes even a bit of luck.</em></p>

      <p class="interviewHeader">Has Kaggle helped you in your career? If so, how?</p>
      <p class="normal"><em class="italic">I got my current job thanks to the fact I was a Kaggle Competition Grandmaster. For my current employer, this fact was evidence enough of my expertise in the field.</em></p>
      <p class="interviewHeader">In your experience, what do inexperienced Kagglers often overlook? What do you know now that you wish you’d known when you first started?</p>
      <p class="normal"><em class="italic">Mostly they overlook the right validation scheme and follow the feedback from the public leaderboard. That ends badly in most cases, leading to something known as a “shake-up” on Kaggle.</em></p>
      <p class="normal"><em class="italic">Also, they rush to skip exploratory data analysis and build models right away, which leads to simplistic solutions and mediocre leaderboard scores.</em></p>
      <p class="interviewHeader">What mistakes have you made in competitions in the past?</p>
      <p class="normal"><em class="italic">My main mistake is really the same that an inexperienced person will make – following the leaderboard score and not my internal validation. Every time I decided to do so, it cost me several places on the leaderboard.</em></p>
      <p class="interviewHeader">Are there any particular tools or libraries that you would recommend using for data analysis or machine learning?</p>
      <p class="normal"><em class="italic">That would be the usual suspects. For tabular data: LightGBM, XGBoost, CatBoost; for deep learning: PyTorch, PyTorch-Lightning, timm; and Scikit-learn for everyone.</em></p>
      <p class="interviewHeader">What’s the most important thing someone should keep in mind or do when they’re entering a competition?</p>
      <p class="normal"><em class="italic">Start simple, always validate; believe in your validation score and not the leaderboard score.</em></p>
    </div>
    <h2 id="_idParaDest-97" class="heading-2">Bias and variance</h2>
    <p class="normal">A good validation system helps you with metrics that are more reliable than the error measures you get from your training set. In fact, metrics obtained on the training set are affected by the capacity and complexity of each model. You can think of the <strong class="keyWord">capacity</strong> of a model as its memory that it can use to learn from data. </p>
    <p class="normal">Each model has a set of internal parameters that help the model to record the patterns taken from the data. Every model has its own skills for acquiring patterns, and some models will spot certain rules or associations whereas others will spot others. As a model extracts patterns from data, it records them in its “memory.”</p>
    <p class="normal">You also hear about the capacity or expressiveness of a model as a matter of <strong class="keyWord">bias and variance</strong>. In this case, the bias and variance of a model refer to the predictions, but the underlying principle <a id="_idIndexMarker425"/>is strictly related to the expressiveness of a <a id="_idIndexMarker426"/>model. Models can be reduced to mathematical functions that map an input (the observed data) to a result (the predictions). Some mathematical functions are more complex than others, in the number of internal parameters they have and in the ways they use them:</p>
    <ul>
      <li class="bulletList">If the mathematical function of a model is not complex or expressive enough to capture the complexity of the problem you are trying to solve, we talk of <strong class="keyWord">bias</strong>, because your predictions will be limited (“biased”) by the limits of the model itself.</li>
      <li class="bulletList">If the mathematical function at the core of a model is too complex for the problem at hand, we have a <strong class="keyWord">variance</strong> problem, because the model will record more details and noise in the training data than needed and its predictions will be deeply influenced by them and become erratic.</li>
    </ul>
    <p class="normal">Nowadays, given the advances in machine learning and the available computation resources, the problem is always due to variance, since deep neural networks and gradient boosting, the most commonly used solutions, often have a mathematical expressiveness that exceeds what most of the problems you will face need in order to be solved.</p>
    <p class="normal">When all the useful patterns that a certain model can extract have been captured, if the model has not exhausted <a id="_idIndexMarker427"/>its capacity, it will then start memorizing <a id="_idIndexMarker428"/>data characteristics and signals that are unrelated to the prediction (usually referred to as <strong class="keyWord">noise</strong>). While the initially extracted patterns will help the model to generalize to a test dataset and predict more correctly, not everything that it learns specifically about the training set will help; instead, it may damage its performance. The process of learning elements of the training set that have no <a id="_idIndexMarker429"/>generalization value is commonly called <strong class="keyWord">overfitting</strong>.</p>
    <p class="normal">The core purpose of validation is to explicitly define a score or loss value that separates the generalizable part of that <a id="_idIndexMarker430"/>value from that due to overfitting the training set characteristics. </p>
    <p class="normal">This is the <strong class="keyWord">validation loss</strong>. You can see the situation visualized in the following figure of learning curves:</p>
    <figure class="mediaobject"><img src="../Images/B17574_06_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.1: Learning more from the training data does not always mean learning to predict</p>
    <p class="normal">If you graph the loss measure on the <em class="italic">y</em>-axis against some measure of learning effort of the model (this could be epochs for neural networks, or rounds for gradient boosting) on the <em class="italic">x</em>-axis, you will notice that learning always seems to happen on the training dataset, but this is not always true on other data.</p>
    <p class="normal">The same thing happens even if you change the hyperparameters, process the data, or decide on a different model altogether. The curves will change shape, but you’ll always have a sweet point where overfitting starts. That point can be different across models and between the <a id="_idIndexMarker431"/>various choices that you make in your modeling <a id="_idIndexMarker432"/>efforts. If you have properly computed the point when overfitting starts thanks to a correct validation strategy, your model’s performance will surely correlate with the leaderboard results (both public and private), and your validation metrics will provide you with a proxy to evaluate your work without making any submissions.</p>
    <p class="normal">You can hear about overfitting at various levels:</p>
    <ul>
      <li class="bulletList">At the level of the training data, when you use a model that is too complex for the problem</li>
      <li class="bulletList">At the level of the validation set itself, when you tune your model too much with respect to a specific validation set</li>
      <li class="bulletList">At the level of the public leaderboard, when your results are far from what you would expect from your training</li>
      <li class="bulletList">At the level of the private leaderboard, when in spite of the good results on the public leaderboard, your private scores will be disappointing</li>
    </ul>
    <p class="normal">Though slightly different in meaning, they all equally imply that your model is not generalizable, as we have described in this section.</p>
    <h1 id="_idParaDest-98" class="heading-1">Trying different splitting strategies</h1>
    <p class="normal">As previously discussed, the validation loss is based on a data sample that is not part of the training set. It is an empirical measure that tells you how good your model is at predicting, and a more <a id="_idIndexMarker433"/>correct one than the score you get from your training, which will tell you mostly how much your model has memorized the training data patterns. Correctly choosing the data sample you use for validation constitutes your validation strategy.</p>
    <p class="normal">To summarize the strategies for validating your model and measuring its performance correctly, you <a id="_idIndexMarker434"/>have a couple of choices:</p>
    <ul>
      <li class="bulletList">The first choice is to <strong class="keyWord">work with a holdout system</strong>, incurring the risk of not properly choosing a representative sample of the data or overfitting to your validation holdout.</li>
      <li class="bulletList">The second option is to <strong class="keyWord">use a probabilistic approach</strong> and rely on a series of samples to draw your <a id="_idIndexMarker435"/>conclusions on your models. Among the probabilistic approaches, you have cross-validation, <strong class="keyWord">leave-one-out</strong> (<strong class="keyWord">LOO</strong>), and bootstrap. Among the cross-validation strategies, there are different <a id="_idIndexMarker436"/>nuances depending on the sampling strategies you take based on the characteristic of your data (simple random sampling, stratified sampling, sampling by groups, time sampling).</li>
    </ul>
    <p class="normal">What all these strategies have in common is that they are <strong class="keyWord">sampling strategies</strong>. It means that they help you to infer a <a id="_idIndexMarker437"/>general measure (the performance of your model) based on a small part of your data, randomly selected. Sampling is at the root of statistics and it is not an exact procedure because, based on your sampling method, your available data, and the randomness of picking up certain cases as part of your sample, you will experience a certain degree of error. </p>
    <p class="normal">For instance, if you rely on a biased sample, your evaluation metric may be estimated incorrectly (over- or under-estimated). However, if properly designed and implemented, sampling strategies generally provide you with a good estimate of your general measure.</p>
    <p class="normal">The other aspect that all these strategies have in common is that they are <strong class="keyWord">partitions</strong>, which divide cases in an exclusive way as either part of the training or part of the validation. In fact, as we discussed, since most models have a certain memorization capability, using the same cases in both training and validation leads to inflated estimates because it allows the model to demonstrate its memorization abilities; instead, we want it to be evaluated on its ability to derive patterns and functions that work on <em class="italic">unseen</em> examples.</p>
    <h2 id="_idParaDest-99" class="heading-2">The basic train-test split</h2>
    <p class="normal">The first strategy that we will analyze is the <strong class="keyWord">train-test split</strong>. In this strategy, you sample a portion of your <a id="_idIndexMarker438"/>training set (also known as the <strong class="keyWord">holdout</strong>) and you use it as a test set for all the models that you train using the remaining part of the data.</p>
    <p class="normal">The great advantage of this strategy is that it is very simple: you pick up a part of your data and you check your <a id="_idIndexMarker439"/>work on that part. You usually split the data 80/20 in favor of the training partition. In Scikit-learn, it is implemented in the <code class="inlineCode">train_test_split</code> function. We’ll draw your attention to a couple of aspects of the method:</p>
    <ul>
      <li class="bulletList">When you have large amounts of data, you can expect that the test data you extract is similar to (representative of) the original distribution on the entire dataset. However, since the extraction process is based on randomness, you always have the chance of extracting a non-representative sample. In particular, the chance increases if the training sample you start from is small. Comparing the extracted holdout partition using <strong class="keyWord">adversarial validation</strong> (more about this in a few sections) can help you to make sure you are evaluating your efforts in a correct way.</li>
      <li class="bulletList">In addition, to ensure that your test sampling is representative, especially with regard to how the <a id="_idIndexMarker440"/>training data relates to the target variable, you can use <strong class="keyWord">stratification</strong>, which ensures that the proportions of certain features are respected in the sampled data. You can use the <code class="inlineCode">stratify</code> parameter in the <code class="inlineCode">train_test_split</code> function and provide an array containing the class distribution to preserve.</li>
    </ul>
    <p class="normal">We have to remark that, even if you have a representative holdout available, sometimes a simple train-test split is not enough for ensuring a correct tracking of your efforts in a competition. </p>
    <p class="normal">In fact, as you keep checking on this test set, you may drive your choices to some kind of adaptation overfitting (in other words, erroneously picking up the noise of the training set as signals), as happens when you frequently evaluate on the public leaderboard. For this reason, a probabilistic evaluation, though more computationally expensive, is more suited for a competition.</p>
    <h2 id="_idParaDest-100" class="heading-2">Probabilistic evaluation methods</h2>
    <p class="normal">Probabilistic evaluation of the performance of a machine learning model is based on the statistical properties <a id="_idIndexMarker441"/>of a sample from a distribution. By sampling, you create a smaller set of your original data that is expected to have the same characteristics. In addition, what is left untouched from the sampling constitutes a sample in itself, and it is also expected to have the same characteristics as the original data. By training and testing your model on this sampled data and repeating this procedure a large number of times, you are basically creating a statistical estimator measuring the performance of your model. Every sample may have some “error” in it; that is, it may not be fully representative of the true distribution of the original data. However, as you sample <a id="_idIndexMarker442"/>more, the mean of your estimators on these multiple samples will converge to the true mean of the measure you are estimating (this is an observed outcome that, in probability, is explained by a theorem called the <em class="italic">Law of Large Numbers</em>).</p>
    <p class="normal">Probabilistic estimators naturally require more computations than a simple train-test split, but they offer more confidence that you are correctly estimating the right measure: the general performance of your model.</p>
    <h3 id="_idParaDest-101" class="heading-3">k-fold cross-validation</h3>
    <p class="normal">The most used probabilistic validation method is <strong class="bold-italic" style="font-style: italic;">k</strong><strong class="keyWord">-fold cross-validation</strong>, which is recognized as having <a id="_idIndexMarker443"/>the ability to <a id="_idIndexMarker444"/>correctly estimate the performance of your model on unseen test data drawn from the same distribution. </p>
    <div class="note">
      <p class="normal">This is clearly explained in the paper Bates, S., Hastie, T., and Tibshirani, R.; <em class="italic">Cross-validation: what does it estimate and how well does it do it?</em> arXiv preprint arX­iv:2104.00673, 2021 (<a href="https://arxiv.org/pdf/2104.00673.pdf"><span class="url">https://arxiv.org/pdf/2104.00673.pdf</span></a>).</p>
    </div>
    <p class="normal"><em class="italic">k</em>-fold cross-validation can be successfully used to compare predictive models, as well as when selecting the hyperparameters for your model that will perform the best on the test set.</p>
    <p class="normal">There are quite a few different variations of <em class="italic">k</em>-fold cross-validation, but the simplest one, which is implemented in the <code class="inlineCode">KFold</code> function in Scikit-learn, is based on the splitting of your available training data into <em class="italic">k</em> partitions. After that, for <em class="italic">k</em> iterations, one of the <em class="italic">k</em> partitions is taken as a test set while the others are used for the training of the model. </p>
    <p class="normal">The <em class="italic">k</em> validation scores are then averaged and that averaged score value is the <em class="italic">k</em>-fold validation score, which will tell you the estimated average model performance on any unseen data. The standard deviation of the scores will inform you about the uncertainty of the estimate. <em class="italic">Figure 6.2</em> demonstrates how 5-fold cross-validation is structured:</p>
    <figure class="mediaobject"><img src="../Images/B17574_06_02.png" alt=""/></figure>
    <figure class="mediaobject">Figure 6.2: How a 5-fold validation scheme is structured</figure>
    <p class="normal">One important aspect of the <em class="italic">k</em>-fold cross-validation score you have to keep in mind is that it estimates the average score of a model trained on the same quantity of data as <em class="italic">k - 1</em> folds. If, afterward, you train your model on all your data, the previous validation estimate no <a id="_idIndexMarker445"/>longer holds. As <em class="italic">k</em> approaches <a id="_idIndexMarker446"/>the number <em class="italic">n</em> of examples, you have an increasingly correct estimate of the model derived on the full training set, yet, due to the growing correlation between the estimates you obtain from each fold, you will lose all the probabilistic estimates of the validation. In this case, you’ll end up having a number showing you the performance of your model on your training data (which is still a useful estimate for comparison reasons, but it won’t help you in correctly estimating the generalization power of your model).</p>
    <p class="normal">When you reach <em class="italic">k = n</em>, you have the LOO validation method, which is useful when you have a few cases available. The method is mostly an unbiased fitting measure since it uses almost all the available data for training and just one example for testing. Yet it is not a good estimate of the expected performance on unseen data. Its repeated tests over the whole dataset are highly correlated with each other and the resulting LOO metric represents more the performance of the model on the dataset itself than the performance the model would have on unknown data.</p>
    <p class="normal">The correct <em class="italic">k</em> number of partitions to choose is decided based on a few aspects relative to the data you have available:</p>
    <ul>
      <li class="bulletList">The smaller the <em class="italic">k</em> (the minimum is 2), the smaller each fold will be, and consequently, the more bias in learning there will be for a model trained on <em class="italic">k - 1</em> folds: your model validated on a smaller <em class="italic">k</em> will be less well-performing with respect to a model trained on a larger <em class="italic">k</em>.</li>
      <li class="bulletList">The higher the <em class="italic">k</em>, the more the data, yet the more correlated your validation estimates: you will lose the interesting properties of <em class="italic">k</em>-fold cross-validation in estimating the performance on unseen data. </li>
    </ul>
    <p class="normal">Commonly, <em class="italic">k</em> is set to 5, 7, or 10, more seldom to 20 folds. We usually regard <em class="italic">k</em> = 5 or <em class="italic">k</em> = 10 as a good choice <a id="_idIndexMarker447"/>for a competition, with the <a id="_idIndexMarker448"/>latter using more data for each training (90% of the available data), and hence being more suitable for figuring out the performance of your model when you retrain on the full dataset.</p>
    <p class="normal">When deciding upon what <em class="italic">k</em> to choose for a specific dataset in a competition, we find it useful to reflect on two perspectives.</p>
    <p class="normal">Firstly, the choice of the number of folds should reflect your goals:</p>
    <ul>
      <li class="bulletList">If your purpose is performance estimation, you need models with low bias estimates (which means no systematic distortion of estimates). You can achieve this by using a higher number of folds, usually between 10 and 20.</li>
      <li class="bulletList">If your aim is parameter tuning, you need a mix of bias and variance, so it is advisable to use a medium number of folds, usually between 5 and 7.</li>
      <li class="bulletList">Finally, if your purpose is just to apply variable selection and simplify your dataset, you need models with low variance estimates (or you will have disagreement). Hence, a lower number of folds will suffice, usually between 3 and 5.</li>
    </ul>
    <p class="normal">When the size of the available data is quite large, you can safely stay on the lower side of the suggested bands.</p>
    <p class="normal">Secondly, if you are just aiming for performance estimation, consider that the more folds you use, the fewer cases you will have in your validation set, so the more the estimates of each fold will be correlated. Beyond a certain point, increasing <em class="italic">k</em> renders your cross-validation estimates less predictive of unseen test sets and more representative of an estimate of how well-performing your model is on your training set. This also means that, with more folds, you can get the perfect out-of-fold prediction for stacking purposes, as we will explain in detail in <em class="chapterRef">Chapter 9</em>, <em class="italic">Ensembling with Blending and Stacking Solutions</em>.</p>
    <div class="note">
      <p class="normal">In Kaggle competitions, <em class="italic">k</em>-fold cross-validation is often applied not only for validating your solution approach and figuring out the performance of your model, but to produce your prediction. When you cross-validate, you are subsampling, and averaging the results of multiple models built on subsamples of the data is an effective strategy for fighting against variance, and often more effective than training on all the data available (we will discuss this more in <em class="chapterRef">Chapter 9</em>). Hence, many Kaggle competitors use the models built during cross-validation to provide a series of predictions on the test set that, averaged, will provide them with the solution.</p>
    </div>
    <h4 class="heading-4">k-fold variations</h4>
    <p class="normal">Since it is based on random sampling, <em class="italic">k</em>-fold can provide unsuitable splits when:</p>
    <ul>
      <li class="bulletList">You have <a id="_idIndexMarker449"/>to preserve the proportion of small classes, both <a id="_idIndexMarker450"/>at a target level and at the level of features. This is typical when your target is highly imbalanced. Typical examples are spam datasets (because spam is a small fraction of the normal email volume) or any credit risk dataset where you have to predict the not-so-frequent event of a defaulted loan.</li>
      <li class="bulletList">You have to preserve the distribution of a numeric variable, both at a target level and at the level of features. This is typical of regression problems where the distribution is quite skewed or you have heavy, long tails. A common example is house price prediction, where you have a consistent small portion of houses on sale that will cost much more than the average house.</li>
      <li class="bulletList">Your cases are non-i.i.d, in particular when dealing with time series forecasting.</li>
    </ul>
    <p class="normal">In the first two scenarios, the solution is the <strong class="keyWord">stratified </strong><strong class="bold-italic" style="font-style: italic;">k</strong><strong class="keyWord">-fold</strong>, where the sampling is done in a controlled way that preserves the distribution you want to preserve. If you need to preserve the distribution of a single class, you can use <code class="inlineCode">StratifiedKFold</code> from Scikit-learn, using a <a id="_idIndexMarker451"/>stratification variable, usually your target variable but also any other feature whose distribution you need to preserve. The function will produce a set of indexes that will help you to partition your data accordingly. You can <a id="_idIndexMarker452"/>also obtain the same result with a numeric variable, after having discretized it, using <code class="inlineCode">pandas.cut</code> or <a id="_idIndexMarker453"/>Scikit-learn’s <code class="inlineCode">KBinsDiscretizer</code>.</p>
    <p class="normal">It is a bit more complicated when you have to stratify based on multiple variables or overlapping labels, such <a id="_idIndexMarker454"/>as in multi-label classification. </p>
    <p class="normal">You can find a solution in the <strong class="keyWord">Scikit-multilearn</strong> package (<a href="http://scikit.ml/"><span class="url">http://scikit.ml/</span></a>), in particular, the <code class="inlineCode">IterativeStratification</code> command that helps you to control the order (the number of combined proportions of multiple variables) that you <a id="_idIndexMarker455"/>want to preserve (<a href="http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html"><span class="url">http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html</span></a>). It implements the algorithm explained by the following papers:</p>
    <ul>
      <li class="bulletList">Sechidis, K., Tsoumakas, G., and Vlahavas, I. (2011). <em class="italic">On the stratification of multi-label data</em>. <em class="italic">Machine Learning and Knowledge Discovery in Databases, 145-158</em>. <a href="http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf"><span class="url">http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf</span></a></li>
      <li class="bulletList">Szymański, P. and Kajdanowicz, T.; <em class="italic">Proceedings of the First International Workshop on Learning with Imbalanced Domains</em>: <em class="italic">Theory and Applications</em>, PMLR 74:22-35, 2017. <a href="http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html"><span class="url">http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html</span></a></li>
    </ul>
    <p class="normal">You can actually make good use of stratification even when your problem is not a classification, but a <a id="_idIndexMarker456"/>regression. Using stratification <a id="_idIndexMarker457"/>in regression problems helps your regressor to fit during cross-validation on a similar distribution of the target (or of the predictors) to the one found in the entire sample. In these cases, in order to have <code class="inlineCode">StratifiedKFold</code> working correctly, you have to use a discrete proxy for your target instead of your continuous target.</p>
    <p class="normal">The first, simplest way of achieving this is to use the pandas <code class="inlineCode">cut</code> function and divide your target into a large enough number of bins, such as 10 or 20:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
y_proxy = pd.cut(y_train, bins=<span class="hljs-number">10</span>, labels=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">In order to determine the number of bins to be used, <em class="italic">Abhishek Thakur</em> prefers to use <strong class="keyWord">Sturges’ rule </strong>based on the number of examples available, and provide that number to the pandas <code class="inlineCode">cut</code> function (see <a href="https://www.kaggle.com/abhishek/step-1-create-folds"><span class="url">https://www.kaggle.com/abhishek/step-1-create-folds</span></a>):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
bins = <span class="hljs-built_in">int</span>(np.floor(<span class="hljs-number">1</span> + np.log2(<span class="hljs-built_in">len</span>(X_train))))
</code></pre>
    <p class="normal">An alternative approach is to focus on the distributions of the features in the training set and aim to reproduce them. This requires the use of <strong class="keyWord">cluster analysis</strong> (an unsupervised approach) on the features <a id="_idIndexMarker458"/>of the training set, thus <a id="_idIndexMarker459"/>excluding the target variable and any identifiers, and then using the predicted clusters as strata. You can see an example in this Notebook (<a href="https://www.kaggle.com/lucamassaron/are-you-doing-cross-validation-the-best-way"><span class="url">https://www.kaggle.com/lucamassaron/are-you-doing-cross-validation-the-best-way</span></a>), where first a PCA (principal component analysis) is performed to remove correlations, and then a <em class="italic">k</em>-means cluster analysis is performed. You can decide on the number of clusters to use by running empirical tests.</p>
    <p class="normal">Proceeding with our discussion of the cases where <em class="italic">k</em>-fold can provide unsuitable splits, things get tricky in the third scenario, when you have non-i.i.d. data, such as in the case of some grouping happening among examples. The problem with non-i.i.d. examples is that the features and target are correlated between the examples (hence it is easier to predict all the examples if you know just one example among them). In fact, if you happen to have the same group divided between training and testing, your model may learn to distinguish the groups and not the target itself, producing a good validation score but very bad <a id="_idIndexMarker460"/>results on the leaderboard. The solution here is to use <code class="inlineCode">GroupKFold</code>: by providing a grouping variable, you will have the assurance that each group will be placed either in the training folds or in the validation ones, but never split between the two.</p>
    <div class="note">
      <p class="normal">Discovering groupings in the data that render your data non-i.i.d. is actually not an easy task to accomplish. Unless stated by the competition problem, you will have to rely on your ability to investigate the data (using unsupervised learning techniques, such as cluster analysis) and the domain of the problem. For instance, if your data is about mobile telephone usage, you may realize that some examples are from the same user by noticing sequences of similar values in the features.</p>
    </div>
    <p class="normal">Time series analysis presents the same problem, and since data is non-i.i.d., you cannot validate by random sampling because you will mix different time frames and later time frames could bear <a id="_idIndexMarker461"/>traces of the previous <a id="_idIndexMarker462"/>ones (a characteristic called <strong class="keyWord">auto-correlation</strong> in statistics). In the most <a id="_idIndexMarker463"/>basic approach to validation in time series, you can use a training and validation split based on time, as illustrated by <em class="italic">Figure 6.3</em>:</p>
    <figure class="mediaobject"><img src="../Images/B17574_06_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.3: Training and validation splits are based on time</p>
    <p class="normal">Your validation capabilities will be limited, however, since your validation will be anchored to a specific time. For a more complex approach, you can use time split validation, <code class="inlineCode">TimeSeriesSplit</code>, as provided by the Scikit-learn package (<code class="inlineCode">sklearn.model_selection.TimeSeriesSplit</code>). <code class="inlineCode">TimeSeriesSplit</code> can help you set the timeframe of your training and testing portions of the time series. </p>
    <p class="normal">In the case of the training timeframe, the <code class="inlineCode">TimeSeriesSplit</code> function can help you to set your training data so it involves all the past data before the test timeframe, or limit it to a fixed period lookback (for instance, always using the data from three months before the test timeframe for training).</p>
    <p class="normal">In <em class="italic">Figure 6.4</em>, you can see the structure of a time-based validation strategy involving a growing training set and a moving validation set:</p>
    <figure class="mediaobject"><img src="../Images/B17574_06_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.4: The training set is growing over time</p>
    <p class="normal">In <em class="italic">Figure 6.5</em>, you can instead see how the strategy changes if you stipulate that the training set has a fixed lookback:</p>
    <figure class="mediaobject"><img src="../Images/B17574_06_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.5: Training and validation splits are moving over time</p>
    <p class="normal">In our experience, going by a fixed lookback helps to provide a fairer evaluation of time series models since <a id="_idIndexMarker464"/>you are always counting on the <a id="_idIndexMarker465"/>same training set size. </p>
    <p class="normal">By instead using a growing training set size over time, you confuse the effects of your model performance across time slices with the decreasing bias in your model (since more examples mean less bias).</p>
    <p class="normal">Finally, remember that <code class="inlineCode">TimeSeriesSplit</code> can be set to keep a pre-defined gap between your training and test time. This is extremely useful when you are told that the test set is a certain amount <a id="_idIndexMarker466"/>of time in the <a id="_idIndexMarker467"/>future (for instance, a month after the training data) and you want to test if your model is robust enough to predict that far into the future.</p>
    <h4 class="heading-4">Nested cross-validation</h4>
    <p class="normal">At this point, it is important to introduce <strong class="keyWord">nested cross-validation</strong>. Up to now, we have only discussed testing <a id="_idIndexMarker468"/>models with respect to their final performance, but often you also need to test their intermediate performance when tuning their hyperparameters. In fact, you cannot test how certain model parameters work on your test set and then use the same data in order to evaluate the final performance. Since you have specifically found the best parameters that work on the test set, your evaluation measure on the same test set will be too optimistic; on a different test set, you will probably not obtain the exact same result. In this case, you have to distinguish between a <strong class="keyWord">validation set</strong>, which is used to evaluate the performance of various models <a id="_idIndexMarker469"/>and hyperparameters, and a <strong class="keyWord">test set</strong>, which will help you to estimate <a id="_idIndexMarker470"/>the final performance of the model.</p>
    <p class="normal">If you are using a test-train split, this is achieved by splitting the test part into two new parts. The usual split is 70/20/10 for training, validation, and testing, respectively (but you can decide differently). If you are using cross-validation, you need nested cross-validation; that is, you do cross-validation based on the split of another cross-validation. Essentially, you run your usual cross-validation, but when you have to evaluate different models or different parameters, you run cross-validation based on the fold split.</p>
    <p class="normal">The example in <em class="italic">Figure 6.6</em> demonstrates this internal and external cross-validation structure. Within the external part, you determine the portion of the data used to test your evaluation metric. Within the internal part, which is fed by the training data from the external part, you arrange training/validation splits in order to evaluate and optimize specific model choices, such as deciding which model or hyperparameter values to pick:</p>
    <figure class="mediaobject"><img src="../Images/B17574_06_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.6: How nested cross-validation is structured in an external and an internal loop</p>
    <p class="normal">This approach has the advantage of making your test and parameter search fully reliable, but in doing so you incur a couple of problems:</p>
    <ul>
      <li class="bulletList">A reduced training set, since you first split by cross-validation, and then you split again</li>
      <li class="bulletList">More importantly, it requires a huge amount of model building: if you run two nested 10-fold cross-validations, you’ll need to run 100 models</li>
    </ul>
    <p class="normal">Especially for this last reason, some Kagglers tend to ignore nested cross-validation and risk some adaptive fitting by using the same cross-validation for both model/parameter search and performance <a id="_idIndexMarker471"/>evaluation, or using a fixed test sample for the final evaluation. In our experience, this approach can work as well, though it may result in overestimating model performance and overfitting if you are generating out-of-fold predictions to be used for successive modeling (something we are going to discuss in the next section). We always suggest you try the most suitable methodology for testing your models. If your aim is to correctly estimate your model’s performance and reuse its predictions in other models, remember that using nested cross-validation, whenever <a id="_idIndexMarker472"/>possible, can provide you with a less overfitting solution and could make the difference in certain competitions.</p>
    <h4 class="heading-4">Producing out-of-fold predictions (OOF)</h4>
    <p class="normal">An interesting application <a id="_idIndexMarker473"/>of cross-validation, besides estimating your evaluation metric performance, is producing test <a id="_idIndexMarker474"/>predictions and out-of-fold predictions. In fact, as you train on portions of your training data and predict on the remaining ones, you can:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Predict on the test set</strong>: The average of all the predictions is often more effective <a id="_idIndexMarker475"/>than re-training the same model on all the data: this is an ensembling technique related to blending, which will be dealt with in <em class="chapterRef">Chapter 9</em>, <em class="italic">Ensembling with Blending and Stacking Solutions</em>.</li>
      <li class="bulletList"><strong class="keyWord">Predict on the validation set</strong>: In the end, you will have predictions for the entire training <a id="_idIndexMarker476"/>set and can re-order them in the same order <a id="_idIndexMarker477"/>as the original training data. These predictions are commonly referred to as <strong class="keyWord">out-of-fold </strong>(<strong class="keyWord">OOF</strong>) <strong class="keyWord">predictions </strong>and they can be extremely useful.</li>
    </ul>
    <p class="normal">The first use of OOF predictions is to estimate your performance since you can compute your evaluation metric directly on the OOF predictions. The performance obtained is different from the cross-validated estimates (based on sampling); it doesn’t have the same probabilistic characteristics, so it is not a valid way to measure generalization performance, but it can inform you about the performance of your model on the specific set you are training on.</p>
    <p class="normal">A second use is to produce a plot and visualize the predictions against the ground truth values or against other predictions obtained from different models. This will help you in understanding how each model works and if their predictions are correlated.</p>
    <p class="normal">The last use is to create meta-features or meta-predictors. This will also be fully explored in <em class="chapterRef">Chapter 9</em>, but it is important to remark on now, as OOF predictions are a byproduct of cross-validation and they work because, during cross-validation, your model is always predicting on examples that it has not seen during training time. </p>
    <p class="normal">Since every prediction in your OOF predictions has been generated by a model trained on different data, these predictions are unbiased and you can use them without any fear of overfitting (though there are some caveats that will be discussed in the next chapter).</p>
    <p class="normal">Generating OOF predictions can be done in two ways:</p>
    <ul>
      <li class="bulletList">By coding a procedure that stores the validation predictions into a prediction <a id="_idIndexMarker478"/>vector, taking care to arrange them in the same index position <a id="_idIndexMarker479"/>as the examples in the training data</li>
      <li class="bulletList">By using <a id="_idIndexMarker480"/>the Scikit-learn function <code class="inlineCode">cross_val_predict</code>, which will automatically generate the OOF predictions for you</li>
    </ul>
    <p class="normal">We will be seeing this second technique in action when we look at adversarial validation later in this chapter.</p>
    <h3 id="_idParaDest-102" class="heading-3">Subsampling</h3>
    <p class="normal">There are other validation strategies aside from <em class="italic">k</em>-fold cross-validation, but they do not have the same generalization <a id="_idIndexMarker481"/>properties. We have already discussed LOO, which is the case when <em class="italic">k = n</em> (where <em class="italic">n</em> is the number of examples). Another choice is <strong class="keyWord">subsampling</strong>. Subsampling is similar to <em class="italic">k</em>-fold, but you do not have fixed folds; you use as many as you think are necessary (in other words, take an educated guess). You repetitively subsample your data, each time using the data that you sampled as training data and the data that has been left unsampled for your validation. By averaging the evaluation metrics of all the subsamples, you will get a validation estimate of the performances of your model.</p>
    <p class="normal">Since you are systematically testing all your examples, as in <em class="italic">k</em>-fold, you actually need quite a lot of trials to have a good chance of testing all of them. For the same reason, some cases may be tested more than others if you do not apply enough subsamples. You can run <a id="_idIndexMarker482"/>this sort of validation using <code class="inlineCode">ShuffleSplit</code> from Scikit-learn.</p>
    <h3 id="_idParaDest-103" class="heading-3">The bootstrap</h3>
    <p class="normal">Finally, another option is to try the <strong class="keyWord">bootstrap</strong>, which has been devised in statistics to conclude the error <a id="_idIndexMarker483"/>distribution of an estimate; for the same reasons, it can be used for performance estimation. The bootstrap requires you to draw a sample, <em class="italic">with replacement</em>, that is the same size as the available data. </p>
    <p class="normal">At this point, you can use the bootstrap in two different ways:</p>
    <ul>
      <li class="bulletList">As in statistics, you can bootstrap multiple times, train your model on the samples, and compute your evaluation metric on the training data itself. The average of the bootstraps will provide your final evaluation.</li>
      <li class="bulletList">Otherwise, as in subsampling, you can use the bootstrapped sample for your training and what is left not sampled from the data as your test set.</li>
    </ul>
    <p class="normal">In our experience, the first method of calculating the evaluation metric on the bootstrapped training data, often used in statistics for linear models in order to estimate the value <a id="_idIndexMarker484"/>of the model’s coefficients and their error distributions, is much less useful in machine learning. This is because many machine learning algorithms tend to overfit the training data, hence you can never have a valid metric evaluation on your training data, even if you bootstrap it. For this reason, Efron and Tibshirani (see <em class="italic">Efron</em>, <em class="italic">B. and Tibshirani, R</em>. <em class="italic">Improvements on cross-validation: the 632+ bootstrap method.</em> Journal of the American Statistical Association 92.438 (1997): 548-560.) proposed <strong class="keyWord">the 632+ estimator</strong> as a final validation metric.</p>
    <p class="normal">At first, they proposed <a id="_idIndexMarker485"/>a simple version, called the 632 bootstrap:</p>
    <p class="center"><img src="../Images/B17574_06_001.png" alt="" style="height: 1.75em !important; vertical-align: -0.30em !important;"/></p>
    <p class="normal">In this formula, given your evaluation metric <em class="italic">err</em>, <em class="italic">err</em><sub class="subscript-italic" style="font-style: italic;">fit</sub> is your metric computed on the training data and <em class="italic">err</em><sub class="subscript-italic" style="font-style: italic;">bootstrap</sub> is the metric computed on the bootstrapped data. However, in the case of an overfitted training model, <em class="italic">err</em><sub class="subscript-italic" style="font-style: italic;">fit</sub> would tend to zero, rendering the estimator not very useful. Therefore, they developed a second version of the 632+ bootstrap:</p>
    <p class="center"><img src="../Images/B17574_06_002.png" alt="" style="height: 1.75em !important; vertical-align: -0.30em !important;"/></p>
    <p class="normal">Where <em class="italic">w</em> is:</p>
    <p class="center"><img src="../Images/B17574_06_003.png" alt="" style="height: 2.5em !important; vertical-align: -0.30em !important;"/></p>
    <p class="center"><img src="../Images/B17574_06_004.png" alt="" style="height: 2.75em !important; vertical-align: -0.30em !important;"/></p>
    <p class="normal">Here you have a new parameter, <img src="../Images/B17574_06_005.png" alt=""/>, which is the <strong class="keyWord">no-information error rate</strong>, estimated by evaluating the <a id="_idIndexMarker486"/>prediction model on all possible combinations of targets and predictors. Calculating <img src="../Images/B17574_06_005.png" alt=""/> is indeed intractable, as discussed by the developers of Scikit-learn (<a href="https://github.com/scikit-learn/scikit-learn/issues/9153"><span class="url">https://github.com/scikit-learn/scikit-learn/issues/9153</span></a>).</p>
    <p class="normal">Given the limits and intractability of using the bootstrap as in classical statistics for machine learning applications, you can instead use the second method, getting your evaluation from the examples left not sampled by the bootstrap. </p>
    <p class="normal">In this form, the bootstrap is an alternative to cross-validation, but as with subsampling, it requires building many more models and testing them than for cross-validation. However, it makes sense to know about such alternatives in case your cross-validation is showing too high a variance in the evaluation metric and you need more intensive checking through testing and re-testing.</p>
    <p class="normal">Previously, this method has been implemented in Scikit-learn (<a href="https://github.com/scikit-learn/scikit-learn/blob/0.16.X/sklearn/cross_validation.py#L613"><span class="url">https://github.com/scikit-learn/scikit-learn/blob/0.16.X/sklearn/cross_validation.py#L613</span></a>) but was then removed. Since you cannot find the bootstrap anymore on Scikit-learn and it bootstrapped even the test data, you can use our own implementation. Here is our example:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> random
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">Bootstrap</span><span class="hljs-function">(</span><span class="hljs-params">n, n_iter=</span><span class="hljs-number">3</span><span class="hljs-params">, random_state=</span><span class="hljs-literal">None</span><span class="hljs-function">):</span>
    <span class="hljs-string">"""</span>
<span class="hljs-string">    Random sampling with replacement cross-validation generator.</span>
<span class="hljs-string">    For each iter a sample bootstrap of the indexes [0, n) is</span>
<span class="hljs-string">    generated and the function returns the obtained sample</span>
<span class="hljs-string">    and a list of all the excluded indexes.</span>
<span class="hljs-string">    """</span>
    <span class="hljs-keyword">if</span> random_state:
        random.seed(random_state)
    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_iter):
        bs = [random.randint(<span class="hljs-number">0</span>, n-<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]
        out_bs = <span class="hljs-built_in">list</span>({i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)} - <span class="hljs-built_in">set</span>(bs))
        <span class="hljs-keyword">yield</span> bs, out_bs
</code></pre>
    <p class="normal">In conclusion, the bootstrap is indeed an alternative to cross-validation. It is certainly more widely used in statistics <a id="_idIndexMarker487"/>and finance. In machine learning, the golden rule is to use the <em class="italic">k</em>-fold cross-validation approach. However, we suggest not forgetting about the bootstrap in all those situations where, due to outliers or a few examples that are too heterogeneous, you have a large standard error of the evaluation metric in cross-validation. In these cases, the bootstrap will prove much more useful in validating your models.</p>
    <div class="interviewBox">
      <div class="intervieweePhoto">
        <img src="../Images/Ryan_Chesler.png" alt=""/>
      </div>
      <p class="intervieweeName">Ryan Chesler</p>
      <p class="normal"><a href="https://www.kaggle.com/ryches"><span class="url">https://www.kaggle.com/ryches</span></a></p>
      <p class="normal">Our second interview of the chapter is with Ryan Chesler, a Discussions Grandmaster and Notebooks and Competitions Master. He is a Data Scientist at H2O.ai and one of the organizers of the San Diego Machine Learning group on Meetup (<a href="https://www.meetup.com/San-Diego-Machine-Learning/"><span class="url">https://www.meetup.com/San-Diego-Machine-Learning/</span></a>). The importance of validation came up in a few of his answers.</p>
      <p class="interviewHeader">What’s your favourite kind of competition and why? In terms of techniques and solving approaches, what is your specialty on Kaggle?</p>
      <p class="normal"><em class="italic">I tend to dabble in all kinds of competitions. It is more interesting to sample varied problems than specialize in a specific niche like computer vision or natural language processing. The ones I find most interesting are the ones where there are deep insights that can be derived from the data and error of predictions. For me, error analysis is one of the most illuminating processes; understanding where the model is failing and trying to find some way to improve the model or input data representation to address the weakness.</em></p>
      <p class="interviewHeader">How do you approach a Kaggle competition? How different is this approach to what you do in your day-to-day work?</p>
      <p class="normal"><em class="italic">My approach is similar in both cases. Many people seem to favor exploratory data analysis before any modeling efforts, but I find that the process of preparing the data for modeling is usually sufficient. My typical approach is to manually view the data and make some preliminary decisions about how I think I can best model the data and different options to explore. After this, I build the model and evaluate performance, and then focus on analysing errors and reason about the next modeling steps based on where I see the model making errors.</em></p>
      <p class="interviewHeader">Has Kaggle helped you in your career? If so, how?</p>
      <p class="normal"><em class="italic">Yes, it helped me get my current job. I work at H2O and they greatly value Kaggle achievements. My previous job also liked that I performed well in competitions.</em></p>
      <p class="interviewHeader">You are also the organizer of a meetup in San Diego with over two thousand participants. Is this related to your experience with Kaggle?</p>
      <p class="normal"><em class="italic">Yes, it is absolutely related. I started from very little knowledge and tried out a Kaggle competition without much success at first. I went to a local meetup and found people to team up with and learn from. At the time, I got to work with people of a much higher skill level than me and we did really well in a competition, 3rd/4500+ teams. </em></p>

      <p class="normal"><em class="italic">After this, the group stopped being as consistent and I wanted to keep the community going, so I made my own group and started organizing my own events. I’ve been doing that for almost 4 years and I get to be on the opposite side of the table teaching people and helping them get started. We originally just focused on Kaggle competitions and trying to form teams, but have slowly started branching off to doing book clubs and lectures on various topics of interest. I attribute a lot of my success to having this dedicated weekly time to study and think about machine learning.</em></p>
      <p class="interviewHeader">In your experience, what do inexperienced Kagglers often overlook? What do you know now that you wish you’d known when you first started?</p>
      <p class="normal"><em class="italic">In my experience, a lot of people overstate the importance of bias-variance trade-off and overfitting. This is something I have seen people consistently worry about too much. The focus should not be making training and validation performance close, but make validation performance as good as possible.</em></p>
      <p class="interviewHeader">What mistakes have you made in competitions in the past?</p>
      <p class="normal"><em class="italic">My consistent mistake is not exploring enough. Sometimes I have ideas that I discount too early and turn out to be important for improving performance. Very often I can get close to competitive performance on the first try, but iterating and continuing to improve as I try new things takes a slightly different skill that I am still working on mastering.</em></p>
      <p class="interviewHeader">Are there any particular tools or libraries that you would recommend using for data analysis or machine learning?</p>
      <p class="normal"><em class="italic">I use a lot of the standard tools: XGBoost, LightGBM, Pytorch, TensorFlow, Scikit-learn. I don’t have any strong affinity for a specific tool or library, just whatever is relevant to the problem.</em></p>
      <p class="interviewHeader">What’s the most important thing someone should keep in mind or do when they’re entering a competition?</p>
      <p class="normal"><em class="italic">I think the most important thing people have to keep in mind is good validation. Very often I see people fooling themselves thinking their performance is improving but then submitting to the leaderboard and realizing it didn’t actually go how they expected. It is an important skill to understand how to match assumptions with your new unseen data and build a model that is robust to new conditions.</em></p>
    </div>
    <h1 id="_idParaDest-104" class="heading-1">Tuning your model validation system</h1>
    <p class="normal">At this point, you should have a complete overview of all possible validation strategies. When you approach <a id="_idIndexMarker488"/>a competition, you devise your validation strategy and you implement it. Then, you test if the strategy you have chosen is correct.</p>
    <p class="normal">As a golden rule, be guided in devising your validation strategy by the idea that you have to replicate the same approach used by the organizers of the competition to split the data into training, private, and public test sets. Ask yourself how the organizers have arranged those splits. Did they draw a random sample? Did they try to preserve some specific distribution in the data? Are the test sets actually drawn from the same distribution as the training data?</p>
    <p class="normal">These are not the questions you would ask yourself in a real-world project. Contrary to a real-world project where you have to generalize at all costs, a competition has a much narrower focus on having a model that performs on the given test set (especially the private one). If you focus on this idea from the beginning, you will have more of a chance of finding out the best validation strategy, which will help you rank more highly in the competition.</p>
    <p class="normal">Since this is a trial-and-error process, as you try to find the best validation strategy for the competition, you can systematically apply the following two consistency checks in order to figure out if you are on the right path:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">First, you have to check if your local tests are consistent, that is, that the single cross-validation fold errors are not so different from each other or, when you opt for a simple train-test split, that the same results are reproducible using different train-test splits.</li>
      <li class="numberedList">Then, you have to check if your local validation error is consistent with the results on the public leaderboard.</li>
    </ol>
    <p class="normal">If you’re failing the first check, you have a few options depending on the following possible origins of the problem:</p>
    <ul>
      <li class="bulletList">You don’t have much training data</li>
      <li class="bulletList">The data is too diverse and every training partition is very different from every other (for instance, if you have too many <strong class="keyWord">high cardinality</strong> features, that is, features with too many levels – like zip codes – or if you have multivariate outliers)</li>
    </ul>
    <p class="normal">In both cases, the point is that you lack data with respect to the model you want to implement. Even when the problem just appears to be that the data is too diverse, plotting learning curves <a id="_idIndexMarker489"/>will make it evident to you that your model needs more data. </p>
    <p class="normal">In this case, unless you find out that moving to a simpler algorithm works on the evaluation metric (in which case trading variance for bias may worsen your model’s performance, but not always), your best choice is to use an extensive validation approach. This can be implemented by:</p>
    <ul>
      <li class="bulletList">Using larger <em class="italic">k</em> values (thus approaching LOO where <em class="italic">k = n</em>). Your validation results will be less about the capability of your model to perform on unseen data, but by using larger training portions, you will have the advantage of more stable evaluations.</li>
      <li class="bulletList">Averaging the results of multiple <em class="italic">k</em>-fold validations (based on different data partitions picked by different random seed initializations).</li>
      <li class="bulletList">Using repetitive bootstrapping.</li>
    </ul>
    <p class="normal">Keep in mind that when you find unstable local validation results, you won’t be the only one to suffer from the problem. Usually, this is a common problem due to the data’s origin and characteristics. By keeping tuned in to the discussion forums, you may get hints at possible solutions. For instance, a good solution for high cardinality features is target encoding; stratification can help with outliers; and so on.</p>
    <p class="normal">The situation is different when you’ve passed the first check but failed the second; your local cross-validation is consistent but you find that it doesn’t hold on the leaderboard. In order to realize this problem exists, you have to keep diligent note of all your experiments, validation test types, random seeds used, and leaderboard results if you submitted the resulting predictions. In this way, you can draw a simple scatterplot and try fitting a linear regression or, even simpler, compute a correlation between your local results and the associated public leaderboard scores. It costs some time and patience to annotate and analyze all of these, but it is the most important meta-analysis of your competition performances that you can keep track of.</p>
    <p class="normal">When the mismatch is because your validation score is systematically lower or higher than the leaderboard score, you actually have a strong signal that something is missing from your validation strategy, but this problem does not prevent you from improving your model. In fact, you can keep on working on your model and expect improvements to be reflected on the leaderboard, though not in a proportional way. However, systematic differences are always a red flag, implying something is different between what you are doing and what the organizers have arranged for testing the model.</p>
    <p class="normal">An even worse scenario occurs when your local cross-validation scores do not correlate at all with the leaderboard feedback. This is really a red flag. When you realize this is the case, you should <a id="_idIndexMarker490"/>immediately run a series of tests and investigations in order to figure out why, because, regardless of whether it is a common problem or not, the situation poses a serious threat to your final rankings. There are a few possibilities in such a scenario:</p>
    <ul>
      <li class="bulletList">You figure out that the test set is drawn from a different distribution to the training set. The adversarial validation test (that we will discuss in the next section) is the method that can enlighten you in such a situation.</li>
      <li class="bulletList">The data is non-i.i.d. but this is not explicit. For instance, in <em class="italic">The Nature Conservancy Fisheries Monitoring</em> competition (<a href="https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring"><span class="url">https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring</span></a>), images in the training set were taken from similar situations (fishing boats). You had to figure out by yourself how to arrange them in order to avoid the model learning to identify the target rather than the context of the images (see, for instance, this work by <em class="italic">Anokas</em>: <a href="https://www.kaggle.com/anokas/finding-boatids"><span class="url">https://www.kaggle.com/anokas/finding-boatids</span></a>).</li>
      <li class="bulletList">The multivariate distribution of the features is the same, but some groups are distributed differently in the test set. If you can figure out the differences, you can set your training set and your validation accordingly and gain an edge. You need to probe the public leaderboard to work this out.</li>
      <li class="bulletList">The test data is drifted or trended, which is usually the case in time series predictions. Again, you need to probe the public leaderboard to get some insight about some possible post-processing that could help your score, for instance, applying a multiplier to your predictions, thus mimicking a decreasing or increasing trend in the test data.</li>
    </ul>
    <p class="normal">As we’ve discussed before, probing the leaderboard is the act of making specifically devised submissions in order to get insights about the composition of the public test set. It works particularly <a id="_idIndexMarker491"/>well if the private test set is similar to the public one. There are no general methods for probing, so you have to devise a probing methodology according to the type of competition and problem.</p>
    <p class="normal">For instance, in the paper <em class="italic">Climbing the Kaggle Leaderboard by Exploiting the Log-Loss Oracle</em> (<a href="https://export.arxiv.org/pdf/1707.01825"><span class="url">https://export.arxiv.org/pdf/1707.01825</span></a>), Jacob explains how to get fourth position in a competition without even downloading the training data.</p>
    <p class="normal">With regard to regression problems, in the recent <em class="italic">30 Days of ML</em> organized by Kaggle, <em class="italic">Hung Khoi</em> explained how probing the leaderboard helped him to understand the differences in the mean and standard deviation of the target column between the training dataset and the public test data (see: <a href="https://www.kaggle.com/c/30-days-of-ml/discussion/269541"><span class="url">https://www.kaggle.com/c/30-days-of-ml/discussion/269541</span></a>). </p>
    <p class="normal">He used the following equation:</p>
    <p class="center"><img src="../Images/B17574_06_007.png" alt="" style="height: 1.5em !important; vertical-align: -0.30em !important;"/></p>
    <p class="normal">Essentially, you need just two submissions to solve for the mean and variance of the test target, since there are two unknown terms – variance and mean.</p>
    <p class="normal">You can also get some other ideas about leaderboard probing from <em class="italic">Chris Deotte</em> (<a href="https://www.kaggle.com/cdeotte"><span class="url">https://www.kaggle.com/cdeotte</span></a>) from this post, <a href="https://www.kaggle.com/cdeotte/lb-probing-strategies-0-890-2nd-place"><span class="url">https://www.kaggle.com/cdeotte/lb-probing-strategies-0-890-2nd-place</span></a>, relevant to the <em class="italic">Don’t Overfit II competition</em> (<a href="https://www.kaggle.com/c/dont-overfit-ii"><span class="url">https://www.kaggle.com/c/dont-overfit-ii</span></a>).</p>
    <div class="note">
      <p class="normal">If you want to get a feeling about how probing information from the leaderboard is a double-edged sword, you can read about how <em class="italic">Zahar Chikishev</em> managed to probe information from the <em class="italic">LANL Earthquake Prediction</em> competition, ending up in 87<sup class="superscript">th</sup> place in the private leaderboard after leading in the public one: <a href="https://towardsdatascience.com/how-to-lb-probe-on-kaggle-c0aa21458bfe"><span class="url">https://towardsdatascience.com/how-to-lb-probe-on-kaggle-c0aa21458bfe</span></a></p>
    </div>
    <h1 id="_idParaDest-105" class="heading-1">Using adversarial validation</h1>
    <p class="normal">As we have discussed, cross-validation allows you to test your model’s ability to generalize to unseen datasets coming from the same distribution as your training data. Hopefully, since in a <a id="_idIndexMarker492"/>Kaggle competition you are asked to create a model that can predict on the public and private datasets, you should expect that such test data is from the same distribution as the training data. In reality, this is not always the case.</p>
    <p class="normal">Even if you do not overfit to the test data because you have based your decision not only on the leaderboard results but also considered your cross-validation, you may still be surprised by the results. This could happen in the event that the test set is even slightly different from the training set on which you have based your model. In fact, the target probability and its distribution, as well as how the predictive variables relate to it, inform your model during training about certain expectations that cannot be satisfied if the test data is different from the training data.</p>
    <p class="normal">Hence, it is not enough to avoid overfitting to the leaderboard as we have discussed up to now, but, in the first place, it is also advisable to find out if your test data is comparable to the training data. Then, if they differ, you have to figure out if there is any chance that you can mitigate the different distributions between training and test data and build a model that performs on that test set.</p>
    <p class="normal"><strong class="keyWord">Adversarial validation</strong> has been developed just for this purpose. It is a technique allowing you to easily estimate the degree of difference between your training and test data. This technique was <a id="_idIndexMarker493"/>long rumored among Kaggle participants and transmitted from team to team until it emerged publicly thanks to a post by <em class="italic">Zygmunt Zając</em> (<a href="https://www.kaggle.com/zygmunt"><span class="url">https://www.kaggle.com/zygmunt</span></a>) on his FastML blog.</p>
    <p class="normal">The idea is simple: take your training data, remove the target, assemble your training data together with your test data, and create a new binary classification target where the positive label is assigned to the test data. At this point, run a machine learning classifier and evaluate for the ROC-AUC evaluation metric (we discussed this metric in the previous chapter on <em class="italic">Detailing Competition Tasks and Metrics</em>).</p>
    <p class="normal">If your ROC-AUC is around 0.5, it means that the training and test data are not easily distinguishable and are apparently from the same distribution. ROC-AUC values higher than 0.5 and nearing 1.0 signal that it is easy for the algorithm to figure out what is from the training set and what is from the test set: in such a case, don’t expect to be able to easily generalize to the test set because it clearly comes from a different distribution.</p>
    <div class="note">
      <p class="normal">You can find an example Notebook written for the <em class="italic">Sberbank Russian Housing Market</em> competition (<a href="https://www.kaggle.com/c/sberbank-russian-housing-market"><span class="url">https://www.kaggle.com/c/sberbank-russian-housing-market</span></a>) that demonstrates a practical example of adversarial validation and its usage in a competition here: <a href="https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms"><span class="url">https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms</span></a>.</p>
    </div>
    <p class="normal">Since your data may be of different types (numeric or string labels) and you may have missing cases, you’ll <a id="_idIndexMarker494"/>need some data processing before being able to successfully run the classifier. Our suggestion is to use the random forest classifier because:</p>
    <ul>
      <li class="bulletList">It doesn’t output true probabilities but its results are intended as simply ordinal, which is a perfect fit for an ROC-AUC score.</li>
      <li class="bulletList">The random forest is a flexible algorithm based on decision trees that can do feature selection by itself and operate on different types of features without any pre-processing, while rendering all the data numeric. It is also quite robust to overfitting and you don’t have to think too much about fixing its hyperparameters.</li>
      <li class="bulletList">You don’t need much data processing because of its tree-based nature. For missing data, you can simply replace the values with an improbable negative value such as -999, and you can deal with string variables by converting their strings into numbers (for instance, using the Scikit-learn label encoder, <code class="inlineCode">sklearn.preprocessing.LabelEncoder</code>). As a solution, it performs less well than one-hot encoding, but it is very speedy and it will work properly for the problem.</li>
    </ul>
    <p class="normal">Although building a classification model is the most direct way to adversarially validate your test set, you can also use other approaches. One approach is to map both training and test data into a lower-dimensional space, as in this post (<a href="https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data"><span class="url">https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data</span></a>) by <em class="italic">NanoMathias</em> (<a href="https://www.kaggle.com/nanomathias"><span class="url">https://www.kaggle.com/nanomathias</span></a>). Although requiring more tuning work, such an approach based on t-SNE and PCA has the great advantage of being graphically representable in an appealing and understandable way. </p>
    <p class="normal">Don’t forget that our brains are more adept at spotting patterns in visual representations than numeric ones (for an articulate discussion about our visual abilities, see <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/qua.24480"><span class="url">https://onlinelibrary.wiley.com/doi/full/10.1002/qua.24480</span></a>).</p>
    <div class="note">
      <p class="normal">PCA and t-SNE are not the only tools that can help you to reduce the dimensionality of your data and allow you to visualize it. UMAP (<a href="https://github.com/lmcinnes/umap"><span class="url">https://github.com/lmcinnes/umap</span></a>) can often provide a faster low dimensionality solution with clear and distinct data clusters. Variational auto-encoders (discussed in <em class="chapterRef">Chapter 7</em>, <em class="italic">Modeling for Tabular Competitions</em>) can instead deal with <a id="_idIndexMarker495"/>non-linear dimensionality reduction and offer a more useful representation than PCA; they are more complicated to set up and tune, however.</p>
    </div>
    <h2 id="_idParaDest-106" class="heading-2">Example implementation</h2>
    <p class="normal">While you can find <a id="_idIndexMarker496"/>examples of adversarial validation in the original article by Zygmunt and the Notebook we linked, we have created a fresh example for you, based on the Playground competition <em class="italic">Tabular Playground Series – Jan 2021</em> (<a href="https://www.kaggle.com/c/tabular-playground-series-jan-2021"><span class="url">https://www.kaggle.com/c/tabular-playground-series-jan-2021</span></a>).</p>
    <p class="normal">You start by importing some Python packages and getting the training and test data from the competition:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_predict
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_auc_score
train = pd.read_csv(<span class="hljs-string">"../input/tabular-playground-series-jan-2021/train.csv"</span>)
test = pd.read_csv(<span class="hljs-string">"../input/tabular-playground-series-jan-2021/test.csv"</span>)
</code></pre>
    <p class="normal">Data preparation is short and to the point. Since all features are numeric, you won’t need any label encoding, but you do have to fill any missing values with a negative number (-1 usually works fine), and drop the target and also any identifiers; when the identifier is progressive, the adversarial validation may return a high ROC-AUC score:</p>
    <pre class="programlisting code"><code class="hljs-code">train = train.fillna(<span class="hljs-number">-1</span>).drop(["<span class="hljs-built_in">id</span>", "target"], axis=<span class="hljs-number">1</span>)
test = test.fillna(<span class="hljs-number">-1</span>).drop(["<span class="hljs-built_in">id</span>", axis=<span class="hljs-number">1</span>])
X = train.append(test)
y = [<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(train) + [<span class="hljs-number">1</span>] * <span class="hljs-built_in">len</span>(test)
</code></pre>
    <p class="normal">At this point, you just need to generate <code class="inlineCode">RandomForestClassifier</code> predictions for your data using the <code class="inlineCode">cross_val_predict</code> function, which automatically creates a cross-validation scheme and stores the predictions on the validation fold:</p>
    <pre class="programlisting code"><code class="hljs-code">model = RandomForestClassifier()
cv_preds = cross_val_predict(model, X, y, cv=<span class="hljs-number">5</span>, n_jobs=<span class="hljs-number">-1</span>, method='predict_proba')
</code></pre>
    <p class="normal">As a result, you obtain predictions that are unbiased (they are not overfit as you did not predict on what you trained) and that can be used for error estimation. Please note that <code class="inlineCode">cross_val_predict</code> won’t fit your instantiated model, so you won’t get any information from it, such as what the important features used by the model are. If you need such information, you just need to fit it first by calling <code class="inlineCode">model.fit(X, y)</code>.</p>
    <p class="normal">Finally, you can query the ROC-AUC score for your predictions:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(roc_auc_score(y_true=y, y_score=cv_preds[:,<span class="hljs-number">1</span>]))
</code></pre>
    <p class="normal">You should obtain a value of around 0.49-0.50 (<code class="inlineCode">cross_val_predict</code> won’t be deterministic <a id="_idIndexMarker497"/>unless you use cross-validation with a fixed <code class="inlineCode">random_seed</code>). This means that you cannot easily distinguish training from test data. Hence, they come from the same distribution.</p>
    <h2 id="_idParaDest-107" class="heading-2">Handling different distributions of training and test data</h2>
    <p class="normal">ROC-AUC scores of 0.8 or more would alert you that the test set is peculiar and quite distinguishable <a id="_idIndexMarker498"/>from the training data. In these cases, what <a id="_idIndexMarker499"/>can you do? You actually have a few strategies at hand:</p>
    <ul>
      <li class="bulletList">Suppression</li>
      <li class="bulletList">Training on cases most similar to the test set</li>
      <li class="bulletList">Validating by mimicking the test set</li>
    </ul>
    <p class="normal">With <strong class="keyWord">suppression</strong>, you remove the variables that most influence the result in the adversarial test set until the distributions are the same again. To do so, you require an iterative approach. This time, you fit your model to all your data, and then you check the importance measures (provided, for instance, by the <code class="inlineCode">feature_importances_</code> method in the Scikit-learn <code class="inlineCode">RandomForest</code> classifier) and the ROC-AUC fit score. At this point, you remove the most important variable for the model from your data and run everything again. You repeat this cycle where you train, measure the ROC-AUC fit, and drop the most important variable from your data until the fitted ROC-AUC score decreases to around 0.5. </p>
    <p class="normal">The only problem with this method is that you may actually be forced to remove the majority of important variables from your data, and any model you then build on such variable censored data won’t be able to predict sufficiently correctly due to the lack of informative features.</p>
    <p class="normal">When you <strong class="keyWord">train on the examples most similar to the test set</strong>, you instead take a different approach, focusing not on the <em class="italic">variables</em> but on the <em class="italic">samples</em> you are using for training. In this case, you pick up from the training set only the samples that fit the test distribution. Any trained model then suits the testing distribution (but it won’t be generalizable to anything else), which should allow you to test the best on the competition problem. The limitation <a id="_idIndexMarker500"/>of this approach is that you <a id="_idIndexMarker501"/>are cutting down the size of your dataset and, depending on the number of samples that fit the test distribution, you may suffer from a very biased resulting model due to the lack of training examples. In our previous example, picking up just the adversarial predictions on the training data that exceed a probability of 0.5 and summing them results in picking only 1,495 cases (the number is so small because the test set is not very different from the training set):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(np.<span class="hljs-built_in">sum</span>(cv_preds[:<span class="hljs-built_in">len</span>(X), <span class="hljs-number">1</span>] &gt; <span class="hljs-number">0.5</span>))
</code></pre>
    <p class="normal">Finally, with the strategy of <strong class="keyWord">validating by mimicking the test set</strong>, you keep on training on all the data, but for validation purposes, you pick your examples only from the adversarial predictions on the training set that exceed a probability of 0.5 (or an even higher threshold such as 0.9). </p>
    <p class="normal">Having a validation set tuned to the test set will allow you to pick all the possible hyperparameters and model choices that will favor a better result on the leaderboard.</p>
    <p class="normal">In our example, we can figure out that <code class="inlineCode">feature_19</code> and <code class="inlineCode">feature_54</code> appear the most different between the training/test split from the output of the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">model.fit(X, y)
ranks = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(X.columns, model.feature_importances_)), 
               key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">for</span> feature, score <span class="hljs-keyword">in</span> ranks:
    <span class="hljs-built_in">print</span>(f"{feature:<span class="hljs-number">10</span>} : {score:<span class="hljs-number">0.4</span>f}")
</code></pre>
    <p class="normal">To conclude, we have a few more remarks on adversarial validation. First, using it will generally help you to perform better in competitions, but not always. Kaggle’s Code competitions, and other competitions where you cannot fully access the test set, cannot be inspected by adversarial validation. In addition, adversarial validation can inform you about the test data as a whole, but it cannot advise you on the split between the private and the public test data, which is the cause of the most common form of public leaderboard overfitting and consequent shake-up.</p>
    <p class="normal">Finally, adversarial validation, though a very specific method devised for competitions, has quite a few practical use cases in the real world: how often have you picked the wrong test set to validate <a id="_idIndexMarker502"/>your models? The method we have <a id="_idIndexMarker503"/>presented here can enlighten you about whether you are using the test data, and any validation data, in your projects properly. Moreover, data changes and models in production may be affected by such changes <a id="_idIndexMarker504"/>and produce bad predictions if you don’t retrain them. This is called <strong class="keyWord">concept drift</strong>, and by using adversarial validation, you can immediately understand if you have to retrain new models to put into production or if you can leave the previous ones in operation.</p>
    <div class="interviewBox">
      <div class="intervieweePhoto">
        <img src="../Images/Giuliano_Janson.png" alt=""/>
      </div>
      <p class="intervieweeName">Giuliano Janson</p>
      <p class="normal"><a href="https://www.kaggle.com/adjgiulio"><span class="url">https://www.kaggle.com/adjgiulio</span></a></p>
      <p class="normal">Giuliano Janson is a Competitions Grandmaster and senior applied <a id="_idIndexMarker505"/>scientist for ML and NLP at Zillow Group. He spoke to us about his competition wins, the importance of cross-validation, and data leakages, the subject of the upcoming section.</p>

      <p class="interviewHeader">What’s your favorite kind of competition and why? In terms of techniques and solving approaches, what is your specialty on Kaggle?</p>
      <p class="normal"><em class="italic">My perfect competition is made up of a) an interesting problem to solve, b) a mid-size dataset that is small enough to fit in memory but not too small to become an overfitting headache, and c) an opportunity to be creative from a feature engineering perspective. The combination of those three dimensions is where I’m at my best in competitive ML because I feel I have the means to use rigor and creativity without having to worry about engineering constraints.</em></p>
      <p class="interviewHeader">How do you approach a Kaggle competition? How different is this approach to what you do in your day-to-day work?</p>
      <p class="normal"><em class="italic">A Kaggle competition is a marathon. Going into a competition, I know I can get 90 to 95% of my best final score with a couple of days of work. The rest is a slow grind. The only success metric is your score; nothing else matters. </em></p>
      <p class="normal"><em class="italic">My daily work looks more like a series of sprints. Model performance is only a small portion of what I need to consider. A go-live date might be just as important, or other aspects such as interpretability, scalability, and maintainability could tip the scale in a totally different direction. After each sprint, priorities are reassessed and the end product might look totally different from what was originally envisioned. Also, modeling is a small part of my day. I spend far more time talking to people, managing priorities, building use cases, scrubbing data, and thinking about everything that it takes to make a prototype model a successful production solution.</em></p>
      <p class="interviewHeader">Tell us about a particularly challenging competition you entered, and what insights you used to tackle the task.</p>
      <p class="normal"><em class="italic">One of the two competitions I won, the Genentech Cancer competition, was a Masters-only competition. The data provided was raw transactional data. There was no nice tabular dataset to start from. This is the type of work I love because feature engineering is actually one of my favorite parts of ML. Since I had worked in healthcare for a decade at the time of the competition, I had business and clinical insights on the data, but most of all, I had engineering insights on the complexity of correctly handling this type of data and about all the things that can go wrong when this type of transactional raw data is not handled carefully. That turned out to be key to winning, as one of the initial hypotheses regarding a possible source of leakage turned out to be true, and provided a “golden feature” that gave the final boost to our model. The insight from the competition is to always be extra careful when doing feature engineering or setting up validation approaches. Leakage can be very hard to detect and the usual train/validation/test approach to model validation will provide no help in identifying leakage in most cases, thus putting a model at risk of underperforming in production.</em></p>

      <p class="interviewHeader">Has Kaggle helped you in your career? If so, how?</p>
      <p class="normal"><em class="italic">Kaggle has helped me in two ways. First, it provided a low barrier entry point to modern ML, a ton of exposure to cutting-edge modeling techniques, and forced me to truly understand the art and science of professional-grade model validation techniques. Second, Kaggle provided access to some of the brightest minds in applied ML. What I learned teaming up with some of the top Kaggle participants are lessons I cherish and try to share with my teammates every day.</em></p>
      <p class="interviewHeader">How have you built up your portfolio thanks to Kaggle?</p>
      <p class="normal"><em class="italic">My professional career hasn’t been directly impacted much by my Kaggle résumé. By that, I mean I haven’t got job offers or interviews as a result of my Kaggle standings. I started Kaggle when I was already in a senior data science role, albeit with not much of an ML focus. Thanks to what I learned on Kaggle, I was able to better advocate a change in my career to move into an ML-focused job.</em></p>
      <p class="normal"><em class="italic">To this date, many folks I work with enjoy chatting about competitive ML and are curious about tips and tricks from my Kaggle experience, but it is also true that a large portion of the ML community might not even know what Kaggle is.</em></p>
      <p class="interviewHeader">In your experience, what do inexperienced Kagglers often overlook? What do you know now that you wish you’d known when you first started?</p>
      <p class="normal"><em class="italic">The importance of proper cross-validation is easily overlooked by participants new to competitive ML. A solid cross-validation framework allows you to measure improvement reliably and objectively. And in a competition that might be as long as six months, the best models do not usually come from those who have the best initial ideas, but from those who are willing to iterate and adjust based on empirical feedback from the data. A great validation framework is at the foundation of it all.</em></p>
      <p class="interviewHeader">What mistakes have you made in competitions in the past?</p>
      <p class="normal"><em class="italic">One of the lessons learned that I always share with people new to ML is to “never get over-enamored with overly complex ideas.” When facing a new complex problem, it is easy to be tempted to build complex solutions. Complex solutions usually require time to develop. But the main issue is that complex solutions are often of marginal value, conditional on robust baselines. For example, imagine you want to model the outcome of an election and start thinking about a series of features to capture complex conditional relationships among observable and latent geographic, socio-economic, and temporal features. You could spend weeks developing these features, under the assumption that because they are so well thought out, they will be impactful. </em></p>

      <p class="normal"><em class="italic">The mistake is that while often those complex features could be very powerful on their own, conditional on a series of simple features and on a model that can already build highly optimized, data-driven deep interaction, all of a sudden, the complex features we built with time and effort may lead to little to no marginal improvement. My advice is to stick to Occam’s razor and try easy things before being tempted by more complex approaches.</em></p>
      <p class="interviewHeader">Are there any particular tools or libraries that you would recommend using for data analysis or machine learning?</p>
      <p class="normal"><em class="italic">I’m a pandas and Scikit-learn person. I love how pandas enables easy data manipulation and exploration and how I can quickly prototype models using Scikit-learn in a matter of minutes. Most of my prototype work is done using these two libraries. That said, my final models are often based on XGBoost. For deep learning, I love using Keras.</em></p>
    </div>
    <h1 id="_idParaDest-108" class="heading-1">Handling leakage</h1>
    <p class="normal">A common issue in Kaggle <a id="_idIndexMarker506"/>competitions that can affect the outcome of the challenge is data leakage. <strong class="keyWord">Data leakage</strong>, often mentioned simply as <strong class="keyWord">leakage</strong> or with other <a id="_idIndexMarker507"/>fancy names (such as <em class="italic">golden features</em>), involves information in the training phase that won’t be available at prediction time. The presence of such information (leakage) will make your model over-perform in training and testing, allowing you to rank highly in the competition, but will render unusable or at best suboptimal any solution based on it from the sponsor’s point of view.</p>
    <div class="note">
      <p class="normal">We can define leakage as “when information concerning the ground truth is artificially and unintentionally introduced within the training feature data, or training metadata” as stated by <em class="italic">Michael Kim</em> (<a href="https://www.kaggle.com/mikeskim"><span class="url">https://www.kaggle.com/mikeskim</span></a>) in his presentation at <em class="italic">Kaggle Days San Francisco</em> in 2019.</p>
    </div>
    <p class="normal">Leakage is often found in Kaggle competitions, despite careful checking from both the sponsor and the Kaggle team. Such situations are due to the subtle and sneaky nature of leakage, which can unexpectedly appear due to the intense searching undertaken by Kagglers, who are always looking for any way to score better in a competition.</p>
    <div class="note">
      <p class="normal">Don’t confuse data leakage with a leaky validation strategy. In a leaky validation strategy, the problem is that you have arranged your validation strategy in a way that favors better validation scores because some information leaks from the training data. It has nothing to do with the competition itself, but it relates to how you are handling your validation. It occurs if you run any pre-processing modifying your data (normalization, dimensionality reduction, missing value imputation) before separating training and validation or test data.</p>
      <p class="normal">In order to prevent leaky validation, if you are using Scikit-learn to manipulate and process your data, you absolutely have to exclude your validation data from any fitting operation. Fitting operations tend to create leakage if applied to any data you use for validation. The best way to avoid this is to use Scikit-learn pipelines (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"><span class="url">https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html</span></a>), which will enclose both your data processing and model together, thereby avoiding any risk of inadvertently applying any leaking transformation to your data.</p>
      <p class="normal">Data leakage instead is therefore something that is not strictly related to validation operations, though it affects them deeply. Even though this chapter is principally devoted to validation strategies, at this point we consider it necessary to discuss data leakage, since this issue can profoundly affect how you evaluate your models and their ability to generalize beyond the competition test sets.</p>
    </div>
    <p class="normal">Generally speaking, leakage can originate at a feature or example level. <strong class="keyWord">Feature leakage</strong> is by far the most common. It can be caused <a id="_idIndexMarker508"/>by the existence of a proxy for the target, or by a feature <a id="_idIndexMarker509"/>that is posterior to the target itself. A target proxy could be anything derived from processing the label itself or from the test split process; for instance, when defining identifiers, specific identifiers (a numeration arc, for instance) may be associated with certain target responses, making it easier for a model to guess if properly fed with the information processed in the right way. A more subtle way in which data processing can cause leakage is when the competition organizers have processed the training and test set together before splitting it. Historically, leakages in Kaggle competitions have been found in:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Mishandled data preparation from organizers, especially when they operate on a combination <a id="_idIndexMarker510"/>of training and test data (for example, in <em class="italic">Loan Default Prediction</em> (<a href="https://www.kaggle.com/c/loan-default-prediction"><span class="url">https://www.kaggle.com/c/loan-default-prediction</span></a>), organizers initially used features with aggregated historical data that leaked future information).</li>
      <li class="numberedList">Row order <a id="_idIndexMarker511"/>when it is connected to a time index or to specific data groups (for instance, in <em class="italic">Telstra Network Disruptions</em> (<a href="https://www.kaggle.com/c/telstra-recruiting-network"><span class="url">https://www.kaggle.com/c/telstra-recruiting-network</span></a>), the order of records in a feature hinted at proxy information, the location, which was not present in the data and which was very predictive).</li>
      <li class="numberedList">Column order when it is connected to a time index (you get hints by using the columns as rows).</li>
      <li class="numberedList">Feature duplication in consecutive rows because it can hint at examples with correlated responses, such as in <em class="italic">Bosch Production Line Performance</em> (see the first-place solution by <em class="italic">Beluga</em> at <a href="https://www.kaggle.com/c/bosch-production-line-performance/discussion/25434"><span class="url">https://www.kaggle.com/c/bosch-production-line-performance/discussion/25434</span></a>).</li>
      <li class="numberedList">Image metadata (as in <em class="italic">Two Sigma Connect: Rental Listing Inquiries</em> (<a href="https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries"><span class="url">https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries</span></a>)).</li>
      <li class="numberedList">Hashes or other easily crackable anonymization practices of encodings and identifiers.</li>
    </ol>
    <p class="normal">The trouble with posterior information originates from the way we deal with information when we do not consider the effects of time and of the sequence of cause and effect that spans <a id="_idIndexMarker512"/>across time. Since we are looking back at the past, we often forget that certain variables that make sense at the present moment do not have value in the past. For instance, if you have to calculate a credit score for a loan to a new company, knowing that payments of the borrowed money are often late is a great indicator of the lower reliability and higher risk represented by the debtor, but you cannot know this before you have lent out the money. This is also a problem that you will commonly find when analyzing company databases in your projects: your query data will represent present situations, not past ones. Reconstructing past information can also be a difficult task if you cannot specify that you wish to retrieve only the information that was present at a certain time. For this reason, great effort has to be spent on finding these leaking features and excluding or adjusting them before building any model.</p>
    <p class="normal">Similar problems are also common in Kaggle competitions based on the same kind of data (banking or insurance, for instance), though, since much care is put into the preparation of the data for the competition, they appear in more subtle ways and forms. In general, it is easy to spot these leaking features since they strongly correlate with the target, and a domain expert can figure out why (for instance, knowing at what stage the data is recorded in the databases). Therefore, in competitions, you never find such obvious features, but derivatives of them, often transformed or processed features that have slipped away <a id="_idIndexMarker513"/>from the control of the sponsor. Since the features are often anonymized to preserve the sponsor’s business, they end up lurking among the others. This has given rise to a series of hunts for the golden/magic features, a search to combine existing features in the dataset in order to have the leakage emerge.</p>
    <div class="note">
      <p class="normal">You can read an enlightening post by <em class="italic">Corey Levison</em> here: <a href="https://www.linkedin.com/pulse/winning-13th-place-kaggles-magic-competition-corey-levinson/"><span class="url">https://www.linkedin.com/pulse/winning-13th-place-kaggles-magic-competition-corey-levinson/</span></a>. It tells the story of how the <em class="italic">Santander Customer Transaction Prediction</em> competition turned into a hunt for magic features for his team.</p>
      <p class="normal">Another good example is provided by <em class="italic">dune_dweller</em> here: <a href="https://www.kaggle.com/c/telstra-recruiting-network/discussion/19239#109766"><span class="url">https://www.kaggle.com/c/telstra-recruiting-network/discussion/19239#109766</span></a>. By looking at how the data was ordered, dune_dweller found out that the data was likely in time order. Putting this information in a new feature increased the score.</p>
    </div>
    <p class="normal">The other way in which leakage can occur is by <strong class="keyWord">training example leakage</strong>. This happens especially with non-i.i.d. data. This means that some cases correlate between themselves because they are from the same period (or from contiguous ones) or the same group. If such cases are not all together either in the training or test data, but separated between them, there is a high chance that the machine learning algorithm will learn how to spot the cases (and derive the predictions) rather than using general rules. An often-cited example of such a situation involves the team of <em class="italic">Prof. Andrew Ng</em> (see <a href="https://twitter.com/nizkroberts/status/931121395748270080"><span class="url">https://twitter.com/nizkroberts/status/931121395748270080</span></a>). In 2017, they wrote a paper using a dataset of 100,000 x-rays from 30,000 patients. They used a random split in order to separate training and test data, not realizing that the x-rays of the same patient could end up partly in the training set and partly in the test set. Practitioners such as Nick Roberts spotted this fact, pointing out a possible leakage that could have inflated the performances of the model and that led to a substantial revision of the paper itself.</p>
    <p class="normal">What happens when there is a data leakage in a Kaggle competition? Kaggle has clear policies about it and will either:</p>
    <ul>
      <li class="bulletList">Let the competition continue as is (especially if the leakage only has a small impact)</li>
      <li class="bulletList">Remove the leakage from the set and relaunch the competition</li>
      <li class="bulletList">Generate a new test set that does not have the leakage present</li>
    </ul>
    <p class="normal">In particular, Kaggle recommends making any leakage found public, though this is not compulsory or sanctioned if it doesn’t happen. However, in our experience, if there is any leakage in a <a id="_idIndexMarker514"/>competition, it will soon become very apparent and the discussion forums will start lighting up with a discussion about magic stuff and the like. You will soon know, if you are attentive to what is being said in the forums and able to put together all the hints provided by different Kagglers.</p>
    <p class="normal">However, please beware that some players may even use discussions about magic features to distract other competitors from serious modeling. For instance, in <em class="italic">Santander Customer Transaction Prediction</em>, there was a famous situation involving some Kagglers who fueled in other participants an interest in magic features that weren’t actually so magic, directing their efforts in the wrong direction (see the discussion here: <a href="https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87057#502362"><span class="url">https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87057#502362</span></a>).</p>
    <p class="normal">Our suggestion is to carefully read the discussions around leakage and magic features that arise in the competition’s forum, and decide whether to pursue the research and use any leakage found based on your own interest and motivations for participating in the competition. </p>
    <p class="normal">Not exploiting any leakage may really damage your final rankings, though it will surely spoil your learning experience (because leakage is a distortion and you cannot claim anything about the models using it). If you are not participating in a competition in order to gain a reputation or to later approach the sponsor for an opportunity to be hired, it is perfectly fine to use any leakage you come across. Otherwise, just ignore it and keep on working hard on your models (who knows; maybe Kaggle will reset or fix the competition by the end, rendering the leakage ineffective to the great disappointment of the many who used it).</p>
    <div class="note">
      <p class="normal">Leakages are very different from competition to competition. If you want to get an idea of a few real leakages that have happened in Kaggle competitions, you can have a look at these three memorable ones:</p>
      <ul>
        <li class="bulletList"><a href="https://www.kaggle.com/c/predicting-red-hat-business-value/discussion/22807"><span class="url">https://www.kaggle.com/c/predicting-red-hat-business-value/discussion/22807</span></a> from <em class="italic">Predicting Red Hat Business Value</em> (<a href="https://www.kaggle.com/c/predicting-red-hat-business-value"><span class="url">https://www.kaggle.com/c/predicting-red-hat-business-value</span></a>) where the problem arose because of an imperfect train/test split methodology of the competition.</li>
        <li class="bulletList"><a href="https://www.kaggle.com/c/talkingdata-mobile-user-demographics/discussion/23403"><span class="url">https://www.kaggle.com/c/talkingdata-mobile-user-demographics /discussion/23403</span></a> from <em class="italic">TalkingData Mobile User Demographics</em> (<a href="https://www.kaggle.com/c/talkingdata-mobile-user-demographics"><span class="url">https://www.kaggle.com/c/talkingdata-mobile-user-demographics</span></a>) where a series of problems and non-i.i.d cases affected the correct train/test split of the competition.</li>
        <li class="bulletList"><a href="https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31870"><span class="url">https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31870</span></a> from <em class="italic">Two Sigma Connect: Rental Listing Inquiries</em> (<a href="https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries"><span class="url">https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries</span></a>) where metadata (the creation time of each folder) did the trick.</li>
      </ul>
    </div>
    <h1 id="_idParaDest-109" class="heading-1">Summary</h1>
    <p class="normal">Having arrived at the end of the chapter, we will summarize the advice we have discussed along the way so you can organize your validation strategy and reach the end of a competition with a few suitable models to submit.</p>
    <p class="normal">In this chapter, we first analyzed the dynamics of the public leaderboard, exploring problems such as adaptive overfitting and shake-ups. We then discussed the importance of validation in a data science competition, building a reliable system, tuning it to the leaderboard, and then keeping track of your efforts.</p>
    <p class="normal">Having discussed the various validation strategies, we also saw the best way of tuning your hyperparameters and checking your test data or validation partitions by using adversarial validation. We concluded by discussing some of the various leakages that have been experienced in Kaggle competitions and we provided advice about how to deal with them.</p>
    <p class="normal">Here are our closing suggestions:</p>
    <ul>
      <li class="bulletList">Always spend the first part of the competition building a reliable validation scheme, favoring more a <em class="italic">k</em>-fold over a train-test split, given its probabilistic nature and ability to generalize to unseen data.</li>
      <li class="bulletList">If your validation scheme is unstable, use more folds or run it multiple times with different data partitions. Always check your test set using adversarial validation.</li>
      <li class="bulletList">Keep track of results based on both your validation scheme and the leaderboard. For the exploration of possible optimizations and breakthroughs (such as magic features or leakages), trust your validation score more.</li>
      <li class="bulletList">As we explained at the beginning of the chapter, use your validation scores when deciding your final submissions to the competition. For your final submissions, depending on the situation and whether or not you trust the leaderboard, choose among your best local cross-validated models and good-scoring submissions on the leaderboard, favoring the first over the second.</li>
    </ul>
    <p class="normal">At this point of our journey, we are ready to discuss how to tackle competitions using tabular data, which is numeric or categorical data arranged in matrices (with rows representing the examples and columns the features). In the next chapter, we discuss the Tabular Playground Series, a monthly contest organized by Kaggle using tabular data (organized by <em class="italic">Inversion</em>: <a href="https://www.kaggle.com/inversion"><span class="url">https://www.kaggle.com/inversion</span></a>). </p>
    <p class="normal">In addition, we will introduce you to some specific techniques to help you shine in these competitions, such as feature engineering, target encoding, denoising autoencoders, and some neural networks for tabular data, as an alternative to the recognized state-of-the-art learning algorithms in tabular data problems (the gradient boosting algorithms such as XGBoost, LightGBM, or CatBoost).</p>
    <h1 class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join the book’s Discord workspace for a monthly <em class="italic">Ask me Anything</em> session with the authors: </p>
    <p class="normal"><a href="https://packt.link/KaggleDiscord"><span class="url">https://packt.link/KaggleDiscord</span></a></p>
    <p class="normal"><img src="../Images/QR_Code40480600921811704671.png" alt=""/></p>
  </div>
</body></html>