<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;2.&#xA0;Photographing Nature and Wildlife with an Automated Camera"><div class="book" id="LTSU2-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02" class="calibre1"/>Chapter 2. Photographing Nature and Wildlife with an Automated Camera</h1></div></div></div><p class="calibre8"><span class="strong"><em class="calibre10">National Geographic</em></span> is<a id="id172" class="calibre1"/> famous for its intimate photos of wild animals. Often, in the magazine's pages, the animals seem larger than life, as if they belong to the same "geographic" scale as the landscapes behind them. This enlargement effect can be achieved by capturing the subject at a very close distance with a wide-angle lens. For example, one memorable photograph by Steve Winter shows a snarling tiger reaching out to strike the lens!</p><p class="calibre8">Let's consider the possible methods behind such a photo. The photographer could try to stalk a wild tiger in person, but for safety, this approach would require some distance and a long lens. A close encounter is likely to endanger the human, the tiger, or both. Alternatively, the photographer could use a remote-controlled rover or drone to approach and photograph the tiger. This would be safer but like the first technique it is laborious, only covers one site at a time, and may spoil opportunities for candid or natural photos because it attracts the animal's attention. Finally, the photographer could deploy concealed and <a id="id173" class="calibre1"/>automated cameras, called <span class="strong"><strong class="calibre9">camera traps</strong></span>, in multiple locations that the tiger is likely to visit.</p><p class="calibre8">This chapter will explore techniques for programming a camera trap. Maybe we will not capture any tigers, but something will wander into our trap!</p><p class="calibre8">Despite the name, a camera trap does not physically "trap" anything. It just captures photos when a trigger is tripped. Different camera traps may use different triggers but in our case, the trigger will be a computer vision system that is sensitive to motion, color, or certain classes of objects. Our system's software components will include OpenCV 3, Python scripts, shell scripts, and a camera control tool called gPhoto2. While building our system, we will address the following questions:</p><div class="book"><ul class="itemizedlist"><li class="listitem">How can we configure and trigger a photo camera from a host computer?</li><li class="listitem">How can we detect the presence of a photogenic subject?</li><li class="listitem">How can we capture and process multiple photos of a subject to create an effective composite image or video?</li></ul></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note26" class="calibre1"/>Note</h3><p class="calibre8">All the <a id="id174" class="calibre1"/>scripts and data for this chapter's project can be found in the book's GitHub repository at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_2/CameraTrap">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_2/CameraTrap</a>.</p></div><p class="calibre8">This chapter will focus on techniques for Unix-like systems, including Linux and Mac. We assume that users will ultimately deploy our camera trap on low-cost, low-powered, single-board computers (SBCs), which will typically run Linux. A good example is the Raspberry Pi 2 hardware, which typically runs the Raspbian distribution of Linux.</p><p class="calibre8">Let's begin with an outline of a few simple tasks that our software will perform before, during, and after image capture.</p></div>

<div class="book" title="Chapter&#xA0;2.&#xA0;Photographing Nature and Wildlife with an Automated Camera">
<div class="book" title="Planning the camera trap"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch02lvl1sec17" class="calibre1"/>Planning the camera trap</h1></div></div></div><p class="calibre8">Our <a id="id175" class="calibre1"/>camera trap will use a computer with two attached cameras. One camera will continuously capture low-resolution images. For example, this first camera may be an ordinary webcam. Our software will analyze the low-resolution images to detect the presence of a subject. We will explore three basic detection techniques based on motion, color, and object classification. When a subject is detected, the second camera will activate to capture and save a finite series of high-resolution images. This second camera will be a dedicated photo camera, with its own battery and storage. We will not necessarily analyze and record images at the fastest possible rate; rather, we will take care to conserve the host computer's resources as well as the photo camera's battery power and storage so that our photo trap can function for a long time.</p><p class="calibre8">Optionally, our <a id="id176" class="calibre1"/>software will configure the photo camera for <span class="strong"><strong class="calibre9">exposure bracketing</strong></span>. This means that some photos in a series will be deliberately underexposed while others will be overexposed. Later, we will upload photos from the<a id="id177" class="calibre1"/> camera to the host computer, and merge the exposures to produce <span class="strong"><strong class="calibre9">high dynamic range</strong></span> (<span class="strong"><strong class="calibre9">HDR</strong></span>) images. This means that the merged photo will exhibit fine details and saturated colors throughout a broader range of shadows, midtones, and highlights than any one exposure could capture. For example, the following lineup illustrates underexposure (left), overexposure (right), and a merged HDR photo (center):</p><div class="mediaobject"><img src="../images/00020.jpeg" alt="Planning the camera trap" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">HDR<a id="id178" class="calibre1"/> imaging is especially important in landscape photography. Typically, the sky is much brighter than the land, yet we want to tame this contrast in order to obtain saturated midtone colors in both these regions, rather than white, featureless sky or black, featureless land. We will also explore techniques for turning a series of images into a time-lapse video.</p><p class="calibre8">Note that the two cameras in this project fulfill different requirements. The webcam provides a stream of images for real-time processing, and the photo camera stores images for high-quality processing later. Consider the following comparison table:</p><div class="informalexample"><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">Feature</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Typical webcam</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Typical photo camera</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">High-end industrial camera</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Price</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Low</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Medium</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">High</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Power consumption</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Low</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">High (but has its own battery)</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Medium</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Configuration options</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Few</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Many</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Many</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Latency</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Low</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">High</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Low</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Resolution</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Low</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Very high</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">High</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Ruggedness</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Poor</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Good</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Fair</p>
</td></tr></tbody></table></div><p class="calibre8">Potentially, a high-end industrial camera could serve both purposes—real-time imaging and high-quality imaging. However, the combination of a webcam and a photo camera is likely to be cheaper. Consider the following examples:</p><div class="informalexample"><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">Name</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Purpose</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Sensor Format</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Highest Res Mode</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Interface</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Price</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Point Grey Research Grasshopper 3 GS3-U3-120S6M-C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Industrial camera</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4242x2830 @ 7 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">USB 3.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$3,700 (new)</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Carl Zeiss Jena DDR Tevidon 10mm f/2 lens</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Lens for industrial camera</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Covers 1"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Sharp, suitable for high res</p>
</td><td valign="top" class="calibre14"> </td><td valign="top" class="calibre14">
<p class="calibre15">$300 (used)</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Nikon 1 J5 with 10-30mm PD-ZOOM lens</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Photo camera and lens</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">5568x3712 @ 20 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">USB 2.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$500 (new)</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Odroid USB-Cam 720p</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Webcam</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1/4"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1280x720 @ 30 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">USB 2.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$20 (new)</p>
</td></tr></tbody></table></div><p class="calibre8">Here, the<a id="id179" class="calibre1"/> industrial camera and lens cost eight times as much as the photo camera, lens, and webcam, yet the photo camera should offer the best image quality. Although the photo camera has a <span class="strong"><em class="calibre10">capture mode</em></span> of 5568x3712 @ 20 FPS, note that its USB 2.0 interface is much too slow to support this as a <span class="strong"><em class="calibre10">transfer mode</em></span>. At the listed resolution and rate, the photo camera can just record the images to its local storage.</p><p class="calibre8">For our purposes, a photo camera's main weakness is its high latency. The latency pertains to not only the electronics, but also the moving mechanical parts. To mitigate the problem, we can take the following steps:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Use a webcam with a slightly wider angle of view than the photo camera. This way, the camera trap may detect the subject early, and provide the photo camera with more lead time to take the first shot.</li><li class="listitem">Put the photo camera in the manual focus mode, and set the focus to the distance where you plan to photograph a subject. Manual focus is quicker and quieter because the autofocus motor does not run.</li><li class="listitem">If you <a id="id180" class="calibre1"/>are using a <span class="strong"><strong class="calibre9">digital single-lens reflex</strong></span> (<span class="strong"><strong class="calibre9">DSLR</strong></span>) camera, put it in <span class="strong"><strong class="calibre9">mirror lock-up</strong></span> (<span class="strong"><strong class="calibre9">MLU</strong></span>) mode (if it <a id="id181" class="calibre1"/>supports MLU). Without MLU, the reflex mirror (which deflects light into the optical viewfinder) must move out of the optical path before each shot. With MLU, the mirror is already out of the way (but the optical viewfinder is disabled). MLU is quicker, quieter, and has less vibration because the mirror does not move. On some cameras, MLU is <a id="id182" class="calibre1"/>called <span class="strong"><strong class="calibre9">live view</strong></span> because the digital ("live") viewfinder may be activated when the optical viewfinder is disabled.</li></ul></div><p class="calibre8">Controlling a photo camera is a big part of this project. Once you learn to write scripts of photographic commands, perhaps you will begin to think about photography in new ways—for it is<a id="id183" class="calibre1"/> a process, not just a final moment when the shutter falls. Let's turn our attention to this scripting topic now.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Controlling a photo camera with gPhoto2"><div class="book" id="MSDG2-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec18" class="calibre1"/>Controlling a photo camera with gPhoto2</h1></div></div></div><p class="calibre8">gPhoto2 is<a id="id184" class="calibre1"/> an open source, vendor-neutral camera <a id="id185" class="calibre1"/>control tool for Unix-like systems, such as Linux and Mac. It<a id="id186" class="calibre1"/> supports photo cameras of multiple brands, including Canon, Nikon, Olympus, Pentax, Sony, and Fuji. The supported features vary by model. The following table lists gPhoto2's major features, alongside the official count of supported cameras for each feature:</p><div class="informalexample"><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">Feature</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Number of Supported Devices</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Description</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">File transfer</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">2105</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">files to and from the device</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Image capture</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">489</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Make the device capture an image to its local storage</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Configuration</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">428</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Change the device's settings, such as shutter speed</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Liveview</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">309</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Continuously grab frames of live video from the device</p>
</td></tr></tbody></table></div><p class="calibre8">These numbers are current as of version 2.5.8, and are conservative. For example, some configuration features are supported on the Nikon D80, even though the gPhoto2 documentation does not list this camera as configurable. For our purposes, image capture and configuration are required features, so gPhoto2 adequately supports at least 428 cameras, and perhaps many more. This number includes all manner of cameras, from point-and-shoot compacts to professional DSLRs.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note27" class="calibre1"/>Note</h3><p class="calibre8">To check<a id="id187" class="calibre1"/> whether the latest version of gPhoto2 officially supports a feature on a specific camera, see the official list at <a class="calibre1" href="http://www.gphoto.org/proj/libgphoto2/support.php">http://www.gphoto.org/proj/libgphoto2/support.php</a>.</p></div><p class="calibre8">Typically, gPhoto2<a id="id188" class="calibre1"/> communicates with a camera via USB using a protocol called <span class="strong"><strong class="calibre9">Picture Transfer Protocol</strong></span> (<span class="strong"><strong class="calibre9">PTP</strong></span>). Before proceeding, check whether your camera has any instructions regarding PTP mode. You might need to change a setting on the camera to ensure that the host computer will see it as a PTP device and not a USB mass storage device. For example, on many Nikon cameras, it is necessary to select <span class="strong"><strong class="calibre9">SETUP MENU</strong></span> | <span class="strong"><strong class="calibre9">USB</strong></span> | <span class="strong"><strong class="calibre9">PTP</strong></span>, as seen in the following image:</p><div class="mediaobject"><img src="../images/00021.jpeg" alt="Controlling a photo camera with gPhoto2" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Moreover, if a <a id="id189" class="calibre1"/>camera is mounted as a disk<a id="id190" class="calibre1"/> drive, gPhoto2 cannot communicate with it. This is slightly problematic because most operating systems automatically mount a camera as a disk drive, regardless of whether the camera is in the PTP mode. Thus, before we proceed to install and use gPhoto2, let's look at ways to programmatically unmount a camera drive.</p></div>

<div class="book" title="Controlling a photo camera with gPhoto2">
<div class="book" title="Writing a shell script to unmount camera drives"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec08" class="calibre1"/>Writing a shell script to unmount camera drives</h2></div></div></div><p class="calibre8">On Mac, a<a id="id191" class="calibre1"/> process called PTPCamera is responsible for mounting and controlling cameras on behalf of applications such as iPhoto. After connecting a camera, we can kill PTPCamera by running the following command in Terminal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ killall PTPCamera</strong></span>
</pre></div><p class="calibre8">Then, the camera will be available to receive commands from gPhoto2. However, keep reading because we want to write code that will support Linux too!</p><p class="calibre8">On most desktop Linux systems, when the camera is connected, it will be mounted as a <span class="strong"><strong class="calibre9">Gnome Virtual </strong></span><a id="id192" class="calibre1"/><span class="strong"><strong class="calibre9">File System</strong></span> (<span class="strong"><strong class="calibre9">GVFS</strong></span>) volume. We can list the mounted GVFS volumes by running the following command in Terminal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ gvfs-mount -l</strong></span>
</pre></div><p class="calibre8">For <a id="id193" class="calibre1"/>example, this command produces the following output in Ubuntu, on a MacBook Pro laptop with a Nikon D80 camera attached via USB:</p><div class="informalexample"><pre class="programlisting">Drive(0): APPLE SSD SM1024F
  Type: GProxyDrive (GProxyVolumeMonitorUDisks2)
  Volume(0): Recovery HD
    Type: GProxyVolume (GProxyVolumeMonitorUDisks2)
  Volume(1): Macintosh HD
    Type: GProxyVolume (GProxyVolumeMonitorUDisks2)
Drive(1): APPLE SD Card Reader
  Type: GProxyDrive (GProxyVolumeMonitorUDisks2)
Volume(0): NIKON DSC D80
  Type: GProxyVolume (GProxyVolumeMonitorGPhoto2)
  Mount(0): NIKON DSC D80 -&gt; <span class="strong"><strong class="calibre9">gphoto2://[usb:001,007]/</strong></span>
    Type: GProxyShadowMount (GProxyVolumeMonitorGPhoto2)
Mount(1): NIKON DSC D80 -&gt; <span class="strong"><strong class="calibre9">gphoto2://[usb:001,007]/</strong></span>
  Type: GDaemonMount</pre></div><p class="calibre8">Note that the output includes the camera's mount point, in this case, <code class="email">gphoto2://[usb:001,007]/</code>. For a camera drive, the GVFS mount point will always start with <code class="email">gphoto2://</code>. We can unmount the camera drive by running a command such as the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ gvfs-mount –u gphoto2://[usb:001,007]/</strong></span>
</pre></div><p class="calibre8">Now, if we run <code class="email">gvfs-mount -l</code> again, we should see that the camera is no longer listed. Thus, it is unmounted and should be available to receive commands from gPhoto2.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip06" class="calibre1"/>Tip</h3><p class="calibre8">Alternatively, a file browser such as Nautilus will show mounted camera drives, and will provide GUI controls to unmount them. However, as programmers, we prefer shell commands because they are easier to automate.</p></div><p class="calibre8">We will need to unmount the camera every time it is plugged in. To simplify this, let's write a Bash shell script that supports multiple operating systems (Mac or any Linux system with GVFS) and multiple cameras. Create a file named <code class="email">unmount_cameras.sh</code> and fill it with the following Bash code:</p><div class="informalexample"><pre class="programlisting">#!/usr/bin/env bash

if [ "$(uname)" == "Darwin" ]; then
  killall PTPCamera
else
  mounted_cameras=`gvfs-mount -l | grep -Po 'gphoto2://.*/' | uniq`
  for mounted_camera in $mounted_cameras; do
    gvfs-mount -u $mounted_camera
  done
fi</pre></div><p class="calibre8">Note that this <a id="id194" class="calibre1"/>script checks the operating system's family (where <code class="email">"Darwin"</code> is the family of Mac). On Mac, it runs <code class="email">killall PTPCamera</code>. On other systems, it uses a combination of the <code class="email">gvfs-mount</code>, <code class="email">grep</code>, and <code class="email">uniq</code> commands to find each unique <code class="email">gphoto2://</code> mount point and then unmount all the cameras.</p><p class="calibre8">Let's give the script "executable" permissions by running the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ chmod +x unmount_cameras.sh</strong></span>
</pre></div><p class="calibre8">Anytime we want to ensure that the camera drives are unmounted, we can execute the script like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ ./unmount_cameras.sh</strong></span>
</pre></div><p class="calibre8">Now, we have a standard way to make a camera available, so we are ready to install and use gPhoto2.</p></div></div>

<div class="book" title="Controlling a photo camera with gPhoto2">
<div class="book" title="Setting up and testing gPhoto2"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec09" class="calibre1"/>Setting up and testing gPhoto2</h2></div></div></div><p class="calibre8">gPhoto2 <a id="id195" class="calibre1"/>and related libraries are widely available in open source software <a id="id196" class="calibre1"/>repositories for Unix-like systems. No wonder—connecting to a photo camera is a common task in desktop computing today!</p><p class="calibre8">For Mac, Apple does not provide a package manager but third parties do. The MacPorts package manager has the most extensive repository.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note28" class="calibre1"/>Note</h3><p class="calibre8">To set <a id="id197" class="calibre1"/>up MacPorts and its dependencies, follow the official guide at <a class="calibre1" href="https://www.macports.org/install.php">https://www.macports.org/install.php</a>.</p></div><p class="calibre8">To install gPhoto2 via MacPorts, run the following command in Terminal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ sudo port install gphoto2</strong></span>
</pre></div><p class="calibre8">On Debian and its derivatives, including Ubuntu, Linux Mint, and Raspbian, we can install gPhoto2 by running the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ sudo apt-get install gphoto2</strong></span>
</pre></div><p class="calibre8">On Fedora and its derivatives, including Red Hat Enterprise Linux (RHEL) and CentOS, we can use the following installation command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ sudo yum install gphoto2</strong></span>
</pre></div><p class="calibre8">OpenSUSE has <a id="id198" class="calibre1"/>a one-click installer for gPhoto2 at <a class="calibre1" href="https://software.opensuse.org/package/gphoto">https://software.opensuse.org/package/gphoto</a>.</p><p class="calibre8">After <a id="id199" class="calibre1"/>installing gPhoto2, let's connect a camera. Ensure that the camera is <a id="id200" class="calibre1"/>turned on and in PTP mode. Then, run the following commands to unmount the camera drive and take a photo:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ ./unmount_cameras.sh</strong></span>
<span class="strong"><strong class="calibre9">$ gphoto2 --capture-image</strong></span>
</pre></div><p class="calibre8">If the camera is in autofocus mode, you might see or hear the lens move. (Ensure that the camera has a subject in view so that autofocus will succeed. Otherwise, no photo will be captured.) Then, you might hear the shutter open and close. Disconnect the camera and use its review menu to browse the captured photos. If a new photo is there, gPhoto2 is working!</p><p class="calibre8">To upload all images from the camera to the current working directory, we could reconnect the camera and run the following commands:</p><div class="informalexample"><pre class="programlisting">$ ./unmount_cameras.sh
$ gphoto2 --get-all-files</pre></div><p class="calibre8">To read about all the flags that gphoto2 supports, we can open its manual by running the following command:</p><div class="informalexample"><pre class="programlisting">$ man gphoto2</pre></div><p class="calibre8">Next, let's try a more advanced task, involving configuration as well as image capture. We will take a series of photos with exposure bracketing.</p></div></div>

<div class="book" title="Controlling a photo camera with gPhoto2">
<div class="book" title="Writing a shell script for exposure bracketing"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch02lvl2sec10" class="calibre1"/>Writing a shell script for exposure bracketing</h2></div></div></div><p class="calibre8">gPhoto2 <a id="id201" class="calibre1"/>provides a flag, <code class="email">--set-config</code>, which<a id="id202" class="calibre1"/> allows us to reconfigure <a id="id203" class="calibre1"/>many camera parameters, including <span class="strong"><strong class="calibre9">exposure compensation</strong></span>. For example, suppose we want to overexpose an image by the equivalent of one full f-stop (doubling the aperture's area or increasing its radius by a factor of sqrt(2)). This bias is <a id="id204" class="calibre1"/>called an exposure compensation (or exposure adjustment) of +1.0 <span class="strong"><strong class="calibre9">exposure value</strong></span> (<span class="strong"><strong class="calibre9">EV</strong></span>). The following command configures the camera to use +1.0 EV and then takes a photo:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ gphoto2 --set-config exposurecompensation=1000 --capture-image</strong></span>
</pre></div><p class="calibre8">Note that the value of <code class="email">exposurecompensation</code> is denominated in thousandths of an EV, so <code class="email">1000</code> is +1.0 EV. To underexpose, we would use a negative value. A series of these commands, each with a different EV, would achieve exposure bracketing.</p><p class="calibre8">We can<a id="id205" class="calibre1"/> use the <code class="email">--set-config</code> flag to control <a id="id206" class="calibre1"/>many photographic properties, not just exposure compensation. For example, the following command captures a photo with an exposure time of one second, while firing the flash in slow sync mode:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ gphoto2 --set-config shutterspeed=1s flashmode=2 --capture-image</strong></span>
</pre></div><p class="calibre8">The following command lists all the supported properties and values for the given camera:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ gphoto2 --list-all-config</strong></span>
</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note29" class="calibre1"/>Note</h3><p class="calibre8">For further discussion of f-stops, exposure, and other photographic properties, refer back to <a class="calibre1" title="Chapter 1. Getting the Most out of Your Camera System" href="part0015_split_000.html#E9OE2-940925703e144daa867f510896bffb69">Chapter 1</a>, <span class="strong"><em class="calibre10">Getting the Most out of Your Camera System</em></span>, especially the <span class="strong"><em class="calibre10">Capturing the subject in the moment</em></span> section.</p></div><p class="calibre8">Before taking a series of exposure bracketed photos, dial your camera to the <span class="strong"><strong class="calibre9">aperture priority (A)</strong></span> mode. This<a id="id207" class="calibre1"/> means that the aperture will be held constant while the shutter speed will vary based on lighting and EV. A constant aperture will help ensure that the same region is in focus in all images.</p><p class="calibre8">Let's automate the exposure bracketing commands with another shell script, which we will call <code class="email">capture_exposure_bracket.sh</code>. It will accept a flag, <code class="email">-s</code>, to specify the exposure step size between frames (in thousandths of an EV), and another flag <code class="email">-f</code>, to specify the number of frames. The defaults will be 3 frames spaced at an interval of 1.0 EV. Here is the script's implementation:</p><div class="informalexample"><pre class="programlisting">#!/usr/bin/env bash

ev_step=1000
frames=3
while getopts s:f: flag; do
  case $flag in
    s)
      ev_step="$OPTARG"
      ;;
    f)
      frames="$OPTARG"
      ;;
    ?)
      exit
      ;;
  esac
done

min_ev=$((-ev_step * (frames - 1) / 2))
for ((i=0; i&lt;frames; i++)); do
  ev=$((min_ev + i * ev_step))
  gphoto2 --set-config exposurecompensation=$ev \
    --capture-image
done
gphoto2 --set-config exposurecompensation=0</pre></div><p class="calibre8">All the commands in this script are cross-platform for Linux and Mac. Note that we are using the <code class="email">getopts</code> command to parse arguments, and Bash arithmetic to compute the EV of each photo.</p><p class="calibre8">Remember <a id="id208" class="calibre1"/>to give the script "executable" permissions by running the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ chmod +x capture_exposure_bracket.sh</strong></span>
</pre></div><p class="calibre8">To unmount the<a id="id209" class="calibre1"/> camera and capture 5 photos at an interval of 1.5 EV, we could run the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ ./unmount_cameras.sh</strong></span>
<span class="strong"><strong class="calibre9">$ ./capture_exposure_bracket.sh –s 1500 –f 5</strong></span>
</pre></div><p class="calibre8">Now that we have a clear idea of how to control a camera from the command line, let's consider how to wrap this functionality in a general-purpose programming language that can also interface with OpenCV.</p></div></div>

<div class="book" title="Controlling a photo camera with gPhoto2">
<div class="book" title="Writing a Python script to wrap gPhoto2"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch02lvl2sec11" class="calibre1"/>Writing a Python script to wrap gPhoto2</h2></div></div></div><p class="calibre8">Python is a high-level, dynamic programming language with great third-party libraries for mathematics<a id="id210" class="calibre1"/> and science. OpenCV's Python <a id="id211" class="calibre1"/>bindings are efficient and quite mature, wrapping all the C++ library's major functionality except GPU optimizations. Python is also a convenient scripting language, as its standard libraries provide cross-platform interfaces to access much of the system's functionality. For example, it is easy to write Python code to spawn a subprocess (also called a child process), which may run any executable, even another interpreter such as a Bash shell.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note30" class="calibre1"/>Note</h3><p class="calibre8">For more information on spawning and communicating with a child process from Python, see<a id="id212" class="calibre1"/> the <code class="email">subprocess</code> module's documentation at <a class="calibre1" href="https://docs.python.org/2/library/subprocess.html">https://docs.python.org/2/library/subprocess.html</a>. For the special case where the child process is an additional Python interpreter, see the documentation<a id="id213" class="calibre1"/> of the <code class="email">multiprocessing</code> module at <a class="calibre1" href="https://docs.python.org/2/library/multiprocessing.html">https://docs.python.org/2/library/multiprocessing.html</a>.</p></div><p class="calibre8">We will use Python's standard subprocessing functionality to wrap gPhoto2 and our own shell <a id="id214" class="calibre1"/>scripts. By sending camera commands<a id="id215" class="calibre1"/> from a child process, we will enable the caller (in Python) to treat these as "fire and forget" commands. That is to say, functions in the Python process return immediately so that the caller is not obliged to wait for the camera to handle the commands. This is a good thing because a camera might typically require several seconds to autofocus and capture a series of photos.</p><p class="calibre8">Let's create a new file, <code class="email">CameraCommander.py</code>, and begin its implementation with the following import statements:</p><div class="informalexample"><pre class="programlisting">import os
import subprocess</pre></div><p class="calibre8">We will write a class, <code class="email">CameraCommander</code>. As member variables, it will have a current capture process (which may be <code class="email">None</code>) and a log file. By default, the log file will be <code class="email">/dev/null</code>, which means that the log output will be discarded. After setting member variables, the initialization method will call a helper method to unmount thecamera drive so that the camera is ready to receive commands. Here are the class's declaration and initializer:</p><div class="informalexample"><pre class="programlisting">class CameraCommander(object):

  def __init__(self, logPath=os.devnull):
    self._logFile = open(logPath, 'w')
    self._capProc = None
    self.unmount_cameras()</pre></div><p class="calibre8">When an instance of <code class="email">CameraCommander</code> is deleted, it should close the log file, as seen in the following code:</p><div class="informalexample"><pre class="programlisting">  def __del__(self):
    self._logFile.close()</pre></div><p class="calibre8">Every time <code class="email">CameraCommander</code> opens a subprocess, the command should be interpreted by the shell (Bash), and the command's print output and errors should be redirected to the log file. Let's standardize this configuration of a subprocess in the following helper method:</p><div class="informalexample"><pre class="programlisting">  def _open_proc(self, command):
    return subprocess.Popen(
      command, shell=True, stdout=self._logFile,
      stderr=self._logFile)</pre></div><p class="calibre8">Now, as our first wrapper around a shell command, let's write a method to run <code class="email">unmount_cameras.sh</code> in a subprocess. Unmounting the camera drives is a short process, and it must finish before other camera commands can run. Thus, we will implement our wrapper method so that it does not return until <code class="email">unmount_cameras.sh</code> returns. That is to say, the subprocess will run synchronously<a id="id216" class="calibre1"/> in this case. Here is the wrapper's implementation:</p><div class="informalexample"><pre class="programlisting">  def unmount_cameras(self):
    proc = self._open_proc('./unmount_cameras.sh')
    proc.wait()</pre></div><p class="calibre8">Next, let's<a id="id217" class="calibre1"/> consider how to capture a single image. We will start by calling a helper method to stop any previous, conflicting command. Then, we will invoke the <code class="email">gphoto2</code> command with the usual <code class="email">--capture-image</code> flag. Here is the implementation of the wrapper method:</p><div class="informalexample"><pre class="programlisting">  def capture_image(self):
    self.stop_capture()
    self._capProc = self._open_proc(
      'gphoto2 --capture-image')</pre></div><p class="calibre8">As another capture mode, we can invoke <code class="email">gphoto2</code> to record a time-lapse series. The <code class="email">-I</code> or <code class="email">--interval</code> flag, with an integer value, specifies the delay between frames, in seconds. The <code class="email">-F</code> or <code class="email">--frames</code> flag also takes an integer value, specifying the number of frames in the series. If the <code class="email">-I</code> flag is used but <code class="email">-F</code> is omitted, the process continues to capture frames indefinitely until forced to terminate. Let's provide the following wrapper for time-lapse functionality:</p><div class="informalexample"><pre class="programlisting">  def capture_time_lapse(self, interval, frames=0):
    self.stop_capture()
    if frames &lt;= 0:
      # Capture an indefinite number of images.
      command = 'gphoto2 --capture-image -I %d' % interval
    else:
      command = 'gphoto2 --capture-image -I %d -F %d' %\
        (interval, frames)
    self._capProc = self._open_proc(command)</pre></div><p class="calibre8">Before taking a series of time-lapse photos, you might want to dial your camera to the <span class="strong"><strong class="calibre9">manual exposure</strong></span> (<span class="strong"><strong class="calibre9">M</strong></span>) mode. This means that the aperture and shutter speed will be held constant at <a id="id218" class="calibre1"/>manually specified values. Assuming that the scene's light level is approximately constant, a constant exposure will help prevent unpleasant flickering in the time-lapse video. On the other hand, if we do expect lighting conditions to vary a lot over the course of the time-lapse series, the M mode may be inappropriate because in these circumstances, it will cause some frames to be noticeably underexposed and others overexposed.</p><p class="calibre8">To allow for exposure bracketing, we can simply wrap our <code class="email">capture_exposure_bracket.sh</code> script, as seen in the following code:</p><div class="informalexample"><pre class="programlisting">  def capture_exposure_bracket(self, ev_step=1.0, frames=3):
    self.stop_capture()
    self._capProc = self._open_proc(
      './capture_exposure_bracket.sh -s %d -f %d' %\
        (int(ev_step * 1000), frames))</pre></div><p class="calibre8">As we<a id="id219" class="calibre1"/> have seen in the previous three methods, it is <a id="id220" class="calibre1"/>sensible to terminate any ongoing capture process before trying to start another. (After all, a camera can only process one command at a time). Moreover, a caller might have other reasons to terminate a capture process. For example, the subject might have gone away. We will provide the following method to force the termination of any ongoing capture process:</p><div class="informalexample"><pre class="programlisting">  def stop_capture(self):
    if self._capProc is not None:
      if self._capProc.poll() is None:
        # The process is currently running but might finish
        # before the next function call.
        try:
          self._capProc.terminate()
        except:
          # The process already finished.
          pass
      self._capProc = None</pre></div><p class="calibre8">Similarly, we will provide the following method to await the completion of any currently running capture process:</p><div class="informalexample"><pre class="programlisting">  def wait_capture(self):
    if self._capProc is not None:
      self._capProc.wait()
      self._capProc = None</pre></div><p class="calibre8">Finally, we will provide the following property getter to enable a caller to check whether a capture process is currently running:</p><div class="informalexample"><pre class="programlisting">  @property
  def capturing(self):
    if self._capProc is None:
      return False
    elif self._capProc.poll() is None:
      return True
    else:
      self._capProc = None
      return False</pre></div><p class="calibre8">This concludes the <code class="email">CameraCommander</code> module. To test our work, let's write another script, <code class="email">test_camera_commands.py</code>, with the following implementation:</p><div class="informalexample"><pre class="programlisting">#!/usr/bin/env python

import CameraCommander

def main():

  cc = CameraCommander.CameraCommander('test_camera_commands.log')

  cc.capture_image()
  print('Capturing image...')
  cc.wait_capture()
  print('Done')

  cc.capture_time_lapse(3, 2)
  print('Capturing 2 images at time interval of 3 seconds...')
  cc.wait_capture()
  print('Done')

  cc.capture_exposure_bracket(1.0, 3)
  print('Capturing 3 images at exposure interval of 1.0 EV...')
  cc.wait_capture()
  print('Done')

if __name__ == '__main__':
  main()</pre></div><p class="calibre8">Ensure <a id="id221" class="calibre1"/>that your camera is on, is in the PTP mode, and is<a id="id222" class="calibre1"/> connected. Then, make the test script executable and run it, like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ chmod +x test_camera_commands.py</strong></span>
<span class="strong"><strong class="calibre9">$ ./test_camera_commands.py</strong></span>
</pre></div><p class="calibre8">Wait for all the commands to finish, and then disconnect the camera to review the images. Check the timestamp and EV number of each photo. Ideally, a total of six photos should have been captured. However, the actual number could vary depending on factors such as the success or failure of autofocus, and the time spent capturing and saving each image. In case of any doubts, review the log file, <code class="email">test_camera_commands.log</code>, in a text editor.</p></div></div>

<div class="book" title="Controlling a photo camera with gPhoto2">
<div class="book" title="Finding libgphoto2 and wrappers"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch02lvl2sec12" class="calibre1"/>Finding libgphoto2 and wrappers</h2></div></div></div><p class="calibre8">As an<a id="id223" class="calibre1"/> alternative<a id="id224" class="calibre1"/> to using the gPhoto2 command line tool, we could use the <a id="id225" class="calibre1"/>underlying C library, libgphoto2 (<a class="calibre1" href="https://github.com/gphoto/libgphoto2">https://github.com/gphoto/libgphoto2</a>). The library has several third-party wrappers, including <a id="id226" class="calibre1"/>a set of up-to-date Python bindings called python-gphoto2 (<a class="calibre1" href="https://github.com/gphoto/libgphoto2-python">https://github.com/gphoto/libgphoto2-python</a>).</p><p class="calibre8">OpenCV 3's videoio module<a id="id227" class="calibre1"/> has optional support for libgphoto2. To <a id="id228" class="calibre1"/>enable this feature, we could configure and build OpenCV from source using the <code class="email">WITH_GPHOTO2</code> CMake definition. Of course, for this option to work, the system must already have an installation of libgphoto2 and its header files. For example, these can be installed by the following command on Debian, Ubuntu, Linux Mint, Raspbian, and similar systems:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ sudo apt-get install libgphoto2-dev</strong></span>
</pre></div><p class="calibre8">For our purposes, controlling the photo camera via libgphoto2 or OpenCV's videoio module is overkill. We do not want to grab frames for real-time processing. We simply want our Python scripts to initiate additional processes to unmount and configure the camera, and make it capture photos to its local storage. The gPhoto2 command-line tool and our own shell scripts are perfectly convenient to use as subprocesses, so we will continue to rely on them throughout the rest of this chapter.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note31" class="calibre1"/>Note</h3><p class="calibre8">One of OpenCV's official samples demonstrates the use of a gPhoto2-compatible <a id="id229" class="calibre1"/>camera via the videoio module. Specifically, the sample deals with focus control. See the source code in OpenCV's GitHub repository at <a class="calibre1" href="https://github.com/Itseez/opencv/blob/master/samples/cpp/autofocus.cpp">https://github.com/Itseez/opencv/blob/master/samples/cpp/autofocus.cpp</a>.</p></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Detecting the presence of a photogenic subject"><div class="book" id="NQU22-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec19" class="calibre1"/>Detecting the presence of a photogenic subject</h1></div></div></div><p class="calibre8"><a class="calibre1" title="Chapter 1. Getting the Most out of Your Camera System" href="part0015_split_000.html#E9OE2-940925703e144daa867f510896bffb69">Chapter 1</a>, <span class="strong"><em class="calibre10">Getting the Most out of Your Camera System</em></span>, proposed that a photograph ought to<a id="id230" class="calibre1"/> capture a subject in a moment. Let's explore this notion further as we search for ways to detect a desirable or "photogenic" subject and moment.</p><p class="calibre8">As a medium, photography uses light, an aperture, a photosensitive surface, and time to draw an image of a scene. The earliest photographic technology, in the 1820s, lacked the resolution and speed to convey a detailed subject in a precise moment, but it was able to capture a grainy scene on a sunny day. Later, with better lenses, flashes, and photosensitive surfaces, photography became capable of capturing a sharp scene, a formal portrait, a faster and more natural portrait, and finally a moment of action, frozen in time.</p><p class="calibre8">Consider the following series of famous photographs, ranging from 1826 to 1942:</p><div class="mediaobject"><img src="../images/00022.jpeg" alt="Detecting the presence of a photogenic subject" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">For<a id="id231" class="calibre1"/> general interest, here are some details about the preceding photographs:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Upper left: <span class="strong"><em class="calibre10">View from the Window at Le Gras</em></span> is history's earliest surviving photograph, taken by Nicéphore Niépce in 1826 or 1827 at Saint-Loup-de-Varennes, France. The scene includes parts of the rooftops and countryside at Niépce's estate.</li><li class="listitem">Middle left: <span class="strong"><em class="calibre10">Boulevard du Temple</em></span>, taken by Louis Daguerre in 1838, is believed to be the first photograph to include people. The scene is a busy street in<a id="id232" class="calibre1"/> Paris, but most of the passersby are invisible because of the photograph's slow speed. Near the street corner, one man is polishing another man's boots, so these two people were in one place long enough to be recorded.</li><li class="listitem">Upper right: Jean-Baptiste Sabatier-Blot captured this formal portrait of Louis Daguerre in 1844.</li><li class="listitem">Lower left: Sergei Prokudin-Gorsky, a pioneer of color photography, captured this relatively informal portrait of factory workers in Kasli, Russia, in 1910. The men in the photograph are creating casts at the Kasli Iron Works, which produced sculptures and luxury furniture in the 19th and early 20th centuries.</li><li class="listitem">Lower right: Max Alpert took this combat photograph on July 12, 1942, near Luhansk (today in Ukraine). The subject is Aleksey Gordeyevich Yeremenko, a 23-year-old junior political officer in the Red Army. At the moment of the photograph, Yeremenko is rallying his regiment to attack. A few seconds later, he was shot dead.</li></ul></div><p class="calibre8">Even from these few examples, we can infer a historical trend toward more dynamic images, which capture an atmosphere of activity, change, or even violence. Let's contemplate this trend in the context of nature and wildlife photography. Color photography began to reach the public eye around 1907, and after several more decades of technological improvements, it prevailed as a more popular format than black and white. Color is dynamic. Landscapes, plants, and even animals change color depending on the season, weather, time of day, and their age. Today, it would seem strange to see a nature documentary shot in black and white.</p><p class="calibre8">Changes in lens technology have also had a profound influence on nature and wildlife photography. With longer, faster, and sharper lenses, photographers have been able to peer into the lives of wild animals from a distance. For example, today, documentaries are filled with scenes of predatory animals chasing their prey. To shoot these scenes would have been difficult, or impossible, with lenses of the 1920s. Similarly, the quality of macro (close-up) lenses has improved a lot, and has been a boon to documentary work on insects and other small creatures.</p><p class="calibre8">Finally, as we discussed in the opening of this chapter, advances in automation have enabled photographers to deploy cameras in remote wilderness, in the midst of the action. With digital technology, a remote camera can store a huge number of photos, and these photos can be combined easily to produce effects, such as time-lapse (which accentuates motion) or HDR (which accentuates color). Today, these techniques are in widespread use, so documentary fans may be familiar with the sight of time-lapse flowers rocketing up from<a id="id233" class="calibre1"/> the ground, or time-lapse clouds racing across saturated HDR skies. Whether small or large, everything is portrayed as dynamic.</p><p class="calibre8">We can <a id="id234" class="calibre1"/>design a few simple rules to help distinguish between a dynamic scene and a static scene. Here are some useful cues:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Motion</strong></span>: We may assume that any movement in a scene represents a chance to capture a subject in a moment of action or change. Without needing to know what the subject is, we can detect its motion and capture a photograph.</li><li class="listitem"><span class="strong"><strong class="calibre9">Color</strong></span>: We may assume that certain color patterns are unusual in a given environment, and that they arise in a dynamic situation. Without needing to know exactly what the colorful subject is, we can detect its presence and photograph it. For example, a big new splash of color could be a sunset as the clouds part, or a flower as it opens.</li><li class="listitem"><span class="strong"><strong class="calibre9">Classification</strong></span>: We may assume that certain kinds of subjects are alive and will interact with their environment, creating opportunities for dynamic photos. When we detect a given class of subject, we can respond by photographing it. As an example, we will detect and photograph the faces of mammals.</li></ul></div><p class="calibre8">Regardless of the approach to detecting a subject, we must ensure that our webcam and photo camera have a similar view of the scene. They should point at the same target. The webcam's angle of view should be as wide as the photo camera's, and perhaps a little wider to allow the webcam to detect a subject just before it enters the photo camera's view. Both cameras should be fixed firmly in place so that they do not become misaligned due to vibrations, wind, or other typical disturbances. For example, the photo camera could be mounted on a sturdy tripod, and the webcam could be taped to the photo camera's hotshoe (the slot that is typically reserved for an external flash or external viewfinder). The following image shows an example of this setup:</p><div class="mediaobject"><img src="../images/00023.jpeg" alt="Detecting the presence of a photogenic subject" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">For<a id="id235" class="calibre1"/> context, the following image is a slightly more distant view of the same setup. Observe that the webcam and photo camera point at the same subject:</p><div class="mediaobject"><img src="../images/00024.jpeg" alt="Detecting the presence of a photogenic subject" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">We will<a id="id236" class="calibre1"/> implement each type of camera trap as a separate script, which will accept command-line arguments to adjust the trap's sensitivity. To begin, let's develop a motion-sensitive trap.</p></div>

<div class="book" title="Detecting the presence of a photogenic subject">
<div class="book" title="Detecting a moving subject"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec13" class="calibre1"/>Detecting a moving subject</h2></div></div></div><p class="calibre8">Our<a id="id237" class="calibre1"/> motion-activated camera trap will rely on the <code class="email">CameraCommander</code> module that we implemented earlier, in the section <span class="strong"><em class="calibre10">Writing a Python script to wrap gPhoto2</em></span>. Also, we will use OpenCV and NumPy to capture and analyze webcam images. Finally, from Python's standard library, we will import the <code class="email">argparse</code> module, which will help parse command-line arguments, and the <code class="email">time</code> module, which we will use to control the time delay between detection attempts. Let's create a file, <code class="email">set_motion_trap.py</code>, and begin its implementation with the following imports:</p><div class="informalexample"><pre class="programlisting">#!/usr/bin/env python

import argparse
import time

import numpy
import cv2

import CameraCommander</pre></div><p class="calibre8">This script will have a simple structure, with just a <code class="email">main()</code> function that reads command-line arguments and performs motion detection in a loop. Several arguments pertain to the use of the<a id="id238" class="calibre1"/> webcam, which we will call the detection camera. Other arguments concern the motion detection algorithm and the use of the photo camera. The <code class="email">main()</code> function begins with the following definitions of the command-line arguments:</p><div class="informalexample"><pre class="programlisting">def main():

  parser = argparse.ArgumentParser(
    description='This script detects motion using an '
                'attached webcam. When it detects '
                'motion, it captures photos on an '
                'attached gPhoto2-compatible photo '
                'camera.')

  parser.add_argument(
    '--debug', type=bool, default=False,
    help='print debugging information')

  parser.add_argument(
    '--cam-index', type=int, default=-1,
    help='device index for detection camera '
         '(default=0)')
  parser.add_argument(
    '--width', type=int, default=320,
    help='capture width for detection camera '
         '(default=320)')
  parser.add_argument(
    '--height', type=int, default=240,
    help='capture height for detection camera '
         '(default=240)')
  parser.add_argument(
    '--detection-interval', type=float, default=0.25,
    help='interval between detection frames, in seconds '
         '(default=0.25)')

  parser.add_argument(
    '--learning-rate', type=float, default=0.008,
    help='learning rate for background subtractor, which '
         'is used in motion detection (default=0.008)')
  parser.add_argument(
    '--min-motion', type=float, default=0.15,
    help='proportion of frame that must be classified as '
         'foreground to trigger motion event '
         '(default=0.15, valid_range=[0.0, 1.0])')

  parser.add_argument(
    '--photo-count', type=int, default=1,
    help='number of photo frames per motion event '
         '(default=1)')
  parser.add_argument(
    '--photo-interval', type=float, default=3.0,
    help='interval between photo frames, in seconds '
         '(default=3.0)')
  parser.add_argument(
    '--photo-ev-step', type=float, default=None,
    help='exposure step between photo frames, in EV. If '
         'this is specified, --photo-interval is ignored '
         'and --photo-count refers to the length of an '
         'exposure bracketing sequence, not a time-lapse '
         'sequence.')</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note32" class="calibre1"/>Note</h3><p class="calibre8">The arguments' <code class="email">help</code> text will appear when we run our script with the <code class="email">-h</code> or <code class="email">--help</code> flag, like this:</p><div class="informalexample"><pre class="programlisting">$ ./set_motion_trap.py -h</pre></div></div><p class="calibre8">At this<a id="id239" class="calibre1"/> point, we have only declared the arguments. Next, we need to parse them and access their values, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">  args = parser.parse_args()

  debug = args.debug

  cam_index = args.cam_index
  w, h = args.width, args.height
  detection_interval = args.detection_interval

  learning_rate = args.learning_rate
  min_motion = args.min_motion

  photo_count = args.photo_count
  photo_interval = args.photo_interval
  photo_ev_step = args.photo_ev_step</pre></div><p class="calibre8">Besides the arguments, we will use several variables. A <code class="email">VideoCapture</code> object will enable us to configure and capture from the webcam. Matrices (which are actually NumPy arrays in OpenCV's Python wrapper) will enable us to store BGR and grayscale versions of each webcam frame as well as a <span class="strong"><strong class="calibre9">foreground mask</strong></span>. The foreground mask will be output from a motion detection algorithm, and it will be a grayscale image that is white in<a id="id240" class="calibre1"/> foreground (moving) areas, gray <a id="id241" class="calibre1"/>in shadow areas, and black in background areas. Specifically, in our case, the motion detector will be an instance of OpenCV's <code class="email">BackgroundSubtractorMOG2</code> class. Last, we need an instance of our <code class="email">CameraCommander</code> class to control the photo camera. Here are the declarations of the relevant variables:</p><div class="informalexample"><pre class="programlisting">  cap = cv2.VideoCapture(cam_index)
  cap.set(cv2.CAP_PROP_FRAME_WIDTH, w)
  cap.set(cv2.CAP_PROP_FRAME_HEIGHT, h)

  bgr = None
  gray = None
  fg_mask = None

  bg_sub = cv2.createBackgroundSubtractorMOG2()

  cc = CameraCommander.CameraCommander()</pre></div><p class="calibre8">The remainder of the <code class="email">main()</code> function's implementation is a loop. On each iteration, we will put the thread to sleep for a specified interval (by default, 0.25 seconds) because this will conserve system resources. As a result, we will skip some of the webcam's frames, but we probably do not need the full frame rate to detect the subject. If we did not impose a sleep period, the camera trap could utilize 100% of a CPU core all the time, particularly on a slow CPU in a low-powered SBC. Here is this first part of the loop's implementation:</p><div class="informalexample"><pre class="programlisting">  while True:
    time.sleep(detection_interval)</pre></div><p class="calibre8">When we do read a frame, we will convert it to grayscale and equalize it:</p><div class="informalexample"><pre class="programlisting">    success, bgr = cap.read(bgr)
    if success:
      gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY, gray)
      gray = cv2.equalizeHist(gray, gray)</pre></div><p class="calibre8">We will pass the equalized frame and the foreground mask to the <code class="email">BackgroundSubtractorMOG2</code>'s <code class="email">apply</code> method. This method accumulates a history of frames, and estimates whether each pixel is part of a foreground region, shadow, or background region based on differences between frames in the history. As a third argument, we will <a id="id242" class="calibre1"/>pass a <span class="strong"><strong class="calibre9">learning rate</strong></span>, which is a value in the range [0.0, 1.0]. A low value means that more weight will be given to old frames, and thus the estimates will change slowly. See how we call the method in this line of code:</p><div class="informalexample"><pre class="programlisting">      fg_mask = bg_sub.apply(gray, fg_mask, learning_rate)</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note33" class="calibre1"/>Note</h3><p class="calibre8">Note that in background subtraction algorithms such as MOG2, the foreground is defined as a region whose pixel values have changed in recent history. Conversely, the background is a region whose pixel values have not changed. The shadow refers to the foreground's shadow. For details about MOG2 and the other <a id="id243" class="calibre1"/>background subtraction algorithms supported in OpenCV, see the official documentation at <a class="calibre1" href="http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html#backgroundsubtractormog2">http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html#backgroundsubtractormog2</a>.</p></div><p class="calibre8">As an<a id="id244" class="calibre1"/> example of the background subtractor's input and output, consider the following pair of images. The top image is an RGB frame from a video, while the bottom image is a foreground mask based on the video. Note that the scene is a rocky seacoast with waves breaking in the foreground and boats going past in the distance:</p><div class="mediaobject"><img src="../images/00025.jpeg" alt="Detecting a moving subject" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">By counting the white (foreground) values in the foreground mask, we can get a rough measurement of the amount of movement that the webcam has captured in recent history. We should normalize this figure based on the number of pixels in the frame. Here is the relevant code:</p><div class="informalexample"><pre class="programlisting">      h, w = fg_mask.shape
      motion = numpy.sum(numpy.where(fg_mask == 255, 1, 0))
      motion /= float(h * w)</pre></div><p class="calibre8">If the script is running with the <code class="email">--debug</code> flag, we will print the measurement of motion:</p><div class="informalexample"><pre class="programlisting">      if debug:
        print('motion=%f' % motion)</pre></div><p class="calibre8">If the <a id="id245" class="calibre1"/>motion exceeds a specified threshold, and if we are not already capturing photos, we will start to capture photos now. Depending on the command-line arguments, we may capture either an exposure-bracketed series or a time-lapse series, as seen in the next block of code:</p><div class="informalexample"><pre class="programlisting">      if motion &gt;= min_motion and not cc.capturing:
        if photo_ev_step is not None:
          cc.capture_exposure_bracket(photo_ev_step, photo_count)
        else:
          cc.capture_time_lapse(photo_interval, photo_count)</pre></div><p class="calibre8">Here, the loop and the <code class="email">main()</code> function end. To ensure that <code class="email">main()</code> runs when the script is executed, we must add the following code to the script:</p><div class="informalexample"><pre class="programlisting">if __name__ == '__main__':
  main()</pre></div><p class="calibre8">We can give this Python script "executable" permissions and then run it like any other shell script, as seen in following example:</p><div class="informalexample"><pre class="programlisting">$ chmod +x set_motion_trap.py
$ ./set_motion_trap.py --debug True</pre></div><p class="calibre8">Consider the pair of images below. The left-hand image shows the physical setup of the motion-activated camera trap, which happens to be running <code class="email">set_motion_trap.py</code> with default parameters. The right-hand image is one of the resulting photos:</p><div class="mediaobject"><img src="../images/00026.jpeg" alt="Detecting a moving subject" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">These images<a id="id246" class="calibre1"/> were taken with two different cameras, and for this reason they differ in color and contrast. However, they represent the same scene.</p><p class="calibre8">Experiment with the optional arguments to see which settings work best for a given camera and a particular kind of moving subject. Once you have gained an understanding of this camera trap's sensitivities, let's proceed to another design, using a set of color values as the trigger.</p></div></div>

<div class="book" title="Detecting the presence of a photogenic subject">
<div class="book" title="Detecting a colorful subject"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec14" class="calibre1"/>Detecting a colorful subject</h2></div></div></div><p class="calibre8">OpenCV provides <a id="id247" class="calibre1"/>a set of functions to measure and compare the color distribution in images. This field is called histogram analysis. A histogram is just an array of pixel counts for various colors or ranges of colors. Thus, for a BGR image with 256 possible values per channel, a histogram can have as many as 256 ^ 3 = 16.8 million elements. To create such a histogram, we can use the following code:</p><div class="informalexample"><pre class="programlisting">images = [myImage]  # One or more input images
channels =  [0, 1, 2]  # The channel indices
mask = None  # The image region, or None for everything
histSize = [256, 256, 256]  # The channel depths
ranges = [0, 255, 0, 255, 0, 255]  # The color bin boundaries
hist = cv2.calcHist(images, channels, mask, histSize, ranges)</pre></div><p class="calibre8">The sum of the histogram's values equals the total number of pixels in the input images. To facilitate comparisons, we should normalize the histogram so that the sum of its values is 1.0 or, in other words, each value represents the <span class="strong"><em class="calibre10">proportion</em></span> of pixels belonging to the given color bin. We can use the following code to perform this type of normalization:</p><div class="informalexample"><pre class="programlisting">normalizedHist = cv2.normalize(hist, norm_type=cv2.NORM_L1)</pre></div><p class="calibre8">Then, to obtain a similarity measurement for two normalized histograms, we can use code such as the following:</p><div class="informalexample"><pre class="programlisting">method = cv2.HISTCMP_INTERSECT  # A method of comparison
similarity = cv2.compareHist(
  normalizedHist, otherNormalizedHist, method)</pre></div><p class="calibre8">For the <code class="email">HISTCMP_INTERSECT</code> method, the similarity is the sum of the per-element minimums of the two histograms. If we consider the histograms as two curves, this value measures the intersecting area beneath the curves.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note34" class="calibre1"/>Note</h3><p class="calibre8">For a list of all the supported methods of histogram comparison and their mathematical definitions, see the official documentation at <a class="calibre1" href="http://docs.opencv.org/3.0-beta/modules/imgproc/doc/histograms.html#comparehist">http://docs.opencv.org/3.0-beta/modules/imgproc/doc/histograms.html#comparehist</a>.</p></div><p class="calibre8">We will <a id="id248" class="calibre1"/>build a camera trap that uses histogram similarity as a trigger. When the histogram of the webcam's image is sufficiently similar to the histogram of a reference image, we will activate the photo camera. The reference image could be a colorful landscape (if we are interested in all the colors of the landscape), or it could be a tightly cropped photo of a colorful object (if we are interested in just the object's colors, regardless of the surroundings). Consider the following examples of tightly cropped photos:</p><div class="mediaobject"><img src="../images/00027.jpeg" alt="Detecting a colorful subject" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The<a id="id249" class="calibre1"/> first image (left) shows an orange jacket, which is a common piece of outdoor clothing during hunting season. (The intense, warm color makes the wearer more visible, reducing the risk of hunting accidents.) This is potentially a good reference image if we want to detect people in the woods. The second image (right) shows an alpine poppy with red petals and yellow stamen. This may be a good reference image if we want to detect a flower when it opens.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note35" class="calibre1"/>Note</h3><p class="calibre8">These<a id="id250" class="calibre1"/> and other colorful images can be found in the book's GitHub repository at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_2/CameraTrap/media">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_2/CameraTrap/media</a>.</p></div><p class="calibre8">Let's implement the color-based camera trap in a new script called <code class="email">set_color_trap.py</code>. Much of the code will be similar to <code class="email">set_motion_trap.py</code>, but we will cover the differences here.</p><p class="calibre8">Under some circumstances, <code class="email">set_color_trap.py</code> will print error messages to <code class="email">stderr</code>. For this functionality, Python 2 and Python 3 have different syntax. We will add the following import statement for compatibility, to make Python 3's <code class="email">print</code> syntax available even if we are running Python 2:</p><div class="informalexample"><pre class="programlisting">from __future__ import print_function</pre></div><p class="calibre8">Our script's command-line arguments will include the path to the reference image, and a similarity threshold, which will determine the trap's sensitivity. Here are the definitions of the<a id="id251" class="calibre1"/> arguments:</p><div class="informalexample"><pre class="programlisting">def main():

  parser = argparse.ArgumentParser(
    description='This script detects colors using an '
                'attached webcam. When it detects colors '
                'that match the histogram of a reference '
                'image, it captures photos on an '
                'attached gPhoto2-compatible photo '
                'camera.')

  # ...

  parser.add_argument(
    '--reference-image', type=str, required=True,
    help='path to reference image, whose colors will be '
         'detected in scene')
  parser.add_argument(
    '--min-similarity', type=float, default=0.02,
    help='similarity score that histogram comparator '
         'must find in order to trigger similarity event '
         '(default=0.02, valid_range=[0.0, 1.0])')

  # ...</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note36" class="calibre1"/>Note</h3><p class="calibre8">To<a id="id252" class="calibre1"/> read the omitted sections of this script, go to the book's GitHub repository at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/chapter_2/CameraTrap/set_color_trap.py">https://github.com/OpenCVBlueprints/OpenCVBlueprints/chapter_2/CameraTrap/set_color_trap.py</a>.</p></div><p class="calibre8">We will parse the arguments and try to load the reference image from file. If the image cannot be loaded, the script will print an error message and exit prematurely, as the following code shows:</p><div class="informalexample"><pre class="programlisting">  args = parser.parse_args()

  # ...

  reference_image = cv2.imread(args.reference_image,
                               cv2.IMREAD_COLOR)
  if reference_image is None:
    print('Failed to read reference image: %s' %
          args.reference_image, file=sys.stderr)
    return

  min_similarity = args.min_similarity

  # ...</pre></div><p class="calibre8">We will create a normalized histogram of the reference image, and later, we will also create a normalized histogram of each frame from the webcam. To help with the creation of a normalized<a id="id253" class="calibre1"/> histogram, we will define another function locally. (Python allows nested function definitions.) Here is the relevant code:</p><div class="informalexample"><pre class="programlisting">  # ...

  channels = range(3)
  hist_size = [256] * 3
  ranges = [0, 255] * 3

  def create_normalized_hist(image, hist=None):
    hist = cv2.calcHist(
      [image], channels, None, hist_size, ranges, hist)
    return cv2.normalize(hist, hist, norm_type=cv2.NORM_L1)

  reference_hist = create_normalized_hist(reference_image)
  query_hist = None

  # ...</pre></div><p class="calibre8">To reiterate, every time we capture a frame from the webcam, we will find its normalized histogram. Then, we will measure the similarity of the reference histogram and the current scene's histogram based on the <code class="email">HISTCMP_INTERSECT</code> method of comparison, meaning that we simply want to calculate the histograms' intersection or overlapping area. If the similarity is equal to or greater than the threshold, we will begin to capture photos.</p><p class="calibre8">Here is the main loop's implementation:</p><div class="informalexample"><pre class="programlisting">  while True:
    time.sleep(detection_interval)
    success, bgr = cap.read(bgr)
    if success:
      query_hist = create_normalized_hist(
        bgr, query_hist)
      similarity = cv2.compareHist(
        reference_hist, query_hist, cv2.HISTCMP_INTERSECT)
      if debug:
        print('similarity=%f' % similarity)
      if similarity &gt;= min_similarity and not cc.capturing:
        if photo_ev_step is not None:
          cc.capture_exposure_bracket(photo_ev_step, photo_count)
        else:
          cc.capture_time_lapse(photo_interval, photo_count)</pre></div><p class="calibre8">This concludes the <code class="email">main()</code> function. Again, to ensure that <code class="email">main()</code> is called when the script is executed, we will add the following code:</p><div class="informalexample"><pre class="programlisting">if __name__ == '__main__':
  main()</pre></div><p class="calibre8">Make the script executable. Then, for example, we can run it like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ ./set_color_trap.py --reference-image media/OrangeCoat.jpg --min-similarity 0.13 --width 640 --height 480 --debug True</strong></span>
</pre></div><p class="calibre8">See the <a id="id254" class="calibre1"/>pair of the following images. The left-hand image shows the physical setup of the camera trap, which is running <code class="email">set_color_trap.py</code> with the custom parameters that we just noted. The right-hand image is one of the resulting photos:</p><div class="mediaobject"><img src="../images/00028.jpeg" alt="Detecting a colorful subject" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Again, these images come from different cameras, which give different renditions of the scene's color and contrast.</p><p class="calibre8">You may wish to experiment with <code class="email">set_color_trap</code>'s arguments, especially the reference image and similarity threshold. Note that the <code class="email">HISTCMP_INTERSECT</code> method of comparison tends to produce low similarities, so the default threshold is just 0.02, or a 2% overlap <a id="id255" class="calibre1"/>of the histograms. If you modify the code to use a different method of comparison, you may need a much higher threshold, and the maximum similarity may exceed 1.0.</p><p class="calibre8">Once you finish testing the color-based camera trap, let's proceed to use face detection as our final kind of trigger.</p></div></div>

<div class="book" title="Detecting the presence of a photogenic subject">
<div class="book" title="Detecting the face of a mammal"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch02lvl2sec15" class="calibre1"/>Detecting the face of a mammal</h2></div></div></div><p class="calibre8">As you<a id="id256" class="calibre1"/> probably know, OpenCV's <code class="email">CascadeClassifier</code> class is useful for face detection and other kinds of object detection, using a model of the object's features called a cascade, which is loaded from an XML file. We used <code class="email">CascadeClassifier</code> and <code class="email">haarcascade_frontalface_alt.xml</code> for human face detection in the section <span class="strong"><em class="calibre10">Supercharging the GS3-U3-23S6M-C and other Point Grey Research cameras</em></span> of <a class="calibre1" title="Chapter 1. Getting the Most out of Your Camera System" href="part0015_split_000.html#E9OE2-940925703e144daa867f510896bffb69">Chapter 1</a>, <span class="strong"><em class="calibre10">Getting the Most out of Your Camera System</em></span>. Later in this book, in <a class="calibre1" title="Chapter 5. Generic Object Detection for Industrial Applications" href="part0043_split_000.html#190862-940925703e144daa867f510896bffb69">Chapter 5</a>, <span class="strong"><em class="calibre10">Generic Object Detection for Industrial Applications</em></span>, we will examine all of <code class="email">CascadeClassifier</code>'s functionality, along with a set of tools to create a cascade for any kind of object. For now, we will continue to use pretrained cascades that come with OpenCV. Notably, OpenCV offers the following cascade files for human and cat face detection:</p><div class="book"><ul class="itemizedlist"><li class="listitem">For human frontal faces:<div class="book"><ul class="itemizedlist1"><li class="listitem"><code class="email">data/haarcascades/haarcascade_frontalface_default.xml</code></li><li class="listitem"><code class="email">data/haarcascades/haarcascade_frontalface_alt.xml</code></li><li class="listitem"><code class="email">data/haarcascades/haarcascade_frontalface_alt2.xml</code></li><li class="listitem"><code class="email">data/lbpcascades/lbpcascade_frontalface.xml</code></li></ul></div></li><li class="listitem">For human profile faces:<div class="book"><ul class="itemizedlist1"><li class="listitem"><code class="email">data/haarcascades/haarcascade_profileface.xml</code></li><li class="listitem"><code class="email">data/lbpcascades/lbpcascade_profileface.xml</code></li></ul></div></li><li class="listitem">For cat frontal faces:<div class="book"><ul class="itemizedlist1"><li class="listitem"><code class="email">data/haarcascades/haarcascade_frontalcatface.xml</code></li><li class="listitem"><code class="email">data/haarcascades/haarcascade_frontalcatface_extended.xml</code></li><li class="listitem"><code class="email">data/lbpcascades/lbpcascade_frontalcatface.xml</code></li></ul></div></li></ul></div><p class="calibre8">LBP cascades are faster but slightly less accurate than Haar cascades. The extended version of Haar cascades (as used in <code class="email">haarcascade_frontalcatface_extended.xml</code>) is sensitive to both horizontal and diagonal features, whereas standard Haar cascades are only sensitive to horizontal features. For<a id="id257" class="calibre1"/> example, a cat's whiskers could register as diagonal features.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note37" class="calibre1"/>Note</h3><p class="calibre8"><a class="calibre1" title="Chapter 5. Generic Object Detection for Industrial Applications" href="part0043_split_000.html#190862-940925703e144daa867f510896bffb69">Chapter 5</a>, <span class="strong"><em class="calibre10">Generic Object Detection for Industrial Applications</em></span>, in this book, will discuss types of cascades in detail. Also, for a complete tutorial on how OpenCV's cat cascades were trained, see <a class="calibre1" title="Chapter 3. Recognizing Facial Expressions with Machine Learning" href="part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69">Chapter 3</a>, <span class="strong"><em class="calibre10">Training a Smart Alarm to Recognize the Villain and His Cat</em></span>, in the book <span class="strong"><em class="calibre10">OpenCV for Secret Agents</em></span>, by Joseph Howse (Packt Publishing, 2015).</p></div><p class="calibre8">Incidentally, the cat face detection cascades may also detect other mammal faces. The following images are visualizations of the detection results using <code class="email">haarcascade_frontalcatface_extended.xml</code> on photos of a cat (left), a red panda (upper right), and a lynx (lower right):</p><div class="mediaobject"><img src="../images/00029.jpeg" alt="Detecting the face of a mammal" class="calibre11"/></div><p class="calibre12"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note38" class="calibre1"/>Note</h3><p class="calibre8">The photos <a id="id258" class="calibre1"/>of the red panda and lynx are by Mathias Appel, who has generously released these and many other images into the public domain. See his Flickr page at <a class="calibre1" href="https://www.flickr.com/photos/mathiasappel/">https://www.flickr.com/photos/mathiasappel/</a>.</p></div><p class="calibre8">Let's implement the classification-based <a id="id259" class="calibre1"/>camera trap in a new script called <code class="email">set_classifier_trap.py</code>. The necessary imports are the same as for <code class="email">set_color_trap.py</code>. The command-line arguments for <code class="email">set_classifier_trap.py</code> include the path to the cascade file as well as other parameters that affect the use of <code class="email">CascadeClassifer</code>. Here is the relevant code:</p><div class="informalexample"><pre class="programlisting">def main():

  parser = argparse.ArgumentParser(
    description='This script detects objects using an '
                'attached webcam. When it detects '
                'objects that match a given cascade '
                'file, it captures photos on an attached '
                'gPhoto2-compatible photo camera.')

  # ...

  parser.add_argument(
    '--cascade-file', type=str, required=True,
    help='path to cascade file that classifier will use '
         'to detect objects in scene')
  parser.add_argument(
    '--scale-factor', type=float, default=1.05,
    help='relative difference in scale between '
         'iterations of multi-scale classification '
         '(default=1.05)')
  parser.add_argument(
    '--min-neighbors', type=int, default=8,
    help='minimum number of overlapping objects that '
         'classifier must detect in order to trigger '
         'classification event (default=8)')
  parser.add_argument(
    '--min-object-width', type=int, default=40,
    help='minimum width of each detected object'
         '(default=40)')
  parser.add_argument(
    '--min-object-height', type=int, default=40,
    help='minimum height of each detected object'
         '(default=40)')

  # ...</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note39" class="calibre1"/>Note</h3><p class="calibre8">To<a id="id260" class="calibre1"/> read the omitted sections of this script, go to the book's GitHub repository at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/chapter_2/CameraTrap/set_classifier_trap.py">https://github.com/OpenCVBlueprints/OpenCVBlueprints/chapter_2/CameraTrap/set_classifier_trap.py</a>.</p></div><p class="calibre8">After parsing the arguments as usual, we <a id="id261" class="calibre1"/>will initialize an instance of <code class="email">CascadeClassifier</code> with the specified cascade file. If the file failed to load, we will print an error message and exit the script prematurely. See the following code:</p><div class="informalexample"><pre class="programlisting">  args = parser.parse_args()

  # ...

  classifier = cv2.CascadeClassifier(args.cascade_file)
  if classifier.empty():
    print('Failed to read cascade file: %s' %
          args.cascade_file, file=sys.stderr)
    return

  scale_factor = args.scale_factor
  min_neighbors = args.min_neighbors
  min_size = (args.min_object_width, args.min_object_height)

  # ...</pre></div><p class="calibre8">On each iteration of the script's main loop, we will convert the webcam image to an equalized black and white version, which we will pass to the <code class="email">CascadeClassifier</code>'s <code class="email">detectMultiScale</code> method. We will use some of the command-line arguments as additional parameters to control the sensitivity of <code class="email">detectMultiScale</code>. If at least one face (or other relevant object) is detected, we will start to capture photos, as usual. Here is the loop's implementation:</p><div class="informalexample"><pre class="programlisting">  while True:
    time.sleep(detection_interval)
    success, bgr = cap.read(bgr)
    if success:
      gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY, gray)
      gray = cv2.equalizeHist(gray, gray)
      objects = classifier.detectMultiScale(
        gray, scaleFactor=scale_factor,
        minNeighbors=min_neighbors, minSize=min_size)
      num_objects = len(objects)
      if debug:
        print('num_objects=%d' % num_objects)
      if num_objects &gt; 0 and not cc.capturing:
        if photo_ev_step is not None:
          cc.capture_exposure_bracket(photo_ev_step, photo_count)
        else:
          cc.capture_time_lapse(photo_interval, photo_count)</pre></div><p class="calibre8">This completes the <code class="email">main()</code> function, and all <a id="id262" class="calibre1"/>that remains is to call <code class="email">main()</code> when the script executes, as usual:</p><div class="informalexample"><pre class="programlisting">if __name__ == '__main__':
  main()</pre></div><p class="calibre8">Make the script executable. Then, for example, we can run it like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ ./set_classifier_trap.py --cascade-file cascades/haarcascade_frontalcatface_extended.xml --min-neighbors 16 --scale-factor 1.2 --width 640 --height 480 --debug True</strong></span>
</pre></div><p class="calibre8">Refer to the following set of images. The left-hand image shows the physical setup of the camera trap, which is running <code class="email">set_classifier_trap.py</code> with the custom parameters that we just noted. The right-hand images are two of the resulting photos:</p><div class="mediaobject"><img src="../images/00030.jpeg" alt="Detecting the face of a mammal" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The left-hand image and right-hand images <a id="id263" class="calibre1"/>come from two different cameras, so the color and contrast differ. Also, the two right-hand images come from separate runs of <code class="email">set_classifier_trap.py</code>, and the lighting conditions and camera position have changed very slightly.</p><p class="calibre8">Feel free to experiment with the arguments of <code class="email">set_classifier_trap.py</code>. You might even want to create your own cascade files to detect different kinds of faces or objects. <a class="calibre1" title="Chapter 5. Generic Object Detection for Industrial Applications" href="part0043_split_000.html#190862-940925703e144daa867f510896bffb69">Chapter 5</a>, <span class="strong"><em class="calibre10">Generic Object Detection for Industrial Applications</em></span>, will provide a wealth of information to help you do more with <code class="email">CascadeClassifier</code> and cascade files.</p><p class="calibre8">Next, we will consider ways to process the photos that we may capture with any of our scripts, or with simple gPhoto2 commands.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Processing images to show subtle colors and motion" id="OPEK1-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec20" class="calibre1"/>Processing images to show subtle colors and motion</h1></div></div></div><p class="calibre8">By now, you<a id="id264" class="calibre1"/> have probably captured some exposure-bracketed photos and time-lapse photos. Upload them onto your computer using a photo management application, a file browser, or the following gPhoto2 command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">$ gphoto2 --get-all-files</strong></span>
</pre></div><p class="calibre8">The latter command will upload the files to the current working directory.</p><p class="calibre8">We will merge exposure-bracketed photos to create HDR images, which will improve color rendition in shadows and highlights. Similarly, we will merge time-lapse photos to create time-lapse videos, which will show gradual motion on an accelerated scale. We will start by<a id="id265" class="calibre1"/> processing some of the sample photos from the book's GitHub repository at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_2/CameraTrap/media">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_2/CameraTrap/media</a>, and then you will be able to adapt the code to use your photos instead.</p></div>

<div class="book" title="Processing images to show subtle colors and motion" id="OPEK1-940925703e144daa867f510896bffb69">
<div class="book" title="Creating HDR images"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec16" class="calibre1"/>Creating HDR images</h2></div></div></div><p class="calibre8">OpenCV 3 has <a id="id266" class="calibre1"/>a new module called "photo". Two of its classes, <code class="email">MergeDebevec</code> and <code class="email">MergeMertens</code>, create an HDR image by merging exposure-bracketed <a id="id267" class="calibre1"/>photos. Regardless of which class is used, the resulting HDR image has channel values in the range [0.0, 1.0]. <code class="email">MergeDebevec</code> produces an HDR image that requires gamma correction before it can be displayed or printed. The photo <a id="id268" class="calibre1"/>module provides several <span class="strong"><strong class="calibre9">tone mapping</strong></span> functions that are capable of performing the correction.</p><p class="calibre8">On the other hand, the HDR image from <code class="email">MergeMertens</code> does not require gamma correction. Its channel values just need to be scaled up to the range [0, 255]. We will use <code class="email">MergeMertens</code> because it is simpler and tends to be better at preserving color saturation.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note40" class="calibre1"/>Note</h3><p class="calibre8">For <a id="id269" class="calibre1"/>more information about HDR imaging and tone mapping in OpenCV 3, see the official documentation at <a class="calibre1" href="http://docs.opencv.org/3.0-beta/modules/photo/doc/hdr_imaging.html">http://docs.opencv.org/3.0-beta/modules/photo/doc/hdr_imaging.html</a>. Also, see the official tutorial at <a class="calibre1" href="http://docs.opencv.org/3.0-beta/doc/tutorials/photo/hdr_imaging/hdr_imaging.html">http://docs.opencv.org/3.0-beta/doc/tutorials/photo/hdr_imaging/hdr_imaging.html</a>.</p><p class="calibre8">The <code class="email">MergeDebevec</code> and <code class="email">MergeMertens</code> classes are based on the following papers, respectively:</p><p class="calibre8">P. Debevec, and J. Malik, <span class="strong"><em class="calibre10">Recovering High Dynamic Range Radiance Maps from Photographs</em></span>, Proceedings OF ACM SIGGRAPH, 1997, 369 - 378.</p><p class="calibre8">T. Mertens, J. Kautz, and F. Van Reeth, <span class="strong"><em class="calibre10">Exposure Fusion</em></span>, Proceedings of the 15th Pacific Conference on Computer Graphics and Applications, 2007, 382 - 390.</p></div><p class="calibre8">For demonstration purposes, the GitHub repository contains a pair of exposure-bracketed photos of a cat named Plasma. (Her photos and the HDR merged version appear earlier in this chapter, in<a id="id270" class="calibre1"/> the section <span class="strong"><em class="calibre10">Planning the camera trap</em></span>.) Let's create <a id="id271" class="calibre1"/>a script, <code class="email">test_hdr_merge.py</code>, to merge the unprocessed photos, <code class="email">media/PlasmaWink_0.jpg</code> and <code class="email">media/PlasmaWink_1.jpg</code>. Here is the implementation:</p><div class="informalexample"><pre class="programlisting">#!/usr/bin/env python

import cv2

def main():

  ldr_images = [
    cv2.imread('media/PlasmaWink_0.jpg'),
    cv2.imread('media/PlasmaWink_1.jpg')]

  hdr_processor = cv2.createMergeMertens()
  hdr_image = hdr_processor.process(ldr_images) * 255
  cv2.imwrite('media/PlasmaWink_HDR.jpg', hdr_image)

if __name__ == '__main__':
  main()</pre></div><p class="calibre8">Obtain the script and media from the repository, run the script, and view the resulting HDR image. Then, adapt the script to process your own exposure-bracketed photos. HDR can produce dramatic results for any scene that has intense light and deep shadows. Landscapes and sunlit rooms are good examples.</p><p class="calibre8">With HDR imaging, we have compressed differences in exposure. Next, with time-lapse videography, we will compress differences in time.</p></div></div>

<div class="book" title="Processing images to show subtle colors and motion" id="OPEK1-940925703e144daa867f510896bffb69">
<div class="book" title="Creating time-lapse videos"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec17" class="calibre1"/>Creating time-lapse videos</h2></div></div></div><p class="calibre8">Previously, in the section <span class="strong"><em class="calibre10">Supercharging the PlayStation Eye</em></span> in <a class="calibre1" title="Chapter 1. Getting the Most out of Your Camera System" href="part0015_split_000.html#E9OE2-940925703e144daa867f510896bffb69">Chapter 1</a>, <span class="strong"><em class="calibre10">Getting the Most out of Your Camera System</em></span>, we created a slow-motion video. Remember that we simply <a id="id272" class="calibre1"/>captured images at a high speed (187 FPS) and <a id="id273" class="calibre1"/>put them in a video that was configured to play at a normal speed (60 FPS). Similarly, to create a time-lapse video, we will read image files that were captured at a low speed (less than 1 FPS) and put them in a video that is configured to play at a normal speed (60 FPS).</p><p class="calibre8">For demonstration purposes, the book's GitHub repository contains a set of time-lapse photographs of a cat named Josephine. When we make a time-lapse video of Josephine, we will see that she is very dynamic, even when she is sitting in a chair! As a preview, here are three consecutive frames of the time lapse:</p><div class="mediaobject"><img src="../images/00031.jpeg" alt="Creating time-lapse videos" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The series<a id="id274" class="calibre1"/> spans 56 photos with names ranging<a id="id275" class="calibre1"/> from <code class="email">media/JosephineChair_00.jpg</code> to <code class="email">media/JosephineChair_55.jpg</code>. The following script, which we will call <code class="email">test_time_lapse_merge.py</code>, will read the photos and produce a one-second time-lapse video named <code class="email">media/JosephineChair_TimeLapse.avi</code>:</p><div class="informalexample"><pre class="programlisting">#!/usr/bin/env python

import cv2

def main():

  num_input_files = 56
  input_filename_pattern = 'media/JosephineChair_%02d.jpg'
  output_filename = 'media/JosephineChair_TimeLapse.avi'
  fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')
  fps = 60.0
  writer = None

  for i in range(num_input_files):
    input_filename = input_filename_pattern % i
    image = cv2.imread(input_filename)
    if writer is None:
      is_color = (len(image.shape) &gt; 2)
      h, w = image.shape[:2]
      writer = cv2.VideoWriter(
        output_filename, fourcc, fps, (w, h), is_color)
    writer.write(image)

if __name__ == '__main__':
    main()</pre></div><p class="calibre8">Obtain the script and media from the repository, run the script, and view the resulting video of Josephine watching the world from her chair. Then, adapt the script to process some of your own images. Perhaps you will capture the motion of other slow animals, flowers as they bloom, or sunlight and clouds as they cross a landscape.</p><p class="calibre8">As a further <a id="id276" class="calibre1"/>project, you may wish to create HDR<a id="id277" class="calibre1"/> time-lapse videos. You could start by modifying our <code class="email">capture_exposure_bracket.sh</code> script to capture multiple batches of exposure-bracketed images, with a time delay between each batch. (For example, the command <code class="email">sleep 3</code> could be used to delay for 3 seconds.) After uploading the captured images onto your computer, you can merge each batch into an HDR image, and then merge the HDR images into a time-lapse video.</p><p class="calibre8">Explore other photographic techniques, and then try to automate them!</p></div></div>
<div class="book" title="Further study" id="PNV61-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec21" class="calibre1"/>Further study</h1></div></div></div><p class="calibre8">Computational photography is a diverse and popular field, which combines the work of artists, technicians, and scientists. Thus, there are many types of authors, instructors, and mentors who can help you become a better "computational photographer". Here are just a few examples of helpful guides:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre10">Learning Image Processing with OpenCV</em></span>, by Gloria Bueno García et al (Packt Publishing, 2015), covers a wide range of OpenCV 3's capabilities with respect to image capture, image editing, and computational photography. The book uses C++, and is suitable for beginners in computer vision.</li><li class="listitem">The <span class="strong"><em class="calibre10">National Geographic Masters of Photography</em></span> video lectures (The Great Courses, 2015) provide great insight into the goals and techniques of master photographers. Several of the lecturers are wildlife photographers, whose use of camera traps was an inspiration for this chapter.</li><li class="listitem"><span class="strong"><em class="calibre10">OpenSource Astrophotography</em></span>, by Karl Sarnow (CreateSpace Independent Publishing Platform, 2013), covers the use of gPhoto2 and other open source software, along with photographic hardware, to capture and process detailed images of the night sky.</li><li class="listitem"><span class="strong"><em class="calibre10">Science for the Curious Photographer</em></span>, by Charles S. Johnson, Jr. (CRC Press, 2010), explains the scientific history and principles of light, lenses, and photography. Moreover, it provides practical solutions to common photographic problems, such as selecting and setting up good equipment for macro photography.</li></ul></div><p class="calibre8">Whether as a hobby or a profession, computational photography is a great way to explore and chronicle the world from a particular viewpoint. It requires observation, experimentation, and patience, so slow down! Take time to learn from other people's explorations, and to share yours.</p></div>
<div class="book" title="Summary" id="QMFO1-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec22" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">This chapter has demonstrated a set of surprisingly flexible commands and classes, which enable us to conduct experiments in computational photography, with short and simple code. We have written scripts to control a photo camera. Along the way, we have acquainted ourselves with gPhoto2, the Bash shell, PTP communication, GVFS mount points, and Python's support for subprocesses. We have also scripted several variations of a photo trap to take pictures when a subject comes into view. For this, OpenCV has provided us with the capability to detect motion, measure color similarities, and classify objects. Finally, we have used OpenCV to combine a set of photos into a time-lapse video or HDR image.</p><p class="calibre8">So far, this book has provided a fairly broad survey of ways to capture light as data, control a camera, detect a subject, and process a photo. The remaining chapters will focus on a selection of advanced techniques, which will enable us to perform much finer classification and identification of an image's subject, and to process photos and videos in ways that account for camera motion and perspective.</p></div></body></html>