<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Should I Take the Job â€“ Decision Trees in Action</h1>
                </header>
            
            <article>
                
<p>This chapter will focus on:</p>
<ul>
<li>Decision trees</li>
<li>How to create decision trees for your application</li>
<li>Understanding truth tables</li>
<li>Visual intuition regarding false negatives and false positives</li>
</ul>
<p>Before we dive right in, let's gain some background information which will be helpful to us.</p>
<p>For a decision tree to be complete and effective, it must contain all possibilities, meaning every pathway in and out. Event sequences must also be supplied and be mutually exclusive, meaning if one event happens, the other one cannot.</p>
<p>Decision trees are a form of <strong>supervised machine learning</strong>, in that we have to explain what the input and output should be. There are decision nodes and leaves. The leaves are the decisions, final or not, and the nodes are where decision splits occur.</p>
<p>Although there are many algorithms available for our use, we are going to use the <span><strong>Iterative Dichotomizer 3</strong> (<strong>I</strong></span><strong>D3</strong>) algorithm. During each recursive step, the attribute that best classifies the set of inputs we are working with is selected according to a criterion <strong>(InfoGain</strong>, <strong>GainRatio</strong>, and so on). It must be pointed out that regardless of the algorithm that we use, none are guaranteed to produce the smallest tree possible. This has a direct implication on the performance of our algorithm. Keep in mind that with decision trees, learning is based solely on heuristics, not true optimized criteria. Let's use an example to explain this further.</p>
<p>The following example is from <a href="http://jmlr.csail.mit.edu/papers/volume8/esmeir07a/esmeir07a.pdf" target="_blank">http://jmlr.csail.mit.edu/papers/volume8/esmeir07a/esmeir07a.pdf</a>, and it illustrates the XOR learning concept, which all of us developers are (or should be) familiar with. You will see this happen in a later example as well, but for now, <strong>a<sub>3</sub></strong> and <strong>a<sub>4</sub></strong> are completely irrelevant to the problem we are trying to solve. They have zero influence on our answer. That being said, the ID3 algorithm will select one of them to belong to the tree, and in fact, it will use <strong>a<sub>4</sub></strong> as the root! Remember that this is heuristic learning of the algorithm and not optimized findings:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ecc3b212-9667-450d-a320-6daea4b9b330.png" style=""/></div>
<p>Hopefully this visual will make it easier to understand what we mean. The goal here isn't to get too deep into decision tree mechanics and theory. After all of that, you might be asking why we are even talking about decision trees. Despite any issues they may have, decision trees work as the base for many algorithms, especially those that need a human description of the results. They are also the basis for the Viola &amp; Jones (2001) real-time facial detection algorithm we used in an earlier chapter. As perhaps a better example, the <strong>Kinect of Microsoft Xbox 360</strong> uses decision trees as well.</p>
<p>Once again, we will turn to the Accord.NET open source framework to illustrate our concept. In our sample, we'll be dealing with the following decision tree objects, so it's best that we discuss them upfront.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision tree</h1>
                </header>
            
            <article>
                
<p>This is our main class.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision node</h1>
                </header>
            
            <article>
                
<p>This is a node for our decision tree. Each node may or may not have children associated with it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision variable</h1>
                </header>
            
            <article>
                
<p>This object defines the nature of each decision variable that the tree and nodes can process. The values can be ranges, continuous, or discrete.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision branch node collection</h1>
                </header>
            
            <article>
                
<p>This collection contains decision nodes grouped together with additional information about their decision variables for comparison.</p>
<p>Here is an example of a decision tree for determining financial risks. It is very easy to follow simply by navigating the nodes, making a decision as to which way to go, until you get to the final answer. In this case, someone is applying for credit and we need to make a decision on their credit worthiness. A decision tree is a great approach to solving this problem:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-636 image-border" src="assets/6259d852-b213-4ae2-b21d-8336f635d200.png" style=""/></div>
<p>With this simplistic visual diagram behind us, let's visit the problem we are trying to solve. It's one that all of us developers face (hopefully) from time to time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Should I take the job?</h1>
                </header>
            
            <article>
                
<p>You have just been offered a new job, and you need to decide whether or not you are going to take it. There are a few things that are important for you to consider, so we will use those as input variables, or features, for our decision tree. Here is what is important to you: pay, benefits, company culture, and of course, <em>Can I work from home?</em></p>
<p>Rather than load data from disk storage, we are going to create an in-memory database and add our features this way. We will create <kbd>DataTable</kbd> and create <kbd>Columns</kbd> as features as shown here:</p>
<pre>DataTable data = new DataTable("Should I Go To Work For Company<br/>  X");<br/>data.Columns.Add("Scenario");<br/>data.Columns.Add("Pay");<br/>data.Columns.Add("Benefits");<br/>data.Columns.Add("Culture");<br/>data.Columns.Add("WorkFromHome");<br/>data.Columns.Add("ShouldITakeJob");</pre>
<p class="mce-root">After this, we will load several rows of data, each with a different set of features, and our last column, <kbd>ShouldITakeJob</kbd> being a <kbd>Yes</kbd> or <kbd>No</kbd>, as our final decision:</p>
<pre>data.Rows.Add("D1", "Good", "Good", "Mean", "Yes", "Yes");<br/>data.Rows.Add("D2", "Good", "Good", "Mean", "No", "Yes");<br/>data.Rows.Add("D3", "Average", "Good", "Good", "Yes", "Yes");<br/>data.Rows.Add("D4", "Average", "Good", "Good", "No", "Yes");<br/>data.Rows.Add("D5", "Bad", "Good", "Good", "Yes", "No");<br/>data.Rows.Add("D6", "Bad", "Good", "Good", "No", "No");<br/>data.Rows.Add("D7", "Good", "Average", "Mean", "Yes", "Yes");<br/>data.Rows.Add("D8", "Good", "Average", "Mean", "No", "Yes");<br/>data.Rows.Add("D9", "Average", "Average", "Good", "Yes", "No");<br/>data.Rows.Add("D10", "Average", "Average", "Good", "No", "No");<br/>data.Rows.Add("D11", "Bad", "Average", "Good", "Yes", "No");<br/>data.Rows.Add("D12", "Bad", "Average", "Good", "No", "No");</pre>
<pre>data.Rows.Add("D13", "Good", "Bad", "Mean", "Yes", "Yes");<br/>data.Rows.Add("D14", "Good", "Bad", "Mean", "No", "Yes");<br/>data.Rows.Add("D15", "Average", "Bad", "Good", "Yes", "No");<br/>data.Rows.Add("D16", "Average", "Bad", "Good", "No", "No");<br/>data.Rows.Add("D17", "Bad", "Bad", "Good", "Yes", "No");<br/>data.Rows.Add("D18", "Bad", "Bad", "Good", "No", "No");<br/>data.Rows.Add("D19", "Good", "Good", "Good", "Yes", "Yes"); data.Rows.Add("D20", "Good", "Good", "Good", "No", "Yes");</pre>
<p>Once all of the data is created and in our table, we need to put our previous features into a form of representation that our computer can understand. Since all our features are categories, it doesn't really matter how we represent them, if we are consistent. Since numbers are easier, we will convert our features (categories) to a code book through a process known as <kbd>Codification</kbd>. This <kbd>codebook</kbd> effectively transforms every value into an integer. Notice that we will pass in our <kbd>data</kbd> categories as input:</p>
<pre>Codification codebook = new Codification(data);</pre>
<p>Next, we need to create decision variables for our decision tree to use. The tree will be trying to help us determine if we should take our new job offer or not. For this decision, there will be several categories of inputs, which we will specify in our decision variable array, and two possible decisions, <kbd>Yes</kbd> or <kbd>No</kbd>.</p>
<p>The <kbd>DecisionVariable</kbd> array will hold the name of each category as well as the total count of possible attributes for this category. For example, the <kbd>Pay</kbd> category has three possible values, <kbd>Good</kbd>, <kbd>Average</kbd>, or  <kbd>Poor</kbd>. So, we specify the category name and the number <kbd>3</kbd>. We then repeat this for all our other categories except the last one, which is our decision:</p>
<pre>DecisionVariable[] attributes =<br/>{<br/>  new DecisionVariable("Pay", 3),<br/>  new DecisionVariable("Benefits", 3),<br/>  new DecisionVariable("Culture", 3),<br/>  new DecisionVariable("WorkFromHome", 2)<br/>};<br/>int outputValues = 2; // 2 possible output values: yes or no<br/>DecisionTree tree = new DecisionTree(attributes, outputValues);</pre>
<p>Now that we have our decision tree created, we have to teach it the problem we are trying to solve. At this point, it really knows nothing. In order to do that, we will have to create a learning algorithm for the tree to use. In our case, that would be the ID3 algorithm we discussed earlier. Since we have only categorical values for this sample, the ID3 algorithm is the simplest choice. Please feel free to replace it with C4.5, C5.0, or whatever you want to try:</p>
<pre>ID3Learning id3 = new ID3Learning(tree);<br/>Now, with our tree fully created and ready to go, we <br/>  are ready to classify new samples.<br/>// Translate our training data into integer symbols using our codebook:<br/>DataTable symbols = codebook.Apply(data);<br/>int[][] inputs = symbols.ToArray&lt;int&gt;("Pay", "Benefits", "Culture",<br/>  "WorkFromHome");<br/>int[] outputs = symbols.ToIntArray("ShouldITakeJob").GetColumn(0);<br/>// Learn the training instances!<br/>id3.Run(inputs, outputs);</pre>
<p>Once the learning algorithm has been run, it is trained and ready to use. We simply provide a sample dataset to the algorithm so that it can give us back an answer. In this case, the pay is good, the company culture is good, the benefits are good, and I can work from home. If the decision tree is trained properly, the answer is a resounding <kbd>Yes</kbd>:</p>
<pre>int[] query = codebook.Translate("Good", "Good", "Good", "Yes");<br/>int output = tree.Compute(query);<br/>string answer = codebook.Translate("ShouldITakeJob", output); <br/>// answer   will be "Yes".</pre>
<p>Next, we will turn our attention to using the <kbd>numl</kbd> open source machine learning package to show you another example of training and using a decision tree.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">numl</h1>
                </header>
            
            <article>
                
<p><kbd>numl</kbd> is a very famous open source machine learning toolkit. As with most machine learning frameworks, it too uses the <kbd>Iris</kbd> dataset for many of its examples, including the one we will use for decision trees.</p>
<p>Here is an example of our <kbd><span><span>n</span></span>uml</kbd> output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-638 image-border" src="assets/98c83b86-cbb0-45fe-9484-ca73bde79182.png" style=""/></div>
<p>Let's take a look at the code behind that example:</p>
<pre>static void Main(string[] args)<br/>{<br/>Console.WriteLine("Hello World!");<br/>var description = Descriptor.Create&lt;Iris&gt;();<br/>Console.WriteLine(description);<br/>var generator = new DecisionTreeGenerator();<br/>var data = Iris.Load();<br/>var model = generator.Generate(description, data);<br/>Console.WriteLine("Generated model:");<br/>Console.WriteLine(model);<br/>Console.ReadKey();<br/>}</pre>
<p>Definitely not the most complex of functions, is it? This is the beauty of using <kbd>numl</kbd> in your applications; it's incredibly easy to use and to integrate.</p>
<p>The preceding code creates a descriptor and <kbd>DecisionTreeGenerator</kbd>, loads the <kbd>Iris</kbd> dataset, and then generates a model. Here is just a sample of the data being loaded:</p>
<pre>public static Iris[] Load()<br/>{<br/>return new Iris[]<br/>{<br/>new Iris { SepalLength = 5.1m, SepalWidth = 3.5m, PetalLength =<br/>  1.4m, PetalWidth = 0.2m, Class = "Iris-setosa" },<br/>new Iris { SepalLength = 4.9m, SepalWidth = 3m, PetalLength = <br/>  1.4m, PetalWidth = 0.2m, Class = "Iris-setosa" },<br/>new Iris { SepalLength = 4.7m, SepalWidth = 3.2m, PetalLength =<br/>  1.3m, PetalWidth = 0.2m, Class = "Iris-setosa" },<br/>new Iris { SepalLength = 4.6m, SepalWidth = 3.1m, PetalLength = <br/>  1.5m, PetalWidth = 0.2m, Class = "Iris-setosa" },<br/>new Iris { SepalLength = 5m, SepalWidth = 3.6m, PetalLength = <br/>  1.4m, PetalWidth = 0.2m, Class = "Iris-setosa" },<br/>new Iris { SepalLength = 5.4m, SepalWidth = 3.9m, PetalLength = <br/>  1.7m, PetalWidth = 0.4m, Class = "Iris-setosa" },</pre>
<p>And so on...</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Accord.NET decision trees</h1>
                </header>
            
            <article>
                
<p>The Accord.NET framework has its own decision tree example that we should point out as well. It takes a different, more graphical approach to decision trees, but the call is yours to make as to which you like and feel most comfortable with.</p>
<p>Once the data is loaded you can create your decision tree and get it ready for learning. You will see a data plot similar to what you see here, using two categories, <span class="packt_screen">X</span> and <span class="packt_screen">Y</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-640 image-border" src="assets/82332643-96dd-4752-b9b2-d7b2ee02fec0.png" style=""/></div>
<p>The next tab will let you see the tree nodes, leafs, and decisions. There is also a top-down graphical view of the tree on the right. The most useful information is located within the <span class="packt_screen">Tree View</span> on the left, where you can see the nodes, their values, and the decisions that were made:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-643 image-border" src="assets/4d2c9930-b95d-4b85-a279-73a823e9521a.png" style=""/></div>
<p>Finally, the last tab will let you perform <span class="packt_screen">Model Testing</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-644 image-border" src="assets/53650f92-6a31-4922-939c-400023ae0da3.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning code</h1>
                </header>
            
            <article>
                
<p>Following is the learning code:</p>
<pre>// Specify the input variables<br/>DecisionVariable[] variables =<br/>{<br/>new DecisionVariable("x", DecisionVariableKind.<strong>Continuous</strong>),<br/>new DecisionVariable("y", DecisionVariableKind.<strong>Continuous</strong>),<br/>};<br/>// Create the C4.5 learning algorithm<br/>var c45 = new C45Learning(variables);<br/>// Learn the decision tree using C4.5<br/>tree = c45.Learn(inputs, outputs);<br/>// Show the learned tree in the view<br/>decisionTreeView1.TreeSource = tree;<br/>// Get the ranges for each variable (X and Y)<br/>DoubleRange[] ranges = table.GetRange(0);<br/>// Generate a Cartesian coordinate system<br/>double[][] map = Matrix.Mesh(ranges[0], <br/>  200, ranges[1], 200);<br/>// Classify each point in the Cartesian coordinate system<br/>double[,] surface = map.ToMatrix().<br/>  InsertColumn(tree.Decide(map));<br/>CreateScatterplot(zedGraphControl2, surface);<br/>//Testing<br/>// Creates a matrix from the entire source data table<br/>double[][] table = (dgvLearningSource.DataSource as<br/>  DataTable).ToJagged(out columnNames);<br/>// Get only the input vector values (first two columns)<br/>double[][] inputs = table.GetColumns(0, 1);<br/>// Get the expected output labels (last column)<br/>int[] expected = table.GetColumn(2).ToInt32();<br/>// Compute the actual tree outputs and turn <br/>  a Boolean into a 0 or 1<br/>int[] actual = tree.Decide(inputs);<br/>// Use confusion matrix to compute some statistics.<br/>ConfusionMatrix confusionMatrix = <br/>  new ConfusionMatrix(actual, expected, 1, 0);<br/>dgvPerformance.DataSource = new [] { confusionMatrix };<br/>// Create performance scatter plot<br/>CreateResultScatterplot(zedGraphControl1,<br/>  inputs, expected.ToDouble(), actual.ToDouble());<br/>// As you can see above, the tree is making <br/>  the decision via the code line<br/>int[] actual = tree.Decide(inputs);</pre>
<p>This value is then fed to a <kbd>ConfusionMatrix</kbd>. For those of you that are not familiar with this, let me briefly explain.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Confusion matrix</h1>
                </header>
            
            <article>
                
<p>A <strong>confusion matrix</strong> is a table used to describe the performance of a classification model. It is run on a test dataset for which the truth values are known. This is how we arrive at things such as the following.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">True positives</h1>
                </header>
            
            <article>
                
<p>This is a case where we predicted yes, and this was the truth.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">True negatives</h1>
                </header>
            
            <article>
                
<p>This is a case where we predicted no, and this was the truth.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">False positives</h1>
                </header>
            
            <article>
                
<p>This is a case where we predicted yes but the truth was no. You might sometimes see this referred to as a <strong>type 1</strong> error.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">False negatives</h1>
                </header>
            
            <article>
                
<p>This is a case where we predicted no but the truth was yes. You might sometimes see this referred to as <strong>type II</strong> error.</p>
<p>Now, with all that being said, we need to talk about two other important terms, <strong>precision</strong> and <strong>recall</strong>.</p>
<p>Let's describe them this way. For the past week, it has rained every day. That's 7 days out of 7. Simple enough. A week later, you are asked <em>How often did it rain last week?</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recall</h1>
                </header>
            
            <article>
                
<p>It is the ratio of the number of days you correctly recalled the rain relative to the overall number of correct events. If you said it rained 7 days, that's 100%. If you said it rained 4 days, then that's 57% recall. In this case, it means your recall was not so precise, so we have precision to recognize.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Precision</h1>
                </header>
            
            <article>
                
<p>It is the ratio of times you correctly recalled it was going to rain, relative to the total number of days in that week.</p>
<p>For us, if our machine learning algorithm is good at recall, it doesn't necessarily mean it's good at precision. Makes sense? That gets us into other things such as F1 scores, which we'll leave for another day.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Error type visualization</h1>
                </header>
            
            <article>
                
<p>Here are some visualizations that may be of help:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-647 image-border" src="assets/ea7d0942-166e-4cef-befb-29221e671d56.png" style=""/></div>
<p>Identification of true positives versus false negatives:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-649 image-border" src="assets/c1a2f8e6-bd4d-4696-ab9e-27bf8a3ca144.png" style=""/></div>
<p>After using the confusion matrix to compute the statistics, the scatter plot is created and everything is identified:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d96075af-68b0-44bf-919b-1505b414d992.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we devoted a lot of time towards decision trees; what they are, how we can use them, and how they can benefit us in our applications. In our next chapter we are going to enter the world of <strong>Deep Belief Networks</strong> (<strong>DBNs</strong>), what they are, and how we can use them. We'll even talk a little bit about what a computer dreams, when it dreams!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Bishop, C. M., 2007. <em>Pattern Recognition and Machine Learning (Information Science and Statistics).</em> 1st ed. 2006. Corr. 2nd printing ed. s.l.: Springer.</li>
<li>Fayyad, U. M. &amp; Irani, K. B., 1992. <a href="http://deepblue.lib.umich.edu/bitstream/2027.42/46964/1/10994_2004_Article_422458.pdf" target="_blank">http://deepblue.lib.umich.edu/bitstream/2027.42/46964/1/10994_2004_Article_422458.pdf</a>. <em>Machine Learning,</em> January, 8(1), pp. 87-102.</li>
<li>Quinlan, J. R., 1986. <a href="http://www.dmi.unict.it/~apulvirenti/agd/Qui86.pdf" target="_blank">http://www.dmi.unict.it/~apulvirenti/agd/Qui86.pdf</a>. <em>Machine Learning,</em> 1(1), pp. 81-106.</li>
<li>Quinlan, J. R., 1993. <em>C4.5: Programs for Machine Learning (Morgan Kaufmann Series in Machine Learning).</em> 1 ed. s.l.: Morgan Kaufmann.</li>
<li>Shotton, J. et al., 2011. <a href="http://research.microsoft.com/apps/pubs/default.aspx?id=145347" target="_blank">http://research.microsoft.com/apps/pubs/default.aspx?id=145347</a><em>.</em> s.l., s.n.</li>
<li>Viola, P. &amp; Jones, M., 2001. <em>Robust Real-time Object Detection.</em> s.l., s.n.</li>
<li>Mitchell, T. M., 1997. Decision Tree Learning. In:: <em>Machine Learning (McGraw-Hill Series in Computer Science).</em> s.l.: McGraw Hill.</li>
<li>Mitchell, T. M., 1997. <em>Machine Learning (McGraw-Hill Series in Computer Science).</em> Boston(MA): WCB/McGraw-Hill.</li>
<li>Esmeir, S. &amp; Markovitch, S., 2007. <a href="http://jmlr.csail.mit.edu/papers/volume8/esmeir07a/esmeir07a.pdf" target="_blank">http://jmlr.csail.mit.edu/papers/volume8/esmeir07a/esmeir07a.pdf</a>. <em>J. Mach. Learn. Res.,</em> May, Volume 8, pp. 891-933.</li>
<li>Hyafil, L. &amp; Rivest, R. L., 1976. Constructing Optimal Binary Decision Trees is NP-complete. <em>Information Processing Letters,</em> 5(1), pp. 15-17.</li>
<li><a href="https://en.wikipedia.org/wiki/Ross_Quinlan" target="_blank">https://en.wikipedia.org/wiki/Ross_Quinlan</a>.</li>
</ul>


            </article>

            
        </section>
    </body></html>