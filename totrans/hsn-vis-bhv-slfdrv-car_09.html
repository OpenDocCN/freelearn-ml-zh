<html><head></head><body><div><div><h1 id="_idParaDest-154"><a id="_idTextAnchor158"/><em class="italic">Chapter 7</em>: Detecting Pedestrians and Traffic Lights</h1>
			<p>Congratulations on covering deep learning and progressing to this new section! Now that you know the basics of how to build and tune neural networks, it is time to move toward more advanced topics.</p>
			<p>If you remember, in <a href="B16322_01_Final_NM_ePUB.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">OpenCV Basics and Camera Calibration</em>, we already detected pedestrians using OpenCV. In this chapter, we will learn how to detect objects using a very powerful neural network called <strong class="bold">Single Shot MultiBox Detector</strong> (<strong class="bold">SSD</strong>), and we will use it to detect not only pedestrians but also vehicles and traffic lights. In addition, we will train a neural network to detect the color of the traffic lights using transfer learning, a powerful technique that can help you achieve good results using a relatively small dataset.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Detecting pedestrians, vehicles, and traffic lights</li>
				<li>Collecting images with CARLA</li>
				<li>Object detection with <strong class="bold">Single Shot MultiBox Detector</strong> (<strong class="bold">SSD</strong>)</li>
				<li>Detecting the color of a traffic lights</li>
				<li>Understanding transfer learning</li>
				<li>The ideas behind Inception</li>
				<li>Recognizing traffic lights and their colors</li>
			</ul>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor159"/>Technical requirements</h1>
			<p>To be able to use the code explained in this chapter, you need to have installed the following tools and modules:</p>
			<ul>
				<li>The Carla simulator</li>
				<li>Python 3.7</li>
				<li>The NumPy module</li>
				<li>The TensorFlow module</li>
				<li>The Keras module</li>
				<li>The OpenCV-Python module</li>
				<li>A GPU (recommended)</li>
			</ul>
			<p>The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter7">https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter7</a>.</p>
			<p>The Code in Action videos for this chapter can be found here:</p>
			<p><a href="https://bit.ly/3o8C79Q">https://bit.ly/3o8C79Q</a></p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor160"/>Detecting pedestrians, vehicles, and traffic lights with SSD</h1>
			<p>When a <a id="_idIndexMarker444"/>self-driving car is on a road, it surely needs to know where the lanes are and detect obstacles (including people!) that can be <a id="_idIndexMarker445"/>present on the road, and it also needs to detect traffic signs and traffic lights.</p>
			<p>In this <a id="_idIndexMarker446"/>chapter, we will take a big step forward, as we will learn how to detect pedestrians, vehicles, and traffic lights, including the traffic light colors. We will use Carla to generate the images that we need.</p>
			<p>Solving our task is a two-step process:</p>
			<ol>
				<li value="1">Firstly, we will detect vehicles, pedestrians, and traffic lights (no color information), where we will use a pre-trained neural network called SSD.</li>
				<li>Then, we will detect the color of the traffic lights, where we will need to train a neural network starting from a pre-trained <a id="_idIndexMarker447"/>neural network called <strong class="bold">Inception v3</strong>, using a technique called transfer learning, and we will also need <a id="_idIndexMarker448"/>to collect a small dataset.</li>
			</ol>
			<p>So, let's <a id="_idIndexMarker449"/>begin by <a id="_idTextAnchor161"/>using <a id="_idIndexMarker450"/>Carla to collect the images.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor162"/>Collecting some images with Carla</h2>
			<p>We need <a id="_idIndexMarker451"/>some pictures of a street, with pedestrians, vehicles, and <a id="_idIndexMarker452"/>traffic lights. We can use Carla for that, but this time, we will <a id="_idIndexMarker453"/>discuss in further detail how to collect the dataset with Carla. You can find Carla at <a href="https://carla.org/">https://carla.org/</a>.</p>
			<p>You can <a id="_idIndexMarker454"/>find the binaries for Linux and Windows on the Carla GitHub page:</p>
			<p><a href="https://github.com/carla-simulator/carla/releases">https://github.com/carla-simulator/carla/releases</a></p>
			<p>The <a id="_idIndexMarker455"/>installation instruction can be found on the Carla website:</p>
			<p><a href="https://carla.readthedocs.io/en/latest/start_quickstart/">https://carla.readthedocs.io/en/latest/start_quickstart/</a></p>
			<p>If you are using Linux, Carla starts with the <code>CarlaUE4.sh</code> command, while on Windows, it is called <code>CarlaUE4.exe</code>. We will just call it <code>CarlaUE4</code>. You can run it without arguments, or you could manually set the resolution, as follows:</p>
			<pre>CarlaUE4 -windowed -ResX=640 -ResY=480</pre>
			<p>In Carla, you can move around the track using some keys:</p>
			<ul>
				<li><em class="italic">W</em>: Forward</li>
				<li><em class="italic">S</em>: Backward</li>
				<li><em class="italic">A</em>: Left, sideways</li>
				<li><em class="italic">D</em>: Right, sideways</li>
			</ul>
			<p>In addition, in Carla, you can use the mouse, pressing the left mouse button and moving the cursor to change the angle of the view and to move along other angles.</p>
			<p>You should see something like this:</p>
			<div><div><img src="img/Figure_7.1_B16322.jpg" alt="Figure 7.1 – Carla – default track" width="389" height="271"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Carla – default track</p>
			<p>While the <a id="_idIndexMarker456"/>server is sometimes useful, you probably want to run some <a id="_idIndexMarker457"/>of the files present in <code>PythonAPI\util</code> and <code>PythonAPI\examples</code>.</p>
			<p>For this task, we are going to change track, using <code>Town01</code>. You can do this using the <code>PythonAPI\util\config.py</code> file, as follows:</p>
			<pre><code>python config.py -m=Town01</code></pre>
			<p>You should now see a different track:</p>
			<div><div><img src="img/Figure_7.2_B16322.jpg" alt="Figure 7.2 – The Town01 track" width="565" height="328"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – The Town01 track</p>
			<p>Your city <a id="_idIndexMarker458"/>is empty, so we need to add some vehicles <a id="_idIndexMarker459"/>and some pedestrians. We can do this using <code>PythonAPI\examples\spawn_npc.py</code>, as follows:</p>
			<pre>python spawn_npc.py  -w=100 -n=100</pre>
			<p>The <code>-w</code> parameter specifies the number of walkers, and <code>–n</code> the number of vehicles, that you want to create. Now, you should see some action:</p>
			<div><div><img src="img/Figure_7.3_B16322.jpg" alt="Figure 7.3 – The Town01 track with vehicles and pedestrians" width="1650" height="509"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – The Town01 track with vehicles and pedestrians</p>
			<p>Much better.</p>
			<p>Carla is <a id="_idIndexMarker460"/>intended to run as a server to which you can connect multiple clients, which should allow more interesting simulations.</p>
			<p>When you <a id="_idIndexMarker461"/>run Carla, it starts a server. You can go around a bit using the server, but most likely, you will want to run a client as it can provide much more functionality. If you run a client, you will have two windows with Carla, which are expected:</p>
			<ol>
				<li value="1">Let's run a client using <code>PythonAPI\examples\manual_control.py</code>, as follows:<pre>python manual_control.py</pre><p>You might see something like the following:</p><div><img src="img/Figure_7.4_B16322.jpg" alt="Figure 7.4 – The Town01 track using manual_control.py" width="377" height="224"/></div><p class="figure-caption">Figure 7.4 – The Town01 track using manual_control.py</p><p>You can see a lot of statistics on the left, and you can toggle them using the <em class="italic">F1</em> key. You will notice that now you have a vehicle, and you can change it with the Backspace key.</p></li>
				<li>You can <a id="_idIndexMarker462"/>move with the same keys as before, but this time, the behavior is more useful and realistic, as there is some physical simulation. You can also use <a id="_idIndexMarker463"/>the arrow keys to move.<p>You can use the <em class="italic">Tab</em> key to change camera, and the <em class="italic">C</em> key changes the weather, as we can see in the following screenshot:</p></li>
			</ol>
			<div><div><img src="img/Figure_7.5_B16322.jpg" alt="Figure 7.5 – The Town01 track; hard rain at noon and clear sky at sunset" width="521" height="167"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – The Town01 track; hard rain at noon and clear sky at sunset</p>
			<p>Carla has many sensors, one of which is the RGB camera, and you can switch between them using <code>`</code>, the backtick key. Now, refer to the following screenshots:</p>
			<div><div><img src="img/Figure_7.6_B16322.jpg" alt="Figure 7.6 – The Town01 track – left: depth (raw), right: semantic segmentation" width="462" height="135"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – The Town01 track – left: depth (raw), right: semantic segmentation</p>
			<p>The preceding screenshots show a couple of very interesting sensors:</p>
			<ul>
				<li>The depth sensor, which provides the distance from the camera for each pixel</li>
				<li>The semantic segmentation sensor, which classifies every object using a different color</li>
			</ul>
			<p>At the <a id="_idIndexMarker464"/>time of writing, the full list of camera sensors is as follows:</p>
			<ul>
				<li>Camera RGB</li>
				<li>Camera depth (raw)</li>
				<li>Camera depth (grayscale)</li>
				<li>Camera depth (logarithmic grayscale)</li>
				<li>Camera <a id="_idIndexMarker465"/>semantic segmentation (CityScapes Palette)</li>
				<li>Lidar (raycast)</li>
				<li><strong class="bold">Dynamic Vision Sensor </strong>(<strong class="bold">DVS</strong>)</li>
				<li>Camera RGB distorted</li>
			</ul>
			<p>Lidar is a <a id="_idIndexMarker466"/>sensor that detects the distance of an object using a laser; the DVS, also called the neuromorphic camera, is a camera that records local changes of brightness, overcoming some limitations of RGB cameras. Camera RGB distorted is just an RGB camera simulating the effects of a lens, and of course, you can customize the distortion as needed.</p>
			<p>The <a id="_idIndexMarker467"/>following screenshot shows the Lidar camera view:</p>
			<div><div><img src="img/Figure_7.7_B16322.jpg" alt="Figure 7.7 – The Lidar camera view" width="477" height="266"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – The Lidar camera view</p>
			<p>The following <a id="_idIndexMarker468"/>screenshot shows the output of DVS:</p>
			<div><div><img src="img/Figure_7.8_B16322.jpg" alt="Figure 7.8 – DVS" width="477" height="268"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – DVS</p>
			<p>You can <a id="_idIndexMarker469"/>now just go around and collect some images from the RGB camera, or you <a id="_idIndexMarker470"/>can use the ones in the GitHub repository.</p>
			<p>Now that we have some images, it is time to detect pedestrians, vehicles, and traffic lights, using a pre-trained network called SSD.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor163"/>Understanding SSD</h2>
			<p>In previous chapters, we created a classifier, a neural network able to tell what is present in a picture from a <a id="_idIndexMarker471"/>predefined set of options. Later in this chapter, we will see a pre-trained neural network that can classify images in a very precise way.</p>
			<p>SSD stands out compared to many neural networks, as it is able to detect multiple objects in the same picture. The details of SSD are a bit complicated, and if you are interested, you can check the <em class="italic">Further reading</em> section for some inspiration.</p>
			<p>Not only can SSD detect multiple objects, but it can also output the area where the object is present! Internally, this is done by checking 8,732 positions at different aspect ratios. SSD is also fast enough that with a good GPU, it can be used to analyze videos in real time.</p>
			<p>But where can we find SSD? The answer is the TensorFlow detection model zoo. Let's see what this is.</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor164"/>Discovering the TensorFlow detection model zoo</h2>
			<p>The TensorFlow <a id="_idIndexMarker472"/>detection model zoo is a useful collection of pre-trained neural networks, which supports several architectures trained on several datasets. We are interested in SSD, so we will focus on that.</p>
			<p>Of the datasets <a id="_idIndexMarker473"/>supported by the model zoo, we are interested in COCO. <strong class="bold">COCO</strong> is the Microsoft<strong class="bold"> Common Objects in Context</strong> dataset, a collection of 2,500,000 (2.5 million) images, classified by type. You can find a link with the 90 labels of COCO in the <em class="italic">Further reading</em> section, but we are interested in the following ones:</p>
			<ul>
				<li><code>1</code>: <code>person</code></li>
				<li><code>3</code>: <code>car</code></li>
				<li><code>6</code>: <code>bus</code></li>
				<li><code>8</code>: <code>truck</code></li>
				<li><code>10</code>: <code>traffic light</code></li>
			</ul>
			<p>You might also be interested in the following:</p>
			<ul>
				<li><code>2</code>: <code>bicycle</code></li>
				<li><code>4</code>: <code>motorcycle</code></li>
				<li><code>13</code>: <code>stop sign</code></li>
			</ul>
			<p>Notably, SSD trained on COCO is available on several versions, using different neural networks as the backend to <a id="_idIndexMarker474"/>reach the desired speed/precision ratio. Refer to the following screenshot:</p>
			<div><div><img src="img/Figure_7.9_B16322.jpg" alt="Figure 7.9 – The TensorFlow detection model zoo of SSDs trained on COCO" width="1650" height="662"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9 – The TensorFlow detection model zoo of SSDs trained on COCO</p>
			<p>Here, the <code>mAP</code> column is the mean average precision, so the higher the better. MobileNet is a neural network developed to perform particularly well on mobiles and embedded devices, and thanks to its performance, it is a classical choice for SSD when you need to perform inference in real time.</p>
			<p>To detect the objects on the road, we will <a id="_idIndexMarker475"/>use an SSD built using <strong class="bold">ResNet50</strong> as a backbone, a neural network with 50 layers developed by Microsoft Research Asia. A characteristic of ResNet is the presence of <strong class="bold">skip connections</strong>, shortcuts that can connect a layer to another one, skipping some layers in the middle. This helps in solving the <strong class="bold">vanishing gradient problem</strong>. With deep neural networks, the gradient during training can become so small that the network can basically stop learning.</p>
			<p>But how do we use <code>ssd_resnet_50_fpn_coco</code>, our selected model? Let's check it out!</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor165"/>Downloading and loading SSD</h2>
			<p>On the <a id="_idIndexMarker476"/>model zoo page, if you click on <strong class="bold">ssd_resnet_50_fpn_coco</strong>, you get a URL that Keras needs to download the model from; at the time of <a id="_idIndexMarker477"/>writing, the URL is as follows:</p>
			<pre><a href="http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz">http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz</a></pre>
			<p>The full name of the model is the following:</p>
			<pre>ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.</pre>
			<p>To load the model, you can use the following code:</p>
			<pre>url = 'http://download.tensorflow.org/models/object_detection/'
+ model_name + '.tar.gz'
model_dir = tf.keras.utils.get_file(fname=model_name,
untar=True, origin=url)
 
print("Model path: ", model_dir)
model_dir = pathlib.Path(model_dir) / "saved_model"
model = tf.saved_model.load(str(model_dir))
model = model.signatures['serving_default']</pre>
			<p>If this is the first time that you have run this code, it will take more time because Keras will <a id="_idIndexMarker478"/>download the model and save it <a id="_idIndexMarker479"/>on your hard drive.</p>
			<p>Now that we have loaded the model, is time to use it to detect some objects.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor166"/>Running SSD</h2>
			<p>Running SSD <a id="_idIndexMarker480"/>requires just a few lines of code. You can load the image (with a resolution of 299x299) with OpenCV, then you need to convert the image into a tensor, a type of multi-dimensional array used by TensorFlow that is similar to NumPy arrays. Refer to the following code:</p>
			<pre><code>img = cv2.imread(file_name)</code>
<code>img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</code>
<code>input_tensor = tf.convert_to_tensor(img)</code>
<code>input_tensor = input_tensor[tf.newaxis, ...]</code>
<code># Run inference</code>
<code>output = model(input_tensor)</code></pre>
			<p>Please note that we feed the network with an <code>RGB</code> image, not <code>BGR</code>. You might remember from the previous chapters that OpenCV uses pictures in <code>BGR</code> format, so we need to pay attention to the order of the channels.</p>
			<p>As you can see, running SSD is quite easy, but the output is relatively complex, and it needs some code to be converted into a useful and more compact form. The <code>output</code> variable is a Python dictionary, but the values that it contains are tensors, so you need to convert them.</p>
			<p>For example, printing <code>output['num_detections']</code>, which contains the number of predictions (for example, objects found in the image), would give the following as a result:</p>
			<pre><code>tf.Tensor([1.], shape=(1,), dtype=float32)</code></pre>
			<p>For the conversion, we can use <code>int()</code>.</p>
			<p>All the other tensors are arrays, and they can be converted using their <code>numpy()</code> function. So then, your code might look like this:</p>
			<pre><code>num_detections = int(output.pop('num_detections'))</code>
<code>output = {key: value[0, :num_detections].numpy()</code>
<code>          for key, value in output.items()}</code>
<code>output['num_detections'] = num_detections</code></pre>
			<p>There are still the following two things to fix:</p>
			<ul>
				<li>The detection classes are floating point, while, as they are our labels, they should be integers.</li>
				<li>The coordinates of the boxes are in percentage form.</li>
			</ul>
			<p>We can <a id="_idIndexMarker481"/>fix these problems with just a few lines of code:</p>
			<pre>output['detection_classes'] =    output['detection_classes'].astype(np.int64)
output['boxes'] = [
    {"y": int(box[0] * img.shape[0]), 
     "x": int(box[1] * img.shape[1]), 
     "y2": int(box[2] * img.shape[0]),
     "x2": int(box[3] * img.shape[1])} 
        for box in output['detection_boxes']]</pre>
			<p>Let's apply SSD to this image:</p>
			<div><div><img src="img/Figure_7.10_B16322.jpg" alt="Figure 7.10 – Image from Town01" width="701" height="473"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10 – Image from Town01</p>
			<p>We get <a id="_idIndexMarker482"/>this output:</p>
			<pre>{ 'detection_scores': array([0.4976843, 0.44799107, 0.36753723,      0.3548107 ], dtype=float32),    'detection_classes': array([ 8, 10,  6,  3], dtype=int64),  'detection_boxes': array([     [0.46678272, 0.2595877, 0.6488052, 0.40986294],     [0.3679817, 0.76321596, 0.45684734, 0.7875406],     [0.46517858, 0.26020002, 0.6488801, 0.41080648],     [0.46678272, 0.2595877, 0.6488052, 0.40986294]],      dtype=float32),  'num_detections': 4,  'boxes': [{'y': 220, 'x': 164, 'y2': 306, 'x2': 260},            {'y': 174, 'x': 484, 'y2': 216, 'x2': 500},            {'y': 220, 'x': 165, 'y2': 306, 'x2': 260},            {'y': 220, 'x': 164, 'y2': 306, 'x2': 260}]}</pre>
			<p>This is what the code means:</p>
			<ul>
				<li><code>detection_scores</code>: A higher score means higher confidence in the prediction.</li>
				<li><code>detection_classes</code>: The predicted labels – in this case, truck (<code>8</code>), traffic light (<code>10</code>), bus (<code>6</code>), and car (<code>3</code>).</li>
				<li><code>detection_boxes</code>: The original boxes, with coordinates in percentage form.</li>
				<li><code>num_detections</code>: The number of predictions.</li>
				<li><code>boxes</code>: The boxes with coordinates converted to the resolution of the original image.</li>
			</ul>
			<p>Please <a id="_idIndexMarker483"/>notice that three predictions are basically in the same area, and they are ordered by score. We will need to fix this overlapping.</p>
			<p>To be able to better see what has been detected, we will now annotate the image.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor167"/>Annotating the image</h2>
			<p>To properly <a id="_idIndexMarker484"/>annotate the image, we need to perform the following operations:</p>
			<ol>
				<li value="1">Consider only the labels that are interesting to us.</li>
				<li>Remove the overlapping of labels.</li>
				<li>Draw a rectangle on each prediction.</li>
				<li>Write the label and its score.</li>
			</ol>
			<p>To remove the overlapping labels, it's enough to compare them, and if the center of the boxes is similar, we will keep only the label with a higher score.</p>
			<p>This is the result:</p>
			<div><div><img src="img/Figure_7.11_B16322.jpg" alt="Figure 7.11 – Image from Town01, annotated with SSD only" width="508" height="312"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.11 – Image from Town01, annotated with SSD only</p>
			<p>It's a good <a id="_idIndexMarker485"/>starting point, even if other images are not recognized so well. The vehicle has been recognized as a truck, which is not completely accurate, but we don't really care about that.</p>
			<p>The main problem is that we know that there is a traffic light, but we don't know the color.Unfortunately, SSD cannot help us; we need to do it by ourselves.</p>
			<p>In the next section, we will develop a neural network that is able to detect the color of the traffic light, using a technique called transfer learning.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor168"/>Detecting the color of a traffic light</h1>
			<p>In principle, we <a id="_idIndexMarker486"/>could try to detect the color of a traffic light using some computer vision technique –  for example, checking the red and the green channel could be a starting point. In addition, verifying the luminosity of the bottom and upper part of the crossing light should help. This could work, even if some crossing lights can be problematic.</p>
			<p>However, we will use deep learning, as this task is well-suited to exploring more advanced techniques. We will also go the extra mile to use a small dataset, even though it would be easy for us to create a big dataset; the reason being that we don't always have the <a id="_idIndexMarker487"/>luxury of easily increasing the size of the dataset.</p>
			<p>To be able to detect the color of a traffic light, we need to complete three steps:</p>
			<ol>
				<li value="1">Collect a dataset of crossing lights.</li>
				<li>Train a neural network to recognize the color.</li>
				<li>Use the network with SSD to get the final result.</li>
			</ol>
			<p>There is a traffic light dataset that you could use, the <em class="italic">Bosch Small Traffic Lights</em> dataset; however, we will generate our own dataset using Carla. Let's see how.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor169"/>Creating a traffic light dataset</h2>
			<p>We are going <a id="_idIndexMarker488"/>to create a dataset using Carla. In principle, it could be as big as we want. The bigger the better, but a big dataset would make training slower and, of course, it will require some more time to be created. In our case, as the task is simple, we will create a <a id="_idIndexMarker489"/>relatively small dataset of a few hundreds of images. We will explore <strong class="bold">transfer learning</strong> later, a technique that can be used when the dataset is not particularly big.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">On GitHub, you can find the dataset that I created for this task, but if you have some time, collecting the dataset by yourself could be a nice exercise.</p>
			<p>Creating this dataset is a three-step task:</p>
			<ol>
				<li value="1">Collect images of streets.</li>
				<li>Find and crop all the traffic lights.</li>
				<li>Classify the traffic lights.</li>
			</ol>
			<p>The first task of collecting images is very simple. Just start Carla using <code>manual_control.py</code> and press the <em class="italic">R</em> key. Carla will start recording and it will stop after you press <em class="italic">R</em> again.</p>
			<p>Consider that we want to record four types of images:</p>
			<ul>
				<li>Red lights</li>
				<li>Yellow lights</li>
				<li>Green lights</li>
				<li>The back of traffic lights (negative samples)</li>
			</ul>
			<p>The reason why we want to collect the back of the traffic light is that SSD is recognizing it as a traffic light, but we have no use for it, so we don't want to use it. These are <strong class="bold">negative samples</strong>, and they could also include pieces of road or buildings or anything that SSD wrongly classifies as a traffic light.</p>
			<p>So, while you <a id="_idIndexMarker490"/>record your images, please try to get enough samples for each category.</p>
			<p>These are some example of images that you might want to collect:</p>
			<div><div><img src="img/Figure_7.12_B16322.jpg" alt="Figure 7.12 – Town01 – left: red light, right: green light" width="602" height="177"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.12 – Town01 – left: red light, right: green light</p>
			<p>The second step is applying SSD and extracting the image of the crossing light. It's quite simple; refer to the following code:</p>
			<pre><code>obj_class = out["detection_classes"][idx]if obj_class == object_detection.LABEL_TRAFFIC_LIGHT:    box = out["boxes"][idx]    traffic_light = img[box["y"]:box["y2"], box["x"]:box["x2"]]</code></pre>
			<p>In the previous code, assuming that the <code>out</code> variable contains the result of running the SSD, calling <code>model(input_tensor)</code>, and <code>idx</code> contains the current detection among the predictions, you just need to select the detections containing traffic lights and crop them using the coordinates that we computed earlier.</p>
			<p>I ended up with 291 detections, with images like these:</p>
			<div><div><img src="img/Figure_7.13_B16322.jpg" alt="Figure 7.13 – Town01, from the left: small red light, small green light, green light, yellow light, back of a traffic light, piece of a building wrongly classified as a traffic light" width="712" height="271"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.13 – Town01, from the left: small red light, small green light, green light, yellow light, back of a traffic light, piece of a building wrongly classified as a traffic light</p>
			<p>As you can see, the images have different resolutions and ratios, and that's perfectly fine.</p>
			<p>There are <a id="_idIndexMarker491"/>also some images that are completely unrelated, such as a piece of a building, and these are excellent negative samples for things that are not a traffic light because SSD misclassified them, so it is also a way to improve the output of SSD.</p>
			<p>The last step is just classifying the images. With a few hundred pictures of this type, it takes only a few minutes. You can create a directory for each label and move the appropriate images there.</p>
			<p>Congratulations, now you have a custom dataset to detect the color of traffic lights.</p>
			<p>As you know, the dataset is small, so, as we already said, we are going to use transfer learning. The next section will explain what it is.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor170"/>Understanding transfer learning</h2>
			<p>Transfer learning <a id="_idIndexMarker492"/>is a very appropriate name. From a conceptual point of view, indeed, it is about taking what a neural network learned on one task and transferring this knowledge to a different but related task.</p>
			<p>There are several approaches to transfer learning; we will discuss two, and we will choose one of them and use it to detect the colors of the traffic lights. In both cases, the starting point is a neural network that has been pre-trained on a similar task – for example, a classification of images. We will talk more <a id="_idIndexMarker493"/>about this in the next section, <em class="italic">Getting to know ImageNet</em>. We are focusing on a <strong class="bold">Convolutional Neural Network</strong> (<strong class="bold">CNN</strong>) used as a classifier, as this is what we need to recognize the color of the traffic lights.</p>
			<p>The first approach is to load the pre-trained neural network, adapting the number of outputs to the new problem (either replacing some or all the dense layers or sometimes just adding an additional dense layer), and basically keep training it on the new dataset. You might need to use a small learning rate. This approach might work if the number of samples in the new dataset is smaller than the dataset used for the original training, but still significantly big. For example, the size of our custom dataset could be 10% of the site of the original dataset. One drawback is that training might take a long time, as typically you are training a relatively big network.</p>
			<p>A second approach, the one that we are going to follow, is similar to the first one, but you are going to freeze all the convolutional layers, meaning that their parameters are fixed and will not change during training. This has the advantage that training is much faster as you don't need to train the convolutional layers. The idea here is that the convolutional layers have been trained on a huge dataset, and they are able to detect so many features that it is going to be fine for the new task also, while the real classifier, composed of the dense layers, can be replaced and trained from scratch.</p>
			<p>Intermediate approaches are also possible, where you train some of the convolutional layers, but typically keep at least the first layers frozen.</p>
			<p>Before seeing how to do transfer learning with Keras, let's think a bit more about what we just discussed. A key assumption is that this hypothetical network that we want to learn from has been trained on a huge dataset, where the network can learn to recognize many features and patterns. Turns out that there is a very big dataset that fits the bill—ImageNet. Let's talk a bit more about it.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor171"/>Getting to know ImageNet</h2>
			<p>ImageNet is a <a id="_idIndexMarker494"/>huge dataset, and at the time of writing, it is composed of 14,197,122 (over 14 million) images! In reality, it does not provide the images, but just the URLs to download the images. Those images are classified on 27 categories and a total of 21,841 subcategories! These <a id="_idIndexMarker495"/>subcategories, called synsets, are based on a classification hierarchy called <strong class="bold">WordNet</strong>.</p>
			<p>ImageNet <a id="_idIndexMarker496"/>has been a very influential dataset, thanks also to a competition used to measure the advancements in computer vision: the <strong class="bold">ImageNet Large-Scale Visual Recognition Challenge</strong> (<strong class="bold">ILSVRC</strong>).</p>
			<p>These are the main categories:</p>
			<ul>
				<li><code>amphibian</code></li>
				<li><code>animal</code></li>
				<li><code>appliance</code></li>
				<li><code>bird</code></li>
				<li><code>covering</code></li>
				<li><code>device</code></li>
				<li><code>fabric</code></li>
				<li><code>fish</code></li>
				<li><code>flower</code></li>
				<li><code>food</code></li>
				<li><code>fruit</code></li>
				<li><code>fungus</code></li>
				<li><code>furniture</code></li>
				<li><code>geological formation</code></li>
				<li><code>invertebrate</code></li>
				<li><code>mammal</code></li>
				<li><code>musical instrument</code></li>
				<li><code>plant</code></li>
				<li><code>reptile</code></li>
				<li><code>sport</code></li>
				<li><code>structure</code></li>
				<li><code>tool</code></li>
				<li><code>tree</code></li>
				<li><code>utensil</code></li>
				<li><code>vegetable</code></li>
				<li><code>vehicle</code></li>
				<li><code>person</code></li>
			</ul>
			<p>The number <a id="_idIndexMarker497"/>of subcategories is impressively high; as an example, the <code>tree</code> category has 993 subcategories, covered by more than half a million of pictures!</p>
			<p>Surely, a neural network performing well on this dataset will be very good at recognizing patterns on many types of images, and it might also have a quite big capacity. So, yes, it will overfit your dataset, but as we know how to deal with overfitting, we will keep an eye on this problem, but not get too worried about it.</p>
			<p>As so much research has been devoted to performing well on ImageNet, it's not surprising that many of the most influential neural networks have been trained on it.</p>
			<p>One, in particular, stood out when it first appeared in 2012—AlexNet. Let's see why.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor172"/>Discovering AlexNet</h2>
			<p>When AlexNet <a id="_idIndexMarker498"/>was released in 2012, it was over 10% more precise than the best neural network of the time! Clearly, these solutions have been studied extensively, and some are now very common.</p>
			<p>AlexNet introduced several ground-breaking innovations:</p>
			<ul>
				<li>Multi-GPU training where AlexNet was trained half on one GPU and half on another one, allowing the model to be twice the size.</li>
				<li>ReLU activation instead of Tanh, which apparently allowed the training to be six times faster.</li>
				<li>Overlapping pooling added, where AlexNet used max-pooling of 3x3, but the pooling area is moved only by 2x2, meaning that there is an overlap between pools. According to the original paper, this gave a 0.3–0.4% improvement in accuracy. In Keras, you can achieve similar overlapping pooling using <code>MaxPooling2D(pool_size=(3,3), strides=(2,2))</code>.</li>
			</ul>
			<p>With over 60 million parameters, AlexNet was pretty big, so to reduce overfitting, it made extensive use of data augmentation and dropout.</p>
			<p>While AlexNet <a id="_idIndexMarker499"/>was state-of-the-art and ground-breaking in 2012, by today's standards, it is quite inefficient. In the next section, we will discuss a neural network that can achieve substantially better accuracy than AlexNet using only one-tenth of the parameters: <strong class="bold">Inception</strong>.</p>
			<h3>The ideas behind Inception</h3>
			<p>Having a huge <a id="_idIndexMarker500"/>dataset like ImageNet at our disposal is great, but it would be much easier to have a neural network already trained with it. It turns out that Keras provides several of them. One is ResNet, which we have already encountered. Another one, which is very influential and with great innovations, is Inception. Let's talk a bit about it.</p>
			<p>Inception is a family of neural networks, meaning that there are several of them, refining the initial concept. Inception was designed by Google, and the version that participated in the ILSVRC 2014 (ImageNet) competition <a id="_idIndexMarker501"/>and won is called <strong class="bold">GoogLeNet</strong>, in honor of the LeNet architecture.</p>
			<p>If you are wondering whether Inception took its name from the famous movie <em class="italic">Inception</em>, yes it did, because they wanted to go deeper! And Inception is a deep network, with a version called <code>InceptionResNetV2</code> arriving at a staggering 572 layers! This, of course, is if we count every layer, including the activations. We are going to use Inception v3, which has <em class="italic">only</em> 159 layers.</p>
			<p>We will focus on Inception v1, because it is easier, and we will briefly discuss some improvements added later, as they can be a source of inspiration for you.</p>
			<p>A key observation made by Google is that given the variety of positions where a subject can be on a picture, it is difficult to know in advance which kernel size of a convolutional layer would be the best, so they added 1x1, 3x3, and 5x5 convolutions in parallel to cover the main cases, plus max pooling, as it is often useful, and concatenated the results. One <a id="_idIndexMarker502"/>advantage of doing it in parallel is that the network does not get too deep, which keeps the training easier.</p>
			<p>What we <a id="_idIndexMarker503"/>just described is the <strong class="bold">naïve</strong> Inception block:</p>
			<div><div><img src="img/Figure_7.14_B16322.jpg" alt="Figure 7.14 – Naïve Inception block" width="960" height="522"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.14 – Naïve Inception block</p>
			<p>You might have noticed a 1x1 convolution. What's that? Just multiplying a channel by a number? Not exactly. The 1x1 convolution is very cheap to perform, as there is only 1 multiplication instead of 9 (as in 3x3 convolutions) or 25 (as in 5x5 convolutions), and it can be used to change the number of filters. In addition, you can add a ReLU, introducing a non-linear operation that increases the complexity of the functions that the network can learn.</p>
			<p>This module is <a id="_idIndexMarker504"/>called naïve because it is too computationally expensive. As the number of channels grows, the 3x3 and 5x5 convolutions become slow. The solution is to put 1x1 convolutions in front of them, to reduce the number of channels where the more expensive convolutions need to operate:</p>
			<div><div><img src="img/Figure_7.15_B16322.jpg" alt="Figure 7.15 – Inception block including dimension reductions" width="1085" height="666"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.15 – Inception block including dimension reductions</p>
			<p>The key to understanding this block is to remember that the 1x1 convolutions are used to reduce the <a id="_idIndexMarker505"/>channels and improve performance significantly.</p>
			<p>For example, the first Inception block in GoogLeNet has an input of 192 channels, and the 5x5 convolution would create 32 channels, so the number of multiplications would be proportional to 25 x 32 x 192 = 153,600.</p>
			<p>They added a 1x1 convolution with an output of 16 filters, so the number of multiplications would be proportional to 16 x 192 + 25 x 32 x 16 = 3,072 + 12,800 = 15,872. Almost a 10x reduction. Not bad!</p>
			<p>One more thing. For the concatenation to work, all the convolutions need to have an output of the same size, which means that they need the padding, which keeps the same resolution of the input image. And what about max pooling? It also needs to have an output of the same size as the convolutions, so even if it finds the maximum in a 3x3 grid, it cannot reduce the size.</p>
			<p>In Keras, this means it would be something like this:</p>
			<pre><code>MaxPooling2D(pool_size=(3, 3), padding='same', strides=(1, 1))</code></pre>
			<p>The <code>strides</code> parameter indicates how many pixels to move after calculating the maximum. By default, it's set at the same value of <code>pool_size</code>, which in our example would reduce the size 3 times. Setting it to <code>(1, 1) </code>with the same padding has the effect of not changing the size. The Conv2D layers also have a <code>strides</code> parameter, which can be used to reduce the size of the output; however, usually it is more effective to do so using a max pooling layer.</p>
			<p>Inception v2 introduced some optimizations, among them the following:</p>
			<ul>
				<li>A 5x5 convolution is similar to two stacked 3x3 convolutions, but slower, so they refactored it with 3x3 convolutions.</li>
				<li>A 3x3 convolution is equivalent to a 1x3 convolution followed by a 3x1 convolution, but using two convolutions is 33% faster.</li>
			</ul>
			<p>Inception v3 introduced the following optimizations:</p>
			<ul>
				<li>Factorized 7x7 convolutions, created using several smaller and faster convolutions</li>
				<li>Some batch normalization layers</li>
			</ul>
			<p>Inception-ResNet <a id="_idIndexMarker506"/>introduced the residual connections typical of ResNet, to skip some layers.</p>
			<p>Now that you have a better understanding of the concepts behind Inception, let's see how to use it in Keras.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor173"/>Using Inception for image classification</h2>
			<p>Loading <a id="_idIndexMarker507"/>Inception in Keras could not be simpler, as <a id="_idIndexMarker508"/>we can see here:</p>
			<pre><code>model = InceptionV3(weights='imagenet', input_shape=(299,299,3))</code></pre>
			<p>As Inception can tell us the content of an image, let's try it with the test image that we used at the beginning of the book:</p>
			<pre><code>img = cv2.resize(preprocess_input(cv2.imread("test.jpg")),</code>
<code>(299, 299))</code>
<code>out_inception = model.predict(np.array([img]))</code>
<code>out_inception = imagenet_utils.decode_predictions(out_inception)</code>
<code>print(out_inception[0][0][1], out_inception[0][0][2], "%")</code></pre>
			<p>This is the result:</p>
			<pre><code>sea_lion 0.99184495 %</code></pre>
			<p>It is indeed right: our image depicts a sea lion from the Galapagos Islands:</p>
			<div><div><img src="img/Figure_7.16_B16322.jpg" alt="Figure 7.16 – Recognized by Inception as a sea lion with 0.99184495 confidence" width="845" height="480"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.16 – Recognized by Inception as a sea lion with 0.99184495 confidence</p>
			<p>But we <a id="_idIndexMarker509"/>want to use Inception for transfer learning, not for <a id="_idIndexMarker510"/>image classification, so we need to use it in a different way. Let's see how.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor174"/>Using Inception for transfer learning</h2>
			<p>The loading <a id="_idIndexMarker511"/>for transfer learning is a bit different, because <a id="_idIndexMarker512"/>we need to remove the classifier on top of Inception, as follows:</p>
			<pre><code>base_model = InceptionV3(include_top=False, input_shape=    (299,299,3))</code></pre>
			<p>With <code>input_shape</code>, we use the original size of Inception, but you could use a different shape, as long as it has 3 channels and the resolution is at least 75x75.</p>
			<p>The important parameter is <code>include_top</code>, as setting it to <code>False</code> would remove the top part of Inception—the classifier with the dense filters—which means that the network will be ready for transfer learning.</p>
			<p>We will now create a neural network that is based on Inception but can be modified by us:</p>
			<pre><code>top_model = Sequential()top_model.add(base_model) # Join the networks</code></pre>
			<p>Now, we can add a classifier on top of it, as follows:</p>
			<pre><code>top_model.add(GlobalAveragePooling2D())top_model.add(Dense(1024, activation='relu'))top_model.add(Dropout(0.5))top_model.add(Dense(512, activation='relu'))top_model.add(Dropout(0.5))top_model.add(Dense(n_classes, activation='softmax'))</code></pre>
			<p>We added some dropout, as we expect Inception to overfit quite a lot on our dataset. But please pay attention to the <code>GlobalAveragePooling2D</code>. What it does is compute the average of the channels. </p>
			<p>We could use <code>Flatten</code>, but as Inception outputs 2,048 convolutional channels of 8x8, and we are using a dense layer with 1,024 neurons, the number of parameters would be huge—134,217,728! Using <code>GlobalAveragePooling2D</code>, we need only 2,097,152 parameters. Even counting the parameters of Inception, the saving is quite significant—24,427,812 parameters instead of 156,548,388.</p>
			<p>There is one more thing that we need to do: freeze the layers of Inception that we don't want to train. In this case, we want to freeze all of them, but this might not always be the case. This is how you can freeze them:</p>
			<pre><code>for layer in base_model.layers:</code>
<code>    layer.trainable = False</code></pre>
			<p>Let's check <a id="_idIndexMarker513"/>how our network looks. Inception is too big, so I will <a id="_idIndexMarker514"/>only show the data of the parameters:</p>
			<pre><code>Total params: 21,802,784</code>
<code>Trainable params: 21,768,352</code>
<code>Non-trainable params: 34,432</code></pre>
			<p>Please note that <code>summary()</code> will actually print two summaries: one for Inception and one for our network; this is the output of the first summary:</p>
			<p><code>Model: "sequential_1"</code></p>
			<pre><code>____________________________________________________________</code>
<code>Layer (type)                 Output Shape              Param #   </code>
<code>============================================================</code>
<code>inception_v3 (Model)         (None, 8, 8, 2048)        21802784  </code>
<code>____________________________________________________________</code>
<code>global_average_pooling2d_1 ( (None, 2048)              0         </code>
<code>____________________________________________________________</code>
<code>dense_1 (Dense)              (None, 1024)              2098176   </code>
<code>____________________________________________________________</code>
<code>dropout_1 (Dropout)          (None, 1024)              0         </code>
<code>____________________________________________________________</code>
<code>dense_2 (Dense)              (None, 512)               524800    </code>
<code>____________________________________________________________</code>
<code>dropout_2 (Dropout)          (None, 512)               0         </code>
<code>____________________________________________________________</code>
<code>dense_3 (Dense)              (None, 4)                 2052      </code>
<code>============================================================</code>
<code>Total params: 24,427,812</code>
<code>Trainable params: 2,625,028</code>
<code>Non-trainable params: 21,802,784</code>
<code>____________________________________________________________</code></pre>
			<p>As you can see, the first layer is Inception. In the second summary, you also have a confirmation that Inception has the layers frozen, because we have more than 21 million non-trainable parameters, matching exactly the total number of parameters of Inception.</p>
			<p>To reduce the overfitting and to compensate for the small dataset, we will use data augmentation:</p>
			<pre><code>datagen = ImageDataGenerator(rotation_range=5, width_shift_</code>
<code>range=[-5, -2, -1, 0, 1, 2, 5], horizontal_flip=True,</code>
<code>height_shift_range=[-30, -20, -10, -5, -2, 0, 2, 5, 10, 20,</code>
<code>30])</code></pre>
			<p>I only applied <a id="_idIndexMarker515"/>a small rotation because traffic lights are usually <a id="_idIndexMarker516"/>pretty straight, and I also added only a small width shift because the traffic lights are detected by a neural network (SSD), so the cut tends to be very consistent. I also added a higher height shift because I saw that SSD sometimes wrongly cut the traffic lights, removing one-third of it.</p>
			<p>Now that the network is ready, we just need to feed it our dataset.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor175"/>Feeding our dataset to Inception</h2>
			<p>Let's assume <a id="_idIndexMarker517"/>that you loaded your dataset in two variables: <code>images</code> and <code>labels</code>.</p>
			<p>Inception <a id="_idIndexMarker518"/>needs some preprocessing, to map the values of the images to the <code>[-1, +1]</code> range. Keras has a function that takes care of this, <code>preprocess_input()</code>. Please take care to import it from the <code>keras.applications.inception_v3</code> module, because there are other functions with the same name and different behavior in other modules:</p>
			<pre><code>from keras.applications.inception_v3 import preprocess_input</code>
<code>images = [preprocess_input(img) for img in images]</code></pre>
			<p>We need to divide the dataset into training and validation, which is easy, but we also need to randomize the order, to be sure that the split is meaningful; for example, my code loads all the images with the same label, so a split without randomization would put only one or two labels in validation, and one of them might even not be present in training.</p>
			<p>NumPy has a very handy function to generate new index positions, <code>permutation()</code>:</p>
			<pre><code>indexes = np.random.permutation(len(images))</code></pre>
			<p>Then, you can use <strong class="bold">for comprehension</strong>, a feature of Python, to change the order in your lists:</p>
			<pre><code>images = [images[idx] for idx in indexes]</code>
<code>labels = [labels[idx] for idx in indexes]</code></pre>
			<p>If your labels are numeric, you can use <code>to_categorical()</code> to convert them into one-hot encoding.</p>
			<p>Now, it's just a matter of slicing. We will use 20% of the samples for validation, so the code can be like this:</p>
			<pre><code>idx_split = int(len(labels_np) * 0.8)</code>
<code>x_train = images[0:idx_split]</code>
<code>x_valid = images[idx_split:]</code>
<code>y_train = labels[0:idx_split]</code>
<code>y_valid = labels[idx_split:]</code></pre>
			<p>Now, you can train the network as usual. Let's see how it performs!</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor176"/>Performance with transfer learning</h2>
			<p>The <a id="_idIndexMarker519"/>performance of the model is very good:</p>
			<pre><code>Min Loss: 0.028652783162121116</code>
<code>Min Validation Loss: 0.011525456588399612</code>
<code>Max Accuracy: 1.0</code>
<code>Max Validation Accuracy: 1.0</code></pre>
			<p>Yes, 100% accuracy and validation accuracy! Not bad, not bad. Actually, it's very rewarding. However, the dataset was very simple, so it was fair to expect a very good result.</p>
			<p>This is the graph of the losses:</p>
			<div><div><img src="img/Figure_7.17_B16322.jpg" alt="Figure 7.17 – Losses with transfer learning from Inception" width="894" height="548"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.17 – Losses with transfer learning from Inception</p>
			<p>The problem is that despite the great results, the network does not work too well on my test images. Probably, it overfits, and I suspect that as the images are normally smaller than the <a id="_idIndexMarker520"/>native resolution of Inception, the network might experience patterns that are a result of interpolation instead of true patterns in the images, and maybe get confused by them, but that's just my theory.</p>
			<p>To get good results, we need to try harder.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor177"/>Improving transfer learning</h2>
			<p>We can <a id="_idIndexMarker521"/>assume that the network is overfitting, and the standard response would be to increase the dataset. In this case, it would be easy to do that, but let's pretend that we cannot do it, so we can explore other options that can be useful to you in similar cases.</p>
			<p>There are some very easy things that we can do to reduce overfitting:</p>
			<ul>
				<li>Increase the variety of the data augmentation.</li>
				<li>Increase dropout.</li>
			</ul>
			<p>Despite the fact that Inception is clearly able to process tasks much more complex than this, it is not optimized for this specific task, and it is also possible that it could benefit from a bigger classifier, so I will add a layer:</p>
			<ul>
				<li> This is the new data augmentation, after a few tests:<pre>datagen = ImageDataGenerator(rotation_range=5, width_
shift_range= [-10, -5, -2, 0, 2, 5, 10],
zoom_range=[0.7, 1.5],
height_shift_range=[-10, -5, -2, 0, 2, 5, 10],
horizontal_flip=True)</pre></li>
				<li>This is the new model, with more dropout and an additional layer:<pre>top_model.add(GlobalAveragePooling2D())top_model.add(Dropout(0.5))top_model.add(Dense(1024, activation='relu'))top_model.add(BatchNormalization())top_model.add(Dropout(0.5))top_model.add(Dense(512, activation='relu'))top_model.add(Dropout(0.5))top_model.add(Dense(128, activation='relu'))top_model.add(Dense(n_classes, activation='softmax'))</pre></li>
				<li>I added a dropout after the global average pooling, to reduce overfitting, and I also added a batch normalization layer, which can also help to reduce overfitting.</li>
				<li>Then, I added a dense layer, but I did not put a dropout on it, because I noticed the network had problems training with so much dropout.<p>Even if we don't want to increase the dataset, we can still do something about it. Let's look at the distribution of the classes:</p><pre>print('Labels:', collections.Counter(labels))</pre><p>This is the result:</p><pre>Labels: Counter({0: 123, 2: 79, 1: 66, 3: 23})</pre></li>
			</ul>
			<p>As you see, the dataset has much more green than yellow or red and not many negative samples.</p>
			<p>In general, it is not <a id="_idIndexMarker522"/>good to have imbalanced labels, and the network was indeed predicting more green lights than there were, because statistically, it is more rewarding to predict a green than any other label. To improve this situation, we can instruct Keras to customize the loss in a way that predicting a wrong red would be worse than predicting a wrong green, as this would have an effect similar to making the dataset balanced.</p>
			<p>You can do this with these two lines of code:</p>
			<pre><code>n = len(labels)</code>
<code>class_weight = {0: n/cnt[0], 1: n/cnt[1], 2: n/cnt[2], 3: n/cnt[3]}</code></pre>
			<p>The following is the result:</p>
			<pre><code>Class weight: {0: 2.365, 1: 4.409, 2: 3.683, 3: 12.652}</code></pre>
			<p>As you can see, the loss penalty would be less for green (label 0) than for the other ones.</p>
			<p>This is how the network performs:</p>
			<pre><code>Min Loss: 0.10114006596268155</code>
<code>Min Validation Loss: 0.012583946840742887</code>
<code>Max Accuracy: 0.99568963</code>
<code>Max Validation Accuracy: 1.0</code></pre>
			<p>Not much <a id="_idIndexMarker523"/>different than before, but this time, the network works better and nails all the traffic lights in my test images. This should be a reminder to not trust the validation accuracy completely unless you are sure that your validation dataset is excellent.</p>
			<p>This is the graph of the losses:</p>
			<div><div><img src="img/Figure_7.18_B16322.jpg" alt="Figure 7.18 – Losses with transfer learning from Inception, improved" width="798" height="442"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.18 – Losses with transfer learning from Inception, improved</p>
			<p>Now that we have a good network, it's time to complete our task, using the new network in combination with SSD, as detailed in the next section.</p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor178"/>Recognizing traffic lights and their colors</h1>
			<p>We are <a id="_idIndexMarker524"/>almost done. From the code using SSD, we just need to manage the <a id="_idIndexMarker525"/>traffic light in a different way. So, when the label is <code>10</code> (traffic light), we need to do the following:</p>
			<ul>
				<li>Crop the area with the traffic light.</li>
				<li>Resize it to 299x299.</li>
				<li>Preprocess it.</li>
				<li>Run it through our network.</li>
			</ul>
			<p>Then, we will get the prediction:</p>
			<pre>img_traffic_light = img[box["y"]:box["y2"], box["x"]:box["x2"]]img_inception = cv2.resize(img_traffic_light, (299, 299))img_inception = np.array([preprocess_input(img_inception)])prediction = model_traffic_lights.predict(img_inception)label = np.argmax(prediction)</pre>
			<p>If you run the code of this chapter that is in GitHub, the label <code>0</code> is the green light, <code>1</code> is yellow, <code>2</code> is red, and <code>3</code> means that it is not a traffic light.</p>
			<p>The whole <a id="_idIndexMarker526"/>process involves first detecting objects with SSD, and then using our <a id="_idIndexMarker527"/>network to detect the color of traffic lights, if any are present in the image, as explained in the following diagram:</p>
			<div><div><img src="img/Figure_7.19_B16322.jpg" alt="Figure 7.19 – A diagram showing how to use SSD and our network together" width="1650" height="1328"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.19 – A diagram showing how to use SSD and our network together</p>
			<p>These are examples obtained running SSD followed by our network:</p>
			<div><div><img src="img/Figure_7.20_B16322.jpg" alt="Figure 7.20 – Some detections with traffic lights" width="729" height="189"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.20 – Some detections with traffic lights</p>
			<p>The colors of <a id="_idIndexMarker528"/>the traffic lights are now detected properly. There are some false detections: for example, in the preceding figure, the image on the right marks a person where there <a id="_idIndexMarker529"/>is a tree. Unfortunately, this can happen. In a video, we could require detection for a few frames before accepting it, always considering that in a real self-driving car, you cannot introduce high latency, because the car needs to react quickly to what's happening on the street.</p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor179"/>Summary</h1>
			<p>In this chapter, we focused on pre-trained neural networks, and how we can leverage them for our purposes. We combined two neural networks to detect pedestrians, vehicles, and traffic lights, including their color. We first discussed how to use Carla to collect images, and then we discovered SSD, a powerful neural network that stands out for its capacity to detect not only objects, but also their position in an image. We also saw the TensorFlow detection model zoo and how to use Keras to download the desired version of SSD, trained on a dataset called COCO.</p>
			<p>In the second part of the chapter, we discussed a powerful technique called transfer learning, and we studied some of the solutions of a neural network called Inception, which we trained on our dataset using transfer learning, to be able to detect the colors of traffic lights. In the process, we also talked about ImageNet, and we saw how achieving 100% validation accuracy was misleading, and as a result, we had to reduce the overfitting to improve the real precision of the network. In the end, we succeeded in using the two networks together—one to detect pedestrians, vehicles, and traffic lights, and one to detect the color of the traffic lights.</p>
			<p>Now that we know how to build knowledge about the road, it's time to move forward to the next task—driving! In the next chapter, we will literally sit in the driving seat (of Carla), and teach our neural network how to drive, using a technique called behavioral cloning, where our neural network will try to mimic our behavior.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor180"/>Questions</h1>
			<p>You should now be able to answer the following questions:</p>
			<ol>
				<li value="1">What is SSD?</li>
				<li>What is Inception?</li>
				<li>What does it mean to freeze a layer?</li>
				<li>Can SSD detect the color of a traffic light?</li>
				<li>What is transfer learning?</li>
				<li>Can you name some techniques to reduce overfitting?</li>
				<li>Can you describe the idea behind the Inception block?</li>
			</ol>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor181"/>Further reading</h1>
			<ul>
				<li>SSD: <a href="https://arxiv.org/abs/1512.02325">https://arxiv.org/abs/1512.02325</a></li>
				<li>TensorFlow model zoo: <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md</a></li>
				<li>COCO labels: <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt">https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt</a></li>
				<li>Vanishing gradient problem: <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">https://en.wikipedia.org/wiki/Vanishing_gradient_problem</a></li>
				<li>The Bosch Small Traffic Lights dataset: <a href="https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset">https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset</a></li>
				<li>ImageNet: <a href="http://www.image-net.org/">http://www.image-net.org/</a></li>
				<li>Inception paper: <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf">https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf</a></li>
				<li>AlexNet paper: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></li>
			</ul>
		</div>
	</div>



  </body></html>