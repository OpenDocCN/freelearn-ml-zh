- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Elements of a Machine Learning System
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习系统的要素
- en: Data and algorithms are crucial for machine learning systems, but they are far
    from sufficient. Algorithms are the smallest part of a production machine learning
    system. Machine learning systems also require data, infrastructure, monitoring,
    and storage to function efficiently. For a large-scale machine learning system,
    we need to ensure that we can include a good user interface or package model in
    microservices.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据和算法对于机器学习系统至关重要，但它们远远不够。算法是生产级机器学习系统中最小的一部分。机器学习系统还需要数据、基础设施、监控和存储才能高效运行。对于大规模机器学习系统，我们需要确保我们可以包含一个良好的用户界面或包装模型在微服务中。
- en: In modern software systems, combining all necessary elements requires different
    professional competencies – including machine learning/data science engineering
    expertise, database engineering, software engineering, and finally interaction
    design. In these professional systems, it is more important to provide reliable
    results that bring value to users rather than include a lot of unnecessary functionality.
    It is also important to orchestrate all elements of machine learning together
    (data, algorithms, storage, configuration, and infrastructure) rather than optimize
    each one of them separately – all to provide the most optimal system for one or
    more use cases from end users.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代软件系统中，结合所有必要的元素需要不同的专业能力——包括机器学习/数据科学工程专业知识、数据库工程、软件工程，以及最终的用户交互设计。在这些专业系统中，提供可靠的结果，为用户提供价值，比包含大量不必要的功能更为重要。同时，也很重要将机器学习的所有元素（数据、算法、存储、配置和基础设施）协同起来，而不是单独优化每一个元素——所有这些都是为了提供一个最优化系统，用于满足一个或多个最终用户的使用案例。
- en: In this chapter, we’ll review each element of a professional machine learning
    system. We’ll start by understanding which elements are important and why. Then,
    we’ll explore how to create such elements and how to put them together into a
    single machine learning system – a so-called machine learning pipeline.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回顾专业机器学习系统的每个要素。我们将首先了解哪些要素是重要的以及为什么重要。然后，我们将探讨如何创建这些要素以及如何将它们组合成一个单一的机器学习系统——所谓的机器学习管道。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Machine learning is more than just algorithms and data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习不仅仅是算法和数据
- en: Data and algorithms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据和算法
- en: Configuration and monitoring
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置和监控
- en: Infrastructure and resource management
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施和资源管理
- en: Machine learning pipelines
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习管道
- en: Elements of a production machine learning system
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产级机器学习系统的要素
- en: Modern machine learning algorithms are very capable because they use large quantities
    of data and consist of a large number of trainable parameters. The largest available
    models are **Generative Pre-trained Transformer-3** (**GPT-3**) from OpenAI (with
    175 billion parameters) and Megatron-Turing from NVidia (356 billion parameters).
    These models can create texts (novels) and make conversations but also write program
    code, create user interfaces, or write requirements.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习算法非常强大，因为它们使用大量数据，并包含大量可训练的参数。目前可用的最大模型是来自OpenAI的**生成预训练转换器3**（**GPT-3**）（拥有1750亿个参数）和来自NVIDIA的Megatron-Turing（3560亿个参数）。这些模型可以创建文本（小说）和进行对话，还可以编写程序代码、创建用户界面或编写需求。
- en: Now, such large models cannot be used on a desktop computer, laptop, or even
    in a dedicated server. They need advanced computing infrastructure, which can
    withstand long-term training and evaluation of such large models. Such infrastructure
    also needs to provide means to automatically provide these models with data, monitor
    the training process, and, finally, provide the possibility for the users to access
    the models to make inferences. One of the modern ways of providing such infrastructure
    is the concept of **Machine learning as a service** (**MLaaS**). MLaaS provides
    an easy way to use machine learning from the perspective of data analysts of software
    integrators since it delegates the management, monitoring, and configuration of
    the infrastructure to specialized companies.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这样的大型模型无法在台式电脑、笔记本电脑甚至专用服务器上使用。它们需要先进的计算基础设施，能够承受长期训练和评估这样的大型模型。这样的基础设施还需要提供自动为这些模型提供数据、监控训练过程，以及最终为用户提供访问模型进行推理的可能性。提供这种基础设施的现代方式之一是**机器学习即服务**（**MLaaS**）。MLaaS为数据分析师或软件集成商提供了一个简单的方式来使用机器学习，因为它将基础设施的管理、监控和配置委托给专业公司。
- en: '*Figure 2**.1* shows elements of modern machine learning-based software systems.
    Google has used these to describe production machine learning systems since then.
    Although variations in this setup exist, the principles remain:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2**.1* 展示了基于现代机器学习软件系统的元素。从那时起，谷歌已经使用这些来描述生产机器学习系统。尽管这种设置存在变化，但原则仍然适用：'
- en: '![Figure 2.1 – Elements of a production machine learning system](img/B19548_02_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 生产机器学习系统的元素](img/B19548_02_01.jpg)'
- en: Figure 2.1 – Elements of a production machine learning system
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 生产机器学习系统的元素
- en: 'Here, the machine learning model (**ML code**) is the smallest of these elements
    (Google, under the Creative Commons 4.0 Attribution License, [https://developers.google.com/machine-learning/crash-course/production-ml-systems](https://developers.google.com/machine-learning/crash-course/production-ml-systems)).
    In terms of the actual source code, in Python, model creation, training, and validation
    are just three lines of code (at least for some of the models):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，机器学习模型（**ML 代码**）是这些元素中最小的（谷歌，根据创意共享 4.0 属性许可，[https://developers.google.com/machine-learning/crash-course/production-ml-systems](https://developers.google.com/machine-learning/crash-course/production-ml-systems)）。从实际源代码的角度来看，在
    Python 中，模型创建、训练和验证只需要三行代码（至少对于某些模型）：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first line creates the model from a template – in this case, it is a random
    forest model with 10 trees, each of which has a maximum of two splits. Random
    forest is an ensemble learning method that constructs multiple decision trees
    during training and outputs the mode of the classes (classification) of the individual
    trees for a given input. By aggregating the results of multiple trees, it reduces
    overfitting and provides higher accuracy compared to a single decision tree. The
    second line trains the model based on the training data (`X_train`, which contains
    only the preditors/input features, and `Y_train`, which contains the predicted
    class/output features). Finally, the last line makes predictions for the test
    data (`X_test`) to compare it to the oracle (the expected value) in subsequent
    steps. Even though this `model.predict(X_test)` line is not part of a production
    system, we still need to make inferences, so there is always a similar line in
    our software.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行从模板创建模型——在这种情况下，它是一个包含 10 棵树的随机森林模型，每棵树最多有两个分割。随机森林是一种集成学习方法，在训练过程中构建多个决策树，并输出给定输入的各个树的类（分类）的模态。通过聚合多个树的结果，它减少了过拟合，并比单个决策树提供了更高的准确性。第二行基于训练数据（`X_train`，其中只包含预测者/输入特征，以及
    `Y_train`，其中包含预测的类/输出特征）训练模型。最后，最后一行对测试数据（`X_test`）进行预测，以便在后续步骤中将预测结果与先知（预期值）进行比较。尽管这一行
    `model.predict(X_test)` 不是生产系统的一部分，但我们仍然需要进行推断，因此我们的软件中始终存在类似的行。
- en: Therefore, we can introduce the next best practice.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以介绍下一个最佳实践。
- en: 'Best practice #5'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #5'
- en: When designing machine learning software, prioritize your data and the problem
    to solve over the algorithm.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计机器学习软件时，应优先考虑数据和要解决的问题，而不是算法。
- en: In this example, we saw that the machine learning code, from the perspective
    of the software engineer, is rather small. Before applying algorithms, we need
    to prepare the data correctly as the algorithms (`model.fit(X_train, Y_train)`)
    require the data to be in a specific format – the first parameter is the data
    that’s used to make inferences (so-called feature vectors or input data samples),
    while the second parameter is the target values (so-called decision classes, reference
    values, or target values, depending on the algorithm).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们从软件工程师的角度看到，机器学习代码相当小。在应用算法之前，我们需要正确准备数据，因为算法（`model.fit(X_train, Y_train)`）需要数据以特定格式存在——第一个参数是用于推断的数据（所谓的特征向量或输入数据样本），而第二个参数是目标值（所谓的决策类、参考值或目标值，具体取决于算法）。
- en: Data and algorithms
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据和算法
- en: 'Now, if using the algorithms is not the main part of the machine learning code,
    then something else must be – that is, data handling. Managing data in machine
    learning software, as shown in *Figure 2**.1*, consists of three areas:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果使用算法不是机器学习代码的主要部分，那么其他部分必须是——也就是说，数据处理。如图 *图 2**.1* 所示，在机器学习软件中管理数据包括三个领域：
- en: Data collection.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据收集。
- en: Feature extraction.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取。
- en: Data validation.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据验证。
- en: 'Although we will go back to these areas throughout this book, let’s explore
    what they contain. *Figure 2**.2* shows the processing pipeline for these areas:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们将在整本书中回顾这些领域，但让我们来看看它们包含的内容。*图 2**.2* 展示了这些领域的处理流程：
- en: '![Figure 2.2 – Data collection and preparation pipeline](img/B19548_02_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 数据收集和准备流程](img/B19548_02_02.jpg)'
- en: Figure 2.2 – Data collection and preparation pipeline
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 数据收集和准备流程
- en: Note that the process of preparing the data for the algorithms can become quite
    complex. First, we need to extract data from its source, which is usually a database.
    It can be a database of measurements, images, texts, or any other raw data. Once
    we’ve exported/extracted the data we need, we must store it in a raw data format.
    This can be in the form of a table, as shown in the preceding figure, or it can
    be in a set of raw files, such as images.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为算法准备数据的过程可能变得相当复杂。首先，我们需要从其来源中提取数据，这通常是一个数据库。它可以是测量、图像、文本或其他任何原始数据的数据库。一旦我们导出/提取了所需的数据，我们必须以原始数据格式存储它。这可以是表格的形式，如图中所示，也可以是一组原始文件，如图像。
- en: Data collection
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集
- en: 'Data collection is a procedure of transforming data from its raw format to
    a format that a machine learning algorithm can take as input. Depending on the
    data and the algorithm, this process can take different forms, as illustrated
    in *Figure 2**.3*:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集是将数据从其原始格式转换为机器学习算法可以接受的输入格式的过程。根据数据和算法的不同，这个过程可以采取不同的形式，如图 *图 2**.3* 所示：
- en: '![Figure 2.3 – Different forms of data collection – examples](img/B19548_02_03.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 不同形式的数据收集 – 示例](img/B19548_02_03.jpg)'
- en: Figure 2.3 – Different forms of data collection – examples
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 不同形式的数据收集 – 示例
- en: Data from images and measurements such as time series is usually collected to
    make classifications and predictions. These two classes of problems require the
    ground truth to be available, which we saw as `Y_train` in the previous code example.
    These target labels are either extracted automatically from the raw data or added
    manually through the process of labeling. The manual process is time-consuming,
    so the automated one is preferred.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 来自图像和测量（如时间序列）的数据通常被收集来进行分类和预测。这两类问题需要可用的事实数据，这在之前的代码示例中我们将其视为 `Y_train`。这些目标标签要么从原始数据中自动提取，要么通过标记过程手动添加。手动过程耗时较长，因此更倾向于自动化。
- en: The data that’s used in non-supervised learning and reinforcement learning models
    is often extracted as tabular data without labels. This data is used in the decision
    process or the optimization process to find the best solution to the given problem.
    Without optimization, there is a risk that our results are not representative
    of new data. The preceding figure shows two examples of such problems – optimizations
    of smart factories of Industry 4.0 and self-driving vehicles. In smart factories,
    reinforcement learning models are used to optimize production processes or control
    so-called *dark factories*, which operate entirely without human intervention
    (the name *dark factories* comes from the fact that there is no need for lights
    in these factories; robots do not need light to work).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 用于非监督学习和强化学习模型的数据通常被提取为无标签的表格数据。这些数据用于决策过程或优化过程，以找到给定问题的最佳解决方案。没有优化，我们的结果可能无法代表新数据。前面的图显示了此类问题的两个示例——工业
    4.0 智能工厂的优化和自动驾驶汽车。在智能工厂中，强化学习模型用于优化生产过程或控制所谓的*暗工厂*，这些工厂完全不需要人工干预（*暗工厂*这个名字来源于这些工厂不需要灯光；机器人不需要灯光工作）。
- en: The data that’s used for modern self-supervised models often comes from such
    sources as text or speech. These models do not require a tabular form of the data,
    but they require structure. For example, to train text transformer models, we
    need to tokenize the text per sentence (or per paragraph) for the model to learn
    the context of the words.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 用于现代自监督模型的常用数据通常来自文本或语音等来源。这些模型不需要数据的表格形式，但需要结构。例如，为了训练文本转换器模型，我们需要按句子（或段落）对文本进行分词，以便模型学习单词的上下文。
- en: Hence, here comes my next best practice.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，接下来是我的下一个最佳实践。
- en: 'Best practice #6'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #6'
- en: Once you’ve explored the problem you wish to solve and understood the data availability,
    decide whether you want to use supervised, self-supervised, unsupervised, or reinforcement
    learning algorithms.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你探索了你想要解决的问题，并了解了数据的可用性，决定你是否想使用监督学习、自监督学习、无监督学习或强化学习算法。
- en: The fact that we need different data for different algorithms is natural. However,
    we have not discussed how to decide upon the algorithm. Choosing supervised learning
    only makes sense when we want to predict or classify data statically – that is,
    we train the model and then we use it to make inferences. When we train and then
    make inferences, the model does not change. There is no adjustment as we go and
    re-training the algorithm is done periodically – I call it a *train once, predict*
    *many* principle.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为不同的算法使用不同的数据是自然的。然而，我们还没有讨论如何决定算法。仅当我们想要静态地预测或分类数据时，选择监督学习才有意义——也就是说，我们训练模型，然后使用它进行推断。当我们训练并做出推断时，模型不会改变。我们没有进行调整，并且算法的重新训练是定期进行的——我把这称为“训练一次，预测多次”原则。
- en: We can choose unsupervised methods when we use/train and apply the algorithm
    without the target class. Some of these algorithms are also used to group data
    based on the data’s property, for example, to cluster it. I call this the *train
    once, predict* *once* principle.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在没有目标类的情况下使用/训练和应用算法时，我们可以选择无监督方法。其中一些算法也用于根据数据的属性对数据进行分组，例如，进行聚类。我把这称为“训练一次，预测一次”原则。
- en: For self-supervised models, the situation is a bit more interesting. There,
    we can use something called *pre-training*. Pre-training means that we can train
    a model on a large corpus of data without any specific context – for example,
    we train language models on large corpora of English texts from Wikipedia. Then,
    when we want to use the model for a specific task, such as to write new text,
    we train it a bit more on that task. I call this the *train many, predict once*
    principle as we must pre-train and train the model for each task.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自监督模型，情况要有趣得多。在那里，我们可以使用一种称为“预训练”的方法。预训练意味着我们可以在没有任何特定上下文的大数据语料库上训练一个模型——例如，我们在维基百科的英文文本大型语料库上训练语言模型。然后，当我们想要将模型用于特定任务时，例如编写新文本，我们会在该任务上对其进行更多训练。我把这称为“训练多次，预测一次”原则，因为我们必须为每个任务进行预训练和训练模型。
- en: Finally, reinforcement learning needs data that is changed every time the model
    is used. For example, when we use a reinforcement learning algorithm to optimize
    a process, it updates the model each time it is used – we could say it learns
    from its mistakes. I call this the *train many, predict* *many* principle.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，强化学习需要每次使用模型时都发生变化的数据。例如，当我们使用强化学习算法优化一个过程时，它每次使用时都会更新模型——我们可以说是从错误中学习。我把这称为“训练多次，预测多次”原则。
- en: Usually, the raw data is not ready to be used with machine learning as it can
    contain empty data points, noise, or broken files. Therefore, we need to clean
    up these erroneous data points, such as by removing empty data points (using Python
    commands such as `dataFrame.dropna(…)`) or using data imputation techniques.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，原始数据不适合用于机器学习，因为它可能包含空数据点、噪声或损坏的文件。因此，我们需要清理这些错误数据点，例如通过删除空数据点（使用Python命令如`dataFrame.dropna(…)`)或使用数据插补技术。
- en: Now, there is a fundamental difference between the removal of data points and
    their imputation. The data imputation process is when we add missing properties
    of data based on similar data points. It’s like filling in blanks in a sequence
    of numbers – 1, 2, 3, …, 5, where we fill in the number 4\. Although filling in
    the data increases the number of data points available (thus making the models
    better), it can strengthen certain properties of the data, which can cause the
    model to learn. Imputation is also relevant when the size of the data is small;
    in large datasets, it is better (more resource-efficient and fair) to drop the
    missing data points. With that, we’ve come to my next best practice.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，删除数据点和它们的插补之间存在根本性的区别。数据插补过程是我们根据相似数据点添加数据缺失属性。这就像在数字序列中填补空白——1, 2, 3, …,
    5，其中我们填补数字4。尽管填补数据增加了可用的数据点数量（从而使得模型更好），但它可以加强数据的某些属性，这可能导致模型学习。当数据量较小时，插补也相关；在大数据集中，删除缺失数据点更好（更资源高效且公平）。有了这个，我们就来到了我的下一个最佳实践。
- en: 'Best practice #7'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #7'
- en: Use data imputation only when you know which properties of data you wish to
    strengthen and only do so for small datasets.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当你知道你希望加强哪些数据属性时，才使用数据插补，并且只为小数据集这样做。
- en: Finally, once we have clean data to work with, we can extract features. There,
    we can use algorithms that are specific to our problem at hand. For example, when
    we work with textual data, we could use a simple bag-of-words to count the frequencies
    of words, though we can also use the word2vec algorithm to embed the frequencies
    of the co-occurrence of words (algorithms that we’ll discuss in the next few chapters).
    Once we’ve extracted features, we can start validating the data. The features
    can emphasize certain properties of data that we didn’t see before.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦我们有了可以工作的干净数据，我们就可以提取特征。在那里，我们可以使用针对我们手头问题的特定算法。例如，当我们处理文本数据时，我们可以使用简单的词袋模型来计算单词的频率，尽管我们也可以使用word2vec算法来嵌入单词共现的频率（我们将在下一章讨论的算法）。一旦我们提取了特征，我们就可以开始验证数据。这些特征可以强调我们之前没有看到的数据的某些属性。
- en: One such example is noise – when we have data in a feature format, we can check
    whether there is *attribute* or *class noise* in the data. Class noise is a phenomenon
    that is related to labeling errors – one or more data points have been labeled
    incorrectly. Class noise can be either contradictory examples or wrongly labeled
    data points. It is a dangerous phenomenon since it can cause low performance when
    training and using machine learning models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个这样的例子：噪声——当我们有特征格式的数据时，我们可以检查数据中是否存在*属性*或*类别噪声*。类别噪声是与标签错误相关的一种现象——一个或多个数据点被错误地标记。类别噪声可以是矛盾示例或错误标记的数据点。这是一个危险的现象，因为它可能导致在训练和使用机器学习模型时性能低下。
- en: Attribute noise is when one (or more) attributes is corrupted with wrong values.
    Examples include wrong values, missing attribute (feature) values, and irrelevant
    values.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 属性噪声是指一个（或多个）属性被错误值所损坏。例如，包括错误值、缺失属性（特征）值和不相关值。
- en: Once the data has been validated, it can be used in algorithms. So, let’s dive
    a bit deeper into each step of the data processing pipeline.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据经过验证，就可以用于算法。所以，让我们深入探讨数据处理管道的每个步骤。
- en: Now, since different algorithms use data in different ways, the data has a different
    form. Let’s explore how the data should be structured for each of the algorithms.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于不同的算法以不同的方式使用数据，数据具有不同的形式。让我们探索每种算法应该如何构建数据结构。
- en: Feature extraction
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征提取
- en: The process of transforming raw data into a format that can be used by algorithms
    is called feature extraction. It is a process where we apply a feature extraction
    algorithm to find properties of interest in the data. The algorithm for extracting
    features varies depending on the problem and the data type.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始数据转换为算法可用的格式的过程称为特征提取。这是一个应用特征提取算法来寻找数据中感兴趣属性的过程。提取特征算法根据问题和数据类型的不同而有所不同。
- en: 'When we work with textual data, we can use several algorithms, but let me illustrate
    the use of one of the simplest ones – bag-of-words. The algorithm simply counts
    the occurrence of words in the sentence – it either counts a pre-defined set of
    words or uses statistics to find the most frequent words. Let’s consider the following
    sentence:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理文本数据时，我们可以使用几个算法，但让我来展示最简单的一个——词袋模型的使用。该算法简单地计算句子中单词的出现次数——它要么计算一个预定义的单词集，要么使用统计方法来找到最频繁的单词。让我们考虑以下句子：
- en: '`Mike is a` `tall boy.`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`Mike is a` `tall boy.`'
- en: 'When we use the bag-of-words algorithm without any constraints, it provides
    us with the following table:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们无约束地使用词袋算法时，它提供了以下表格：
- en: '| **Sentence ID** | **Mike** | **Is** | **A** | **tall** | **Boy** |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| **句子 ID** | **Mike** | **Is** | **A** | **tall** | **Boy** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| `0` | `1` | `1` | `1` | `1` | `1` |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `0` | `1` | `1` | `1` | `1` | `1` |'
- en: Figure 2.4 – Features extracted using bag-of-words
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 使用词袋模型提取的特征
- en: 'The table contains all the words in the sentence as features. It is not very
    useful for just one sentence, but if we add another one (sentence 1), things become
    more obvious. So, let’s add the following sentence:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 该表包含句子中的所有单词作为特征。对于单个句子来说，它并不很有用，但如果我们添加另一个（句子1），事情就会变得更加明显。所以，让我们添加以下句子：
- en: '`Mary is a` `smart girl.`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`Mary is a` `smart girl.`'
- en: 'This will result in the following feature table:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下特征表：
- en: '| **Sentence ID** | **Mike** | **Is** | **A** | **Tall** | **boy** | **smart**
    | **girl** |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| **句子 ID** | **Mike** | **Is** | **A** | **Tall** | **boy** | **smart** |
    **girl** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| `0` | `1` | `1` | `1` | `1` | `1` | `0` | `0` |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `0` | `1` | `1` | `1` | `1` | `1` | `0` | `0` |'
- en: '| `1` | `0` | `1` | `1` | `0` | `0` | `1` | `1` |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `0` | `1` | `1` | `0` | `0` | `1` | `1` |'
- en: Figure 2.5 – Features extracted from two sentences
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 从两个句子中提取的特征
- en: 'We are now ready to add the label column to the data. Let’s say that we want
    to label each sentence as being positive or negative. The table then gets one
    more column – `label` – where **1** means that the sentence is positive and **0**
    otherwise:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好将标签列添加到数据中。假设我们想要将每个句子标记为正面或负面。然后表格将增加一列 – `label` – 其中 **1** 表示句子是正面的，否则为
    **0**：
- en: '| **Sentence ID** | **Mike** | **Is** | **A** | **Tall** | **boy** | **smart**
    | **girl** | **Label** |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| **句子 ID** | **Mike** | **Is** | **A** | **Tall** | **boy** | **smart** |
    **girl** | **Label** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| `0` | `1` | `1` | `1` | `1` | `1` | `0` | `0` | `1` |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `0` | `1` | `1` | `1` | `1` | `1` | `0` | `0` | `1` |'
- en: '| `1` | `0` | `1` | `1` | `0` | `0` | `1` | `1` | `1` |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `0` | `1` | `1` | `0` | `0` | `1` | `1` | `1` |'
- en: Figure 2.6 – Labels added to the data
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 添加到数据中的标签
- en: Now, these features allow us to see the difference between two sentences, which
    we can then use to train and test machine learning algorithms.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些特征使我们能够看到两个句子之间的差异，然后我们可以使用这些差异来训练和测试机器学习算法。
- en: There are, however, two important limitations of this approach. The first one
    is that it is impractical (if not impossible) to have all words from all sentences
    as columns/features. For any non-trivial text, this would result in large and
    sparse matrices – a lot of wasted space. The second limitation is the fact that
    we usually lose important information – for example, the sentence “Is Mike a boy?”
    would result in the same feature vector as the first sentence. A feature vector
    is an *n*-dimensional vector of numerical features that describe some object in
    pattern recognition in machine learning. Although these sentences are not identical,
    they become undistinguishable, which can lead to class noise if they are labeled
    differently.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法有两个重要的局限性。第一个局限性是，将所有句子中的所有单词作为列/特征是不切实际（如果不是不可能）的。对于任何非平凡文本，这会导致大型稀疏矩阵
    – 浪费大量空间。第二个局限性是，我们通常会丢失重要的信息 – 例如，句子“Is Mike a boy?”会产生与第一个句子相同的特征向量。特征向量是描述机器学习中模式识别中某个对象的
    *n*- 维数值特征向量。尽管这些句子并不相同，但它们变得无法区分，这可能导致如果它们被标记为不同类别时出现类别噪声。
- en: The problem with adding this kind of noise becomes even more evident if we use
    statistics to choose the most frequent words as features. Here, we can lose important
    features that discriminate data points in a useful way. Therefore, this bag-of-words
    approach is only for illustration here. Later in this book, we’ll dive deeper
    into so-called transformers, which use more advanced techniques to extract features.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用统计方法来选择最频繁的单词作为特征，那么添加这种噪声的问题就会变得更加明显。在这里，我们可能会丢失一些重要的特征，这些特征以有用的方式区分数据点。因此，这种词袋方法在这里仅用于说明。在本书的后面部分，我们将更深入地探讨所谓的转换器，它们使用更先进的技术来提取特征。
- en: Data validation
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据验证
- en: Feature vectors are the core of machine learning algorithms. They are most prominently,
    and directly, used by supervised machine learning algorithms. However, the same
    concepts of data validation apply to data used in other types of validation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量是机器学习算法的核心。它们最显著且直接地被监督机器学习算法使用。然而，数据验证的相同概念也适用于其他类型验证中使用的数据。
- en: 'Every form of data validation is a set of checks that ensure the data contains
    the desired properties. An example of such a set of checks is presented in *Figure
    2**.7*:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每种形式的数据验证都是一系列检查，确保数据包含所需的属性。以下是一个此类检查集的示例 *图 2**.7*：
- en: '![Figure 2.7 – An example of data quality checks](img/B19548_02_04.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7 – 数据质量检查示例](img/B19548_02_04.jpg)'
- en: Figure 2.7 – An example of data quality checks
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – 数据质量检查示例
- en: Completeness of data is a property that describes how much of the total distribution
    our data covers. This can be measured in terms of object distribution – for example,
    how many types/models/colors of cars we have in our image dataset – or it can
    be measured in terms of properties – for example, how many of the words in the
    language our data contains.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的完整性是描述我们的数据覆盖总分布多少的一个属性。这可以通过对象分布来衡量 – 例如，在我们的图像数据集中有多少种/型号/颜色的汽车 – 或者它可以通过属性来衡量
    – 例如，我们的数据中包含的语言中有多少个单词。
- en: Accuracy is a property that describes how well our data is related to the empirical
    (real) world. For example, we may want to check whether all images in our dataset
    are linked to an object or whether all objects in the image are annotated.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性是描述我们的数据与经验（现实）世界相关性的一个属性。例如，我们可能想要检查我们数据集中的所有图像是否都与一个对象相关联，或者图像中的所有对象是否都被标注了。
- en: Consistency describes how well the data is structured internally and whether
    the same data points are annotated in the same way. For example, in binary classification,
    we want all data points to be labeled “0” and “1” or “True” and “False,” but not
    both.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性描述了数据内部结构的好坏以及相同的数据点是否以相同的方式进行标注。例如，在二元分类中，我们希望所有数据点都被标注为“0”和“1”或“True”和“False”，而不是两者都有。
- en: Integrity is the property where we check that the data can be integrated with
    other data. The integration can be done through common keys or other means. For
    example, we can check if our images contain metadata that will allow us to know
    where the image was taken.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 完整性是检查数据是否可以与其他数据集成的属性。集成可以通过公共键或其他方式完成。例如，我们可以检查我们的图像是否包含允许我们知道图像拍摄地点的元数据。
- en: Finally, timeliness is a property that describes how fresh the data is. It checks
    whether the data contains the latest required information. For example, when we
    design a recommendation system, we would like to recommend both the new items
    and the old ones, so timeliness is important.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，及时性是描述数据新鲜程度的属性。它检查数据是否包含最新的所需信息。例如，当我们设计推荐系统时，我们希望推荐新项目和旧项目，因此及时性很重要。
- en: So, here is the next best practice.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里是下一个最佳实践。
- en: 'Best practice #8'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #8'
- en: Choose the data validation attributes that are the most relevant for your system.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 选择对您的系统最相关的数据验证属性。
- en: Since every check requires additional steps in the workflow and can slow down
    the processing of the data, we should choose the data quality checks that impact
    our business and our architecture. If we develop a system where we want to provide
    up-to-date recommendations, then timeliness is our top priority, and we should
    focus on that rather than on completeness.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个检查都需要在工作流程中增加额外的步骤，可能会减慢数据处理的速度，因此我们应该选择影响我们业务和架构的数据质量检查。如果我们开发一个希望提供最新推荐的系统，那么及时性是我们的首要任务，我们应该专注于这一点，而不是完整性。
- en: 'Although there are a lot of frameworks for data validation and assessing data
    quality, I usually use a subset of data quality attributes from the AIMQ framework.
    The AIMQ framework has been designed to quantify data quality based on several
    quality attributes, similar to quality frameworks in software engineering such
    as the ISO 25000 series. I find the following properties of data to be the most
    important to validate:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有很多用于数据验证和评估数据质量的框架，但我通常使用来自AIMQ框架的数据质量属性子集。AIMQ框架已被设计为根据几个质量属性量化数据质量，类似于软件工程中的质量框架，如ISO
    25000系列。我发现以下数据属性对于验证来说是最重要的：
- en: The data is free of noise
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据无噪声
- en: The data is fresh
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是新鲜的
- en: The data is fit for purpose
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据适合用途
- en: 'The first property is the most important as noisy data can cause low performance
    in machine learning algorithms. For class noise, which we introduced previously,
    it is important to check if the data labels are not contradictory and to check
    whether the labels are assigned correctly. Contradictory labels can be found automatically,
    but wrongly annotated data points need manual assessment. For attribute noise,
    we can use statistical approaches to identify attributes that have low variability
    (or even the ones that are constant) or attributes that are completely random
    and do not contribute to the model’s learning. Let’s consider an example of a
    sentence:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个属性是最重要的，因为噪声数据会导致机器学习算法性能低下。对于之前介绍的类别噪声，重要的是检查数据标签是否没有矛盾，并检查标签是否分配正确。矛盾标签可以自动找到，但错误标注的数据点需要人工评估。对于属性噪声，我们可以使用统计方法来识别具有低变异性（甚至完全恒定的）或对模型学习不贡献的完全随机的属性。让我们考虑一个句子的例子：
- en: '`Mike is not a` `tall boy.`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`Mike is not a` `tall boy.`'
- en: 'If we use the same feature extraction technique as for the previous sentences,
    our feature matrix looks like what’s shown in *Figure 2**.8*. We use the same
    features as for sentences 0 and 1 for sentence 2\. Since the last sentence differs
    only in terms of one word (not), this can lead to class noise. The last column
    has a label, `not`. This can happen when we train the model on one dataset and
    apply it to another one. This means that the first sentence and the last sentence
    have identical feature vectors, but different labels:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用与之前句子相同的特征提取技术，我们的特征矩阵看起来就像*图2*.8中所示的那样。对于句子2，我们使用与句子0和1相同的特征。由于最后一个句子仅在一词（not）上有所不同，这可能导致类别噪声。最后一列有一个标签，`not`。这可能会发生在我们在一个数据集上训练模型并将其应用于另一个数据集时。这意味着第一个句子和最后一个句子具有相同的特征向量，但标签不同：
- en: '| **Sentence ID** | **Mike** | **Is** | **A** | **tall** | **boy** | **smart**
    | **girl** | **Label** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **句子ID** | **Mike** | **Is** | **A** | **tall** | **boy** | **smart** | **girl**
    | **标签** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| `0` | `1` | `1` | `1` | `1` | `1` | `0` | `0` | `1` |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| `0` | `1` | `1` | `1` | `1` | `1` | `0` | `0` | `1` |'
- en: '| `1` | `0` | `1` | `1` | `0` | `0` | `1` | `1` | `1` |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `0` | `1` | `1` | `0` | `0` | `1` | `1` | `1` |'
- en: '| `2` | `1` | `1` | `1` | `1` | `1` | `0` | `0` | `0` |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `2` | `1` | `1` | `1` | `1` | `1` | `0` | `0` | `0` |'
- en: Figure 2.8 – Noisy dataset
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 – 噪声数据集
- en: Having two different annotations for the same feature vectors is problematic
    as the machine learning algorithms cannot learn the pattern since there isn’t
    one for these noisy data points. Therefore, we need to validate the data in terms
    of it being free from noise.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于同一特征向量存在两种不同的标注是有问题的，因为机器学习算法无法学习这些噪声数据点的模式。因此，我们需要验证数据是否无噪声。
- en: Another property that the data needs to possess is its timeliness – that is,
    being fresh. We must use current, not old, data. One of the areas where this is
    of utmost importance is autonomous driving, where we need to keep the models up
    to date with the latest conditions (for example, the latest traffic signs).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 数据需要具备的另一属性是其时效性——也就是说，数据必须是新鲜的。我们必须使用当前数据，而不是旧数据。在自动驾驶领域，这一点尤为重要，我们需要确保模型与最新条件（例如，最新的交通标志）保持更新。
- en: Finally, the most important part of validation is assessing whether the data
    is fit for purpose. Note that this assessment cannot be done automatically as
    it needs to be done expertly.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，验证最重要的部分是评估数据是否适合使用。请注意，这种评估不能自动完成，因为它需要专家进行。
- en: Configuration and monitoring
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置和监控
- en: Machine learning software is meant to be professionally engineered, deployed,
    and maintained. Modern companies call this process *MLOps*, which means that the
    same team needs to take responsibility for both the development and operations
    of the machine learning system. The rationale behind this extended responsibility
    is that the team knows the system best and therefore can configure, monitor, and
    maintain it in the best possible way. The teams know the design decisions that
    must be taken when developing the system, assumptions made about the data, and
    potential risks to monitor after the deployment.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习软件旨在进行专业化的设计、部署和维护。现代公司称这个过程为*MLOps*，这意味着同一个团队需要负责机器学习系统的开发和运营。这种扩展责任背后的逻辑是，团队最了解系统，因此可以以最佳方式对其进行配置、监控和维护。团队知道在开发系统时必须做出的设计决策，对数据的假设，以及在部署后需要监控的潜在风险。
- en: Configuration
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置
- en: Configuration is one such design decision that’s made by the development team.
    The team configures the parameters of the machine learning models, the execution
    environment, and the monitoring infrastructure. Let’s explore the first one; the
    latter two will be discussed in the next few sections.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 配置是开发团队做出的设计决策之一。团队配置机器学习模型的参数、执行环境和监控基础设施。让我们探讨第一个；后两个将在接下来的几节中讨论。
- en: To exemplify this challenge, let’s look at a random forest classifier for a
    dataset for classifying events during a specific surgery. The classifier, at least
    in its Python implementation, has 16 hyperparameters ([https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)).
    Each of these hyperparameters has several values, which means that finding the
    optimal set of hyperparameters can be a tedious task.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个挑战，让我们看看一个用于在特定手术期间分类事件的随机森林分类器。该分类器，至少在其Python实现中，有16个超参数（[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)）。这些超参数中的每一个都有几个值，这意味着找到最佳的超参数集可能是一项繁琐的任务。
- en: 'However, in practice, we do not need to explore all hyperparameters’ values
    and we do not need to explore all combinations. We should only explore the ones
    that are the most relevant for our task and, by extension, the dataset. I usually
    explore only two hyperparameters as these are the most important: the number of
    trees and the depth of the tree. The first determines how broad the forest is,
    while the second determines how deep it is. The code to specify these parameters
    can look something like this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，我们不需要探索所有超参数的值，也不需要探索所有组合。我们只需探索与我们任务和，通过扩展，数据集最相关的那些。我通常只探索两个超参数，因为它们是最重要的：树木的数量和树的深度。第一个决定了森林有多宽，而第二个决定了它有多深。指定这些参数的代码可能看起来像这样：
- en: '[PRE1]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `n_estimators` hyperparameter is the number of trees, while `max_depth`
    hyperparameter is the depth of each tree. The values of these parameters depend
    on the dataset – how many features we have and how many data points we have. If
    we have too many trees and too many leaves compared to the number of features
    and data points, the classifier gets overfitted and cannot generalize from the
    data. This means that the classifier has learned to recognize each instance rather
    than recognize patterns in the data – we call this *overfitting*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_estimators`超参数是树木的数量，而`max_depth`超参数是每棵树的深度。这些参数的值取决于数据集——我们有多少个特征以及有多少个数据点。如果我们有太多的树木和叶子，与特征和数据点的数量相比，分类器就会过拟合，无法从数据中泛化。这意味着分类器学会了识别每个实例，而不是识别数据中的模式——我们称这种情况为*过拟合*。'
- en: If we have too few trees or leaves, then the generalized patterns will be too
    broad and therefore we observe errors in classification – at least more errors
    than the optimal classifier. We call this *underfitting* as the model does not
    learn the pattern correctly.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果树木或叶子太少，那么泛化模式就会过于宽泛，因此我们在分类中观察到错误——至少比最佳分类器更多的错误。我们称这种情况为*欠拟合*，因为模型没有正确学习到模式。
- en: 'So, we can write a piece of code that would search for the best combination
    of these two parameters based on the pre-defined set of values. The code to find
    the best parameters manually would look something like this:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以编写一段代码，根据预定义的值集搜索这两个参数的最佳组合。手动寻找最佳参数的代码可能看起来像这样：
- en: '[PRE2]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The two lines emphasized in orange show two loops that explore these parameters
    – the content of the inner loop trains the classifier with these parameters and
    prints out the output.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 突出显示的两行橙色代码展示了探索这些参数的两个循环——内循环循环使用这些参数训练分类器并打印输出。
- en: 'Let’s apply this algorithm to physiological data from patients who have undergone
    operations. When we plot the output on the diagram, as shown in *Figure 2**.9*,
    we can observe how the accuracy evolves. If we set the number of trees to 2, the
    classifier’s best performance is the best for 8 leaves, but even then, it does
    not classify the events perfectly. For four trees, the classifier achieves the
    best performance with 128 leaves, and the accuracy is 1.0\. From the following
    diagram, we can see that adding more trees does not improve the results significantly:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个算法应用于经历过手术的患者的生理数据。当我们把输出绘制在图表上，如图*图2**.9所示，我们可以观察到准确率是如何演变的。如果我们把树木的数量设置为2，分类器的最佳性能是在8个叶子时达到的，但即使如此，它也无法完美地分类事件。对于四棵树，分类器在128个叶子时达到最佳性能，准确率为1.0。从下面的图表中，我们可以看到增加更多的树木并没有显著提高结果：
- en: "![Figure 2.9 – Accuracy per number of estimators and leaves. The labels of\
    \ the \uFEFFx axis show the number of trees (before the underscore) and the number\
    \ of leaves (after the underscore)](img/B19548_02_05.jpg)"
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.9 – 每个估计器和叶子数目的准确率。x 轴的标签显示了树的数量（下划线之前）和叶子的数量（下划线之后）](img/B19548_02_05.jpg)'
- en: Figure 2.9 – Accuracy per number of estimators and leaves. The labels of the
    x axis show the number of trees (before the underscore) and the number of leaves
    (after the underscore)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 – 每个估计器和叶子数目的准确率。x 轴的标签显示了树的数量（下划线之前）和叶子的数量（下划线之后）
- en: For this example, the time required to search for the best result is relatively
    short – it takes up to 1-2 minutes on a standard laptop. However, if we want to
    find the optimal combination of all 16 parameters, we will spend a significant
    amount of time doing this.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，搜索最佳结果所需的时间相对较短——在标准笔记本电脑上最多需要 1-2 分钟。然而，如果我们想找到所有 16 个参数的最佳组合，我们将花费相当多的时间来做这件事。
- en: 'There is a more automated way of finding the optimal parameters of machine
    learning classifiers – different types of search algorithms. One of the most popular
    ones is the GridSearch algorithm ([https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)),
    which works similarly to our manual script, except that it can do cross-validation
    with multiple splits and many other statistical tricks to improve the search.
    Searching for the optimal solution with the GridSearch algorithm can look something
    like this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种更自动化的方法来找到机器学习分类器的最佳参数——不同类型的搜索算法。其中最受欢迎的一种是 GridSearch 算法 ([https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html))，它的工作方式与我们的手动脚本类似，只不过它可以进行交叉验证，有多个分割，以及许多其他统计技巧来改进搜索。使用
    GridSearch 算法搜索最佳解决方案可能看起来像这样：
- en: '[PRE3]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The preceding code finds the best solution and saves it as the `best_estimator_`
    parameter of the GridSearch model. In the case of this dataset and this model,
    the algorithm finds the best random forest to be the one with 128 trees (`n_estimators`)
    and 4 levels (`max_depth`). The results are a bit different than the ones found
    manually, but this does not mean that one of the methods is superior.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码找到了最佳解决方案并将其保存为 GridSearch 模型的 `best_estimator_` 参数。在这个数据集和模型的情况下，算法找到了最佳随机森林，它有
    128 棵树（`n_estimators`）和 4 个层级（`max_depth`）。结果与手动找到的结果略有不同，但这并不意味着其中一种方法更优越。
- en: However, having too many trees can result in overfitting, so I would choose
    the model with 4 trees and 128 leaves over the one with 128 trees and 4 levels.
    Or maybe I would also use a model that is somewhere in-between – that is, a model
    that has the same accuracy but is less prone to overfitting in either the depth
    or the width.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，树的数量过多可能会导致过拟合，因此我宁愿选择有 4 棵树和 128 个叶子的模型，而不是有 128 棵树和 4 个层级的模型。或者，也许我还会使用一个介于两者之间的模型——也就是说，一个具有相同准确率但不太容易在深度或宽度上过拟合的模型。
- en: This leads to my next best practice.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致我的下一个最佳实践。
- en: 'Best practice #9'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #9'
- en: Use GridSearch and other algorithms after you have explored the parameter search
    space manually.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在手动探索参数搜索空间之后，使用 GridSearch 和其他算法。
- en: Although the automated parameter search algorithms are very useful, they hide
    properties of the data from us and do not allow us to explore the data and the
    parameters ourselves. From my experience, understanding the data, the model, its
    parameters, and its configuration is crucial to the successful deployment of machine
    learning software. I only use GridSearch (or other optimization algorithms) after
    I’ve tried to find some optima manually since I would like to understand the data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自动参数搜索算法非常有用，但它们隐藏了数据的一些特性，并且不允许我们自行探索数据和参数。根据我的经验，理解数据、模型、其参数及其配置对于成功部署机器学习软件至关重要。我只有在尝试手动找到一些最优解之后才会使用
    GridSearch（或其他优化算法），因为我希望理解数据。
- en: Monitoring
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控
- en: Once the machine learning system has been configured, it is set into production,
    often as part of the larger software system. The inferences that are made by machine
    learning are the basis for the features of the product and the business model
    behind this. Therefore, the machine learning component should make as few mistakes
    as possible. Unfortunately, the customers take failures and mistakes more seriously
    than correctly functioning products.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦机器学习系统配置完成，它就会被投入生产，通常作为更大软件系统的一部分。机器学习所做的推断是产品特性和背后商业模式的基础。因此，机器学习组件应尽可能减少错误。不幸的是，客户对失败和错误的关注程度高于正确运行的产品。
- en: However, the performance of the machine learning system degrades over time,
    but not because of low-quality programming or design – this is the nature of probabilistic
    computing. Therefore, all machine learning systems need to be monitored and maintained.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，机器学习系统的性能会随时间退化，但这并不是因为编程或设计质量低劣——这是概率计算的本质。因此，所有机器学习系统都需要监控和维护。
- en: 'One of the aspects that needs to be monitored is called *concept drift*. Concept
    drift is a phenomenon in the data, which means that the distribution of entities
    in the data changes over time for natural reasons. *Figure 2**.10* illustrates
    concept drift for a machine learning classifier (blue and red lines) of images
    of yellow and orange trucks:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 需要监控的一个方面被称为*概念漂移*。概念漂移是数据中的一个现象，这意味着数据中实体的分布因自然原因随时间变化。*图2.10*展示了机器学习分类器（蓝色和红色线条）对黄色和橙色卡车图像的概念漂移：
- en: '![Figure 2.10 – Illustration of concept drift. The original distribution of
    the objects (on the left) changes over time (on the right), so the classifier
    must be retrained (blue versus red lines)](img/B19548_02_06.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图2.10 – 概念漂移的示意图。物体（左侧）的原始分布随时间（右侧）变化，因此分类器必须重新训练（蓝色与红色线条）](img/B19548_02_06.jpg)'
- en: Figure 2.10 – Illustration of concept drift. The original distribution of the
    objects (on the left) changes over time (on the right), so the classifier must
    be retrained (blue versus red lines)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 – 概念漂移的示意图。物体（左侧）的原始分布随时间（右侧）变化，因此分类器必须重新训练（蓝色与红色线条）
- en: The left-hand side shows the original distribution of data that was used to
    train the model. The model is conceptually shown as the blue dotted line. The
    model recognizes the differences between the two classes of images. However, over
    time, the data can change. New images can appear in the dataset and the distribution
    can change. The original model starts to make mistakes in inference and therefore
    needs to be adjusted. The re-trained model – that is, the solid red line – captures
    the new distribution of the data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧显示了用于训练模型的原始数据分布。模型在概念上表示为蓝色虚线。模型能够识别两种图像类别之间的差异。然而，随着时间的推移，数据可能会发生变化。新的图像可能会出现在数据集中，分布也会发生变化。原始模型在推理中开始出错，因此需要调整。重新训练的模型——即实心红色线条——捕捉了数据的新分布。
- en: It is this change in the dataset that we call concept drift. It is more common
    in complex datasets and supervised learning models, but the effects of concept
    drift are equally problematic for non-supervised models and reinforcement learning
    models.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集中的这种变化称为概念漂移。它在复杂的数据集和监督学习模型中更为常见，但概念漂移对非监督模型和强化学习模型的影响同样成问题。
- en: '*Figure 2**.11* presents the performance of the same random forest model applied
    to the data from the same distribution (directly after training) and on the data
    after some operation. Concept drift is visible in the accuracy reduction from
    1.0 to 0.44\. The model has been trained on the same data as in the example in
    *Figure 2**.9* but has been applied to data from another patient:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.11*展示了应用于相同分布（直接在训练后）和经过某些操作后的数据的同一随机森林模型的表现。概念漂移在准确率从1.0降至0.44的降低中可见。该模型与*图2.9*中的示例使用相同的数据进行训练，但应用于来自另一位患者的数据：'
- en: '![Figure 2.11 – An example of performance decrease before and after concept
    drift](img/B19548_02_07.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图2.11 – 概念漂移前后性能降低的示例](img/B19548_02_07.jpg)'
- en: Figure 2.11 – An example of performance decrease before and after concept drift
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 – 概念漂移前后性能降低的示例
- en: Therefore, I would like to introduce my next best practice.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我想介绍我的下一个最佳实践。
- en: 'Best practice #10'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践#10
- en: Always include monitoring mechanisms in your machine learning systems.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的机器学习系统中始终包含监控机制。
- en: Including mechanisms to monitor concept drift, even a simple mechanism such
    as using a Chi-square statistical test for the similarity of distributions, makes
    a lot of difference. It allows us to identify problems in the system, troubleshoot
    them, and prevent them from spreading to other parts of the software, or even
    to the end user of the software.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是使用卡方统计测试来比较分布相似性的简单机制，包括监控概念漂移的机制，也能产生很大的影响。它使我们能够识别系统中的问题，解决问题，并防止它们扩散到软件的其他部分，甚至影响到软件的最终用户。
- en: Professional machine learning engineers set up monitoring mechanisms for concept
    drift in production, which indicates the degradation of AI software.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 专业机器学习工程师在生产中设置了概念漂移的监控机制，这表明人工智能软件的退化。
- en: Infrastructure and resource management
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础设施和资源管理
- en: The infrastructure and resources needed for the machine learning software are
    organized into two areas – data serving infrastructure (for example, databases)
    and computational infrastructure (for example, GPU computing platforms). There
    is also serving infrastructure, which is used to provide the services to the end
    users. The serving infrastructure could be in the form of desktop applications,
    embedded software (such as the one in autonomous vehicles), add-ins to tools (as
    in the case of GitHub Co-pilot), or websites (such as ChatGPT). However, in this
    book, we’ll focus on the data-serving infrastructure and the computational infrastructure.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习软件所需的基础设施和资源被组织成两个区域——数据服务基础设施（例如，数据库）和计算基础设施（例如，GPU计算平台）。还有服务基础设施，用于向最终用户提供服务。服务基础设施可以是桌面应用程序、嵌入式软件（例如自动驾驶汽车中的软件）、工具的插件（如GitHub
    Co-pilot的情况）或网站（如ChatGPT）。然而，在这本书中，我们将重点关注数据服务基础设施和计算基础设施。
- en: Both areas can be deployed locally or remotely. Local deployment means that
    we use our own infrastructure at the company, while remote infrastructure means
    that we use cloud services or services of another supplier.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个区域都可以本地或远程部署。本地部署意味着我们使用公司自己的基础设施，而远程基础设施意味着我们使用云服务或其他供应商的服务。
- en: 'Conceptually, we could see these two areas as co-dependent, as depicted in
    *Figure 2**.12*:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，我们可以将这些区域视为相互依赖的，如图*图2.12*所示：
- en: '![Figure 2.12 – Co-dependency between serving, computing, and data-serving
    infrastructure](img/B19548_02_08.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图2.12 – 服务、计算和数据服务基础设施之间的相互依赖关系](img/B19548_02_08.jpg)'
- en: Figure 2.12 – Co-dependency between serving, computing, and data-serving infrastructure
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 – 服务、计算和数据服务基础设施之间的相互依赖关系
- en: The data serving infrastructure provisions data that’s used for the computation
    infrastructure. It consists of databases and other data sources (for example,
    raw files). The computation infrastructure consists of computing infrastructure
    to train and test machine learning models. Finally, the user-serving infrastructure
    uses the models to make inferences and provides services and functionality to
    the end users.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 数据服务基础设施提供用于计算基础设施的数据。它包括数据库和其他数据源（例如，原始文件）。计算基础设施包括用于训练和测试机器学习模型的计算资源。最后，用户服务基础设施使用模型进行推理，并向最终用户提供服务和功能。
- en: Data serving infrastructure
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据服务基础设施
- en: Data serving infrastructure is one of the fundamental parts of the machine learning
    software because there is no machine learning if there is no data. Data-hungry
    machine learning applications pose new requirements for the infrastructure in
    terms of performance, reliability, and traceability. The last requirement is very
    important as the machine learning training data determines how the trained machine
    learning model makes inferences. In the case of defects in the end user function,
    software engineers need to scrutinize the algorithms, the models, and the data
    that were used to construct the failing machine learning system.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 数据服务基础设施是机器学习软件的基本组成部分之一，因为没有数据就没有机器学习。对数据有需求的人工智能应用对基础设施在性能、可靠性和可追溯性方面提出了新的要求。最后一个要求非常重要，因为机器学习训练数据决定了训练好的机器学习模型如何进行推理。在终端用户功能出现缺陷的情况下，软件工程师需要仔细审查用于构建失败机器学习系统的算法、模型和数据。
- en: 'In contrast to traditional software, the data-serving infrastructure is often
    composed of three different parts, as shown in *Figure 2**.13*:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统软件相比，数据服务基础设施通常由三个不同的部分组成，如图*图2.13*所示：
- en: '![Figure 2.13 – Data serving infrastructure](img/B19548_02_09.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图2.13 – 数据服务基础设施](img/B19548_02_09.jpg)'
- en: Figure 2.13 – Data serving infrastructure
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 – 数据服务基础设施
- en: The data is stored in persistent storage, such as in a database on a hard drive.
    It can be stored locally or on a cloud server. The most important part is that
    this data is secure and can be accessed for further processing. The persistently
    stored data needs to be extracted so that it can be used in machine learning.
    The first step is to find the snapshot of the data that is needed – for example,
    by selecting data for training. The snapshot needs to be prepared and formatted
    in tabular form so that the machine learning algorithms can use the data to make
    inferences.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储在持久存储中，例如硬盘上的数据库。它可以是本地存储或云服务器上的存储。最重要的是，这些数据是安全的，并且可以用于进一步处理。持久存储的数据需要提取出来，以便在机器学习中使用。第一步是找到所需的数据快照——例如，通过选择用于训练的数据。快照需要准备并格式化为表格形式，以便机器学习算法可以使用这些数据进行推理。
- en: There are several different types of databases today that machine learning systems
    use. First, there are standard relational databases, where data is stored in the
    form of tables. These are the databases that are well known and used widely both
    in traditional and machine learning software.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，机器学习系统使用了几种不同类型的数据库。首先，有标准的数据库，数据以表格形式存储。这些是众所周知且在传统和机器学习软件中广泛使用的数据库。
- en: Newer types of databases are non-SQL databases such as Elasticsearch ([https://www.elastic.co](https://www.elastic.co)),
    which are designed to store documents, not tables. These documents are indexed
    flexibly so that data can be stored and retrieved based on these documents. In
    the case of machine learning software, these documents can be images, entire text
    documents, or even sound data. This is important for storing data in the same
    format as it is collected so that we can trace the data when needed.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 新类型的数据库是非SQL数据库，如Elasticsearch ([https://www.elastic.co](https://www.elastic.co))，它被设计用于存储文档，而不是表格。这些文档可以灵活索引，以便根据这些文档存储和检索数据。在机器学习软件的情况下，这些文档可以是图像、整个文本文档，甚至是声音数据。这对于以收集时的相同格式存储数据非常重要，这样我们就可以在需要时追踪数据。
- en: Regardless of the format of the data in the databases, it is retrieved from
    the database and transformed into tabular form; we’ll discuss this in [*Chapter
    3*](B19548_03.xhtml#_idTextAnchor038). This tabular form is required for the data
    to be processed by the computational infrastructure.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 不论数据库中数据的格式如何，它都是从数据库中检索出来并转换为表格形式的；我们将在[*第3章*](B19548_03.xhtml#_idTextAnchor038)中讨论这一点。这种表格形式是数据处理所需的基础设施。
- en: With that, we’ve come to my next best practice.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就来到了我的下一个最佳实践。
- en: 'Best practice #11'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践#11
- en: Choose the right database for your data – look at this from the perspective
    of the data, not the system.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为您的数据选择正确的数据库——从数据的角度而不是系统的角度来考虑。
- en: Although it sounds obvious that we should choose the right database for the
    data that we have, for machine learning systems, it is important to select the
    database that works best for the data at hand, not the system. For example, when
    we use natural language processing models, we should store the data in documents
    that we can easily retrieve in an organized form.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然选择适合我们数据的正确数据库听起来很明显，但对于机器学习系统来说，选择最适合当前数据的数据库，而不是系统本身，这一点非常重要。例如，当我们使用自然语言处理模型时，我们应该将数据存储在我们可以轻松检索并按组织形式存储的文档中。
- en: Computational infrastructure
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算基础设施
- en: The computational infrastructure can change over time. In the early phases of
    the development of machine learning systems, software developers often use pre-configured
    experimentation environments. These can be in the form of Jupyter Notebooks on
    their computers or in the form of pre-configured services such as Google Colab
    or Microsoft Azure Notebooks. This kind of infrastructure supports rapid prototyping
    of machine learning, easy provisioning of data, and no need for setting up advanced
    features. They also allow us to easily scale the computational resources up and
    down without the need to add or remove extra hardware.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 计算基础设施可能会随时间变化。在机器学习系统开发的早期阶段，软件开发者通常会使用预配置的实验环境。这些可以是他们电脑上的Jupyter Notebooks，也可以是预配置的服务，如Google
    Colab或Microsoft Azure Notebooks。这种基础设施支持机器学习的快速原型设计，易于数据提供，无需设置高级功能。它们还允许我们轻松地根据需要调整计算资源，而无需添加或移除额外硬件。
- en: An alternative to this approach is to use our own infrastructure, where we set
    up your own servers and runtime environments. It requires more effort, but it
    provides us with full control over the computational resources, as well as full
    control over data processing. Having full control over data processing can sometimes
    be the most important factor for selecting the infrastructure.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的替代方案是使用我们自己的基础设施，在那里我们为您设置自己的服务器和运行时环境。这需要更多的努力，但它使我们能够完全控制计算资源，以及完全控制数据处理。对数据处理拥有完全控制权有时可能是选择基础设施最重要的因素。
- en: Hence, my next best practice.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我的下一个最佳实践。
- en: 'Best practice #12'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #12'
- en: Use cloud infrastructure if you can as it saves resources and reduces the need
    for specialized competence.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能，请使用云基础设施，因为它可以节省资源并减少对专业技术的需求。
- en: Professional AI engineers use self-owned infrastructure for prototyping and
    training and cloud-based infrastructure for production as it scales better with
    the number of users. The opposite, which is to use our own infrastructure, is
    true only if we need to retain full control over data or infrastructure. Full
    control can be required for applications that use sensitive customer data, military
    applications, security applications, and other applications where data is extremely
    sensitive.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 专业AI工程师使用自有的基础设施进行原型设计和训练，并使用基于云的基础设施进行生产，因为它的扩展性更好，可以随着用户数量的增加而扩展。相反，即使用我们自己的基础设施，也只有在我们需要保留对数据或基础设施的完全控制时才是正确的。对于使用敏感客户数据、军事应用、安全应用和其他数据极其敏感的应用程序，可能需要完全控制。
- en: Luckily, we have several large actors providing the computational infrastructure,
    as well as a large ecosystem of small actors. The three largest – Amazon Web Services,
    Google Cloud, and Microsoft Azure – can provide all kinds of services for both
    small and large enterprises. Amazon Web Services ([https://aws.amazon.com](https://aws.amazon.com))
    specializes in provisioning data storage and processing infrastructure. It is
    often used for applications that must quickly process large quantities of data.
    The infrastructure is professionally maintained and can be used to achieve near-perfect
    reliability of the products built on that platform. To use it efficiently, you
    must usually work with containers and virtual machines that execute the code of
    the machine learning application.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们有几个大型演员提供计算基础设施，以及一个庞大的小型演员生态系统。其中最大的三个是亚马逊网络服务（Amazon Web Services）、谷歌云（Google
    Cloud）和微软Azure（Microsoft Azure），它们可以为小型和大型企业提供各种服务。亚马逊网络服务（[https://aws.amazon.com](https://aws.amazon.com)）专注于提供数据存储和处理基础设施。它通常用于必须快速处理大量数据的应用程序。该基础设施由专业人员维护，并可用于实现基于该平台构建的产品的高近完美可靠性。为了高效使用它，你通常必须与执行机器学习应用程序代码的容器和虚拟机一起工作。
- en: Google Cloud ([https://cloud.google.com](https://cloud.google.com)) specializes
    in provisioning platforms for data-intensive applications and computation-intensive
    solutions. Thanks to Google’s processors (**tensor processing units** (**TPUs**)),
    the platform provides a very efficient environment for both training and using
    machine learning solutions. Google Cloud also provides free solutions for learning
    machine learning in the form of Google Colab, which is an extension of the Jupyter
    Notebook ([https://jupyter.org](https://jupyter.org)) platform on Python.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌云（[https://cloud.google.com](https://cloud.google.com)）专注于为数据密集型应用程序和计算密集型解决方案提供平台。多亏了谷歌的处理器（**张量处理单元**（**TPUs**）），该平台为训练和使用机器学习解决方案提供了一个非常高效的环境。谷歌云还提供免费的学习机器学习解决方案，形式为Google
    Colab，它是Python平台Jupyter Notebook（[https://jupyter.org](https://jupyter.org)）的扩展。
- en: Microsoft Azure ([https://azure.microsoft.com](https://azure.microsoft.com))
    specializes in provisioning platforms for training and deploying machine learning
    systems in the form of virtual machines. It also provides ready-to-deploy models
    for image recognition, classification, and natural language processing, and even
    platforms for training generic machine learning models. It is the most flexible
    of the available platforms and the most scalable.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 微软Azure（[https://azure.microsoft.com](https://azure.microsoft.com)）专注于提供虚拟机形式的机器学习系统训练和部署平台。它还提供用于图像识别、分类和自然语言处理的现成可部署模型，甚至提供用于训练通用机器学习模型的平台。它是所有可用平台中最灵活的，也是最具可扩展性的。
- en: In addition to these platforms, you can use several specialized ones, such as
    Facebook’s platform for machine learning, which specializes in recommender systems.
    However, since the specialized platforms are often narrow, we need to remember
    the potential issues if we want to port our software from one platform to another.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些平台，你还可以使用几个专门的平台，例如Facebook的机器学习平台，该平台专注于推荐系统。然而，由于专门的平台通常比较狭窄，如果我们想将我们的软件从一个平台迁移到另一个平台，我们需要记住可能存在的问题。
- en: Hence, my next best practice.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我的下一个最佳实践。
- en: 'Best practice #13'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践#13
- en: Decide on which production environment you wish to use early and align your
    process with that environment.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 早期确定你希望使用的生产环境，并将你的流程与该环境对齐。
- en: We need to decide if we want to use Amazon’s, Google’s, or Microsoft’s cloud
    environment or whether we want to use our own infrastructure to reduce the cost
    of software development. Although it is possible to move our software between
    these environments, it is not straightforward and requires (at best) significant
    testing and pre-deployment validation, which often comes with significant costs.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要决定是想使用亚马逊的、谷歌的，还是微软的云环境，或者我们是否想使用自己的基础设施以降低软件开发成本。虽然可以在这些环境之间移动我们的软件，但这并不简单，并且需要（最好）进行大量的测试和预部署验证，这通常伴随着显著的成本。
- en: How this all comes together – machine learning pipelines
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所有这些如何结合在一起——机器学习管道
- en: 'In this chapter, we explored the main characteristics of machine learning systems
    and compared them to traditional software systems. Let’s finish this comparison
    by summarizing how we usually design and describe machine learning systems – by
    using pipelines. A pipeline is a sequence of data processing steps, including
    the machine learning models. The typical set of steps (also called phases) is
    shown in *Figure 2**.14*:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了机器学习系统的主要特征，并将它们与传统软件系统进行了比较。让我们通过总结我们通常如何设计和描述机器学习系统——通过使用管道来完成这一比较。管道是一系列数据处理步骤，包括机器学习模型。典型的步骤集（也称为阶段）如图*2.14*所示：
- en: '![Figure 2.14 – A typical sequence of steps in a machine learning pipeline](img/B19548_02_10.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图2.14 – 机器学习管道中的典型步骤序列](img/B19548_02_10.jpg)'
- en: Figure 2.14 – A typical sequence of steps in a machine learning pipeline
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 – 机器学习管道中的典型步骤序列
- en: This kind of pipeline, although drawn linearly, is usually processed in cycles,
    where, for example, monitoring for concept drift can trigger re-training, re-testing,
    and re-deployment.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的管道，尽管是线性绘制的，但通常是在循环中处理的，例如，监控概念漂移可以触发重新训练、重新测试和重新部署。
- en: Machine learning pipelines, just like the one presented in *Figure 2**.14*,
    are often depicted as a set of components as parts of the entire system. However,
    presenting it using the pipeline analogy helps us understand that machine learning
    systems process data in steps.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习管道，就像图*2.14*中展示的那样，通常被描绘为整个系统的一部分组件集。然而，使用管道类比来展示它有助于我们理解机器学习系统是按步骤处理数据的。
- en: In the next chapter, we’ll explore the first part of the pipeline – working
    with data. We’ll start by exploring different types of data and how these types
    of data are collected, processed, and used in modern software systems.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨管道的第一部分——数据处理。我们将从探索不同类型的数据以及这些类型的数据在现代软件系统中的收集、处理和使用开始。
- en: References
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Shortliffe, E.H., et al., Computer-based consultations in clinical therapeutics:
    explanation and rule acquisition capabilities of the MYCIN system. Computers and
    biomedical research, 1975\. 8(4):* *p. 303-320.*'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Shortliffe, E.H.，等人，基于计算机的临床治疗咨询：MYCIN系统的解释和规则获取能力。计算机与生物医学研究，1975年。8(4)：*
    *p. 303-320。*'
- en: '*Vaswani, A., et al., Attention is all you need. Advances in neural information
    processing systems,* *2017\. 30.*'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Vaswani, A.，等人，注意力就是一切。神经信息处理系统进展，2017年。30。*'
- en: '*Dale, R., GPT-3: What’s it good for? Natural Language Engineering, 2021\.
    27(1):* *p. 113-118.*'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dale, R.，GPT-3：它有什么用？自然语言工程，2021年。27(1)：* *p. 113-118。*'
- en: '*Smith, S., et al., Using deepspeed and megatron to train megatron-turing nlg
    530b, a large-scale generative language model. arXiv preprint* *arXiv:2201.11990,
    2022.*'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Smith, S.，等人，使用deepspeed和megatron训练megatron-turing nlg 530b，一个大规模生成语言模型。arXiv预印本*
    *arXiv:2201.11990，2022年。*'
- en: '*Lee, Y.W., et al., AIMQ: a methodology for information quality assessment.
    Information & management, 2002\. 40(2):* *p. 133-146.*'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Lee, Y.W.，等人，AIMQ：信息质量评估的方法。信息与管理，2002年。40(2)：* *p. 133-146。*'
- en: '*Zenisek, J., F. Holzinger, and M. Affenzeller, Machine learning based concept
    drift detection for predictive maintenance. Computers & Industrial Engineering,
    2019\. 137:* *p. 106031.*'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Zenisek, J., F. Holzinger, and M. Affenzeller, 基于机器学习的概念漂移检测用于预测性维护。计算机与工业工程，2019\.
    137:* *p. 106031.*'
- en: '*Amershi, S., et al. Software engineering for machine learning: A case study.
    in 2019 IEEE/ACM 41st International Conference on Software Engineering: Software
    Engineering in Practice (ICSE-SEIP).* *2019\. IEEE.*'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Amershi, S., 等人. 软件工程在机器学习中的应用：一个案例研究。在2019 IEEE/ACM 第41届国际软件工程会议：软件工程实践（ICSE-SEIP）。*
    *2019\. IEEE.*'
