- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elements of a Machine Learning System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data and algorithms are crucial for machine learning systems, but they are far
    from sufficient. Algorithms are the smallest part of a production machine learning
    system. Machine learning systems also require data, infrastructure, monitoring,
    and storage to function efficiently. For a large-scale machine learning system,
    we need to ensure that we can include a good user interface or package model in
    microservices.
  prefs: []
  type: TYPE_NORMAL
- en: In modern software systems, combining all necessary elements requires different
    professional competencies – including machine learning/data science engineering
    expertise, database engineering, software engineering, and finally interaction
    design. In these professional systems, it is more important to provide reliable
    results that bring value to users rather than include a lot of unnecessary functionality.
    It is also important to orchestrate all elements of machine learning together
    (data, algorithms, storage, configuration, and infrastructure) rather than optimize
    each one of them separately – all to provide the most optimal system for one or
    more use cases from end users.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll review each element of a professional machine learning
    system. We’ll start by understanding which elements are important and why. Then,
    we’ll explore how to create such elements and how to put them together into a
    single machine learning system – a so-called machine learning pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is more than just algorithms and data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data and algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration and monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure and resource management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elements of a production machine learning system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern machine learning algorithms are very capable because they use large quantities
    of data and consist of a large number of trainable parameters. The largest available
    models are **Generative Pre-trained Transformer-3** (**GPT-3**) from OpenAI (with
    175 billion parameters) and Megatron-Turing from NVidia (356 billion parameters).
    These models can create texts (novels) and make conversations but also write program
    code, create user interfaces, or write requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Now, such large models cannot be used on a desktop computer, laptop, or even
    in a dedicated server. They need advanced computing infrastructure, which can
    withstand long-term training and evaluation of such large models. Such infrastructure
    also needs to provide means to automatically provide these models with data, monitor
    the training process, and, finally, provide the possibility for the users to access
    the models to make inferences. One of the modern ways of providing such infrastructure
    is the concept of **Machine learning as a service** (**MLaaS**). MLaaS provides
    an easy way to use machine learning from the perspective of data analysts of software
    integrators since it delegates the management, monitoring, and configuration of
    the infrastructure to specialized companies.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.1* shows elements of modern machine learning-based software systems.
    Google has used these to describe production machine learning systems since then.
    Although variations in this setup exist, the principles remain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Elements of a production machine learning system](img/B19548_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Elements of a production machine learning system
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the machine learning model (**ML code**) is the smallest of these elements
    (Google, under the Creative Commons 4.0 Attribution License, [https://developers.google.com/machine-learning/crash-course/production-ml-systems](https://developers.google.com/machine-learning/crash-course/production-ml-systems)).
    In terms of the actual source code, in Python, model creation, training, and validation
    are just three lines of code (at least for some of the models):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first line creates the model from a template – in this case, it is a random
    forest model with 10 trees, each of which has a maximum of two splits. Random
    forest is an ensemble learning method that constructs multiple decision trees
    during training and outputs the mode of the classes (classification) of the individual
    trees for a given input. By aggregating the results of multiple trees, it reduces
    overfitting and provides higher accuracy compared to a single decision tree. The
    second line trains the model based on the training data (`X_train`, which contains
    only the preditors/input features, and `Y_train`, which contains the predicted
    class/output features). Finally, the last line makes predictions for the test
    data (`X_test`) to compare it to the oracle (the expected value) in subsequent
    steps. Even though this `model.predict(X_test)` line is not part of a production
    system, we still need to make inferences, so there is always a similar line in
    our software.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we can introduce the next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #5'
  prefs: []
  type: TYPE_NORMAL
- en: When designing machine learning software, prioritize your data and the problem
    to solve over the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we saw that the machine learning code, from the perspective
    of the software engineer, is rather small. Before applying algorithms, we need
    to prepare the data correctly as the algorithms (`model.fit(X_train, Y_train)`)
    require the data to be in a specific format – the first parameter is the data
    that’s used to make inferences (so-called feature vectors or input data samples),
    while the second parameter is the target values (so-called decision classes, reference
    values, or target values, depending on the algorithm).
  prefs: []
  type: TYPE_NORMAL
- en: Data and algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, if using the algorithms is not the main part of the machine learning code,
    then something else must be – that is, data handling. Managing data in machine
    learning software, as shown in *Figure 2**.1*, consists of three areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Data collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature extraction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data validation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Although we will go back to these areas throughout this book, let’s explore
    what they contain. *Figure 2**.2* shows the processing pipeline for these areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Data collection and preparation pipeline](img/B19548_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Data collection and preparation pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Note that the process of preparing the data for the algorithms can become quite
    complex. First, we need to extract data from its source, which is usually a database.
    It can be a database of measurements, images, texts, or any other raw data. Once
    we’ve exported/extracted the data we need, we must store it in a raw data format.
    This can be in the form of a table, as shown in the preceding figure, or it can
    be in a set of raw files, such as images.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data collection is a procedure of transforming data from its raw format to
    a format that a machine learning algorithm can take as input. Depending on the
    data and the algorithm, this process can take different forms, as illustrated
    in *Figure 2**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Different forms of data collection – examples](img/B19548_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Different forms of data collection – examples
  prefs: []
  type: TYPE_NORMAL
- en: Data from images and measurements such as time series is usually collected to
    make classifications and predictions. These two classes of problems require the
    ground truth to be available, which we saw as `Y_train` in the previous code example.
    These target labels are either extracted automatically from the raw data or added
    manually through the process of labeling. The manual process is time-consuming,
    so the automated one is preferred.
  prefs: []
  type: TYPE_NORMAL
- en: The data that’s used in non-supervised learning and reinforcement learning models
    is often extracted as tabular data without labels. This data is used in the decision
    process or the optimization process to find the best solution to the given problem.
    Without optimization, there is a risk that our results are not representative
    of new data. The preceding figure shows two examples of such problems – optimizations
    of smart factories of Industry 4.0 and self-driving vehicles. In smart factories,
    reinforcement learning models are used to optimize production processes or control
    so-called *dark factories*, which operate entirely without human intervention
    (the name *dark factories* comes from the fact that there is no need for lights
    in these factories; robots do not need light to work).
  prefs: []
  type: TYPE_NORMAL
- en: The data that’s used for modern self-supervised models often comes from such
    sources as text or speech. These models do not require a tabular form of the data,
    but they require structure. For example, to train text transformer models, we
    need to tokenize the text per sentence (or per paragraph) for the model to learn
    the context of the words.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, here comes my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #6'
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve explored the problem you wish to solve and understood the data availability,
    decide whether you want to use supervised, self-supervised, unsupervised, or reinforcement
    learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that we need different data for different algorithms is natural. However,
    we have not discussed how to decide upon the algorithm. Choosing supervised learning
    only makes sense when we want to predict or classify data statically – that is,
    we train the model and then we use it to make inferences. When we train and then
    make inferences, the model does not change. There is no adjustment as we go and
    re-training the algorithm is done periodically – I call it a *train once, predict*
    *many* principle.
  prefs: []
  type: TYPE_NORMAL
- en: We can choose unsupervised methods when we use/train and apply the algorithm
    without the target class. Some of these algorithms are also used to group data
    based on the data’s property, for example, to cluster it. I call this the *train
    once, predict* *once* principle.
  prefs: []
  type: TYPE_NORMAL
- en: For self-supervised models, the situation is a bit more interesting. There,
    we can use something called *pre-training*. Pre-training means that we can train
    a model on a large corpus of data without any specific context – for example,
    we train language models on large corpora of English texts from Wikipedia. Then,
    when we want to use the model for a specific task, such as to write new text,
    we train it a bit more on that task. I call this the *train many, predict once*
    principle as we must pre-train and train the model for each task.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, reinforcement learning needs data that is changed every time the model
    is used. For example, when we use a reinforcement learning algorithm to optimize
    a process, it updates the model each time it is used – we could say it learns
    from its mistakes. I call this the *train many, predict* *many* principle.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the raw data is not ready to be used with machine learning as it can
    contain empty data points, noise, or broken files. Therefore, we need to clean
    up these erroneous data points, such as by removing empty data points (using Python
    commands such as `dataFrame.dropna(…)`) or using data imputation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Now, there is a fundamental difference between the removal of data points and
    their imputation. The data imputation process is when we add missing properties
    of data based on similar data points. It’s like filling in blanks in a sequence
    of numbers – 1, 2, 3, …, 5, where we fill in the number 4\. Although filling in
    the data increases the number of data points available (thus making the models
    better), it can strengthen certain properties of the data, which can cause the
    model to learn. Imputation is also relevant when the size of the data is small;
    in large datasets, it is better (more resource-efficient and fair) to drop the
    missing data points. With that, we’ve come to my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #7'
  prefs: []
  type: TYPE_NORMAL
- en: Use data imputation only when you know which properties of data you wish to
    strengthen and only do so for small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, once we have clean data to work with, we can extract features. There,
    we can use algorithms that are specific to our problem at hand. For example, when
    we work with textual data, we could use a simple bag-of-words to count the frequencies
    of words, though we can also use the word2vec algorithm to embed the frequencies
    of the co-occurrence of words (algorithms that we’ll discuss in the next few chapters).
    Once we’ve extracted features, we can start validating the data. The features
    can emphasize certain properties of data that we didn’t see before.
  prefs: []
  type: TYPE_NORMAL
- en: One such example is noise – when we have data in a feature format, we can check
    whether there is *attribute* or *class noise* in the data. Class noise is a phenomenon
    that is related to labeling errors – one or more data points have been labeled
    incorrectly. Class noise can be either contradictory examples or wrongly labeled
    data points. It is a dangerous phenomenon since it can cause low performance when
    training and using machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Attribute noise is when one (or more) attributes is corrupted with wrong values.
    Examples include wrong values, missing attribute (feature) values, and irrelevant
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been validated, it can be used in algorithms. So, let’s dive
    a bit deeper into each step of the data processing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Now, since different algorithms use data in different ways, the data has a different
    form. Let’s explore how the data should be structured for each of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of transforming raw data into a format that can be used by algorithms
    is called feature extraction. It is a process where we apply a feature extraction
    algorithm to find properties of interest in the data. The algorithm for extracting
    features varies depending on the problem and the data type.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we work with textual data, we can use several algorithms, but let me illustrate
    the use of one of the simplest ones – bag-of-words. The algorithm simply counts
    the occurrence of words in the sentence – it either counts a pre-defined set of
    words or uses statistics to find the most frequent words. Let’s consider the following
    sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Mike is a` `tall boy.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use the bag-of-words algorithm without any constraints, it provides
    us with the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sentence ID** | **Mike** | **Is** | **A** | **tall** | **Boy** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `1` | `1` | `1` | `1` | `1` |'
  prefs: []
  type: TYPE_TB
- en: Figure 2.4 – Features extracted using bag-of-words
  prefs: []
  type: TYPE_NORMAL
- en: 'The table contains all the words in the sentence as features. It is not very
    useful for just one sentence, but if we add another one (sentence 1), things become
    more obvious. So, let’s add the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Mary is a` `smart girl.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This will result in the following feature table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sentence ID** | **Mike** | **Is** | **A** | **Tall** | **boy** | **smart**
    | **girl** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `1` | `1` | `1` | `1` | `1` | `0` | `0` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `0` | `1` | `1` | `0` | `0` | `1` | `1` |'
  prefs: []
  type: TYPE_TB
- en: Figure 2.5 – Features extracted from two sentences
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to add the label column to the data. Let’s say that we want
    to label each sentence as being positive or negative. The table then gets one
    more column – `label` – where **1** means that the sentence is positive and **0**
    otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sentence ID** | **Mike** | **Is** | **A** | **Tall** | **boy** | **smart**
    | **girl** | **Label** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `1` | `1` | `1` | `1` | `1` | `0` | `0` | `1` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `0` | `1` | `1` | `0` | `0` | `1` | `1` | `1` |'
  prefs: []
  type: TYPE_TB
- en: Figure 2.6 – Labels added to the data
  prefs: []
  type: TYPE_NORMAL
- en: Now, these features allow us to see the difference between two sentences, which
    we can then use to train and test machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: There are, however, two important limitations of this approach. The first one
    is that it is impractical (if not impossible) to have all words from all sentences
    as columns/features. For any non-trivial text, this would result in large and
    sparse matrices – a lot of wasted space. The second limitation is the fact that
    we usually lose important information – for example, the sentence “Is Mike a boy?”
    would result in the same feature vector as the first sentence. A feature vector
    is an *n*-dimensional vector of numerical features that describe some object in
    pattern recognition in machine learning. Although these sentences are not identical,
    they become undistinguishable, which can lead to class noise if they are labeled
    differently.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with adding this kind of noise becomes even more evident if we use
    statistics to choose the most frequent words as features. Here, we can lose important
    features that discriminate data points in a useful way. Therefore, this bag-of-words
    approach is only for illustration here. Later in this book, we’ll dive deeper
    into so-called transformers, which use more advanced techniques to extract features.
  prefs: []
  type: TYPE_NORMAL
- en: Data validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature vectors are the core of machine learning algorithms. They are most prominently,
    and directly, used by supervised machine learning algorithms. However, the same
    concepts of data validation apply to data used in other types of validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every form of data validation is a set of checks that ensure the data contains
    the desired properties. An example of such a set of checks is presented in *Figure
    2**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – An example of data quality checks](img/B19548_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – An example of data quality checks
  prefs: []
  type: TYPE_NORMAL
- en: Completeness of data is a property that describes how much of the total distribution
    our data covers. This can be measured in terms of object distribution – for example,
    how many types/models/colors of cars we have in our image dataset – or it can
    be measured in terms of properties – for example, how many of the words in the
    language our data contains.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is a property that describes how well our data is related to the empirical
    (real) world. For example, we may want to check whether all images in our dataset
    are linked to an object or whether all objects in the image are annotated.
  prefs: []
  type: TYPE_NORMAL
- en: Consistency describes how well the data is structured internally and whether
    the same data points are annotated in the same way. For example, in binary classification,
    we want all data points to be labeled “0” and “1” or “True” and “False,” but not
    both.
  prefs: []
  type: TYPE_NORMAL
- en: Integrity is the property where we check that the data can be integrated with
    other data. The integration can be done through common keys or other means. For
    example, we can check if our images contain metadata that will allow us to know
    where the image was taken.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, timeliness is a property that describes how fresh the data is. It checks
    whether the data contains the latest required information. For example, when we
    design a recommendation system, we would like to recommend both the new items
    and the old ones, so timeliness is important.
  prefs: []
  type: TYPE_NORMAL
- en: So, here is the next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #8'
  prefs: []
  type: TYPE_NORMAL
- en: Choose the data validation attributes that are the most relevant for your system.
  prefs: []
  type: TYPE_NORMAL
- en: Since every check requires additional steps in the workflow and can slow down
    the processing of the data, we should choose the data quality checks that impact
    our business and our architecture. If we develop a system where we want to provide
    up-to-date recommendations, then timeliness is our top priority, and we should
    focus on that rather than on completeness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there are a lot of frameworks for data validation and assessing data
    quality, I usually use a subset of data quality attributes from the AIMQ framework.
    The AIMQ framework has been designed to quantify data quality based on several
    quality attributes, similar to quality frameworks in software engineering such
    as the ISO 25000 series. I find the following properties of data to be the most
    important to validate:'
  prefs: []
  type: TYPE_NORMAL
- en: The data is free of noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data is fresh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data is fit for purpose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first property is the most important as noisy data can cause low performance
    in machine learning algorithms. For class noise, which we introduced previously,
    it is important to check if the data labels are not contradictory and to check
    whether the labels are assigned correctly. Contradictory labels can be found automatically,
    but wrongly annotated data points need manual assessment. For attribute noise,
    we can use statistical approaches to identify attributes that have low variability
    (or even the ones that are constant) or attributes that are completely random
    and do not contribute to the model’s learning. Let’s consider an example of a
    sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Mike is not a` `tall boy.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use the same feature extraction technique as for the previous sentences,
    our feature matrix looks like what’s shown in *Figure 2**.8*. We use the same
    features as for sentences 0 and 1 for sentence 2\. Since the last sentence differs
    only in terms of one word (not), this can lead to class noise. The last column
    has a label, `not`. This can happen when we train the model on one dataset and
    apply it to another one. This means that the first sentence and the last sentence
    have identical feature vectors, but different labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sentence ID** | **Mike** | **Is** | **A** | **tall** | **boy** | **smart**
    | **girl** | **Label** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `1` | `1` | `1` | `1` | `1` | `0` | `0` | `1` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `0` | `1` | `1` | `0` | `0` | `1` | `1` | `1` |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `1` | `1` | `1` | `1` | `1` | `0` | `0` | `0` |'
  prefs: []
  type: TYPE_TB
- en: Figure 2.8 – Noisy dataset
  prefs: []
  type: TYPE_NORMAL
- en: Having two different annotations for the same feature vectors is problematic
    as the machine learning algorithms cannot learn the pattern since there isn’t
    one for these noisy data points. Therefore, we need to validate the data in terms
    of it being free from noise.
  prefs: []
  type: TYPE_NORMAL
- en: Another property that the data needs to possess is its timeliness – that is,
    being fresh. We must use current, not old, data. One of the areas where this is
    of utmost importance is autonomous driving, where we need to keep the models up
    to date with the latest conditions (for example, the latest traffic signs).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the most important part of validation is assessing whether the data
    is fit for purpose. Note that this assessment cannot be done automatically as
    it needs to be done expertly.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration and monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning software is meant to be professionally engineered, deployed,
    and maintained. Modern companies call this process *MLOps*, which means that the
    same team needs to take responsibility for both the development and operations
    of the machine learning system. The rationale behind this extended responsibility
    is that the team knows the system best and therefore can configure, monitor, and
    maintain it in the best possible way. The teams know the design decisions that
    must be taken when developing the system, assumptions made about the data, and
    potential risks to monitor after the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Configuration is one such design decision that’s made by the development team.
    The team configures the parameters of the machine learning models, the execution
    environment, and the monitoring infrastructure. Let’s explore the first one; the
    latter two will be discussed in the next few sections.
  prefs: []
  type: TYPE_NORMAL
- en: To exemplify this challenge, let’s look at a random forest classifier for a
    dataset for classifying events during a specific surgery. The classifier, at least
    in its Python implementation, has 16 hyperparameters ([https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)).
    Each of these hyperparameters has several values, which means that finding the
    optimal set of hyperparameters can be a tedious task.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in practice, we do not need to explore all hyperparameters’ values
    and we do not need to explore all combinations. We should only explore the ones
    that are the most relevant for our task and, by extension, the dataset. I usually
    explore only two hyperparameters as these are the most important: the number of
    trees and the depth of the tree. The first determines how broad the forest is,
    while the second determines how deep it is. The code to specify these parameters
    can look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `n_estimators` hyperparameter is the number of trees, while `max_depth`
    hyperparameter is the depth of each tree. The values of these parameters depend
    on the dataset – how many features we have and how many data points we have. If
    we have too many trees and too many leaves compared to the number of features
    and data points, the classifier gets overfitted and cannot generalize from the
    data. This means that the classifier has learned to recognize each instance rather
    than recognize patterns in the data – we call this *overfitting*.
  prefs: []
  type: TYPE_NORMAL
- en: If we have too few trees or leaves, then the generalized patterns will be too
    broad and therefore we observe errors in classification – at least more errors
    than the optimal classifier. We call this *underfitting* as the model does not
    learn the pattern correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we can write a piece of code that would search for the best combination
    of these two parameters based on the pre-defined set of values. The code to find
    the best parameters manually would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The two lines emphasized in orange show two loops that explore these parameters
    – the content of the inner loop trains the classifier with these parameters and
    prints out the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply this algorithm to physiological data from patients who have undergone
    operations. When we plot the output on the diagram, as shown in *Figure 2**.9*,
    we can observe how the accuracy evolves. If we set the number of trees to 2, the
    classifier’s best performance is the best for 8 leaves, but even then, it does
    not classify the events perfectly. For four trees, the classifier achieves the
    best performance with 128 leaves, and the accuracy is 1.0\. From the following
    diagram, we can see that adding more trees does not improve the results significantly:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.9 – Accuracy per number of estimators and leaves. The labels of\
    \ the \uFEFFx axis show the number of trees (before the underscore) and the number\
    \ of leaves (after the underscore)](img/B19548_02_05.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Accuracy per number of estimators and leaves. The labels of the
    x axis show the number of trees (before the underscore) and the number of leaves
    (after the underscore)
  prefs: []
  type: TYPE_NORMAL
- en: For this example, the time required to search for the best result is relatively
    short – it takes up to 1-2 minutes on a standard laptop. However, if we want to
    find the optimal combination of all 16 parameters, we will spend a significant
    amount of time doing this.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a more automated way of finding the optimal parameters of machine
    learning classifiers – different types of search algorithms. One of the most popular
    ones is the GridSearch algorithm ([https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)),
    which works similarly to our manual script, except that it can do cross-validation
    with multiple splits and many other statistical tricks to improve the search.
    Searching for the optimal solution with the GridSearch algorithm can look something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code finds the best solution and saves it as the `best_estimator_`
    parameter of the GridSearch model. In the case of this dataset and this model,
    the algorithm finds the best random forest to be the one with 128 trees (`n_estimators`)
    and 4 levels (`max_depth`). The results are a bit different than the ones found
    manually, but this does not mean that one of the methods is superior.
  prefs: []
  type: TYPE_NORMAL
- en: However, having too many trees can result in overfitting, so I would choose
    the model with 4 trees and 128 leaves over the one with 128 trees and 4 levels.
    Or maybe I would also use a model that is somewhere in-between – that is, a model
    that has the same accuracy but is less prone to overfitting in either the depth
    or the width.
  prefs: []
  type: TYPE_NORMAL
- en: This leads to my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #9'
  prefs: []
  type: TYPE_NORMAL
- en: Use GridSearch and other algorithms after you have explored the parameter search
    space manually.
  prefs: []
  type: TYPE_NORMAL
- en: Although the automated parameter search algorithms are very useful, they hide
    properties of the data from us and do not allow us to explore the data and the
    parameters ourselves. From my experience, understanding the data, the model, its
    parameters, and its configuration is crucial to the successful deployment of machine
    learning software. I only use GridSearch (or other optimization algorithms) after
    I’ve tried to find some optima manually since I would like to understand the data.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the machine learning system has been configured, it is set into production,
    often as part of the larger software system. The inferences that are made by machine
    learning are the basis for the features of the product and the business model
    behind this. Therefore, the machine learning component should make as few mistakes
    as possible. Unfortunately, the customers take failures and mistakes more seriously
    than correctly functioning products.
  prefs: []
  type: TYPE_NORMAL
- en: However, the performance of the machine learning system degrades over time,
    but not because of low-quality programming or design – this is the nature of probabilistic
    computing. Therefore, all machine learning systems need to be monitored and maintained.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the aspects that needs to be monitored is called *concept drift*. Concept
    drift is a phenomenon in the data, which means that the distribution of entities
    in the data changes over time for natural reasons. *Figure 2**.10* illustrates
    concept drift for a machine learning classifier (blue and red lines) of images
    of yellow and orange trucks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Illustration of concept drift. The original distribution of
    the objects (on the left) changes over time (on the right), so the classifier
    must be retrained (blue versus red lines)](img/B19548_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Illustration of concept drift. The original distribution of the
    objects (on the left) changes over time (on the right), so the classifier must
    be retrained (blue versus red lines)
  prefs: []
  type: TYPE_NORMAL
- en: The left-hand side shows the original distribution of data that was used to
    train the model. The model is conceptually shown as the blue dotted line. The
    model recognizes the differences between the two classes of images. However, over
    time, the data can change. New images can appear in the dataset and the distribution
    can change. The original model starts to make mistakes in inference and therefore
    needs to be adjusted. The re-trained model – that is, the solid red line – captures
    the new distribution of the data.
  prefs: []
  type: TYPE_NORMAL
- en: It is this change in the dataset that we call concept drift. It is more common
    in complex datasets and supervised learning models, but the effects of concept
    drift are equally problematic for non-supervised models and reinforcement learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.11* presents the performance of the same random forest model applied
    to the data from the same distribution (directly after training) and on the data
    after some operation. Concept drift is visible in the accuracy reduction from
    1.0 to 0.44\. The model has been trained on the same data as in the example in
    *Figure 2**.9* but has been applied to data from another patient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – An example of performance decrease before and after concept
    drift](img/B19548_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – An example of performance decrease before and after concept drift
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, I would like to introduce my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #10'
  prefs: []
  type: TYPE_NORMAL
- en: Always include monitoring mechanisms in your machine learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: Including mechanisms to monitor concept drift, even a simple mechanism such
    as using a Chi-square statistical test for the similarity of distributions, makes
    a lot of difference. It allows us to identify problems in the system, troubleshoot
    them, and prevent them from spreading to other parts of the software, or even
    to the end user of the software.
  prefs: []
  type: TYPE_NORMAL
- en: Professional machine learning engineers set up monitoring mechanisms for concept
    drift in production, which indicates the degradation of AI software.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure and resource management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The infrastructure and resources needed for the machine learning software are
    organized into two areas – data serving infrastructure (for example, databases)
    and computational infrastructure (for example, GPU computing platforms). There
    is also serving infrastructure, which is used to provide the services to the end
    users. The serving infrastructure could be in the form of desktop applications,
    embedded software (such as the one in autonomous vehicles), add-ins to tools (as
    in the case of GitHub Co-pilot), or websites (such as ChatGPT). However, in this
    book, we’ll focus on the data-serving infrastructure and the computational infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Both areas can be deployed locally or remotely. Local deployment means that
    we use our own infrastructure at the company, while remote infrastructure means
    that we use cloud services or services of another supplier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conceptually, we could see these two areas as co-dependent, as depicted in
    *Figure 2**.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Co-dependency between serving, computing, and data-serving
    infrastructure](img/B19548_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – Co-dependency between serving, computing, and data-serving infrastructure
  prefs: []
  type: TYPE_NORMAL
- en: The data serving infrastructure provisions data that’s used for the computation
    infrastructure. It consists of databases and other data sources (for example,
    raw files). The computation infrastructure consists of computing infrastructure
    to train and test machine learning models. Finally, the user-serving infrastructure
    uses the models to make inferences and provides services and functionality to
    the end users.
  prefs: []
  type: TYPE_NORMAL
- en: Data serving infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data serving infrastructure is one of the fundamental parts of the machine learning
    software because there is no machine learning if there is no data. Data-hungry
    machine learning applications pose new requirements for the infrastructure in
    terms of performance, reliability, and traceability. The last requirement is very
    important as the machine learning training data determines how the trained machine
    learning model makes inferences. In the case of defects in the end user function,
    software engineers need to scrutinize the algorithms, the models, and the data
    that were used to construct the failing machine learning system.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to traditional software, the data-serving infrastructure is often
    composed of three different parts, as shown in *Figure 2**.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Data serving infrastructure](img/B19548_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Data serving infrastructure
  prefs: []
  type: TYPE_NORMAL
- en: The data is stored in persistent storage, such as in a database on a hard drive.
    It can be stored locally or on a cloud server. The most important part is that
    this data is secure and can be accessed for further processing. The persistently
    stored data needs to be extracted so that it can be used in machine learning.
    The first step is to find the snapshot of the data that is needed – for example,
    by selecting data for training. The snapshot needs to be prepared and formatted
    in tabular form so that the machine learning algorithms can use the data to make
    inferences.
  prefs: []
  type: TYPE_NORMAL
- en: There are several different types of databases today that machine learning systems
    use. First, there are standard relational databases, where data is stored in the
    form of tables. These are the databases that are well known and used widely both
    in traditional and machine learning software.
  prefs: []
  type: TYPE_NORMAL
- en: Newer types of databases are non-SQL databases such as Elasticsearch ([https://www.elastic.co](https://www.elastic.co)),
    which are designed to store documents, not tables. These documents are indexed
    flexibly so that data can be stored and retrieved based on these documents. In
    the case of machine learning software, these documents can be images, entire text
    documents, or even sound data. This is important for storing data in the same
    format as it is collected so that we can trace the data when needed.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the format of the data in the databases, it is retrieved from
    the database and transformed into tabular form; we’ll discuss this in [*Chapter
    3*](B19548_03.xhtml#_idTextAnchor038). This tabular form is required for the data
    to be processed by the computational infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’ve come to my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #11'
  prefs: []
  type: TYPE_NORMAL
- en: Choose the right database for your data – look at this from the perspective
    of the data, not the system.
  prefs: []
  type: TYPE_NORMAL
- en: Although it sounds obvious that we should choose the right database for the
    data that we have, for machine learning systems, it is important to select the
    database that works best for the data at hand, not the system. For example, when
    we use natural language processing models, we should store the data in documents
    that we can easily retrieve in an organized form.
  prefs: []
  type: TYPE_NORMAL
- en: Computational infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The computational infrastructure can change over time. In the early phases of
    the development of machine learning systems, software developers often use pre-configured
    experimentation environments. These can be in the form of Jupyter Notebooks on
    their computers or in the form of pre-configured services such as Google Colab
    or Microsoft Azure Notebooks. This kind of infrastructure supports rapid prototyping
    of machine learning, easy provisioning of data, and no need for setting up advanced
    features. They also allow us to easily scale the computational resources up and
    down without the need to add or remove extra hardware.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to this approach is to use our own infrastructure, where we set
    up your own servers and runtime environments. It requires more effort, but it
    provides us with full control over the computational resources, as well as full
    control over data processing. Having full control over data processing can sometimes
    be the most important factor for selecting the infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #12'
  prefs: []
  type: TYPE_NORMAL
- en: Use cloud infrastructure if you can as it saves resources and reduces the need
    for specialized competence.
  prefs: []
  type: TYPE_NORMAL
- en: Professional AI engineers use self-owned infrastructure for prototyping and
    training and cloud-based infrastructure for production as it scales better with
    the number of users. The opposite, which is to use our own infrastructure, is
    true only if we need to retain full control over data or infrastructure. Full
    control can be required for applications that use sensitive customer data, military
    applications, security applications, and other applications where data is extremely
    sensitive.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, we have several large actors providing the computational infrastructure,
    as well as a large ecosystem of small actors. The three largest – Amazon Web Services,
    Google Cloud, and Microsoft Azure – can provide all kinds of services for both
    small and large enterprises. Amazon Web Services ([https://aws.amazon.com](https://aws.amazon.com))
    specializes in provisioning data storage and processing infrastructure. It is
    often used for applications that must quickly process large quantities of data.
    The infrastructure is professionally maintained and can be used to achieve near-perfect
    reliability of the products built on that platform. To use it efficiently, you
    must usually work with containers and virtual machines that execute the code of
    the machine learning application.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud ([https://cloud.google.com](https://cloud.google.com)) specializes
    in provisioning platforms for data-intensive applications and computation-intensive
    solutions. Thanks to Google’s processors (**tensor processing units** (**TPUs**)),
    the platform provides a very efficient environment for both training and using
    machine learning solutions. Google Cloud also provides free solutions for learning
    machine learning in the form of Google Colab, which is an extension of the Jupyter
    Notebook ([https://jupyter.org](https://jupyter.org)) platform on Python.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Azure ([https://azure.microsoft.com](https://azure.microsoft.com))
    specializes in provisioning platforms for training and deploying machine learning
    systems in the form of virtual machines. It also provides ready-to-deploy models
    for image recognition, classification, and natural language processing, and even
    platforms for training generic machine learning models. It is the most flexible
    of the available platforms and the most scalable.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these platforms, you can use several specialized ones, such as
    Facebook’s platform for machine learning, which specializes in recommender systems.
    However, since the specialized platforms are often narrow, we need to remember
    the potential issues if we want to port our software from one platform to another.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #13'
  prefs: []
  type: TYPE_NORMAL
- en: Decide on which production environment you wish to use early and align your
    process with that environment.
  prefs: []
  type: TYPE_NORMAL
- en: We need to decide if we want to use Amazon’s, Google’s, or Microsoft’s cloud
    environment or whether we want to use our own infrastructure to reduce the cost
    of software development. Although it is possible to move our software between
    these environments, it is not straightforward and requires (at best) significant
    testing and pre-deployment validation, which often comes with significant costs.
  prefs: []
  type: TYPE_NORMAL
- en: How this all comes together – machine learning pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we explored the main characteristics of machine learning systems
    and compared them to traditional software systems. Let’s finish this comparison
    by summarizing how we usually design and describe machine learning systems – by
    using pipelines. A pipeline is a sequence of data processing steps, including
    the machine learning models. The typical set of steps (also called phases) is
    shown in *Figure 2**.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – A typical sequence of steps in a machine learning pipeline](img/B19548_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – A typical sequence of steps in a machine learning pipeline
  prefs: []
  type: TYPE_NORMAL
- en: This kind of pipeline, although drawn linearly, is usually processed in cycles,
    where, for example, monitoring for concept drift can trigger re-training, re-testing,
    and re-deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning pipelines, just like the one presented in *Figure 2**.14*,
    are often depicted as a set of components as parts of the entire system. However,
    presenting it using the pipeline analogy helps us understand that machine learning
    systems process data in steps.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll explore the first part of the pipeline – working
    with data. We’ll start by exploring different types of data and how these types
    of data are collected, processed, and used in modern software systems.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Shortliffe, E.H., et al., Computer-based consultations in clinical therapeutics:
    explanation and rule acquisition capabilities of the MYCIN system. Computers and
    biomedical research, 1975\. 8(4):* *p. 303-320.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vaswani, A., et al., Attention is all you need. Advances in neural information
    processing systems,* *2017\. 30.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dale, R., GPT-3: What’s it good for? Natural Language Engineering, 2021\.
    27(1):* *p. 113-118.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Smith, S., et al., Using deepspeed and megatron to train megatron-turing nlg
    530b, a large-scale generative language model. arXiv preprint* *arXiv:2201.11990,
    2022.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lee, Y.W., et al., AIMQ: a methodology for information quality assessment.
    Information & management, 2002\. 40(2):* *p. 133-146.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zenisek, J., F. Holzinger, and M. Affenzeller, Machine learning based concept
    drift detection for predictive maintenance. Computers & Industrial Engineering,
    2019\. 137:* *p. 106031.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Amershi, S., et al. Software engineering for machine learning: A case study.
    in 2019 IEEE/ACM 41st International Conference on Software Engineering: Software
    Engineering in Practice (ICSE-SEIP).* *2019\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
