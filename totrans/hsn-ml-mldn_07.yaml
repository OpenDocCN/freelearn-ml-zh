- en: Clustering Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With classification models behind us, it is now time to dive into clustering
    models. Currently, in ML.NET there is only one cluster algorithm, k-means. In
    this chapter, we will dive into k-means clustering as well as the various applications
    best suited to utilizing a clustering algorithm. In addition, we will build a
    new ML.NET clustering application that determines the type of a file simply by
    looking at the content. Finally, we will explore how to evaluate a k-means clustering
    model with the properties that ML.NET exposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down the k-means algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the clustering application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating a k-means model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breaking down the k-means algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in [Chapter 1](b8d873e1-9234-4f11-ad94-76df5ffbb228.xhtml), *Getting
    Started with Machine Learning and ML.NET,* k-means clustering, by definition,
    is an unsupervised learning algorithm. This means that data is grouped into clusters
    based on the data provided to the model for training. In this section, we will
    dive into a number of use cases for clustering and the k-means trainer.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases for clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering, as you may be beginning to realize, has numerous applications where
    the output categorizes similar outputs into groups of similar data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of its potential applications include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Natural disaster tracking such as earthquakes or hurricanes and creating clusters
    of high-danger zones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Book or document grouping based on the authors, subject matter, and sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping customer data into targeted marketing predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search result grouping of similar results that other users found useful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, it has numerous other applications such as predicting malware families
    or medical purposes for cancer research.
  prefs: []
  type: TYPE_NORMAL
- en: Diving into the k-means trainer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-means trainer used in ML.NET is based on the Yinyang method as opposed
    to a classic k-means implementation. Like some of the trainers we have looked
    at in previous chapters, all of the input must be of the Float type. In addition,
    all input must be normalized into a single feature vector. Fortunately, the k-means
    trainer is included in the main ML.NET NuGet package; therefore, no additional
    dependencies are required.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the Yinyang implementation, Microsoft Research published
    a white paper here: [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ding15.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ding15.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following diagram, showing three clusters and a data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7759f97-a307-4176-a449-ad7e854ac847.png)'
  prefs: []
  type: TYPE_IMG
- en: In clustering, each of these clusters represents a grouping of similar data
    points. With k-means clustering (and other clustering algorithms), the distances
    between the data point and each of the clusters are the measures of which cluster
    the model will return. For k-means clustering specifically, it uses the center
    point of each of these clusters (also called a centroid) and then calculates the
    distance to the data point. The smallest of these values is the predicted cluster.
  prefs: []
  type: TYPE_NORMAL
- en: For the k-means trainer, it can be initialized in one of three ways. One way
    is to utilize a randomized initialization—as you have probably guessed, this can
    lead to randomized prediction results. Another way is to utilize k-means++, which
    strives to produce O(log K) predictions. Lastly, k-means||, the default method
    in ML.NET, uses a parallel method to reduce the number of passes required to initialize.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on k-means||, you can refer to a paper published by Stanford,
    which explains it in detail: [https://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](https://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf).
    [](https://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on k-means++, you can refer to a paper published by Stanford in
    2006, explaining it in detail: [http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: We will demonstrate this trainer in the example application in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the clustering application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, the application we will be creating is a file type classifier.
    Given a set of attributes statically extracted from a file, the prediction will
    return if it is a document, an executable, or a script. For those of you who have
    used the Linux `file` command, this is a simplified version but based on machine
    learning. The attributes included in this example aren't the definitive list of
    attributes, nor should they be used as-is in a production environment; however,
    you could use this as a starting point for creating a true ML-based replacement
    for the Linux `file` command.
  prefs: []
  type: TYPE_NORMAL
- en: As with previous chapters, the completed project code, sample dataset, and project
    files can be downloaded here: [https://github.com/PacktPublishing/Hands-On-Machine-Learning-With-ML.NET/tree/master/chapter05](https://github.com/PacktPublishing/Hands-On-Machine-Learning-With-ML.NET/tree/master/chapter05).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the project architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building on the project architecture and code we created in previous chapters,
    the major change architecturally is in the feature extraction being done on both
    the training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you will find the Visual Studio Solution Explorer view of the project.
    The new additions to the solution are the `FileTypes`, `FileData`, and `FilePrediction`
    files that we will review later on in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b2bdf6a-0c57-43f5-bd7b-22b0dba1c1c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `sampledata.csv` file contains 80 rows of random files I had on my system,
    comprising 30 Windows executables, 20 PowerShell scripts, and 20 Word documents.
    Feel free to adjust the data to fit your own observations or to adjust the trained
    model. Here is a snippet of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Each of these rows contains the value for the properties in the newly created
    `FileData` class that we will review later on in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to this, we added the `testdata.csv` file, which contains additional
    data points to test the newly trained model against and evaluate. The breakdown
    was even with 10 Windows executables, 10 PowerShell scripts, and 10 Word documents.
    Here is a snippet of the data inside `testdata.csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Diving into the code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this application, as noted in the previous section, we are building on top
    of the work completed in [Chapter 4](da0d1d99-ad37-498b-8670-f8cee6ad49bc.xhtml),
    *Classification Model*. For this deep dive, we are going to focus solely on the
    code that was changed for this application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classes that were changed or added are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Constants`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`` `BaseML` ``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileTypes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileData`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileTypePrediction`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FeatureExtractor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Predictor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Trainer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Program`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Constants class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Constants` class has been changed to save the model to `chapter5.mdl`,
    in addition to supporting a feature-extracted `testdata.csv` variable. The following
    code block reflects these changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The BaseML class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The sole change in the `BaseML` class is the addition of the `FEATURES` variable.
    By using a variable here, we can remove the use of a magic string in our `Trainer`
    class (we will discuss this later in this section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The FileTypes enumeration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `FileTypes` enumeration contains a strongly typed method for mapping our
    classifications and a numeric value. As we discovered in our previous examples,
    utilizing an enumeration as opposed to magic or constant values provides better
    flexibility, as shown here and throughout the remaining classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The FileData class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `FileData`class is the container class that contains the data to both predict
    and train our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we add constant values for `True` and `False` since k-means requires
    floating-point values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a constructor that supports both our prediction and training.
    We optionally pass in the filename for the training to provide a label, in this
    case, `ps1`, `exe`, and `doc` for scripts, executables, and documents, respectively.
    We also call helper methods to determine whether the file is binary, or whether
    it starts with MZ or PK:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: MZ and PK are considered to be magic numbers of Windows executables and modern
    Microsoft Office files. Magic numbers are unique byte strings that are found at
    the beginning of every file. In this case, both are simply two bytes. When performing
    analysis on files, making quick determinations is crucial for performance. For
    the keen reader, PK is also the magic number for ZIP. Modern Microsoft Office
    documents are actually ZIP archives. For the sake of simplicity in this example,
    PK is used as opposed to performing an additional level of detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we also add an additional constructor to support the hard truth setting
    of values. We will deep dive into the purpose of this addition later on in this
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we implement our two helper methods. The first, `HasBinaryContent`, as
    the name implies, takes the raw binary data and searches for non-text characters
    to ensure it is a binary file. Secondly, we define `HasHeaderBytes`; this method
    takes an array of bytes, converts it into a `UTF8` string, and then checks to
    see whether the string matches the string passed in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we add the properties used for prediction, training, and testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we override the `ToString` method to be used with the feature extraction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The FileTypePrediction class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `FileTypePrediction`class contains the properties mapped to our prediction
    output. In k-means clustering, the `PredictedClusterId` property stores the closest
    cluster found. In addition to this, the `Distances` array contains the distances
    from the data point to each of the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The FeatureExtractor class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `FeatureExtractor` class that we utilized in the logistic regression example
    from [Chapter 3](8bcfc000-9adc-4eda-a91a-e09f676eac85.xhtml), *Regression Model*,
    has been adapted to support both test and training data extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we generalize the extraction to take the folder path and the output
    file. As noted earlier, we also pass in the filename, providing the `Labeling`
    to occur cleanly inside the `FileData` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we take the two parameters from the command line (called from the `Program`
    class) and simply call the preceding method a second time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The Predictor class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a couple of changes in this class to handle the file type prediction
    scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we add a helper method, `GetClusterToMap`, which maps known values to
    the prediction clusters. Note the use of `Enum.GetValues` here; as you add more
    file types, this method does not need to be modified:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we pass in the `FileData` and `FileTypePrediction` types into the `CreatePredictionEngine`
    method to create our prediction engine. Then, we read the file in as a binary
    file and pass these bytes into the constructor of `FileData` prior to running
    the prediction and mapping initialization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we need to adjust the output to match the output that a k-means prediction
    returns, including the Euclidean distances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The Trainer class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Inside the `Trainer` class, several modifications need to be made to support
    k-means classification:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first change is the addition of a `GetDataView` helper method, which builds
    the `IDataView` object from the columns previously defined in the `FileData` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We then build the data process pipeline, transforming the columns into a single
    `Features` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then create the k-means trainer with a cluster size of 3 and create
    the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The default value for the number of clusters is 5\. An interesting experiment
    to run based either on this dataset or one modified by you is to see how the prediction
    results change by adjusting this value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we evaluate the model we just trained using the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we output all of the classification metrics, each of which we will
    detail in the next section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The Program class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Program` class, as mentioned in previous chapters, is the main entry point
    for our application. The only change in the `Program` class is the help text to
    indicate usage for the extract to accept the test folder path for extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we modify the `switch`/`case` statement to support the additional
    parameter to the `extract` method to support both the training and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Running the application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the application, the process is nearly identical to [Chapter 3](8bcfc000-9adc-4eda-a91a-e09f676eac85.xhtml), *Regression
    Model*''s example application with the addition of passing in the test dataset
    when training:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the training on the command line as we did in previous chapters, simply
    pass in the following command (assuming you have added two sets of files; one
    each for your training and test sets):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Included in the code repository are two pre-feature extracted files (`sampledata.csv`
    and t`estdata.csv`) to allow you to train a model without performing your own
    feature extraction.  If you would like to perform your own feature extraction,
    create a `TestData` and `TrainingData` folder.  Populate these folders with a
    sampling of **PowerShell **(**PS1**), **Windows Executables** (**EXE**) and **Microsoft
    Word documents** (**DOCX**).
  prefs: []
  type: TYPE_NORMAL
- en: 'After extracting the data, we must then train the model by passing in the newly
    created `sampledata.csv` and `testdata.csv` files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the model with this file, simply pass in the filename to the built application
    (in this case, the compiled `chapter05.exe` is used) and the predicted output
    will show:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note the expanded output to include several metric data points—we will go through
    what each one of these means at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feel free to modify the values and see how the prediction changes based on
    the dataset that the model was trained on. A few areas of experimentation from
    this point could include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding some additional features to increase the prediction accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding additional file types to the clusters such as video or audio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a new range of files to generate new sample and test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating a k-means model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in previous chapters, evaluating a model is a critical part of
    the overall model-building process. A poorly trained model will only provide inaccurate
    predictions. Fortunately, ML.NET provides many popular attributes to calculate
    model accuracy based on a test set at the time of training to give you an idea
    of how well your model will perform in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In ML.NET, as noted in the example application, there are three properties
    that comprise the `ClusteringMetrics` class object. Let''s dive into the properties
    exposed in the `ClusteringMetrics` object:'
  prefs: []
  type: TYPE_NORMAL
- en: Average distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Davies-Bouldin index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalized mutual information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next sections, we will break down how these values are calculated and
    the ideal values to look for.
  prefs: []
  type: TYPE_NORMAL
- en: Average distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Also referred to as the **average score** is the distance from the center of
    a cluster to the test data. The value, of type double, will decrease as the number
    of clusters increases, effectively creating clusters for the edge cases. In addition
    to this, a value of 0, such as the one found in our example, is possible when
    your features create distinct clusters. This means that, if you find yourself
    seeing poor prediction performance, you should increase the number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The Davies-Bouldin Index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Davies-Bouldin Index is another measure for the quality of the clustering.
    Specifically, the Davies-Bouldin Index measures the scatter of cluster separation
    with values ranging from 0 to 1 (of type double), with a value of 0 being ideal
    (as was the case of our example).
  prefs: []
  type: TYPE_NORMAL
- en: For more details on the Davies-Bouldin Index, specifically the math behind the
    algorithm, a good resource can be found here: [https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index](https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index).
  prefs: []
  type: TYPE_NORMAL
- en: Normalized mutual information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The normalized mutual information metric is used to measure the mutual dependence
    of the feature variables.
  prefs: []
  type: TYPE_NORMAL
- en: The range of values is from 0 to 1 (the type is of double)—closer to or equal
    to 1 is ideal, akin to the model we trained earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on normalized mutual information along with the math behind
    the algorithm, please read [http://en.wikipedia.org/wiki/Mutual_information#Normalized_variants](http://en.wikipedia.org/wiki/Mutual_information#Normalized_variants).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the course of this chapter, we dove into ML.NET's clustering support via
    the k-means clustering algorithm. We have also created and trained our first clustering
    application using k-means to predict what file type a file is. Lastly, we dove
    into how to evaluate a k-means clustering model and the various properties that
    ML.NET exposes to achieve a proper evaluation of a k-means clustering model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will deep dive into anomaly detection algorithms with
    ML.NET by creating a login anomaly predictor.
  prefs: []
  type: TYPE_NORMAL
