<html><head></head><body><div id="sbo-rt-content"><div class="chapter" title="Chapter 2. Stock Market Price Prediction"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Stock Market Price Prediction</h1></div></div></div><p>In this chapter, we will cover an amazing application that belongs to predictive analysis. I hope the name of the chapter has already given you a rough idea of what this chapter is going to be all about. We will try to predict the price of the stock index. We will apply some modern machine learning techniques as well as deep learning techniques.</p><p>We will cover the following topics in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introducing the problem statement </li><li class="listitem" style="list-style-type: disc">Collecting the dataset</li><li class="listitem" style="list-style-type: disc">Understanding the dataset</li><li class="listitem" style="list-style-type: disc">Data preprocessing and data analysis</li><li class="listitem" style="list-style-type: disc">Feature engineering</li><li class="listitem" style="list-style-type: disc">Selecting the Machine Learning (ML) algorithm</li><li class="listitem" style="list-style-type: disc">Training the baseline model</li><li class="listitem" style="list-style-type: disc">Understanding the testing matrix</li><li class="listitem" style="list-style-type: disc">Testing the baseline model</li><li class="listitem" style="list-style-type: disc">Exploring problems with the existing approach</li><li class="listitem" style="list-style-type: disc">Understanding the revised approach<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding concepts and approaches</li></ul></div></li><li class="listitem" style="list-style-type: disc">Implementing the revised approach<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Testing the revised approach</li><li class="listitem" style="list-style-type: disc">Understanding problems with the revised approach</li></ul></div></li><li class="listitem" style="list-style-type: disc">The best approach</li><li class="listitem" style="list-style-type: disc">Summary</li></ul></div><p>So, let's get started!</p><div class="section" title="Introducing the problem statement"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec22"/>Introducing the problem statement</h1></div></div></div><p>The stock market is a place where you can buy and sell units of ownership in the company, which we call <span class="strong"><strong>stocks</strong></span>. If the company performs well and increases its profit, then you <a id="id165" class="indexterm"/>will earn some profit as well because you have the stocks of the company, but if the company's profit goes down, then you will lose the money you have with the company. So if you invest your money in the right company at the right time, it could lead to you earning quite a lot of money. The question is which company's stock should you buy? Is there any way we can predict the future prices of the stock of any company given the historical prices of the company's stock so that we can have higher chances of getting good returns? The answer is yes. This is what we will explore in this chapter.</p><p>If you invest in the stock market, then you may have heard that stock prices are completely random and unpredictable. This is called the <span class="emphasis"><em>efficient market hypothesis,</em></span> but a majority of the big financial firms, such as JP Morgan, Citigroup, and Goldman Sachs, have mathematical and quantitative analysts who are trying to develop predictive models to help these big firms decide when to invest and in which stock.</p><p>Before investing in any stock, we do some basic research regarding the company's profile. We try to understand its business model. We also check the balance sheets of the company to get to know what the profit and loss of the company is. What are the products that the company will launch in the next couple of months? What kind of news is coming in about the company? What are the current industry trends? After researching all these parameters, we will invest our money in a particular company's stock if we feel we will gain some profit; otherwise, we won't invest in that company.</p><p>We depend on various sources of information to get an idea about whether we need to buy stocks or sell stocks. Don't you think all this analysis takes a lot of our time? I want to put two questions in front of you. First, can we use some of the data points discussed here and build a system that will help us find out future stock prices? And can we use historical stock prices to predict future stock prices? The answer to both of these questions is yes, definitely: we can build a system that will use historical stock prices and some of the other data points so that we can predict the future prices of stock. As per the efficient market hypothesis, by using historical prices of stock and various other data points, we can obtain the future prices of the stock, which will be better than a random guess. In this chapter, we will build a predictive model, which will predict the close price of the stock. In the next section, we will look at how to collect the dataset in order to build the model. So, let's get started!</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Collecting the dataset"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec23"/>Collecting the dataset</h1></div></div></div><p>In order <a id="id166" class="indexterm"/>to build the model, first we need to collect the data. We will use the following two data points:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Dow Jones Industrial Average</strong></span> (<span class="strong"><strong>DJIA</strong></span>) index prices</li><li class="listitem" style="list-style-type: disc">News articles </li></ul></div><p>DJIA index <a id="id167" class="indexterm"/>prices give us an overall idea about the stock market's movements on a particular day, whereas news articles help us find out how news affects the stock prices. We will build our model using these two data points. Now let's collect the data.</p><div class="section" title="Collecting DJIA index prices"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec20"/>Collecting DJIA index prices</h2></div></div></div><p>In order <a id="id168" class="indexterm"/>to collect the DJIA index prices, we will use Yahoo Finance. You can visit this link: <a class="ulink" href="https://finance.yahoo.com/quote/%5EDJI/history?period1=1196706600&amp;period2=1512325800&amp;interval=1d&amp;filter=history&amp;frequency=1d">https://finance.yahoo.com/quote/%5EDJI/history?period1=1196706600&amp;period2=1512325800&amp;interval=1d&amp;filter=history&amp;frequency=1d</a>. Once you click on this link, you can see that the price data shows up. You can change the time period and click on the <span class="strong"><strong>Download Data</strong></span> link and that's it; you can have all the data in <code class="literal">.csv</code> file format. Refer to the following screenshot of the Yahoo finance DJIA index price page:</p><div class="mediaobject"><img src="Images/B08394_02_01.jpg" alt="Collecting DJIA index prices" width="919" height="571"/><div class="caption"><p>Figure 2.1: Yahoo Finance page for DJIA index price</p></div></div><p>Here, we have downloaded the dataset for the years 2007-2016, which means we have 10 years of data for DJIA index prices. You can see this in <span class="emphasis"><em>Figure 2.1</em></span>, as well. You can find this dataset using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/DJIA_data.csv">https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/DJIA_data.csv</a>.</p><p>Just bear with me for a while; we will understand the meaning of each of the data attributes in the <span class="emphasis"><em>Understand the dataset</em></span> section in this chapter. Now, let's look at how we can <a id="id169" class="indexterm"/>collect the news articles.</p></div><div class="section" title="Collecting news articles"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec21"/>Collecting news articles</h2></div></div></div><p>We want <a id="id170" class="indexterm"/>to collect news articles so that we can establish the correlation between how news affects the DJIA index value. We are going to perform a sentiment analysis on the news articles. You may wonder why we need to perform sentiment analysis. If any news has a negative effect on the financial market, then it is likely that the prices of stocks will go down, and if news about the financial market is positive, then it is likely that prices of the stocks will go up. For this dataset, we will use news articles from the New York Times (NYTimes). In order to collect the dataset of news articles, we will use the New York Times' developer API. So, let's start coding!</p><p>First of all, register yourself on the NYTimes developer website and generate your API key. The link is <a class="ulink" href="https://developer.nytimes.com/signup">https://developer.nytimes.com/signup</a>. I have generated the API key for the Archive API. Here, we are using <span class="emphasis"><em>newsapi, JSON, requests</em></span>, and <span class="emphasis"><em>sys</em></span> dependencies. You can also refer to the NYTimes developer documentation using this link: <a class="ulink" href="https://developer.nytimes.com/archive_api.json#/Documentation/GET/%7Byear%7D/%7Bmonth%7D.json">https://developer.nytimes.com/archive_api.json#/Documentation/GET/%7Byear%7D/%7Bmonth%7D.json</a>.</p><p>You can find the code at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/getdata_NYtimes.py">https://github.com/jalajthanaki/stock_price_prediction/blob/master/getdata_NYtimes.py</a>. You can see the code snippet in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_02_02.jpg" alt="Collecting news articles" width="863" height="694"/><div class="caption"><p>Figure 2.2: Code snippet for getting the news article data from the New York Times</p></div></div><p>As you <a id="id171" class="indexterm"/>can see in the code, there are three methods. The first two methods are for exceptions and the third method checks for the validation and requests the URL that can generate the news article data for us. This NYTimes API URL takes three parameters, which are given as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Year</li><li class="listitem" style="list-style-type: disc">Month</li><li class="listitem" style="list-style-type: disc">API key</li></ul></div><p>After this step, we will call the third function and pass the year value from 2007 to 2016. We will save the data in the <span class="emphasis"><em>JSON</em></span> format. You can refer to the code snippet in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_02_03.jpg" alt="Collecting news articles" width="679" height="210"/><div class="caption"><p>Figure 2.3: Code snippet for getting news article data from the New York Times</p></div></div><p>You can <a id="id172" class="indexterm"/>find the raw JSON dataset using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/2016-01.json">https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/2016-01.json</a>.</p><p>Now let's move on to the next section, in which we will understand the dataset and the attributes that we have collected so far.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding the dataset"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec24"/>Understanding the dataset</h1></div></div></div><p>In this section, we will understand the meaning of data attributes, which will help us understand <a id="id173" class="indexterm"/>what kind of dataset we are going to deal with and what kind of preprocessing is needed for the dataset. We understand our dataset in two sections, and those sections are given as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the DJIA dataset</li><li class="listitem" style="list-style-type: disc">Understanding the NYTimes news article dataset</li></ul></div><div class="section" title="Understanding the DJIA dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec22"/>Understanding the DJIA dataset</h2></div></div></div><p>In the DJIA <a id="id174" class="indexterm"/>dataset, we have seven data attributes. They are quite easy to understand, so let's look at each of them one by one:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Date</code>: The first column indicates the date in the YYYY-MM-DD format when you see data in the .csv file.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Open</code>: This indicates the price at which the market opens, so it is the opening value for the DJIA index for that particular trading day.</li><li class="listitem" style="list-style-type: disc"><code class="literal">High</code>: This is the highest price for the DJIA index for a particular trading day.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Low</code>: This is the lowest price for DJIA index for a particular trading day.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Close</code>: The price of DJIA index at the close of the trading day.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Adj close</code>: The adjusted closing price (adj close price) uses the closing price as a starting point and takes into account components such as dividends, stock splits, and new stock offerings. The adj close price represents the true reflection of the DJIA index. Let me give you an example so that you can understand the adj close price better: if a company offers a dividend of $5 per share, and if the closing price of that company share is $100, then the adj close price will become $95. So, the adj close price considers various factors and, based on them, generates the true value of the company's stock. Here, we are looking at the DJIA index value so, most of the time, the closing price and the adj close price are the same.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Volume</code>: These values indicate the number of index traded on exchange for a particular trading day.</li></ul></div><p>These are <a id="id175" class="indexterm"/>the basic details of the DJIA index dataset. We use historical data and try to predict future movement in the DJIA index.</p><p>In the next section, we will look at the NYTimes news article dataset.</p></div><div class="section" title="Understanding the NYTimes news article dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec23"/>Understanding the NYTimes news article dataset</h2></div></div></div><p>We have <a id="id176" class="indexterm"/>used the NYTimes developer API and collected the news articles in a JSON form, so, here, we will look at the JSON response so we can identify the data attributes that are the most important and that we can focus on. In the next figure, you can see the JSON response that we get from the NYTimes:</p><div class="mediaobject"><img src="Images/B08394_02_04.jpg" alt="Understanding the NYTimes news article dataset" width="1000" height="626"/><div class="caption"><p>Figure 2.4: JSON response for news articles using the NYTimes developer tool</p></div></div><p>In this figure, we can see the JSON response for a single news article. As you can see, there is <a id="id177" class="indexterm"/>a main data attribute response that carries all other data attributes. We will focus on the data attributes that are given inside the docs array. Don't worry; we will not use all the data attributes. Here, we will focus on the following data attributes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">type_of_material</code>: This attribute <a id="id178" class="indexterm"/>indicates that a particular news article is derived from a particular kind of source, whether it's a blog, a news article, analysis, and so on.</li><li class="listitem" style="list-style-type: disc"><code class="literal">headlines</code>: The headline <a id="id179" class="indexterm"/>data attribute has the two sub-data attributes. The main data attribute contains the actual headline of the news and the kicker data attribute is convey the highlight of the article.</li><li class="listitem" style="list-style-type: disc"><code class="literal">pub_date</code>: This data <a id="id180" class="indexterm"/>attribute indicates the publication of the news article. You can find this attribute in the second-last section of the doc array.</li><li class="listitem" style="list-style-type: disc"><code class="literal">section_name</code>: This data <a id="id181" class="indexterm"/>attribute appeared in the preceding image in the last section. It provides the category of the news article.</li><li class="listitem" style="list-style-type: disc"><code class="literal">news_desk</code>: This data attribute also indicates the news category. When <code class="literal">section_name</code> is absent in a response, we will refer to this attribute.</li></ul></div><p>As we <a id="id182" class="indexterm"/>understand data attributes properly, we should move on to the next section, which is the data preprocessing and data analysis part.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Data preprocessing and data analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec25"/>Data preprocessing and data analysis</h1></div></div></div><p>In this <a id="id183" class="indexterm"/>section, we will mainly cover data preprocessing and data analysis. As a part of data preprocessing, we are preparing our training dataset. You may be wondering what kind of data preparation I'm talking about, considering we already <a id="id184" class="indexterm"/>have the data. Allow me to tell you that we have two different datasets and both datasets are independent. So, we need to merge the DJIA dataset and NYTimes news article dataset in order to get meaningful insights from these datasets. Once we prepare our training dataset, we can train the data using different machine learning (ML) algorithms.</p><p>Now let's start the coding to prepare the training dataset. We will be using <code class="literal">numpy</code>, <code class="literal">csv</code>, <code class="literal">JSON</code>, and <code class="literal">pandas</code> as our dependency libraries. Here, our code is divided into two parts. First, we will prepare the dataset for the DJIA index dataset and then we will move to the next part, which is preparing the NYTimes news article dataset. During the preparation of the training dataset, we will code the basic data analysis steps as well.</p><div class="section" title="Preparing the DJIA training dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec24"/>Preparing the DJIA training dataset</h2></div></div></div><p>You can <a id="id185" class="indexterm"/>see the code snippet in the following screenshot. You can find the code at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb">https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb</a>.</p><div class="mediaobject"><img src="Images/B08394_02_05.jpg" alt="Preparing the DJIA training dataset" width="606" height="500"/><div class="caption"><p>Figure 2.5: Code snippet for preparing the DJIA dataset</p></div></div><p>As you can <a id="id186" class="indexterm"/>see in the preceding code snippet, we are reading the csv file that we downloaded from the Yahoo Finance page earlier. After that, we convert the data into a list format. We also separated the header and actual data from the list. Once we have the data in list format, we convert the data into a numpy array. We have selected only three columns from the DIJA dataset, as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Date</li><li class="listitem" style="list-style-type: disc">Close price</li><li class="listitem" style="list-style-type: disc">Adj close price</li></ul></div><p>You may have one question in mind: why have we considered only close price and Adj close price from the DJIA csv file? Let me clarify: as we know that open price is mostly a nearby value of the last day's close price, we haven't considered the open price. We haven't considered the high price and low price because we don't know in which particular <a id="id187" class="indexterm"/>timestamp these high and low prices occurred. For the first iteration, it is quite complicated to predict when the stock index reach a high or low value, so, in the meantime, we ignore these two columns. We are mainly interested in the overall trend for the DJIA index. If we figure out the trend precisely, we can predict the high and low price values later on. Here, we restrict our goal to predicting the closing prices for the DJIA index for future trading days.</p><p>Now back to the coding part: we built the pandas dataframe in such a way that the date column acts as the index column, and close price and adj close price are the two other columns of the dataset. You can see the output of the dataframe defined in the form of the <code class="literal">df</code> variable in the code snippet given in <span class="emphasis"><em>Figure 2.5</em></span>. You can see the output of dataframe df in the following figure:</p><div class="mediaobject"><img src="Images/B08394_02_06.jpg" alt="Preparing the DJIA training dataset" width="269" height="220"/><div class="caption"><p>Figure 2.6: Output of pandas dataframe, which is defined as the <span class="emphasis"><em>df</em></span> variable in the code snippet in Figure 2.5</p></div></div><p>Hopefully now you have a clear understanding of the kind of steps we have followed so far. We have <a id="id188" class="indexterm"/>created the basic dataframe, so now we will move on to the basic data analysis part for a DJIA dataset.</p></div><div class="section" title="Basic data analysis for a DJIA dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec25"/>Basic data analysis for a DJIA dataset</h2></div></div></div><p>In this section, we will perform basic data analysis on a DJIA dataset. This dataset has the date <a id="id189" class="indexterm"/>value, but if you look at the values of the date carefully, then you will see that there are some missing dates. Suppose data is missing for 30-12-2006, 31-12-2006, 1-1-2007, and many other dates. In such cases, we will add the date values that are missing. You can refer to the code snippet given in <span class="emphasis"><em>Figure 2.7</em></span>, as well as find the code for this on this GitHub: <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb">https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb</a>.</p><div class="mediaobject"><img src="Images/B08394_02_07.jpg" alt="Basic data analysis for a DJIA dataset" width="612" height="418"/><div class="caption"><p>Figure 2.7: Code snippet for adding all the missing date values in the DJIA dataset</p></div></div><p>As you can see in the preceding figure, we come across another challenge after adding these missing date values. We have added the date value, but there is no close price or adj close price available corresponding to each of them, so we need to replace the NaN values logically, not randomly.</p><p>In order <a id="id190" class="indexterm"/>to replace the NaN values of close price and adj close price, we will use the pandas interpolation functionality. We use linear interpolation to generate the missing values for NaN. There are many types of interpolation available, but here we are using linear interpolation, and the mathematical equation for linear interpolation is as follows:</p><div class="mediaobject"><img src="Images/B08394_02_39.jpg" alt="Basic data analysis for a DJIA dataset" width="230" height="127"/><div class="caption"><p>Equation 2.1: Linear interpolation math formula</p></div></div><p>If the two known points are given by the coordinates (x1,y_1) and (x_3,y_3), the linear interpolant is the straight line between these points.</p><p>You can refer to the code snippet in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_02_08.jpg" alt="Basic data analysis for a DJIA dataset" width="855" height="392"/><div class="caption"><p>Figure 2.8: Code snippet for basic data analysis and interpolation implementation</p></div></div><p>The code for this is available on GitHub at <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb">https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb</a>.</p><p>As you can see in the code snippet, we haven't defined which type of interpolation should be <a id="id191" class="indexterm"/>performed on our dataset; in this case, linear interpolation has been performed by default. So after applying the linear interpolation, we can replace the NaN values with the actual logical values. We have also removed three records from the year 2006. So now, we have a total of 3653 records.</p><p>This is the kind of basic data preprocessing and data analysis we did for the DJIA index dataset. Now let's move on to the NYTimes news article dataset. We need to prepare the training dataset first, so let's begin with it.</p></div><div class="section" title="Preparing the NYTimes news dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec26"/>Preparing the NYTimes news dataset</h2></div></div></div><p>In this section, we will see how we can prepare the NYTimes news dataset. We have downloaded <a id="id192" class="indexterm"/>the whole news article dataset but we have not put in a filtering mechanism for choosing news article categories. Perform the following steps when preparing the NYTimes dataset:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Converting publication date into the YYYY-MM-DD format.</li><li class="listitem">Filtering news articles by their category.</li><li class="listitem">Implementing the filter functionality and merge the dataset.</li><li class="listitem">Saving the merged dataset in the pickle file format.</li></ol></div><p>So, let's start coding for each of these steps.</p><div class="section" title="Converting publication date into the YYYY-MM-DD format"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec11"/>Converting publication date into the YYYY-MM-DD format</h3></div></div></div><p>First, we will <a id="id193" class="indexterm"/>convert the publication date of the news articles into the YYYY-MM-DD format so that we can merge DJIA and NYTimes news article datasets later on. In order to achieve this, you can refer to the following code snippet:</p><div class="mediaobject"><img src="Images/B08394_02_09.jpg" alt="Converting publication date into the YYYY-MM-DD format" width="882" height="195"/><div class="caption"><p>Figure 2.9: Code snippet for converting the date format of the publication date of the news article</p></div></div><p>Here, we have written a function that can parse and convert the publication date format into the necessary YYYY-MM-DD format. We will call this function later on when we read the JSON files in which we have stored the JSON response.</p></div><div class="section" title="Filtering news articles by category"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec12"/>Filtering news articles by category</h3></div></div></div><p>The other <a id="id194" class="indexterm"/>thing that we are going to do here is filter our news article dataset by news category. We have downloaded all types of news articles, but for the stock market price prediction application, we need news articles that belong to specific news categories. So, we need to implement filters that will help us extract the necessary subset of news articles. You can refer to the following code snippet:</p><div class="mediaobject"><img src="Images/B08394_02_10.jpg" alt="Filtering news articles by category" width="982" height="282"/><div class="caption"><p>Figure 2.10: Code snippet for filtering news articles by their categories</p></div></div><p>You can <a id="id195" class="indexterm"/>refer to the code provided at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb"> https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb</a>.</p><p>As shown in the preceding figure, we are extracting news articles that belong to the following news categories:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Business</li><li class="listitem" style="list-style-type: disc">National</li><li class="listitem" style="list-style-type: disc">World</li><li class="listitem" style="list-style-type: disc">U.S.A.</li><li class="listitem" style="list-style-type: disc">Politics</li><li class="listitem" style="list-style-type: disc">Opinion</li><li class="listitem" style="list-style-type: disc">Tech</li><li class="listitem" style="list-style-type: disc">Science</li><li class="listitem" style="list-style-type: disc">Health</li><li class="listitem" style="list-style-type: disc">Foreign</li></ul></div></div><div class="section" title="Implementing the filter functionality and merging the dataset"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec13"/>Implementing the filter functionality and merging the dataset</h3></div></div></div><p>Now, we need <a id="id196" class="indexterm"/>to iterate each of the JSON files and extract the news articles that have one of the news categories defined <a id="id197" class="indexterm"/>in the previous section. You can refer to the code snippet for the implementation of the filter functionality. In the upcoming code snippet, you can also find the implementation for merging the DJIA dataset and the NYTimes news articles dataset. To merge the two datasets, we are adding each of the news article headlines to the pandas dataframe,and from this we will generate our final training dataset. This functionality is shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_02_11.jpg" alt="Implementing the filter functionality and merging the dataset" width="981" height="762"/><div class="caption"><p>Figure 2.11: Code snippet for the filtering and merging functionalities</p></div></div><p>We have also coded a bit of the exceptional handling functionality. This is done so that if any JSON <a id="id198" class="indexterm"/>response does not have <a id="id199" class="indexterm"/>the value for the data attributes section_name, news_desk, or type_of_material, then this code will throw an exception. You can refer to the code snippet in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_02_12.jpg" alt="Implementing the filter functionality and merging the dataset" width="862" height="822"/><div class="caption"><p>Figure 2.12: Implementation of exception handling </p></div></div><p>We will consider news articles that have no <code class="literal">section_name</code> and <code class="literal">news_desk</code> as well. We will add all the news article headlines to our dataset and put them into the pandas dataframe. You can <a id="id200" class="indexterm"/>see the code <a id="id201" class="indexterm"/>snippet in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_02_13.jpg" alt="Implementing the filter functionality and merging the dataset" width="982" height="606"/><div class="caption"><p>Figure 2.13: Handling news articles that have no section_name and news_desk</p></div></div><p>You can see the final merged dataset in the form of the pandas dataframe, as shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_02_14.jpg" alt="Implementing the filter functionality and merging the dataset" width="549" height="222"/><div class="caption"><p>Figure 2.14: Final merged training dataset</p></div></div><p>Here, for <a id="id202" class="indexterm"/>each date, we correspond <a id="id203" class="indexterm"/>all the news headlines that belong to the business, national, world, U.S.A., politics, opinion, technology, science, and heath categories. We have downloaded 1,248,084 news articles, and from these articles, we have considered 461,738 news articles for our model.</p><p>You can access the code using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb">https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb</a>.</p></div><div class="section" title="Saving the merged dataset in the pickle file format"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec14"/>Saving the merged dataset in the pickle file format</h3></div></div></div><p>Once <a id="id204" class="indexterm"/>we merge the data, we need to save the data objects, so we will use the pickle module of Python. Pickle helps us serialize and de-serialize the data. The pickle dependency library is fast because the bulk of it is written in C, like the Python interpreter itself. Here, we save our training dataset as a <code class="literal">.pkl</code> file format. You can refer to the following code snippet<span class="emphasis"><em>:</em></span>
</p><div class="mediaobject"><img src="Images/B08394_02_15.jpg" alt="Saving the merged dataset in the pickle file format" width="751" height="222"/><div class="caption"><p>Figure 2.15: Code snippet for saving data in the pickle format</p></div></div><p>We have <a id="id205" class="indexterm"/>saved the dataset as the <code class="literal">pickled_ten_year_filtered_lead_para.pkl</code> file. You can find the code on GitHub at <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb">https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb</a>.</p><p>In the next section, we will mainly focus on the feature engineering part. We will also perform some minor data cleaning steps. So let's jump to the next section.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Feature engineering"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec26"/>Feature engineering</h1></div></div></div><p>As discussed <a id="id206" class="indexterm"/>earlier, we want to predict the close price for the DJIA index for a particular trading day. In this section, we will do feature selection based on our intuition for our basic prediction model for stock prices. We have already generated the training dataset. So, now we will load the saved .pkl format dataset and perform feature selection as well as minor data processing. We will also generate the sentiment score for each of the filtered NYTimes news articles and will use this sentiment score to train our baseline model. We will use the following Python dependencies:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">numpy</li><li class="listitem" style="list-style-type: disc">pandas</li><li class="listitem" style="list-style-type: disc">nltk</li></ul></div><p>This section <a id="id207" class="indexterm"/>has the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Loading the dataset</li><li class="listitem">Minor preprocessing</li><li class="listitem">Feature selection</li><li class="listitem">Sentiment analysis</li></ol></div><p>So, let's begin coding!</p><div class="section" title="Loading the dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec27"/>Loading the dataset</h2></div></div></div><p>We have <a id="id208" class="indexterm"/>saved the data in the pickle format, and now we need to load <a id="id209" class="indexterm"/>data from it. You can refer to the following code snippet<span class="emphasis"><em>:</em></span>
</p><div class="mediaobject"><img src="Images/B08394_02_16.jpg" alt="Loading the dataset" width="691" height="338"/><div class="caption"><p>Figure 2.16: Code snippet for loading the dataset from the pickle file</p></div></div><p>You can refer to the code by clicking on this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb">https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb</a>.</p><p>As you can see, in the dataframe output, there is a dot (.) before every article headline in the <a id="id210" class="indexterm"/>entire dataset, so we need to remove these dots. We will execute <a id="id211" class="indexterm"/>this change in the next section.</p></div><div class="section" title="Minor preprocessing"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec28"/>Minor preprocessing</h2></div></div></div><p>As a part <a id="id212" class="indexterm"/>of minor preprocessing, we will be <a id="id213" class="indexterm"/>performing the following two changes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Converting the adj close prices into the integer format</li><li class="listitem" style="list-style-type: disc">Removing the leftmost dot (.) from news headlines</li></ul></div><div class="section" title="Converting adj close price into the integer format"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec15"/>Converting adj close price into the integer format</h3></div></div></div><p>We know <a id="id214" class="indexterm"/>that the adj close price is in the form of a float format. So, here we will convert float values into the integer format as well as store the converted values as <span class="emphasis"><em>price</em></span> attributes in our pandas dataframe. Now, you may wonder why we consider only the adj close prices. Bear with me for a while, and I will give you the reason for that. You can find the convergence code snippet in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_02_17.jpg" alt="Converting adj close price into the integer format" width="537" height="324"/><div class="caption"><p>Figure 2.17: Code snippet for converting the adj close price into the integer format</p></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip02"/>Tip</h3><p>You can refer to the code at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb">https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb</a>.</p></div></div><p>Now, let's <a id="id215" class="indexterm"/>move on to the second change.</p></div><div class="section" title="Removing the leftmost dot from news headlines"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec16"/>Removing the leftmost dot from news headlines</h3></div></div></div><p>In this <a id="id216" class="indexterm"/>section, we will see the implementation for removing the leftmost dot. We will be using the <code class="literal">lstrip()</code> function to remove the dot. You can refer to the code snippet in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_02_18.jpg" alt="Removing the leftmost dot from news headlines" width="669" height="261"/><div class="caption"><p>Figure 2.18: Code snippet for removing <span class="emphasis"><em>dot</em></span> from the news article headlines</p></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip03"/>Tip</h3><p>You can refer to the code at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb">https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb</a>.</p></div></div><p>Now, let's move <a id="id217" class="indexterm"/>on to our next section, which is feature engineering.</p></div></div><div class="section" title="Feature engineering"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec29"/>Feature engineering</h2></div></div></div><p>Feature <a id="id218" class="indexterm"/>selection is one of the most important aspects of feature engineering and any <span class="strong"><strong>Machine Learning</strong></span> (<span class="strong"><strong>ML</strong></span>) application. So, here we will focus on feature selection. In the previous section, I raised the question of why we select only the <span class="emphasis"><em>adj close price</em></span> and not the <span class="emphasis"><em>close price.</em></span> The answer to this question lies in the feature selection. We select the <span class="emphasis"><em>adj close prices</em></span> because these prices give us a better idea about what the last price of the DJIA index is, including the stock, mutual funds, dividends, and so on. In our dataset, <span class="emphasis"><em>close prices</em></span> are mostly the same as the <span class="emphasis"><em>adj close price</em></span> and in future, if we consider the <span class="emphasis"><em>close price</em></span> for unseen data records, we can't derive the <span class="emphasis"><em>adj close price</em></span> because it may be equal to the <span class="emphasis"><em>close price</em></span> or higher than the <span class="emphasis"><em>close price,</em></span> The <span class="emphasis"><em>adj close price</em></span> for DJIA index may higher than the c<span class="emphasis"><em>lose price</em></span> because it will include stocks, mutual funds, dividend and so on. but we don't know how much higher it will be for unseen dataset where we have just considered <span class="emphasis"><em>close price</em></span>. So if we consider the <span class="emphasis"><em>adj close price,</em></span> then we will know that the <span class="emphasis"><em>close price</em></span> may be less than or equal to the <span class="emphasis"><em>adj close price</em></span>, but not more than the <span class="emphasis"><em>adj close price</em></span>. The <span class="emphasis"><em>adj close price</em></span> is kind of maximum possible value for closing price. So, we have considered the <span class="emphasis"><em>adj close price</em></span> for the <a id="id219" class="indexterm"/>development. For the baseline model, we will be considering the <span class="emphasis"><em>adj close price</em></span>. We have renamed the column to <span class="emphasis"><em>price</em></span>. You can refer to the following code snippet:</p><div class="mediaobject"><img src="Images/B08394_02_19.jpg" alt="Feature engineering" width="312" height="238"/><div class="caption"><p>Figure 2.19: Code snippet for considering the adj close price as a part of feature selection</p></div></div><p>As a next step, we will now perform sentiment analysis on the news article dataset. We can use <a id="id220" class="indexterm"/>the sentiment score when we train our model. So, let's move on to the sentiment analysis part.</p></div><div class="section" title="Sentiment analysis of NYTimes news articles"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec30"/>Sentiment analysis of NYTimes news articles</h2></div></div></div><p>In order <a id="id221" class="indexterm"/>to implement sentiment analysis, we are <a id="id222" class="indexterm"/>using the nltk inbuilt sentiment analysis module. We will obtain negative, positive, and compound sentiment scores. We have used a lexicon-based approach. In the lexicon-based approach, words of each sentence are analyzed, and based on the <code class="literal">sentiwordnet</code> score, each word is given a specific sentiment score; then, the aggregate sentence level score is decided.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note02"/>Note</h3><p>Sentiwordnet is the dictionary which contain sentiment score for words.</p></div></div><p>We will <a id="id223" class="indexterm"/>cover details related to sentiment analysis in <a class="link" href="ch05.xhtml" title="Chapter 5. Sentiment Analysis">Chapter 5</a>, <span class="emphasis"><em>Sentiment Analysis</em></span>. You can refer to the following sentiment <a id="id224" class="indexterm"/>analysis code snippet:</p><div class="mediaobject"><img src="Images/B08394_02_20.jpg" alt="Sentiment analysis of NYTimes news articles" width="942" height="373"/><div class="caption"><p>Figure 2.20: Sentiment analysis code snippet </p></div></div><p>All scores generated by the preceding code are stored in the dataframe, so you can see the aggregate score of news article headlines in the following screenshot<span class="emphasis"><em>:</em></span>
</p><div class="mediaobject"><img src="Images/B08394_02_21.jpg" alt="Sentiment analysis of NYTimes news articles" width="359" height="221"/><div class="caption"><p>Figure 2.21: Aggregate sentiment analysis score stored in the dateframe</p></div></div><p>By the end of this section, we will obtain the sentiment score for the NYTimes news articles <a id="id225" class="indexterm"/>dataset and combine these sentiment <a id="id226" class="indexterm"/>scores as part of the training dataset. So far, we have done minor preprocessing, selected the data attribute as per our intuition, and generated the sentiment score. Now, we will select the machine learning algorithm and try to build the baseline model. So, let's move on to the next section.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Selecting the Machine Learning algorithm"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec27"/>Selecting the Machine Learning algorithm</h1></div></div></div><p>In this section, we will choose the Machine Learning (ML) algorithm based on our intuition <a id="id227" class="indexterm"/>and then perform training using our training dataset. This is the first model for this particular chapter, so the trained model is our baseline model, which we will improve later on. So, let's decide which kind of ML algorithm suits this stock price prediction application.</p><p>The stock price prediction application is a time-series analysis problem, where we need to predict the next point in the time series. This prediction activity is similar to linear regression, so we can say that this application is a kind of regression problem and any algorithm from the regression family should work. Let's select the ensemble algorithm, which is <span class="emphasis"><em>RandomForestRegressor</em></span>, in order to develop our baseline model. So let's train our <a id="id228" class="indexterm"/>baseline model, and, based on the result of that model, we will modify our approach.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Training the baseline model"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec28"/>Training the baseline model</h1></div></div></div><p>As you <a id="id229" class="indexterm"/>know, we have selected the <span class="strong"><strong>RandomForestRegressor</strong></span> algorithm. We will be using the scikit-learn library to train the model. These are the steps we need to follow:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Splitting the training and testing dataset</li><li class="listitem">Splitting prediction labels for the training and testing dataset </li><li class="listitem">Converting sentiment scores into the numpy array</li><li class="listitem">Training the ML model</li></ol></div><p>So, let's implement each of these steps one by one.</p><div class="section" title="Splitting the training and testing dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec31"/>Splitting the training and testing dataset</h2></div></div></div><p>We have <a id="id230" class="indexterm"/>10 years of data values. So for training purposes, we will be using 8 years of the data, which means the dataset from 2007 to 2014. For testing <a id="id231" class="indexterm"/>purposes, we will be using 2 years of the data, which means data from 2015 and 2016. You can refer to the code snippet in the following screenshot to implement this:</p><div class="mediaobject"><img src="Images/B08394_02_22.jpg" alt="Splitting the training and testing dataset" width="464" height="180"/><div class="caption"><p>Figure 2.22: Splitting the training and testing dataset </p></div></div><p>As you <a id="id232" class="indexterm"/>can see from the preceding screenshot, our training <a id="id233" class="indexterm"/>dataset has been stored in the train dataframe and our testing dataset has been stored in the test dataframe.</p></div><div class="section" title="Splitting prediction labels for the training and testing datasets"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec32"/>Splitting prediction labels for the training and testing datasets</h2></div></div></div><p>As we <a id="id234" class="indexterm"/>split the training and testing dataset, we also need to store the adj close price separately because we need to predict these <span class="emphasis"><em>adj close prices</em></span> (indicated in the code as <code class="literal">prices</code>); these price values are labels <a id="id235" class="indexterm"/>for our training data, and this training becomes supervised training as we will provide the actual price in the form of labels. You can refer to the following code for the implementation:</p><div class="mediaobject"><img src="Images/B08394_02_23.jpg" alt="Splitting prediction labels for the training and testing datasets" width="493" height="114"/><div class="caption"><p>Figure 2.23: Splitting the prediction labels for training and testing datasets</p></div></div><p>Here, all attributes except the price are given in a feature vector format and the price is in the form of labels. The ML algorithm takes this feature vector, labels the pair, learns the necessary pattern, and predicts the price for the unseen data.</p></div><div class="section" title="Converting sentiment scores into the numpy array"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec33"/>Converting sentiment scores into the numpy array</h2></div></div></div><p>Before we <a id="id236" class="indexterm"/>start the training, there is one last, necessary point that we need to keep in mind: we are converting the sentiment analysis scores into the numpy array format. This is because once we set the price attribute as a prediction label, our features vector will contain only the sentiment scores and date. So in order to generate a proper feature vector, we have converted <a id="id237" class="indexterm"/>the sentiment score into a numpy array. The code snippet to implement this is provided in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_02_24.jpg" alt="Converting sentiment scores into the numpy array" width="853" height="291"/><div class="caption"><p>Figure 2.24: Code snippet for converting sentiment analysis score into the numpy array</p></div></div><p>As you can see from the code snippet, we have performed the same conversion operation for both training the dataset and testing the dataset.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>Note that if you get a value error, check the dataset because there may be a chance that a column in the dataset has a blank or null value.</p></div></div><p>Now, let's train our model!</p></div><div class="section" title="Training of the ML model"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec34"/>Training of the ML model</h2></div></div></div><p>In the <a id="id238" class="indexterm"/>first iteration, we use the RandomForestRegressor algorithm, which is provided as part of the scikit-learn dependency. You can find the code for this in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_02_25.jpg" alt="Training of the ML model" width="1000" height="318"/><div class="caption"><p>Figure 2.25: Code snippet for training using RandomForestRegressor</p></div></div><p>As you <a id="id239" class="indexterm"/>can see from the preceding screenshot, we have used all the default values for our hyperparameters. For a more detailed description regarding hyperparameters, you can refer to <a class="ulink" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html">http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html</a>.</p><p>Now that our model has been trained, we need to test it using our testing dataset. Before we test, let's discuss the approach we will take to test our model.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding the testing matrix"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec29"/>Understanding the testing matrix</h1></div></div></div><p>In this <a id="id240" class="indexterm"/>section, we will understand the testing matrix and visualization approaches to evaluate the performance of the trained ML model. So let's understand both approaches, which are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The default testing matrix</li><li class="listitem" style="list-style-type: disc">The visualization approach</li></ul></div><div class="section" title="The default testing matrix"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec35"/>The default testing matrix</h2></div></div></div><p>We are <a id="id241" class="indexterm"/>using the default score API of scikit-learn to check how well the ML is performing. In this application, the score function is the coefficient of the sum of the squared error. It is also called the coefficient of R2, which is defined by the following equation:</p><div class="mediaobject"><img src="Images/B08394_02_40.jpg" alt="The default testing matrix" width="44" height="52"/></div><p>Here, <span class="emphasis"><em>u</em></span> indicates the residual sum of squares. The equation for <span class="emphasis"><em>u</em></span> is as follows:</p><div class="mediaobject"><img src="Images/B08394_02_41.jpg" alt="The default testing matrix" width="252" height="47"/></div><p>The variable <span class="emphasis"><em>v</em></span> indicates the total sum of squares. The equation for <span class="emphasis"><em>v</em></span> is as follows:</p><div class="mediaobject"><img src="Images/B08394_02_42.jpg" alt="The default testing matrix" width="295" height="47"/></div><p>The best <a id="id242" class="indexterm"/>possible score is 1.0, and it can be a negative score as well. A negative score indicates that the trained model can be arbitrarily worse. A constant model that always predicts the expected value for label <span class="emphasis"><em>y</em></span>, disregarding the input features, will produce an R2 score of 0.0.</p><p>In order to obtain the score, we just need to call the score function. The code for testing will be the same as that in the <span class="emphasis"><em>Test baseline model </em></span>section. Now let's take a look at another testing approach that is quite helpful in understanding the output with respect to true testing labels. So, let's check that out!</p></div><div class="section" title="The visualization approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec36"/>The visualization approach</h2></div></div></div><p>In this section, we will <a id="id243" class="indexterm"/>be exploring an effective <a id="id244" class="indexterm"/>and intuitive approach, which is the <span class="strong"><strong>visualization</strong></span> of the predicted output versus real output. This approach gives you a lot of insight as the graphs are easy to understand and you can decide the next steps to improve the model.</p><p>In this application, we will be using the actual prices from the testing dataset and the predicted <a id="id245" class="indexterm"/>prices for the testing dataset, which will indicate how good or bad the predictions are. You will find the code and graph for this process in the next section, named <span class="emphasis"><em>Testing the baseline model</em></span>.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Testing the baseline model"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec30"/>Testing the baseline model</h1></div></div></div><p>In this <a id="id246" class="indexterm"/>section, we will be implementing our testing approach so that we can evaluate our model's accuracy. We will first generate the output prediction and then we'll start testing it. We will be implementing the following steps here:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Generating and interpreting the output </li><li class="listitem">Generating the score </li><li class="listitem">Visualizing the output</li></ol></div><div class="section" title="Generating and interpreting the output"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec37"/>Generating and interpreting the output</h2></div></div></div><p>To generate <a id="id247" class="indexterm"/>the prediction, we are using the <code class="literal">treeinterpreter</code> library. We are predicting the price value for each of our testing dataset <a id="id248" class="indexterm"/>records using the following code:</p><div class="mediaobject"><img src="Images/B08394_02_26.jpg" alt="Generating and interpreting the output" width="713" height="401"/><div class="caption"><p>Figure 2.26: Code snippet for generating the prediction</p></div></div><p>Here, <span class="emphasis"><em>prediction</em></span> is the array in which we have elements that are the corresponding predicted <span class="emphasis"><em>adj close price </em></span>for all records of the testing dataset. Now, we will compare this predicted <a id="id249" class="indexterm"/>output with the actual <span class="emphasis"><em>adj close price</em></span> of the <a id="id250" class="indexterm"/>testing dataset. By doing this, we will get to know how accurately our first model is predicting the <span class="emphasis"><em>adj close price</em></span>. In order to evaluate further, we will generate the accuracy score.</p></div><div class="section" title="Generating the accuracy score"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec38"/>Generating the accuracy score</h2></div></div></div><p>In this <a id="id251" class="indexterm"/>section, we will generate the accuracy score as per the equations provided in the <span class="emphasis"><em>default testing matrix</em></span> section. The code for this is as follows:</p><div class="mediaobject"><img src="Images/B08394_02_27.jpg" alt="Generating the accuracy score" width="278" height="67"/><div class="caption"><p>Figure 2.27: Code snippet for generating the score for the test dataset</p></div></div><p>As you can see from the preceding code snippet, our model is not doing too well. At this point, we don't know what mistakes we've made or what went wrong. This kind of situation is common when you are trying to solve or build an ML model. We can grasp the problem better using visualization techniques.</p></div><div class="section" title="Visualizing the output"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec39"/>Visualizing the output</h2></div></div></div><p>We will be <a id="id252" class="indexterm"/>using the visualization graph in this section. Using the graph, we will identify the kind of error we have committed so that we can fix that error in the next iteration. We will plot a graph where the <span class="emphasis"><em>y-axis</em></span> represents the <span class="emphasis"><em>adj close prices</em></span> and the <span class="emphasis"><em>x-axis</em></span> represent the <span class="emphasis"><em>dates</em></span>. We plot the <span class="emphasis"><em>actual prices</em></span> and <span class="emphasis"><em>predicted prices</em></span> on the graph so that we will get a brief idea about how our algorithm is performing. We will use the following code snippet to generate the graph:</p><div class="mediaobject"><img src="Images/B08394_02_28.jpg" alt="Visualizing the output" width="878" height="752"/><div class="caption"><p>Figure 2.28: Code snippet for generating graph for predicted prices vs actual prices.</p></div></div><p>As you <a id="id253" class="indexterm"/>can see from the preceding graph, the top single line (orange color) represents the actual price and the messy spikes (blue color) below the line represent the predicted prices. From this plot, we can summarize that our model can't predict the proper prices. Here, you can see that the actual prices and predicted prices are not aligned with each other. We need to fix this issue. There are some techniques that we can try, such as alignment, smoothing, and trying a different algorithm. So, let's cover the problems of this approach in the next section.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>You can access the entire code on this topic from the GitHub link at <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb">https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb</a>.</p></div></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Exploring problems with the existing approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec31"/>Exploring problems with the existing approach</h1></div></div></div><p>In this <a id="id254" class="indexterm"/>section, we will be discussing the problems of the existing approach. There are mainly three errors we could have possibly committed, which are listed as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Alignment</li><li class="listitem" style="list-style-type: disc">Smoothing</li><li class="listitem" style="list-style-type: disc">Trying a different ML algorithm</li></ul></div><p>Let's discuss each of the points one by one.</p><div class="section" title="Alignment"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec40"/>Alignment</h2></div></div></div><p>As we <a id="id255" class="indexterm"/>have seen in the graph, our actual price and predicted prices are not aligned with each other. This becomes a problem. We need to perform alignment on the price of the stocks. We need to consider the average value of our dataset, and based on that, we will generate the alignment. You can understand more about alignment in upcoming section called <span class="emphasis"><em>Alignment-based approach</em></span>.</p></div><div class="section" title="Smoothing"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec41"/>Smoothing</h2></div></div></div><p>The second <a id="id256" class="indexterm"/>problem I feel we have with our first model is that we haven't applied any smoothing techniques. So for our model, we need to apply smoothing techniques as well. We will be using the <span class="strong"><strong>Exponentially Weighted Moving Average</strong></span> (<span class="strong"><strong>EWMA</strong></span>) technique for smoothing<span class="emphasis"><em>. </em></span>This technique is used to adjust the variance of the dataset.</p></div><div class="section" title="Trying a different ML algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec42"/>Trying a different ML algorithm</h2></div></div></div><p>For our <a id="id257" class="indexterm"/>model, we have used the <code class="literal">RandomForestRegressor</code> algorithm. But what if we try the same thing with our model using a different algorithm, say <span class="emphasis"><em>Logistic Regression</em></span>? In the next section, you will learn how to implement this algorithm—after applying the necessary alignment and smoothing, of course.</p><p>We have seen the possible problems with our first baseline approach. Now, we will try to understand the approach for implementing the alignment, smoothing, and <code class="literal">Logistic Regression </code>algorithms.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding the revised approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec32"/>Understanding the revised approach</h1></div></div></div><p>In this <a id="id258" class="indexterm"/>section, we will be looking at the key concepts and approaches for alignment and smoothing. It is not that difficult to implement the <span class="emphasis"><em>Logistic Regression</em></span> algorithm; we will be using the scikit-learn API. So, we will start with understanding the concepts and approaches for implementation.</p><div class="section" title="Understanding concepts and approaches"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec43"/>Understanding concepts and approaches</h2></div></div></div><p>Here, we will <a id="id259" class="indexterm"/>discuss how alignment and smoothing will work. Once we understand the technicality behind alignment and smoothing, we will focus on the Logistic Regression-based approach.</p><div class="section" title="Alignment-based approach"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec17"/>Alignment-based approach</h3></div></div></div><p>Using this <a id="id260" class="indexterm"/>approach, we will be increasing the prices using a constant value so that our predicted price and actual price in testing the dataset will be aligned. Suppose we take 10 days into consideration. We will generate the average of the value of the prices. After that, we generate the average value for the prices that have been predicted by the first ML model. Once we generate both average values, we need to subtract the values, and the answer is the alignment value for those <code class="literal">10</code> days.</p><p>Let's take an intuitive working example that will help clear your vision. Consider 10 days from January 2, 2015, to January 11, 2015. For each record, you will take the average value for <a id="id261" class="indexterm"/>the actual price. Suppose the number will come to 17,676 and the average of predicted price value will be 13,175. In this case, you will get a difference of 4,501, which is the value for the alignment. We will add this value to our testing dataset so that testing price values and predicted price values will be aligned. You will find the code implementation in the <span class="emphasis"><em>Implement revised approach</em></span> section.</p></div><div class="section" title="Smoothing-based approach"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec18"/>Smoothing-based approach</h3></div></div></div><p>In this <a id="id262" class="indexterm"/>approach, we will be using EWMA. <span class="strong"><strong>EWMA</strong></span> stands for <span class="strong"><strong>Exponentially Weighted Moving Average</strong></span>. The smoothing approach is based on the weighted average concept. In <a id="id263" class="indexterm"/>general, a weighted moving average is calculated by the following equation:</p><div class="mediaobject"><img src="Images/B08394_02_43.jpg" alt="Smoothing-based approach" width="132" height="74"/></div><p>Here, <span class="emphasis"><em>x<sub>t</sub></em></span> is the input and <span class="emphasis"><em>y<sub>t</sub></em></span> is the output. Weights are calculated using the following equations:</p><div class="mediaobject"><img src="Images/B08394_02_29.jpg" alt="Smoothing-based approach" width="694" height="317"/><div class="caption"><p>Figure 2.29: Equation for calculating the weight for EWMA</p><p>Image source: <a class="ulink" href="http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows">http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows</a></p></div></div><p>Here, α is the smoothing constant. If the value of the smoothing constant is high, then it will be close to the actual value, and if the smoothing constant is low, then it will be smoother <a id="id264" class="indexterm"/>but not close to the actual value. Typically, in statistics the smoothing constant ranges between 0.1 and 0.3. Therefore, we can generate the smoothed value using the smoothing constant.</p><p>Let's take a working example. Take a smoothing constant = 0.3; if the actual value is 100 and the predicted value is 110, then the smoothed value can be obtain using this equation, which is (smoothing constant * actual value ) + (1- smoothing constant) * predicted value. The value that we will obtain is <span class="emphasis"><em>(0.3* 100) + (1-0.3)*110 = 107</em></span>. For more information, you can refer to <a class="ulink" href="http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows">http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows</a>.</p><p>We will see the actual code-level implementation in the Implement revised approach section. pandas already has an API, so we can easily implement EWMA.</p></div><div class="section" title="Logistic Regression-based approach"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec19"/>Logistic Regression-based approach</h3></div></div></div><p>Implementing <a id="id265" class="indexterm"/>the Logistic Regression algorithm is a simple task because we just need to use the scikit-learn API. For the testing dataset, we will apply alignment and smoothing. After evaluating accuracy, we will decide whether we need to change the ML algorithm or not. We started with our intuition and slowly we improved our approaches. I don't really need to explain the Logistic Regression algorithm itself, but during the implementation, we will discuss the important points.</p><p>Now, it is time to move on to the implementation part of our revised approach. So, let's take a look at the next section.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Implementing the revised approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec33"/>Implementing the revised approach</h1></div></div></div><p>In this <a id="id266" class="indexterm"/>section, we will discuss the three parts of implementation, which are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementation</li><li class="listitem" style="list-style-type: disc">Testing the revised approach</li><li class="listitem" style="list-style-type: disc">Understanding the problem with the revised approach</li></ul></div><div class="section" title="Implementation"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec44"/>Implementation</h2></div></div></div><p>Here, we <a id="id267" class="indexterm"/>are implementing the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Alignment</li><li class="listitem" style="list-style-type: disc">Smoothing</li><li class="listitem" style="list-style-type: disc">Logistic Regression</li></ul></div><p>We have already discussed the approach and key concepts, so now we just focus on the code part here. You can find all the code at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb">https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb</a>.</p><div class="section" title="Implementing alignment"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec20"/>Implementing alignment</h3></div></div></div><p>The alignment <a id="id268" class="indexterm"/>is performed on the testing dataset. You can refer to the following code snippet:</p><div class="mediaobject"><img src="Images/B08394_02_30.jpg" alt="Implementing alignment" width="831" height="668"/><div class="caption"><p>Figure 2.30: Code snippet for alignment on the test dataset</p></div></div><p>As you can see in the preceding code snippet, we obtain a difference of 10 days <span class="emphasis"><em>adj close price</em></span> using the average price of the last 5 days and the average price of the predicted upcoming 5 days in order to align the test data. Here, we also convert the date from the string into the date format. As you can see, 5096.99 is the difference in the test prediction price, which we will add to our predicted <span class="emphasis"><em>adj close price</em></span> value. We have generated the graph again so we can easily understand that the alignment approach is implemented nicely. You can refer to the following code snippet:</p><div class="mediaobject"><img src="Images/B08394_02_31.jpg" alt="Implementing alignment" width="1000" height="684"/><div class="caption"><p>Figure 2.31: Code snippet of the graph for the alignment approach</p></div></div><p>As you <a id="id269" class="indexterm"/>can see in the preceding code snippet, the alignment graph shows that our testing dataset price and predicted prices are aligned. The benefit of the aligned graph is that now we can define in a precise manner that <code class="literal">RandomForestRegressor</code> didn't do its job with high accuracy as its performance was not great for all data records. The alignment graph gave us a crystal clear picture of our previous iteration. So when we train the logistic regression now, we will evaluate the predicted prices using alignment.</p></div><div class="section" title="Implementing smoothing"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec21"/>Implementing smoothing</h3></div></div></div><p>We are <a id="id270" class="indexterm"/>using the pandas EWMA API using 60 days' time span and frequency time <span class="emphasis"><em>D. </em></span>This "D" indicates that we are dealing with the datetime format in our dataset. You can see the code implementation in the following code snippet:</p><div class="mediaobject"><img src="Images/B08394_02_32.jpg" alt="Implementing smoothing" width="850" height="630"/><div class="caption"><p>Figure 2.32: Code snippet for EWMA smoothing </p></div></div><p>We are <a id="id271" class="indexterm"/>also generating the graph in which we put the <span class="emphasis"><em>predicted price, average predicted price, actual price</em></span>, and <span class="emphasis"><em>average actual price</em></span>. You can refer to the following code and graph:</p><div class="mediaobject"><img src="Images/B08394_02_33.jpg" alt="Implementing smoothing" width="1000" height="475"/><div class="caption"><p>Figure 2.33: Code snippet for generating the graph after smoothing</p></div></div><p>In this <a id="id272" class="indexterm"/>graph, you can see that after smoothing the <span class="emphasis"><em>average predicted price,</em></span> the curve follows the <span class="emphasis"><em>actual price</em></span> trend. Although the accuracy is not great, we will move toward a positive direction. The smoothing technique will be useful for us if we want to tune our algorithm. You can refer to the following graph for the <span class="emphasis"><em>average predicted price versus actual price:</em></span>
</p><div class="mediaobject"><img src="Images/B08394_02_34.jpg" alt="Implementing smoothing" width="934" height="443"/><div class="caption"><p>Figure 2.34: Code snippet for the graph, indicating average_predicted_price versus actual_price</p></div></div><p>By referring to the preceding graph, we can indicate that we apply alignment and smoothing <a id="id273" class="indexterm"/>because it helps tune our ML model for the next iteration.</p></div><div class="section" title="Implementing logistic regression"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec22"/>Implementing logistic regression</h3></div></div></div><p>In this <a id="id274" class="indexterm"/>section, we will be implementing logistic regression. Take a look at the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_02_35.jpg" alt="Implementing logistic regression" width="893" height="791"/><div class="caption"><p>Figure 2.35: Code snippet for logistic regression</p></div></div><p>Here, we have trained the model again using the logistic regression ML algorithm. We have also <a id="id275" class="indexterm"/>implemented alignment and smoothing for the test dataset. Now, let's evaluate the logistic regression model.</p></div></div><div class="section" title="Testing the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec45"/>Testing the revised approach</h2></div></div></div><p>We have <a id="id276" class="indexterm"/>tested the logistic regression model. You can refer to the visualization in the form of graphs that show that this revised approach is certainly better than <span class="emphasis"><em>RandomForesRegressor (without alignment and smoothing), </em></span>but it is not up to the mark:</p><div class="mediaobject"><img src="Images/B08394_02_36.jpg" alt="Testing the revised approach" width="437" height="802"/><div class="caption"><p>Figure 2.36: Year-wise prediction graph</p></div></div><p>As you can see in the preceding screenshot, we have generated a year-wise graph for <span class="emphasis"><em>logistic Regression</em></span>; we can see a slight improvement using this model. We have also used alignment <a id="id277" class="indexterm"/>and smoothing, but they are not too effective.</p><p>Now, let's discuss what the problems with this revised approach are, and then we can implement the best approach.</p></div><div class="section" title="Understanding the problem with the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec46"/>Understanding the problem with the revised approach</h2></div></div></div><p>In this <a id="id278" class="indexterm"/>section, we will discuss why our revised approach doesn't give us good results. ML models don't work because datasets are not normalized. The second reason is that even after alignment and smoothing, the <span class="emphasis"><em>RandomForestRegression</em></span> ML model faces an overfitting issue. For the best approach, we need to handle normalization and overfitting. We can solve this issue using a neural network-based ML algorithm. So in our last iteration, we will develop the neural network that can give us the best accuracy.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="The best approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec34"/>The best approach</h1></div></div></div><p>Here, we <a id="id279" class="indexterm"/>are going to implement the neural <a id="id280" class="indexterm"/>network-based algorithm <span class="strong"><strong>multilayer perceptron</strong></span> (<span class="strong"><strong>MLP</strong></span>). You can refer to the following code snippet:</p><div class="mediaobject"><img src="Images/B08394_02_37.jpg" alt="The best approach" width="899" height="778"/><div class="caption"><p>Figure 2.37: Code snippet for multilayer perceptron</p></div></div><p>Here, you <a id="id281" class="indexterm"/>can see that we are using the Relu activation function, and the gradient descent solver function is ADAM. We are using a learning rate of 0.0001. You can evaluate the result by referring to the following graph:</p><div class="mediaobject"><img src="Images/B08394_02_38.jpg" alt="The best approach" width="651" height="329"/><div class="caption"><p>Figure 2.38: Code snippet for generating the graph for the actual and predicted prices</p></div></div><p>This graph <a id="id282" class="indexterm"/>shows that all the data records' predicted prices follow the actual price pattern. You can say that our MLP model works well to predict the stock market prices. You can find the code at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb">https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb</a>.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec35"/>Summary</h1></div></div></div><p>In this chapter, you learned how to predict stock prices. We covered the different machine learning algorithms that can help us in this. We tried Random Forest Regressor, Logistic Regression, and multilayer perceptron. We found out that the multilayer perceptron works really well. I really want to discuss something beyond what we have done so far. If you are under the impression that using the sentiment analysis of news and predictive methods, we can now correctly predict the stock market price with a hundred percent accuracy, then you would be wrong. We can't predict stock prices with a hundred percent accuracy. Many communities, financial organizations, and academic researchers are working in this direction in order to make a stock market price predictive model that is highly accurate. This is an active research area.</p><p>So if you are interested in research and freelancing, then you can join some pretty cool communities. There are two communities that are quite popular. One of these is quantopian (<a class="ulink" href="https://www.quantopian.com/">https://www.quantopian.com/</a>). In this community, you can submit your stock price prediction algorithm, and if it outperforms other competitors' algorithms, then you will win a cash price, and if you get the license for your algorithm, then you get some profit from transactions that will be done through your licensed algorithm. The second community is numer.ai (<a class="ulink" href="https://numer.ai/">https://numer.ai/</a>). This community is similar to quantopian. So, the possibilities of this application are limitless. Both communities offer some great tutorials. So try something different, and hopefully you will come up with a great algorithm.</p><p>In the next chapter, we will tap the retail or e-commerce domain and try to figure out some interesting facts about the user behavior dataset and users' social footprint. This will help us understand how the company should change their website or some functionality on the website. What are the chances of the email campaign going well and which type of users will respond to this campaign? Keep reading this book! We will discuss all these things in the next chapter.</p></div></div>



  </body></html>