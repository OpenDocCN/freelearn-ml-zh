<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Decomposing CO2 Trends Using Time Series Analysis</h1>
                </header>
            
            <article>
                
<p class="mce-root">If you are reading this book in the year 2055—assuming you're still using a year system based on the Common Era (a year is the time taken by the planet you're on to go around the sun once)—congratulations! You have survived. This book is written in the year 2018, and we as humans have much to worry about in terms of the survival of our species.</p>
<p class="mce-root">By and large, we have managed to work our way into a relatively stable peace, but the future of our species as a whole is somewhat at risk from various threats. Most of these threats have been caused by our own actions in the past. I'd like to emphasize a point here: I'm not assigning blame to anyone in the past for causing these threats. Our ancestors were busy optimizing to different goals, and the threats are typically an unforeseen/unforeseeable side-effect of the actions at that time.</p>
<p class="mce-root">A compounding factor is that humans are, biologically speaking, not very well suited to thinking about the future. Our brains simply do not see our future selves as a continuity of our current selves [0],[1]. As a result, we often think of things that may happen to us in the future as things that happen to someone else, or that the future is exaggerated. This has led to decisions made today without consideration to the effect in the future. This has led to many threats that arise from past actions of our species.</p>
<p class="mce-root">One of those threats is runaway climate change that could ruin our entire way of living, and potentially threaten the entire human species with extinction. It is very real and very unexaggerated. Human-induced climate change is a very wide topic with many niches. The primary gist of the major cause of human-induced climate change is the increased rates release of <strong>carbon dioxide</strong> (<strong>CO<sub><span>2</span></sub></strong>) into the air.</p>
<p class="mce-root">In this chapter, we will perform a time series analysis on CO<sub><span>2</span></sub> in the air. The main goal of this chapter is to serve as an introduction to time series analysis. On the technical end, you will learn the finer side of plotting using <strong>Gonum</strong>. Also, we'll learn how to deal with non-conventional data formats.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Exploratory data analysis</h1>
                </header>
            
            <article>
                
<p class="mce-root">The amount of CO<sub>2</sub> in the air can be measured. The <strong>National Oceanic and Atmospheric Administration</strong> (<strong>NOAA</strong>) department has been collecting data on the amount of CO<sub>2</sub> in the air since the early 1950s. The data we'll be using can be found at <a href="https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html">https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html</a>. We'll specifically be using that Mauna Loa monthly mean data.</p>
<p class="mce-root">The data, after removing the comments, looks something like this:</p>
<pre class="mce-root"># decimal average interpolated trend #days<br/># date (season corr)<br/>1958 3 1958.208 315.71 315.71 314.62 -1<br/>1958 4 1958.292 317.45 317.45 315.29 -1<br/>1958 5 1958.375 317.50 317.50 314.71 -1<br/>1958 6 1958.458 -99.99 317.10 314.85 -1<br/>1958 7 1958.542 315.86 315.86 314.98 -1<br/>1958 8 1958.625 314.93 314.93 315.94 -1<br/><br/></pre>
<p class="mce-root">In particular, we are interested in the <kbd>interpolated</kbd> column.</p>
<p class="mce-root">Because this is a particularly interesting dataset, it might be worth looking at how to download and preprocess the data directly in Go.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Downloading from non-HTTP sources</h1>
                </header>
            
            <article>
                
<p class="mce-root">We'll start by writing the function that will download the data, as follows:</p>
<pre class="mce-root">func download() io.Reader {<br/>  client, err := ftp.Dial("aftp.cmdl.noaa.gov:21")<br/>  dieIfErr(err)<br/>  dieIfErr(client.Login("anonymous", "anonymous"))<br/>  reader, err := client.Retr("products/trends/co2/co2_mm_mlo.txt")<br/>  dieIfErr(err)<br/>  return reader<br/>}</pre>
<p class="mce-root">The NOAA data sits on a publicly accessible FTP server: <a href="ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt">ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt</a>. If you visit the URI via a web browser, you would see the data immediately. To access the data programmatically is a little tricky, as this is not a typical HTTP URL.</p>
<p class="mce-root">To handle FTP connections, we will be using the <kbd>github.com/jlaffaye/ftp</kbd> package. The package can be installed using the standard <kbd>go get</kbd> method: <kbd>go get -u github.com/jlaffaye/ftp</kbd>. The documentation for the package is a little sparse and somewhat requires you to understand the FTP standards. But, fear not, using FTP to acquire the file is relatively simple.</p>
<p class="mce-root">First we need to dial in to the server (you would need to do the same if you were working with HTTP endpoints—<kbd>net/http</kbd> merely abstracts out the dialing in so you wouldn't necessarily see what's happening in the background). Because dialing in is a fairly low-level procedure, we would need to supply the ports as well. Just like the convention for HTTP is for the server to listen on port <kbd>80</kbd>, the convention for an FTP server is to listen to port <kbd>21</kbd>, so we'd have to connect to a server specifying that we want to connect on port <kbd>21</kbd>.</p>
<p class="mce-root">An additional oddity to those not used to working with FTP is that FTP requires a login to the server. For servers with anonymous read-only access, the convention is typically to use "anonymous" as the username and password.</p>
<p class="mce-root">After successfully logging in, we retrieve the requested resource (the file that we want) and download the file. The <kbd>fttp</kbd> library at <a href="https://github.com/jlaffaye/ftp"><span>github.com/jlaffaye/ftp</span></a> returns <kbd>io.Reader.</kbd> Think of it as a file that contains the data.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Handling non-standard data</h1>
                </header>
            
            <article>
                
<p class="mce-root">Parsing the data is a piece of cake with only the standard library:</p>
<pre class="mce-root">func parse(l loader) (dates []string, co2s []float64) {<br/>  s := bufio.NewScanner(l())<br/>  for s.Scan() {<br/>    row := s.Text()<br/>    if strings.HasPrefix(row, "#") {<br/>      continue<br/>    }<br/>    fields := strings.Fields(row)<br/>    dates = append(dates, fields[2])<br/>    co2, err := strconv.ParseFloat(fields[4], 64)<br/>    dieIfErr(err)<br/>    co2s = append(co2s, co2)<br/>  }<br/>  return<br/>}</pre>
<p class="mce-root">The parsing function takes a <kbd>loader</kbd>, which when called, returns a <kbd>io.Reader</kbd>. We then wrap the <kbd>io.Reader</kbd> in a <kbd>bufio.Scanner</kbd>. Recall that the format is not standard. There are some things that we want and some things we don't. The data however is in a fairly consistent format—we can use the standard library functions to filter the ones we want and the ones we don't.</p>
<p class="mce-root">The <kbd>s.Scan()</kbd> method scans <kbd>io.Reader</kbd> until it encounters a newline. We can retrieve the string using <kbd>s.Text()</kbd>. If the string starts with <kbd>#</kbd>, we skip the line.</p>
<p class="mce-root">Otherwise, we use <kbd>strings.Fields</kbd> to split the string into fields. The reason why we use <kbd>strings.Fields</kbd> instead of <kbd>strings.Split</kbd> is because the latter does not handle multiple spaces well.</p>
<p class="mce-root">Following the splitting of the row into fields, we parse things that are necessary:</p>
<pre class="mce-root">type loader func() io.Reader</pre>
<p class="mce-root">Why do we need a <kbd>loader</kbd> type?</p>
<p class="mce-root">The reason is simple: we want to be good citizens— we should not be repeatedly requesting data from the FTP server while we are developing the program. Rather, we would cache the file and work with that single file while in development mode. This way, we wouldn't have to download from the internet all the time.</p>
<p class="mce-root">The corresponding <kbd>loader</kbd> type that reads from the file looks something like this, and is rather self-explanatory:</p>
<pre class="mce-root">func readFromFile() io.Reader {<br/>  reader, err := os.Open("data.txt")<br/>  dieIfErr(err)<br/>  return reader<br/>}</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dealing with decimal dates</h1>
                </header>
            
            <article>
                
<p class="mce-root">One of the more interesting custom formats used in this data is dates. It's a format known as <strong>decimal dates</strong>. They look like as follows:</p>
<pre class="mce-root">2018.5</pre>
<p class="mce-root">What this means is that this date represents the halfway point of the year 2018. There are 365 days in 2018. The 50% mark would be 183 days into the year: July 3 2018.</p>
<p class="mce-root">We can translate this logic into the following code:</p>
<pre>// parseDecimalDate takes a string in format of a decimal date<br/>// "2018.05" and converts it into a date.<br/>//<br/>func parseDecimalDate(a string, loc *time.Location) (time.Time, error) {<br/>  split := strings.Split(a, ".")<br/>  if len(split) != 2 {<br/>    return time.Time{}, errors.Errorf("Unable to split %q into a year followed by a decimal", a)<br/>  }<br/>  year, err := strconv.Atoi(split[0])<br/>  if err != nil {<br/>    return time.Time{}, err<br/>  }<br/>  dec, err := strconv.ParseFloat("0."+split[1], 64) // bugs can happen if you forget to add "0."<br/>  if err != nil {<br/>    return time.Time{}, err<br/>  }<br/><br/>  // handle leap years<br/>  var days float64 = 365<br/>  if year%400 == 0 || year%4 == 0 &amp;&amp; year%100 != 0 {<br/>    days = 366<br/>  }<br/><br/>  start := time.Date(year, time.January, 1, 0, 0, 0, 0, loc)<br/>  daysIntoYear := int(dec * days)<br/>  retVal := start.AddDate(0, 0, daysIntoYear)<br/>  return retVal, nil<br/>}</pre>
<p class="mce-root">The first step is to split the string into the year and the decimal portion. The year is parsed as an <kbd>int</kbd> datatype, while the decimal part is parsed as a floating point number to ensure we can perform math. Here, it's important to note that a bug can happen if you're not careful about it: after splitting the string, <kbd>"0."</kbd> needs to be prepended to the string.</p>
<p class="mce-root">A cleaner alternative would be to parse the string as <kbd>float64</kbd>, and then use <kbd>math.Modf</kbd> to split the float into the integer component and the decimal component. </p>
<p class="mce-root">Either way, once we have the decimal component, we can use it to figure out how many days into the year it is. But first we'd have to figure out if the year is a leap year.</p>
<p class="mce-root">We can calculate the number of days into the years simply by multiplying the decimal number by the number of days in the year. Following from that, we simply add the number of dates, and return the date.</p>
<div class="mce-root packt_infobox">One thing to note is that we pass in a <kbd>*time.Location</kbd>—in this specific instance, we know that the observatory is in Hawaii, and therefore we set it to <kbd>"Pacific/Honolulu"</kbd>. Although in this case, we could set the location to any other location in the world, and it wouldn't change the results of the data. But this is unique to this project—in other time series data, time zones may be important as the data collection method may involve time data from different time zones.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Plotting</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now that we've finished with getting the file and parsing it, let's plot the data. Again, as in <a href="12c81095-6fcf-4da9-b554-6367d45b34f8.xhtml">Chapter 2</a>, <em>Linear Regression-House Price Prediction</em>,we will be using Gonum's excellent plotting library. This time around, we're going to be exploring more of it in detail. We'll learn the following:</p>
<ul>
<li class="mce-root">How to plot a time series</li>
<li>How a plot breaks down into its elements and how we can manipulate those elements to style a chart</li>
<li class="mce-root">How to create plotters for chart types that Gonum does not provide for</li>
</ul>
<p class="mce-root">We'll start by writing a function to plot a time series:</p>
<pre>func newTSPlot(xs []time.Time, ys []float64, seriesName string) *plot.Plot {<br/>  p, err := plot.New()<br/>  dieIfErr(err)<br/>  xys := make(plotter.XYs, len(ys))<br/>  for i := range ys {<br/>    xys[i].X = float64(xs[i].Unix())<br/>    xys[i].Y = ys[i]<br/>  }<br/>  l, err := plotter.NewLine(xys)<br/>  dieIfErr(err)<br/>  l.LineStyle.Color = color.RGBA{A: 255} // black<br/>  p.Add(l)<br/>  p.Legend.Add(seriesName, l)<br/>  p.Legend.TextStyle.Font = defaultFont<br/><br/>  // dieIfErr(plotutil.AddLines(p, seriesName, xys))<br/>  p.X.Tick.Marker = plot.TimeTicks{Format: "2006-01-01"}<br/>  p.Y.Label.TextStyle.Font = defaultFont<br/>  p.X.Label.TextStyle.Font = defaultFont<br/>  p.X.Tick.Label.Font = defaultFont<br/>  p.Y.Tick.Label.Font = defaultFont<br/>  p.Title.Font = defaultFont<br/>  p.Title.Font.Size = 16<br/><br/>  return p<br/>}</pre>
<p class="mce-root">Here, we use the already familiar <kbd>plotter.XYs</kbd> (which you would have been acquainted with in the first chapter). Instead of using <kbd>plotutil.AddLines</kbd> as we did the last time, we shall do it manually, which allows us to control the styling of the lines a bit better.</p>
<p class="mce-root">We simply create a new <kbd>*Line</kbd> object with <kbd>plotter.NewLine</kbd>. The <kbd>*Line</kbd> object is primarily <kbd>plot.Plotter</kbd>, which is any type that can draw itself onto a canvas. In the later part of this chapter, we shall explore how to create our own <kbd>plot.Plotter</kbd> interface and other associated types to draw a custom type.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Styling</h1>
                </header>
            
            <article>
                
<p class="mce-root">But, for now, having access to the <kbd>*Line</kbd> object allows us to play around with the styling a bit more. To set the right mood with the rather gloomy nature of this chapter, I have chosen a stark black line (in fact, I have grown rather fond of the stark black line charts and have started using them in my daily plots as well). A point to note is that I did this:</p>
<pre class="mce-root">l.LineStyle.Color = color.RGBA{A: 255}</pre>
<p class="mce-root"><kbd>l.LineStyle.Color</kbd> takes <kbd>color.Color</kbd>—<kbd>color.RGBA</kbd> is a struct found in the <kbd>color</kbd> library in the standard library. It's a struct that has four fields representing a color, such as <kbd>Red</kbd>, <kbd>Green</kbd>, <kbd>Blue</kbd>, and <kbd>Alpha</kbd>. Here I take advantage of Go's default values—0s. But having an <kbd>Alpha</kbd> value of <kbd>0</kbd> would mean that it's invisible. Hence, I only set the <kbd>A</kbd> field to <kbd>255</kbd>—the rest of the fields are defaulted to <kbd>0</kbd>, which gives it a stark black color.</p>
<p class="mce-root">After we set the line style, we add the line to the plot with <kbd>p.Add(l)</kbd>. Because we're not using <kbd>plotutil.AddLines</kbd>, which abstracts away some of the manual work, we may find that if we run the function there isn't a legend in the plot. A plot without legends is generally useless. So, we also need to add a legend by using <kbd>p.Legend.Add(seriesName, l)</kbd>.</p>
<p class="mce-root">Aside from color, width, and the like, I also want to set a more brutal feel to the plots I make for this chapter—after all, this chapter is rather doom and gloom. I feel that the default font, which is Times New Roman is a little too humanist. So, we'd need to change fonts. Luckily, the extended Go standard library comes with a font-processing library. While usually I'd choose to go with slab serif style fonts for the brutal look, Go itself comes with a font that works well—the Go family of fonts.</p>
<p class="mce-root">How do we change fonts in a <kbd>*plot.Plot</kbd>? Most components of <kbd>*plot.Plot</kbd> take a <kbd>draw.TextStyle</kbd>, which is a data structure that configures the styling of text, including fonts. So, we can set those fields to indicate we want to use the font we chose.</p>
<p class="mce-root">As I mentioned, in the extended standard library, Go comes with fonts and font-processing utilities. We'll be using it here. First, we'd have to install the packages: <kbd>go get -u golang.org/x/image/font/gofont/gomono</kbd> and <kbd>go get -u github.com/golang/freetype/truetype</kbd>. The former is the official <strong><span>Monospace Type</span></strong> of the Go family of typefaces. The latter is a library to handle TrueType fonts.</p>
<p class="mce-root">Here, a caveat must be mentioned—while <kbd>draw.TextStyle</kbd> does allow for the configuration of fonts, the fonts are in a <kbd>vg.Font</kbd> type, which wraps a <kbd>*truetype.Font</kbd> type. If we use <kbd>truetype.Parse(gomono.TTF)</kbd>, we will get <kbd>*truetype.Font</kbd>. The <kbd>vg</kbd> package provides a function to make those fonts—<kbd>vg.MakeFont</kbd>. The reason why this is necessary instead of just using <kbd>*truetype.Font</kbd> is because <kbd>vg</kbd> has plenty of backends—some that could render fonts would require information about the font size.</p>
<p class="mce-root">So, to avoid having many calls to parse the font and making a <kbd>vg.Font</kbd> type, we can safely put it in a global variable, given we've already decided ahead that all fonts will be of the same brutal style:</p>
<pre>var defaultFont vg.Font<br/><br/>func init() {<br/>  font, err := truetype.Parse(gomono.TTF)<br/>  if err != nil {<br/>    panic(err)<br/>  }<br/>  vg.AddFont("gomono", font)<br/>  defaultFont, err = vg.MakeFont("gomono", 12)<br/>  if err != nil {<br/>    panic(err)<br/>  }<br/>}</pre>
<p class="mce-root">Once that's done, we can set all <kbd>draw.TextStyle.Font</kbd> to be <kbd>defaultFont</kbd>. Setting a default font size of 12 does not, however, mean that you're stuck with the size for everything. Because <kbd>vg.Font</kbd> is a struct, not a pointer to a struct, once set in an object, you are free to change the font size of that particular field, as I have shown in the following two lines:</p>
<pre class="mce-root">  p.Title.Font = defaultFont<br/>  p.Title.Font.Size = 16</pre>
<p class="mce-root">With our <kbd>main</kbd> function we can execute the following code:</p>
<pre class="mce-root">func main() {<br/>  dateStrings, co2s := parse(readFromFile)<br/>  dates := parseDates(dateStrings)<br/>  plt := newTSPlot(dates, co2s, "CO2 Level")<br/>  plt.X.Label.Text = "Time"<br/>  plt.Y.Label.Text = "CO2 in the atmosphere (ppm)"<br/>  plt.Title.Text = "CO2 in the atmosphere (ppm) over time\nTaken over the Mauna-Loa observatory"<br/>  dieIfErr(plt.Save(25*vg.Centimeter, 25*vg.Centimeter, "Moana-Loa.png"))<br/>}</pre>
<p class="mce-root">The result is stark , as shown in the following screenshot:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="Images/18f0f7aa-e074-4ee2-a7d5-df9335a9c668.png" style="width:45.83em;height:45.83em;" width="945" height="945"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Decomposition</h1>
                </header>
            
            <article>
                
<p class="mce-root">There are two things to note about the previous screenshot:</p>
<ul>
<li class="mce-root">CO<sub>2</sub> levels in the air are steadily rising over time.</li>
<li class="mce-root">There are dips and then bumps in the levels of CO<sub>2</sub>, but the result still ends up rising overall. These dips and bumps happen on a regular pattern.</li>
</ul>
<p class="mce-root">The first point is what is known to statisticians as a <strong>trend</strong>. You may already be familiar with the notion of a Trend Line from Microsoft Excel. A trend is a kind of pattern that describes gradual change over time. In our case, it is quite clear that the trend is upward.</p>
<p class="mce-root">The second point is called <strong>seasonality</strong>—for very apt reasons, as it may turn out. Seasonality describes the pattern of variance that happens regularly. If you carefully look at the chart, typically at around August to October of each year, the CO<sub>2</sub> levels drop to the lowest point of the year. After which, they rise steadily again until around May, where they peak. Here's a good hint as to why this happens: plants suck CO<sub>2</sub> from the air through a process called <strong>photosynthesis</strong>. Photosynthesis requires a organelle in a plant's cell called a <strong>chloroplast</strong>, which contains a green pigment called <strong>chlorophyll</strong>. If you live in the Northern Hemisphere, you would be well aware that trees are greenest from Spring till Autumn. This largely coincides with the period from May till October. The changing of seasons cause a change in atmospheric carbon dioxide levels. You can certainly see why the term "seasonality" is quite apt.</p>
<p class="mce-root">A good question to ask  might be this: Can we separate the trend out from the seasonality so that we may be able to work on each component individually? The answer is yes, we can. In fact, in the remaining parts of this section, I'll show how to do so.</p>
<p class="mce-root">Now, as to why you would want to do that, well, in our project so far, we've seen seasonalities that are affected by real-life calendar seasons. Imagine you were doing statistical analysis for a toy company in a Western country. You'd see a yearly spike around Christmas time. Often seasonality adds noise to our analysis—it's hard to tell whether a bump in sales was due to Christmas time or an actual increase in sales. Furthermore, there are some cycles that don't necessarily follow the calendar year. If you are dealing with sales in a largely Chinese/Vietnamese community, you'd see spikes in sales before Chinese New Year/Tet. Those do not follow our calendar year. Ditto, if you were in the dates industry—you'd see spikes around Ramadan as demand for dates increases sharply during the Muslim fasting period.</p>
<p class="mce-root">While it's true that most time series would have some kind of trend and seasonality component, it would be remiss for me to mention that not all trends and seasonalities are particularly useful. You might be tempted to take what you learn in this chapter and apply it on the stock markets but buyer beware! Analyzing complex market places is quite different from analyzing trends of CO<sub>2</sub> in the air or sales from a business. The fundamental properties of time series in markets are somewhat different—it's a process that has the Markov property, which is best described as <strong>past performance does not indicate future performance</strong>. By contrast, we shall see, for this project, that the past is quite well correlated with the present and the future.</p>
<p class="mce-root">But back to the topic at hand—decomposition. If you read the comments on the data file (the lines we skipped from importing), the following is mentioned:</p>
<div class="mce-root packt_quote">"First, we compute for each month the average seasonal cycle in a 7-year window around each monthly value. In this way, the seasonal cycle is allowed to change slowly over time. We then determine the "trend" value for each month by removing the seasonal cycle; this result is shown in the "trend" column."</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">STL</h1>
                </header>
            
            <article>
                
<p class="mce-root">But how does one calculate a seasonal cycle? In this section, we'll be using an algorithm invented in the late 1980s called <strong>Seasonal and Trend Decomposition</strong> (<strong>STL</strong>) by LOESS by Cleveland et al. I wrote a library that implements that. You can install it by running <kbd>go get -u github.com/chewxy/stl</kbd>.</p>
<p class="mce-root">The library is really small—there is only one <kbd>main</kbd> function to call (<kbd>stl.Dcompose</kbd>), and the library comes with a litany of features to aid with decomposition of data.</p>
<p class="mce-root">Despite that, I think it would be a good idea to have a rough understanding of the STL algorithm before using it, as usage requires knowledge.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">LOESS</h1>
                </header>
            
            <article>
                
<p class="mce-root">The thing that powers STL is the notion of local regression—LOESS itself is a terrible acronym formed from <strong>LO</strong>cal regr<strong>ESS</strong>ion—whatever drugs the statisticians were on in the 1990s, sign me up for them. We're already familiar with the idea of linear regression from <a href="3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml">Chapter 1</a>, <em>How to Solve All Machine Learning</em>. </p>
<p class="mce-root">Recall that the role of linear regression is that given a straight line function: <img class="fm-editor-equation" src="Images/f064c2e5-f70b-4a80-a051-c6cee555629e.png" style="width:6.83em;height:1.33em;" width="920" height="180"/>. We want to estimate <img class="fm-editor-equation" src="Images/44aac8a5-8a88-4adf-8423-9f978c3c58b9.png" style="width:1.33em;height:0.92em;" width="160" height="110"/> and <img class="fm-editor-equation" src="Images/7a3e53ed-c83b-448e-8ec3-50662b599ee7.png" style="width:0.67em;height:0.92em;" width="80" height="110"/>. Instead of trying to fit the whole dataset at once, what if we broke the dataset up into many small <strong>local</strong> components, and ran a regression on each small dataset? Here's an example of what I mean:</p>
<pre class="mce-root">| X | Y |<br/> |:--:|:--|<br/> | -1 | 1 |<br/> | -0.9 | 0.81 |<br/> | -0.8 | 0.64 |<br/> | -0.7 | 0.49 |<br/> | -0.6 | 0.36 |<br/> | -0.5 | 0.25 |<br/> | -0.4 | 0.16 |<br/> | -0.3 | 0.09 |<br/> | -0.2 | 0.04 |<br/> | -0.1 | 0.01 |<br/> | 0 | 0 |<br/> | 0.1 | 0.01 |<br/> | 0.2 | 0.04 |<br/> | 0.3 | 0.09 |<br/> | 0.4 | 0.16 |<br/> | 0.5 | 0.25 |<br/> | 0.6 | 0.36 |<br/> | 0.7 | 0.49 |<br/> | 0.8 | 0.64 |<br/> | 0.9 | 0.81 |</pre>
<p class="mce-root">The preceding table is a function representing <img class="fm-editor-equation" src="Images/dd55f36b-8373-4801-819c-016180524f17.png" style="width:4.42em;height:2.00em;" width="530" height="240"/>. Instead of pulling in the entire dataset for a regression, what if we did a running regression of every three rows? We'd start with row 2 (x = -0.9). And the data points under consideration are <kbd>1</kbd> before it and <kbd>1</kbd> after it (<em>x = -1</em> and <em>x = -0.8</em>). And for row 3, we'd do a linear regression using row <kbd>2</kbd>, <kbd>3</kbd>, <kbd>4</kbd> as data points. At this point, we're not particularly interested in the errors of the local regression. We just want an estimate of the gradient and the crossings. Here's the resulting table:</p>
<pre class="mce-root">| X | Y | m | c<br/> |:--:|:--:|:--:|:--:|<br/> | -0.9 | 0.81 | -1.8 | -0.803333333333333 |<br/> | -0.8 | 0.64 | -1.6 | -0.633333333333334 |<br/> | -0.7 | 0.49 | -1.4 | -0.483333333333334 |<br/> | -0.6 | 0.36 | -1.2 | -0.353333333333333 |<br/> | -0.5 | 0.25 | -1 | -0.243333333333333 |<br/> | -0.4 | 0.16 | -0.8 | -0.153333333333333 |<br/> | -0.3 | 0.09 | -0.6 | -0.083333333333333 |<br/> | -0.2 | 0.04 | -0.4 | -0.033333333333333 |<br/> | -0.1 | 0.01 | -0.2 | -0.003333333333333 |<br/> | 0 | 0 | -2.71050543121376E-17 | 0.006666666666667 |<br/> | 0.1 | 0.01 | 0.2 | -0.003333333333333 |<br/> | 0.2 | 0.04 | 0.4 | -0.033333333333333 |<br/> | 0.3 | 0.09 | 0.6 | -0.083333333333333 |<br/> | 0.4 | 0.16 | 0.8 | -0.153333333333333 |<br/> | 0.5 | 0.25 | 1 | -0.243333333333333 |<br/> | 0.6 | 0.36 | 1.2 | -0.353333333333333 |<br/> | 0.7 | 0.49 | 1.4 | -0.483333333333334 |<br/> | 0.8 | 0.64 | 1.6 | -0.633333333333333 |<br/> | 0.9 | 0.81 | 1.8 | -0.803333333333333 |</pre>
<p class="mce-root">In fact, we can show that if you plot each line individually, you will have a somewhat "curved" shape. So, here's a side program I wrote to plot this out:</p>
<pre>// +build sidenote<br/><br/>package main<br/><br/>import (<br/>  "image/color"<br/><br/>  "github.com/golang/freetype/truetype"<br/>  "golang.org/x/image/font/gofont/gomono"<br/>  "gonum.org/v1/plot"<br/>  "gonum.org/v1/plot/plotter"<br/>  "gonum.org/v1/plot/vg"<br/>  "gonum.org/v1/plot/vg/draw"<br/>)<br/><br/>var defaultFont vg.Font<br/><br/>func init() {<br/>  font, err := truetype.Parse(gomono.TTF)<br/>  if err != nil {<br/>    panic(err)<br/>  }<br/>  vg.AddFont("gomono", font)<br/>  defaultFont, err = vg.MakeFont("gomono", 12)<br/>  if err != nil {<br/>    panic(err)<br/>  }<br/>}<br/><br/>var table = []struct {<br/>  x, m, c float64<br/>}{<br/>  {-0.9, -1.8, -0.803333333333333},<br/>  {-0.8, -1.6, -0.633333333333334},<br/>  {-0.7, -1.4, -0.483333333333334},<br/>  {-0.6, -1.2, -0.353333333333333},<br/>  {-0.5, -1, -0.243333333333333},<br/>  {-0.4, -0.8, -0.153333333333333},<br/>  {-0.3, -0.6, -0.083333333333333},<br/>  {-0.2, -0.4, -0.033333333333333},<br/>  {-0.1, -0.2, -0.003333333333333},<br/>  {0, -2.71050543121376E-17, 0.006666666666667},<br/>  {0.1, 0.2, -0.003333333333333},<br/>  {0.2, 0.4, -0.033333333333333},<br/>  {0.3, 0.6, -0.083333333333333},<br/>  {0.4, 0.8, -0.153333333333333},<br/>  {0.5, 1, -0.243333333333333},<br/>  {0.6, 1.2, -0.353333333333333},<br/>  {0.7, 1.4, -0.483333333333334},<br/>  {0.8, 1.6, -0.633333333333333},<br/>  {0.9, 1.8, -0.803333333333333},<br/>}<br/><br/>type estimates []struct{ x, m, c float64 }<br/><br/>func (es estimates) Plot(c draw.Canvas, p *plot.Plot) {<br/>  trX, trY := p.Transforms(&amp;c)<br/>  lineStyle := plotter.DefaultLineStyle<br/>  lineStyle.Dashes = []vg.Length{vg.Points(2), vg.Points(2)}<br/>  lineStyle.Color = color.RGBA{A: 255}<br/>  for i, e := range es {<br/>    if i == 0 || i == len(es)-1 {<br/>      continue<br/>    }<br/>    strokeStartX := es[i-1].x<br/>    strokeStartY := e.m*strokeStartX + e.c<br/>    strokeEndX := es[i+1].x<br/>    strokeEndY := e.m*strokeEndX + e.c<br/>    x1 := trX(strokeStartX)<br/>    y1 := trY(strokeStartY)<br/>    x2 := trX(strokeEndX)<br/>    y2 := trY(strokeEndY)<br/>    x := trX(e.x)<br/>    y := trY(e.x*e.m + e.c)<br/><br/>    c.DrawGlyph(plotter.DefaultGlyphStyle, vg.Point{X: x, Y: y})<br/>    c.StrokeLine2(lineStyle, x1, y1, x2, y2)<br/>  }<br/>}<br/><br/>func main() {<br/>  p, err := plot.New()<br/>  if err != nil {<br/>    panic(err)<br/>  }<br/>  p.Title.Text = "X^2 Function and Its Estimates"<br/>  p.X.Label.Text = "X"<br/>  p.Y.Label.Text = "Y"<br/>  p.X.Min = -1.1<br/>  p.X.Max = 1.1<br/>  p.Y.Min = -0.1<br/>  p.Y.Max = 1.1<br/>  p.Y.Label.TextStyle.Font = defaultFont<br/>  p.X.Label.TextStyle.Font = defaultFont<br/>  p.X.Tick.Label.Font = defaultFont<br/>  p.Y.Tick.Label.Font = defaultFont<br/>  p.Title.Font = defaultFont<br/>  p.Title.Font.Size = 16</pre>
<p>Now, we will see how to plot the original function:</p>
<pre>  // Original function<br/>  original := plotter.NewFunction(func(x float64) float64 { return x * x })<br/>  original.Color = color.RGBA{A: 16}<br/>  original.Width = 10<br/>  p.Add(original)<br/><br/>  // Plot estimates<br/>  est := estimates(table)<br/>  p.Add(est)<br/><br/>  if err := p.Save(25*vg.Centimeter, 25*vg.Centimeter, "functions.png"); err != nil {<br/>    panic(err)<br/>  }<br/>}</pre>
<p class="mce-root">The preceding code yields a chart, as shown in the following screenshot:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="Images/c8f5286c-098a-47da-adde-0df30cd3c54b.png" style="width:50.75em;height:50.75em;" width="945" height="945"/></div>
<p class="mce-root">Most of the code will be explained in the latter parts of this chapter, but, for now, let's focus on the fact that you can indeed run many small linear regressions on "local" subsets of the data to plot a curve.</p>
<p class="mce-root">LOESS brings this idea further, by stating that if you have a window of values (in the toy example, we used <kbd>3</kbd>), then the values should be weighted. The logic is simple: the closer a value is to the row in consideration, the higher the weight. If we had used a window size of <kbd>5</kbd>, then when considering row <kbd>3</kbd>, <kbd>2</kbd>, and <kbd>4</kbd> would be weighted more heavily than rows <kbd>1</kbd> and <kbd>5</kbd>. This <strong>width</strong>, it turns out, is important to our smoothing.</p>
<p class="mce-root">The subpackage, <kbd>"github.com/chewxy/stl/loess"</kbd>, implements LOESS as a smoothing algorithm. Do read through the code if you're interested in knowing more about the details.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The algorithm</h1>
                </header>
            
            <article>
                
<p class="mce-root">Recall that our goal is to split a time series into seasonality and trend. Obviously, once we've removed the seasonality and trend, there will be some remaining parts. We call those <strong>residuals</strong>. So, how do we do it?</p>
<p class="mce-root">The algorithm has a lot of fine tuning for the sake of robustness. I will elide on explaining on the various robustness optimizations performed, but I think it is important to have a rough idea of how the algorithm works in general.</p>
<p class="mce-root">The following is a rough overview of the algorithm:</p>
<ol>
<li class="mce-root">Calculate trend (on the first loop, the trend is all 0s).</li>
<li class="mce-root">Subtract the trend from the input data. This is called <strong>detrending</strong>.</li>
<li class="mce-root">Cycle subseries smoothing: the data is partitioned into <kbd>N</kbd> subcycles. Each subcycle corresponds to a period. The data is then smoothed using LOESS. The result is a temporary seasonal dataset.</li>
<li class="mce-root">For each temporary seasonal dataset (one per period), we perform a low pass filter—we keep the values with a low frequency.</li>
<li class="mce-root">The low pass filtered values are subtracted from temporary seasonal dataset. This is the seasonal data.</li>
<li class="mce-root">Subtract the seasonal data from the input data. This is the new trend data.</li>
<li class="mce-root">Iterate step 1 to step 6 until the number of iterations is desired. This is typically 1 or 2.</li>
</ol>
<p class="mce-root">As you can see, the algorithm is iterative—each iteration improves on the trend, which is then used to find the new seasonal data, which is then used to update the trend, and so on and so forth. But there is a very important blink-and-you-miss-it "magic" that STL relies on.</p>
<p class="mce-root">And so we come to the second important reason to understand the algorithm: <strong>STL is dependent upon the definition of how many periods the dataset has</strong>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using STL</h1>
                </header>
            
            <article>
                
<p class="mce-root">To recap, there are two important parts that are fundamental to the STL algorithm:</p>
<ul>
<li class="mce-root">The <strong>width</strong> used for smoothing</li>
<li class="mce-root">The <strong>periods</strong> in the dataset</li>
</ul>
<p class="mce-root">When we look at the CO<sub>2</sub> dataset, we can count the periods by counting the number of peaks in the chart. I counted 60 peaks. This corresponds to the fact that the observatory has been collecting data for the past 60 years.</p>
<p class="mce-root">From here, we move from the hard sciences of statistics into the softer realms of interpretation. This is often true in data science and machine learning—we often have to use our intuition to guide us.</p>
<p class="mce-root">In this case, we have a hard starting point: there has been 60 years so we expect at least 60 periods. Another starting point can be found in the notes of the dataset itself: the NOAA uses a seven-year window to calculate the seasonal component. I don't see any reason to not use those values. So, let's decompose our time series into the <strong>trend</strong>, <strong>seasonal</strong>, and <strong>residual</strong> components.</p>
<p class="mce-root">But before we begin, there is an additional note to make: we want to decompose the time series into three components, but how do these three components recompose to become whole again? In general, there are two methods: additive or multiplicative. Simply put, we can decompose the data as either one of the following equations:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/401f677e-a121-48ed-89e2-b6fb91b10c77.png" style="width:21.50em;height:1.17em;" width="3120" height="170"/></p>
<p class="mce-root">This can also be stated as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4f5383b7-4228-4077-87d3-c6448772a58a.png" style="width:21.00em;height:1.08em;" width="3120" height="160"/></p>
<p class="mce-root">The <kbd>github.com/chewxy/stl</kbd> package supports both models, and even supports custom models that fall "in-between" additive and multiplicative models.</p>
<div class="mce-root packt_infobox"><strong><br/>
When to use an additive model</strong>: Use an additive model when the seasonality does not vary with the level of the time series. Most standard business case time series fall in this category.<br/>
<br/>
<span><strong>When to use a multiplicative model</strong>: Use a multiplicative model when the seasonality or trend does vary with the level of the time series. Most econometric models fall in this category.</span></div>
<p class="mce-root">For the purpose of this project, we will be using an additive model. Here's the <kbd>main</kbd> function again:</p>
<pre>func main() {<br/>  dateStrings, co2s := parse(readFromFile)<br/>  dates := parseDates(dateStrings)<br/>  plt := newTSPlot(dates, co2s, "CO2 Level")<br/>  plt.X.Label.Text = "Time"<br/>  plt.Y.Label.Text = "CO2 in the atmosphere (ppm)"<br/>  plt.Title.Text = "CO2 in the atmosphere (ppm) over time\nTaken over the Mauna-Loa observatory"<br/>  dieIfErr(plt.Save(25*vg.Centimeter, 25*vg.Centimeter, "Moana-Loa.png"))<br/><br/>  decomposed := stl.Decompose(co2s, 12, 84, stl.Additive(), stl.WithIter(1))<br/>  dieIfErr(decomposed.Err)<br/>  plts := plotDecomposed(dates, decomposed)<br/>  writeToPng(plts, "decomposed.png", 25, 25)<br/>}</pre>
<p class="mce-root">Let's break this down; in particular, the parameters:</p>
<pre class="mce-root">decomposed := stl.Decompose(co2s, 12, 84, stl.Additive(), stl.WithIter(1))</pre>
<p>Take a look at the following terms from the preceding code:</p>
<ul>
<li class="mce-root"><kbd>12</kbd>: We counted 60 periods. The data is monthly data; therefore, it would make sense that a period takes 12 months, or as we know it—a year.</li>
<li class="mce-root"> <kbd>84</kbd>: We use the smoothing window as specified by the NOAA. Seven years is 84 months.</li>
<li class="mce-root"> <kbd>stl.Additive()</kbd>: We want to use an additive model.</li>
<li class="mce-root"><kbd>stl.WithIter(1)</kbd>: STL is fairly sensitive to the number of iterations run. The default is <kbd>2</kbd>. But if you run it too many times, everything gets iteratively "smoothed" out. So, instead, we stick with <kbd>1</kbd>.</li>
</ul>
<p>In the following sections, I'll show examples of misuse and why despite everything, 1 and 2 are still pretty good iteration counts.</p>
<p class="mce-root">You may note that instead of specifying the number of periods, we specified the length of a period. The package expects the data to be evenly spaced—the distance between any two rows should be the same.</p>
<p class="mce-root">Running this yields the following plot:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-416 image-border" src="Images/2d3062fd-419f-4ef8-b71b-3bf621deb3c4.png" style="width:52.58em;height:51.92em;" width="967" height="955"/></div>
<p class="mce-root">The first chart is the original data, followed by the extracted trend and seasonality, and, finally, the residuals. There remains some weirdness with regards to the beginning of the graph, but that artifact is solely due to the fact that the <kbd>github.com/chewxy/stl</kbd> library does not "backcast". Hence, it's always a good idea to start with at least one extra period.</p>
<p class="mce-root">How to interpret the plot? Well, since this is an additive model, interpretation is a lot simpler—the <kbd>Y</kbd> values indicate the ppm of carbon dioxide in the air that each component contributes to the actual data, so the first chart is literally the result of adding the bottom charts together.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to lie with statistics</h1>
                </header>
            
            <article>
                
<p class="mce-root">It is important to note that these parameters essentially control how much to attribute the CO<sub>2</sub> in the atmosphere to each component. And these controls are rather subjective. The <kbd>stl</kbd> package offers a lot of control over how a time series is decomposed, and I think it's up to the data scientist or statistician reading this book (that is you), to do statistics responsibly.</p>
<p class="mce-root">What if we said that a period was five years? Keeping everything the same, we can use the following code and find out:</p>
<pre class="mce-root">lies := stl.Decompose(co2s, 60, 84, stl.Additive(), stl.WithIter(1))<br/>dieIfErr(lies.Err)<br/>plts2 := plotDecomposed(dates, lies)<br/>writeToPng(plts2, "CO2 in the atmosphere (ppm), decomposed (Liar Edition)", "lies.png", 25, 25)</pre>
<p class="mce-root">The following chart is produced:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-417 image-border" src="Images/5158c053-0d20-4eba-b007-5635417914cb.png" style="width:47.50em;height:47.42em;" width="961" height="958"/></p>
<p class="mce-root">We could then take this chart and parade the top two sections and say "Look! Statistics tells us that despite the data looking like it's going up, it's in fact trending down. Hashtag science."</p>
<p class="mce-root">You're of course free to do so. But I know you're not a dishonest person. Instead, I hope that you are reading this book with good intentions of saving the world.</p>
<p class="mce-root">But knowing the correct parameters to use is difficult. One suggestion I have is to go to extremes and then come back down. This is what I mean—we have a rough idea of how the STL algorithm works. A known controlling factor is the iteration count, which defaults to 2. Here's the original correct version, with 1, 2, 5, 10, 20, and 100 iterations:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-418 image-border" src="Images/28ab26b1-2529-420e-9616-f6bff601de06.png" style="width:42.50em;height:42.08em;" width="977" height="967"/></p>
<p>Interations:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-419 image-border" src="Images/196427d7-6a3c-4edd-ad52-c7a2480c6d19.png" style="width:62.17em;height:61.42em;" width="974" height="962"/><img class="alignnone size-full wp-image-420 image-border" src="Images/3d4a4d29-ea3e-49db-b06d-037c6b16b5f3.png" style="width:58.50em;height:57.92em;" width="973" height="964"/><img class="alignnone size-full wp-image-421 image-border" src="Images/119460c2-8b34-4580-86e1-7140ad61bf0a.png" style="width:46.83em;height:46.25em;" width="972" height="960"/><img class="alignnone size-full wp-image-422 image-border" src="Images/f7f12afb-c01f-4777-b99d-2213193305b1.png" style="width:80.08em;height:80.33em;" width="961" height="964"/><img class="alignnone size-full wp-image-423 image-border" src="Images/c52bcaa6-f1f1-42f8-913a-b3c5b8529520.png" style="width:80.83em;height:80.17em;" width="970" height="962"/></p>
<p class="mce-root">Over the iterations, having been smoothed iteratively, the seasonality loses its jaggedness. Nonetheless, the shape of the trend stays the same. Therefore, in this case, increasing the iteration counts merely shifts the seasonal contribution to the trend component. This implies that the trend component is the stronger "signal" of sorts.</p>
<p class="mce-root">By contrast, if we run the "lies" version, we see that at two iterations, the shape of the trend changes, and by the 10th iteration onward, the shape of the trend stays the same. This gives us a clue as to what the "real" trend is.</p>
<p class="mce-root">With STL, the thing that we're really controlling is the seasonality. What we're saying to the algorithm is that we believe that a period is 12 months; therefore, please find a seasonality that fits. If we say to the algorithm that we believe that a period is five years (60 months), the algorithm will try its best to find a seasonality and trend that fits that pattern.</p>
<p class="mce-root">I wish to be clear—the notion of a seasonality that happens every five years is <strong>not wrong</strong>. In fact, it is common for business-related forecasting to work on multiple levels of seasonalities. But knowing how many iterations to run, that comes with experience and wisdom.</p>
<div class="mce-root packt_tip">Check the units! If the units don't make sense, like in the "lies" chart, then it probably isn't real.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">More plotting</h1>
                </header>
            
            <article>
                
<p class="mce-root">A major theme in this chapter other than time series analysis is plotting. You may have also noticed a few new functions in the <kbd>main</kbd> function earlier. Now it's time to revisit them.</p>
<p class="mce-root">We start with the output of <kbd>stl.Decompose</kbd>. This is the definition:</p>
<pre>type Result struct {<br/>  Data []float64<br/>  Trend []float64<br/>  Seasonal []float64<br/>  Resid []float64<br/>  Err error<br/>}</pre>
<div class="mce-root packt_infobox">There is no notion of time in the result. It's assumed that when you pass in data into <kbd>stl.Decompose</kbd>, the data is ordered by the time series. The result also follows this notion.</div>
<p class="mce-root">We've already defined <kbd>newTSPlot</kbd> previously, which works fine for the data, trend, and seasonal, but not the residuals. The reason why we don't want to plot residuals as a line chart is because if done right, the residuals should be more or less random. Having a line plot run through random points would be rather messy.</p>
<p class="mce-root">Typical residual plots are simply scatter plots of the residuals. However, that too is relatively uninterpretable when squashed into a multiplot image.</p>
<p class="mce-root">Instead, we want to draw a straight vertical line for each residual value.</p>
<p class="mce-root">To recap, this is what we want to do:</p>
<ol>
<li class="mce-root">Plot a time series chart for each of <kbd>Data</kbd>, <kbd>Trend</kbd>, and <kbd>Seasonal</kbd>.</li>
<li class="mce-root">Plot a residuals chart for <kbd>Resid</kbd>.</li>
<li class="mce-root">Combine all the preceding plots into one image.</li>
</ol>
<p class="mce-root">Step 1 is easy, as we simply call <kbd>newTSPlot</kbd> with the parsed dates from earlier for each of the components. Step 2 is a little trickier. Gonum doesn't have the residuals plots that we want by default.</p>
<p class="mce-root">To plot it, we'd need to create a new <kbd>plot.Plotter</kbd> interface. Here's the definition:</p>
<pre>type residChart struct {<br/>  plotter.XYs<br/>  draw.LineStyle<br/>}<br/><br/>func (r *residChart) Plot(c draw.Canvas, p *plot.Plot) {<br/>  xmin, xmax, ymin, ymax := r.DataRange()<br/>  p.Y.Min = ymin<br/>  p.Y.Max = ymax<br/>  p.X.Min = xmin<br/>  p.X.Max = xmax<br/><br/>  trX, trY := p.Transforms(&amp;c)<br/>  zero := trY(0)<br/>  lineStyle := r.LineStyle<br/>  for _, xy := range r.XYs {<br/>    x := trX(xy.X)<br/>    y := trY(xy.Y)<br/>    c.StrokeLine2(lineStyle, x, zero, x, y)<br/>  }<br/>}<br/><br/>func (r *residChart) DataRange() (xmin, xmax, ymin, ymax float64) {<br/>  xmin = math.Inf(1)<br/>  xmax = math.Inf(-1)<br/>  ymin = math.Inf(1)<br/>  ymax = math.Inf(-1)<br/>  for _, xy := range r.XYs {<br/>    xmin = math.Min(xmin, xy.X)<br/>    xmax = math.Max(xmax, xy.X)<br/>    ymin = math.Min(ymin, xy.Y)<br/>    ymax = math.Max(ymax, xy.Y)<br/>  }<br/>  return<br/>}</pre>
<p>Despite the fact that Gonum doesn't have the chart type that we want, as you can see it doesn't take very many lines of code for us to define our own chart type. This is part of the power of Gonum's <kbd>plot</kbd> library—it's abstract enough to enable you to write your own chart type, and at the same time, it provides all the helper functions necessary to make it work without much code.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">A primer on Gonum plots</h1>
                </header>
            
            <article>
                
<p class="mce-root">Before we go further, I think it might be worth it to have an understanding of Gonum's plotting library in general. We've so far been using Gonum's <kbd>plot</kbd> library in rather ad hoc ways. This was to familiarize you with how to use the library. Now that you're somewhat familiar, it's time to learn more about the internals in order to plot better in the future.</p>
<p class="mce-root">A <kbd>*plot.Plot</kbd> object holds the metadata of a plot. A plot consists of the following features:</p>
<ul>
<li class="mce-root">A title</li>
<li class="mce-root"><kbd>X</kbd> and <kbd>Y</kbd> axes</li>
<li class="mce-root">A legend</li>
<li class="mce-root">A list of <kbd>plot.Plotter</kbd></li>
</ul>
<p class="mce-root">A <kbd>plot.Plotter</kbd> interface is simply anything that can take a <kbd>*plot.Plot</kbd> object and draw it on to <kbd>draw.Canvas</kbd>, defined as follows:</p>
<pre class="mce-root">type Plotter interface {<br/><span> </span>Plot(draw.Canvas, *Plot)<br/>}</pre>
<p class="mce-root">By separating the notions of a <kbd>plot</kbd> object and the canvas upon which the plot will be drawn, this opens Gonum's plots to a variety of different plotting backend options. To see what I mean about backend options, we need to take a closer look at <kbd>draw.Canvas</kbd>.</p>
<p class="mce-root">The <kbd>draw.Canvas</kbd> is a tuple of <kbd>vg.Canvas</kbd> and <kbd>vg.Rectangle</kbd>. So what exactly is <kbd>vg</kbd>? <kbd>vg</kbd>, it turns out, stands for <strong>vector graphics</strong>. In it, the <kbd>Canvas</kbd> type is defined as an interface with a bunch of methods. This allows for the rich variety of backends that <kbd>vg</kbd> has:</p>
<ul>
<li class="mce-root"><kbd>vg/vgimg</kbd>: This is the primary package we've been using so far; it writes to an image file.</li>
<li class="mce-root"><kbd>vg/vgpdf</kbd>: <span>This package </span>writes to a PDF file.</li>
<li class="mce-root"><kbd>vg/vgsvg</kbd>: <span>This package </span>writes to a SVG file.</li>
<li class="mce-root"><kbd>vg/vgeps</kbd>: <span>This package </span>writes to a EPS file.</li>
<li class="mce-root"><kbd>vg/vgtex</kbd>: <span>This package </span>writes to a TEX file.</li>
</ul>
<p class="mce-root">Each of these canvas implementations has a coordinate system that begins with (0, 0) at the bottom left.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The residuals plotter</h1>
                </header>
            
            <article>
                
<p class="mce-root">A deeper look at the canvasing system will be explored later in the chapter. For now, let's return to the <kbd>Plot</kbd> method that satisfies the <kbd>plot.Plotter</kbd> interface. </p>
<p class="mce-root">Most interesting are the following lines:</p>
<pre class="mce-root">  trX, trY := p.Transforms(&amp;c)<br/>  zero := trY(0)<br/>  lineStyle := r.LineStyle<br/>  for _, xy := range r.XYs {<br/>    x := trX(xy.X)<br/>    y := trY(xy.Y)<br/>    c.StrokeLine2(lineStyle, x, zero, x, y)<br/>  }</pre>
<p class="mce-root"><kbd>p.Transforms(&amp;c)</kbd> returns two functions, which will transform the coordinate of our data point to the coordinate of the backend. This way we wouldn't have to worry about the absolute location of each point. Instead, it will be treated in relation to the absolute location in the final image.</p>
<p class="mce-root">Having gotten the transformation functions, we then loop through the residuals that we have, and transform each to the coordinate (<kbd>x := trX(xy.X)</kbd> and <kbd>y := trY(xy.Y)</kbd>) within the canvas.</p>
<p class="mce-root">Finally, we tell the canvas to draw a straight line between two points: (<em>x</em>, 0) and (<em>x</em>, <em>y</em>). This draws a straight line up or down from the <kbd>X</kbd> axis.</p>
<p class="mce-root">Thus, we have created our own <kbd>plot.Plotter</kbd> interface, which we can now add to the <kbd>plot</kbd> object. But adding to a <kbd>*plot.Plot</kbd> object directly requires a lot of tinkering. So, here's a function to nicely wrap all that up:</p>
<pre>func newResidPlot(xs []time.Time, ys []float64) *plot.Plot {<br/>  p, err := plot.New()<br/>  dieIfErr(err)<br/>  xys := make(plotter.XYs, len(ys))<br/>  for i := range ys {<br/>    xys[i].X = float64(xs[i].Unix())<br/>    xys[i].Y = ys[i]<br/>  }<br/>  r := &amp;residChart{XYs: xys, LineStyle: plotter.DefaultLineStyle}<br/>  r.LineStyle.Color = color.RGBA{A: 255}<br/>  p.Add(r)<br/>  p.Legend.Add("Residuals", r)<br/><br/>  p.Legend.TextStyle.Font = defaultFont<br/>  p.X.Tick.Marker = plot.TimeTicks{Format: "2006-01-01"}<br/>  p.Y.Label.TextStyle.Font = defaultFont<br/>  p.X.Label.TextStyle.Font = defaultFont<br/>  p.X.Tick.Label.Font = defaultFont<br/>  p.Y.Tick.Label.Font = defaultFont<br/>  p.Title.Font.Size = 16<br/>  return p<br/>}</pre>
<p class="mce-root">This function is reminiscent of <kbd>newTSPlot</kbd>—you provide it the <kbd>X</kbd> and <kbd>Y</kbd> values, and get a <kbd>*plot.Plot</kbd> object back out, with everything properly styled and formatted.</p>
<p class="mce-root">You may note that we're also adding the plotter object as a legend. To do this without an error, the <kbd>residChart</kbd> type needs to implement <kbd>plot.Thumbnailer</kbd>. Again, that's fairly straightforward:</p>
<pre class="mce-root">func (r *residChart) Thumbnail(c *draw.Canvas) {<br/>  y := c.Center().Y<br/>  c.StrokeLine2(r.LineStyle, c.Min.X, y, c.Max.X, y)<br/>}</pre>
<p class="mce-root">At this point, you may be wondering about the <kbd>canvas</kbd> object. If we are to draw a line between the canvas's minimum <kbd>X</kbd> to maximum <kbd>X</kbd>, wouldn't that just cause a horizontal line across the entire canvas?</p>
<p class="mce-root">The answer is not really. Recall earlier that the canvas is provided in the backend, and <kbd>draw.Canvas</kbd> is simply a tuple of a canvas backend and a rectangle? The rectangle actually serves to subset and constrain the canvas upon which it is being drawn.</p>
<p class="mce-root">We shall see this in action. Now that we've finished, we can turn our attention to the next section, which depicts a combination of all the plots into one image.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Combining plots</h1>
                </header>
            
            <article>
                
<p class="mce-root">A key function that allows us to do this is the <kbd>plot.Align</kbd> function. For us to see this in action, we need to write a that allows us to plot any number of plots to a file, as follows:</p>
<pre>func writeToPng(a interface{}, title, filename string, width, height vg.Length) {<br/>  switch at := a.(type) {<br/>  case *plot.Plot:<br/>    dieIfErr(at.Save(width*vg.Centimeter, height*vg.Centimeter, filename))<br/>    return<br/>  case [][]*plot.Plot:<br/>    rows := len(at)<br/>    cols := len(at[0])<br/>    t := draw.Tiles{<br/>      Rows: rows,<br/>      Cols: cols,<br/>    }<br/>    img := vgimg.New(width*vg.Centimeter, height*vg.Centimeter)<br/>    dc := draw.New(img)<br/><br/>    if title != "" {<br/>      at[0][0].Title.Text = title<br/>    }<br/><br/>    canvases := plot.Align(at, t, dc)<br/>    for i := 0; i &lt; t.Rows; i++ {<br/>      for j := 0; j &lt; t.Cols; j++ {<br/>        at[i][j].Draw(canvases[i][j])<br/>      }<br/>    }<br/><br/>    w, err := os.Create(filename)<br/>    dieIfErr(err)<br/><br/>    png := vgimg.PngCanvas{Canvas: img}<br/>    _, err = png.WriteTo(w)<br/>    dieIfErr(err)<br/>    return<br/>  }<br/>  panic("Unreachable")<br/>}</pre>
<p class="mce-root">We'll skip the part where if <kbd>a</kbd> is <kbd>plot.Plot</kbd>, we simply call the <kbd>.Save</kbd> method. Instead, we'll look at the second case, where <kbd>a</kbd> is <kbd>[][]*plot.Plot</kbd>.</p>
<p class="mce-root">At first this may seem rather strange—why have a slice of slice of plots when all we want to do is to combine them in quick succession. The key to understanding this is that Gonum supports the tiling of charts so if you want four charts arranged in 2x2 fashion, it can be done. Having four charts in a row is simply a special case of a 4x1 layout.</p>
<p class="mce-root">We can arrange the layouts using a function, as follows:</p>
<pre class="mce-root">func plotDecomposed(xs []time.Time, a stl.Result) [][]*plot.Plot {<br/>  plots := make([][]*plot.Plot, 4)<br/>  plots[0] = []*plot.Plot{newTSPlot(xs, a.Data, "Data")}<br/>  plots[1] = []*plot.Plot{newTSPlot(xs, a.Trend, "Trend")}<br/>  plots[2] = []*plot.Plot{newTSPlot(xs, a.Seasonal, "Seasonal")}<br/>  plots[3] = []*plot.Plot{newResidPlot(xs, a.Resid, "Residuals")}<br/><br/>  return plots<br/>}</pre>
<p class="mce-root">Having acquired <kbd>[][]*plot.Plot</kbd>, we need to tell Gonum the tiling format that we're interested in, so the following code snippet defines the tiling format:</p>
<pre class="mce-root">  t := draw.Tiles{<br/>      Rows: rows,<br/>      Cols: cols,<br/>  }</pre>
<p class="mce-root">If you're following along with the code, you will realize that <kbd>rows</kbd> is <kbd>3</kbd> and <kbd>cols</kbd> is <kbd>1</kbd>.</p>
<p class="mce-root">Next, we have to provide a canvas to draw on:</p>
<pre class="mce-root">    img := vgimg.New(width*vg.Centimeter, height*vg.Centimeter)<br/>    dc := draw.New(img)</pre>
<p class="mce-root">Here, we use the <kbd>vgimg</kbd> backend because we want to write to a PNG image. If, for example, you want to set the DPI of the image, you may use <kbd>vgimg.NewWith</kbd> instead, and pass in the DPI option.</p>
<p class="mce-root"><kbd>dc</kbd> is <kbd>draw.Canvas</kbd> initiated from the large piece of canvas <kbd>img</kbd>. Now comes the magic: <kbd>canvases := plot.Align(at, t, dc)</kbd> basically splits the big canvas (<kbd>img</kbd>) into various smaller canvases—they're still part of the big canvas, but now, each <kbd>*plot.Plot</kbd> object gets allocated a smaller piece of the canvas, each with their own coordinate systems that are relative to the bigger canvas.</p>
<p class="mce-root">The following code simply draws the plots onto their respective mini-canvases:</p>
<pre class="mce-root">    for i := 0; i &lt; t.Rows; i++ {<br/>      for j := 0; j &lt; t.Cols; j++ {<br/>        at[i][j].Draw(canvases[i][j])<br/>      }<br/>    }</pre>
<p class="mce-root">Naturally, this process can be recursively repeated. A <kbd>Legend</kbd> object in <kbd>*plot.Plot</kbd> simply gets a smaller chunk of the canvas, and drawing a straight line from minimum <kbd>X</kbd> to maximum <kbd>X</kbd> simply draws a horizontal line across the entire mini canvas.</p>
<p class="mce-root">And this is how plots are made.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Forecasting</h1>
                </header>
            
            <article>
                
<p class="mce-root">We're decomposing a time series here with the STL algorithm. There are other methods of decomposing time series—you may be familiar with one: the discrete Fourier transform. If your data is a time-based signal (like electrical pulses or music), a Fourier transform essentially allows you to decompose a time series into various parts. Bear in mind that they are no longer seasonality and trend, but rather decompositions of different time and frequency domains.</p>
<p class="mce-root">This begs the question: what is the point of decomposing a time series?</p>
<p class="mce-root">A primary reason why we do any machine learning at all is to be able to predict values based on an input. When done on time series, this is called <strong>forecasting</strong>.</p>
<p class="mce-root">Think about this for a bit: if a time series is made up of multiple components, wouldn't it be better to be able to predict per component? If we are able to break a time series up into its components, be it by STL or by Fourier transforms, we would get better results if we predict per component and then recombine the data at the end.</p>
<p class="mce-root">Since we work on STL, we already have our series decomposed. A very simple exponential smoothing algorithm invented by Holt in 1957 allows us to use the trend and seasonal components, along with the original data, to forecast.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Holt-Winters</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, I shall explain a modified form of the Holt-Winters exponential smoothing algorithm, which is quite useful for forecasting. Holt-Winters is a fairly simple algorithm. Here it is:</p>
<pre>func hw(a stl.Result, periodicity, forward int, alpha, beta, gamma float64) []float64 {<br/>  level := make([]float64, len(a.Data))<br/>  trend := make([]float64, len(a.Trend))<br/>  seasonal := make([]float64, len(a.Seasonal))<br/>  forecast := make([]float64, len(a.Data)+forward)<br/>  copy(seasonal, a.Seasonal)<br/><br/>  for i := range a.Data {<br/>    if i == 0 {<br/>      continue<br/>    }<br/>    level[i] = alpha*a.Data[i] + (1-alpha)*(level[i-1]+trend[i-1])<br/>    trend[i] = beta*(level[i]-level[i-1]) + (1-beta)*(trend[i-1])<br/>    if i-periodicity &lt; 0 {<br/>      continue<br/>    }<br/>    seasonal[i] = gamma*(a.Data[i]-level[i-1]-trend[i-1]) + (1-gamma)*(seasonal[i-periodicity])<br/>  }<br/><br/>  hplus := ((periodicity - 1) % forward) + 1<br/>  for i := 0; i+forward &lt; len(forecast); i++ {<br/>    forecast[i+forward] = level[i] + float64(forward)*trend[i] + seasonal[i-periodicity+hplus]<br/>  }<br/>  copy(forecast, a.Data)<br/><br/>  return forecast<br/>}</pre>
<p class="mce-root">Calling it is rather easy. We would wind up with a time series with a number of additional periods. Hence, we would also need to extend our dates range before we call <kbd>newTSPlot</kbd>. Again, it's a rather simple matter:</p>
<pre class="mce-root">func forecastTime(dates []time.Time, forwards int) []time.Time {<br/>  retVal := append(dates, make([]time.Time, forwards)...)<br/>  lastDate := dates[len(dates)-1]<br/>  for i := len(dates); i &lt; len(retVal); i++ {<br/>    retVal[i] = lastDate.AddDate(0, 1, 0)<br/>    lastDate = retVal[i]<br/>  }<br/>  return retVal<br/>}</pre>
<p class="mce-root">Ideally, we would also like to draw a gray background indicating that the values in the area are forecasts. Putting it all together, it looks rather like this:</p>
<pre>  fwd := 120<br/>  forecast := hw(decomposed, 12, fwd, 0.1, 0.05, 0.1)<br/>  datesplus := forecastTime(dates, fwd)<br/>  forecastPlot := newTSPlot(datesplus, forecast, "")<br/>  maxY := math.Inf(-1)<br/>  minY := math.Inf(1)<br/>  for i := range forecast {<br/>    if forecast[i] &gt; maxY {<br/>      maxY = forecast[i]<br/>    }<br/>    if forecast[i] &lt; minY {<br/>      minY = forecast[i]<br/>    }<br/>  }<br/>  // extend the range a little<br/>  minY--<br/>  maxY++<br/>  maxX := float64(datesplus[len(datesplus)-1].Unix())<br/>  minX := float64(datesplus[len(dates)-1].Unix())<br/><br/>  shadePoly := plotter.XYs{<br/>    {X: minX, Y: minY},<br/>    {X: maxX, Y: minY},<br/>    {X: maxX, Y: maxY},<br/>    {X: minX, Y: maxY},<br/>  }<br/>  poly, err := plotter.NewPolygon(shadePoly)<br/>  dieIfErr(err)<br/>  poly.Color = color.RGBA{A: 16}<br/>  poly.LineStyle.Color = color.RGBA{}<br/>  forecastPlot.Add(poly)<br/><br/>  writeToPng(forecastPlot, "Forecasted CO2 levels\n(10 years)", "forecast.png", 25, 25)</pre>
<p class="mce-root">This would yield the following plot:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-424 image-border" src="Images/5c0f2a7f-b316-46a3-b56e-4f83cb42a4ce.png" style="width:80.58em;height:78.83em;" width="967" height="946"/></div>
<p class="mce-root">If everything keeps going as it is, we can expect to see an increased CO<sub>2</sub> level in 10 years. Of course, it could go down if we take action now.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">This has been a rather hard chapter to write. The primary subject matter, <span>without exaggeration, </span>is one of existential threat. The methods used in the science at large are far more sophisticated than what I have covered in this chapter. </p>
<p class="mce-root">The techniques I covered is a small part of a large field of statistics known as time series analysis, where we've yet to even scratch the surface of it with this composition technique.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p>The following are the references:</p>
<ul>
<li class="mce-root"><em>[0] Hershfield, Hal. (2011). Future self-continuity</em>: How conceptions of the future self transform intertemporal choice. Annals of the New York Academy of Sciences. 1235. 30-43. 10.1111/j.1749-6632.2011.06201.x.</li>
<li class="mce-root"><em>[1] Qin, P. and Northoff, G. (2011)</em>: How is our self related to midline regions and the default-mode network?. NeuroImage, 57(3), pp.1221-1233.</li>
</ul>


            </article>

            
        </section>
    </div>



  </body></html>