<html><head></head><body>
		<div id="_idContainer137">
			<h1 id="_idParaDest-194" class="chapter-number"><a id="_idTextAnchor873"/><st c="0">7</st></h1>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor874"/><a id="_idTextAnchor875"/><st c="2">Performing Feature Scaling</st></h1>
			<p><st c="28">Many machine learning algorithms are sensitive to the variable scale. </st><st c="99">For example, the coefficients of linear models depend on the scale of the feature – that is, changing the feature scale will change the coefficient’s value. </st><st c="256">In linear models, as well as in algorithms that depend on distance calculations such as clustering and principal component analysis, features with larger value ranges tend to dominate over features with smaller ranges. </st><st c="475">Therefore, having features on a similar scale allows us to compare feature importance and may help algorithms converge faster, improving performance and </st><span class="No-Break"><st c="628">training times.</st></span></p>
			<p><st c="643">Scaling techniques, in general, divide the variables by some constant; therefore, it is important to highlight that the shape of the variable distribution does not change when we rescale the variables. </st><st c="846">If you want to change the distribution shape, check out </st><a href="B22396_03.xhtml#_idTextAnchor351"><span class="No-Break"><em class="italic"><st c="902">Chapter 3</st></em></span></a><st c="911">, </st><em class="italic"><st c="913">Transforming </st></em><span class="No-Break"><em class="italic"><st c="926">Numerical Variables</st></em></span><span class="No-Break"><st c="945">.</st></span></p>
			<p><st c="946">In this chapter, we will describe different methods to set features on a </st><span class="No-Break"><st c="1020">similar scale.</st></span></p>
			<p><st c="1034">This chapter will cover the </st><span class="No-Break"><st c="1063">following recipes:</st></span></p>
			<ul>
				<li><st c="1081">Standardizing </st><span class="No-Break"><st c="1096">the features</st></span></li>
				<li><st c="1108">Scaling to the maximum and </st><span class="No-Break"><st c="1136">minimum values</st></span></li>
				<li><st c="1150">Scaling with the median </st><span class="No-Break"><st c="1175">and quantiles</st></span></li>
				<li><st c="1188">Performing </st><span class="No-Break"><st c="1200">mean normalization</st></span></li>
				<li><st c="1218">Implementing maximum </st><span class="No-Break"><st c="1240">absolute scaling</st></span></li>
				<li><a id="_idTextAnchor876"/><st c="1256">Scaling to vector </st><span class="No-Break"><st c="1275">unit length</st></span></li>
			</ul>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor877"/><a id="_idTextAnchor878"/><st c="1286">Technical requirements</st></h1>
			<p><st c="1309">The main libraries that we use in this chapter are scikit-learn (</st><strong class="source-inline"><st c="1375">sklearn</st></strong><st c="1383">) for scaling, </st><strong class="source-inline"><st c="1399">pandas</st></strong><st c="1405"> to handle the data, and </st><strong class="source-inline"><st c="1430">matplotlib</st></strong> <span class="No-Break"><st c="1440">for plotting.</st></span></p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor879"/><st c="1454">S</st><a id="_idTextAnchor880"/><st c="1456">tandardizing the features</st></h1>
			<p><st c="1481">Standardization</st><a id="_idIndexMarker514"/><st c="1497"> is the process of centering the variable at </st><strong class="source-inline"><st c="1542">0</st></strong><st c="1543"> and standardizing the variance to </st><strong class="source-inline"><st c="1578">1</st></strong><st c="1579">. To</st><a id="_idTextAnchor881"/><st c="1583"> standardize features, we subtract the mean from each observation and then divide the result by the </st><span class="No-Break"><st c="1683">standard deviation:</st></span></p>
			<p><img src="image/25.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.675em;height:1.698em;width:7.146em"/><st c="1702"/></p>
			<p><st c="1733">The result of the preceding transformation is called</st><a id="_idIndexMarker515"/><st c="1785"> the </st><strong class="bold"><st c="1790">z-score</st></strong><st c="1797"> and represents how many standard deviations a given observation </st><em class="italic"><st c="1862">deviates</st></em><st c="1870"> from </st><span class="No-Break"><st c="1876">the mean.</st></span></p>
			<p><st c="1885">Standardization is generally useful when models require the variables to be centered at zero and data is not sparse (centering sparse data will destroy its sparse nature). </st><st c="2058">On the downside, standardization is sensitive to outliers and the z-score does not keep the symmetric properties if the variables are highly skewed, as we discuss in the </st><span class="No-Break"><st c="2228">following section.</st></span></p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor882"/><st c="2246">Getting ready</st></h2>
			<p><st c="2260">With standardization, the variable distribution does not change; what changes is the magnitude of their values, as we see in the </st><span class="No-Break"><st c="2390">following figure:</st></span></p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B22396_07_1.jpg" alt="Figure 7.1 – Distribution of a normal and skewed variable before and after standardization."/><st c="2407"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="2777">Figure 7.1 – Distribution of a normal and skewed variable before and after standardization.</st></p>
			<p><st c="2868">The z-score (</st><em class="italic"><st c="2882">x</st></em><st c="2884"> axis in the bottom panels) indicates</st><a id="_idIndexMarker516"/><st c="2921"> how many standard deviations an observation deviates from the mean. </st><st c="2990">When the z-score is </st><strong class="source-inline"><st c="3010">1</st></strong><st c="3011">, the observation lies 1 standard deviation to the right of the mean, whereas when the z-score is </st><strong class="source-inline"><st c="3109">-1</st></strong><st c="3111">, the sample is 1 standard deviation to the left of </st><span class="No-Break"><st c="3163">the mean.</st></span></p>
			<p><st c="3172">In normally distributed variables, we can estimate the probability of a value being greater or smaller than a given z-score, and this probability distribution is symmetric. </st><st c="3346">The probability of an observation being smaller than a z-score of </st><strong class="source-inline"><st c="3412">-1</st></strong><st c="3414"> is equivalent to the probability of a value being greater than </st><strong class="source-inline"><st c="3478">1</st></strong><st c="3479"> (horizontal line in the bottom-left panel). </st><st c="3524">This symmetry is fundamental to many statistical tests. </st><st c="3580">In skewed distributions, this symmetry does not hold. </st><st c="3634">As illustrated in the bottom-right panel of </st><span class="No-Break"><em class="italic"><st c="3678">Figure 7</st></em></span><em class="italic"><st c="3686">.1</st></em><st c="3688"> (horizontal lines), the probability of a value being smaller than </st><strong class="source-inline"><st c="3755">-1</st></strong><st c="3757"> is different from that of being greater </st><span class="No-Break"><st c="3798">than </st></span><span class="No-Break"><strong class="source-inline"><st c="3803">1</st></strong></span><span class="No-Break"><st c="3804">.</st></span></p>
			<p class="callout-heading"><st c="3805">Note</st></p>
			<p class="callout"><st c="3810">The mean and the standard deviation are sensitive to outliers; therefore, the features may scale differently from each other in the presence of outliers when </st><span class="No-Break"><st c="3969">using standardization.</st></span></p>
			<p><st c="3991">In practice, we often apply standardization ignoring the shape of the distribution. </st><st c="4076">However, keep in mind that if the models or tests you are using make assumptions about the data’s distribution, you might benefit from transforming the variables before standardization, or trying a different </st><span class="No-Break"><st c="4284">scaling metho</st><a id="_idTextAnchor883"/><a id="_idTextAnchor884"/><st c="4297">d.</st></span></p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor885"/><st c="4300">How to do it...</st></h2>
			<p><st c="4316">In this recipe, we’ll apply </st><a id="_idIndexMarker517"/><st c="4345">standardization to the variables of the California </st><span class="No-Break"><st c="4396">housing dataset:</st></span></p>
			<ol>
				<li><st c="4412">Let’s begin by importing the required Python packages, classes, </st><span class="No-Break"><st c="4477">and functions:</st></span><pre class="source-code"><st c="4491">
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler</st></pre></li>				<li><st c="4667">Let’s load the California housing dataset from scikit-learn into a DataFrame and drop the </st><strong class="source-inline"><st c="4758">Latitude</st></strong><st c="4766"> and </st><span class="No-Break"><strong class="source-inline"><st c="4771">Longitude</st></strong></span><span class="No-Break"><st c="4780"> variables:</st></span><pre class="source-code"><st c="4791">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)
X.drop(labels=["Latitude", "Longitude"], axis=1,
    inplace=True)</st></pre></li>				<li><st c="4919">Now, let’s divide the data into train and </st><span class="No-Break"><st c="4962">test sets:</st></span><pre class="source-code"><st c="4972">
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)</st></pre></li>				<li><st c="5062">Next, we’ll set up the </st><strong class="source-inline"><st c="5086">StandardScaler()</st></strong><st c="5102"> function from scikit-learn and fit it</st><a id="_idTextAnchor886"/><st c="5140"> to the train set so that it learns each variable’s mean and </st><span class="No-Break"><st c="5201">standard deviation:</st></span><pre class="source-code"><st c="5220">
scaler = StandardScaler().set_output(
    transform="pandas")
scaler.fit(X_train)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="5298">Note</st></p>
			<p class="callout"><st c="5303">Scikit-learn scalers, like any scikit-learn transformer, return NumPy arrays by default. </st><st c="5393">To return </st><strong class="source-inline"><st c="5403">pandas</st></strong><st c="5409"> or </st><strong class="source-inline"><st c="5413">polars</st></strong><st c="5419"> DataFrames, we need to specify the output container with the </st><strong class="source-inline"><st c="5481">set_output()</st></strong><st c="5493"> method, as we did in </st><span class="No-Break"><em class="italic"><st c="5515">Step 4</st></em></span><span class="No-Break"><st c="5521">.</st></span></p>
			<ol>
				<li value="5"><st c="5522">Now, let’s </st><a id="_idIndexMarker518"/><st c="5534">standardize the train and test sets with the </st><span class="No-Break"><st c="5579">trained scaler:</st></span><pre class="source-code"><st c="5594">
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)</st></pre><p class="list-inset"><strong class="source-inline"><st c="5678">StandardScaler()</st></strong><st c="5695"> stores the mean and standard deviation learned from the training set during </st><strong class="source-inline"><st c="5772">fit()</st></strong><st c="5777">. Let’s visualize the </st><span class="No-Break"><st c="5799">learned parameters.</st></span></p></li>				<li><st c="5818">First, we’ll print the mean values that were learned </st><span class="No-Break"><st c="5872">by </st></span><span class="No-Break"><strong class="source-inline"><st c="5875">scaler</st></strong></span><span class="No-Break"><st c="5881">:</st></span><pre class="source-code"><st c="5883">
scaler.mean_</st></pre><p class="list-inset"><st c="5896">We see the mean values of each variable in the </st><span class="No-Break"><st c="5944">following output:</st></span></p><pre class="source-code"><strong class="bold"><st c="5961">array([3.86666741e+00, 2.86187016e+01, 5.42340368e+00,                 1.09477484e+00,1.42515732e+03, 3.04051776e+00])</st></strong></pre></li>				<li><st c="6064">Now, let’s print the standard deviation values that were learned </st><span class="No-Break"><st c="6130">by </st></span><span class="No-Break"><strong class="source-inline"><st c="6133">scaler</st></strong></span><span class="No-Break"><st c="6139">:</st></span><pre class="source-code"><st c="6141">
scaler.scale_</st></pre><p class="list-inset"><st c="6155">We see the standard deviation of each variable in the </st><span class="No-Break"><st c="6210">following output:</st></span></p><pre class="source-code"><strong class="bold"><st c="6227">array([1.89109236e+00, 1.25962585e+01, 2.28754018e+00,                          4.52736275e-01, 1.14954037e+03, 6.86792905e+00])</st></strong></pre><p class="list-inset"><st c="6331">Let’s compare the transformed data with the original data to understand </st><span class="No-Break"><st c="6404">the changes.</st></span></p></li>				<li><st c="6416">Let’s print</st><a id="_idIndexMarker519"/><st c="6428"> the descriptive statistics from the original variables in the </st><span class="No-Break"><st c="6491">test set:</st></span><pre class="source-code"><st c="6500">
X_test.describe()</st></pre><p class="list-inset"><st c="6518">In the following output, we see that the variables’ mean values are different from zero and the </st><span class="No-Break"><st c="6615">variance varies:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B22396_07_2.jpg" alt="Figure 7.2 – Descriptive statistical parameters of the variables before scaling"/><st c="6631"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="7205">Figure 7.2 – Descriptive statistical parameters of the variables before scaling</st></p>
			<ol>
				<li value="9"><st c="7284">Let’s now print</st><a id="_idTextAnchor887"/><st c="7300"> the descriptive statistical values from the </st><span class="No-Break"><st c="7345">transformed variables:</st></span><pre class="source-code"><st c="7367">
X_test_scaled.describe()</st></pre><p class="list-inset"><st c="7392">In the following output, we see that the variables’ mean is now centered at </st><strong class="source-inline"><st c="7469">0</st></strong><st c="7470"> and the variance is </st><span class="No-Break"><st c="7491">approximately </st><a id="_idTextAnchor888"/></span><span class="No-Break"><strong class="source-inline"><st c="7505">1</st></strong></span><span class="No-Break"><st c="7506">:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B22396_07_3.jpg" alt="Figure 7.3 – Descriptive statistical parameters of the scaled variables showing a mean of 0 and variance of approximately 1"/><st c="7507"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="8073">Figure 7.3 – Descriptive statistical parameters of the scaled variables showing a mean of 0 and variance of approximately 1</st></p>
			<p class="callout-heading"><st c="8196">Note</st></p>
			<p class="callout"><st c="8201">The </st><strong class="source-inline"><st c="8206">AveRooms</st></strong><st c="8214">, </st><strong class="source-inline"><st c="8216">AveBedrms</st></strong><st c="8225">, and </st><strong class="source-inline"><st c="8231">AveOccup</st></strong><st c="8239"> variables are highly skewed, which can lead to observed values in the test set that are much greater or much smaller than those in the training set, and hence we see that the variance deviates from </st><strong class="source-inline"><st c="8438">1</st></strong><st c="8439">. This is to be expected because standardization is sensitive to outliers and very </st><span class="No-Break"><st c="8522">skewed distributions.</st></span></p>
			<p><st c="8543">We mentioned, in</st><a id="_idIndexMarker520"/><st c="8560"> the </st><em class="italic"><st c="8565">Getting ready</st></em><st c="8578"> section, that the shape of the distribution does not change with standardization. </st><st c="8661">Go ahead and corroborate that by executing  </st><strong class="source-inline"><st c="8704">X_test.hist()</st></strong><st c="8717"> and then </st><strong class="source-inline"><st c="8727">X_test_scaled.hist()</st></strong><st c="8747"> to compare the variables’ distribution before and after </st><span class="No-Break"><st c="8804">the transformati</st><a id="_idTextAnchor889"/><a id="_idTextAnchor890"/><st c="8820">on.</st></span></p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor891"/><st c="8824">How it works...</st></h2>
			<p><st c="8840">In this recipe, we standardized the variables of the California housing dataset by utilizing scikit-learn. </st><st c="8948">We split the data into train and test sets because the parameters for the standardization should be learned from the train set. </st><st c="9076">This is to avoid leaking data from the test to the train set during the preprocessing steps and to ensure the test set remains naïve to all feature </st><span class="No-Break"><st c="9224">transformation processes.</st></span></p>
			<p><st c="9249">To standardize these features, we used scikit-learn’s </st><strong class="source-inline"><st c="9304">StandardScaler()</st></strong><st c="9320"> function, which is able to learn and store the parameters utilized in the transformation. </st><st c="9411">Using </st><strong class="source-inline"><st c="9417">fit()</st></strong><st c="9422">, the scaler learned each variable’s mean and standard deviation and stored them in its </st><strong class="source-inline"><st c="9510">mean_</st></strong><st c="9515"> and </st><strong class="source-inline"><st c="9520">scale_</st></strong><st c="9526"> attributes. </st><st c="9539">Using </st><strong class="source-inline"><st c="9545">transform()</st></strong><st c="9556">, the scaler standardized the variables in the train and test sets. </st><st c="9624">The default output of </st><strong class="source-inline"><st c="9646">StandardScaler()</st></strong><st c="9662"> is a NumPy array, but through the </st><strong class="source-inline"><st c="9697">set_output()</st></strong><st c="9709"> parameter, we can change the output </st><a id="_idIndexMarker521"/><st c="9746">container to a </st><strong class="source-inline"><st c="9761">pandas</st></strong><st c="9767"> DataFrame, as we did in </st><em class="italic"><st c="9792">Step 4</st></em><st c="9798">, or to </st><strong class="source-inline"><st c="9806">polars</st></strong><st c="9812">, by </st><span class="No-Break"><st c="9817">setting </st></span><span class="No-Break"><strong class="source-inline"><st c="9825">transform="polars"</st></strong></span><span class="No-Break"><st c="9843">.</st></span></p>
			<p class="callout-heading"><st c="9844">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="9849">StandardScaler()</st></strong><st c="9866"> will subtract the mean and divide it by the standard deviation by default. </st><st c="9942">If we want to just center the distributions without standardizing the variance, we can do so by setting </st><strong class="source-inline"><st c="10046">with_std=False</st></strong><st c="10060"> when initializing the transformer. </st><st c="10096">If we want to set the variance to </st><strong class="source-inline"><st c="10130">1</st></strong><st c="10131">, without cantering the distribution, we can do so by setting </st><strong class="source-inline"><st c="10193">with_mean=False</st></strong><st c="10208"> in </st><span class="No-Break"><em class="italic"><st c="10212">Step</st><a id="_idTextAnchor892"/><a id="_idTextAnchor893"/><st c="10216"> 4</st></em></span><span class="No-Break"><st c="10218">.</st></span></p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor894"/><st c="10219">Scaling to the maximum and minimum values</st></h1>
			<p><st c="10261">Scaling </st><a id="_idIndexMarker522"/><st c="10270">to the minimu</st><a id="_idTextAnchor895"/><st c="10283">m and maximum values squeezes the values of </st><a id="_idIndexMarker523"/><st c="10328">the variables between </st><strong class="source-inline"><st c="10350">0</st></strong><st c="10351"> and </st><strong class="source-inline"><st c="10356">1</st></strong><st c="10357">. To implement this scaling method, we subtract the minimu</st><a id="_idTextAnchor896"/><st c="10415">m value from all the observations and divide the result by the value range – that is, the difference between the maximum and </st><span class="No-Break"><st c="10541">minimum values:</st></span></p>
			<p><img src="image/26.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;max&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.557em;height:1.635em;width:8.738em"/><st c="10556"/></p>
			<p><st c="10591">Scaling to the minimum and maximum is suitable for variables with very small standard deviations, when the models do not require data to be centered at zero, and when we want to preserve zero entries in sparse data, such as in one-hot encoded variables. </st><st c="10845">On the downside, it is sensitive </st><span class="No-Break"><st c="10878">to outliers.</st></span></p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor897"/><st c="10890">Getting ready</st></h2>
			<p><st c="10904">Scaling to the minimum and maximum value does not change the distribution of the variables, as illustrated in the </st><span class="No-Break"><st c="11019">following figure:</st></span></p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B22396_07_4.jpg" alt="Figure 7.4 – Distribution of a normal and skewed variable before and after scaling to the minimum and maximum value"/><st c="11036"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="11270">Figure 7.4 – Distribution of a normal and skewed variable before and after scaling to the minimum and maximum value</st></p>
			<p><st c="11385">This </st><a id="_idIndexMarker524"/><st c="11391">scaling method standardizes the maximum value of the</st><a id="_idIndexMarker525"/><st c="11443"> variables to a unit size. </st><st c="11470">Scaling to the minimum and maximum value tends to be the preferred alternative to standardization, and it is suitable for variables with very small standard deviations and when we want to preserve zero entries in sparse data, such as in one-hot encoded variables, or variables derived from counts, such as bag of words. </st><st c="11790">However, this procedure does not center the variables at zero, so if the algorithm has that requirement, this method might not be the </st><span class="No-Break"><st c="11924">best choice.</st></span></p>
			<p class="callout-heading"><st c="11936">Note</st></p>
			<p class="callout"><st c="11941">Scaling to the minimum and maximum values is sensitive to outliers. </st><st c="12010">If outliers are present in the training set, the scaling will squeeze the values toward one of the tails. </st><st c="12116">If, on the contrary, outliers are in the test set, the variable will show values greater than </st><strong class="source-inline"><st c="12210">1</st></strong><st c="12211"> or smaller than </st><strong class="source-inline"><st c="12228">0</st></strong><st c="12229"> after scaling, depending on whether the outlier is on the left or </st><span class="No-Break"><st c="12296">righ</st><a id="_idTextAnchor898"/><a id="_idTextAnchor899"/><st c="12300">t tail.</st></span></p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor900"/><st c="12308">How to do it...</st></h2>
			<p><st c="12324">In this recipe, we’ll scale </st><a id="_idIndexMarker526"/><st c="12353">the variables of the California housing dataset </st><a id="_idIndexMarker527"/><st c="12401">to values between </st><strong class="source-inline"><st c="12419">0</st></strong> <span class="No-Break"><st c="12420">and </st></span><span class="No-Break"><strong class="source-inline"><st c="12424">1</st></strong></span><span class="No-Break"><st c="12425">:</st></span></p>
			<ol>
				<li><st c="12426">Let’s start by importing </st><strong class="source-inline"><st c="12451">pandas</st></strong><st c="12457"> and the required classes </st><span class="No-Break"><st c="12483">and functions:</st></span><pre class="source-code"><st c="12497">
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler</st></pre></li>				<li><st c="12671">Let’s load the </st><a id="_idTextAnchor901"/><st c="12687">California housing dataset from scikit-learn into a </st><strong class="source-inline"><st c="12739">pandas</st></strong><st c="12745"> DataFrame, dropping the </st><strong class="source-inline"><st c="12770">Latitude</st></strong><st c="12778"> and </st><span class="No-Break"><strong class="source-inline"><st c="12783">Longitude</st></strong></span><span class="No-Break"><st c="12792"> variables:</st></span><pre class="source-code"><st c="12803">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)
X.dro</st><a id="_idTextAnchor902"/><st c="12874">p(labels=["Latitude", "Longitude"], axis=1,
    inplace=True)</st></pre></li>				<li><st c="12932">Let’s divide the data into training and </st><span class="No-Break"><st c="12973">test sets:</st></span><pre class="source-code"><st c="12983">
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)</st></pre></li>				<li><st c="13073">Let’s set up the scaler and then fit it to the train set so that it learns each variable’s minimum and maximum values and the </st><span class="No-Break"><st c="13200">value range:</st></span><pre class="source-code"><st c="13212">
scaler = MinMaxScaler().set_output(
    transform="pandas"")
scaler.fit(X_train)</st></pre></li>				<li><st c="13289">Finally, let’s</st><a id="_idTextAnchor903"/><st c="13304"> scale the variables in the train and test sets with the </st><span class="No-Break"><st c="13361">trained scaler:</st></span><pre class="source-code"><st c="13376">
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="13460">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="13465">MinMaxScale</st><a id="_idTextAnchor904"/><st c="13477">r()</st></strong><st c="13481"> stores the maximum and minimum values and the value ranges in its </st><strong class="source-inline"><st c="13548">data_max_</st></strong><st c="13557">, </st><strong class="source-inline"><st c="13559">min_</st></strong><st c="13563">, and </st><strong class="source-inline"><st c="13569">data_range_</st></strong> <span class="No-Break"><st c="13580">attributes, respectively.</st></span></p>
			<p class="list-inset"><st c="13606">We can</st><a id="_idIndexMarker528"/><st c="13613"> corroborate the minimum values of the</st><a id="_idIndexMarker529"/><st c="13651"> transformed variables by executing </st><strong class="source-inline"><st c="13687">X_test_scaled.min()</st></strong><st c="13706">, which will return the </st><span class="No-Break"><st c="13730">following output:</st></span></p>
			<pre class="source-code">
<strong class="bold"><st c="13747">MedInc           0.000000</st></strong>
<strong class="bold"><st c="13763">HouseAge        0.000000</st></strong>
<strong class="bold"><st c="13781">AveRooms        0.004705</st></strong>
<strong class="bold"><st c="13799">AveBedrms      0.004941</st></strong>
<strong class="bold"><st c="13818">Population     0.000140</st></strong>
<strong class="bold"><st c="13838">AveOccup      -0.000096</st></strong>
<strong class="bold"><st c="13857">dtype: float64</st></strong></pre>			<p class="list-inset"><st c="13872">By executing </st><strong class="source-inline"><st c="13886">X_test_scaled.max()</st></strong><st c="13905">, we see that the maximum values of the variables are </st><span class="No-Break"><st c="13959">around </st></span><span class="No-Break"><strong class="source-inline"><st c="13966">1</st></strong></span><span class="No-Break"><st c="13967">:</st></span></p>
			<pre class="source-code">
<strong class="bold"><st c="13968">MedInc           1.000000</st></strong>
<strong class="bold"><st c="13983">HouseAge        1.000000</st></strong>
<strong class="bold"><st c="14001">AveRooms        1.071197</st></strong>
<strong class="bold"><st c="14019">AveBedrms      0.750090</st></strong>
<strong class="bold"><st c="14038">Population     0.456907</st></strong>
<strong class="bold"><st c="14058">AveOccup        2.074553</st></strong>
<strong class="bold"><st c="14076">dtype: float64</st></strong></pre>			<p class="callout-heading"><st c="14091">Note</st></p>
			<p class="callout"><st c="14096">If you check the maximum values of the variables in the train set after the transformation, you’ll see that they are exactly </st><strong class="source-inline"><st c="14222">1</st></strong><st c="14223">. Yet, in the test set, we see values greater and smaller than </st><strong class="source-inline"><st c="14286">1</st></strong><st c="14287">. This occurs because, in the test set, there are observations with larger or smaller magnitudes than those in the train set. </st><st c="14413">In fact, we see the greatest differences in the variables that deviate the most from the normal distribution (the last four variables in the dataset). </st><st c="14564">This behavior is expected because scaling to the minimum and maximum values is sensitive to outliers and very </st><span class="No-Break"><st c="14674">skewed distributions.</st></span></p>
			<p><st c="14695">Scaling to the</st><a id="_idIndexMarker530"/><st c="14710"> minimum and maximum value does not change the shape</st><a id="_idIndexMarker531"/><st c="14762"> of the variable’s distribution. </st><st c="14795">You can corroborate that by displaying the histograms before and after </st><span class="No-Break"><st c="14866">the tr</st><a id="_idTextAnchor905"/><a id="_idTextAnchor906"/><st c="14872">ansformation.</st></span></p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor907"/><st c="14886">How it works...</st></h2>
			<p><st c="14902">In this rec</st><a id="_idTextAnchor908"/><st c="14914">ipe, we scaled the variables of the California housing dataset to values between </st><strong class="source-inline"><st c="14996">0</st></strong> <span class="No-Break"><st c="14997">and </st></span><span class="No-Break"><strong class="source-inline"><st c="15001">1</st></strong></span><span class="No-Break"><st c="15002">.</st></span></p>
			<p><strong class="source-inline"><st c="15003">MinMaxScaler()</st></strong><st c="15018"> from scikit-learn learned the minimum and maximum values and the value range of each variable when we called the </st><strong class="source-inline"><st c="15132">fit()</st></strong><st c="15137"> method and stored these parameters in its </st><strong class="source-inline"><st c="15180">data_max_</st></strong><st c="15189">, </st><strong class="source-inline"><st c="15191">min_</st></strong><st c="15195">, and </st><strong class="source-inline"><st c="15201">data_range_</st></strong><st c="15212"> attributes. </st><st c="15225">By using </st><strong class="source-inline"><st c="15234">transform()</st></strong><st c="15245">, we made the scaler remove the minimum value from each variable in the train and test sets and divide the result by the </st><span class="No-Break"><st c="15366">value range.</st></span></p>
			<p class="callout-heading"><st c="15378">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="15383">MinMaxScaler()</st></strong><st c="15398"> will scale all variables by default. </st><st c="15436">To scale only a subset of the variables in the dataset, you can use </st><strong class="source-inline"><st c="15504">ColumnTransformer()</st></strong><st c="15523"> from scikit-learn or </st><strong class="source-inline"><st c="15545">SklearnTransformerWrapper()</st></strong> <span class="No-Break"><st c="15572">from </st></span><span class="No-Break"><strong class="source-inline"><st c="15578">Feature-engine</st></strong></span><span class="No-Break"><st c="15592">.</st></span></p>
			<p><strong class="source-inline"><st c="15593">MinMaxScaler()</st></strong><st c="15608"> will scale the variables between </st><strong class="source-inline"><st c="15642">0</st></strong><st c="15643"> and </st><strong class="source-inline"><st c="15648">1</st></strong><st c="15649"> by default. </st><st c="15662">However, we have the option to scale to a different range by adjusting the tuple passed to the </st><span class="No-Break"><strong class="source-inline"><st c="15757">feature_range</st></strong></span><span class="No-Break"><st c="15770"> parameter.</st></span></p>
			<p><st c="15781">By default, </st><strong class="source-inline"><st c="15794">MinMaxScaler()</st></strong><st c="15808"> returns </st><a id="_idIndexMarker532"/><st c="15817">NumPy arrays, but we can modify this </st><a id="_idIndexMarker533"/><st c="15854">behavior to return </st><strong class="source-inline"><st c="15873">pandas</st></strong><st c="15879"> DataFrames with the </st><strong class="source-inline"><st c="15900">set_output()</st></strong><st c="15912"> method, as we</st><a id="_idTextAnchor909"/><a id="_idTextAnchor910"/><st c="15926"> did in </st><span class="No-Break"><em class="italic"><st c="15934">Step 4</st></em></span><span class="No-Break"><st c="15940">.</st></span></p>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor911"/><st c="15941">Scaling with the median and quantiles</st></h1>
			<p><st c="15979">When </st><a id="_idIndexMarker534"/><a id="_idTextAnchor912"/><st c="15985">scaling variab</st><a id="_idTextAnchor913"/><st c="15999">les to </st><a id="_idIndexMarker535"/><st c="16007">the median and quantiles, the median value is removed from the observations, and the result is divided by</st><a id="_idIndexMarker536"/><st c="16112"> the </st><strong class="bold"><st c="16117">Inter-Quarti</st><a id="_idTextAnchor914"/><st c="16129">le Range</st></strong><st c="16138"> (</st><strong class="bold"><st c="16140">IQR</st></strong><st c="16143">). </st><st c="16147">The IQR is the difference between the 3rd quartile and the 1st quartile, or, in other words, the difference between the 75th percentile and the </st><span class="No-Break"><st c="16291">25th percent</st><a id="_idTextAnchor915"/><st c="16303">ile:</st></span></p>
			<p><img src="image/27.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;_&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.758em;height:1.961em;width:15.169em"/><st c="16308"/></p>
			<p><st c="16310">This method is known</st><a id="_idIndexMarker537"/><st c="16330"> as </st><strong class="bold"><st c="16334">robust</st><a id="_idTextAnchor916"/><st c="16340"> scaling</st></strong><st c="16348"> because it produces more robust estimates for the center and value range of the variable. </st><st c="16439">Robust scaling is a suitable alternative to standardization when models require the variables to be centered and the data contains outliers. </st><st c="16580">It is worth noting that robust scaling will not change the overall shape of the </st><span class="No-Break"><st c="16660">vari</st><a id="_idTextAnchor917"/><a id="_idTextAnchor918"/><st c="16664">able distribution.</st></span></p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor919"/><st c="16683">How to do it...</st></h2>
			<p><st c="16699">In this recipe, we will implement scaling with the median and IQR by </st><span class="No-Break"><st c="16769">utilizing scikit-learn:</st></span></p>
			<ol>
				<li><st c="16792">Let’s start </st><a id="_idTextAnchor920"/><st c="16805">by importing </st><strong class="source-inline"><st c="16818">pandas</st></strong><st c="16824"> and the required scikit-learn classes </st><span class="No-Break"><st c="16863">and functions:</st></span><pre class="source-code"><st c="16877">
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler</st></pre></li>				<li><st c="17051">Let’s load the California housing dataset into a </st><strong class="source-inline"><st c="17101">pandas</st></strong><st c="17107"> DataFrame and drop the </st><strong class="source-inline"><st c="17131">Latitude</st></strong><st c="17139"> and </st><span class="No-Break"><strong class="source-inline"><st c="17144">Longitude</st></strong></span><span class="No-Break"><st c="17153"> variables:</st></span><pre class="source-code"><st c="17164">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)
X.drop(labels=[     "Latitude", "Longitude"], axis=1,
    inplac</st><a id="_idTextAnchor921"/><st c="17286">e=True)</st></pre></li>				<li><st c="17294">Let’s divide the data into train and </st><span class="No-Break"><st c="17332">test sets:</st></span><pre class="source-code"><st c="17342">
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0</st><a id="_idTextAnchor922"/><st c="17413">.3, random_state=0)</st></pre></li>				<li><st c="17432">Let’s set up scikit-learn’s </st><strong class="source-inline"><st c="17461">RobustScaler()</st></strong><st c="17475">and fit it to the train set so that it learns and stores the median </st><span class="No-Break"><st c="17544">and IQR:</st></span><pre class="source-code"><st c="17552">
scaler = RobustScaler().set_output(
    transform="pandas")
scaler.fit(X_train)</st></pre></li>				<li><st c="17628">Finally, let’s</st><a id="_idIndexMarker538"/><st c="17643"> scale the </st><a id="_idIndexMarker539"/><st c="17654">variables in the train and test sets with the </st><span class="No-Break"><st c="17700">trained scaler:</st></span><pre class="source-code"><st c="17715">
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)</st></pre></li>				<li><st c="17799">Let’s print the variable median values learned </st><span class="No-Break"><st c="17847">by </st></span><span class="No-Break"><strong class="source-inline"><st c="17850">RobustScaler()</st></strong></span><span class="No-Break"><st c="17864">:</st></span><pre class="source-code"><st c="17866">
scaler.center_</st></pre><p class="list-inset"><st c="17881">We see the parameters learned by </st><strong class="source-inline"><st c="17915">RobustScaler()</st></strong><st c="17929"> in the </st><span class="No-Break"><st c="17937">following output:</st></span></p><pre class="source-code"><strong class="bold"><st c="17954">array([3.53910000e+00, 2.90000000e+01, 5.22931763e+00,                 1.04878049e+00, 1.16500000e+03, 2.81635506e+00])</st></strong></pre></li>				<li><st c="18058">Now, let’s display the IQR learned </st><span class="No-Break"><st c="18094">by </st></span><span class="No-Break"><strong class="source-inline"><st c="18097">RobustScaler()</st></strong></span><span class="No-Break"><st c="18111">:</st></span><pre class="source-code"><st c="18113">
scaler.scale_</st></pre><p class="list-inset"><st c="18127">We can see the IQR for each variable in the </st><span class="No-Break"><st c="18172">following output:</st></span></p><pre class="source-code"><strong class="bold"><st c="18189">array([2.16550000e+00, 1.90000000e+01, 1.59537022e+00,</st></strong><strong class="bold"><st c="18244">                 9.41284380e-02, 9.40000000e</st><a id="_idTextAnchor923"/><st c="18272">+02, 8.53176853e-01])</st></strong></pre><p class="list-inset"><st c="18294">This scaling procedure does not change the variable distributions. </st><st c="18362">Go ahead and compare the distribution of the variables before and after the transformation</st><a id="_idTextAnchor924"/><a id="_idTextAnchor925"/><st c="18452"> by </st><span class="No-Break"><st c="18456">using histograms.</st></span></p></li>			</ol>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor926"/><st c="18473">How it works...</st></h2>
			<p><st c="18489">To scale the</st><a id="_idIndexMarker540"/><st c="18502"> features using</st><a id="_idIndexMarker541"/><st c="18517"> the median and IQR, we created an instance of </st><strong class="source-inline"><st c="18564">RobustScaler()</st></strong><st c="18578">. With </st><strong class="source-inline"><st c="18585">fit()</st></strong><st c="18590">, the scaler learned the median and IQR for each variable from the train set. </st><st c="18668">With </st><strong class="source-inline"><st c="18673">transform()</st></strong><st c="18684">, the scaler subtracted the median from each variable in the train and test sets and divided the result by </st><span class="No-Break"><st c="18791">the IQR.</st></span></p>
			<p><st c="18799">After the transformation, the median values of the variables were centered at </st><strong class="source-inline"><st c="18878">0</st></strong><st c="18879">, but the overall shape of the distributions did not change. </st><st c="18940">You can corroborate the effect of the transformation by displaying the histograms of the variables before and after the transformation and by printing out the main statistical parameters through </st><strong class="source-inline"><st c="19135">X_test.describe()</st><a id="_idTextAnchor927"/><a id="_idTextAnchor928"/></strong> <span class="No-Break"><st c="19152">and </st></span><span class="No-Break"><strong class="source-inline"><st c="19157">X_test_scaled.b()</st></strong></span><span class="No-Break"><st c="19174">.</st></span></p>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor929"/><st c="19175">Performing mean normalization</st></h1>
			<p><st c="19205">I</st><a id="_idTextAnchor930"/><st c="19207">n mean normalization, we </st><a id="_idIndexMarker542"/><st c="19232">center the variable at </st><strong class="source-inline"><st c="19255">0</st></strong><st c="19256"> and rescale the distribution to the value range, so that its values lie between </st><strong class="source-inline"><st c="19337">-1</st></strong><st c="19339"> and </st><strong class="source-inline"><st c="19344">1</st></strong><st c="19345">. This procedure involves subtracting the mean from each observation and then dividing the result by the difference between the minimum and maximum values, as </st><span class="No-Break"><st c="19504">sh</st><a id="_idTextAnchor931"/><st c="19506">own here:</st></span></p>
			<p><img src="image/28.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;max&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.557em;height:1.580em;width:8.713em"/><st c="19516"/></p>
			<p class="callout-heading"><st c="19558">Note</st></p>
			<p class="callout"><st c="19562">Mean normalization is an alternative to standardization. </st><st c="19620">In both cases, the variables are centered at </st><strong class="source-inline"><st c="19665">0</st></strong><st c="19666">. In mean normalization, the variance varies, while the values lie between </st><strong class="source-inline"><st c="19741">-1</st></strong><st c="19743"> and </st><strong class="source-inline"><st c="19748">1</st></strong><st c="19749">. On the other hand, in standardization, the variance is set to </st><strong class="source-inline"><st c="19813">1</st></strong><st c="19814"> and the value </st><span class="No-Break"><st c="19829">range varies.</st></span></p>
			<p><st c="19842">Mean </st><a id="_idIndexMarker543"/><st c="19848">normalization is a suitable alternative for models that need the variables to be centered at zero. </st><st c="19947">However, it is sensitive to outliers and not a suitable option for sparse data, as it will d</st><a id="_idTextAnchor932"/><a id="_idTextAnchor933"/><st c="20039">estroy the </st><span class="No-Break"><st c="20051">sparse nature.</st></span></p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor934"/><st c="20065">How to do it...</st></h2>
			<p><st c="20081">In this recipe, we will implement mean normalization </st><span class="No-Break"><st c="20135">with </st></span><span class="No-Break"><strong class="source-inline"><st c="20140">pandas</st></strong></span><span class="No-Break"><st c="20146">:</st></span></p>
			<ol>
				<li><st c="20148">Let’s import </st><strong class="source-inline"><st c="20161">pandas</st></strong><st c="20167"> and the required scikit-learn class </st><span class="No-Break"><st c="20204">and function:</st></span><pre class="source-code"><st c="20217">
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split</st></pre></li>				<li><st c="20344">Let’s load the California housing dataset from scikit-learn into a </st><strong class="source-inline"><st c="20412">pandas</st></strong><st c="20418"> DataFrame, dropping the </st><strong class="source-inline"><st c="20443">Latitude</st></strong><st c="20451"> and </st><span class="No-Break"><strong class="source-inline"><st c="20456">Longitude</st></strong></span><span class="No-Break"><st c="20465"> variables:</st></span><pre class="source-code"><st c="20476">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)
X.drop(labels=[
   "Latitude", "Longitude"], axis=1, inplace=T</st><a id="_idTextAnchor935"/><st c="20601">rue)</st></pre></li>				<li><st c="20606">Let’s divide the data into train and </st><span class="No-Break"><st c="20644">test sets:</st></span><pre class="source-code"><st c="20654">
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)</st></pre></li>				<li><st c="20744">Let’s learn the mean values from the variables in the </st><span class="No-Break"><st c="20799">train set:</st></span><pre class="source-code"><st c="20809">
means = X_train.mean(axi</st><a id="_idTextAnchor936"/><st c="20834">s=0)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="20839">Note</st></p>
			<p class="callout"><st c="20844">We set </st><strong class="source-inline"><st c="20852">axis=0</st></strong><st c="20858"> to take the mean of the rows – that is, of the observations in each variable. </st><st c="20937">If we set </st><strong class="source-inline"><st c="20947">axis=1</st></strong><st c="20953"> instead, </st><strong class="source-inline"><st c="20963">pandas</st></strong><st c="20969"> will calculate the mean value per observation across all </st><span class="No-Break"><st c="21027">the columns.</st></span></p>
			<p class="list-inset"><st c="21039">By </st><a id="_idIndexMarker544"/><st c="21043">executing </st><strong class="source-inline"><st c="21053">print(mean)</st></strong><st c="21064">, we display the mean values </st><span class="No-Break"><st c="21093">per variable:</st></span></p>
			<pre class="source-code">
<strong class="bold"><st c="21106">MedInc           3.866667</st></strong>
<strong class="bold"><st c="21122">HouseAge        28.618702</st></strong>
<strong class="bold"><st c="21141">AveRooms         5.423404</st></strong>
<strong class="bold"><st c="21159">AveBedrms        1.094775</st></strong>
<strong class="bold"><st c="21178">Population    1425.157323</st></strong>
<strong class="bold"><st c="21201">AveOccup         3.040518</st></strong>
<strong class="bold"><st c="21219">dtype: float</st><a id="_idTextAnchor937"/><st c="21232">64</st></strong></pre>			<ol>
				<li value="5"><st c="21235">Now, let’s determine the difference between the maximum and minimum values </st><span class="No-Break"><st c="21311">per variable:</st></span><pre class="source-code"><st c="21324">
ranges = X_train.max(axis=0)-X_train.min(axis=0)</st></pre><p class="list-inset"><st c="21373">By executing </st><strong class="source-inline"><st c="21387">print(ranges)</st></strong><st c="21400">, we display the value ranges </st><span class="No-Break"><st c="21430">per variable:</st></span></p><pre class="source-code"><strong class="bold"><st c="21443">MedInc</st></strong><strong class="bold"><st c="21450">           14.500200</st></strong>
<strong class="bold"><st c="21460">HouseAge         51.000000</st></strong>
<strong class="bold"><st c="21479">AveRooms        131.687179</st></strong>
<strong class="bold"><st c="21499">AveBedrms        33.733333</st></strong>
<strong class="bold"><st c="21519">Population    35679.000000</st></strong>
<strong class="bold"><st c="21543">AveOccup        598.964286</st></strong>
<strong class="bold"><st c="21563">dtype: float64</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="21578">Note</st></p>
			<p class="callout"><st c="21583">The </st><strong class="source-inline"><st c="21588">pandas</st></strong> <strong class="source-inline"><st c="21594">mean()</st></strong><st c="21601">, </st><strong class="source-inline"><st c="21603">max()</st></strong><st c="21608">, and </st><strong class="source-inline"><st c="21614">min()</st></strong><st c="21619"> methods return a </st><span class="No-Break"><strong class="source-inline"><st c="21637">pandas</st></strong></span><span class="No-Break"><st c="21643"> series.</st></span></p>
			<ol>
				<li value="6"><st c="21651">Now, we’ll</st><a id="_idIndexMarker545"/><st c="21662"> apply mean normalization to the train and test se</st><a id="_idTextAnchor938"/><st c="21712">ts by utilizing the </st><span class="No-Break"><st c="21733">learned parameters:</st></span><pre class="source-code"><st c="21752">
X_train_scaled = (X_train - means) / ranges
X_test_scaled = (X_test - means) / ranges</st></pre></li>			</ol>
			<p class="callout-heading"><st c="21838">Note</st></p>
			<p class="callout"><st c="21843">In order to transform future data, you will need to store these parameters, for example, in a </st><strong class="source-inline"><st c="21938">.txt</st></strong><st c="21942"> or </st><strong class="source-inline"><st c="21946">.</st></strong><span class="No-Break"><strong class="source-inline"><st c="21947">csv</st></strong></span><span class="No-Break"><st c="21951"> file.</st></span></p>
			<p class="list-inset"><em class="italic"><st c="21957">Step 6</st></em><st c="21964"> returns </st><strong class="source-inline"><st c="21973">pandas</st></strong><st c="21979"> DataFrames with the transformed train and test sets. </st><st c="22033">Go ahead and compare the variables before and after the transformations. </st><st c="22106">You’ll see that the distributions did not change, but the variables are centered at </st><strong class="source-inline"><st c="22190">0</st></strong><st c="22191">, and their v</st><a id="_idTextAnchor939"/><a id="_idTextAnchor940"/><st c="22204">alues lie between </st><strong class="source-inline"><st c="22223">-1</st></strong> <span class="No-Break"><st c="22225">and </st></span><span class="No-Break"><strong class="source-inline"><st c="22230">1</st></strong></span><span class="No-Break"><st c="22231">.</st></span></p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor941"/><st c="22232">How it works…</st></h2>
			<p><st c="22246">To implement mean normalization, we captured the mean values of the numerical variables in the train set using </st><strong class="source-inline"><st c="22358">mean()</st></strong><st c="22364"> from </st><strong class="source-inline"><st c="22370">pandas</st></strong><st c="22376">. Next, we determined the difference between the maximum and minimum values of the numerical variables in the train set by utilizing </st><strong class="source-inline"><st c="22509">max()</st></strong><st c="22514"> and </st><strong class="source-inline"><st c="22519">min()</st></strong><st c="22524"> from </st><strong class="source-inline"><st c="22530">pandas</st></strong><st c="22536">. Finally, we used the </st><strong class="source-inline"><st c="22559">pandas</st></strong><st c="22565"> series returned by these functions containing the mean values and the value ranges to normalize the train and test sets. </st><st c="22687">We subtracted the mean from each observation in our train and test sets</st><a id="_idIndexMarker546"/><st c="22758"> and divided the result by the value ranges. </st><st c="22803">This returned the normalized vari</st><a id="_idTextAnchor942"/><a id="_idTextAnchor943"/><st c="22836">ables in a </st><span class="No-Break"><strong class="source-inline"><st c="22848">pandas</st></strong></span><span class="No-Break"><st c="22854"> DataFrame.</st></span></p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor944"/><st c="22865">There’s more...</st></h2>
			<p><st c="22881">There is no dedicated scikit-learn transformer to implement mean normalization, but we can combine the use of two transformers to </st><span class="No-Break"><st c="23012">do so.</st></span></p>
			<p><st c="23018">To do this, we need to import </st><strong class="source-inline"><st c="23049">pandas</st></strong><st c="23055"> and load the data, just like we did in </st><em class="italic"><st c="23095">Steps 1</st></em><st c="23102"> to</st><em class="italic"><st c="23105"> 3</st></em><st c="23107"> in the </st><em class="italic"><st c="23115">How to do it...</st></em><st c="23130"> section of this recipe. </st><st c="23155">Then, follow </st><span class="No-Break"><st c="23168">these steps:</st></span></p>
			<ol>
				<li><st c="23180">Import the </st><span class="No-Break"><st c="23192">scikit-learn transformers:</st></span><pre class="source-code"><st c="23218">
from sklearn.preprocessing import (
    StandardScaler, RobustScaler
)</st></pre></li>				<li><st c="23285">Let’s set up </st><strong class="source-inline"><st c="23299">StandardScaler()</st></strong><st c="23315"> to learn and subtract </st><a id="_idTextAnchor945"/><st c="23338">the mean without dividing the result by the </st><span class="No-Break"><st c="23382">standard deviation:</st></span><pre class="source-code"><st c="23401">
scaler_mean = StandardScaler(
    with_mean=True, with_std=False,
).set_output(transform="pandas")</st></pre></li>				<li><st c="23496">Now, let’s set up </st><strong class="source-inline"><st c="23515">RobustScaler()</st></strong><st c="23529"> so that it does not remove the median from the values but divides them by the value range – that is, the difference between the maximum and </st><span class="No-Break"><st c="23670">minimum values:</st></span><pre class="source-code"><st c="23685">
scaler_minmax = RobustScaler(
    with_centering=False,
    with_scaling=True,
    quantile_range=(0, 100)
).set_output(transform="pandas")</st></pre></li>			</ol>
			<p class="callout-heading"><st c="23813">Note</st></p>
			<p class="callout"><st c="23818">To divide by the difference between the minimum and maximum values, we need to specify </st><strong class="source-inline"><st c="23906">(0, 100)</st></strong><st c="23914"> in the </st><strong class="source-inline"><st c="23922">quantile_range</st></strong><st c="23936"> argument </st><span class="No-Break"><st c="23946">of </st></span><span class="No-Break"><strong class="source-inline"><st c="23949">RobustScaler()</st></strong></span><span class="No-Break"><st c="23963">.</st></span></p>
			<ol>
				<li value="4"><st c="23964">Let’s fit the scalers to the train set so that they learn and store the mean, maximum, and </st><span class="No-Break"><st c="24056">minimum values:</st></span><pre class="source-code"><st c="24071">
scaler_mean.fit(X_train)
scaler_minmax.fit(X_train)</st></pre></li>				<li><st c="24123">Finally, let’s apply mean normalization to the train and </st><span class="No-Break"><st c="24181">test sets:</st></span><pre class="source-code"><st c="24191">
X_train_scaled = scaler_minmax.transform(
    scaler_mean.transform(X_train)
)
X_test_scaled = scaler_minmax.transform(
    scaler_mean.transform(X_test)
)</st></pre></li>			</ol>
			<p><st c="24339">We transformed the data with </st><strong class="source-inline"><st c="24369">StandardScaler()</st></strong><st c="24385"> to remove the mean and then transformed the resulting DataFrame with </st><strong class="source-inline"><st c="24455">RobustScaler()</st></strong><st c="24469"> to divide the result by the range between the minimum and maximum values. </st><st c="24544">We described the functionality of </st><strong class="source-inline"><st c="24578">StandardScaler()</st></strong><st c="24594"> in this chapter’s </st><em class="italic"><st c="24613">Standardizing the features</st></em><st c="24639"> recipe and </st><strong class="source-inline"><st c="24651">RobustScaler()</st></strong><st c="24665"> in the </st><em class="italic"><st c="24673">Scaling with the median and quant</st><a id="_idTextAnchor946"/><a id="_idTextAnchor947"/><st c="24706">iles</st></em><st c="24711"> recipe of </st><span class="No-Break"><st c="24722">this chapter.</st></span></p>
			<h1 id="_idParaDest-212"><st c="24735">Implementing maximum absol</st><a id="_idTextAnchor948"/><st c="24762">ute scaling</st></h1>
			<p><st c="24774">Maximum absolute scaling </st><a id="_idIndexMarker547"/><st c="24800">scales the data to its maximum value – that is, it divides every observation by the maximum value of </st><span class="No-Break"><st c="24901">the variable:</st></span></p>
			<p><img src="image/29.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;a&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.496em;height:1.312em;width:5.443em"/><st c="24914"/></p>
			<p><st c="24936">As a result, the maximum value of each feature will be </st><strong class="source-inline"><st c="24991">1.0</st></strong><st c="24994">. Note that maximum absolute scaling does not center the data, and hence, it’s suitable for scaling sparse data. </st><st c="25107">In this recipe, we will implement maximum absolute scaling </st><span class="No-Break"><st c="25166">with scikit-learn.</st></span></p>
			<p class="callout-heading"><st c="25184">Note</st></p>
			<p class="callout"><st c="25189">Scikit-learn recommends using this transformer on data that is cen</st><a id="_idTextAnchor949"/><a id="_idTextAnchor950"/><st c="25256">tered at </st><strong class="source-inline"><st c="25266">0</st></strong><st c="25267"> or on </st><span class="No-Break"><st c="25274">sparse data.</st></span></p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor951"/><st c="25286">Getting ready</st></h2>
			<p><st c="25300">Maximum absolute scaling</st><a id="_idIndexMarker548"/><st c="25325"> was specifically designed to scale sparse data. </st><st c="25374">Thus, we will use a bag-of-words dataset that contains sparse variables for the recipe. </st><st c="25462">In this dataset, the variables are words, the observations are documents, and the values are the number of times each word appears in the document. </st><st c="25610">Most entries </st><a id="_idTextAnchor952"/><st c="25623">in the data </st><span class="No-Break"><st c="25635">are </st></span><span class="No-Break"><strong class="source-inline"><st c="25639">0</st></strong></span><span class="No-Break"><st c="25640">.</st></span></p>
			<p><st c="25641">We will use a dataset consisting of a bag of words, which is available in the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Bag+of+Words), which is licensed under CC BY </st><span class="No-Break"><st c="25838">4.0 (</st></span><span class="No-Break"><st c="25843">https://creativecommons.org/licenses/by/4.0/legalcode</st></span><span class="No-Break"><st c="25897">).</st></span></p>
			<p><st c="25900">I downloaded and prepared a small bag of words representing a simplified version of one of those datasets. </st><st c="26008">You will find this dataset in the accompanying GitHub </st><span class="No-Break"><st c="26062">repository: </st></span><a href="https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/tree/main/ch07-scaling"><span class="No-Break"><st c="26074">https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/tree/main/ch07-scaling</st></span></a><span class="No-Break"><st c="26181">.</st></span></p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor953"/><st c="26182">How to do it...</st></h2>
			<p><st c="26198">Let’s begin by</st><a id="_idIndexMarker549"/><st c="26213"> importing the required packages and loading </st><span class="No-Break"><st c="26258">the dataset:</st></span></p>
			<ol>
				<li><st c="26270">Let’s import the required libraries and </st><span class="No-Break"><st c="26311">the scaler:</st></span><pre class="source-code"><st c="26322">
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MaxAbsScaler</st></pre></li>				<li><st c="26421">Let’s load the </st><span class="No-Break"><st c="26437">bag-of-words dataset:</st></span><pre class="source-code"><st c="26458">
data = pd.read_csv("bag_of_words.csv")</st></pre><p class="list-inset"><st c="26497">If we execute </st><strong class="source-inline"><st c="26512">data.head()</st></strong><st c="26523">, we will see the DataFrame consisting of the words as columns, the documents as rows, and the number of times each word appeared in a document </st><span class="No-Break"><st c="26667">as values:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B22396_07_5.jpg" alt="Figure 7.5 – DataFrame with the bag of words"/><st c="26677"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="26944">Figure 7.5 – DataFrame with the bag of words</st></p>
			<p class="callout-heading"><st c="26988">Note</st></p>
			<p class="callout"><st c="26993">Although we omit this step in the recipe, remember that the maximum absolute values should be learned from a training dataset only. </st><st c="27126">Split the dataset into train and test sets when carrying out </st><span class="No-Break"><st c="27187">your analysis.</st></span></p>
			<ol>
				<li value="3"><st c="27201">Let’s set</st><a id="_idIndexMarker550"/><st c="27211"> up </st><strong class="source-inline"><st c="27215">MaxAbsScaler()</st></strong><st c="27229"> and fit it to the data so that it learns the variables’ </st><span class="No-Break"><st c="27286">maximum values:</st></span><pre class="source-code"><st c="27301">
scaler = MaxAbsScaler().set_output(
    transform="pandas")
scaler.fit(data)</st></pre></li>				<li><st c="27374">Now, let’s scale the variables by utilizing the </st><span class="No-Break"><st c="27423">trained scaler:</st></span><pre class="source-code"><st c="27438">
data_scaled = scaler.transform(data)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="27475">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="27480">MaxAbsScaler ()</st></strong><st c="27496"> stores the maximum values in its </st><span class="No-Break"><strong class="source-inline"><st c="27530">max_ab</st><a id="_idTextAnchor954"/><st c="27536">s_</st></strong></span><span class="No-Break"><st c="27539"> attribute.</st></span></p>
			<ol>
				<li value="5"><st c="27550">Let’s display the maximum values stored by </st><span class="No-Break"><st c="27594">the scaler:</st></span><pre class="source-code"><st c="27605">
scaler.max_abs_</st></pre><p class="list-inset"><st c="27621">In the following output, we see the maximum number of times each word appeared in </st><span class="No-Break"><st c="27704">a document:</st></span></p><pre class="source-code"><strong class="bold"><st c="27715">array([ 7.,  6.,  2.,  2., 11.,  4.,  3.,  6., 52.,  2.])</st></strong></pre><p class="list-inset"><st c="27766">To follow up, let’s plot the distributions of the original and </st><span class="No-Break"><st c="27830">scaled variables.</st></span></p></li>				<li><st c="27847">Let’s make a histogram with the bag of words before </st><span class="No-Break"><st c="27900">the scaling:</st></span><pre class="source-code"><st c="27912">
data.hist(bins=20, figsize=(20, 20))
plt.show()</st></pre><p class="list-inset"><st c="27960">In the </st><a id="_idIndexMarker551"/><st c="27968">following output, we see histograms with the number of times e</st><a id="_idTextAnchor955"/><st c="28030">ach word appears in </st><span class="No-Break"><st c="28051">a document:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B22396_07_6.jpg" alt="Figure 7.6 – Histograms with differen﻿t word counts"/><st c="28062"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="28575">Figure 7.6 – Histograms with differen</st><a id="_idTextAnchor956"/><st c="28612">t word counts</st></p>
			<ol>
				<li value="7"><st c="28626">Now, let’s make a histogram with the </st><span class="No-Break"><st c="28664">scaled variables:</st></span><pre class="source-code"><st c="28681">
data_scaled.hist(bins=20, figsize=(20, 20))
plt.show()</st></pre><p class="list-inset"><st c="28736">In the following output, we can corroborate the change of scale of the v</st><a id="_idTextAnchor957"/><st c="28809">ariables, but their </st><a id="_idIndexMarker552"/><st c="28830">distr</st><a id="_idTextAnchor958"/><st c="28835">ibution shape remains </st><span class="No-Break"><st c="28858">the same:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B22396_07_7.jpg" alt="Figure 7.7 – Histograms of the word counts after the scaling"/><st c="28867"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="29068">Figure 7.7 – Histograms of the word counts after the scaling</st></p>
			<p><st c="29128">With scaling to the maximum absolute value, we linearly scale down </st><a id="_idTextAnchor959"/><a id="_idTextAnchor960"/><st c="29196">the magnitude of </st><span class="No-Break"><st c="29213">the features.</st></span></p>
			<p><st c="29226">How </st><span class="No-Break"><st c="29231">it works...</st></span></p>
			<p><st c="29242">In this recipe, we</st><a id="_idIndexMarker553"/><st c="29261"> scaled the sparse variables of a bag of words to their absolute maximum values by using </st><strong class="source-inline"><st c="29350">MaxAbsScaler()</st></strong><st c="29364">. With </st><strong class="source-inline"><st c="29371">fit()</st></strong><st c="29376">, the scaler learned the maximum absolute values for each variable and stored them in its </st><strong class="source-inline"><st c="29466">max_abs_</st></strong><st c="29474"> attribute. </st><st c="29486">With </st><strong class="source-inline"><st c="29491">transform()</st></strong><st c="29502">, the scaler divided the variables by their ab</st><a id="_idTextAnchor961"/><st c="29548">solute maximum values, returning a </st><span class="No-Break"><strong class="source-inline"><st c="29584">pandas</st></strong></span><span class="No-Break"><st c="29590"> DataFrame.</st></span></p>
			<p class="callout-heading"><st c="29601">Note</st></p>
			<p class="callout"><st c="29606">Remember that you can change the output container to a NumPy array or a </st><strong class="source-inline"><st c="29679">polars</st></strong><st c="29685"> DataFrame through the </st><strong class="source-inline"><st c="29708">set_output()</st></strong><st c="29720"> method of the scik</st><a id="_idTextAnchor962"/><a id="_idTextAnchor963"/><st c="29739">it-learn </st><span class="No-Break"><st c="29749">library’s transformers.</st></span></p>
			<h2 id="_idParaDest-215"><st c="29772">There’s m</st><a id="_idTextAnchor964"/><st c="29782">ore...</st></h2>
			<p><st c="29789">If you want to center the variables’ distribution at </st><strong class="source-inline"><st c="29843">0</st></strong><st c="29844"> and then scale them to their absolute maximum, you can do so by combining the use of two scikit-learn transformers within </st><span class="No-Break"><st c="29967">a pipeline:</st></span></p>
			<ol>
				<li><st c="29978">Let’s import the required libraries, transformers, </st><span class="No-Break"><st c="30030">and functions:</st></span><pre class="source-code"><st c="30044">
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import (
    MaxAbsScaler, StandardScaler)
from sklearn.pipeline import Pipeline</st></pre></li>				<li><st c="30275">Let’s load the California housing dataset and split it into train and </st><span class="No-Break"><st c="30346">test sets:</st></span><pre class="source-code"><st c="30356">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)
X.drop( labels=[ "Latitude",
    "Longitude"], axis=1, inplace=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)</st></pre></li>				<li><st c="30576">Let’s set up </st><strong class="source-inline"><st c="30590">StandardScaler()</st></strong><st c="30606"> from scikit-learn so that it learns and subtracts the mean but does not divide the result by the </st><span class="No-Break"><st c="30704">standard deviation:</st></span><pre class="source-code"><st c="30723">
scaler_mean = StandardScaler(
    with_mean=True, with_std=False)</st></pre></li>				<li><st c="30785">Now, let’s set up </st><strong class="source-inline"><st c="30804">MaxAbsScaler()</st></strong><st c="30818"> with its </st><span class="No-Break"><st c="30828">default parameters:</st></span><pre class="source-code"><st c="30847">
scaler_maxabs = MaxAbsScaler()</st></pre></li>				<li><st c="30878">Let’s include both scalers within a pipeline that returns </st><span class="No-Break"><st c="30937">pandas DataFrames:</st></span><pre class="source-code"><st c="30955">
scaler = Pipeline([
    ("scaler_mean", scaler_mean),
    ("scaler_max", scaler_maxabs),
]).set_output(transform="pandas")</st></pre></li>				<li><st c="31070">Let’s fit the scalers to the train set so that they learn the </st><span class="No-Break"><st c="31133">required parameters:</st></span><pre class="source-code"><st c="31153">
s</st><a id="_idTextAnchor965"/><st c="31155">caler.fit(X_train)</st></pre></li>				<li><st c="31173">Finally, let’s transform the train and </st><span class="No-Break"><st c="31213">test sets:</st></span><pre class="source-code"><st c="31223">
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)</st></pre><p class="list-inset"><st c="31307">The pipeline applies </st><strong class="source-inline"><st c="31329">StandardScaler()</st></strong><st c="31345"> and </st><strong class="source-inline"><st c="31350">MaxAbsScaler()</st></strong><st c="31364"> in sequence to first remove the mean and then scale </st><a id="_idTextAnchor966"/><st c="31417">the resulting </st><a id="_idTextAnchor967"/><st c="31431">va</st><a id="_idTextAnchor968"/><a id="_idTextAnchor969"/><st c="31433">riables to their </st><span class="No-Break"><st c="31451">maximum values.</st></span></p></li>			</ol>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor970"/><st c="31466">Scaling to vector unit length</st></h1>
			<p><st c="31496">Scaling to the vector unit length involves scaling individual observations (not features) to have a unit norm. </st><st c="31608">Each sample (that is, each row of the data) is rescaled independently of other samples so that its norm equals one. </st><st c="31724">Each </st><a id="_idIndexMarker554"/><st c="31729">row constitutes a </st><strong class="bold"><st c="31747">feature vector</st></strong><st c="31761"> containing the values of every variable for that row. </st><st c="31816">Hence, with this scaling method, we rescale the </st><span class="No-Break"><st c="31864">feature vector.</st></span></p>
			<p><st c="31879">The norm of a vector is a measure of its magnitude or length in a given space and it can be determined by using the Manhattan (</st><em class="italic"><st c="32007">l1</st></em><st c="32010">) or the Euclidean (</st><em class="italic"><st c="32031">l2</st></em><st c="32034">) distance. </st><st c="32047">The Manhattan distance is given by the sum of the absolute components of </st><span class="No-Break"><st c="32120">the vector:</st></span></p>
			<p><img src="image/30.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mfenced open=&quot;|&quot; close=&quot;|&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;..&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.383em;height:1.274em;width:11.278em"/><st c="32131"/></p>
			<p><st c="32163">The Euclidean distance is given by the square root of the square sum of the component of </st><span class="No-Break"><st c="32252">the vector:</st></span></p>
			<p><img src="image/31.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.380em;height:1.253em;width:10.323em"/><st c="32263"/></p>
			<p><st c="32284">Here, </st><img src="image/32.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.333em;height:0.781em;width:2.662em"/><st c="32290"/><st c="32301">and </st><img src="image/33.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.340em;height:0.788em;width:0.999em"/><st c="32305"/><st c="32306">are the values of variables </st><em class="italic"><st c="32334">1</st></em><st c="32335">, </st><em class="italic"><st c="32337">2</st></em><st c="32338">, and </st><em class="italic"><st c="32344">n</st></em><st c="32345"> for each observation. </st><st c="32368">Scaling to unit norm consists of dividing each feature vector’s value by either </st><em class="italic"><st c="32448">l1</st></em><st c="32450"> or </st><em class="italic"><st c="32454">l2</st></em><st c="32456">, so that after the</st><a id="_idIndexMarker555"/><st c="32475"> scaling, the norm of the feature vector is </st><em class="italic"><st c="32519">1</st></em><st c="32520">. To be clear, we divide each of </st><img src="image/34.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.333em;height:0.781em;width:2.626em"/><st c="32553"/><st c="32564">and </st><img src="image/35.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.340em;height:0.788em;width:1.215em"/><st c="32568"/><st c="32569">by </st><em class="italic"><st c="32572">l1</st></em> <span class="No-Break"><st c="32574">or </st></span><span class="No-Break"><em class="italic"><st c="32578">l2</st></em></span><span class="No-Break"><st c="32580">.</st></span></p>
			<p><st c="32581">This scaling procedure changes the variables’ distribution, as illustrated in the </st><span class="No-Break"><st c="32664">following figure:</st></span></p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B22396_07_8.jpg" alt="Figure 7.8 – Distribution of a normal and skewed variable before and after scaling each observation’s feature vector to its norm"/><st c="32681"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="33144">Figure 7.8 – Distribution of a normal and skewed variable before and after scaling each observation’s feature vector to its norm</st></p>
			<p class="callout-heading"><st c="33272">Note</st></p>
			<p class="callout"><st c="33277">This scaling technique scales each observation and not each variable. </st><st c="33348">The scaling methods that we discussed so far in this chapter aimed at shifting and resetting the scale of the variables’ distribution. </st><st c="33483">When we scale to the unit length, however, we normalize each observation individually, contemplating </st><a id="_idTextAnchor971"/><st c="33584">their values across</st><a id="_idTextAnchor972"/> <span class="No-Break"><st c="33603">all features.</st></span></p>
			<p><st c="33617">Scaling </st><a id="_idIndexMarker556"/><st c="33626">to the unit norm can be used when utilizing kernels to quantify similarity for text classification and clustering. </st><st c="33741">In this recipe, we will scale each observation’s feature vector to a un</st><a id="_idTextAnchor973"/><a id="_idTextAnchor974"/><st c="33812">it length of </st><strong class="source-inline"><st c="33826">1</st></strong> <span class="No-Break"><st c="33827">using scikit-learn.</st></span></p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor975"/><st c="33846">How to do it...</st></h2>
			<p><st c="33862">To begin, we’ll </st><a id="_idIndexMarker557"/><st c="33879">import the required packages, load the dataset, and prepare the train and </st><span class="No-Break"><st c="33953">test sets:</st></span></p>
			<ol>
				<li><st c="33963">Let’s import the required Python packages, classes, </st><span class="No-Break"><st c="34016">and functions:</st></span><pre class="source-code"><st c="34030">
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import Normalizer</st></pre></li>				<li><st c="34221">Let’s load the California housing dataset into a </st><span class="No-Break"><strong class="source-inline"><st c="34271">pandas</st></strong></span><span class="No-Break"><st c="34277"> DataFrame:</st></span><pre class="source-code"><st c="34288">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)
X.drop(labels=[
    "Latitude", "Longitude"], axi</st><a id="_idTextAnchor976"/><st c="34399">s=1, inplace=True)</st></pre></li>				<li><st c="34418">Let’s divide the data into train and </st><span class="No-Break"><st c="34456">test sets:</st></span><pre class="source-code"><st c="34466">
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)</st></pre></li>				<li><st c="34556">Let’s set up the scikit-learn library’s </st><strong class="source-inline"><st c="34597">Normalizer()</st></strong><st c="34609"> transformer to scale each observation to the Manhattan distance </st><span class="No-Break"><st c="34674">or </st></span><span class="No-Break"><strong class="source-inline"><st c="34677">l1</st></strong></span><span class="No-Break"><st c="34679">:</st></span><pre class="source-code"><st c="34681">
scaler = Normalizer(norm='l1')</st></pre></li>			</ol>
			<p class="callout-heading"><st c="34712">Note</st></p>
			<p class="callout"><st c="34717">To normalize to the Euclidean distance, you need to set the norm to </st><strong class="source-inline"><st c="34786">l2</st></strong><st c="34788"> using </st><strong class="source-inline"><st c="34795">scaler = </st></strong><span class="No-Break"><strong class="source-inline"><st c="34804">Normalizer(no</st><a id="_idTextAnchor977"/><st c="34817">rm='l2')</st></strong></span><span class="No-Break"><st c="34826">.</st></span></p>
			<ol>
				<li value="5"><st c="34827">Let’s </st><a id="_idIndexMarker558"/><st c="34834">transform the train and test sets – that is, we’ll divide each observation’s feature vector by </st><span class="No-Break"><st c="34929">its norm:</st></span><pre class="source-code"><st c="34938">
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)</st></pre><p class="list-inset"><st c="35026">We can calculate the length (that is, the Manhattan distance of each observation’s feature vector) using </st><strong class="source-inline"><st c="35132">linalg()</st></strong> <span class="No-Break"><st c="35140">from NumPy.</st></span></p></li>				<li><st c="35152">Let’s calculate the norm (Manhattan distance) before scaling </st><span class="No-Break"><st c="35214">the variables:</st></span><pre class="source-code"><st c="35228">
np.round(np.linalg.norm(X_train, ord=1, axis=1), 1)</st></pre><p class="list-inset"><st c="35280">As expected, the norm of each </st><span class="No-Break"><st c="35311">observation varies:</st></span></p><pre class="source-code"><strong class="bold"><st c="35330">array([ 255.3,  889.1, 1421.7, ...,  744.6, 1099.5,</st></strong>
<strong class="bold"><st c="35380">           1048.9])</st></strong></pre></li>				<li><st c="35389">Let’s now calculate the norm after </st><span class="No-Break"><st c="35425">the scaling:</st></span><pre class="source-code"><st c="35437">
np.round(np.linalg.norm(
    X_train_scaled, ord=1, axis=1), 1)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="35497">Note</st></p>
			<p class="callout"><st c="35502">You need to set </st><strong class="source-inline"><st c="35519">ord=1</st></strong><st c="35524"> for the Manhattan distance and </st><strong class="source-inline"><st c="35556">ord=2</st></strong><st c="35561"> for the Euclidean distance as arguments of NumPy’s </st><strong class="source-inline"><st c="35613">linalg()</st></strong><st c="35621">function, depending on whether you scaled the features to the </st><strong class="source-inline"><st c="35684">l1</st></strong><st c="35686"> or </st><span class="No-Break"><strong class="source-inline"><st c="35690">l2</st></strong></span><span class="No-Break"><st c="35692"> norm.</st></span></p>
			<p class="list-inset"><st c="35698">We see that the Manhattan distance of each feature vector is </st><strong class="source-inline"><st c="35760">1</st></strong> <span class="No-Break"><st c="35761">after scaling:</st></span></p>
			<pre class="source-code">
<a id="_idTextAnchor978"/><st c="35775">array([1., 1., 1., ..., 1., 1., 1.])</st></pre>			<p><st c="35812">Based on the scikit-learn library’s documentation, this scaling method can be useful when using a quadratic form such as the dot-product or any other kernel to quantify </st><a id="_idTextAnchor979"/><a id="_idTextAnchor980"/><st c="35982">the similarity of a pair </st><span class="No-Break"><st c="36007">of samples.</st></span></p>
			<h2 id="_idParaDest-218"><st c="36018">How it wo</st><a id="_idTextAnchor981"/><st c="36028">rks...</st></h2>
			<p><st c="36035">In this recipe, we</st><a id="_idIndexMarker559"/><st c="36054"> scaled the observations from the California housing dataset to their feature vector unit norm by utilizing the Manhattan or Euclidean distance. </st><st c="36199">To scale the feature vectors, we created an instance of </st><strong class="source-inline"><st c="36255">Normalizer()</st></strong><st c="36267"> from scikit-learn and set the norm to </st><strong class="source-inline"><st c="36306">l1</st></strong><st c="36308"> for the Manhattan distance. </st><st c="36337">For the Euclidean distance, we set the norm to </st><strong class="source-inline"><st c="36384">l2</st></strong><st c="36386">. Then, we applied the </st><strong class="source-inline"><st c="36409">fit()</st></strong><st c="36414"> method, although there were no parameters to be learned, as this normalization procedure depends exclusively on the values of the features for each observation. </st><st c="36576">Finally, with the </st><strong class="source-inline"><st c="36594">transform()</st></strong><st c="36605"> method, the scaler divided each observation’s feat</st><a id="_idTextAnchor982"/><st c="36656">ure vector by its norm. </st><st c="36681">This returned </st><a id="_idTextAnchor983"/><st c="36695">a NumPy array with the scaled dataset. </st><st c="36734">After the scaling, we used NumPy’s </st><strong class="source-inline"><st c="36769">linalg.norm</st></strong><st c="36780"> function to calculate the norm (</st><strong class="source-inline"><st c="36813">l1</st></strong><st c="36816"> and </st><strong class="source-inline"><st c="36821">l2</st></strong><st c="36823">) of each vector to confirm that after the transformation, it </st><span class="No-Break"><st c="36886">was </st></span><span class="No-Break"><strong class="source-inline"><st c="36890">1</st></strong></span><span class="No-Break"><st c="36891">.</st></span></p>
		</div>
	<div id="charCountTotal" value="36892"/></body></html>