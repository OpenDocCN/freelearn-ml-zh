- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Data for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine embarking on a journey through an expansive ocean of data, where within
    this vastness are untold stories, patterns, and insights waiting to be discovered.
    Welcome to the world of data exploration in **machine learning** (**ML**). In
    this chapter, I encourage you to put on your analytical lenses as we embark on
    a thrilling quest. Here, we will delve deep into the heart of your data, armed
    with powerful techniques and heuristics, to uncover its secrets. As you embark
    on this adventure, you will discover that beneath the surface of raw numbers and
    statistics, there exists a treasure trove of patterns that, once revealed, can
    transform your data into a valuable asset. The journey begins with **exploratory
    data analysis** (**EDA**), a crucial phase where we unravel the mysteries of data,
    laying the foundation for automated labeling and, ultimately, building smarter
    and more accurate ML models. In this age of **generative AI**, the preparation
    of quality training data is essential to the fine-tuning of domain-specific **large
    language models (LLMs)**. Fine-tuning involves the curation of additional domain-specific
    labeled data for training publicly available LLMs. So, fasten your seatbelts for
    a captivating voyage into the art and science of data exploration for **data labeling**.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s start with the question: What is data exploration? It is the initial
    phase of data analysis, where raw data is examined, visualized, and summarized
    to uncover patterns, trends, and insights. It serves as a crucial step in understanding
    the nature of the data before applying advanced analytics or ML techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore tabular data using various libraries and packages
    in Python, including Pandas, NumPy, and Seaborn. We will also plot different bar
    charts and histograms to visualize data to find the relationships between various
    features, which is useful for labeling data. We will be exploring the *Income*
    dataset located in this book’s GitHub repository (a link for which is located
    in the *Technical requirements* section). A good understanding of the data is
    necessary in order to define business rules, identify matching patterns, and,
    subsequently, label the data using Python labeling functions.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we will be able to generate summary statistics for
    the given dataset. We will derive aggregates of the features for each target group.
    We will also learn how to perform univariate and bivariate analyses of the features
    in the given dataset. We will create a report using the `ydata-profiling` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: EDA and data labeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary statistics and data aggregates with Pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization with Seaborn for univariate and bivariate analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling data using the `ydata-profiling` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlocking insights from data with OpenAI and LangChain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the following Python IDE and software tools needs to be installed before
    running the notebook in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Anaconda Navigator**: Download and install the open source Anaconda Navigator
    from the following URL:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.anaconda.com/navigator/install/#system-requirements](https://docs.anaconda.com/navigator/install/#system-requirements)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Jupyter Notebook**: Download and install Jupyter Notebook:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://jupyter.org/install](https://jupyter.org/install)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can also use open source, online Python editors such as **Google Colab**
    ([https://colab.research.google.com/](https://colab.research.google.com/)) or
    **Replit** ([https://replit.com/](https://replit.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Python source code and the entire notebook created in this chapter are
    available in this book’s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python](https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python)'
  prefs: []
  type: TYPE_NORMAL
- en: You also need to create an Azure account and add an OpenAI resource for working
    with generative AI. To sign up for a free Azure subscription, visit [https://azure.microsoft.com/free](https://azure.microsoft.com/free).
    To request access to the Azure OpenAI service, visit [https://aka.ms/oaiapply](https://aka.ms/oaiapply).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have provisioned the Azure OpenAI service, deploy the LLM model –
    either GPT-3.5-Turbo or GPT 4.0 – from Azure OpenAI Studio. Then copy the keys
    for OpenAI from OpenAI Studio and set up the following environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Your endpoint should look like this: [https://YOUR_RESOURCE_NAME.openai.azure.com/](https://YOUR_RESOURCE_NAME.openai.azure.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: EDA and data labeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will gain an understanding of what EDA is. We will see why
    we need to perform it and discuss its advantages. We will also look at the life
    cycle of an ML project and learn about the role of data labeling in this cycle.
  prefs: []
  type: TYPE_NORMAL
- en: EDA comprises **data discovery**, **data collection**, **data cleaning**, and
    **data exploration**. These steps are part of any machine learning project. The
    data exploration step includes tasks such as data visualization, summary statistics,
    correlation analysis, and data distribution analysis. We will dive deep into these
    steps in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some real-world examples of EDA:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Customer churn analysis**: Suppose you work for a telecommunications company
    and you want to understand why customers are churning (canceling their subscriptions);
    in this case, conducting EDA on customer churn data can provide valuable insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Income data analysis**: EDA on the *Income* dataset with predictive features
    such as education, employment status, and marital status helps to predict whether
    the salary of a person is greater than $50K.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EDA is a critical process for any ML or data science project, and it allows
    us to understand the data and gain some valuable insights into the data domain
    and business.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use various Python libraries, such as Pandas, and call
    the `describe` and `info` functions on Pandas to generate data summaries. We will
    discover anomalies in the data and any outliers in the given dataset. We will
    also figure out various data types and any missing values in the data. We will
    understand whether any data type conversions are required, such as converting
    `string` to `float`, for performing further analysis. We will also analyze the
    data formats and see whether any transformations are required to standardize them,
    such as the date format. We will analyze the counts of different labels and understand
    whether the dataset is balanced or imbalanced. We will understand the relationships
    between various features in the data and calculate the correlations between features.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we will understand the patterns in the given dataset and also
    identify the relationships between various features in the data samples. Finally,
    we will come up with a strategy and domain rules for data cleaning and transformation.
    This helps us to predict labels for unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: We will plot various data visualizations using Python libraries such as `seaborn`
    and `matplotlib`. We will create bar charts, histograms, heatmaps, and various
    charts to visualize the importance of features in the dataset and how they depend
    on each other.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the ML project life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the major steps in an ML project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – ML project life cycle diagram](img/B18944_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – ML project life cycle diagram
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the business problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in every ML project is to understand the business problem and
    define clear goals that can be measured at the end of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Data discovery and data collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, you identify and gather potential data sources that may be relevant
    to your project’s objectives. This involves finding datasets, databases, APIs,
    or any other sources that may contain the data needed for your analysis and modeling.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of **data discovery** is to understand the landscape of available data
    and assess its quality, relevance, and potential limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Data discovery can also involve discussions with domain experts and stakeholders
    to identify what data is essential for solving business problems or achieving
    the project’s goals.
  prefs: []
  type: TYPE_NORMAL
- en: After identifying various sources for data, data engineers will develop data
    pipelines to extract and load the data to the target data lake and perform some
    data preprocessing tasks such as data cleaning, de-duplication, and making data
    readily available to ML engineers and data scientists for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data exploration** follows data discovery and is primarily focused on understanding
    the data, gaining insights, and identifying patterns or anomalies.'
  prefs: []
  type: TYPE_NORMAL
- en: During data exploration, you may perform basic statistical analysis, create
    data visualizations, and conduct initial observations to understand the data’s
    characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration can also involve identifying missing values, outliers, and
    potential data quality issues, but it typically does not involve making systematic
    changes to the data.
  prefs: []
  type: TYPE_NORMAL
- en: During data exploration, you assess the available labeled data and determine
    whether it’s sufficient for your ML task. If you find that the labeled data is
    small and insufficient for model training, you may identify the need for additional
    labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Data labeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data labeling** involves acquiring or generating more labeled examples to
    supplement your training dataset. You may need to manually label additional data
    points or use programming techniques such as data augmentation to expand your
    labeled dataset. The process of assigning labels to data samples is called **data
    annotation** or data labeling.'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, it is too expensive or time-consuming to outsource the manual
    data labeling task. Also, data is often not allowed to be shared with external
    third-party organizations due to data privacy. So, automating the data labeling
    process with an in-house development team using Python helps to label the data
    quickly and at an affordable cost.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the data science books available on the market are lacking information
    about this important step. So, this book aims to address the various methods to
    programmatically label data using Python as well as the annotation tools available
    on the market.
  prefs: []
  type: TYPE_NORMAL
- en: After obtaining a sufficient amount of labeled data, you proceed with traditional
    data preprocessing tasks, such as handling missing values, encoding features,
    scaling, and feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the data is adequately prepared, then that dataset is fed into the model
    by ML engineers to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the model is trained, the next step is to evaluate the model on a validation
    dataset to see how good the model is and avoid bias and overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: You can evaluate the model’s performance using various metrics and techniques
    and iterate on the model-building process as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, you deploy your model into production and monitor for continuous improvement
    using **ML Operations** (**MLOps**). MLOps aims to streamline the process of taking
    ML models to production and maintaining and monitoring them.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will focus on data labeling. In a real-world project, the datasets
    that sources provide us with for analytics and ML are not clean and not labeled.
    So, we need to explore unlabeled data to understand correlations and patterns
    and help us define the rules for data labeling using Python labeling functions.
    Data exploration helps us to understand the level of cleaning and transformation
    required before starting data labeling and model training.
  prefs: []
  type: TYPE_NORMAL
- en: This is where Python helps us to explore and perform a quick analysis of raw
    data using various libraries (such as Pandas, Seaborn, and ydata-profiling libraries),
    otherwise known as EDA.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Pandas DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pandas is an open source library used for data analysis and manipulation. It
    provides various functions for data wrangling, cleaning, and merging operations.
    Let us see how to explore data using the `pandas` library. For this, we will use
    the *Income* dataset located on GitHub and explore it to find the following insights:'
  prefs: []
  type: TYPE_NORMAL
- en: How many unique values are there for age, education, and profession in the *Income*
    dataset? What are the observations for each unique age?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary statistics such as mean value and quantile values for each feature.
    What is the average age of the adult for the income range > $50K?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is income dependent on independent variables such as age, education, and
    profession using bivariate analysis?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us first read the data into a DataFrame using the `pandas` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'A DataFrame is a structure that represents two-dimensional data with columns
    and rows, and it is similar to a SQL table. To get started, ensure that you create
    the `requirements.txt` file and add the required Python libraries as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Contents of the requirements.txt file](img/B18944_01_02..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Contents of the requirements.txt file
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, run the following command from your Python notebook cell to install the
    libraries added in the `requirements.txt` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s import the required Python libraries using the following `import`
    statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, in the following code snippet, we are reading the `adult_income.csv`
    file and writing to the DataFrame (`df`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now the data is loaded to `df`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see the size of the DataFrame using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see the shape of the DataFrame as a result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Shape of the DataFrame](img/B18944_01_03..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – Shape of the DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: So, we can see that there are 32,561 observations (rows) and 15 features (columns)
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us print the 15 column names in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – The names of the columns in our dataset](img/B18944_01_04..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – The names of the columns in our dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see the first five rows of the data in the dataset with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the output in *Figure 1**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – The first five rows of data](img/B18944_01_05..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – The first five rows of data
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the last five rows of the dataset using `tail`, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – The last five rows of data](img/B18944_01_06..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – The last five rows of data
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, `education` and `education.num` are redundant columns, as `education.num`
    is just the ordinal representation of the `education` column. So, we will remove
    the redundant `education.num` column from the dataset as one column is enough
    for model training. We will also drop the `race` column from the dataset using
    the following code snippet as we will not use it here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, `axis = 1` refers to the columns axis, which means that you are specifying
    that you want to drop a column. In this case, you are dropping the columns labeled
    `education.num` and `race` from the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s print the columns using `info()` to make sure the `race` and `education.num`
    columns are dropped from the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Columns in the DataFrame](img/B18944_01_07..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – Columns in the DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: We can see in the preceding data there are now only 13 columns as we deleted
    2 of them from the previous total of 15 columns.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have seen what a Pandas DataFrame is and loaded a CSV dataset
    into one. We also saw the various columns in the DataFrame and their data types.
    In the following section, we will generate the summary statistics for the important
    features using Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Summary statistics and data aggregates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will derive the summary statistics for numerical columns.
  prefs: []
  type: TYPE_NORMAL
- en: Before generating summary statistics, we will identify the categorical columns
    and numerical columns in the dataset. Then, we will calculate the summary statistics
    for all numerical columns.
  prefs: []
  type: TYPE_NORMAL
- en: We will also calculate the mean value of each numerical column for the target
    class. Summary statistics are useful to gain insights about each feature’s mean
    values and their effect on the target label class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s print the `categorical` columns using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – Categorical columns](img/B18944_01_08..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – Categorical columns
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s print the `numerical` columns using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Numerical columns](img/B18944_01_09..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 – Numerical columns
  prefs: []
  type: TYPE_NORMAL
- en: Summary statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s generate summary statistics (i.e., mean, standard deviation, minimum
    value, maximum value, and lower (25%), middle (50%), and higher (75%) percentiles)
    using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10 – Summary statistics](img/B18944_01_10..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 – Summary statistics
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the results, the mean value of `age` is 38.5 years, the minimum
    age is 17 years, and the maximum age is 90 years in the dataset. As we have only
    five numerical columns in the dataset, we can only see five rows in this summary
    statistics table.
  prefs: []
  type: TYPE_NORMAL
- en: Data aggregates of the feature for each target class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s calculate the average age of the people for each income group range
    using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – Average age by income group](img/B18944_01_11..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 – Average age by income group
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the results, we have used the `groupby` clause on the target variable
    and calculated the mean of the age in each group. The mean age is 36.78 for people
    with an income group of less than or equal to $50K. Similarly, the mean age is
    44.2 for the income group greater than $50K.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s calculate the average hours per week of the people for each income
    group range using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12 – Average hours per week by income group](img/B18944_01_12..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 – Average hours per week by income group
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the results, the average hours per week for the income group =<
    $50K is 38.8 hours. Similarly, the average hours per week for the income group
    > $50K is 45.47 hours.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can write a generic reusable function for calculating the
    `numerical` column group by the `categorical` column as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to get aggregations of multiple columns for each target income group,
    then we can calculate aggregations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13 – Aggregations for multiple columns](img/B18944_01_13..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 – Aggregations for multiple columns
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the results, we have calculated the summary statistics for age and
    hours per week for each income group.
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to calculate the aggregate values of features for the target
    group using reusable functions. This aggregate value gives us a correlation of
    those features for the target label value.
  prefs: []
  type: TYPE_NORMAL
- en: Creating visualizations using Seaborn for univariate and bivariate analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to explore each variable separately. We are going
    to summarize the data for each feature and analyze the pattern present in it.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate analysis is an analysis using individual features. We will also perform
    a bivariate analysis later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let us do a univariate analysis for the age, education, work class, hours
    per week, and occupation features.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s get the counts of unique values for each column using the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 1.14 – Unique values for each column](img/B18944_01_14..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14 – Unique values for each column
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the results, there are 73 unique values for `age`, 9 unique values
    for `workclass`, 16 unique values for `education`, 15 unique values for `occupation`,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us see the unique values count for `age` in the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.15 – Value counts for age](img/B18944_01_15..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15 – Value counts for age
  prefs: []
  type: TYPE_NORMAL
- en: We can see in the results that there are 898 observations (rows) with the age
    of 36\. Similarly, there are 6 observations with the age of 83.
  prefs: []
  type: TYPE_NORMAL
- en: Histogram of age
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Histograms are used to visualize the distribution of continuous data. Continuous
    data is data that can take on any value within a range (e.g., age, height, weight,
    temperature, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us plot a histogram using Seaborn to see the distribution of `age` in the
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.16 – The histogram of age](img/B18944_01_16..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16 – The histogram of age
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the age histogram, there are many people in the age range of
    23 to 45 in the given observations in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Bar plot of education
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let us check the distribution of `education` in the given dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 1.17 – The bar chart of education](img/B18944_01_17..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.17 – The bar chart of education
  prefs: []
  type: TYPE_NORMAL
- en: As we see, the `HS.grad` count is higher than that for the `Bachelors` degree
    holders. Similarly, the `Masters` degree holders count is lower than the `Bachelors`
    degree holders count.
  prefs: []
  type: TYPE_NORMAL
- en: Bar chart of workclass
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let’s see the distribution of `workclass` in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the bar chart to visualize the distribution of different values
    of `workclass`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.18 – Bar chart of workclass](img/B18944_01_18..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.18 – Bar chart of workclass
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the `workclass` bar chart, there are more private employees than
    other kinds.
  prefs: []
  type: TYPE_NORMAL
- en: Bar chart of income
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s see the unique value for the `income` target variable and see the distribution
    of `income`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.19 – Distribution of income](img/B18944_01_19..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.19 – Distribution of income
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the results, there are 24,720 observations with an income greater
    than $50K and 7,841 observations with an income of less than $50K. In the real
    world, more people have an income greater than $50K and a small portion of people
    have less than $50K income, assuming the income is in US dollars and for 1 year.
    As this ratio closely reflects the real-world scenario, we do not need to balance
    the minority class dataset using synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.20 – Bar chart of income](img/B18944_01_20..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.20 – Bar chart of income
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have seen the size of the data, column names, and data types,
    and the first and last five rows of the dataset. We also dropped some unnecessary
    columns. We performed univariate analysis to see the unique value counts and plotted
    the bar charts and histograms to understand the distribution of values for important
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: Bivariate analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s do a bivariate analysis of age and income to find the relationship between
    them. Bivariate analysis is the analysis of two variables to find the relationship
    between them. We will plot a histogram using the Python Seaborn library to visualize
    the relationship between `age` and `income`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.21 – Histogram of age with income](img/B18944_01_21..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.21 – Histogram of age with income
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding histogram, we can see that income is greater than $50K for
    the age group between 30 and 60\. Similarly, for the age group less than 30, income
    is less than $50K.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s plot the histogram to do a bivariate analysis of `education` and
    `income`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.22 – Histogram of education with income](img/B18944_01_22..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.22 – Histogram of education with income
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding histogram, we can see that income is greater than $50K for
    the majority of the `Masters` education adults. On the other hand, income is less
    than $50K for the majority of `HS-grad adults`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s plot the histogram to do a bivariate analysis of `workclass` and
    `income`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.23 – Histogram of workclass and income](img/B18944_01_23..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.23 – Histogram of workclass and income
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding histogram, we can see that income is greater than $50K for
    `Self-emp-inc` adults. On the other hand, income is less than $50K for the majority
    of `Private` and `Self-emp-not-inc` employees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s plot the histogram to do a bivariate analysis of `sex` and `income`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 1.24 – Histogram of sex and income](img/B18944_01_24..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.24 – Histogram of sex and income
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding histogram, we can see that income is more than $50K for male
    adults and less than $50K for most female employees.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to analyze data using Seaborn visualization
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can explore data using the ydata-profiling library with a
    few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling data using the ydata-profiling library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let us explore the dataset and generate a profiling report
    with various statistics using the `ydata-profiling` library ([https://docs.profiling.ydata.ai/4.5/](https://docs.profiling.ydata.ai/4.5/)).
  prefs: []
  type: TYPE_NORMAL
- en: The `ydata-profiling` library is a Python library for easy EDA, profiling, and
    report generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see how to use `ydata-profiling` for fast and efficient EDA:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the `ydata-profiling` library using `pip` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First, let us import the Pandas profiling library as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we can use Pandas profiling to generate reports.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we will read the *Income* dataset into the Pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '%pip install ydata-profiling --upgrade'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let us run the following commands to generate the profiling report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can also generate the report using the `profile_report()` function on the
    Pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: After running the preceding cell, all the data loaded in `df` will be analyzed
    and the report will be generated. The time taken to generate the report depends
    on the size of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the preceding cell is a report with sections. Let us understand
    the report that is generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generated profiling report contains the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overview**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variables**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactions**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correlations**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing values**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sample**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Duplicate rows**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Under the **Overview** section in the report, there are three tabs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overview**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alerts**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reproduction**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As shown in the following figure, the `Numeric` and `Categorical` variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.25 – Statistics of the dataset](img/B18944_01_25..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.25 – Statistics of the dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Alerts** tab under **Overview** shows all the variables that are highly
    correlated with each other and the number of cells that have zero values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.26 – Alerts](img/B18944_01_26..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.26 – Alerts
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Reproduction** tab under **Overview** shows the duration it took for
    the analysis to generate this report, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.27 – Reproduction](img/B18944_01_27..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.27 – Reproduction
  prefs: []
  type: TYPE_NORMAL
- en: Variables section
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us walk through the **Variables** section in the report.
  prefs: []
  type: TYPE_NORMAL
- en: Under the **Variables** section, we can select any variable in the dataset under
    the dropdown and see the statistical information about the dataset, such as the
    number of unique values for that variable, missing values for that variable, the
    size of that variable, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, we selected the `age` variable in the dropdown and
    can see the statistics about that variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.28 – Variables](img/B18944_01_28..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.28 – Variables
  prefs: []
  type: TYPE_NORMAL
- en: Interactions section
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As shown in the following figure, this report also contains the **Interactions**
    plot to show how one variable relates to another variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.29 – Interactions](img/B18944_01_29..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.29 – Interactions
  prefs: []
  type: TYPE_NORMAL
- en: Correlations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let's see the **Correlations** section in the report; we can see the correlation
    between various variables in **Heatmap**. Also, we can see various correlation
    coefficients in the **Table** form.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 1.30 \uFEFF– Correlations](img/B18944_01_30..jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 1.30 – Correlations
  prefs: []
  type: TYPE_NORMAL
- en: Heatmaps use color intensity to represent values. The colors typically range
    from cool to warm hues, with cool colors (e.g., blue or green) indicating low
    values and warm colors (e.g., red or orange) indicating high values. Rows and
    columns of the matrix are represented on both the *x* axis and *y* axis of the
    heatmap. Each cell at the intersection of a row and column represents a specific
    value in the data.
  prefs: []
  type: TYPE_NORMAL
- en: The color intensity of each cell corresponds to the magnitude of the value it
    represents. Darker colors indicate higher values, while lighter colors represent
    lower values.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding figure, the intersection cell between income
    and hours per week shows a high-intensity blue color, which indicates there is
    a high correlation between income and hours per week. Similarly, the intersection
    cell between income and capital gain shows a high-intensity blue color, indicating
    a high correlation between those two features.
  prefs: []
  type: TYPE_NORMAL
- en: Missing values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section of the report shows the counts of total values present within the
    data and provides a good understanding of whether there are any missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under **Missing values**, we can see two tabs:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Count** plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Matrix** plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Count plot
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Figure 1**.31*, the shows that all variables have a count of 32,561, which
    is the count of rows (observations) in the dataset. That indicates that there
    are no missing values in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.31 – Missing values count](img/B18944_01_31..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.31 – Missing values count
  prefs: []
  type: TYPE_NORMAL
- en: Matrix plot
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following **Matrix** plot indicates where the missing values are (if there
    are any missing values in the dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.32 – Missing values matrix](img/B18944_01_32..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.32 – Missing values matrix
  prefs: []
  type: TYPE_NORMAL
- en: Sample data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section shows the sample data for the first 10 rows and the last 10 rows
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.33 – Sample data](img/B18944_01_33..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.33 – Sample data
  prefs: []
  type: TYPE_NORMAL
- en: This section shows the most frequently occurring rows and the number of duplicates
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.34 – Duplicate rows](img/B18944_01_34..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.34 – Duplicate rows
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how to analyze the data using Pandas and then how to visualize
    the data by plotting various plots such as bar charts and histograms using sns,
    seaborn, and pandas-ydata-profiling. Next, let us see how to perform data analysis
    using OpenAI LLM and the LangChain Pandas Dataframe agent by asking questions
    with natural language.
  prefs: []
  type: TYPE_NORMAL
- en: Unlocking insights from data with OpenAI and LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial intelligence** is transforming how people analyze and interpret
    data. Exciting **generative AI** systems allow anyone to have natural conversations
    with their data, even if they have no coding or data science expertise. This democratization
    of data promises to uncover insights and patterns that may have previously remained
    hidden.'
  prefs: []
  type: TYPE_NORMAL
- en: One pioneering system in this space is **LangChain**’s **Pandas DataFrame agent**,
    which leverages the power of **large language models** (**LLMs**) such as **Azure
    OpenAI**’s **GPT-4**. LLMs are AI systems trained on massive text datasets, allowing
    them to generate human-like text. LangChain provides a framework to connect LLMs
    with external data sources.
  prefs: []
  type: TYPE_NORMAL
- en: By simply describing in plain English what you want to know about your data
    stored in a Pandas DataFrame, this agent can automatically respond in natural
    language.
  prefs: []
  type: TYPE_NORMAL
- en: The user experience feels like magic. You upload a CSV dataset and ask a question
    by typing or speaking. For example, *“What were the top 3 best-selling products
    last year?”* The agent interprets your intent and writes and runs Pandas and Python
    code to load the data, analyze it, and formulate a response...all within seconds.
    The barrier between human language and data analysis dissolves.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, the LLM generates Python code based on your question, which
    gets passed to the LangChain agent for execution. The agent handles running the
    code against your DataFrame, capturing any output or errors, and iterating if
    necessary to refine the analysis until an accurate human-readable answer is reached.
  prefs: []
  type: TYPE_NORMAL
- en: By collaborating, the agent and LLM remove the need to worry about syntax, APIs,
    parameters, or debugging data analysis code. The system understands what you want
    to know and makes it happen automatically through the magic of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: This natural language interface to data analysis opens game-changing potential.
    Subject-matter experts without programming skills can independently extract insights
    from data in their field. Data-driven decisions can happen faster. Exploratory
    analysis and ideation are simpler. The future where analytics is available to
    all AI assistants has arrived.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how the agent works behind the scenes to send a response.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a user sends a query to the LangChain `create_pandas_dataframe_agent`
    agent and LLM, the following steps are performed behind the scenes:'
  prefs: []
  type: TYPE_NORMAL
- en: The user’s query is received by the LangChain agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent interprets the user’s query and analyzes its intention.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent then generates the necessary commands to perform the first step of
    the analysis. For example, it could generate an SQL query that is sent to the
    tool that the agent knows will execute SQL queries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent analyzes the response it receives from the tool and determines whether
    it is what the user wants. If it is, the agent returns the answer; if not, the
    agent analyzes what the next step should be and iterates again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent keeps generating commands for the tools it can control until it obtains
    the response the user is looking for. It is even capable of interpreting execution
    errors that occur and generating the corrected command. The agent iterates until
    it satisfies the user’s question or reaches the limit we have set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can represent this with the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.35 – LangChain Pandas agent flow for Data analysis](img/B18944_01_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.35 – LangChain Pandas agent flow for Data analysis
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how to perform data analysis and find insights about the `income`
    dataset using the LangChain `create_pandas_dataframe_agent` agent and LLM.
  prefs: []
  type: TYPE_NORMAL
- en: The key steps are importing the necessary LangChain modules, loading data into
    a DataFrame, instantiating an LLM, and creating the DataFrame agent by passing
    the required objects. The agent can now analyze the data through natural language
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s install the required libraries. To install the LangChain library,
    open your Python notebook and type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This installs the `langchain` and `langchain_experimental` packages so you can
    import the necessary modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import `AzureChatOpenAI`, the Pandas DataFrame agent, and other required
    libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s configure the OpenAI endpoint and keys. Your OpenAI endpoint and key
    values are available in the Azure OpenAI portal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Let’s load CSV data into Pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `adult.csv` dataset is the dataset that we want to analyze and we have
    placed this CSV file in the same folder where we are running this Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Let’s instantiate the GPT-4 LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming, you have deployed the GPT-4 model in Azure OpenAI Studio as per the
    *Technical requirements* section, here, we are passing the `gpt4` endpoint, key,
    and deployment name to create the instance of GPT-4 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Setting the temperature to `0.0` has the model return the most accurate outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a Pandas DataFrame agent. To create the Pandas DataFrame agent,
    we need to pass the `gpt4` model instance and the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Pass the `gpt4` LLM instance and the DataFrame, and set `verbose` to `True`
    to see the output. Finally, let’s ask a question and run the agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in *Figure 1**.36*, when we ask the following questions to the
    LangChain agent in the Python notebook, the question is passed to the LLM. The
    LLM generates Python code for this query and sends it back to the agent. The agent
    then executes this code in the Python environment with the CSV file, obtains a
    response, and the LLM converts that response to natural language before sending
    it back to the agent and the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.36 – Agent response for row and column count](img/B18944_01_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.36 – Agent response for row and column count
  prefs: []
  type: TYPE_NORMAL
- en: 'We try the next question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.37 – agent response for first five records](img/B18944_01_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.37 – agent response for first five records
  prefs: []
  type: TYPE_NORMAL
- en: This way, the LangChain Pandas DataFrame agent facilitates interaction with
    the DataFrame by interpreting natural language queries, generating corresponding
    Python code, and presenting the results in a human-readable format.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can try these questions and see the responses from the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '`query = "calculate the average age of the people for each income` `group ?"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query ="provide summary statistics for` `this dataset"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query = "provide count of unique values for` `each column"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query = "draw the histogram of` `the age"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, let’s try the following query to plot the bar chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The Langchain agent responded with a bar chart that shows the counts for different
    education levels, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.38 – Agent response for bar chart](img/B18944_01_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.38 – Agent response for bar chart
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot of the following query shows a comparison of income for different
    education levels – master’s and HS-GRAD. And we can see the income is less than
    $5,000 for `education.num` 8 to 10 when compared to higher education:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.39 – Agent response for comparison of income](img/B18944_01_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.39 – Agent response for comparison of income
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s try the following query to find any outliers in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This plot shows outliers in age greater than 80 years.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.40 – Agent response for outliers](img/B18944_01_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.40 – Agent response for outliers
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how to perform data analysis and find insights about the `income`
    dataset using natural language with the power of LangChain and OpenAI LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to use Pandas and matplotlib to analyze
    a dataset and understand the data and correlations between various features. This
    understanding of data and patterns in the data is required to build the rules
    for labeling raw data before using it for training ML models and fine-tuning LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: We also went through various examples for aggregating columns and categorical
    values using `groupby` and `mean`. Then, we created reusable functions so that
    those functions can be reused simply by calling and passing column names to get
    aggregates of one or more columns.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw a fast and easy exploration of data using the `ydata-profiling`
    library with simple one-line Python code. Using this library, we need not remember
    many Pandas functions. We can simply call one line of code to perform a detailed
    analysis of data. We can create detailed reports of statistics for each variable
    with missing values, correlations, interactions, and duplicate rows.
  prefs: []
  type: TYPE_NORMAL
- en: Once we get a good sense of our data using EDA, we will be able to build the
    rules for creating labels for the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to build these rules using Python libraries
    such as `snorkel` and `compose` to label an unlabeled dataset. We will also explore
    other methods, such as pseudo-labeling and K-means clustering, for data labeling.
  prefs: []
  type: TYPE_NORMAL
