<html><head></head><body>
		<div id="_idContainer022">
			<h1 id="_idParaDest-43"><em class="italic"><a id="_idTextAnchor042"/>Chapter 3</em>: Fundamental Workflow – Data to Deployable Model</h1>
			<p>In this chapter, we will walk through a minimal model-building workflow for H2O at scale. We will refer to<a id="_idIndexMarker123"/> this as the <em class="italic">fundamental workflow</em> because it omits the wide range of functionality and user choices to build accurate, trusted models while nevertheless touching on the main steps.</p>
			<p>The fundamental workflow will serve as a basis to build your understanding of H2O technology and coding steps so that in the next part of the book you can dive fully into advanced techniques to build state-of-the-art models.</p>
			<p>To develop the fundamental workflow, we will cover the following main topics in this chapter:</p>
			<ul>
				<li>Use case and data overview </li>
				<li>The fundamental workflow </li>
				<li>Variation points – alternatives and extensions to the fundamental workflow</li>
			</ul>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/>Technical requirements</h1>
			<p>For this chapter, we will focus on using Enterprise Steam to launch H2O clusters on an enterprise server cluster. Enterprise Steam technically is not required to launch H2O clusters but enterprise stakeholders typically view Enterprise Steam as a security, governance, and administrator requirement for implementing H2O in enterprise environments.</p>
			<p>Enterprise Steam requires a license purchased from H2O.ai. If your organization does not have an instance of Enterprise Steam installed, you can access Enterprise Steam and an enterprise server cluster through a temporary trial license of the larger H2O platform. Alternatively, for ease of conducting the exercises in this book, you may wish to launch H2O clusters as a sandbox in your local environment (for example, on your laptop or desktop workstation) and bypass the use of Enterprise Steam. </p>
			<p>See <a href="B16721_Appendix_Final_SK_ePub.xhtml#_idTextAnchor268"><em class="italic">Appendix</em></a><em class="italic"> – Alternative Methods to Launch H2O Clusters for this Book</em> to help you decide on how you wish to launch H2O clusters for the exercises in this book and how to set up your environment to do so.  </p>
			<p class="callout-heading">Enterprise Steam: Enterprise Environment versus Coding Exercises in the Book</p>
			<p class="callout">Enterprise stakeholders typically view <a id="_idIndexMarker124"/>Enterprise Steam as a security, governance, and administrator requirement for implementing H2O in enterprise environments. This chapter shows how data scientists use Enterprise Steam in this enterprise context. Enterprise Steam, however, requires an H2O.ai license to implement and will not be available to all readers of this book.</p>
			<p class="callout">A simple sandbox (non-enterprise) experience is to use H2O exclusively on your local environment (laptop or workstation) and this does not require Enterprise Steam. Coding exercises in subsequent chapters will leverage the local sandbox environment but also can be performed using Enterprise Steam as demonstrated in this chapter. </p>
			<p class="callout">Note that the distinction between the data scientist workflow with and without Enterprise Steam is isolated to the first step of the workflow (launching the H2O cluster) and will be made clearer later in this chapter. See also <a href="B16721_Appendix_Final_SK_ePub.xhtml#_idTextAnchor268"><em class="italic">Appendix</em></a><em class="italic"> – Alternative Methods to Launch H2O Clusters</em>.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Use case and data overview</h1>
			<p>To demonstrate the<a id="_idIndexMarker125"/> fundamental workflow, we will implement a binary classification problem where we predict the likelihood that a loan will default or not. The dataset we use in this chapter can be found at <a href="https://github.com/PacktPublishing/Machine-Learning-at-Scale-with-H2O/blob/main/chapt3/loans-lite.csv">https://github.com/PacktPublishing/Machine-Learning-at-Scale-with-H2O/blob/main/chapt3/loans-lite.csv</a>. (This is a simplified version of the Kaggle <em class="italic">Lending Club Loan </em>dataset: <a href="https://www.kaggle.com/imsparsh/lending-club-loan-dataset-2007-2011">https://www.kaggle.com/imsparsh/lending-club-loan-dataset-2007-2011</a>.)  </p>
			<p>We are using a simplified version of the dataset to streamline the workflow in this chapter. In <em class="italic">Part 2, Building State-of-the-Art Models at Scale</em>, we will develop this use case using advanced H2O model-building capabilities on the original loan dataset.</p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor045"/>The fundamental workflow</h1>
			<p>Our fundamental <a id="_idIndexMarker126"/>workflow will proceed through the following steps:</p>
			<ol>
				<li>Launching the H2O cluster (Enterprise Steam UI)</li>
				<li>Connecting to the H2O cluster (your IDE from this point onward)</li>
				<li>Building the model</li>
				<li>Evaluating and explaining the model</li>
				<li>Exporting the model for production deployment</li>
				<li>Shutting down the H2O clus<a id="_idTextAnchor046"/>ter</li>
			</ol>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor047"/>Step 1 – launching the H2O cluster</h2>
			<p>This<a id="_idIndexMarker127"/> step is done<a id="_idIndexMarker128"/> from the Enterprise Steam UI. You will select whether you want an H2O-3 or Sparkling Water cluster and then you will configure the H2O cluster behavior, such as the duration of idle time before it times out and terminates and whether you want to save the state at termination so you can restart the cluster and pick up where you left off (this must be enabled by the administrator). Nicely, Enterprise Steam will auto-size the H2O cluster (number of nodes, memory per node, CPUs) based on your data size.</p>
			<h3>Logging in to Steam</h3>
			<p>Open<a id="_idIndexMarker129"/> a web browser <a id="_idIndexMarker130"/>and go to <strong class="source-inline">https://steam-url:9555/login</strong> and log<a id="_idIndexMarker131"/> in to Enterprise Steam, where <strong class="source-inline">steam-url</strong> is the URL of your specific Steam instance. (Your administrator may have changed the port number, but typically it is <strong class="source-inline">9555</strong> as shown in the URL.)</p>
			<h3>Selecting an H2O for H2O-3 (versus Sparkling Water) cluster</h3>
			<p>Here, we<a id="_idIndexMarker132"/> will launch an H2O-3 cluster (and not Sparkling Water, which we will do in the next part of the book), so click on the <strong class="bold">H2O</strong> link in the left panel and then click <strong class="bold">LAUNCH NEW CLUSTER</strong>.</p>
			<h3>Configuring the H2O-3 cluster</h3>
			<p>This <a id="_idIndexMarker133"/>brings us to the following form, which you will configure: </p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B16721_03_01.jpg" alt="Figure 3.1 – UI to launch an H2O-3 cluster on Kubernetes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – UI to launch an H2O-3 cluster on Kubernetes</p>
			<p>For now, we will ignore most configurations. These will be covered more fully in <a href="B16721_11_Final_SK_ePub.xhtml#_idTextAnchor207"><em class="italic">Chapter 11</em></a>, <em class="italic">The Administrator and Operations Views</em>, where Enterprise Steam is overviewed in detail. Note that the configuration page uses the term <em class="italic">H2O cluster</em> to represent an<a id="_idIndexMarker134"/> H2O-3 cluster specifically, whereas in this book we use the term H2O cluster to represent either an H2O-3 or Sparkling Water cluster.</p>
			<p class="callout-heading">Note on the "Configuring the H2O-3 cluster" Screenshot</p>
			<p class="callout">Details on the screen shown in <em class="italic">Figure 3.1</em> will vary depending on whether the H2O cluster is launched on a Kubernetes environment or on a YARN-based Hadoop or Spark environment. Details will also vary based on whether the H2O cluster is an H2O-3 cluster or a Sparkling Water cluster. In all cases, however, the fundamental concepts of H2O cluster size (number of nodes, CPU/GPU per node, and memory per node) and maximum idle/uptime are common throughout.</p>
			<p>Give your cluster a name and for <strong class="bold">DATASET PARAMETERS</strong>, click <strong class="bold">Set parameters</strong> to arrive at the following popup: </p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B16721_03_02.jpg" alt="Figure 3.2 – Popup to automatically size the H2O-3 cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Popup to automatically size the H2O-3 cluster</p>
			<p>The inputs here are used by Enterprise Steam to auto-size your H2O cluster (that is, to determine the number of H2O nodes and memory allocated for each node and CPU allocations for each node). Recall the <em class="italic">key concepts</em> of an H2O cluster as presented in the previous chapter. </p>
			<h3>Waiting briefly for the cluster to start</h3>
			<p>The <strong class="bold">STATUS</strong> field in<a id="_idIndexMarker135"/> the UI will state <strong class="bold">Starting</strong>, signifying that the H2O cluster is being launched on the enterprise server cluster. This will take a minute or two. When the status changes to <strong class="bold">Running</strong>, your H2O cluster is ready to use. </p>
			<h3>Viewing details of the cluster</h3>
			<p>Let's first <a id="_idIndexMarker136"/>learn a few things about the cluster by clicking on <strong class="bold">Actions</strong> and then <strong class="bold">Detail</strong>. This generates a popup describing the cluster. </p>
			<p>Notice in this case that <strong class="bold">Number of nodes</strong> is <strong class="bold">6</strong> and <strong class="bold">Memory per node</strong> is <strong class="bold">48 GB</strong> as auto-sized by Enterprise Steam for a dataset size of 50 GB, as shown in <em class="italic">Figure 3.1</em>. Recall from the <em class="italic">H2O key concepts</em> section in the previous chapter that our dataset is partitioned and distributed in memory across this number of H2O cluster nodes on the enterprise server cluster and that compute is done in parallel on these H2O nodes.</p>
			<p class="callout-heading">Note on H2O Cluster Sizing</p>
			<p class="callout">In<a id="_idIndexMarker137"/> general, an H2O cluster is sized so the total memory allocated to the cluster (that is, the product of <em class="italic">N</em> H2O nodes and <em class="italic">X</em> GB memory per node) is roughly 5 times the size of the uncompressed dataset that will be used for model building. The calculation minimizes the number of nodes (that is, fewer nodes with more memory per node is better). </p>
			<p class="callout">Enterprise Steam will calculate this sizing based on your description of the dataset, but alternatively, you can size the cluster yourself through the Enterprise Steam UI. The total memory allocated to the H2O cluster will be released when the H2O cluster is terminated.</p>
			<p class="callout">Note that the Enterprise Steam administrator sets the minimum and maximum configuration values a user may have when launching an H2O cluster (see <em class="italic">Figure 3.1</em>) and thus the maximum H2O cluster size a user may launch. These boundaries set by the administrator can be configured differently for different users.</p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor048"/>Step 2 – connecting to the H2O cluster </h2>
			<p>This and all <a id="_idIndexMarker138"/>subsequent steps are from your IDE. We will use a Jupyter notebook and write code in Python (though other options include writing H2O in R, Java, or Scala using your preferred IDE). </p>
			<p>Open the notebook and connect to the H2O cluster you launched in Enterprise Steam by writing the following code:</p>
			<pre class="source-code">import h2o</pre>
			<pre class="source-code">import h2osteam</pre>
			<pre class="source-code">from h2osteam.clients import H2oKubernetesClient</pre>
			<pre class="source-code">conn = h2osteam.login(url="https://steam-url:9555",</pre>
			<pre class="source-code">                      username="my-steam-username",</pre>
			<pre class="source-code">                      password="my-steam-password")</pre>
			<pre class="source-code">cluster = H2oKubernetesClient().get_cluster("cluster-name")</pre>
			<pre class="source-code">cluster.connect()</pre>
			<p>You have now connected to the H2O cluster and can start building models. Note that after you connect, you will see H2O cluster details similar to those viewed from the Enterprise Steam UI when you configured the cluster before launching.</p>
			<p>Let's understand what the code is doing:</p>
			<ol>
				<li value="1">You referenced the <strong class="source-inline">h2osteam</strong> and <strong class="source-inline">h2o</strong> Python libraries that were downloaded from H2O and implemented in the IDE environment. (The <strong class="source-inline">h2o</strong> library is not used by the code shown here but will be used by subsequent model building steps that follow.)</li>
				<li>Then you logged into the Enterprise Steam server via the <strong class="source-inline">h2osteam</strong> API (library). You used the same URL, username, and password that was used to log in to the UI of Enterprise Steam. </li>
				<li>You then retrieved your H2O cluster information from Enterprise Steam via the <strong class="source-inline">h2osteam</strong> API. </li>
				<li>Note that you are using <strong class="source-inline">H2oKubernetesClient</strong> here because you are connecting to an H2O cluster launched on a Kubernetes environment. If, alternatively, your enterprise environment is Hadoop or Spark, you use <strong class="source-inline">H2oClient</strong> or <strong class="source-inline">SparklingClient</strong>, respectively.</li>
				<li>You<a id="_idIndexMarker139"/> connected to your H2O cluster using <strong class="source-inline">cluster.connect()</strong> and passed the cluster information to the <strong class="source-inline">h2o</strong> API. Note that you did not have to specify any URL to the H2O cluster because Steam returned this behind the scenes with <strong class="source-inline">H2oKubernetesClient().get_cluster("cluster-name")</strong>.<p class="callout-heading">Creating an H2O Sandbox Environment</p><p class="callout">If you want<a id="_idIndexMarker140"/> to create a small H2O sandbox on your local machine instead of using Enterprise Steam and your enterprise server cluster, simply implement the following two lines of code from your IDE:</p><p class="callout"><strong class="source-inline">import h2o</strong></p><p class="callout"><strong class="source-inline">h2o.init()</strong></p><p class="callout">The result is identical to performing <em class="italic">steps 1–2</em> using Enterprise Steam, except that it launches an H2O cluster with one node on your local machine and connects to it. </p><p class="callout">Whether connecting to an H2O cluster in your enterprise environment or on your local machine, you can now write model-building steps identically from your IDE against the respective cluster. For the sandbox, you will be constrained, of course, to much smaller data volumes because of its small cluster size of one node with low memory.</p></li>
			</ol>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>Step 3 – building the model</h2>
			<p>Now that we <a id="_idIndexMarker141"/>have connected to our H2O cluster, it is time to build the model. From this point onward, you will be using the <strong class="source-inline">h2o</strong> API to communicate with the H2O cluster to which you launched and connected.</p>
			<p>Here in our fundamental workflow, we will take a minimal approach to import data, clean it, engineer features from it, and then train the model. </p>
			<h3>Importing the data </h3>
			<p>The loans <a id="_idIndexMarker142"/>dataset is loaded from the source into the H2O-3 cluster memory using the <strong class="source-inline">h2o.import_file</strong> command as follows: </p>
			<pre class="source-code">input_csv = "https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-at-Scale-with-H2O/main/chapt3/loans-lite.csv"</pre>
			<pre class="source-code">loans = h2o.import_file(input_csv)</pre>
			<pre class="source-code">loans.dim</pre>
			<pre class="source-code">loans.head()</pre>
			<p>The <strong class="source-inline">loans.dim</strong> line gives us the number of rows and columns and <strong class="source-inline">loans.head()</strong> displays the first 10 rows. Quite simple data exploration for now.</p>
			<p>Note that the dataset is now partitioned and distributed in memory across the H2O cluster. From our coding standpoint in the IDE, it is treated as a single two-dimensional data structure of columns and rows<a id="_idIndexMarker143"/> called an <strong class="bold">H2OFrame</strong>. </p>
			<h3>Cleaning the data</h3>
			<p>Let's perform <a id="_idIndexMarker144"/>one simple data cleaning step. The target or response column is called <strong class="source-inline">bad_loan</strong> and it holds values of either 0 or 1 for good and bad loans respectively. We need to transform the integers in this column to categorical values, as shown next:</p>
			<pre class="source-code">loans["bad_loan"] = loans["bad_loan"].asfactor()</pre>
			<h3>Engineering new features from the original data</h3>
			<p>Feature engineering <a id="_idIndexMarker145"/>is often considered the <em class="italic">secret sauce</em> in building a superior predictive model. For our purposes now, we will do basic feature engineering by extracting year and month as separate features from the <strong class="source-inline">issue_d</strong> column, which holds day, month, and year as a single value:</p>
			<pre class="source-code">loans["issue_d_year"] = loans["issue_d"].year().asfactor()</pre>
			<pre class="source-code">loans["issue_d_month"] = loans["issue_d"].month().asfactor()</pre>
			<p>We have<a id="_idIndexMarker146"/> just created two new categorical columns in our <strong class="source-inline">loans</strong> dataset: <strong class="source-inline">issue_d_year</strong> and <strong class="source-inline">issue_d_month</strong>.</p>
			<h3>Model training</h3>
			<p>We will <a id="_idIndexMarker147"/>next train a model to predict bad loans. We first split our data into <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong>:</p>
			<pre class="source-code">train, validate, test = loans.split_frame(seed=1, ratios=[0.7, 0.15])</pre>
			<p>We now need to identify which columns we will use to predict whether a loan is bad or not. We will do this by removing two columns from the current loans H2OFrame, which hold the cleaned and engineered data:</p>
			<pre class="source-code">predictors = list(loans.col_names)</pre>
			<pre class="source-code">predictors.remove("bad_loan)</pre>
			<pre class="source-code">predictors.remove("issue_d")</pre>
			<p>Note that we removed <strong class="source-inline">bad_loan</strong> from the columns used as features because this is what we are predicting. We also removed <strong class="source-inline">issue_d</strong> because we engineered new features from this and do not want it as a predictor.</p>
			<p>Next, let's create an XGBoost model to predict loan default:</p>
			<pre class="source-code">from h2o.estimators import H2OXGBoostEstimator</pre>
			<pre class="source-code">param = {</pre>
			<pre class="source-code">         "ntrees" : 20,</pre>
			<pre class="source-code">         "nfolds" : 5,</pre>
			<pre class="source-code">         "seed": 12345</pre>
			<pre class="source-code">}</pre>
			<pre class="source-code">model = H2OXGBoostEstimator(**param)</pre>
			<pre class="source-code">model.train(x = predictors,</pre>
			<pre class="source-code">            y = "bad_loan",</pre>
			<pre class="source-code">            training_frame = train,</pre>
			<pre class="source-code">            validation_frame = validate)</pre>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/>Step 4 – evaluating and explaining the model</h2>
			<p>Let's evaluate<a id="_idIndexMarker148"/> the performance of the model that we just trained:</p>
			<pre class="source-code">perf = model.model_performance(test)</pre>
			<pre class="source-code">perf</pre>
			<p>The <a id="_idIndexMarker149"/>output of <strong class="source-inline">perf</strong> shows details on model performance, including model metrics such as MSE, Logloss, AUC, and others, as well as a confusion matrix, maximum metrics thresholds, and a gains/lift table.</p>
			<p>Now let's look at one simple view of model explainability by generating variable importance from the model result:</p>
			<pre class="source-code">explain = model.explain(test,include_explanations="varimp")</pre>
			<pre class="source-code">explain</pre>
			<p>The output of <strong class="source-inline">explain</strong> shows the variable importance of the trained model run against the test dataset. This is a table listing how strongly each feature contributed to the model.</p>
			<p>H2O's model explainability capabilities go much further than variable importance, as we shall see later in the book.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/>Step 5 – exporting the model's scoring artifact</h2>
			<p>Now let's generate <a id="_idIndexMarker150"/>and export the model as a scoring artifact that can be deployed to a production environment by the DevOps group:</p>
			<pre class="source-code">model.download_mojo("download-destination-path")</pre>
			<p>In the real world, of course, we would train many models and compare their performance and explainability to evaluate which (if any) should make it to production.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor052"/>Step 6 – shutting down the cluster</h2>
			<p>When your <a id="_idIndexMarker151"/>work is complete, shut down the H2O-3 cluster to free up the resources that were reserved by it:</p>
			<pre class="source-code">h2o.cluster().shutdown()</pre>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor053"/>Variation points – alternatives and extensions to the fundamental workflow</h1>
			<p>The <a id="_idIndexMarker152"/>fundamental workflow we developed here is a simple example. For each step we performed, there are multiple alternatives and extensions to what has been shown. All of <em class="italic">Part 2:, Building State-of-the-Art Models at Scale,</em> is dedicated to understanding these alternatives and elaborations and to putting them together to build superior models at scale.</p>
			<p>Let's first touch on some key variation points here.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor054"/>Launching an H2O cluster using the Enterprise Steam API versus the UI (step 1)</h2>
			<p>In our <a id="_idIndexMarker153"/>example, we used the convenience of the Enterprise Steam UI to configure and launch an H2O cluster. Alternatively, we could have used the Steam API from our IDE to do so. See the full H2O Enterprise Steam API documentation at <a href="https://docs.h2o.ai/enterprise-steam/latest-stable/docs/python-docs/index.html">https://docs.h2o.ai/enterprise-steam/latest-stable/docs/python-docs/index.html</a> for the Python API and <a href="https://docs.h2o.ai/enterprise-steam/latest-stable/docs/r-docs/index.html">https://docs.h2o.ai/enterprise-steam/latest-stable/docs/r-docs/index.html</a> for the R API. </p>
			<p>By launching the H2O cluster from our IDE, we therefore could have completed all of <em class="italic">steps 1–6</em> of our workflow exclusively from the IDE.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor055"/>Launching an H2O-3 versus Sparkling Water cluster (step 1)</h2>
			<p>In our <a id="_idIndexMarker154"/>example, we launched an H2O-3 cluster. We could alternatively launch an H2O Sparkling Water cluster. As we will see, Sparkling Water clusters have the same capability set as H2O-3 clusters but with the additional ability to integrate Spark code and Spark DataFrames with H2O code and H2O DataFrames. This is particularly powerful when leveraging Spark for advanced data exploration and data munging before building models in H2O.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor056"/>Implementing Enterprise Steam or not (steps 1–2)</h2>
			<p>Know that Enterprise Steam<a id="_idIndexMarker155"/> is not a requirement for launching and connecting to an enterprise server cluster: it is possible for a data scientist to use only the <strong class="source-inline">h2o</strong> (and not <strong class="source-inline">h2osteam</strong>) API in the IDE to configure, launch, and connect to an enterprise server cluster, but this is low-level coding and configuration and requires detailed integration information. Importantly, this approach lacks sound enterprise security, governance, and integration practices. </p>
			<p>In the enterprise setting, Enterprise Steam is viewed as essential to centralize, manage, and govern H2O technology and H2O users in the enterprise server cluster environment. These capabilities are elaborated on in <a href="B16721_11_Final_SK_ePub.xhtml#_idTextAnchor207"><em class="italic">Chapter 11</em></a>, <em class="italic">The Administrator and Operations Views</em>.</p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor057"/>Using a personal access token to log in to Enterprise Steam (step 2)</h2>
			<p>For <em class="italic">Step 2 – connecting to the H2O cluster</em>, we authenticated to Enterprise Steam from our IDE <a id="_idIndexMarker156"/>using the Enterprise Steam API. In the example code, we used a clear text password (which was the same password used to log into the Enterprise Steam UI). This is not secure if, for example, you shared the notebook. </p>
			<p>Alternatively, and more securely, you can<a id="_idIndexMarker157"/> use a <strong class="bold">Personal Access Token</strong> (<strong class="bold">PAT</strong>) as the API login password to Enterprise Steam. A PAT can be generated as often as you wish, with each newly generated PAT revoking the previous one. Thus, if you shared a Jupyter notebook with your login credentials using a PAT as your password, the recipient of the notebook would not know your Enterprise Steam UI login password and could not authenticate via the API using the revoked password in your shared notebook. You can take the PAT one step further and implement it as an environment variable outside the IDE.</p>
			<p>Enterprise Steam lets you generate a PAT from the UI. To generate a PAT, log in to Enterprise Steam UI, click <strong class="bold">Configurations,</strong> and follow the brief token workflow. Copy the result (a long string) for use in your current notebook or script or to set it as an environment variable.</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor058"/>Building the model (step 3)</h2>
			<p>H2O offers a <a id="_idIndexMarker158"/>much more powerful model-building experience than what was shown in our fundamental workflow. This larger experience is touched on here and explored fully in <em class="italic">Part 2, Building State-of-the-Art Models at Scale</em>.</p>
			<h3>Language and IDE</h3>
			<p>We are <a id="_idIndexMarker159"/>writing H2O code in Python in a Jupyter notebook. You can also choose R for the Enterprise Steam API and use the Python or R IDE of your choice. Additionally, you can use H2O's UI-rich IDE called <strong class="bold">H2O Flow</strong> to <a id="_idIndexMarker160"/>perform the full workflow or to quickly understand aspects of an H2O cluster workflow that is progressing from your own IDE.</p>
			<h3>Importing data </h3>
			<p>Data can <a id="_idIndexMarker161"/>be imported from many sources into H2O clusters, including cloud object storage (for example, S3 or Azure Delta Lake), database tables (via JDBC), HDFS, and more. Additionally, source files can have many formats, including Parquet, ORC, ARFF, and more. </p>
			<h3>Cleaning data and engineering features</h3>
			<p>H2O-3 has capabilities<a id="_idIndexMarker162"/> for basic data manipulation (for example, changing column types, combining or slicing<a id="_idIndexMarker163"/> rows or columns, group by, impute, and so on). </p>
			<p>Recall that launching a Sparkling Water cluster gives us full H2O-3 capabilities with the addition of Spark's more powerful data exploration and engineering capabilities. </p>
			<h3>Model training</h3>
			<p>In <a id="_idIndexMarker164"/>our fundamental workflow, we explored only one type of model (XGBoost) while changing only a few default parameters. H2O-3 (and its Sparkling Water extension) has an extensive list of both supervised and unsupervised learning algorithms and a wide range of parameters and hyperparameters to set to your specification. In addition, these algorithms can be combined powerfully into an AutoML workflow that explores multiple models and hyperparameter<a id="_idIndexMarker165"/> space and arranges the resulting best models on a leaderboard. You also have control over cross-validation techniques, checkpointing, retraining, and reproducibility.</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor059"/>Evaluating and explaining the model (step 4)</h2>
			<p>H2O has<a id="_idIndexMarker166"/> numerous explainability methods<a id="_idIndexMarker167"/> and visualizations for both local (individual) and global (model-level) explainability, including residual analysis, variable importance heatmaps, Shapley summaries, <strong class="bold">Partial Dependence Plots</strong> (<strong class="bold">PDPs</strong>), and <strong class="bold">Individual Conditional Expectation</strong> (<strong class="bold">ICE</strong>).</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor060"/>Exporting the model's scoring artifact (step 5)</h2>
			<p>Once<a id="_idIndexMarker168"/> you export the model's scoring artifact (called an H2O MOJO), it is ready for DevOps to deploy and monitor in live scoring environments. It likely will enter the organization's CI/CD process. We will pick it up at this point in <em class="italic">Part 3, Deploying Your Models to Production Environments</em>.</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor061"/>Shutting down the cluster (step 6)</h2>
			<p>You can<a id="_idIndexMarker169"/> shut down your cluster from your IDE as shown in our example workflow. If you noticed, when configuring your cluster in Enterprise Steam, however, there are two configurations that automate the shutdown process: <strong class="bold">MAXIMUM IDLE TIME</strong> and <strong class="bold">MAXIMUM UPTIME</strong>. The first shuts down the cluster after it has not been used for the configured amount of time. The second shuts down the cluster after it has been up for the configured amount of time. Shutting down clusters (manually or automatically) saves resources for others using the enterprise server cluster. </p>
			<p>The administrator assigns minimum and maximum values for these auto-terminate configurations. Note that when enabled by administrators, Enterprise Steam saves all models and DataFrames when the H2O cluster has been auto-terminated. You can restart the cluster later and pick up where the cluster terminated. </p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor062"/>Summary</h1>
			<p>In this chapter, you learned how to launch an H2O cluster and build a model on it from your IDE. This fundamental workflow is a bare skeleton that you will flesh out much more fully with a deep set of advanced H2O model-building techniques that we will now learn in <em class="italic">Part 2, Building State-of-the-Art Models at Scale,</em> of the book. </p>
			<p>We will start this advanced journey in the next chapter by overviewing these capabilities before using them.</p>
		</div>
	</body></html>