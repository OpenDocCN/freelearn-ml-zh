<html><head></head><body>
		<div id="_idContainer145">
			<h1 id="_idParaDest-137"><a id="_idTextAnchor138"/>Chapter 8: Use Case – Customer Churn Prediction</h1>
			<p>In the last chapter, we discussed the alternatives to the <strong class="bold">Feast</strong> feature store available on the market. We looked at a few feature store offerings from cloud providers that are part of <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) platform offerings, namely, SageMaker, Vertex AI, and Databricks. We also looked at a couple of other vendors that offer managed feature stores that can be used with your cloud provider, namely, Tecton and Hopsworks, of which Hopsworks is also open source. To get a feel for a managed feature store, we tried out an exercise on the SageMaker Feature Store and also briefly discussed ML best practices.</p>
			<p>In this chapter, we will discuss an end-to-end use case of customer churn using a telecom dataset. We will walk through data cleaning, feature engineering, feature ingestion, model training, deployment, and monitoring. For this exercise, we will use a managed feature store – Amazon SageMaker. The reason for choosing SageMaker over other alternatives that we discussed in the last chapter is simply the easy accessibility to the trial version of the software. </p>
			<p>The aim of this chapter is to go through a customer churn prediction ML use case, step by step, using a managed feature store. This should give you an idea of how it differs from self-managed feature stores and also basic feature monitoring and model monitoring aspects the feature store helps with.   </p>
			<p>In this chapter, we will discuss the following topics in order: </p>
			<ul>
				<li>Infrastructure setup</li>
				<li>Introduction to the problem and the dataset</li>
				<li>Data processing and feature engineering</li>
				<li>Feature group definitions and ingestion</li>
				<li>Model training</li>
				<li>Model prediction</li>
				<li>Feature monitoring</li>
				<li>Model monitoring</li>
			</ul>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor139"/>Technical requirements</h1>
			<p>To run through the examples and to get a better understanding of this chapter, an understanding of the topics covered in previous chapters will be useful but is not required. To follow the code examples in this chapter, you need familiarity with a notebook environment, which could be a local setup such as the Jupyter Notebook or an online notebook environment such as Google Colab, Kaggle, or SageMaker. You will also need an AWS account with full access to SageMaker and the Glue console. You can create a new account and use all the services for free during the trial period. You can find the code examples of the book at the following GitHub link:</p>
			<p>https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter08</p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor140"/>Infrastructure setup</h1>
			<p>For the exercises<a id="_idIndexMarker525"/> in this chapter, we will need an S3 bucket to store data, an IAM role, and an IAM user that has access to both the SageMaker Feature Store and the S3 bucket. Since we have already gone through creating all these resources, I will skip through this. Please refer to <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>, for S3 bucket creation, and <a href="B18024_07_ePub.xhtml#_idTextAnchor113"><em class="italic">Chapter 7</em></a>, <em class="italic">Feast Alternatives and ML Best Practices</em>, for IAM role and IAM user creation. That is all we need, in terms of initial setup for this chapter.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">I am trying to use as few resources of AWS SageMaker as possible since it will incur costs if your free trial has come to an end. You can use SageMaker Studio for a better experience with notebooks and also the UI of the feature store. </p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor141"/>Introduction to the problem and the dataset</h1>
			<p>In this exercise, we <a id="_idIndexMarker526"/>will use the telecom customer churn dataset, which is available on Kaggle at the URL https://www.kaggle.com/datasets/blastchar/telco-customer-churn. The aim of the exercise<a id="_idIndexMarker527"/> is to use this dataset, prepare the data for model training, and train an XGBoost model to predict customer churn. The dataset has 21 columns and the column names are self-explanatory. The following is a preview of the dataset:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B18024_08_001.jpg" alt="Figure 8.1 – Telecom dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Telecom dataset</p>
			<p><em class="italic">Figure 8.1</em> shows <a id="_idIndexMarker528"/>the labeled telecom customer churn dataset. The <strong class="source-inline">customerID</strong> column is the ID of the customers. All other columns except <strong class="source-inline">Churn</strong> represent the set of attributes, and the <strong class="source-inline">Churn</strong> column is the target column.</p>
			<p>Let's get our hands dirty and perform feature engineering next.</p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor142"/>Data processing and feature engineering</h1>
			<p>In this <a id="_idIndexMarker529"/>section, let's use the telecom customer <a id="_idIndexMarker530"/>churn dataset and generate the features that can be used for training the model. Let's create a notebook, call it <strong class="source-inline">feature-engineering.ipynb</strong>, and install the required dependencies:</p>
			<pre class="source-code">!pip install pandas sklearn python-slugify s3fs sagemaker</pre>
			<p>Once the installation of the libraries is complete, read the data. For this exercise, I have downloaded the data from Kaggle and saved it in a location where it is accessible from the notebook.</p>
			<p>The<a id="_idIndexMarker531"/> following <a id="_idIndexMarker532"/>command reads the data from S3:</p>
			<pre class="source-code">import os</pre>
			<pre class="source-code">import numpy as np</pre>
			<pre class="source-code">import pandas as pd</pre>
			<pre class="source-code">from slugify import slugify</pre>
			<pre class="source-code">from sklearn.preprocessing import LabelEncoder</pre>
			<pre class="source-code">from sklearn.preprocessing import StandardScaler</pre>
			<pre class="source-code">""" If you are executing the notebook outside AWS(Local jupyter lab, google collab or kaggle etc.), please uncomment the following 3 lines of code and set the AWS credentials """</pre>
			<pre class="source-code">#os.environ["AWS_ACCESS_KEY_ID"] = "&lt;aws_key&gt;"</pre>
			<pre class="source-code">#os.environ["AWS_SECRET_ACCESS_KEY"] = "&lt;aws_secret&gt;"</pre>
			<pre class="source-code">#os.environ["AWS_DEFAULT_REGION"] = "us-east-1"</pre>
			<pre class="source-code">telcom = pd.read_csv("s3://&lt;bucket_name_path&gt;/telco-customer-churn.csv")</pre>
			<p class="callout-heading">Important Note</p>
			<p class="callout"> If you are executing the notebook outside AWS, then set the user credentials using the environment variables.</p>
			<p>If you preview that dataset, there are a few columns that need to be reformatted, converted into a categorical column, or have empty values removed. Let's perform those transformations one after the other.</p>
			<p>The <strong class="source-inline">TotalCharges</strong> column <a id="_idIndexMarker533"/>contains a few empty strings. Let's remove the rows that contain empty or null values for <strong class="source-inline">TotalCharges</strong>:</p>
			<pre class="source-code"># Replace empty strings with nan</pre>
			<pre class="source-code">churn_data['TotalCharges'] = churn_data["TotalCharges"].replace(" ",np.nan)</pre>
			<pre class="source-code"># remove null values</pre>
			<pre class="source-code">churn_data = churn_data[churn_data["TotalCharges"].notnull()]</pre>
			<pre class="source-code">churn_data = churn_data.reset_index()[churn_data.columns]</pre>
			<pre class="source-code">churn_data["TotalCharges"] = churn_data["TotalCharges"].astype(float)</pre>
			<p>The<a id="_idIndexMarker534"/> preceding code block replaces all the empty strings with <strong class="source-inline">np.nan</strong> and drops all the rows that contain null in the <strong class="source-inline">TotalCharges</strong> column.</p>
			<p>Next, let's look at the <strong class="source-inline">tenure</strong> column. This one has integer values that represent the tenure of the customer in months. Along with the value, we can also group the customers into three groups: short (0-24 months), mid (24-48 months), and long (greater than 48 months). </p>
			<p>The following code adds the customer <strong class="source-inline">tenure_group</strong> column with the defined groups:</p>
			<pre class="source-code"># Create tenure_group columns using the tenure</pre>
			<pre class="source-code">def tenure_label(churn_data) :</pre>
			<pre class="source-code">    if churn_data["tenure"] &lt;= 24 :</pre>
			<pre class="source-code">        return "0-24"</pre>
			<pre class="source-code">    elif (churn_data["tenure"] &gt; 24) &amp; (churn_data["tenure"] &lt;= 48) :</pre>
			<pre class="source-code">        return "24-48"</pre>
			<pre class="source-code">    elif churn_data["tenure"] &gt; 48:</pre>
			<pre class="source-code">        return "48-end"</pre>
			<pre class="source-code">churn_data["tenure_group"] = churn_data.apply(</pre>
			<pre class="source-code">    lambda churn_data: tenure_label(churn_data), axis = 1)</pre>
			<p>The preceding code block creates the categorical column <strong class="source-inline">tenure_group</strong>, which will have three values, <strong class="source-inline">0-24</strong>, <strong class="source-inline">24-48</strong>, and <strong class="source-inline">48-end</strong>, depending on the length of the customer tenure.</p>
			<p>A few<a id="_idIndexMarker535"/> columns in the <a id="_idIndexMarker536"/>dataset are dependent on others. For example, <strong class="source-inline">OnlineSecurity</strong> depends on whether the customer has <strong class="source-inline">InternetService</strong> or not. Hence, some of these columns, namely, <strong class="source-inline">OnlineSecurity</strong>, <strong class="source-inline">OnlineBackup</strong>, <strong class="source-inline">DeviceProtection</strong>, <strong class="source-inline">TechSupport</strong>, <strong class="source-inline">StreamingTV</strong>, and <strong class="source-inline">StreamingMovies</strong> have <strong class="source-inline">No internet service</strong> as the value instead of <strong class="source-inline">No</strong>. Let's replace <strong class="source-inline">No internet service</strong> with <strong class="source-inline">No</strong> in those columns.</p>
			<p>The following code block performs the replacement:</p>
			<pre class="source-code"># Replace 'No internet service' to No for the following columns</pre>
			<pre class="source-code">replace_cols = ['OnlineSecurity', 'OnlineBackup', </pre>
			<pre class="source-code">                'DeviceProtection', 'TechSupport',</pre>
			<pre class="source-code">                'StreamingTV', 'StreamingMovies']</pre>
			<pre class="source-code">for i in replace_cols : </pre>
			<pre class="source-code">    churn_data[i] = churn_data[i].replace({'No internet service' : 'No'})</pre>
			<p>We have done a set of data cleaning operations so far. Let's preview the dataset once before we proceed and do further transformations.</p>
			<p>The following code samples the <strong class="source-inline">churn_data</strong> DataFrame:</p>
			<pre class="source-code">churn_data.sample(5)</pre>
			<p>The following code outputs a sample preview as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B18024_08_002.jpg" alt="Figure 8.2 – Churn dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Churn dataset</p>
			<p>As you can <a id="_idIndexMarker537"/>see in <em class="italic">Figure 8.2</em>, the<a id="_idIndexMarker538"/> dataset is clean and only has categorical or numerical columns. The next step is to covert these categorical values into numerical encoding. Let's look at the dataset and see which ones are categorical and which ones are numerical. </p>
			<p>The following code calculates the unique values in every column:</p>
			<pre class="source-code">churn_data.nunique()</pre>
			<p>The preceding code displays the following output:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B18024_08_003.jpg" alt="Figure 8.3 – Unique value count of every column&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Unique value count of every column</p>
			<p>As you can see in <em class="italic">Figure 8.3</em>, except <strong class="source-inline">MonthlyCharges</strong>, <strong class="source-inline">tenure</strong>, and <strong class="source-inline">TotalCharges</strong>, all other columns are categorical. </p>
			<p>In the dataset, there <a id="_idIndexMarker539"/>are binary <a id="_idIndexMarker540"/>columns and multi-value category columns. Let's find out which ones are binary and which ones are multi-value columns. The following code block checks if the column is binary from the list of columns:</p>
			<pre class="source-code"># filter all the col if unique values in the column is 2</pre>
			<pre class="source-code">bin_cols = churn_data.nunique()[churn_data.nunique() == 2].keys().tolist()</pre>
			<p>Now that we have the list of binary columns, let's transform them into 0s and 1s using the label encoder.</p>
			<p>The following code uses the label encoder to perform the transformation on the binary columns:</p>
			<pre class="source-code">le = LabelEncoder()</pre>
			<pre class="source-code">for i in bin_cols :</pre>
			<pre class="source-code">    churn_data[i] = le.fit_transform(churn_data[i])</pre>
			<p>The next step is to transform the multi-value categorical columns into 0s and 1s. To do that, let's filter out the multi-value column names first.</p>
			<p>The following <a id="_idIndexMarker541"/>code block selects the multi-value columns:</p>
			<pre class="source-code">all_categorical_cols = churn_data.nunique()[churn_data.nunique() &lt;=4].keys().tolist()</pre>
			<pre class="source-code">multi_value_cols = [col for col in all_categorical_cols if col not in bin_cols]</pre>
			<p>The preceding code block filters out all the categorical columns first and filters out the binary columns so that we are left with only the multi-value columns. </p>
			<p>The following code block transforms the multi-value columns into binary encodings:</p>
			<pre class="source-code">churn_data = pd.get_dummies(data = churn_data, columns=multi_value_cols)</pre>
			<p>The last part is<a id="_idIndexMarker542"/> transforming the numerical values. Since numerical columns can have different ranges, scaling the columns to a standard range can be beneficial for ML algorithms. It also helps algorithms converge faster. Hence, let's scale the number columns to a standard range.</p>
			<p>The following code block uses <strong class="source-inline">StandardScaler</strong> to scale all the numerical columns to a standard range: </p>
			<pre class="source-code">numerical_cols = ['tenure','MonthlyCharges','TotalCharges']</pre>
			<pre class="source-code">std = StandardScaler()</pre>
			<pre class="source-code">churn_data[numerical_cols] = std.fit_transform(churn_data[numerical_cols])</pre>
			<p>The preceding code block scales the numerical columns: <strong class="source-inline">tenure</strong>, <strong class="source-inline">MonthlyCharges</strong>, and <strong class="source-inline">TotalCharges</strong>. Now that our feature engineering is complete, let's preview the final feature set and ingest it into the SageMaker Feature Store.</p>
			<p>The following code block shows the feature set preview:</p>
			<pre class="source-code">churn_data.columns = [slugify(col, lowercase=True, separator='_') for col in churn_data.columns]</pre>
			<pre class="source-code">churn_data.head()</pre>
			<p>The preceding <a id="_idIndexMarker543"/>code block formats the <a id="_idIndexMarker544"/>column names as lowercase and replaces all the separators in the string, such as spaces and hyphens, with an underscore. The final features are shown in the following screenshot:</p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B18024_08_004.jpg" alt="Figure 8.4 – Feature set&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Feature set</p>
			<p>The final feature <a id="_idIndexMarker545"/>set has 33 columns as shown in <em class="italic">Figure 8.4</em>. If you recall in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>, while <a id="_idIndexMarker546"/>creating feature definitions, we identified entities and grouped features based on their entities or logical groups. Though these features can be grouped into multiple groups, we will be creating a single feature group and ingesting all the features into it.</p>
			<p>In the next section, let's create the feature definitions and ingest the data. </p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor143"/>Feature group definitions and feature ingestion</h1>
			<p>Now that <a id="_idIndexMarker547"/>we have the feature set ready for ingestion, let's create the feature definitions and ingest the features into a feature store. For this exercise, as mentioned before, we will be using the<a id="_idIndexMarker548"/> SageMaker Feature Store. If you recall from the previous chapters, we always kept feature definitions in a separate notebook, as it is a one-time activity. In this exercise, we are going to try a different method, which is using a conditional statement to create a feature group if it doesn't exist. You can use either of the approaches. </p>
			<p>Let's continue in the same notebook and initialize the boto3 session and check whether our feature group exists already or not:</p>
			<pre class="source-code">import boto3</pre>
			<pre class="source-code">FE<a id="_idTextAnchor144"/>ATURE_GROUP_NAME = "telcom-customer-features"</pre>
			<pre class="source-code">feature_group_exist = False</pre>
			<pre class="source-code">client = boto3.client('sagemaker')</pre>
			<pre class="source-code">response = client.list_feature_groups(</pre>
			<pre class="source-code">    NameContains=FEATURE_GROUP_NAME)</pre>
			<pre class="source-code">if FEATURE_GROUP_NAME in response["FeatureGroupSummaries"]:</pre>
			<pre class="source-code">  feature_group_exist = True</pre>
			<p>The preceding code block queries SageMaker to check whether the feature group with the name <strong class="source-inline">telcom-customer-features</strong> exists or not and sets a Boolean based on that. We will use this Boolean to either create the feature group or to skip creation and just ingest the data into the feature store.</p>
			<p>The following code block initializes the objects required for interacting with the SageMaker Feature Store:</p>
			<pre class="source-code">import sagemaker</pre>
			<pre class="source-code">from sagemaker.session import Session</pre>
			<pre class="source-code">import time</pre>
			<pre class="source-code">from sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum</pre>
			<pre class="source-code">role = "arn:aws:iam::&lt;account_number&gt;:role/sagemaker-iam-role"</pre>
			<pre class="source-code">sagemaker_session = sagemaker.Session()</pre>
			<pre class="source-code">region = sagemaker_session.boto_region_name</pre>
			<pre class="source-code">s3_bucket_name = "feast-demo-mar-2022"</pre>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Use the IAM role created in the earlier section, in the preceding code block. The IAM role should have <strong class="bold">AmazonSageMakerFullAccess</strong> and <strong class="bold">AmazonS3FullAccess</strong>.</p>
			<p>The <a id="_idIndexMarker549"/>next<a id="_idIndexMarker550"/> step is to initialize the <strong class="source-inline">FeatureGroup</strong> object. The following code initializes the feature group object:</p>
			<pre class="source-code">from sagemaker.feature_store.feature_group import FeatureGroup</pre>
			<pre class="source-code">customers_feature_group = FeatureGroup(</pre>
			<pre class="source-code">    name=FEATURE_GROUP_NAME, </pre>
			<pre class="source-code">    sagemaker_session=sagemaker_session</pre>
			<pre class="source-code">)</pre>
			<p>Now we will use the Boolean that was set earlier to conditionally create the feature group if the feature group doesn't exist. The following code block loads the feature definitions and <a id="_idIndexMarker551"/>calls <strong class="source-inline">create</strong> if the<a id="_idIndexMarker552"/> feature group doesn't exist:</p>
			<pre class="source-code">churn_data["event_timestamp"] = float(round(time.time()))</pre>
			<pre class="source-code">if not feature_group_exist:</pre>
			<pre class="source-code">  customers_feature_group.load_feature_definitions(</pre>
			<pre class="source-code">      churn_data[[col </pre>
			<pre class="source-code">                  for col in churn_data.columns </pre>
			<pre class="source-code">                  if col not in ["customerid"]]]) </pre>
			<pre class="source-code">  customer_id_def = FeatureDefinition(</pre>
			<pre class="source-code">      feature_name='customerid', </pre>
			<pre class="source-code">      feature_type=FeatureTypeEnum.STRING)</pre>
			<pre class="source-code">  customers_feature_group.feature_definitions = [customer_id_def] + customers_feature_group.feature_definitions</pre>
			<pre class="source-code">  customers_feature_group.create(</pre>
			<pre class="source-code">    s3_uri=f"s3://{s3_bucket_name}/{FEATURE_GROUP_NAME}",</pre>
			<pre class="source-code">    record_identifier_name="customerid",</pre>
			<pre class="source-code">    event_time_feature_name="event_timestamp",</pre>
			<pre class="source-code">    role_arn=role,</pre>
			<pre class="source-code">    enable_online_store=False</pre>
			<pre class="source-code">    )</pre>
			<p class="callout-heading">Important Note</p>
			<p class="callout">In the <strong class="source-inline">load_feature_definitions</strong> call, if you notice, I'm loading all the feature definition columns except the <strong class="source-inline">customerid</strong> column and manually adding the <strong class="source-inline">customerid</strong> column to the feature definitions list in the following line. The reason for this is the <strong class="source-inline">sagemaker</strong> library fails to figure out the <strong class="source-inline">string</strong> data type as the pandas <strong class="source-inline">dtype</strong> for <strong class="source-inline">string</strong> is <strong class="source-inline">object</strong>.</p>
			<p>The <strong class="source-inline">create</strong> feature group call is straightforward. I am disabling the online store by passing <strong class="source-inline">enable_online_store</strong> as <strong class="source-inline">False</strong>, since we will be trying out the batch pipeline and I will leave the online model as an exercise. Once the preceding code block executes, based on <a id="_idIndexMarker553"/>the conditional statement, for the first time, it will create the feature group and for<a id="_idIndexMarker554"/> the subsequent runs, it will skip the feature group creation.</p>
			<p>The final step is to ingest the DataFrame. The following code block performs the ingestion and prints any failures:</p>
			<pre class="source-code">ingestion_results = customers_feature_group.ingest(</pre>
			<pre class="source-code">    churn_data, max_workers=1)</pre>
			<pre class="source-code">ingestion_results.failed_rows</pre>
			<p class="callout-heading">Important Note</p>
			<p class="callout">If you only have the batch use case, SageMaker has a Spark library that can be used to ingest to the offline store directly, which is also cost-effective.</p>
			<p>That completes the feature engineering and ingestion. In the next section, let's look at model training.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor145"/>Model training</h1>
			<p>As before, for the <a id="_idIndexMarker555"/>model training, the feature store is the source. Hence, let's create our model training notebook and install and initialize the required objects for querying the feature store. Here is the link to the notebook:</p>
			<p><a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_model_training.ipynb">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_model_training.ipynb</a></p>
			<p>The following <a id="_idIndexMarker556"/>code block installs the required libraries for model training:</p>
			<pre class="source-code">!pip install sagemaker==2.88.0 s3fs joblib scikit-learn==1.0.2 xgboost</pre>
			<p>After installing the required libraries, initialize the SageMaker session and the required objects:</p>
			<pre class="source-code">import sagemaker</pre>
			<pre class="source-code">from sagemaker.session import Session</pre>
			<pre class="source-code">from sagemaker.feature_store.feature_group import FeatureGroup</pre>
			<pre class="source-code">#import os</pre>
			<pre class="source-code">#os.environ["AWS_ACCESS_KEY_ID"] = "&lt;aws_key_id&gt;"</pre>
			<pre class="source-code">#os.environ["AWS_SECRET_ACCESS_KEY"] = "&lt;aws_secret&gt;"</pre>
			<pre class="source-code">#os.environ["AWS_DEFAULT_REGION"] = "us-east-1"</pre>
			<pre class="source-code">role = "arn:aws:iam::&lt;account_number&gt;:role/sagemaker-iam-role"</pre>
			<pre class="source-code">FEATURE_GROUP_NAME = "telcom-customer-features"</pre>
			<pre class="source-code">sagemaker_session = sagemaker.Session()</pre>
			<pre class="source-code">region = sagemaker_session.boto_region_name</pre>
			<pre class="source-code">s3_bucket_name = "feast-demo-mar-2022"</pre>
			<pre class="source-code">customers_feature_group = FeatureGroup(</pre>
			<pre class="source-code">    name=FEATURE_GROUP_NAME, </pre>
			<pre class="source-code">    sagemaker_session=sagemaker_session</pre>
			<pre class="source-code">)</pre>
			<p>The preceding code block initializes the SageMaker session and initializes the feature group object. The <strong class="source-inline">name</strong> of the feature group should be the same as the <strong class="source-inline">name</strong> of the feature <a id="_idIndexMarker557"/>group that we created in our feature engineering notebook.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Assign the IAM role that was created earlier to the <strong class="source-inline">role</strong> variable. Also, if you are running the notebook outside AWS, you need to uncomment and set up AWS credentials in the preceding code block.</p>
			<p>The next part is to query the historical store to generate the training data. Unlike Feast, we don't need an entity DataFrame here. Instead, we use SQL queries to fetch the historical data. It has the same time travel capabilities as Feast has. For this exercise, let's fetch the latest features for all customers, using a similar query to one that we used in the last chapter, during the SageMaker overview:</p>
			<pre class="source-code">get_latest_snapshot_query = customers_feature_group.athena_query()</pre>
			<pre class="source-code">query = f"""SELECT *</pre>
			<pre class="source-code">FROM</pre>
			<pre class="source-code">    (SELECT *,</pre>
			<pre class="source-code">         row_number()</pre>
			<pre class="source-code">        OVER (PARTITION BY customerid</pre>
			<pre class="source-code">    ORDER BY  event_timestamp desc, Api_Invocation_Time DESC, write_time DESC) AS row_num</pre>
			<pre class="source-code">    FROM "{get_latest_snapshot_query.table_name}")</pre>
			<pre class="source-code">WHERE row_num = 1 and </pre>
			<pre class="source-code">NOT is_deleted;"""</pre>
			<pre class="source-code">get_latest_snapshot_query.run(</pre>
			<pre class="source-code">    query_string=query, </pre>
			<pre class="source-code">    output_location=f"s3://{s3_bucket_name}/output")</pre>
			<pre class="source-code">get_latest_snapshot_query.wait()</pre>
			<p>If you recall correctly, we used a similar nested query in the last chapter. The preceding code block fetches all the customers and their latest features. The output of the query will be written <a id="_idIndexMarker558"/>to a specific S3 location as mentioned in the <strong class="source-inline">run</strong> API call.</p>
			<p>Once the query runs successfully, the dataset can be fetched using the following code block:</p>
			<pre class="source-code">churn_data = get_latest_snapshot_query.as_dataframe()</pre>
			<pre class="source-code">churn_data = churn_data.drop<a id="_idTextAnchor146"/>(columns=["event_timestamp", "write_time", "api_invocation_time", "is_deleted", "row_num"])</pre>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Please note that, we will perform the same steps for model prediction and feature monitoring as we have in this section (<em class="italic">Model training</em>) from the beginning till the preceding code block. </p>
			<p>The preceding code block fetches the dataset and drops the unwanted columns. The fetched dataset is similar to the data shown in <em class="italic">Figure 8.4</em> with additional columns: <strong class="source-inline">write_time</strong>, <strong class="source-inline">api_invocation_time</strong>, <strong class="source-inline">is_deleted</strong>, and <strong class="source-inline">row_num</strong>. The first three are additional metadata columns added by SageMaker during ingestion and <strong class="source-inline">row_num</strong> is the column that we created in the query for fetching the latest features for every customer.</p>
			<p>Now that we have the dataset, let's split it for training and testing. The following code block drops the columns unwanted for training from the dataset and splits the data for training and testing:</p>
			<pre class="source-code">from sklearn.model_selection import train_test_split</pre>
			<pre class="source-code">from sklearn.linear_model import LogisticRegression</pre>
			<pre class="source-code">from sklearn.metrics import confusion_matrix,accuracy_score,classification_report</pre>
			<pre class="source-code">from sklearn.metrics import roc_auc_score,roc_curve</pre>
			<pre class="source-code">from sklearn.metrics import precision_score,recall_score</pre>
			<pre class="source-code">Id_col = ["customerid"]</pre>
			<pre class="source-code">target_col = ["churn"]</pre>
			<pre class="source-code"># Split into a train and test set</pre>
			<pre class="source-code">train, test = train_test_split(churn_data,</pre>
			<pre class="source-code">                               test_size = .25,</pre>
			<pre class="source-code">                               random_state = 111)</pre>
			<pre class="source-code">cols    = [i for i in churn_data.columns if i not in Id_col + target_col]</pre>
			<pre class="source-code">training_x = train[cols]</pre>
			<pre class="source-code">training_y = train[target_col]</pre>
			<pre class="source-code">testing_x  = test[cols]</pre>
			<pre class="source-code">testing_y  = test[target_col]</pre>
			<p>The preceding <a id="_idIndexMarker559"/>code block leaves out the ID column and performs a 75/25 split for training and testing. </p>
			<p>The rest of it is straightforward, which is basically training an XGBoost model, parameter tuning, and comparing the performance. The following is an example code block for training, sample analysis, and logging the model:</p>
			<pre class="source-code">import joblib</pre>
			<pre class="source-code">import boto3</pre>
			<pre class="source-code">model = XGBClassifier(max_depth=7, </pre>
			<pre class="source-code">                      objective='binary:logistic')</pre>
			<pre class="source-code">model.fit(training_x, training_y)</pre>
			<pre class="source-code">predictions = model.predict(testing_x)</pre>
			<pre class="source-code">probabilities = model.predict_proba(testing_x)</pre>
			<pre class="source-code">print("\n Classification report : \n", </pre>
			<pre class="source-code">      classification_report(testing_y, predictions))</pre>
			<pre class="source-code">print("Accuracy   Score : ", </pre>
			<pre class="source-code">      accuracy_score(testing_y, predictions))</pre>
			<pre class="source-code"># confusion matrix</pre>
			<pre class="source-code">conf_matrix = confusion_matrix(testing_y, predictions)</pre>
			<pre class="source-code">model_roc_auc = roc_auc_score(testing_y, predictions)</pre>
			<pre class="source-code">print("Area under curve : ", model_roc_auc, "\n")</pre>
			<pre class="source-code">joblib.dump(model, '/content/customer-churn-v0.0')</pre>
			<pre class="source-code">s3_client = boto3.client('s3')</pre>
			<pre class="source-code">response = s3_client.upload_file('/content/customer-churn-v0.0', s3_bucket_name, "model-repo/customer-churn-v0.0")</pre>
			<p>The preceding <a id="_idIndexMarker560"/>code block also logs the model to a specific location in S3. This is a crude way of doing it. It is always better to use an experiment training tool for logging the performance and the model. </p>
			<p>Now that the model training is complete, let's look at model scoring.</p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor147"/>Model prediction</h1>
			<p>As <a id="_idIndexMarker561"/>mentioned in the last note in the previous section, as this is a batch model, the steps are similar for model scoring for fetching the data from the offline store. However, depending on which customers need to be scored (maybe all), you might filter out the dataset. Once you filter out the dataset, the rest of the steps are again straightforward, which is to load the model, run predictions, and store the results. </p>
			<p>The following is a sample code block for loading the model, running predictions, and also storing the results back in S3 for consumption:</p>
			<pre class="source-code">import boto3</pre>
			<pre class="source-code">from datetime import date</pre>
			<pre class="source-code">s3 = boto3.client('s3')</pre>
			<pre class="source-code">s3.download_file(s3_bucket_name, f"model-repo/customer-churn-v0.0", "customer-churn-v0.0")</pre>
			<pre class="source-code">features = churn_data.drop(['customerid', 'churn'], axis=1)</pre>
			<pre class="source-code">loaded_model = joblib.load('/content/customer-churn-v0.0')</pre>
			<pre class="source-code">prediction = loaded_model.predict(features)</pre>
			<pre class="source-code">prediction.tolist()</pre>
			<pre class="source-code">file_name = f"customer_churn_prediction_{date.today()}.parquet"</pre>
			<pre class="source-code">churn_data["predicted_churn"] = prediction.tolist()</pre>
			<pre class="source-code">s3_url = f's3://{s3_bucket_name}/prediction_results/{file_name}'</pre>
			<pre class="source-code">churn_data.to_parquet(s3_url)</pre>
			<p>The <a id="_idIndexMarker562"/>preceding code block downloads the model from S3, loads the model, scores it against the data fetched from the historical store, and also stores the result in the S3 bucket for consumption. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The library versions of XGBoost, Joblib, and scikit-learn should be the same as what was used while saving the model, otherwise loading of the model might fail.</p>
			<p>To productionize this ML pipeline, we can use orchestration similar to what we did in <a href="B18024_06_ePub.xhtml#_idTextAnchor096"><em class="italic">Chapter 6</em></a>, <em class="italic">Model to Production and Beyond</em>. I will leave that as an exercise since it's duplicate content. Let's look at an example of feature monitoring next.</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor148"/>Feature monitoring</h1>
			<p>We have <a id="_idIndexMarker563"/>discussed how important feature monitoring is in an ML system a few times in the book. We have also talked about how a feature store standardizes feature monitoring. In this section, let's look at an example of feature monitoring that can be useful for any model. As feature monitoring is calculating a set of statistics on feature data and notifying the data scientist or data engineer of changes, it needs the latest features used by the model. </p>
			<p>In this <a id="_idIndexMarker564"/>section, let's calculate the summary stats on the feature data and also feature correlation, which can be run on a schedule and sent to people of interest regularly so that they can take action based on it. As mentioned in the last note of the <em class="italic">Model training</em> section, the steps to fetch the features are the same as what was done in that section. Once you have all the features, the next step is to calculate the required stats. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Please note you may have to install additional libraries. Here is the URL for the notebook: <a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_feature_monitoring.ipynb">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_feature_monitoring.ipynb</a>.</p>
			<p>The following code block calculates the summary stats on the feature data and also plots the correlation metrics:</p>
			<pre class="source-code">import numpy as np</pre>
			<pre class="source-code">import warnings</pre>
			<pre class="source-code">warnings.filterwarnings("ignore")</pre>
			<pre class="source-code">import plotly.offline as py</pre>
			<pre class="source-code">import plotly.graph_objs as go</pre>
			<pre class="source-code">churn_data.describe(include='all').T</pre>
			<p>The preceding line of code produces a descriptive statistic of the dataset, which includes min, max, count, standard deviation, and more.</p>
			<p>Apart from the descriptive statistics, the correlation matrix of the features is another thing that could be useful for all the ML models. The following code block calculates the correlation matrix of the features and plots a heatmap:</p>
			<pre class="source-code">corr = churn_data.corr()</pre>
			<pre class="source-code">cols = corr.columns.tolist()</pre>
			<pre class="source-code">trace = go.Heatmap(z=np.array(corr),</pre>
			<pre class="source-code">                   x=cols,</pre>
			<pre class="source-code">                   y=cols,</pre>
			<pre class="source-code">                   colorscale="Viridis",</pre>
			<pre class="source-code">                   colorbar=dict(</pre>
			<pre class="source-code">                       title="Pearson Coefficient",</pre>
			<pre class="source-code">                       titleside="right"</pre>
			<pre class="source-code">                       ),</pre>
			<pre class="source-code">                   )</pre>
			<pre class="source-code">layout = go.Layout(dict(title="Correlation Matrix",</pre>
			<pre class="source-code">                        height=720,</pre>
			<pre class="source-code">                        width=800,</pre>
			<pre class="source-code">                        margin=dict(r=0, l=210,</pre>
			<pre class="source-code">                                    t=25, b=210,</pre>
			<pre class="source-code">                                    ),</pre>
			<pre class="source-code">                        )</pre>
			<pre class="source-code">                   )</pre>
			<pre class="source-code">fig = go.Figure(data=[trace], layout=layout)</pre>
			<pre class="source-code">py.iplot(fig)</pre>
			<p>The preceding <a id="_idIndexMarker565"/>code block outputs the following heatmap:</p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B18024_08_005.jpg" alt="Figure 8.5 – Feature correlation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – Feature correlation</p>
			<p>You can <a id="_idIndexMarker566"/>add more statistics, in comparison to the previous run, alerts through emails, Slack notifications, and more. This could be in another notebook/Python script, which can be scheduled at the same or at a lesser frequency than the feature engineering notebook and have automated reports sent to you. Here is the link to the complete notebook: <a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_feature_monitoring.ipynb">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_feature_monitoring.ipynb</a>.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">This is just an example of feature monitoring. There are more sophisticated statistics and metrics that can be used to determine the health of the features.</p>
			<p>Let's look at model monitoring next.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor149"/>Model monitoring</h1>
			<p>Another important <a id="_idIndexMarker567"/>aspect of ML is model monitoring. There are different aspects of model monitoring: one could be system monitoring in the case of online models, where you monitor the latency, CPU, memory utilization, requests per minute of the model, and more. The other aspect is performance monitoring of the model. Again, there are many different ways of measuring performance. In this example, we will look at a simple classification report and the accuracy of the model. </p>
			<p>To generate the classification report and calculate the accuracy of the live model, you need the prediction data and also the ground truth of the live data. For this example, let's say that the churn model is run once a week to generate the churn prediction and the ground truth will be available every 4 weeks from the day the model is run. That means if the model predicts customer x's churn as <strong class="source-inline">True</strong>, and within the next 4 weeks, if we lose the customer for any reason, the model predicted correctly; otherwise, the prediction was wrong. Hence, for every run of the model prediction, we need to wait for 4 weeks to have the ground truth. </p>
			<p>For the simplicity of this exercise, let's also assume that the ground truth is filled back into the feature store every week. That means the feature store always has the latest ground truth. Now, our job is to fetch the prediction results that correspond to the latest features in the feature store (the prediction that was run 4 weeks ago) and calculate the required metrics. Let's do that next.</p>
			<p>As mentioned before, the steps to fetch the latest feature from the feature store are the same as what we did in the last three sections. Once you have fetched the data from the feature store, the following code fetches the corresponding prediction results and merges the dataset:</p>
			<pre class="source-code">from datetime import date, timedelta</pre>
			<pre class="source-code">import pandas as pd</pre>
			<pre class="source-code">pred_date = date.today()-timedelta(weeks=4)</pre>
			<pre class="source-code">file_name = f"customer_churn_prediction_{pred_date}.parquet"</pre>
			<pre class="source-code">prediction_data = pd.read_parquet(f"s3://{s3_bucket_name}/prediction_results/{file_name}")</pre>
			<pre class="source-code">prediction_y = prediction_data[["customerid", </pre>
			<pre class="source-code">                                "predicted_churn"]]</pre>
			<pre class="source-code">acutal_y = churn_data[["customerid", "churn"]]</pre>
			<pre class="source-code">merged_data = prediction_y.merge(acutal_y, on="customerid")</pre>
			<pre class="source-code">merged_data.head()</pre>
			<p>The preceding <a id="_idIndexMarker568"/>code block should produce an output similar to the following snapshot.</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B18024_08_006.jpg" alt="Figure 8.6 – Merged data for model monitoring&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – Merged data for model monitoring</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">As we have assumed the prediction and ground truth to be 4 weeks apart, the previous code block tries to fetch the data that is 4 weeks from today. For the exercise, you can replace the <strong class="source-inline">file_name</strong> variable with the prediction output Parquet file.</p>
			<p>Once you <a id="_idIndexMarker569"/>have the DataFrame in <em class="italic">Figure 8.6</em>, the following code block uses the <strong class="source-inline">predicted_churn</strong> and <strong class="source-inline">churn</strong> columns to produce the classification report and accuracy: </p>
			<pre class="source-code">testing_y = merged_data["churn"]</pre>
			<pre class="source-code">predictions = merged_data["predicted_churn"]</pre>
			<pre class="source-code">print("\n Classification report : \n", </pre>
			<pre class="source-code">      classification_report(testing_y, predictions))</pre>
			<pre class="source-code">print("Accuracy   Score : ", </pre>
			<pre class="source-code">      accuracy_score(testing_y, predictions))</pre>
			<pre class="source-code"># confusion matrix</pre>
			<pre class="source-code">conf_matrix = confusion_matrix(testing_y, predictions)</pre>
			<pre class="source-code"># roc_auc_score</pre>
			<pre class="source-code">model_roc_auc = roc_auc_score(testing_y, predictions)</pre>
			<pre class="source-code">print("Area under curve : ", model_roc_auc, "\n")</pre>
			<p>The previous code block produces output similar to the following.</p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B18024_08_007.jpg" alt="Figure 8.7 – Classification report&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – Classification report</p>
			<p>As mentioned before, this is sample monitoring. This can be scheduled at the same interval as the<a id="_idIndexMarker570"/> feature engineering notebook, though it would fail for the first four iterations due to the unavailability of the prediction data. Also make sure you adjust the prediction filename appropriately for your needs. Here is the URL for the complete notebook: <a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_model_monitoring.ipynb">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter08/Ch8_model_monitoring.ipynb</a>.</p>
			<p>With that, let's summarize what we have learned in this chapter. </p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor150"/>Summary </h1>
			<p>In this chapter, we set out with the aim of trying out a use case, namely telecom customer churn prediction using a dataset available from Kaggle. For this use case, we used a managed SageMaker Feature Store, which was introduced in the last chapter. In the exercise, we went through the different stages of ML, such as data processing, feature engineering, model training, and model prediction. We also looked at a feature monitoring and model monitoring example. The aim of this chapter was not model building but to showcase how to use a managed feature store for model building and the opportunities it opens for monitoring. To learn more about feature stores, the apply conference (<a href="https://www.applyconf.com/">https://www.applyconf.com/</a>) and feature store forum (<a href="https://www.featurestore.org/">https://www.featurestore.org/</a>) are good resources. To stay updated with new developments in ML and how other firms are solving similar problems, there are a few interesting podcasts, such as TWIML AI (<a href="https://twimlai.com/">https://twimlai.com/</a>) and Data Skeptic (https://dataskeptic.com/). These resources should help you find more resources based on your area of interest in ML. With that, let's end this chapter and the book. I hope I was effective in conveying the importance of feature stores in the ML process and it was a good use of your time, and mine. Thank you!</p>
		</div>
	</body></html>