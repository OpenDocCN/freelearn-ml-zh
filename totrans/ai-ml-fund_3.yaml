- en: '3'
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Describe the mathematical logic involved in regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Illustrate the use of the NumPy library for Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify linear regression with one variable and with multiple variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use polynomial regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter covers the fundamentals of linear and polynomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regression is a broad topic that connects mathematical statistics, data science,
    machine learning, and artificial intelligence. As the basics of regression are
    rooted in mathematics, we will start by exploring the mathematical fundamentals.
  prefs: []
  type: TYPE_NORMAL
- en: Most of this topic will deal with different forms of linear regression, including
    linear regression with one variable, linear regression with multiple variables,
    polynomial regression with one variable, and polynomial regression with multiple
    variables. Python provides a lot of support for performing regression operations.
  prefs: []
  type: TYPE_NORMAL
- en: We will also use alternative regression models while comparing and contrasting
    support vector Regression with forms of Linear Regression. Throughout this chapter,
    we will use stock price data loaded from an online service provider. The models
    in this chapter are not intended to provide trading or investment advice.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although it is not suggested to use the models in this chapter to provide trading
    or investment advice, it is a very exciting and interesting journey that explains
    the fundamentals of regression.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression with One Variable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A general regression problem can be defined as follows. Suppose we have a set
    of data points. We need to figure out a best fit curve to approximately fit the
    given data points. This curve will describe the relationship between our input
    variable x, which is the data points, and output variable y, which is the curve.
  prefs: []
  type: TYPE_NORMAL
- en: In real life, we often have multiple input variables determining one output
    variable. Regression helps us understand how the output variable changes when
    we keep all but one input variable fixed, and we change the remaining input variable.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Regression?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we will work with regression on the two-dimensional plane.
    This means that our data points are two-dimensional, and we are looking for a
    curve to approximate how to calculate one variable from another.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will learn about the following types of regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear regression with one variable using a polynomial of degree 1** : This
    is the most basic form of regression, where a straight line approximates the trajectory
    of future datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear regression with multiple variables using a polynomial of degree 1**
    : We will be using equations of degree 1, but we will now allow multiple input
    variables, also known as features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial regression with one variable** : This is a generic form of linear
    regression of one variable. As the polynomial used to approximate the relationship
    between the input and the output is of an arbitrary degree, we can create curves
    that fit the data points better than a straight line. The regression is still
    linear – not because the polynomial is linear, but because the regression problem
    can be modeled using linear algebra.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial regression with multiple variables** : This is the most generic
    regression problem using higher degree polynomials and multiple features to predict
    the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support vector regression** : This form of regression uses support vector
    machines to predict data points. This type of regression is included to compare
    its usage to the other four regression types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this topic, we will deal with the first type of linear regression: we will
    use one variable, and the polynomial of the regression will describe a straight
    line.'
  prefs: []
  type: TYPE_NORMAL
- en: On the two-dimensional plane, we will use the Déscartes coordinate system, more
    commonly known as the Cartesian coordinate system. We have an *X* and a *Y* axis,
    and the intersection of these two axes is the origin. We denote points by their
    X and Y coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, the point (2, 1) corresponds to the orange point on the following
    coordinate system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Representation of point (2,1) on the coordinate system'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A straight line can be described with the equation `y = a*x + b` , where a is
    the slope of the equation, determining how steeply the equation climbs up, and
    b is a constant determining where the line intersects the Y axis
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see three equations:'
  prefs: []
  type: TYPE_NORMAL
- en: The blue line is described with the y = 2*x + 1 equation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The orange line is described with the y = x + 1 equation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The purple line is described with the y = 0.5*x + 1 equation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see that all three equations intersect the y-axis at 1, and their slope
    is determined by the factor by which we multiply x.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you know x, you can figure out y. If you know y, you can figure out x:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Representation of the equations y = 2*x + 1, y = x + 1, and y =
    0.5*x + 1 on the coordinate system'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can describe more complex curves with equations too:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Image showing a complex curve'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you would like to experiment more with the Cartesian coordinate system,
    you can use the following plotter: [https://s3-us-west-2.amazonaws.com/oerfiles/College+Algebra/calculator.html](https://s3-us-west-2.amazonaws.com/oerfiles/College+Algebra/calculator.html)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Features and Labels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In machine learning, we differentiate between features and labels. Features
    are considered our input variables, and labels are our output variables.
  prefs: []
  type: TYPE_NORMAL
- en: When talking about regression, the possible values of labels is a continuous
    set of rational numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Think about features as values on the X-axis, and labels as the value on the
    Y-axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task of regression is to predict label values based on feature values.
    We often create a label by shifting values of a feature forward. For instance,
    if we would like to predict stock prices in 1 month, and we create the label by
    shifting the stock price feature 1 month to the future, then:'
  prefs: []
  type: TYPE_NORMAL
- en: For each stock price feature value that's at least 1 month old, training data
    is available that shows the predicted stock price data 1 month in the future
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the last month, prediction data is not available, so these values are all
    NaN (not a number)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must drop the last month, because we cannot use these values for the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Scaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At times, we have multiple features that may have values within completely different
    ranges. Imagine comparing micrometers on a map to kilometers in the real world.
    They won't be easy to handle because of the magnitudinal difference of nine zeros.
  prefs: []
  type: TYPE_NORMAL
- en: A less dramatic difference is the difference between imperial and metric data.
    Pounds and kilograms, and centimeters and inches, don't compare that well.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we often scale our features to normalized values that are easier
    to handle, as we can compare values of this range more easily. We scale training
    and testing data together. Ranges are typically scaled within [-1;1].
  prefs: []
  type: TYPE_NORMAL
- en: 'We will demonstrate two types of scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: Min-Max normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min-max scaling is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Mean normalization is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s an example of Min-Max and min-max:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Scaling could add to the processing time, but often it is a sensible step to
    add.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the scikit-learn library, we have access to a function that scales NumPy
    arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The scale method performs mean normalization. Notice that the result is a NumPy
    array.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Validation with Training and Test Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cross-validation measures the predictive performance of a statistical model.
    The better the cross-validation result, the more you can trust that your model
    can be used to predict the future.
  prefs: []
  type: TYPE_NORMAL
- en: During cross-validation, we test our model's ability to predict the future on
    real **test data** . Test data is not used in the prediction process.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training data** is used to construct the model that predicts our results.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we load data from a data source, we typically separate data into a larger
    chunk of training data, and a smaller chunk of test data. This separation shuffles
    the entries of training and test data randomly. Then, it gives you an array of
    training features, their corresponding training labels, testing features, and
    their corresponding testing labels.
  prefs: []
  type: TYPE_NORMAL
- en: We can do the training-testing split using the `model_selection` library of
    scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose in our dummy example that we have scaled Fibonacci data and its indices
    as labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let's use 10% of the data as test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: With training and testing, if we get the ratios wrong, we run the risk of overfitting
    or underfitting the model.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting occurs when we train the model too well, and it fits the training
    dataset too well. The model will be very accurate on the training data, but it
    will not be usable in real life, because its accuracy decreases when used on any
    other data. The model adjusts to the random noise in the training data and assumes
    patterns on this noise that yield false predictions. Underfitting occurs when
    the model does not fit the training data well enough to recognize important characteristics
    of the data. As a result, it cannot make the necessary predictions on new data.
    One example for this is when we attempt to do linear regression on data that is
    not linear. For instance, Fibonacci numbers are not linear, therefore, a model
    on a Fibonacci-like sequence cannot be linear either.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you remember the Cartesian coordinate system, you know that the horizontal
    axis is the X axis, and that the vertical axis is the Y axis. Our features are
    on the X axis, while our labels are on the Y axis. Therefore, we use features
    and X as synonyms, while labels are often denoted by Y. Therefore, x_test denotes
    feature test data, x_train denotes feature training data, y_test denotes label
    test data, and y_train denotes label training data.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a Model on Data with scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are illustrating the process of regression on a dummy example, where we only
    have one feature and very limited data.
  prefs: []
  type: TYPE_NORMAL
- en: As we only have one feature, we have to format `x_train` by reshaping it with
    `x_train.reshape (-1,1)` to a NumPy array containing one feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, before executing the code on fitting the best line, execute the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x_train = x_train.reshape(-1, 1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`x_test = x_test.reshape(-1, 1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`# array([a, b, c]).reshape(-1, 1) becomes:`'
  prefs: []
  type: TYPE_NORMAL
- en: '`# array([[a, b, c]])`'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have train and test data for our features and labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can fit a model on this data for performing prediction. We will now use
    linear regression for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also calculate the score associated with the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This score is the mean square error and represents the accuracy of the model.
    It represents how well we can predict features from labels.
  prefs: []
  type: TYPE_NORMAL
- en: This number indicates a very bad model. The best possible score is 1.0\. A score
    of 0.0 can be achieved if we constantly predict the labels by ignoring the features.
    We will omit the mathematical background of this score in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our model does not perform well for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: 11 training data and 2 testing data are simply not enough to perform proper
    predictive analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if we ignore the number of points, the Fibonacci `x -> y` function does
    not describe a linear relationship between x and y. Approximating a non-linear
    function with a line is only useful if we are very close to the training interval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will see a lot of more accurate models in the future, and we may even reach
    model scores of 0.9.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression Using NumPy Arrays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One reason why NumPy arrays are handier than Python lists is that they can be
    treated as vectors. There are a few operations defined on vectors that can simplify
    our calculations. We can perform operations on vectors of similar lengths. The
    sum and the (vectorial) product of two vectors equals a vector, where each coordinate
    is the sum or (vectorial) product of the corresponding coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The product of a vector and a scalar is a vector, where each coordinate is
    multiplied by the scalar:'
  prefs: []
  type: TYPE_NORMAL
- en: '`v1 * 2 # array([2, 4, 6])`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second power of a vector equals the vectorial product of the vector with
    itself. The double asterisk denotes the power operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '`v1 ** 2 # array([1, 4, 9], dtype=int32)`'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a set of points in the plane. Our job is to find the best fit
    line.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see two examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first example contains 13 values that seem linear in nature. We are plotting
    the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you wanted to draw a line that is the closest to these dots, your educated
    guess would be quite close to reality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Plotted graph of values [2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61,
    45, 62]'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Our second example is the first 13 values of the Fibonacci sequence, after
    scaling. Although we can define a line that fits these points the closest, we
    can see from the distribution of the points that our model will not be too useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Plotted graph of Fibonacci values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We have already learned what the equation of a straight line is: `y = a * x
    + b`'
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, `a` is the slope, and `b` is the `y` -intercept. To find the
    line of best fit, we have to find the co-efficients `a` and `b` .
  prefs: []
  type: TYPE_NORMAL
- en: Our job is to minimize the sum of distances from the line of best fit.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will save the thought process behind calculating the coefficients
    `a` and `b` , because you will find little practical use for it. We would rather
    utilize the mean as the arithmetic mean of the values in a list. We can use the
    mean function provided by NumPy for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s find the line of best fit for these two examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we plot the line y = a*x + b with the preceding coefficients, we get the
    following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Plotted graph of array values [2, 8, 8, 18, 25, 21, 32, 44, 32,
    48, 61, 45, 62] and the line y=a*x+b'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find a linear regression calculator at [http://www.endmemo.com/statistics/lr.php](http://www.endmemo.com/statistics/lr.php)
    . You can also check the calculator to get an idea of what lines of best fit look
    like on a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the scaled Fibonacci values, the line of best fit looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Plotted graph showing Fibonacci values and the line y=a*x+b'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The best fit line of the second dataset clearly appears more off from anywhere
    outside the trained interval.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We don't have to use this method to perform linear regression. Many libraries,
    including scikit-learn, will help us automatize this process. Once we perform
    linear regression with multiple variables, we are better off using a library to
    perform regression for us.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a Model Using NumPy Polyfit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NumPy Polyfit can also be used to create a line of best fit for linear regression
    with one variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the calculation for the line of best fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The equation for finding the coefficients a and b is quite long. Fortunately,
    `numpy.polyfit` performs these calculations to find the coefficients of the line
    of best fit. The `polyfit` function accepts three arguments: the array of `x`
    values, the array of `y` values, and the degree of polynomial to look for. As
    we are looking for a straight line, the highest power of `x` is 1 in the polynomial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Plotting the Results in Python
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a set of data points and a regression line. Our task is to
    plot the points and the line together so that we can see the results with our
    own eyes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `matplotlib.pyplot` library for this. This library has two
    important functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scatter:** This displays scattered points on the plane, defined by a list
    of x coordinates and a list of y coordinates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plot:** Along with two arguments, this function plots a segment defined by
    two points, or a sequence of segments defined by multiple points. Plot is like
    scatter, except that instead of displaying the points, they are connected by lines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A plot with three arguments plots a segment and/or two points formatted according
    to the third argument
  prefs: []
  type: TYPE_NORMAL
- en: 'A segment is defined by two points. As `x` ranges between 1 and 14, it makes
    sense to display a segment between 0 and 15\. We must substitute the value of
    `x` in the equation `a*x+b` to get the corresponding `y` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Graph displaying how data points fit a regression line'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You might have to call `plot.show()` to display the preceding graph. In the
    IPython console, the coordinate system shows up automatically.
  prefs: []
  type: TYPE_NORMAL
- en: The segment and the scattered data points are displayed as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot has an advanced signature. You can use one call of plot to draw scattered
    dots, lines, and any curves on this diagram. These variables are interpreted in
    groups of three:'
  prefs: []
  type: TYPE_NORMAL
- en: X values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formatting options in the form of a string
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s create a function for deriving an array of approximated y values from
    an array of approximated x values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the `fit` function to plot values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Graph for the plot function using the fit function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Python plotter library offers a simple solution for most of your graphing
    problems. You can draw as many lines, dots, and curves as you want on this graph.
  prefs: []
  type: TYPE_NORMAL
- en: Every third variable is responsible for formatting. The letter g stands for
    green, while the letter r stands for red. You could have used b for blue, y for
    yellow, and so on. In the absence of a color, each triple will be displayed using
    a different color. The o character symbolizes that we want to display a dot where
    each data point lies. Therefore, 'go' has nothing to do with movement – it requests
    the plotter to plot green dots. The '-' characters are responsible for displaying
    a dashed line. If you just use one minus, a straight line appears instead of the
    dashed line.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we simplify this formatting, we can specify that we only want dots of an
    arbitrary color, and straight lines of another arbitrary color. By doing this,
    we can simply write the following plot call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Graph for plot function with dashed line'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When displaying curves, the plotter connects the dots with segments. Also,
    keep in mind that even a complex sequence of curves is an approximation that connects
    the dots. For instance, if you execute the code from [https://gist.github.com/traeblain/1487795](https://gist.github.com/traeblain/1487795)
    , you will recognize the segments of the batman function as connected lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: Graph for the batman function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There is a wide variety of ways to plot curves. We have seen that the `polyfit`
    method of the NumPy library returns an array of coefficients to describe a linear
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This array describes the equation `4.85714286 * x - 2.76923077` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we now want to plot a curve, `y = -x**2 + 3*x - 2` . This quadratic
    equation is described by the coefficient array `[-1, 3, -2].` We could write our
    own function to calculate the y values belonging to x values. However, the NumPy
    library already has a feature to do this work for us: `np.poly1d` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The f function that''s created by the poly1d call not only works with single
    values, but also with lists or NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use these values to plot a non-linear curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: Graph for pyplot function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Predicting Values with Linear Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose we are interested in the `y` value belonging to the `x` coordinate
    `20` . Based on the linear regression model, all we need to do is substitute the
    value of `20` in the place of `x` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: Graph showing the predicted value using Linear Regression'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, we denoted the predicted value with red. This red point is on the best
    fit line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 5: Predicting Population'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You are working at the government office of Metropolis, trying to forecast
    the need for elementary school capacity. Your task is to figure out a 2025 and
    2030 prediction for the number of children starting elementary school. The past
    data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: A table representing number of kids starting elementary school
    from 2001 to 2018'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot these tendencies on a two-dimensional chart. To do this, you must:'
  prefs: []
  type: TYPE_NORMAL
- en: Use linear regression. Features are the years ranging from 2001 to 2018\. For
    simplicity, we can indicate 2001 as year 1, and 2018 as year 18.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `np.polyfit` to determine the coefficients of the regression line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the results using `matplotlib.pyplot` to determine future tendencies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution to this activity is available at page 269.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Linear Regression with Multiple Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous topic, we dealt with linear regression with one variable. Now
    we will learn an extended version of linear regression, where we will use multiple
    input variables to predict the output.
  prefs: []
  type: TYPE_NORMAL
- en: We will rely on examples where we will load and predict stock prices. Therefore,
    we will experiment with the main libraries used for loading stock prices.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Linear Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you recall the formula for the line of best fit in linear regression, it
    was defined as `y = a*x + b` , where `a` is the slope of the line, `b` is the
    y-intercept of the line, `x` is the feature value, and `y` is the calculated label
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In multiple regression, we have multiple features and one label. Assuming that
    we have three features, `x1` , `x2` , and `x3` , our model changes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In NumPy array format, we can write this equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'For convenience, it makes sense to define the whole equation in a vector multiplication
    form. The coefficient of `b` is going to be `1` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Multiple linear regression is a simple scalar product of two vectors, where
    the coefficients `b` , `a1` , `a2` , and `a3` determine the best fit equation
    in four-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the formula of multiple linear regression, you will need the
    scalar product of two vectors. As the other name for a scalar product is dot product,
    the NumPy function performing this operation is called dot:'
  prefs: []
  type: TYPE_NORMAL
- en: '`import numpy as np`'
  prefs: []
  type: TYPE_NORMAL
- en: '`v1 = [1, 2, 3]`'
  prefs: []
  type: TYPE_NORMAL
- en: '`v2 = [4, 5, 6]`'
  prefs: []
  type: TYPE_NORMAL
- en: '`np.dot( v1, v2 ) = 1 * 4 + 2 * 5 + 3 * 6 = 32`'
  prefs: []
  type: TYPE_NORMAL
- en: We simply sum the product of each respective coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: We can determine these coefficients by minimizing the error between data points
    and the nearest points described by the equation. For simplicity, we will omit
    the mathematical solution of the best fit equation, and use scikit-learn instead.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In n-dimensional spaces, where n is greater than 3, the number of dimensions
    determines the different variables that are in our model. In the preceding example,
    we have three features and one label. This yields four dimensions. If you want
    to imagine a four-dimensional space, you can imagine three-dimensional space and
    time for simplification. A five-dimensional space can be imagined as a four-dimensional
    space, where each point in time has a temperature. Dimensions are just features
    (and labels); they do not necessarily correlate with our concept of three-dimensional
    space.
  prefs: []
  type: TYPE_NORMAL
- en: The Process of Linear Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will follow the following simple steps to solve Linear Regression problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Load data from data sources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare data for prediction (normalize, format, filter).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the parameters of the regression line. Regardless of whether we use
    linear regression with one variable or with multiple variables, we will follow
    these steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Importing Data from Data Sources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are multiple libraries that can provide us with access to data sources.
    As we will be working with stock data, let''s cover two examples that are geared
    toward retrieving financial data, Quandl and Yahoo Finance:'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn comes with a few datasets that can be used for practicing your
    skills.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Quandl.com](http://Quandl.com) provides you with free and paid financial datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[pandas.io](http://pandas.io) helps you load any .csv, .excel, .json, or SQL
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yahoo Finance provides you with financial datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading Stock Prices with Yahoo Finance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The process of loading stock data with Yahoo Finance is straightforward. All
    you need to do is install the fix_yahoo_finance package using the following command
    in the CLI::'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We will download a dataset that has open, high, low, close, adjusted close,
    and volume values of the S&P 500 index, starting from 2015:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: That's all you need to do. The data frame containing the S&P 500 index is ready.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can plot the index prices with the plot method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15: Graph showing stock prices for Yahoo Finance'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It is also possible to save data to a CSV file using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Loading Files with pandas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose a CSV file containing stock data is given. We will now use pandas to
    load data from this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: To properly parse data, we must set the index column name, specify the absence
    of headers, and make sure that dates are parsed as dates.
  prefs: []
  type: TYPE_NORMAL
- en: Loading Stock Prices with Quandl
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quandl.com is a reliable source of financial and economic datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 8: Using Quandl to Load Stock Prices'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Open the Anaconda prompt and install Quandl using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Go to [https://www.quandl.com/](https://www.quandl.com/) .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the Financial data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Among the filters, click the checkbox next to the **Free** label. If you have
    a Quandl subscription, you can use that to download stock data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a stock or index you would like to use. For our example, we will use
    the S&P Composite index data that was collected by the Yale Department of Economics.
    The link for this is [https://www.quandl.com/data/YALE/SPCOMP-S-P-Composite](https://www.quandl.com/data/YALE/SPCOMP-S-P-Composite)
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the Quandl ticker belonging to your instrument you would like to load.
    Our Quandl code for the S&P 500 data is "`YALE/SPCOMP` ".
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the data from the Jupyter QtConsole:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'All the columns of the imported values are features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Preparing Data for Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we perform regression, we must choose the features we are interested
    in, and we also have to figure out the data range on which we do the regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Preparing the data for prediction is the second step in the regression process.
    This step also has several sub-steps. We will go through these sub-steps in the
    following order:'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose a data frame is given with preloaded data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the columns from the dataset you are interested in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace NaN values with a numeric value to avoid getting rid of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the forecast interval T, determining the amount of time or number
    of data rows you wish to look into in the future.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a label column out of the value you wish to forecast. For row i of the
    data frame, the value of the label should belong to the time instant, i+T.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the last T rows, the label value is NaN. Drop these rows from the data frame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create NumPy arrays from the features and the label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale the features array.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separate the training and testing data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A few features highly correlate to each other. For instance, the Real Dividend
    column proportionally grows with Real Price. The ratio between them is not always
    similar, but they do correlate.
  prefs: []
  type: TYPE_NORMAL
- en: As regression is not about detecting correlation between features, we would
    rather get rid of a few attributes we know are redundant and perform regression
    on the features that are non-correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have gone through the Loading stock prices with Quandl section, you
    already have a data frame containing historical data on the S&P 500 Index. We
    will keep the Long Interest Rate, Real Price, and Real Dividend columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'As you cannot work with NaN data, you can replace it by filling in numbers
    in place of NaNs. In general, you have two choices:'
  prefs: []
  type: TYPE_NORMAL
- en: Get rid of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace the data with a default value that makes sense
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check the length of the data frame by using the `len` function, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The length of our data frame is 1771.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to predict the Real Price for the upcoming 20 years, we will have
    to predict 240 values. This is approximately 15% of the length of the data frame,
    which makes perfect sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will therefore create a Real Price Label by shifting the Real Price values
    up by 240 units:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This way, each Real Price Label value will be the Real Price value in 20 years.
  prefs: []
  type: TYPE_NORMAL
- en: 'The side effect of shifting these values is that NaN values appear in the last
    240 values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can get rid of them by executing dropna on the data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This way, we have data up to 1998 July, and we have the future values up to
    2018 in the Real Price Label column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Let's prepare our features and labels for regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the features, we will use the drop method of the data frame. The drop method
    returns a new data frame that doesn''t contain the column that was dropped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The 1 in the second argument specifies that we are dropping columns. As the
    original data frame was not modified, the label can be directly extracted from
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is now time to scale the features with the preprocessing module of Scikit
    Learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the scaled features are easier to read and interpret. While
    scaling data, we must scale all data together, including training and testing
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Performing and Validating Linear Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that scaling is done, our next task is to separate the training and testing
    data from each other. We will be using 90% of the data as training data, and the
    rest (10%) will be used as test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The train_test_split function shuffles the lines of our data, keeping the correspondence,
    and puts approximately 10% of all data in the test variables, keeping 90% for
    the training variables. This will help us evaluate how good our model is.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now create the linear regression model based on the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is ready, we can use it to predict the labels belonging to the
    test feature values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are interested in the relationship between the predicted feature values
    and the accurate test feature values, you can plot them using a Python two-dimensional
    graph plotter utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This gives you an image of a graph where the test data is compared to the results
    of the prediction. The closer these values are to the `y = x` line, the better.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see from the following graph that the predictions do center around
    the `y=x` line with a degree of error. This error is obvious, as otherwise, we
    would be able to make a lot of money with such a simple prediction, and everyone
    would pursue predicting stock prices instead of working in their own field of
    expertise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.16: Graph for the plot scatter function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can conclude that there is a degree of error in the model. The question
    is, how can we quantify this error? The answer is simple: we can score the model
    using a built-in utility that calculates the mean square error of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We can conclude that the model is very accurate. This is not a surprise, because
    every single financial advisor scammer tends to tell us that the market grows
    at around 6-7% a year. This is a linear growth, and the model essentially predicts
    that the markets will continue growing at a linear rate. Making a conclusion that
    markets tend to go up in the long run is not rocket science.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the Future
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have already used prediction on test data. Now, it's time to use the actual
    data to see into the future.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The trick to predicting the future is that we have to save the values belonging
    to the values we dropped when building the model. We built our stock price model
    based on historical data from 20 years ago. Now, we have to keep this data, and
    we also have to include this data in scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have access to the scaled values of the features from the last
    20 years, we can now predict the index prices of the next 20 years using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This sounds great in theory, but in practice, using this model for making money
    by betting on the forecast is by no means better than gambling in a casino. This
    is just an example model to illustrate prediction; it is definitely not sufficient
    to be used for short-term or long-term speculation on market prices.
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the values, you can see why this prediction may easily backfire.
    First, there are a few negative values, which are impossible for indices. Then,
    due to a few major market crashes, linear regression made a doomsday forecast
    a point in the future, where the index will drop from more than 3,000 to literally
    zero within a year. Linear regression is not a perfect tool to look ahead 20 years
    based on limited data. Also, note that stock prices are meant to be close to time
    invariant systems. This means that the past does not imply any patterns in the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s output the prediction belonging to the first ten years:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00036.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.17: Graph for the plot function with a range of 1 to 241 and the label
    predicted as 240'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The graph is hard to read near the end due to extreme values. Let''s draw our
    conclusions by omitting the last five years and just plotting the first 180 months
    out of the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00037.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.18: Graph for the plot function with a range of 1 to 181 and the label
    predicted as 180'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is a scary future for the American economy. According to this model, the
    S&P 500 has a bull run for about 2.5-3 years and doesn't recover for a long time.
    Also, notice that our model does not know that index values cannot be negative.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial and Support Vector Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When performing polynomial regression, the relationship between x and y, or
    using their other names, features and labels, is not a linear equation, but a
    polynomial equation. This means that instead of the `y = a*x+b` equation, we can
    have multiple coefficients and multiple powers of x in the equation.
  prefs: []
  type: TYPE_NORMAL
- en: To make matters even more complicated, we can perform polynomial regression
    using multiple variables, where each feature may have coefficients multiplying
    different powers of the feature.
  prefs: []
  type: TYPE_NORMAL
- en: Our task is to find a curve that best fits our dataset. Once polynomial regression
    is extended to multiple variables, we will learn the Support Vector Machines model
    to perform polynomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial Regression with One Variable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a recap, we have performed two types of regression so far:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Simple linear regression: `y = a*x + b`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiple linear regression: `y = b + a1 * x1 + a2 * x2 + … + an * xn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will now learn how to do polynomial linear regression with one variable.
    The equation for polynomial linear regression is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`y = b + a1*x + a2*(x ** 2) + a3*(x ** 3) + … + an * (x ** n)`'
  prefs: []
  type: TYPE_NORMAL
- en: have a vector of coefficients `(b, a1, a2, …, an)` multiplying a vector of degrees
    of x in the polynomial, `(1, x**1, x**2, …, x**n)` .
  prefs: []
  type: TYPE_NORMAL
- en: At times, polynomial regression works better than linear regression. If the
    relationship between labels and features can be described using a linear equation,
    then using linear equation makes perfect sense. If we have a non-linear growth,
    polynomial regression tends to approximate the relationship between features and
    labels better.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest implementation of linear regression with one variable was the `polyfit`
    method of the NumPy library. In the next exercise, we will perform multiple polynomial
    linear regression with degrees of 2 and 3.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Even though our polynomial regression has an equation containing coefficients
    of x ** n, this equation is still referred to as polynomial linear regression
    in the literature. Regression is made linear not because we restrict the usage
    of higher powers of x in the equation, but because the coefficients a1, a2, …,
    and so on are linear in the equation. This means that we use the toolset of linear
    algebra, and work with matrices and vectors to find the missing coefficients that
    minimize the error of the approximation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 9: 1st, 2nd, and 3rd Degree Polynomial Regression'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perform a 1st, 2nd, and 3rd degree polynomial regression on the following two
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Then plot your results on the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with plotting the first example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00038.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.19: Graph showing the first dataset with linear curves'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As the coefficients are enumerated from left to right in order of decreasing
    degree, we can see that the higher degree coefficients stay close to negligible.
    In other words, the three curves are almost on top of each other, and we can only
    detect a divergence near the right edge. This is because we are working on a dataset
    that can be very well approximated with a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the first dataset was created out of a linear function. Any nonzero
    coefficients for x**2 and x**3 are the results of overfitting the model based
    on the available data. The linear model is better for predicting values outside
    the range of the training data than any higher degree polynomial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s contrast this behavior with the second example. We know that the Fibonacci
    sequence is non-linear. So, using a linear equation to approximate it is a clear
    case for underfitting. Here, we expect a higher degree polynomic to perform better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19: Graph showing second dataset points and three polynomial curves
    ](img/Image00039.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.20: Graph showing second dataset points and three polynomial curves'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The difference is clear. The quadratic curve fits the points a lot better than
    the linear one. The cubic curve is even better.
  prefs: []
  type: TYPE_NORMAL
- en: If you research Binet's formula, you will find out that the Fibonacci function
    is an exponential function, as the xth Fibonacci number is calculated as the xth
    power of a constant. Therefore, the higher degree polynomial we use, the more
    accurate our approximation will be.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial Regression with Multiple Variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When we have one variable of degree n, we have n+1 coefficients in the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Once we deal with multiple features x1, x2, …, xm, and their powers of up to
    the nth degree, we get an m * (n+1) matrix of coefficients. The math will become
    quite lengthy once we start exploring the details and prove how a polynomial model
    works. We will also lose the nice visualizations of two-dimensional curves.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we will apply the chapters learned in previous section on polynomial
    regression with one variable and omit the math. When training and testing a Linear
    Regression model, we can calculate the mean square error to see how good an approximation
    a model is.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, the degree of the polynomials used in the approximation is
    a simple parameter in the model.
  prefs: []
  type: TYPE_NORMAL
- en: As polynomial regression is a form of linear regression, we can perform polynomial
    regression without changing the regression model. All we need to do is transform
    the input and keep the linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The transformation of the input is performed by the `fit_transform` method
    of the `PolynomialFeatures` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The model scores too well. Chances are, the polynomial model is overfitting
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There is another model in scikit-learn that performs polynomial regression called
    the SVM model, which stands for Support Vector Machines.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Support Vector Machines are binary classifiers defined on a vector space. Vector
    Machines divide the state space with a surface. An SVM classifier takes classified
    data and tries to predict where unclassified data belongs. Once the classification
    of a data point is determined, it gets labeled.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines can also be used for regression. Instead of labeling
    data, we can predict future values in a series. The Support Vector Regression
    model uses the space between our data as a margin of error. Based on the margin
    of error, it makes predictions regarding future values.
  prefs: []
  type: TYPE_NORMAL
- en: If the margin of error is too small, we risk overfitting the existing dataset.
    If the margin of error is too big, we risk underfitting the existing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: A kernel describes the surface dividing the state space in the case of a classifier.
    A kernel is also used to measure the margin of error in the case of a regressor.
    This kernel can use a linear model, a polynomial model, or many other possible
    models. The default kernel is **RBF** , which stands for **Radial Basis Function**
    .
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Regression is an advanced topic, which is outside the scope of
    this book. Therefore, we will only stick to a walkthrough of how easy it is to
    try out another regression model on our test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have our features and labels in two separate NumPy arrays. Let''s
    recall how we performed linear regression on them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We can perform regression with Support Vector Machines by changing the linear
    model to a support vector model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00040.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.21: Graph showing Support Vector regression with a linear model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: The model score is quite low, and the points don't align on the `y=x` line.
    Prediction with the default values is quite low.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the model describes the parameters of the Support Vector Machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: We could fiddle with these parameters to increase the accuracy of the prediction
    by creating a better algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines with a 3 Degree Polynomial Kernel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s switch the kernel of the Support Vector Machine to poly. The default
    degree of the polynomial is 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00041.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.22: Graph showing Support Vector regression with a polynomial kernel
    of degree 3'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: With Support Vector Machines, we often end up with points concentrated in small
    areas. We could change the margin of error to separate the points a bit more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6: Stock Price Prediction with Quadratic and Cubic Linear Polynomial
    Regression with Multiple Variables'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will discuss how to perform linear, polynomial, and support
    vector regression with scikit-learn. We will also learn how to find the best fit
    model for a given task. We will be assuming that you are a software engineer at
    a financial institution and your employer wants to know whether linear regression
    or support vector regression is a better fit for predicting stock prices. You
    will have to load all of the data of the S&P 500 from a data source. Then, you
    will need to build a regressor using linear regression, cubic polynomial linear
    regression, and a support vector regression with a polynomial kernel of degree
    3 before separating the training and test data. Plot the test labels and the prediction
    results and compare them with the y=x line. Finally, compare how well the three
    models score.
  prefs: []
  type: TYPE_NORMAL
- en: Load the S&P 500 index data using Quandl, and then prepare the data for prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a polynomial of degree 1 for the evaluation of the model and for the prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The closer the dots are to the y=x line, the less error the model works with.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Perform a linear multiple regression with quadratic polynomials. The only change
    is in the Linear Regression model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Perform Support Vector regression with a polynomial kernel of degree 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model does not look efficient at all. For some reason, this model clearly
    prefers lower values for the S&P 500 that are completely unrealistic, assuming
    that the stock market does not lose 80% of its value within a day.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution to this activity is available at page 271.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have learned the fundamentals of Linear Regression.
  prefs: []
  type: TYPE_NORMAL
- en: After going through some basic mathematics, we dived into the mathematics of
    linear regression using one variable and multiple variables.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges occurring with regression include loading data from external sources
    such as a .csv file, Yahoo Finance, or Quandl were dealt with. After loading the
    data, we learned how to identify the features and labels, how to scale data, and
    how to format data to perform regression.
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to train and test a linear regression engine, and how to predict
    the future. Our results were visualized by an easy-to-use Python graph plotting
    library called `pyplot` .
  prefs: []
  type: TYPE_NORMAL
- en: A more complex form of linear regression is a linear polynomial regression of
    arbitrary degree. We learned how to define these regression problems on multiple
    variables. We compared their performance to each other on stock price prediction
    problems. As an alternative to polynomial regression, we also introduced Support
    Vector Machines as a regression model and experimented with two kernels.
  prefs: []
  type: TYPE_NORMAL
- en: You will soon learn about another field inside machine learning. The setup and
    code structure of this machine learning method will be very similar to regression,
    while the problem domain is somewhat different. In the next chapter, you will
    learn the ins and outs of classification.
  prefs: []
  type: TYPE_NORMAL
