<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Hands-On Examples of Machine Learning Models</h1>
                </header>
            
            <article>
                
<p>Supervised learning is the simplest way of teaching a model about how the world looks. Showing how a given combination of input variables leads to a certain output, that is, using labeled data, makes it possible for a computer to predict the output for another similar dataset that it has never seen. Unsupervised learning deals with finding patterns and useful insights into non-labeled data.</p>
<p>We will study different types of machine learning models, trying to understand the details and actually performing the necessary calculations so that the inner workings of these models are clear and reproducible.</p>
<p>In this chapter, the following topics will be covered:</p>
<ul>
<li><span>Understanding s</span>upervised learning with multiple linear regression</li>
<li><span>Understanding s</span>upervised learning with decision trees</li>
<li><span>Understanding u</span>nsupervised learning with clustering</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>There are no technical requirements for this chapter. We just need to input the values shown in the tables within each section in an Excel sheet in order to follow the explanation closely.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding supervised learning with multiple linear regression</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, we followed an example of linear regression using two variables. It is interesting to see how we can apply regression to more than two variables (called <strong>multiple linear regression</strong>) and extract useful information from the results.</p>
<p class="mce-root">Suppose that you are asked to test whether there exists a hidden policy of gender discrimination in a company. You could be working for a law firm that is leading a trial against this company, and they need data-based evidence to <span>back up</span><span> their claim.</span></p>
<p class="mce-root">You would start by taking a sample of the company's payroll, including several variables that describe each employee and the last salary increase amount. The following screenshot shows a set of values after they've been entered in an Excel worksheet:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/1c0631ca-5c1a-4395-8e26-09a4274b0473.png" style="width:33.33em;height:23.50em;"/></p>
<p class="mce-root">There are four numerical features in the dataset:</p>
<ul>
<li><kbd>ID</kbd>: The employee identification, which is not relevant to our analysis</li>
<li><kbd>Score</kbd>: The result of the last employee's performance evaluation </li>
<li><kbd>Years in company</kbd>: Years that the employee has worked in the company</li>
<li><kbd>Salary increase</kbd>: Amount in dollars of the last salary increase</li>
</ul>
<p class="mce-root">The remaining two are categorical:</p>
<ul>
<li><kbd>Gender</kbd>: Male (<kbd>M</kbd>) or Female (<kbd>F</kbd>)</li>
<li><kbd>Division</kbd>: In which part of the company the employee works</li>
</ul>
<p class="mce-root">Categorical values need to be encoded before being used in a model. The final data table is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4a60e2c4-eb38-4adf-9357-2283b2bcc6a2.png" style="width:43.58em;height:22.58em;"/></p>
<p class="mce-root">The one-hot encoding is easily obtained by applying standard Excel functions. Assuming <em>B2</em> is the first cell containing the gender classification, we can enter <em>=IF(B2="F";1;0) </em>in cell <em>B21</em> and copy this value to all cells down to <em>B37</em>.</p>
<div class="packt_tip">Depending on which character is defined in the Windows list separator option, you should either use a comma (<em>,</em>) or a semi-colon (<em>;</em>) in formulas.</div>
<p class="mce-root">To encode the employee's division, we use one-hot encoding (refer to <a href="b0dde0bb-32ef-4535-9e19-7999e8e9a631.xhtml">Chapter 1</a>, <em>Implementing Machine Learning Algorithms</em>, for a detailed explanation) and create three new variables: <kbd>IsProduction?</kbd><span>,</span> <kbd>IsResearch?</kbd><em>,</em> <span>and</span> <kbd>IsSales?</kbd><em>.</em><span> We can use Excel functions to calculate the encoding if <em>E2</em> is the first row containing the <kbd>Division</kbd> data, then we can use the <em>=IF(E2="Production";1;0)</em>, </span><em>=IF(E2="Research";1;0),</em> and <em>=IF(E2="Sales";1;0)</em> <span>functions </span>in cells <em>E21</em>, <em>F21,</em> and <em>G21</em>, respectively, and then copy them column-wise <span>down to cells <em>E37</em>, <em>F37,</em> and <em>G37</em>.</span></p>
<p class="mce-root">Before trying to use regression on the full dataset, we can try some feature engineering. Let's see how well we can predict the salary increase based on which <kbd>Division</kbd> each employee works. This will give us an idea of how much the <kbd>Salary Increase</kbd><em> </em>target variable correlates with <kbd>Division</kbd> (there will be more details about correlations between variables in <a href="0da64bd8-0bc9-491b-875c-7ec7c35c6165.xhtml">Chapter 5</a>, <em>Correlations and the Importance of Variables</em>).</p>
<p>Let's follow some simple steps to use the built-in regression tool:</p>
<ol>
<li class="mce-root">Navigate to <span class="packt_screen">Data</span>.</li>
<li class="mce-root">Click on <span class="packt_screen">Data Analysis</span>, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/40602c2b-0444-4e3e-b685-20b98aa69cb3.png"/></p>
<ol start="3">
<li class="mce-root">Select <span class="packt_screen">Regression</span>, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/423d05a3-7725-42f1-816e-ff01211cc34e.png" style="width:36.17em;height:16.83em;"/></p>
<ol start="4">
<li class="mce-root">As the <span class="packt_screen">Input Y Range</span>, select the <kbd>Salary</kbd> data and as the <span class="packt_screen">Input X Range</span>, select the three <kbd>Division</kbd> columns:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/11bab1bf-9e2d-4ccb-8054-15419abf959b.png" style="width:32.83em;height:28.25em;"/></p>
<p class="mce-root"><span>The results show <em>R</em></span><em><sup>2</sup></em><span><em> = 0.1</em>, meaning that only 10% of the salary increase is related or can be explained by the fact that the employee belongs to a given division. We can therefore discard these columns as input and concentrate on the rest. </span></p>
<p><span>We repeat the regression, now choosing the X values as the columns</span> <kbd>Gender</kbd>, <kbd>Score</kbd>, <span>and</span> <kbd>Years in company</kbd>.</p>
<p class="mce-root">The results are quite different now, with R<sup>2</sup> close to 0.85, meaning that 85% of the salary increase values are explained by the chosen variables.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">How important is <kbd>Gender</kbd>? Taking a look at the P-value coefficients that Excel gives us, in the following table, we can see that, according to the P-value associated with the input variables, the most important one is gender, followed by the score and the number of years in the company. It is then clear that gender plays an important role when deciding a salary increase, and we have evidence to prove that the company policy is not gender neutral:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td/>
<td>
<p><strong>Coefficients</strong></p>
</td>
<td>
<p><strong>P-value</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>Intercept</strong></p>
</td>
<td>
<p>141.72775</p>
</td>
<td>
<p>0.083481944</p>
</td>
</tr>
<tr>
<td>
<p><strong>Gender</strong></p>
</td>
<td>
<p>-221.9209346</p>
</td>
<td>
<p>6.47796E-05</p>
</td>
</tr>
<tr>
<td>
<p><strong>Score</strong></p>
</td>
<td>
<p>2.697512241</p>
</td>
<td>
<p>0.004201513</p>
</td>
</tr>
<tr>
<td>
<p><strong>Years in company</strong></p>
</td>
<td>
<p>8.118352407</p>
</td>
<td>
<p>0.332588988</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The output results of the regression tell us how well we can explain the data sample, but cannot give us an accurate measure of how the model will predict a salary increase. To explore this, we should do the following:</p>
<ul>
<li>Obtain a different sample of the payroll (in our case, we could generate new data by hand)</li>
<li>Use the coefficients in the previous table to build an expression and calculate the predicted salary increase given the input variables</li>
<li>Compare the predicted and real values using root mean square error, as explained in <a href="b0dde0bb-32ef-4535-9e19-7999e8e9a631.xhtml">Chapter 1</a>, <em>Implementing Machine Learning Algorithms</em></li>
</ul>
<p><span>Let's see if you can finish this exercise; I am hoping that the basic information that's been provided to you carry this out has been understood</span>. </p>
<p>We have shown how to perform a multiple linear regression in data to extract interesting insights from them. Let's continue with another important machine learning model: decision trees.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding supervised learning with decision trees</h1>
                </header>
            
            <article>
                
<p>The decision tree algorithm uses a tree-like model of decisions. Its name is derived from the graphical representation of the cascading process that partitions the records. The algorithm chooses the input variables that better split the dataset into subsets that are more pure in terms of the target variable, ideally a subset that contains only one value of this variable. Decision trees are some of the most widely used and easy to understand classification algorithms. </p>
<p>The outcome of the tree algorithm calculation is a set of simple rules that explain which values or intervals of the input values split the original data better. The fact that the results and the path followed to get to them can be clearly shown gives decision trees an advantage over other algorithms. <strong>Explainability</strong> is a serious problem for some machine learning and artificial intelligence systems – which are mostly used as black boxes – and is a study subject in itself.</p>
<p>In complex problems, we need to decide when to stop the tree development. A large number of features can lead to a very large and complex tree, so the number of branches and the length of the tree are usually limited by the user. </p>
<p>Entropy is a very important concept in decision trees and the way of quantifying the purity of each subsample. It measures the amount of information contained in each leaf of the tree. The lower the entropy, the larger the amount of information. Zero entropy means that a subset contains only one value of the target variable, while a value of one represents a subset that contains the same amount of both values. This concept will be explained later with examples.</p>
<div class="packt_tip"><span>Entropy is an indicator of how messy your data is.</span></div>
<p>Using the entropy that's calculated in every step, the algorithm chooses the best variable to split the data and recursively repeats the same procedure. The user can decide how to stop the calculation, either when all subsets have an entropy of zero, when there are no more features to split by, or a minimum entropy level.</p>
<p>The input features that are best suited for use in a decision tree are the categorical ones. In case of a continuous, numerical variable, it should be first converted into categories by dividing it into ranges; for example, A &gt; 0.5 would be A1 and A ≤ 0.5 would be A2.</p>
<p>Let's look at an example that explains the concept of the decision tree algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deciding whether to train outdoors depending on the weather</h1>
                </header>
            
            <article>
                
<p>Let's suppose we have historical data on the decisions made by an experienced football trainer about training outdoors (outside the gym) or not with her team, including the weather conditions on the days when the decisions were made.</p>
<p>A typical dataset could look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/978298f2-c1f8-4e2d-8e85-1de0fc8e7e74.png" style="width:27.92em;height:19.17em;"/></p>
<p>The dataset was specifically created for this example and, of course, might not represent any real decisions.</p>
<p>In this example, the target variable is <kbd>Train outside</kbd> and the rest of the variables are the model features.</p>
<p>According to the data table, a possible decision tree would be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7e2f36c5-6565-43d4-9a5a-d1b35c0e7724.png" style="width:22.08em;height:23.08em;"/></p>
<p>We choose to start splitting the data by the value of the <strong><span class="packt_screen">Outlook</span> </strong>feature.<em> </em>We can see that if the value is <strong><span class="packt_screen">Overcast</span></strong>,<em> </em>then the decision to train outside is always <strong><span class="packt_screen">Yes</span></strong><em> </em>and does not depend on the values of the other features. <strong><span class="packt_screen">Sunny</span></strong> and <strong><span class="packt_screen">Rainy</span></strong> can be further split to get an answer. </p>
<p><span>How can we decide which feature to use first and how to continue? We will use th</span><span>e value of the </span>entropy<em>,</em> measuring<span> how much its value changes when considering different input features.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Entropy of the target variable</h1>
                </header>
            
            <article>
                
<p>The definition of entropy when looking at a single attribute is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f157814c-737c-458d-9303-4c13687692c9.png" style="width:13.92em;height:3.92em;"/></p>
<p>Here, <em>c</em> is the total number of possible values of the feature <em>f</em>, <em>p<sub>i</sub></em> is the probability of each value, and <em>log</em><sub><em>2</em></sub><em>(p<sub>i</sub>)</em><sub> </sub>is the base two logarithm of the same probability. The calculation details are as follows:</p>
<ol>
<li>We need to count the number of Yes and No decisions in the dataset. In our simple example, they can be counted by hand, but if the dataset is larger, we can use Excel functions:</li>
</ol>
<p style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><em>COUNTIF(F2:F15;"Yes")</em> and <em>COUNTIF(F2:F15;"No")</em></p>
<p style="padding-left: 60px"><span>We then get the calculation that <em>Yes = 9</em> and <em>No = 5</em>.</span></p>
<ol start="2">
<li>When applying the entropy formula to the target variable, we get the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4efa2635-4bc5-4980-89eb-b009696db614.png" style="width:43.92em;height:3.42em;"/></p>
<p style="padding-left: 60px">Here, the probabilities are calculated as the number of <em>Yes</em> (<em>9</em>) or <em>No</em> (<em>5</em>) over the total number (<em>14</em>).</p>
<div class="packt_tip">This calculation can also be easily performed in the Excel sheet using <em>I3/(I3+J3)*LOG(I3/(I3+J3);2)-J3/(I3+J3)*LOG(J3/(I3+J3);2)</em> with <em>I3=9</em> and <em>J3=5</em>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Entropy of each feature with respect to the target variable</h1>
                </header>
            
            <article>
                
<p>The entropy of two variables <em>f</em><sub><em>1</em> </sub>and <em>f<sub>2</sub></em> <span>i</span><span>s</span> <span>defined as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0b6434d3-d2fb-4a52-820d-e3b14870dc30.png" style="width:16.83em;height:3.75em;"/></p>
<p>Here, <em>v</em> represents each possible value of <em><span>f</span><sub>2</sub></em>, <em>P(v)</em> is the probability of each value, and <em>S(v)</em> was defined in the previous equation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Frequency table</h1>
                </header>
            
            <article>
                
<p>Let's b<span>uild a frequency table</span>, which is the usual way of counting the total number of combinations between variables. In our case, we use it to decide which variable choice leads to a larger reduction of the entropy:</p>
<ol>
<li>Count the different combinations of feature values, taking each feature compared to the <kbd>Train outside</kbd><span> target variable.</span> You can count them manually in this particular example, but it is useful to have a general method to do this in case we are working with a larger dataset.</li>
<li>To count the number of feature combinations, we start by concatenating the values in the data table in pairs. For example, <em>CONCATENATE(B2;"_";F2)</em> gives us <kbd>Hot_No</kbd>.</li>
<li>If we copy the formula down to complete the total number of rows, we get all possible combinations of the <kbd>Temperature</kbd> and <kbd>Train outside</kbd> variables.</li>
<li>If we repeat the same calculation with the rest of the features, the results will be as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6c8884f8-2fee-4191-aa34-c46ccf88f197.png" style="width:45.42em;height:18.75em;"/></p>
<ol start="5">
<li>Create pivot tables to count the number of unique values in each column, that is, the number of unique combinations. This can be done by selecting the full range in the column, right-clicking anywhere in the selection, and left-clicking on <span class="packt_screen">Quick Analysis</span>. The following dialogue will pop up:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/05e68fd4-4daf-4008-b468-7fe4b4ea5b14.png" style="width:24.17em;height:29.50em;"/></p>
<ol start="6">
<li>Select <span class="packt_screen">Tables</span> | <span class="packt_screen">PivotTable</span><em> </em>to create a table like the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/df9d5cee-2ee6-4af7-b19a-3e7fb08a59d3.png" style="width:22.42em;height:10.92em;"/></p>
<ol start="7">
<li>Repeat the same procedure with all columns and build all frequency tables and the two-variable entropy. The resulting tables and the entropy calculations are shown in the following subsection.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Entropy calculation</h1>
                </header>
            
            <article>
                
<p>The frequency table for the combination Outlook-Train outside is as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td rowspan="5" colspan="1">
<p><strong>Outlook</strong></p>
</td>
<td rowspan="1" colspan="3">
<p><strong>Train outside</strong></p>
</td>
</tr>
<tr>
<td/>
<td>
<p>Yes</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>Sunny</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>2</p>
</td>
</tr>
<tr>
<td>
<p>Overcast</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>Rainy</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>3</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Using these values, we get the entropy of two variables, as shown here in detail:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/376672c9-2d09-4c28-8fab-67e9601dc316.png" style="width:31.08em;height:3.08em;"/></p>
<p><em>p(Sunny).S(Sunny)+p(Overcast).S(Overcast)+p(Rainy)*S(Rainy)=</em></p>
<p><em>5/14*(-3/5*log2(3/5)-2/5*log2(2/5)) +</em></p>
<p><em>4/14*(-4/4*log2(4/4)-0/4*log2(0/4))+</em></p>
<p><em>5/14*(-2/5*log2(2/5)-3/5*log2(3/5))=</em></p>
<p><em>0.693</em></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Here, <span><em>p(Sunny) = (#Yes+#No)/Total entries = (2+3)/14, p(Overcast) = (#Yes+#No)/Total entries = (4+0)/14,</em> and <em>p(Rainy) = (#Yes+#No)/Total entries = (2+3)/14</em>. The entropy values <em>S(v)</em> are calculated using the corresponding probabilities, that is, <em>#Yes</em> or <em>#No</em> over the total <em>#Yes+#No</em></span>.</p>
<p><span>The frequency table for the combination Temperature-Train<span class="packt_screen"> </span>outside is as follows:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td rowspan="5" colspan="1">
<p><strong>Temperature</strong></p>
</td>
<td/>
<td rowspan="1" colspan="2">
<p><strong>Train Outside</strong></p>
</td>
</tr>
<tr>
<td/>
<td>
<p><strong>Yes</strong></p>
</td>
<td>
<p><strong>No</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>Hot</strong></p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>2</p>
</td>
</tr>
<tr>
<td>
<p><strong>Mild</strong></p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>2</p>
</td>
</tr>
<tr>
<td>
<p><strong>Cool</strong></p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Using these values and an analogous calculation, the entropy is shown in detail here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/eff7388b-ac6d-4b8c-95fa-5d057e97fd1d.png" style="width:34.08em;height:3.08em;"/></p>
<p><em>p(Hot).S(Hot)+p(Mild).S(Mild)+p(Cool)*S(Cool)=</em></p>
<p><em>4/14*(-2/4*log<sub>2</sub>(2/4)-2/4*<span>log</span><sub>2</sub><span>(2/4)) +</span></em></p>
<p><em><span>6/14*(-4/6*log<sub>2</sub>(4/6)-2/6*log<sub>2</sub>(2/6))+</span></em></p>
<p><em><span>4/14*(-3/4*log<sub>2</sub>(3/4)-1/4*log<sub>2</sub>(1/4)) =</span></em></p>
<p><em>0,911</em></p>
<p><span>The frequency table for the combination Humidity-Train outside<span class="packt_screen"> </span>is as follows:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr style="height: 75.7383px">
<td rowspan="4" colspan="1">
<p><strong>Humidity</strong></p>
</td>
<td rowspan="2" colspan="1"/>
<td rowspan="1" colspan="2">
<p><strong>Train Outside</strong></p>
</td>
</tr>
<tr style="height: 35px">
<td>
<p><strong>Yes</strong></p>
</td>
<td>
<p><strong>No</strong></p>
</td>
</tr>
<tr style="height: 40px">
<td>
<p><strong>High</strong></p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>4</p>
</td>
</tr>
<tr style="height: 40px">
<td>
<p><strong>Normal</strong></p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>Using these values, we get the entropy as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/91f2ee7e-5fee-497d-8b8a-3a42b9d21d98.png" style="width:26.50em;height:2.58em;"/></p>
<p><em>p(High).S(High)+p(Normal).S(Normal)=</em></p>
<p><em>7/14*(-3/7*log<sub>2</sub>(3/7)-4/7*<span>log</span><sub>2</sub><span>(4/7)) +</span></em></p>
<p><em><span>7/14*(-6/7*log<sub>2</sub>(6/7)-1/7*log<sub>2</sub>(1/7))</span><span>=</span></em></p>
<p><em>0,788</em></p>
<p><span>The frequency table for the combination Windy-Train outside<span class="packt_screen"><span class="packt_screen"> </span></span>is as follows:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td rowspan="4" colspan="1">
<p><strong>Windy</strong></p>
</td>
<td/>
<td rowspan="1" colspan="2">
<p><strong>Train Outside</strong></p>
</td>
</tr>
<tr>
<td/>
<td>
<p><strong>Yes</strong></p>
</td>
<td>
<p><strong>No</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>TRUE</strong></p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>2</p>
</td>
</tr>
<tr>
<td>
<p><strong>FALSE</strong></p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>3</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Using these values, we get the entropy as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ae116dfd-9968-47ba-ad17-fbd5ec08073e.png" style="width:37.50em;height:3.83em;"/></p>
<p><em>p(True).S(True)+p(False).S(False)=</em></p>
<p><em>8/14*(-6/8*log<sub>2</sub>(6/8)-2/8*<span>log</span><sub>2</sub><span>(2/8)) +</span></em></p>
<p><em><span>6/14*(-3/6*log<sub>2</sub>(3/6)-3/6*log<sub>2</sub>(3/6))</span></em></p>
<p><em><span>=</span>0,892</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparing the entropy differences (information gain)</h1>
                </header>
            
            <article>
                
<p>To know which variable to choose for the first split, we calculate the information gain <em>G</em> when going from the original data to the corresponding subset as the difference between the entropy values:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/35fbfbaa-4a17-4a9b-9029-0e8732cf6118.png" style="width:14.00em;height:1.25em;"/></p>
<p>Here, <em>S(f<sub>1</sub>)</em> is the entropy of the target variable and <em>S(f<sub>1</sub>,f<span>2</span>)</em> is the entropy of each feature with respect to the target variable. The entropy values were calculated in the previous subsections, so we use them here:</p>
<ul>
<li>If we choose <em>Outlook</em> as the first variable to split the tree, the information gain is as follows:</li>
</ul>
<p style="padding-left: 90px"><em><span>G(Train outside,Outlook) = S(Train outside) - S(Train outside,Outlook)<br/></span></em><span><em>                                                 = 0.94-0.693=0.247</em></span></p>
<ul>
<li><span>If we choose <em>Temperature</em>, the information gain is as follows:</span></li>
</ul>
<p style="padding-left: 90px"><em><span>G(Train outside,Temperature) = S(Train outside) - S(Train outside,Temperature)<br/></span></em><em><span>                                                           = 0.94-0.911=0.029</span></em></p>
<ul>
<li><span>If we choose <em>Humidity</em>, the information gain is as follows:</span></li>
</ul>
<p style="padding-left: 90px"><em><span>G(Train outside,Humidity) = S(Train outside) - S(Train outside,Humidity</span><span>)<br/></span></em><em><span>                                                     = 0.94-0.788=0.152</span></em></p>
<ul>
<li>Finally, choosing <em>Windy</em> gives the following information gain:</li>
</ul>
<p style="padding-left: 90px"><em><span>G(Train outside,Windy) = S(Train outside) - S(Train outside,Windy</span><span>)<br/></span><span>                                                  = 0.94-0.892=0.048</span></em></p>
<div class="packt_tip">All these calculations are easily performed in a worksheet using Excel formulas.</div>
<p>The variable to choose for the first splitting of the tree is the one showing the largest information gain, that is, <em>Outlook</em>. If we do this, we will notice that one of the resulting subsets after the splitting has zero entropy, so we don't need to split it further.</p>
<p>To continue building the tree following a similar procedure, the steps to take are as follows:</p>
<ol>
<li>Calculate <em>S(Sunny)</em>, <em>S(Sunny,Temperature)</em>, <em>S(Sunny,Humidity),</em> and <em>S(Sunny,Windy).</em></li>
<li>Calculate <em>G</em><span><em>(Sunny,Temperature)</em>, <em>G(Sunny,Humidity),</em> and <em>G(Sunny,Windy).</em></span></li>
<li>The larger value will tell us what feature to use to split <em>Sunny.</em></li>
<li>Calculate other gains, using <span><em>S(Rainy)</em>, <em>S(Rainy,Temperature)</em>, <em>S(Rainy,Humidity),</em> and <em>S(Rainy,Windy).</em></span></li>
<li>The larger value will tell us what feature to use to split <em>Rainy.</em></li>
<li>Continue iterating until there are no features left to use.</li>
</ol>
<p>As we will see later in this book, trees are never built by hand. It is important to understand how they work and which calculations are involved. Using Excel, it is easy to follow the full process and each step. Following the same principle, we will work through an unsupervised learning example in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding unsupervised learning with clustering</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">Clustering is a statistical method that attempts to group the points in a dataset according to a distance measure, usually the Euclidean distance, which <span>calculates the root of the squared differences between coordinates of a pair of points</span>. To put this simply, those points that are classified within the same cluster are closer (in terms of the distance defined) to each other than they are to the points belonging to other clusters. At the same time, the larger the distance between two clusters, the better we can distinguish them. This is similar to saying that we try to build groups in which members are more alike and are more different to members of other groups.</p>
<p class="CDPAlignLeft CDPAlign">It is clear that the most important part of a clustering algorithm is to define and calculate the distance between two given points and to iteratively assign the points to the defined clusters, until there is no change in the cluster composition. </p>
<p class="CDPAlignLeft CDPAlign">There are a few points to consider before trying a clustering analysis. Not every type of data is adequate for clustering. For example, we cannot use binary data since it is not possible to define distances. The values are either <kbd>1</kbd> or <kbd>0</kbd>, and there is no value in-between. This excludes the type of data generated by one-hot encoding. Only data that shows some ordering or scale is useful for clustering. Even if the data values are real (such as, for example, a client's expenditure amounts or annual income), it is better to group them in a scale of ranges.</p>
<p class="CDPAlignLeft CDPAlign">Some examples of clustering use cases are as follows:</p>
<ul>
<li class="CDPAlignLeft CDPAlign">Automatic grouping of IT alerts to assign priorities and solve them accordingly</li>
<li class="CDPAlignLeft CDPAlign">Analysis of customer communication through different channels (segmentation in time periods) </li>
<li class="CDPAlignLeft CDPAlign">Criminal profiling</li>
<li class="CDPAlignLeft CDPAlign">Urban mobility analysis</li>
<li class="CDPAlignLeft CDPAlign">Fraud detection (looking for outliers)</li>
<li class="CDPAlignLeft CDPAlign">Analysis of athletes' performances</li>
<li class="CDPAlignLeft CDPAlign">Crime analysis by geography</li>
<li class="CDPAlignLeft CDPAlign">Delivery logistics</li>
<li class="CDPAlignLeft CDPAlign">Classification of documents</li>
</ul>
<p>Now, let's go through some examples that explains the concept of clustering algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Grouping customers by monthly purchase amount</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">We will now follow the full calculation and analysis necessary to generate clusters from customer data. This is a simplified version of what would be a typical clustering algorithm, showing all the steps but reducing the number of iterations to make it understandable. Clustering is usually done automatically, but it is important to understand the logic behind the calculation.</p>
<p class="CDPAlignLeft CDPAlign">The dataset to be used contains the total monthly amount spent by 20 different customers in an online store, corresponding to <kbd>May</kbd>, <kbd>June</kbd>, and <kbd>July</kbd> in a given year. Once typed in an Excel sheet, the data looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c1b1a2e3-cefe-4bb3-8fbd-bdc9e5a4e9e9.png" style="width:18.00em;height:26.67em;"/></p>
<p>For each month, we can calculate the main parameters that describe the data: minimum, maximum, median, and average:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr style="height: 14.5pt">
<td/>
<td>
<p><strong>May</strong></p>
</td>
<td>
<p><strong>June</strong></p>
</td>
<td>
<p><strong>July</strong></p>
</td>
</tr>
<tr style="height: 14.5pt">
<td>
<p><strong>Minimum</strong></p>
</td>
<td>
<p>316.89</p>
</td>
<td>
<p>500.66</p>
</td>
<td>
<p>185.63</p>
</td>
</tr>
<tr style="height: 14.5pt">
<td>
<p><strong>Maximum</strong></p>
</td>
<td>
<p>11889.66</p>
</td>
<td>
<p>12214.41</p>
</td>
<td>
<p>11982.64</p>
</td>
</tr>
<tr style="height: 14.5pt">
<td>
<p><strong>Median</strong></p>
</td>
<td>
<p>8388.63</p>
</td>
<td>
<p>8156.16</p>
</td>
<td>
<p>7708.27</p>
</td>
</tr>
<tr style="height: 14.5pt">
<td>
<p><strong>Average</strong></p>
</td>
<td>
<p>6182.20</p>
</td>
<td>
<p>6229.24</p>
</td>
<td>
<p>6227.81</p>
</td>
</tr>
</tbody>
</table>
<p class="CDPAlignLeft CDPAlign">We simply use the Excel built-in functions <em>MIN()</em>, <em>MAX()</em>, <em>MEDIAN()</em>, and <em>AVERAGE()</em>, including the full range of each column.</p>
<p class="CDPAlignLeft CDPAlign">In cluster analysis, it is useful to <em>normalize</em> the dataset, that is, to convert all values so that they fall in to the interval [0,1]. This helps us deal with the <strong>outlier</strong> data points, whose value is very different from the majority of points, which can affect the cluster definition. After normalization, those points are not so far away from the rest and can be easily grouped. Clearly, if the goal of the clustering analysis is to find those outliers, it is a better idea to leave the dataset as it is and highlight the difference between the outliers and the rest of the set.</p>
<p class="CDPAlignLeft CDPAlign">The easiest way to normalize the data is to divide each value by the maximum in the corresponding column. To do this, follow these steps:</p>
<ol start="1">
<li class="CDPAlignLeft CDPAlign">In cell <em>G2,</em> type <kbd>=B2/$B$24</kbd>. <span>We are assuming that <em>B2</em> is the first value in the </span><kbd>May</kbd><span> column an</span><span>d </span>that the<span> maximum value is in <em>B24</em>. </span></li>
<li class="CDPAlignLeft CDPAlign">Copy this formula into the whole column. Recall that adding <em>$</em> to the cell ID in Excel fixes that value when copying the contents into another cell. The normalized table is then as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/eea879ef-2c9f-4b03-a40d-fc677dd1804d.png" style="width:17.75em;height:29.00em;"/></p>
<p class="CDPAlignLeft CDPAlign">Let's take a moment to visualize the data and understand it a little more. If we take the columns in pairs, then it is possible to generate scatter plots and try to find clusters visually by following these steps: </p>
<ol>
<li class="CDPAlignLeft CDPAlign">Select <kbd>May</kbd> and <kbd>June</kbd> data.</li>
<li class="CDPAlignLeft CDPAlign">Click <span class="packt_screen">Insert</span> | <span class="packt_screen">Scatter.</span></li>
</ol>
<p style="padding-left: 60px">The resulting diagram is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/11c7e4a0-ca9a-4662-9d35-a7249965949a.png" style="width:22.83em;height:13.58em;"/></p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">Three clusters can be identified, and are circled in the preceding screenshot. They correspond to groups of customers who spend similar amounts of money monthly.</p>
<ol start="3">
<li class="CDPAlignLeft CDPAlign">Doing the same with <kbd>May</kbd> and <kbd>July</kbd>, we get the following diagram:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c7005221-b01b-49e0-864e-ca94affdd0da.png" style="width:25.25em;height:14.92em;"/></p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">In this case, we could either say that there are two big clusters or that one of them can be further split in two. The separation is not so clear and the choice will depend on other variables (remember that the best model is always the one that best suits the business' needs).</p>
<ol start="4">
<li class="CDPAlignLeft CDPAlign">Finally, we plot <kbd>June</kbd> and <kbd>July</kbd>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b4ef4a9d-7137-44d7-a37e-37f4a22b7078.png" style="width:25.75em;height:15.33em;"/></p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">The division of clusters seems even more clear here, and we can circle three sets of points.</p>
<p class="CDPAlignLeft CDPAlign">What if we want to consider all three months at the same time? There is an iterative process to accomplish this, which is the base of the clustering algorithm known as <strong>K-means</strong>. Let's follow the steps of this algorithm in detail:</p>
<ol>
<li class="CDPAlignLeft CDPAlign">Decide how many clusters you want to split the data into. This is not an easy decision in general. It will strongly depend on the dataset and, in some cases, will be a matter of testing different values until you get a number of clusters that gives useful insights on the data.</li>
<li class="CDPAlignLeft CDPAlign">Taking into account the previous visual analysis, we decide to choose three as the number of clusters.</li>
</ol>
<ol start="3">
<li class="CDPAlignLeft CDPAlign">Take any three points as the center of the clusters. The choice of the starting points is not relevant, as we will repeat the whole process until there is no change in the resulting cluster members. We then choose the first three points in the list, as shown in the following table:</li>
</ol>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td/>
<td>
<p><strong>May</strong></p>
</td>
<td>
<p><strong>June</strong></p>
</td>
<td>
<p><strong>July</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>Random1</strong></p>
</td>
<td>
<p>0.055568104</p>
</td>
<td>
<p>0.043735522</p>
</td>
<td>
<p>0.15581034</p>
</td>
</tr>
<tr>
<td>
<p><strong>Random2</strong></p>
</td>
<td>
<p>0.07079235</p>
</td>
<td>
<p>0.067065974</p>
</td>
<td>
<p>0.079319396</p>
</td>
</tr>
<tr>
<td>
<p><strong>Random3</strong></p>
</td>
<td>
<p>0.026652635</p>
</td>
<td>
<p>0.040988882</p>
</td>
<td>
<p>0.171590079</p>
</td>
</tr>
</tbody>
</table>
<ol start="4">
<li class="CDPAlignLeft CDPAlign"><span>Find the points that are closer to them, </span>computing the distance from all other points to these cluster centers. The Euclidean distance between two points, <em>P<sub>1</sub> =<span>(x</span><sub>1</sub><span>,y</span><sub><span>1</span></sub></em><span><em>,z<sub>1</sub>)</em> and </span><sub> </sub><em>P<sub>2</sub> = <span>(x</span><sub>2</sub><span>,y</span><sub><span>2</span></sub></em><span><em>,z<sub>2</sub>),</em> </span>is defined as follows:</li>
</ol>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/93758e1f-101e-412e-bf3e-023268aa23e2.png" style="width:21.92em;height:2.25em;"/></p>
<div class="packt_tip">Use Excel's built-in <em>SUMXMY2([array1];[array2})</em> function to calculate <em>(DE)<sup>2</sup></em> for each point with respect to the cluster centers.</div>
<ol start="5">
<li class="mce-root">For each data point, you will get three distance values. Pick the smallest one to decide which cluster the point belongs to. For example, for customer ID = 4, we get the following information: </li>
</ol>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>D1</strong></p>
</td>
<td>
<p><strong>D2</strong></p>
</td>
<td>
<p><strong>D3</strong></p>
</td>
<td>
<p><strong>Cluster</strong></p>
</td>
</tr>
<tr>
<td>
<p>0.019689391</p>
</td>
<td>
<p>0.004847815</p>
</td>
<td>
<p>0.025218271</p>
</td>
<td>
<p>2</p>
</td>
</tr>
</tbody>
</table>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">Here, <span><em>D1</em>, <em>D1,</em> and <em>D3</em> are the distances from the point to the respective cluster centers. T</span>he smallest distance tells us that this point belongs to cluster two. As an example, <em>D1</em> for customer ID = 4 is calculated as <em>=SUMXMY2(B5:D5;$B$23:$D$23)</em>, assuming that <span class="packt_screen">Random1</span> <kbd>May</kbd> and <span class="packt_screen"><span class="packt_screen">Random1</span></span> <kbd>June</kbd> are in cells <span><em>$B$23</em> and <em>$D$23</em></span>, <span>respectively.</span></p>
<ol start="6">
<li class="CDPAlignLeft CDPAlign">The complete resulting data table is as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9934f1dc-2e42-47c0-aadb-b636e379059c.png" style="width:33.08em;height:26.00em;"/></p>
<div class="packt_tip CDPAlignLeft CDPAlign">The last column can be created by typing the following formula into the first row and then copying it down: <em>=IF(E2=MIN(E2:G2);1;IF(F2=MIN(E2:G2);2;3))</em>.</div>
<p class="CDPAlignLeft CDPAlign">According to the table, our first result is really unbalanced. Most of the points fall in cluster one, a few in cluster two, and only one in cluster three. We need to continue the calculations and see how the result evolves. Follow these steps:</p>
<ol>
<li class="CDPAlignLeft CDPAlign">Instead of choosing random points, we will now use the mean values of the clusters we obtained.</li>
</ol>
<ol start="2">
<li class="CDPAlignLeft CDPAlign">Order the table by cluster number. The resulting table is a little different now, as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e0519ae9-bd12-4c5b-947f-4d17857df7ed.png" style="width:34.42em;height:26.92em;"/></p>
<ol start="3">
<li><span> Use the <em>MEAN()</em> function to calculate the average value per cluster for </span>each month.<span> You should get the same results that are shown in the following table:</span></li>
</ol>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td/>
<td>
<p><strong>May</strong></p>
</td>
<td>
<p><strong>June</strong></p>
</td>
<td>
<p><strong>July</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>Mean1</strong></p>
</td>
<td>
<p>0.618762809</p>
</td>
<td>
<p>0.605805489</p>
</td>
<td>
<p>0.618056642</p>
</td>
</tr>
<tr>
<td>
<p><strong>Mean2</strong></p>
</td>
<td>
<p>0.157477363</p>
</td>
<td>
<p>0.155314048</p>
</td>
<td>
<p>0.111411008</p>
</td>
</tr>
<tr>
<td>
<p><strong>Mean3</strong></p>
</td>
<td>
<p>0.026652635</p>
</td>
<td>
<p>0.040988882</p>
</td>
<td>
<p>0.171590079</p>
</td>
</tr>
</tbody>
</table>
<div class="CDPAlignLeft CDPAlign packt_tip">As an example, <strong>Mean1</strong> corresponding to <kbd>May</kbd> is calculated as <em>AVERAGE(B2:B17)</em>.</div>
<ol start="4">
<li class="CDPAlignLeft CDPAlign">Using the same formulas as before and calculating the distances from all other points to the mean values, you get a table similar to this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4f669d09-d40e-4b33-8f05-dab4a8ce8214.png" style="width:34.58em;height:27.00em;"/></p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">After the second iteration, a few more points, <span>which, when moved away from cluster one, </span>now belong to cluster two and three.</p>
<ol start="5">
<li class="CDPAlignLeft CDPAlign">Repeat the calculation one more time. The new mean values, according to the preceding table, are as follows:</li>
</ol>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td/>
<td>
<p><strong>May</strong></p>
</td>
<td>
<p><strong>June</strong></p>
</td>
<td>
<p><strong>July</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>Mean1</strong></p>
</td>
<td>
<p>0.843481911</p>
</td>
<td>
<p>0.832289469</p>
</td>
<td>
<p>0.810799822</p>
</td>
</tr>
<tr>
<td>
<p><strong>Mean2</strong></p>
</td>
<td>
<p>0.292624962</p>
</td>
<td>
<p>0.280240044</p>
</td>
<td>
<p>0.310753303</p>
</td>
</tr>
<tr>
<td>
<p><strong>Mean3</strong></p>
</td>
<td>
<p>0.052180197</p>
</td>
<td>
<p>0.048870976</p>
</td>
<td>
<p>0.105552835</p>
</td>
</tr>
</tbody>
</table>
<ol start="6">
<li class="CDPAlignLeft CDPAlign">The table containing the distances and cluster numbers can be given as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8722ed23-eb6b-4609-b9d8-b8fe2d37ff99.png" style="width:35.17em;height:27.08em;"/></p>
<p class="CDPAlignLeft CDPAlign">After the third iteration, only one point changed cluster, from two to three; so, we are getting close to the final result. <span>You should be able to perform one more iteration, following the same steps,</span> proving that it does not change the clustering labels and meaning that the calculations converged to a stable number of clusters.</p>
<p class="CDPAlignLeft CDPAlign">Real-life datasets might not converge so fast. What we have shown is a simplified example, good enough to show every step of the iteration, understand them, and get to a reasonable result. Clustering is not usually calculated manually, but performed by pre-built algorithms.</p>
<p>In the following chapter, you will learn how to import data from different sources to Excel, so you don't need to type in the values manually. This will give you a starting point to analyze real data, usually containing many more variables and values than the examples shown in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have described real life examples of supervised and unsupervised machine learning models that have been applied to solving problems. We covered multiple regression, decision trees, and clustering. We have also shown how to choose and transform the input variables or features to be ingested by the models.</p>
<p>This chapter only shows the basic principles of each algorithm. In real data analysis and prediction using machine learning, models are already programmed and can be used as black boxes. It is, therefore, extremely important to understand the basics of each model and know whether we are using it correctly.</p>
<p>In the following chapters, we will focus on how to extract the data from different sources, transform it according to our needs, and use previously built models for analysis.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Why is it important to encode categorical features?</li>
<li>What are the different ways to stop a decision tree calculation?</li>
<li class="CDPAlignLeft CDPAlign"><kbd>Temperature_hot</kbd> has an entropy value of one in the example. Why?</li>
<li>Following the diagram of the decision tree at the beginning of the <em>Understanding supervised learning with decision trees</em> section, what would be the path to decide whether or not to train outside? Consider using <kbd>IF</kbd> statements.</li>
<li>Would the cluster distribution change if we choose different starting points? You can read about this in the recommended articles.</li>
<li class="CDPAlignLeft CDPAlign">Is the clustering that's obtained with iterative analysis the same as the one that's determined visually? Why?</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li class="CDPAlignLeft CDPAlign"><em>How to Interpret Regression Analysis Results: P-values and Coefficients</em>: <span><a href="http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-values-and-coefficients"><span>http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-values-and-coefficients</span></a></span></li>
<li class="CDPAlignLeft CDPAlign"><em>Teaching Decision Tree Classification Using Microsoft Excel INFORMS Transactions on Education</em> 11(3), pp. 123–131, by Kaan Ataman, George Kulick, Thaddeus Sim: <a href="https://pubsonline.informs.org/doi/10.1287/ited.1100.0060">https://pubsonline.informs.org/doi/10.1287/ited.1100.0060</a></li>
<li class="CDPAlignLeft CDPAlign"><em>A Review of K-mean Algorithm</em>, International Journal of Engineering Trends and Technology (IJETT) – Volume 4 Issue 7- July 2013: <a href="http://www.ijettjournal.org/volume-4/issue-7/IJETT-V4I7P139.pdf">http://www.ijettjournal.org/volume-4/issue-7/IJETT-V4I7P139.pdf</a></li>
</ul>


            </article>

            
        </section>
    </body></html>