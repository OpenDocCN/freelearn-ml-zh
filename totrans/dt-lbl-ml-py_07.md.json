["```py\nos.environ['AZURE_OPENAI_KEY'] = 'your_api_key'\nos.environ['AZURE_OPENAI_ENDPOINT\") ='your_azure_openai_endpoint'\n```", "```py\nimport nltk\nfrom nltk.corpus import gutenberg\nimport string\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n```", "```py\n# Download the Gutenberg corpus\nnltk.download('gutenberg')\n```", "```py\n# Load the data\ndata = []\nfor file_id in gutenberg.fileids():\n    document = ' '.join(gutenberg.words(file_id))\n    data.append(document)\ndf = pd.DataFrame(data, columns=['text'])\n# View the first few rows of the data\nprint(df.head())\n```", "```py\n# Check the size of the dataset\nprint(\"Dataset size:\", df.shape)\n```", "```py\n# Check the length of each document\ndf['text_length'] = df['text'].apply(len)\n Let us plot the histogram plot of the 'text_length' column using seaborn library sns.\n# Visualize the distribution of document lengths\nplt.figure(figsize=(8, 6))\nsns.distplot(df['text_length'], bins=50, kde=False, color='blue')\nplt.title('Distribution of Text Lengths')\nplt.xlabel('Text Length')\nplt.ylabel('Count')\nplt.show()\n```", "```py\n# Remove punctuation and stop words\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n```", "```py\ndef remove_stopwords(text):\n    stopwords_list = nltk.corpus.stopwords.words('english')\n    return \" \".join([word for word in text.split() if \\\n        word.lower() not in stopwords_list])\ndf['text_clean'] = df['text'].apply(remove_punctuation)\ndf['text_clean'] = df['text_clean'].apply(remove_stopwords)\n```", "```py\n# Count the frequency of each word\nword_freq = pd.Series(np.concatenate([x.split() for x in \\\n    df['text_clean']])).value_counts()\n```", "```py\n# Visualize the most frequent words\nplt.figure(figsize=(12, 8))\nword_freq[:20].plot(kind='bar', color='blue')\nplt.title('Most Frequent Words')\nplt.xlabel('Word')\nplt.ylabel('Frequency')\nplt.show()\n```", "```py\nimport os\nopenai.api.key=os.getenv(\"AZURE_OPENAI_KEY\")\nOpenai.api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\nOpenai.api_type='azure'\nOpenai.api_version='2023-5-15' # this might change in the future\n#this will correspond to the custom name you choose for your deployment when you deployed a model.\nmodel_deployment_name = 'your_azure_openai_model_name'\n# Set the input text\ntext = \"create a summary of below text and provide main idea.\\n\\n Dachepalli is popular town in palnadu district in Andhra pradesh, India.I love dachepalli because i born and brought up at Dachepalli. I studied at Dachepalli zph school and got school first and my name was written on school toppers board at high school.My father worked in the same high school as hindi pandit for 20 years.The famous palnadu battle has took place near Naguleru river of Karempudi which flows across Dachepalli.It has lime mines and number of cement factories around Dachepalli.The Nadikudi railway junction connect Dachepalli to Hyderbad and Guntur. being born in Dachepalli and studied at Dachepalli high school, I love Dachepalli.\"\nresponse = openai.Completion.create(\n    engine=model_deployment_name,\n    prompt=text,\n    temperature=0,\n    max_tokens=118,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0,\n    stop=None)\n```", "```py\n# Print the generated summary\nprint(\"Generated summary:\", summary.choices[0].text.strip())\n```", "```py\nGenerated summary: Main Idea: The author loves Dachepalli because he was born and brought up there and studied at Dachepalli high school. The town is located in Palnadu district in Andhra Pradesh, India and is known for its lime mines and cement factories. The Nadikudi railway junction connects Dachepalli to Hyderabad and Guntur. The famous Palnadu battle took place near Naguleru river of Karempudi which flows across Dachepalli. The author's father worked in the same high school as a Hindi pandit for 20 years.\n```", "```py\nnews_headline=\"Label the following news headline into 1 of the following categories: Business, Tech, Politics, Sport, Entertainment\\n\\n Headline 1: Trump is ready to contest in nov 2024 elections\\nCategory:\",\nresponse = openai.Completion.create(\n    engine=model_deployment_name,\n    prompt= news_headline,\n    temperature=0,\n    max_tokens=118,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0,\n    stop=None)\nindex_of_newline=response.choice[0].text.find('\\n')\nprint('category:',response.choices[0].text[:index_of_newline])\n```", "```py\ncategory: Politics\n```", "```py\nsystem_message = f\"\"\"\nWelcome to Customer Order Support!\nYou will receive customer queries related to their orders, each delimited by {delimiter} characters.\nYour task is to classify each query into a primary and secondary category.\nProvide your response in JSON format with the keys: \"primary\" and \"secondary.\"\nPrimary Categories:\n1\\. Order Status\n2\\. Product Inquiries\n3\\. Shipping and Delivery\n4\\. Payment Assistance\nOrder Status Secondary Categories:\n- Tracking Information\n- Order Confirmation\n- Order Modification or Cancellation\n- Refund Status\nProduct Inquiries Secondary Categories:\n- Product Availability\n- Size and Color Options\n- Product Specifications\n- Return and Exchange Policies\nShipping and Delivery Secondary Categories:\n- Delivery Timeframe\n- Shipping Methods\n- Address Changes\n- Lost or Delayed Shipments\nPayment Assistance Secondary Categories:\n- Payment Confirmation\n- Refund Process\n- Payment Errors\n- Billing Inquiries\nPlease review each query and provide the appropriate primary and secondary category in your response.\nThank you for assisting our customers with their orders!\"\"\"\nuser_message=f\"\"\"\\\n I want to cancel my order \"\"\"\nresponse = openai.ChatCompletion.create(\n    engine=deployment_name, # engine = \"deployment_name\".\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": f\"{delimiter}{user_message}\n        {delimiter}\"},],\n    temperature=0,\n    max_tokens=60,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0,\n    stop=None\n)\nprint(response)\nprint(response['choices'][0]['message']['content'])\n```", "```py\n{ \"id\": \"chatcmpl-8eEc86GxAO4BePuRepvve9XhTQZfa\", \"object\": \"chat.completion\", \"created\": 1704599988, \"model\": \"gpt-35-turbo\", \"choices\": [ { \"finish_reason\": \"stop\", \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"{\\n \\\"primary\\\": \\\"Order Status\\\",\\n \\\"secondary\\\": \\\"Order Modification or Cancellation\\\"\\n}\" } } ], \"usage\": { \"prompt_tokens\": 232, \"completion_tokens\": 21, \"total_tokens\": 253 } } { \"primary\": \"Order Status\", \"secondary\": \"Order Modification or Cancellation\" }\n```", "```py\nresponse = openai.Completion.create(\n    engine=\"gpt3.5 deployment name\",\n    prompt = \"Identify the individual's name, organization, geographical location, and contact number in the following text.\\n\\nHello. I'm Sarah Johnson, and I'm reaching out on behalf of XYZ Tech Solutions based in Austin, Texas. Our team believes that our innovative products could greatly benefit your business. Please feel free to contact me at (555) 123-4567 at your convenience, and we can discuss how our solutions align with your needs.\",\n    temperature=0.2,\n    max_tokens=150,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0,\n    stop=None)\nprint(response['choices'])\n```", "```py\n[<OpenAIObject at 0x215d2c40770> JSON: {\n    \"text\": \" Thank you for your time, and I look forward to hearing from you soon. \\n\\nName: Sarah Johnson\\nOrganization: XYZ Tech Solutions\\nGeographical location: Austin, Texas\\nContact number: (555) 123-4567\",\n    \"index\": 0,\n    \"finish_reason\": \"stop\",\n    \"logprobs\": null,\n    \"content_filter_results\": {\n    \"hate\": {\n        \"filtered\": false,\n        \"severity\": \"safe\"\n    },\n    \"self_harm\": {\n        \"filtered\": false,\n        \"severity\": \"safe\"\n    },\n    \"sexual\": {\n        \"filtered\": false,\n        \"severity\": \"safe\"\n    },\n    \"violence\": {\n        \"filtered\": false,\n        \"severity\": \"safe\"\n    }\n}\n}]\n```", "```py\nimport json\n# Parse JSON\njson_data = response['choices']\n# Extract information\n# Extracting information from the JSON object\nfor entry in json_data:\n    text = entry.get(\"text\", \"\")\n    # Extracting information using string manipulation or regular expressions\n    name = text.split(\"Name:\")[1].split(\"\\n\")[0].strip()\n    organization = text.split(\"Organization:\")[1].split(\"\\n\")[0].strip()\n    location = text.split(\"Geographical location:\")[1].split(\"\\n\")[0].strip()\n    contact_number = text.split(\"Contact number:\")[1].split(\"\\n\")[0].strip()\n    # Print the extracted information\n    print(\"Name:\", name)\n    print(\"Organization:\", organization)\n    print(\"Location:\", location)\n    print(\"Contact Number:\", contact_number)\n```", "```py\nName: Sarah Johnson Organization: XYZ Tech Solutions Location: Austin, Texas Contact Number: (555) 123-4567\n```", "```py\nresponse = openai.Completion.create(\n    engine=\"gpt3.5 deployment name\",\nprompt = \"Conduct aspect-based sentiment analysis on the following product reviews:\\n Provide an overall sentiment score between 0 and 5 for each review.\\n Assign a sentiment polarity score between 0 and 5 for each aspect mentioned. \\n Identify the top positive and negative aspects, if any. \\n Review 1: \\n I recently purchased this smartphone, and it has exceeded my expectations! The camera quality is superb, capturing vivid and detailed photos. The battery life is impressive, easily lasting a full day with regular use. The sleek design adds a premium feel to the device. However, the speaker quality could be improved. Overall sentiment score: 4.8 \\nAspects with sentiment polarity score: \\n - Camera: 5 \\n - Battery Life: 5 \\n - Design: 5 \\n - Speaker: 3 \\n \\n Top positive aspect: Camera \\n Top negative aspect: Speaker \\n \\n Review 2: \\n This laptop offers powerful performance and a sleek design. The keyboard is comfortable for extended typing sessions, and the display is vibrant with accurate colors. However, the trackpad responsiveness can be inconsistent at times.\",\n    temperature=0,\n    max_tokens=100,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0,\n    stop=None)\nprint(response.choices[0].text.strip())\n```", "```py\nOverall sentiment score: 4.5\nAspects with sentiment polarity score:\n - Performance: 5\n - Design: 5\n - Keyboard: 5\n - Display: 5\n - Trackpad: 3\n Top positive aspects: Performance, Design, Keyboard, Display\n Top negative aspect: Trackpad\n```", "```py\n!pip install snorkel\n```", "```py\nimport pandas as pd\n# Define the constants\nABSTAIN = -1\nPOS = 0\nNEG = 1\n# Create a DataFrame with more data\ndf = pd.DataFrame({\n    'id': [1, 2, 3, 4, 5, 6, 7, 8],\n    'review': [\n        \"This movie was absolutely wonderful!\",\n        \"The film was terrible and boring.\",\n        \"I have mixed feelings about the movie.\",\n        \"I have no opinion about the movie.\",\n        \"The movie was fantastic and exciting!\",\n        \"I didn't like the movie, it was too slow.\",\n        \"The movie was okay, not great but not bad either.\",\n        \"The movie was confusing and dull.\"\n    ]\n})\n# Split the DataFrame into a training set and a test set\ndf_train = df.iloc[:6]  # First 6 records for training\ndf_test = df.iloc[6:]  # Remaining records for testing\n# Define the true labels for the test set\nY_test = [ABSTAIN, NEG]  # Replace this with the actual labels\n# Convert Y_test to a NumPy array\nY_test = np.array(Y_test)\n```", "```py\n# Define rule-based labeling functions using regular expressions\n@labeling_function()\ndef lf_positive_review(x):\n    return POS if 'wonderful' in x.review or 'fantastic' in x.review else ABSTAIN\n@labeling_function()\ndef lf_negative_review(x):\n    return NEG if 'terrible' in x.review or 'boring' in \\\n        x.review or 'slow' in x.review or 'dull' in \\\n        x.review else ABSTAIN\n@labeling_function()\ndef lf_neutral_review(x):\n    return ABSTAIN if 'mixed feelings' in x.review or \\\n        'no opinion' in x.review or 'okay' in x.review \\\n        else ABSTAIN\n```", "```py\n# Apply the labeling functions to the training set and the test set\nlfs = [lf_positive_review, lf_negative_review, lf_neutral_review]\napplier = PandasLFApplier(lfs=lfs)\nL_train = applier.apply(df=df_train)\nL_test = applier.apply(df=df_test)\nprint(L_train)\nprint(L_test)\nprint(L_test.shape)\nprint(Y_test.shape)\n```", "```py\nfrom snorkel.labeling.model import MajorityLabelVoter, LabelModel\nmajority_model = MajorityLabelVoter()\nmajority_model.predict(L=L_train)\nmajority_acc = majority_model.score(L=L_test, Y=Y_test, \\\n    tie_break_policy=\"random\")[\"accuracy\"]\nprint( majority_acc)\n```", "```py\n1.0\n```", "```py\npreds_train = majority_model.predict(L=L_train)\nprint(preds_train)\n```", "```py\n[ 0  1 -1 -1  0  1]\n```", "```py\nlabel_model = LabelModel(cardinality=2, verbose=True)\nlabel_model.fit(L_train=L_train, n_epochs=500, \\\n    log_freq=100, seed=123)\nlabel_model_acc = label_model.score(L=L_test, Y=Y_test, \\\n    tie_break_policy=\"random\")[\n    \"accuracy\"\n]\nprint(label_model_acc)\n```", "```py\n# Predict the labels for the training data\nY_train_pred = label_model.predict(L=L_train)\n# Print the predicted labels\nprint(Y_train_pred)\n```", "```py\n[ 0  1 -1 -1  0  1]\n```", "```py\n# Analyze the labeled data\nLFAnalysis(L=L_train, lfs=lfs).lf_summary()\n```", "```py\n# Create a DataFrame with the predicted labels\ndf_train_pred = df_train.copy()\ndf_train_pred['predicted_label'] = Y_train_pred\n# Display the DataFrame\nprint(df_train_pred)\n```", "```py\n    from nltk.corpus import stopwords\n    from nltk.stem import WordNetLemmatizer\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.model_selection import train_test_split\n    from sklearn.linear_model import LogisticRegression\n    import nltk\n    from nltk.corpus import movie_reviews\n    from nltk.sentiment import SentimentAnalyzer\n    from nltk.classify import NaiveBayesClassifier\n```", "```py\n    nltk.download('movie_reviews')\n    nltk.download('wordnet')\n    nltk.download('omw-1.4')\n    nltk.download('punkt')\n```", "```py\n    sentiment_analyzer = SentimentAnalyzer()\n    ids = movie_reviews.fileids()\n```", "```py\n    lemmatizer = WordNetLemmatizer()\n    stop_words = set(stopwords.words('english'))\n    def preprocess(document):\n        words = word_tokenize(document)\n        words = [lemmatizer.lemmatize(word) for word in \\\n            words if word not in stop_words]\n        return ' '.join(words)\n```", "```py\n    vectorizer = TfidfVectorizer(preprocessor=preprocess, ngram_range=(1, 2))\n    X = vectorizer.fit_transform( \\\n        [movie_reviews.raw(fileid) for fileid in ids])\n```", "```py\n    y = [movie_reviews.categories([f])[0] for f in ids]\n```", "```py\n    X_train, X_test, y_train, y_test = train_test_split( \\\n        X, y, test_size=0.2, random_state=42)\n```", "```py\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n```", "```py\n    accuracy = model.score(X_test, y_test)\n    print(f\"Accuracy: {accuracy:.2%}\")\n```", "```py\n    custom_sentences = [\n        \"I loved the movie and it was amazing. Best movie I have seen this year.\",\n        \"The movie was terrible. The plot was non-existent and the acting was subpar.\",\n        \"I have mixed feelings about the movie. Some parts were good, but some were not.\",\n    ]\n    for sentence in custom_sentences:\n        preprocessed_sentence = preprocess(sentence)\n        features = vectorizer.transform([preprocessed_sentence])\n        sentiment = model.predict(features)\n        print(f\"Sentence: {sentence}\\nSentiment: {sentiment[0]}\\n\")\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom nltk.corpus import movie_reviews\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nimport re\n# Download the necessary NLTK data\nnltk.download('movie_reviews')\nnltk.download('stopwords')\nnltk.download('wordnet')\n```", "```py\n# Get the reviews\nreviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n# Preprocess the text\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\nreviews = [' '.join(lemmatizer.lemmatize(word) for word in re.sub('[^a-zA-Z]', ' ', review).lower().split() if word not in stop_words) for review in reviews]\n```", "```py\n# Create a TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\n# Transform the reviews into TF-IDF features\nX_tfidf = vectorizer.fit_transform(reviews)\n```", "```py\n# Cluster the reviews using K-means\nkmeans = KMeans(n_clusters=3).fit(X_tfidf)\n```", "```py\n# Define the labels for the clusters\ncluster_labels = {0: \"positive\", 1: \"negative\", 2: \"neutral\"}\n# Test the classifier with custom sentences\ncustom_sentences = [\"I loved the movie and Best movie I have seen this year.\",\n\"The movie was terrible. The plot was non-existent and the acting was subpar.\",\n\"I have mixed feelings about the movie.it is partly good and partly not good.\"]\nfor sentence in custom_sentences:\n    # Preprocess the sentence\n    sentence = ' '.join(lemmatizer.lemmatize(word) for word in re.sub('[^a-zA-Z]', ' ', sentence).lower().split() if word not in stop_words)\n    # Transform the sentence into TF-IDF features\n    features = vectorizer.transform([sentence])\n    # Predict the cluster of the sentence\n    cluster = kmeans.predict(features)\n    # Get the label for the cluster\n    label = cluster_labels[cluster[0]]\n    print(f\"Sentence: {sentence}\\nLabel: {label}\\n\")\n```", "```py\nsentences = [\"I love this movie\", \"This movie is terrible\", \"The acting was amazing\", \"The plot was confusing\"]\nlabels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n```", "```py\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n```", "```py\nsentences = [\"I love this movie\", \"This movie is terrible\", \"The acting was amazing\", \"The plot was confusing\"]\nlabels = [1, 0, 1, 0]\n```", "```py\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\n```", "```py\nmax_sequence_length = max([len(seq) for seq in sequences])\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n```", "```py\nmodel = keras.Sequential([\n    keras.layers.Embedding(len(tokenizer.word_index) + 1, \\\n        16, input_length=max_sequence_length),\n    keras.layers.Flatten(),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n```", "```py\nmodel.compile(optimizer='adam', loss='binary_crossentropy', \\\n    metrics=['accuracy'])\n```", "```py\nmodel.fit(padded_sequences, np.array(labels), epochs=10)\n```", "```py\nnew_sentence = [\"This movie is good\"]\nnew_sequence = tokenizer.texts_to_sequences(new_sentence)\npadded_new_sequence = pad_sequences(new_sequence, \\\n    maxlen=max_sequence_length)\nraw_prediction = model.predict(padded_new_sequence)\nprint(\"raw_prediction:\",raw_prediction)\nprediction = (raw_prediction > 0.5).astype('int32')\nprint(\"prediction:\",prediction)\n```", "```py\nif prediction[0][0] == 1:\n     print(\"Positive\")\nelse:\n     print(\"Negative\")\n```"]