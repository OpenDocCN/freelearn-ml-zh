<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Clustering with Unsupervised Learning</h1>
                </header>
            
            <article>
                
<p> In this chapter, we will cover the following recipes:</p>
<ul>
<li>Clustering data using the k-means algorithm</li>
<li>Compressing an image using vector quantization</li>
<li>Grouping data using agglomerative clustering</li>
<li>Evaluating the performance of clustering algorithms</li>
<li>Estimating the number of clusters using the <strong>Density-Based Spatial Clustering of Applications with Noise</strong> (<strong>DBSCAN</strong>) algorithm</li>
<li>Finding patterns in stock market data</li>
<li>Building a customer segmentation model</li>
<li>Using autoencoders to reconstruct handwritten digit images</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To address the recipes in this chapter, you will need the following files (which are available on GitHub):</p>
<ul>
<li><kbd>kmeans.py</kbd></li>
<li><kbd>data_multivar.txt</kbd></li>
<li><kbd>vector_quantization.py</kbd></li>
<li><kbd>flower_image.jpg</kbd></li>
<li><kbd>agglomerative.py</kbd></li>
<li><kbd>performance.py</kbd></li>
<li><kbd>data_perf.txt</kbd></li>
<li><kbd>estimate_clusters.py</kbd></li>
</ul>
<ul>
<li><kbd>stock_market.py</kbd></li>
<li><kbd>symbol_map.json</kbd></li>
<li><kbd>stock_market_data.xlsx</kbd></li>
<li><kbd>customer_segmentation.py</kbd></li>
<li><kbd>wholesale.csv</kbd></li>
<li><kbd>AutoencMnist.py</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p><strong>Unsupervised learning</strong> is a paradigm in machine learning where we build models without relying on labeled training data. Up to this point, we have dealt with data that was labeled in some way. This means that learning algorithms can look at this data and learn to categorize it them based on labels. In the world of unsupervised learning, we don't have this opportunity! These algorithms are used when we want to find subgroups within datasets using a similarity metric.</p>
<p>In unsupervised learning, information from the database is automatically extracted. All this takes place without prior knowledge of the content to be analyzed. In unsupervised learning, there is no information on the classes that the examples belong to, or on the output corresponding to a given input. We want a model that can discover interesting properties, such as groups with similar characteristics, which happens in <strong>clustering</strong>. An example of the application of these algorithms is a search engine. These applications are able to create a list of links related to our search, starting from one or more keywords. </p>
<p>These algorithms work by comparing data and looking for similarities or differences. <span>The validity of these algorithms depends on the usefulness of the information they can extract from the database. </span>Available data only concerns the set of features that describe each example.</p>
<p>One of the most common methods is clustering. You will have heard this term being used quite frequently; we mainly use it for data analysis when we want to find clusters in our data. These clusters are usually found by using a certain kind of similarity measure, such as the Euclidean distance. Unsupervised learning is used extensively in many fields, such as data mining, medical imaging, stock market analysis, computer vision, and market segmentation.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering data using the k-means algorithm</h1>
                </header>
            
            <article>
                
<p>The k-means algorithm is one of the most popular clustering algorithms. This algorithm is used to divide the input data into <em>k</em> subgroups using various attributes of the data. Grouping is achieved using an optimization technique where we try to minimize the sum of squares of distances between the datapoints and the corresponding centroid of the cluster. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will use the k-means algorithm to group the data into four clusters identified by the relative centroid. We will also be able to trace the boundaries to identify the areas of relevance of each cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to perform a clustering data analysis using the k-means algorithm:</p>
<ol>
<li>The full code for this recipe is given in the <kbd>kmeans.py</kbd> file that has already been provided to you. Now let's take a look at how it's built. Create a new Python file, and import the following packages:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.cluster import KMeans </pre>
<ol start="2">
<li>Now let's load the input data and define the number of clusters. We will use the <kbd>data_multivar.txt</kbd> file that has already been provided to you:</li>
</ol>
<pre style="padding-left: 60px">input_file = ('data_multivar.txt')<br/># Load data<br/>x = []<br/>with open(input_file, 'r') as f:<br/>    for line in f.readlines():<br/>        data = [float(i) for i in line.split(',')]<br/>        x.append(data)<br/><br/>data = np.array(x)
num_clusters = 4</pre>
<ol start="3">
<li>We need to see what the input data looks like. Let's go ahead and add the following lines of code to the Python file:</li>
</ol>
<pre style="padding-left: 60px">plt.figure() 
plt.scatter(data[:,0], data[:,1], marker='o',  
        facecolors='none', edgecolors='k', s=30) 
x_min, x_max = min(data[:, 0]) - 1, max(data[:, 0]) + 1 
y_min, y_max = min(data[:, 1]) - 1, max(data[:, 1]) + 1 
plt.title('Input data') 
plt.xlim(x_min, x_max) 
plt.ylim(y_min, y_max) 
plt.xticks(()) 
plt.yticks(()) </pre>
<p style="padding-left: 60px">If you run this code, you will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-989 image-border" src="assets/f52083d0-7d25-42dd-ad1c-8a8e09e83f0a.png" style="width:85.00em;height:44.33em;"/></p>
<ol start="4">
<li>We are now ready to train the model. Let's initialize the <kbd>kmeans</kbd> object and train it:</li>
</ol>
<pre style="padding-left: 60px">kmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10) 
kmeans.fit(data)</pre>
<ol start="5">
<li>Now that the data is trained, we need to visualize the boundaries. Let's go ahead and add the following lines of code to the Python file:</li>
</ol>
<pre style="padding-left: 60px"># Step size of the mesh 
step_size = 0.01 
 
# Plot the boundaries 
x_min, x_max = min(data[:, 0]) - 1, max(data[:, 0]) + 1 
y_min, y_max = min(data[:, 1]) - 1, max(data[:, 1]) + 1 
x_values, y_values = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size)) 
 
# Predict labels for all points in the mesh 
predicted_labels = kmeans.predict(np.c_[x_values.ravel(), y_values.ravel()]) </pre>
<ol start="6">
<li>We just evaluated the model across a grid of points. Let's plot these results to view the boundaries:</li>
</ol>
<pre style="padding-left: 60px"># Plot the results 
predicted_labels = predicted_labels.reshape(x_values.shape) 
plt.figure() 
plt.clf() 
plt.imshow(predicted_labels, interpolation='nearest', 
           extent=(x_values.min(), x_values.max(), y_values.min(), y_values.max()), 
           cmap=plt.cm.Paired, 
           aspect='auto', origin='lower') 
 
plt.scatter(data[:,0], data[:,1], marker='o',  
        facecolors='none', edgecolors='k', s=30) </pre>
<ol start="7">
<li>Now let's overlay <kbd>centroids</kbd> on top of it:</li>
</ol>
<pre style="padding-left: 60px">centroids = kmeans.cluster_centers_ 
plt.scatter(centroids[:,0], centroids[:,1], marker='o', s=200, linewidths=3, 
        color='k', zorder=10, facecolors='black') 
x_min, x_max = min(data[:, 0]) - 1, max(data[:, 0]) + 1 
y_min, y_max = min(data[:, 1]) - 1, max(data[:, 1]) + 1 
plt.title('Centoids and boundaries obtained using KMeans') 
plt.xlim(x_min, x_max) 
plt.ylim(y_min, y_max) 
plt.xticks(()) 
plt.yticks(()) 
plt.show() </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">If you run this code, you should see the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-990 image-border" src="assets/c419f851-27a7-4d45-adca-58f674a4ca79.png" style="width:84.25em;height:44.58em;"/></p>
<p style="padding-left: 60px">The four centroids and their boundaries are sufficiently highlighted.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>K-means was developed by James MacQueen, who, in 1967, designed it for the purpose of dividing groups of objects into <em>k</em> partitions based on their attributes. It is a variation of the <strong>expectation-maximization</strong> (<strong>EM</strong>) algorithm, whose objective is to determine the groups of k data generated by Gaussian distributions. The difference between the two algorithms lies in the Euclidean distance calculation method. In k-means, it is assumed that the attributes of the object can be represented as vectors, and thus form a vector space. The goal is to minimize the total intra-cluster variance (or standard deviation). Each cluster is identified by a centroid.</p>
<p>The algorithm follows an iterative procedure, as follows:</p>
<ol>
<li>Choose the number of <em>k</em> clusters</li>
<li>Initially, create <em>k</em> partitions and assign each entry partition either randomly, or by using some heuristic information</li>
<li>Calculate the centroid of each group</li>
<li>Calculate the distance between each observation and each cluster centroid</li>
<li>Then, construct a new partition by associating each entry point with the cluster whose centroid is closer to it</li>
<li>The centroid for new clusters is recalculated</li>
<li>Repeat steps 4 to 6 until the algorithm converges</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>The purpose of the algorithm is to locate <em>k</em> centroids, one for each cluster. The position of each centroid is of particular importance as different positions cause different results. The best choice is to put them as far apart as possible from each other. When this is done, you must associate each object with the nearest centroid. In this way, we will get a first grouping. After finishing the first cycle, we go to the next one by recalculating the new <em>k</em> centroids as the cluster's barycenter using the previous one. Once you locate these new <em>k</em> centroids, you need to make a new connection between the same dataset and the new closest centroid. At the end of these operations, a new cycle is performed. Due to this cycle, we can note that the <em>k</em> centroids change their position step by step until they are modified. So, the centroid no longer moves.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Refer to the official documentation of the<span> <kbd>sklearn.cluster.KMeans</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit</a></span></li>
<li><span>Refer to </span><em>MATLAB for Machine Learning,</em> Giuseppe Ciaburro, Packt Publishing</li>
<li><span>Refer to <em>K Means</em> (from Stanford University): <a href="http://stanford.edu/~cpiech/cs221/handouts/kmeans.html">http://stanford.edu/~cpiech/cs221/handouts/kmeans.html</a></span></li>
<li><span>Refer to </span><em>K-means and Hierarchical Clustering</em> (by Andrew Moore): <a href="https://www.autonlab.org/tutorials/kmeans.html">https://www.autonlab.org/tutorials/kmeans.html</a></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compressing an image using vector quantization</h1>
                </header>
            
            <article>
                
<p>One of the main applications of k-means clustering is <strong>vector quantization</strong>. Simply speaking, vector quantization is the <em>N</em>-dimensional version of rounding off. When we deal with one-dimensional data, such as numbers, we use the rounding-off technique to reduce the memory needed to store that value. For example, instead of storing 23.73473572, we just store 23.73 if we want to be accurate up to the second decimal place. Or, we can just store 24 if we don't care about decimal places. It depends on our needs and the trade-off that we are willing to make.</p>
<p>Similarly, when we extend this concept to <em>N</em>-dimensional data, it becomes vector quantization. Of course, there are more nuances to it! Vector quantization is popularly used in image compression where we store each pixel using fewer bits than the original image to achieve compression.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will use a sample image and then we will <span>compress the image further by reducing the number of bits.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to compress an image using vector quantization:</p>
<ol>
<li>The full code for this recipe is given in the <kbd>vector_quantization.py</kbd> file that has already been provided to you. Let's take a look at how it's built. We'll start by importing the required packages. Create a new Python file, and add the following lines:</li>
</ol>
<pre style="padding-left: 60px">import argparse 
 
import numpy as np 
from scipy import misc  
from sklearn import cluster 
import matplotlib.pyplot as plt</pre>
<ol start="2">
<li>Let's create a function to parse the input arguments. We will be able to pass the image and the number of bits per pixel as input arguments:</li>
</ol>
<pre style="padding-left: 60px">def build_arg_parser(): 
    parser = argparse.ArgumentParser(description='Compress the input image \ 
            using clustering') 
    parser.add_argument("--input-file", dest="input_file", required=True, 
            help="Input image") 
    parser.add_argument("--num-bits", dest="num_bits", required=False, 
            type=int, help="Number of bits used to represent each pixel") 
    return parser </pre>
<ol start="3">
<li>Let's create a function to compress the input image:</li>
</ol>
<pre style="padding-left: 60px">def compress_image(img, num_clusters): 
    # Convert input image into (num_samples, num_features)  
    # array to run kmeans clustering algorithm  
   X = img.reshape((-1, 1))   
 
    # Run kmeans on input data 
    kmeans = cluster.KMeans(n_clusters=num_clusters, n_init=4, random_state=5) 
    kmeans.fit(X) 
    centroids = kmeans.cluster_centers_.squeeze() 
    labels = kmeans.labels_ 
 
    # Assign each value to the nearest centroid and  
    # reshape it to the original image shape 
    input_image_compressed = np.choose(labels, centroids).reshape(img.shape) 
 
    return input_image_compressed </pre>
<ol start="4">
<li>Once we compress the image, we need to see how it affects the quality. Let's define a function to plot the output image:</li>
</ol>
<pre style="padding-left: 60px">def plot_image(img, title): 
    vmin = img.min() 
    vmax = img.max() 
    plt.figure() 
    plt.title(title) 
    plt.imshow(img, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)</pre>
<ol start="5">
<li>We are now ready to use all these functions. Let's define the main function that takes the input arguments, processes them, and extracts the output image:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    args = build_arg_parser().parse_args() 
    input_file = args.input_file 
    num_bits = args.num_bits 
 
    if not 1 &lt;= num_bits &lt;= 8: 
        raise TypeError('Number of bits should be between 1 and 8') 
 
    num_clusters = np.power(2, num_bits) 
 
    # Print compression rate 
    compression_rate = round(100 * (8.0 - args.num_bits) / 8.0, 2) 
    print("The size of the image will be reduced by a factor of", 8.0/args.num_bits) 
    print("Compression rate = " + str(compression_rate) + "%") </pre>
<ol start="6">
<li>Let's load the input image:</li>
</ol>
<pre>    # Load input image 
    input_image = misc.imread(input_file, True).astype(np.uint8) 
 
    # original image  
    plot_image(input_image, 'Original image') </pre>
<ol start="7">
<li>Now, let's compress this image using the input argument:</li>
</ol>
<pre>    # compressed image  
    input_image_compressed = compress_image(input_image, num_clusters) 
    plot_image(input_image_compressed, 'Compressed image; compression rate = '  
            + str(compression_rate) + '%') 
 
    plt.show() </pre>
<ol start="8">
<li>We are now ready to run the code; run the following command on your Terminal:</li>
</ol>
<pre>    <strong>$ python vector_quantization.py --input-file flower_image.jpg --num-bits 4</strong></pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre style="padding-left: 60px"><strong>The size of the image will be reduced by a factor of 2.0</strong><br/><strong>Compression rate = 50.0%</strong></pre>
<p style="padding-left: 60px">The input image looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-991 image-border" src="assets/6181237a-5430-43f0-a546-4ee71d70d93d.png" style="width:29.58em;height:29.75em;"/></p>
<p style="padding-left: 60px">You should get a compressed image as the output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-992 image-border" src="assets/49e1ef5c-cf08-4c49-ac10-998b7b994ed2.png" style="width:31.08em;height:31.50em;"/></p>
<ol start="9">
<li>Let's compress the image further by reducing the number of bits to <kbd>2</kbd>. Run the following command on your Terminal:</li>
</ol>
<pre>    <strong>$ python vector_quantization.py --input-file flower_image.jpg --num-bits 2</strong>  </pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre style="padding-left: 60px"><strong>The size of the image will be reduced by a factor of 4.0</strong><br/><strong>Compression rate = 75.0%</strong></pre>
<p style="padding-left: 60px">You should get the following compressed image as the output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-993 image-border" src="assets/dd4dc96c-a349-4d91-a4f6-09dff2fc2014.png" style="width:33.92em;height:34.00em;"/></p>
<ol start="10">
<li>If you reduce the number of bits to <kbd>1</kbd>, you can see that it will become a binary image with black and white as the only two colors. Run the following command:</li>
</ol>
<pre>    <strong>$ </strong><strong>python vector_quantization.py --input-file flower_image.jpg --num-bits 1</strong>  </pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre><strong>The size of the image will be reduced by a factor of 8.0</strong><br/><strong>Compression rate = 87.5%</strong></pre>
<p style="padding-left: 60px">You will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-994 image-border" src="assets/ce68373c-4432-44be-bee8-62aa506869df.png" style="width:30.75em;height:31.33em;"/></p>
<p>We have seen how, by compressing the image further, the quality of the image has undergone considerable downsizing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Vector quantization is an algorithm used for signal compression, image coding, and speech. We use geometric criteria (the Euclidean distance) to find clusters. It is, therefore, an example of unsupervised training. It is a technique that allows the modeling of probability density functions through the distribution of prototype vectors. Vector quantization divides a large set of points (vectors) into clusters by using a similar number of points closer to them. Each cluster is illustrated by its centroid point (as in k-means).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>The vector quantization algorithm can be used to divide a dataset into a number of clusters. The algorithm is based on the calculation of the Euclidean distance for the allocation of the samples to the cluster, to which it belongs. The algorithm consists of the following steps:</p>
<ol>
<li>At the beginning, all the vectors are assigned to the same cluster, whose centroid is calculated as the mean value of all the vectors.</li>
<li>For each centroid, a perturbation is introduced that generates two new cluster centers. The old representative is discarded.</li>
<li>Each carrier is reassigned to one of the new clusters according to the minimum distance criterion.</li>
<li>The new representatives are calculated as the average value of the vectors assigned to each cluster. These will be the new centers of the cluster.</li>
<li>If the end criterion is met, the algorithm terminates. If not, return to step 2.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to the o</span>fficial documentation of the<span> <kbd>sklearn.cluster.KMeans</kbd><span> </span>function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit</a></span></li>
<li><span>Refer to </span><em>Image Compression Using Vector Quantization Algorithms: A Review</em>: <a href="https://pdfs.semanticscholar.org/24d2/db6db81f1000b74246d22641e83390fb1065.pdf">https://pdfs.semanticscholar.org/24d2/db6db81f1000b74246d22641e83390fb1065.pdf</a></li>
<li><span>Refer to the </span><em>Argparse Tutorial</em>: <a href="https://docs.python.org/2/howto/argparse.html">https://docs.python.org/2/howto/argparse.html</a></li>
<li><span>Refer to the o</span>fficial documentation of the <kbd>scipy.misc.imread</kbd> function: <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imread.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imread.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Grouping data using agglomerative clustering</h1>
                </header>
            
            <article>
                
<p>Before we talk about agglomerative clustering, we need to understand <strong>hierarchical clustering</strong>. Hierarchical clustering refers to a set of clustering algorithms that creates tree-like clusters by consecutively splitting or merging them, and they are represented using a tree. <span>Hierarchical clustering algorithms</span> <span>can be either bottom-up or top-down. Now, what does this mean? In bottom-up algorithms, each datapoint is treated as a separate cluster with a single object. These clusters are then successively merged until all the clusters are merged into a single giant cluster. This is called </span><strong>agglomerative clustering</strong><span>. On the other hand, top-down algorithms start with a giant cluster and successively split these clusters until individual datapoints are reached.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In hierarchical clustering, we construct clusters by partitioning the instances <span>recursively using</span> a top-down or bottom-up fashion. We can divide these methods as follows:</p>
<ul>
<li><strong>Agglomerative algorithm</strong> (bottom-up): Here, we obtain the solution from individual statistical units. At each iteration, we aggregate the most closely-related statistical units and the procedure ends when a single cluster is formed.</li>
<li><strong>Divisive algorithm</strong> (top-down): Here, all units are in the same class and the unit that is not similar to others is added to a new cluster for each subsequent iteration.</li>
</ul>
<p>Both methods result in a dendrogram. This represents a nested group of objects, and the similarity levels at which the groups change. By cutting the dendrogram at the desired similarity level, we can get a<span> clustering of data objects</span>. The merging or division of clusters is performed using a similarity measure, which optimizes a criterion.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to group data using agglomerative clustering:</p>
<ol>
<li>The full code for this recipe is given in the <kbd>agglomerative.py</kbd> file that's provided to you. Now let's look at how it's built. Create a new Python file, and import the necessary packages:</li>
</ol>
<pre style="padding-left: 30px">import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.cluster import AgglomerativeClustering 
from sklearn.neighbors import kneighbors_graph </pre>
<ol start="2">
<li>Let's define the function that we need to perform agglomerative clustering:</li>
</ol>
<pre style="padding-left: 60px">def perform_clustering(X, connectivity, title, num_clusters=3, linkage='ward'): 
    plt.figure() 
    model = AgglomerativeClustering(linkage=linkage,  
                    connectivity=connectivity, n_clusters=num_clusters) 
    model.fit(X) </pre>
<ol start="3">
<li>Let's extract the labels and specify the shapes of the markers for the graph:</li>
</ol>
<pre style="padding-left: 60px">    # extract labels 
    labels = model.labels_ 
` 
    # specify marker shapes for different clusters 
    markers = '.vx' </pre>
<ol start="4">
<li>Iterate through the datapoints and plot them accordingly using different markers:</li>
</ol>
<pre style="padding-left: 60px">    for i, marker in zip(range(num_clusters), markers): 
        # plot the points belong to the current cluster 
        plt.scatter(X[labels==i, 0], X[labels==i, 1], s=50,  
                    marker=marker, color='k', facecolors='none') 
 
    plt.title(title)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>In order to demonstrate the advantage of agglomerative clustering, we need to run it on datapoints that are linked spatially, but also located close to each other in space. We want the linked datapoints to belong to the same cluster, as opposed to datapoints that are just spatially close to each other. Let's, now define a function to get a set of datapoints on a spiral:</li>
</ol>
<pre style="padding-left: 60px">def get_spiral(t, noise_amplitude=0.5): 
    r = t 
    x = r * np.cos(t) 
    y = r * np.sin(t) 
 
    return add_noise(x, y, noise_amplitude) </pre>
<ol start="6">
<li>In the previous function, we added some noise to the curve because it adds some uncertainty. Let's define this function:</li>
</ol>
<pre style="padding-left: 60px">def add_noise(x, y, amplitude): 
    X = np.concatenate((x, y)) 
    X += amplitude * np.random.randn(2, X.shape[1]) 
    return X.T </pre>
<ol start="7">
<li>Now let's define another function to get datapoints located on a rose curve:</li>
</ol>
<pre style="padding-left: 60px">def get_rose(t, noise_amplitude=0.02): 
    # Equation for "rose" (or rhodonea curve); if k is odd, then 
    # the curve will have k petals, else it will have 2k petals 
    k = 5        
    r = np.cos(k*t) + 0.25  
    x = r * np.cos(t) 
    y = r * np.sin(t) 
 
    return add_noise(x, y, noise_amplitude) </pre>
<ol start="8">
<li>Just to add more variety, let's also define a <kbd>hypotrochoid</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">def get_hypotrochoid(t, noise_amplitude=0): 
    a, b, h = 10.0, 2.0, 4.0 
    x = (a - b) * np.cos(t) + h * np.cos((a - b) / b * t)  
    y = (a - b) * np.sin(t) - h * np.sin((a - b) / b * t)  
 
    return add_noise(x, y, 0)</pre>
<ol start="9">
<li>We are now ready to define the main function:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    # Generate sample data 
    n_samples = 500  
    np.random.seed(2) 
    t = 2.5 * np.pi * (1 + 2 * np.random.rand(1, n_samples)) 
    X = get_spiral(t) 
 
    # No connectivity 
    connectivity = None  
    perform_clustering(X, connectivity, 'No connectivity') 
 
    # Create K-Neighbors graph  
    connectivity = kneighbors_graph(X, 10, include_self=False) 
    perform_clustering(X, connectivity, 'K-Neighbors connectivity') 
 
    plt.show() </pre>
<p style="padding-left: 60px">If you run this code, you will get the following output if we don't use any connectivity:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-995 image-border" src="assets/b1dce003-053b-48c0-a77a-37baa6c1acef.png" style="width:85.67em;height:46.75em;"/></p>
<p style="padding-left: 60px">The second output diagram looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-996 image-border" src="assets/9229b70e-c4ed-468d-b2f1-f545c25cf904.png" style="width:86.33em;height:46.33em;"/></p>
<p style="padding-left: 60px">As you can see, using the connectivity feature enables us to group the datapoints that are linked to each other as opposed to clustering them, based on their spatial locations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In agglomerative clustering, each observation begins in its cluster and the clusters are subsequently combined. The strategies for joining the clusters are as follows:</p>
<ul>
<li>Ward clustering minimizes the sum of squared differences within all the clusters.</li>
<li>Maximum or complete linkage is used to minimize the maximum distance between observations of pairs of clusters.</li>
<li>Average linkage is used to minimize the average of the distances between all observations of pairs of clusters.</li>
<li>Single linkage is used to minimize the distance between the closest observations of pairs of clusters.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>To decide what clusters must be combined, it is necessary to define a measure of dissimilarity between the clusters. In most hierarchical clustering methods, specific metrics are used to quantify the distance between two pairs of elements, and a linking criterion that defines the dissimilarity of two sets of elements (clusters) as a function of the distance between pairs of elements in the two sets.</p>
<p>These common metrics are as follows:</p>
<ul>
<li class="mce-root">The Euclidean distance</li>
<li class="mce-root">The Manhattan distance </li>
<li class="mce-root">The uniform rule</li>
<li class="mce-root">The Mahalanobis <span>distance</span>, which corrects data by different scales and correlations in variables</li>
<li class="mce-root">The angle between the two vectors</li>
<li class="mce-root">The Hamming distance, which measures the minimum number of substitutions required to change one member into another</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to the o</span>fficial documentation of the<span> <kbd>sklearn.cluster.AgglomerativeClustering</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html</a></span></li>
<li><span>Refer to </span><em>Hierarchical agglomerative clustering</em> (from Stanford University): <a href="https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-agglomerative-clustering-1.html">https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-agglomerative-clustering-1.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the performance of clustering algorithms</h1>
                </header>
            
            <article>
                
<p>So far, we have built different clustering algorithms, but haven't measured their performance. In supervised learning, the predicted values with the original labels are compared to calculate their accuracy. In contrast, in unsupervised learning, we have no labels, so we need to find a way to measure the performance of our algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>A good way to measure a clustering algorithm is by seeing how well the clusters are separated. Are the clusters well separated? Are the datapoints in a cluster that is tight enough? We need a metric that can quantify this behavior. We will use a metric called the <strong>silhouette coefficient</strong> score. This score is defined for each datapoint; this coefficient is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f5ad7de6-612e-4fb8-ba61-d236a35d0e99.png" style="width:11.08em;height:3.17em;"/></p>
<p>Here, <em><strong>x</strong></em> is the average distance between the current datapoint and all the other datapoints in the same cluster, and <em><strong>y</strong></em> is the average distance between the current datapoint and all the datapoints in the next nearest cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to evaluate the performance of clustering algorithms:</p>
<ol>
<li>The full code for this recipe is given in the <kbd>performance.py</kbd> file that has already been provided to you. Now let's look at how it's built. Create a new Python file, and import the following packages:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
from sklearn import metrics 
from sklearn.cluster import KMeans </pre>
<ol start="2">
<li>Let's load the input data from the <kbd>data_perf.txt</kbd> file that has already been provided to you:</li>
</ol>
<pre style="padding-left: 60px">input_file = ('data_perf.txt')<br/><br/>x = []<br/>with open(input_file, 'r') as f:<br/>    for line in f.readlines():<br/>        data = [float(i) for i in line.split(',')]<br/>        x.append(data)<br/><br/>data = np.array(x)</pre>
<ol start="3">
<li>In order to determine the optimal number of clusters, let's iterate through a range of values and see where it peaks:</li>
</ol>
<pre style="padding-left: 60px">scores = [] 
range_values = np.arange(2, 10) 
 
for i in range_values: 
    # Train the model 
    kmeans = KMeans(init='k-means++', n_clusters=i, n_init=10) 
    kmeans.fit(data) 
    score = metrics.silhouette_score(data, kmeans.labels_,  
                metric='euclidean', sample_size=len(data)) 
 
    print("Number of clusters =", i)<br/>    print("Silhouette score =", score)
                     
    scores.append(score) </pre>
<ol start="4">
<li>Now let's plot the graph to see where it peaked:</li>
</ol>
<pre style="padding-left: 60px"># Plot scores 
plt.figure() 
plt.bar(range_values, scores, width=0.6, color='k', align='center') 
plt.title('Silhouette score vs number of clusters') 
 
# Plot data 
plt.figure() 
plt.scatter(data[:,0], data[:,1], color='k', s=30, marker='o', facecolors='none') 
x_min, x_max = min(data[:, 0]) - 1, max(data[:, 0]) + 1 
y_min, y_max = min(data[:, 1]) - 1, max(data[:, 1]) + 1 
plt.title('Input data') 
plt.xlim(x_min, x_max) 
plt.ylim(y_min, y_max) 
plt.xticks(()) 
plt.yticks(()) 
 
plt.show()</pre>
<ol start="5">
<li>If you run this code, you will get the following output on the Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>Number of clusters = 2</strong><br/><strong>Silhouette score = 0.5290397175472954</strong><br/><strong>Number of clusters = 3</strong><br/><strong>Silhouette score = 0.5572466391184153</strong><br/><strong>Number of clusters = 4</strong><br/><strong>Silhouette score = 0.5832757517829593</strong><br/><strong>Number of clusters = 5</strong><br/><strong>Silhouette score = 0.6582796909760834</strong><br/><strong>Number of clusters = 6</strong><br/><strong>Silhouette score = 0.5991736976396735</strong><br/><strong>Number of clusters = 7</strong><br/><strong>Silhouette score = 0.5194660249299737</strong><br/><strong>Number of clusters = 8</strong><br/><strong>Silhouette score = 0.44937089046511863</strong><br/><strong>Number of clusters = 9</strong><br/><strong>Silhouette score = 0.3998899991555578</strong></pre>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">The bar graph looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-997 image-border" src="assets/4b2c8314-3f41-4f14-a81c-ef837e330f53.png" style="width:86.67em;height:41.08em;"/></p>
<p style="padding-left: 60px">As with these scores, the best configuration is five clusters. Let's see what the data actually looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-998 image-border" src="assets/590cfb57-6f65-4c5a-a6bb-9dcf70fbb065.png" style="width:84.08em;height:42.58em;"/></p>
<p style="padding-left: 60px">We can visually confirm that the data, in fact, has five clusters. We just took the example of a small dataset that contains five distinct clusters. This method becomes very useful when you are dealing with a huge dataset that contains high-dimensional data that cannot be visualized easily.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The <kbd>sklearn.metrics.silhouette_score</kbd> function computes the mean silhouette coefficient of all the samples. For each sample, two distances are calculated: the mean intra-cluster distance (<em><strong>x</strong></em>), and the mean nearest-cluster distance (<em><strong>y</strong></em>). The silhouette coefficient for a sample is given by the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/29e29ff3-2b2f-40df-b587-5f0a192a6547.png" style="width:10.25em;height:2.92em;"/></p>
<p class="CDPAlignLeft CDPAlign">Essentially, <strong>y</strong> is the distance between a sample and the nearest cluster that does not include the sample.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p><span>The best value is 1, and the worst value is -1. 0 represents clusters that overlap, while values of less than 0 mean that that particular sample has been attached to the wrong cluster.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to </span>the official documentation of the<span> <kbd>sklearn.metrics.silhouette_score</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html</a></span></li>
<li><span>Refer to </span><em>Silhouette</em> (from Wikipedia):<a href="https://en.wikipedia.org/wiki/Silhouette_(clustering)">https://en.wikipedia.org/wiki/Silhouette_(clustering)</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Estimating the number of clusters using the DBSCAN algorithm</h1>
                </header>
            
            <article>
                
<p>When we discussed the k-means algorithm, we saw that we had to give the number of clusters as one of the input parameters. In the real world, we won't have this information available. We can definitely sweep the parameter space to find out the optimal number of clusters using the silhouette coefficient score, but this will be an expensive process! A method that returns the number of clusters in our data will be an excellent solution to the problem. DBSCAN does just that for us.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will perform a DBSCAN analysis using the <kbd>sklearn.cluster.DBSCAN</kbd> function. We will use <span>the same data that we used in the previous <em>Evaluating the performance of clustering algorithms</em> (<kbd>data_perf.txt</kbd>) recipe, to compare the two methods used.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to automatically estimate the number of clusters using the DBSCAN algorithm:</p>
<ol>
<li>The full code for this recipe is given in the <kbd>estimate_clusters.py</kbd> file that has already been provided to you. Now let's look at how it's built. Create a new Python file, and import the necessary packages:</li>
</ol>
<pre style="padding-left: 60px">from itertools import cycle 
import numpy as np 
from sklearn.cluster import DBSCAN 
from sklearn import metrics 
import matplotlib.pyplot as plt </pre>
<ol start="2">
<li>Load the input data from the <kbd>data_perf.txt</kbd> file. This is the same file that we used in the previous recipe, which will help us to compare the methods on the same dataset:</li>
</ol>
<pre style="padding-left: 60px"># Load data<br/>input_file = ('data_perf.txt')<br/><br/>x = []<br/>with open(input_file, 'r') as f:<br/>    for line in f.readlines():<br/>        data = [float(i) for i in line.split(',')]<br/>        x.append(data)<br/><br/>X = np.array(x)</pre>
<ol start="3">
<li>We need to find the best parameter, so let's initialize a few variables:</li>
</ol>
<pre style="padding-left: 60px"># Find the best epsilon 
eps_grid = np.linspace(0.3, 1.2, num=10) 
silhouette_scores = [] 
eps_best = eps_grid[0] 
silhouette_score_max = -1 
model_best = None 
labels_best = None</pre>
<ol start="4">
<li>Let's sweep the parameter space:</li>
</ol>
<pre style="padding-left: 60px">for eps in eps_grid: 
    # Train DBSCAN clustering model 
    model = DBSCAN(eps=eps, min_samples=5).fit(X) 
 
    # Extract labels 
    labels = model.labels_</pre>
<ol start="5">
<li>For each iteration, we need to extract the performance metric:</li>
</ol>
<pre style="padding-left: 60px">    # Extract performance metric  
    silhouette_score = round(metrics.silhouette_score(X, labels), 4) 
    silhouette_scores.append(silhouette_score) 
 
    print("Epsilon:", eps, " --&gt; silhouette score:", silhouette_score) </pre>
<ol start="6">
<li>We need to store the best score and its associated epsilon value:</li>
</ol>
<pre style="padding-left: 60px">    if silhouette_score &gt; silhouette_score_max: 
        silhouette_score_max = silhouette_score 
        eps_best = eps 
        model_best = model 
        labels_best = labels </pre>
<ol start="7">
<li>Let's now plot the bar graph, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Plot silhouette scores vs epsilon 
plt.figure() 
plt.bar(eps_grid, silhouette_scores, width=0.05, color='k', align='center') 
plt.title('Silhouette score vs epsilon') 
 
# Best params 
print("Best epsilon =", eps_best) </pre>
<ol start="8">
<li>Let's store the best models and labels:</li>
</ol>
<pre style="padding-left: 60px"># Associated model and labels for best epsilon 
model = model_best  
labels = labels_best</pre>
<ol start="9">
<li>Some datapoints may remain unassigned. We need to identify them, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Check for unassigned datapoints in the labels 
offset = 0 
if -1 in labels: 
    offset = 1 </pre>
<ol start="10">
<li>Extract the number of clusters, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Number of clusters in the data  
num_clusters = len(set(labels)) - offset  
 
print("Estimated number of clusters =", num_clusters)</pre>
<ol start="11">
<li>We need to extract all the core samples, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Extracts the core samples from the trained model 
mask_core = np.zeros(labels.shape, dtype=np.bool) 
mask_core[model.core_sample_indices_] = True </pre>
<ol start="12">
<li>Let's visualize the resultant clusters. We will start by extracting the set of unique labels and specifying different markers:</li>
</ol>
<pre style="padding-left: 60px"># Plot resultant clusters  
plt.figure() 
labels_uniq = set(labels) 
markers = cycle('vo^s&lt;&gt;') </pre>
<ol start="13">
<li>Now let's iterate through the clusters and plot the datapoints using different markers:</li>
</ol>
<pre style="padding-left: 60px">for cur_label, marker in zip(labels_uniq, markers): 
    # Use black dots for unassigned datapoints 
    if cur_label == -1: 
        marker = '.' 
 
    # Create mask for the current label 
    cur_mask = (labels == cur_label) 
 
    cur_data = X[cur_mask &amp; mask_core] 
    plt.scatter(cur_data[:, 0], cur_data[:, 1], marker=marker, 
             edgecolors='black', s=96, facecolors='none') 
    cur_data = X[cur_mask &amp; ~mask_core] 
    plt.scatter(cur_data[:, 0], cur_data[:, 1], marker=marker, 
             edgecolors='black', s=32) 
plt.title('Data separated into clusters') 
plt.show()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="14">
<li>If you run this code, you will get the following output on your Terminal:</li>
</ol>
<pre class="CDPAlignLeft CDPAlign"><strong>Epsilon: 0.3 --&gt; silhouette score: 0.1287</strong><br/><strong>Epsilon: 0.39999999999999997 --&gt; silhouette score: 0.3594</strong><br/><strong>Epsilon: 0.5 --&gt; silhouette score: 0.5134</strong><br/><strong>Epsilon: 0.6 --&gt; silhouette score: 0.6165</strong><br/><strong>Epsilon: 0.7 --&gt; silhouette score: 0.6322</strong><br/><strong>Epsilon: 0.7999999999999999 --&gt; silhouette score: 0.6366</strong><br/><strong>Epsilon: 0.8999999999999999 --&gt; silhouette score: 0.5142</strong><br/><strong>Epsilon: 1.0 --&gt; silhouette score: 0.5629</strong><br/><strong>Epsilon: 1.0999999999999999 --&gt; silhouette score: 0.5629</strong><br/><strong>Epsilon: 1.2 --&gt; silhouette score: 0.5629</strong><br/><strong>Best epsilon = 0.7999999999999999</strong><br/><strong>Estimated number of clusters = 5</strong></pre>
<p style="padding-left: 60px">This will produce the following bar graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-999 image-border" src="assets/f2c69ed2-a6e1-4d57-ab31-461187a0346c.png" style="width:84.25em;height:44.08em;"/></p>
<p class="mce-root"/>
<p style="padding-left: 60px">Let's take a look at the labeled datapoints, along with unassigned datapoints marked by solid points in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1000 image-border" src="assets/de991d06-5c2c-464e-a4d3-d4529a12031c.png" style="width:85.75em;height:45.50em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>DBSCAN<span> works by treating datapoints as groups of dense clusters. If a point belongs to a cluster, then there should be a lot of other points that belong to the same cluster. One of the parameters that we can control is the maximum distance of this point from other points. This is called </span><strong>epsilon</strong><span>. No two points in a given cluster should be further away than epsilon. One of the main advantages of this method is that it can deal with outliers. If there are some points located alone in a low-density area, DBSCAN will detect these points as outliers as opposed to forcing them into a cluster.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>DBSCAN presents the following pros and cons:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 51.2397%" class="CDPAlignCenter CDPAlign"><strong>Pros</strong></td>
<td style="width: 47.9339%" class="CDPAlignCenter CDPAlign"><strong>Cons</strong></td>
</tr>
<tr>
<td style="width: 51.2397%">
<ul>
<li>It does not require to know the number of a priori clusters.</li>
<li>It can find clusters of arbitrary forms.</li>
<li>It requires only two parameters.</li>
</ul>
</td>
<td style="width: 47.9339%">
<ul>
<li>The quality of clustering depends on its distance measurement.</li>
<li>It is not able to classify datasets with large differences in density.</li>
</ul>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to the o</span>fficial documentation of the<span> <kbd>sklearn.cluster.DBSCAN</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html</a></span></li>
<li><span>Refer to <em>DBSCAN</em> (from Wikipedia):<a href="https://en.wikipedia.org/wiki/DBSCAN">https://en.wikipedia.org/wiki/DBSCAN</a></span></li>
<li><span>Refer to </span><em>Density-based Methods</em> (from the University at <span>Buffalo</span>): <a href="https://cse.buffalo.edu/~jing/cse601/fa12/materials/clustering_density.pdf">https://cse.buffalo.edu/~jing/cse601/fa12/materials/clustering_density.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding patterns in stock market data</h1>
                </header>
            
            <article>
                
<p>Let's see how we can use unsupervised learning for stock market analysis. Since we don't know how many clusters there are, we'll use an algorithm called <strong>affinity propagation</strong> (<strong>AP</strong>) on the cluster. It tries to find a representative datapoint for each cluster in our data, along with measures of similarity between pairs of datapoints, and considers all our datapoints as potential representatives, also called <strong>exemplars</strong>, of their respective clusters. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will analyze the stock market variations of companies over a specified duration. Our goal is to then find out what companies behave similarly in terms of their quotes over time.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to find patterns in stock market data:</p>
<ol>
<li>The full code for this recipe is given in the <kbd>stock_market.py</kbd> file that has already been provided to you. Now let's look at how it's built. Create a new Python file, and import the following packages:</li>
</ol>
<pre style="padding-left: 60px">import json<br/>import sys<br/>import pandas as pd<br/><br/>import numpy as np<br/>from sklearn import covariance, cluster</pre>
<ol start="2">
<li>We need a file that contains all the symbols and the associated names. This information is located in the <kbd>symbol_map.json</kbd> file provided to you. Let's load this, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Input symbol file 
symbol_file = 'symbol_map.json' </pre>
<ol start="3">
<li>Let's read the data from the <kbd>symbol_map.json</kbd> file:</li>
</ol>
<pre style="padding-left: 60px"># Load the symbol map 
with open(symbol_file, 'r') as f: 
    symbol_dict = json.loads(f.read()) 
 
symbols, names = np.array(list(symbol_dict.items())).T </pre>
<ol start="4">
<li>Now let's load the data. We will use an Excel file (<kbd>stock_market_data.xlsx</kbd>); this is a multisheet file, one for each symbol:</li>
</ol>
<pre style="padding-left: 60px">quotes = []<br/><br/>excel_file = 'stock_market_data.xlsx'<br/><br/>for symbol in symbols:<br/>    print('Quote history for %r' % symbol, file=sys.stderr)<br/>    quotes.append(pd.read_excel(excel_file, symbol))</pre>
<ol start="5">
<li>As we need some feature points for analysis, we will use the difference between the opening and closing quotes every day to analyze the data:</li>
</ol>
<pre style="padding-left: 60px"># Extract opening and closing quotes 
opening_quotes = np.array([quote.open for quote in quotes]).astype(np.float) 
closing_quotes = np.array([quote.close for quote in quotes]).astype(np.float) 
 
# The daily fluctuations of the quotes  
delta_quotes = closing_quotes - opening_quotes </pre>
<ol start="7">
<li>Let's build a graph model:</li>
</ol>
<pre style="padding-left: 60px"># Build a graph model from the correlations 
<span>edge_model = covariance.GraphicalLassoCV(cv=3)</span> </pre>
<ol start="8">
<li>We need to standardize the data before we use it:</li>
</ol>
<pre style="padding-left: 60px"># Standardize the data  
X = delta_quotes.copy().T 
X /= X.std(axis=0) </pre>
<ol start="9">
<li>Now let's train the model using this data:</li>
</ol>
<pre style="padding-left: 60px"># Train the model 
with np.errstate(invalid='ignore'): 
    edge_model.fit(X) </pre>
<ol start="10">
<li>We are now ready to build the clustering model, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Build clustering model using affinity propagation 
_, labels = cluster.affinity_propagation(edge_model.covariance_) 
num_labels = labels.max() 
 
# Print the results of clustering 
for i in range(num_labels + 1): 
    print "Cluster", i+1, "--&gt;", ', '.join(names[labels == i]) </pre>
<ol start="11">
<li>If you run this code, you will get the following output on the Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>Cluster 1 --&gt; Apple, Amazon, Yahoo</strong><br/><strong>Cluster 2 --&gt; AIG, American express, Bank of America, DuPont de Nemours, General Dynamics, General Electrics, Goldman Sachs, GlaxoSmithKline, Home Depot, Kellogg</strong><br/><strong>Cluster 3 --&gt; Boeing, Canon, Caterpillar, Ford, Honda</strong><br/><strong>Cluster 4 --&gt; Colgate-Palmolive, Kimberly-Clark</strong><br/><strong>Cluster 5 --&gt; Cisco, Dell, HP, IBM</strong><br/><strong>Cluster 6 --&gt; Comcast, Cablevision</strong><br/><strong>Cluster 7 --&gt; CVS</strong><br/><strong>Cluster 8 --&gt; ConocoPhillips, Chevron</strong></pre>
<p>Eight clusters are identified. From an initial analysis, we can see that the grouped companies seem to treat the same products: IT, banks, engineering, detergents, and computers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>AP is a clustering algorithm based on the concept of passing messages between points (item). Unlike clustering algorithms such as k-means, AP does not require the cluster number to be defined a priori. AP searches for representative members (exemplars) of the set of inputs, which are, in fact, representative of the individual clusters.</p>
<p>The central point of the AP algorithm is the identification of a subset of exemplars. In the input, a matrix of similarity is taken between pairs of data. The data exchanges real values as messages until suitable specimens emerge, and consequently, good clusters are obtained.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>To perform <span>AP clustering, the <kbd>sklearn.cluster.affinity_propagation()</kbd> function was used. In the case of training samples with similar similarities and preferences, the assignment of cluster centers and labels depends on preference. If the preference is less than the similarities, a single cluster center and a 0 label for each sample will be returned. Otherwise, each training sample becomes its cluster center and a unique mark is assigned.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to the o</span>fficial documentation of the<span> <kbd>sklearn.cluster.affinity_propagation()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.affinity_propagation.html">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.affinity_propagation.html</a></span></li>
<li>Refer to <span><em>AFFINITY PROPAGATION: CLUSTERING DATA BY PASSING MESSAGES</em> (from Toronto University):</span> <span class="URLPACKT"><a href="http://www.cs.columbia.edu/~delbert/docs/DDueck-thesis_small.pdf">http://www.cs.columbia.edu/~delbert/docs/DDueck-thesis_small.pdf</a>.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a customer segmentation model</h1>
                </header>
            
            <article>
                
<p>One of the main applications of unsupervised learning is market segmentation. This is when we don't have labeled data available all the time, but it's important to segment the market so that people can target individual groups. This is very useful in advertising, inventory management, implementing strategies for distribution, and mass media. Let's go ahead and apply unsupervised learning to one such use case to see how it can be useful.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will be dealing with a wholesale vendor and his customers. We will be using the data available at <a href="https://archive.ics.uci.edu/ml/datasets/Wholesale+customers"><span class="URLPACKT">https://archive.ics.uci.edu/ml/datasets/Wholesale+customers</span></a>. The spreadsheet contains data regarding the consumption of different types of items by their customers and our goal is to find clusters so that they can optimize their sales and distribution strategy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to build a customer segmentation model:</p>
<ol>
<li>The full code for this recipe is given in the <kbd>customer_segmentation.py</kbd> file that has already been provided to you. Now let's look at how it's built. Create a new Python file, and import the following packages:</li>
</ol>
<pre style="padding-left: 60px">import csv 
import numpy as np 
from sklearn.cluster import MeanShift, estimate_bandwidth 
import matplotlib.pyplot as plt </pre>
<ol start="2">
<li>Let's load the input data from the <kbd>wholesale.csv</kbd> file that's already provided to you:</li>
</ol>
<pre style="padding-left: 60px"># Load data from input file 
input_file = 'wholesale.csv' 
file_reader = csv.reader(open(input_file, 'rt'), delimiter=',') 
X = [] 
for count, row in enumerate(file_reader): 
    if not count: 
        names = row[2:] 
        continue 
 
    X.append([float(x) for x in row[2:]]) 
 
# Input data as numpy array 
X = np.array(X) </pre>
<ol start="3">
<li>Let's build a mean shift model:</li>
</ol>
<pre style="padding-left: 60px"># Estimating the bandwidth  
bandwidth = estimate_bandwidth(X, quantile=0.8, n_samples=len(X)) 
 
# Compute clustering with MeanShift 
meanshift_estimator = MeanShift(bandwidth=bandwidth, bin_seeding=True) 
meanshift_estimator.fit(X) 
labels = meanshift_estimator.labels_ 
centroids = meanshift_estimator.cluster_centers_ 
num_clusters = len(np.unique(labels)) 
 
print("Number of clusters in input data =", num_clusters) </pre>
<ol start="4">
<li>Let's print the centroids of clusters that we obtained, as follows:</li>
</ol>
<pre style="padding-left: 60px">print("Centroids of clusters:")<br/>print('\t'.join([name[:3] for name in names]))<br/>for centroid in centroids:<br/>    print('\t'.join([str(int(x)) for x in centroid]))</pre>
<ol start="5">
<li>Let's visualize a couple of features to get a sense of the output:</li>
</ol>
<pre style="padding-left: 60px"># Visualizing data 
 
centroids_milk_groceries = centroids[:, 1:3] 
 
# Plot the nodes using the coordinates of our centroids_milk_groceries 
plt.figure() 
plt.scatter(centroids_milk_groceries[:,0], centroids_milk_groceries[:,1],  
        s=100, edgecolors='k', facecolors='none') 
 
offset = 0.2 
plt.xlim(centroids_milk_groceries[:,0].min() - offset * centroids_milk_groceries[:,0].ptp(), 
        centroids_milk_groceries[:,0].max() + offset * centroids_milk_groceries[:,0].ptp(),) 
plt.ylim(centroids_milk_groceries[:,1].min() - offset * centroids_milk_groceries[:,1].ptp(), 
        centroids_milk_groceries[:,1].max() + offset * centroids_milk_groceries[:,1].ptp()) 
 
plt.title('Centroids of clusters for milk and groceries') 
plt.show() </pre>
<ol start="6">
<li>If you run this code, you will get the following output on the Terminal:</li>
</ol>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1001 image-border" src="assets/a543761c-e0ec-4210-9eb4-6e34a1859023.png" style="width:25.58em;height:10.33em;"/></p>
<p style="padding-left: 60px">You will get the following output that depicts the centroids for the features, <em>milk</em> and <em>groceries,</em> where milk is on the <em>x</em> axis and groceries is on the <em>y</em> axis:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1002 image-border" src="assets/8db5b4ab-144e-41b1-99ed-35e5e3204271.png" style="width:86.83em;height:43.83em;"/></p>
<p class="CDPAlignCenter CDPAlign"><span>In this output, the eight centroids of the identified clusters are clearly represented.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we have faced a clustering problem by using the mean shift algorithm. It is a clustering type that assigns datapoints to clusters in an iterative manner by moving points to the mode. The mode is the value that appears most frequently.</p>
<p>The algorithm assigns iteratively each data point to the centroid of the nearest cluster. The centroid of the nearest cluster is determined by where most of the neighboring points are located. Thus, at each iteration, each data point approaches the point where the greatest number of points is located, which is, or will lead to, the cluster center. When the algorithm stops, each point is assigned to a cluster. Unlike the k-means algorithm, the mean shift algorithm is not required in advance to specify the number of clusters; this is determined automatically by the algorithm. The mean shift algorithm is widely used in the field of image processing and artificial vision.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>To perform the mean shift clustering, a <kbd>sklearn.cluster.MeanShift()</kbd> function was used. This function carries out a mean shift clustering using a flat kernel. The mean shift clustering allows us to identify point aggregates in a uniform density of samples. Candidates for the centroids are updated with the average of points within a given region. These points are then filtered in a postprocessing phase to eliminate possible duplicates to form the final set of centroids. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to </span>the official documentation of the<span> <kbd>sklearn.cluster.MeanShift()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html#sklearn-cluster-meanshift">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html#sklearn-cluster-meanshift</a></span></li>
<li><span>Refer to <em>Mean Shift: A Robust Approach Toward Feature Space Analysis</em></span> (by Dorin Comaniciu , Peter Meer): <a href="https://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf">https://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using autoencoders to reconstruct handwritten digit images</h1>
                </header>
            
            <article>
                
<p>An autoencoder is a neural network whose purpose is to code its input into small dimensions, and for the result that is obtained to be able to reconstruct the input itself. Autoencoders are made up by the union of the following two subnets: encoder and decoder. A loss function<span> is added</span> to these functions and it is calculated as the distance between the amount of information loss between the compressed representation of the data and the decompressed representation. The encoder and the decoder will be differentiable with respect to the distance function, so the parameters of the encoding and decoding functions can be optimized to minimize the loss of reconstruction, using the gradient stochastic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><strong>Handwriting recognition</strong> (<strong>HWR</strong>) is widely used in modern technology. The written text image can be taken offline from a piece of paper by optical scanning (<strong>optical character recognition</strong>, <strong>OCR</strong>), or intelligent word recognition. Calligraphy recognition shows the ability of a computer to receive and interpret input that can be understood by hand from sources such as paper documents, touchscreens, photographs, and other devices. HWR consists of various techniques that generally require OCR. However, a complete script recognition system also manages formatting, carries out correct character segmentation, and finds the most plausible words.</p>
<p>The <strong>Modified National Institute of Standards and Technology</strong> (<strong>MNIST</strong>) is a large database of handwritten digits. It has a set of 70,000 examples of data. It is a subset of MNIST's larger dataset. The digits are of 28 x 28 pixel resolution and are stored in a matrix of 70,000 rows and 785 columns; 784 columns form each pixel value from the 28 x 28 matrix, and one value is the actual digit. The digits have been size-normalized and centered in a fixed-size image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to build autoencoders to reconstruct handwritten digit images:</p>
<ol>
<li>The full code for this recipe is given in the <kbd><span>AutoencMnist</span>.py</kbd><span> </span>file that has already been provided to you. Let's look at how it's built. Create a new Python file, and import the following package:</li>
</ol>
<pre style="padding-left: 60px">from keras.datasets import mnist</pre>
<ol start="2">
<li>To import the MNIST dataset, the following code must be used:</li>
</ol>
<pre style="padding-left: 60px">(XTrain, YTrain), (XTest, YTest) = mnist.load_data()<br/><br/>print('XTrain shape = ',XTrain.shape)<br/>print('XTest shape = ',XTest.shape)<br/>print('YTrain shape = ',YTrain.shape)<br/>print('YTest shape = ',YTest.shape)</pre>
<ol start="3">
<li>After importing the dataset, we have printed the shape of the data, and the following results are returned:</li>
</ol>
<pre style="padding-left: 60px">XTrain shape = (60000, 28, 28)<br/>XTest shape = (10000, 28, 28)<br/>YTrain shape = (60000,)<br/>YTest shape = (10000,)</pre>
<ol start="4">
<li>The 70,000 items in the database were divided into 60,000 items for training, and 10,000 items for testing. The data output is represented by integers in the range 0 to 9. Let's check it as follows:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>print('YTrain values = ',np.unique(YTrain))<br/>print('YTest values = ',np.unique(YTest))</pre>
<ol start="5">
<li>The following results are printed<span>:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><strong><span>YTrain values = [0 1 2 3 4 5 6 7 8 9]<br/>YTest values = [0 1 2 3 4 5 6 7 8 9]</span></strong></pre>
<ol start="6">
<li>It may be useful to analyze the distribution of the two values in the available arrays. To start, we count the number of occurrences:</li>
</ol>
<pre style="padding-left: 60px">unique, counts = np.unique(YTrain, return_counts=True)<br/>print('YTrain distribution = ',dict(zip(unique, counts)))<br/>unique, counts = np.unique(YTest, return_counts=True)<br/>print('YTrain distribution = ',dict(zip(unique, counts)))</pre>
<ol start="7">
<li>The following results are returned:</li>
</ol>
<pre style="padding-left: 60px"><strong>YTrain distribution = {0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}</strong><br/><strong>YTrain distribution = {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}</strong></pre>
<ol start="8">
<li>We can also see it in a graph, as follows:</li>
</ol>
<pre>import matplotlib.pyplot as plt<br/>plt.figure(1)<br/>plt.subplot(121)<br/>plt.hist(YTrain, alpha=0.8, ec='black')<br/>plt.xlabel("Classes")<br/>plt.ylabel("Number of occurrences")<br/>plt.title("YTrain data")<br/><br/>plt.subplot(122)<br/>plt.hist(YTest, alpha=0.8, ec='black')<br/>plt.xlabel("Classes")<br/>plt.ylabel("Number of occurrences")<br/>plt.title("YTest data")<br/>plt.show()</pre>
<ol start="9">
<li>To compare the results obtained on both output datasets (<kbd>YTrain</kbd> and <kbd>YTest</kbd>), two histograms were traced and displayed side by side, as shown in the following output:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><br/>
<img class="aligncenter size-full wp-image-1003 image-border" src="assets/0a15b2ac-7b88-48a3-85ec-65d2e28bcc5e.png" style="width:88.17em;height:45.33em;"/></p>
<ol start="10">
<li>From the analysis of the previous output, we can see that in both datasets, the 10 digits are represented in the same proportions. In fact, the bars seem to have the same dimensions, even if the vertical axis has different ranges.</li>
<li>Now, we have to normalize all values between 0 and 1:</li>
</ol>
<pre style="padding-left: 60px">XTrain = XTrain.astype('float32') / 255<br/>XTest = XTest.astype('float32') / 255</pre>
<ol start="12">
<li>To reduce the dimensionality, we will flatten the 28 x 28 images into vectors of size 784:</li>
</ol>
<pre style="padding-left: 60px">XTrain = XTrain.reshape((len(XTrain), np.prod(XTrain.shape[1:])))<br/>XTest = XTest.reshape((len(XTest), np.prod(XTest.shape[1:])))</pre>
<ol start="13">
<li>Now, we will build the model using the Keras functional API. Let's start importing the libraries:</li>
</ol>
<pre style="padding-left: 60px">from keras.layers import Input <br/>from keras.layers import Dense<br/>from keras.models import Model</pre>
<ol start="14">
<li>Then, we can build the Keras model, as follows:</li>
</ol>
<pre style="padding-left: 60px">InputModel = Input(shape=(784,))<br/>EncodedLayer = Dense(32, activation='relu')(InputModel)<br/>DecodedLayer = Dense(784, activation='sigmoid')(EncodedLayer)<br/>AutoencoderModel = Model(InputModel, DecodedLayer)<br/>AutoencoderModel.summary()</pre>
<p style="padding-left: 60px">The following output shows the model architecture:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1004 image-border" src="assets/ac5cbb23-0a56-4f4f-a28c-1f5537a38961.png" style="width:42.25em;height:12.50em;"/></p>
<ol start="15">
<li>So, we have to configure the model for training. To do this, we will use the <kbd>compile</kbd> method, as follows:</li>
</ol>
<pre style="padding-left: 60px">AutoencoderModel.compile(optimizer='adadelta', loss='binary_crossentropy')</pre>
<ol start="16">
<li>At this point, we can train the model, as follows:</li>
</ol>
<pre style="padding-left: 60px">history = AutoencoderModel.fit(XTrain, XTrain,<br/><span>batch_size=256,<br/></span><span>epochs=100,<br/></span><span>shuffle=True,<br/></span><span>validation_data=(XTest, XTest))</span></pre>
<ol start="17">
<li>Our model is now ready, so we can use it to rebuild the handwritten digits automatically. To do this, we will use the <kbd>predict()</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">DecodedDigits = AutoencoderModel.predict(XTest)</pre>
<ol start="18">
<li>We have now finished; the model has been trained and will later be used to make predictions. So, we can just print the starting handwritten digits and those that were reconstructed from our model. Of course, we will do it only for some of the 60,000 digits contained in the dataset. In fact, we will limit ourselves to displaying the first five; we will also use the <kbd>matplotlib</kbd> library in this case:</li>
</ol>
<pre style="padding-left: 60px">n=5<br/>plt.figure(figsize=(20, 4))<br/>for i in range(n):<br/> ax = plt.subplot(2, n, i + 1)<br/> plt.imshow(XTest[i+10].reshape(28, 28))<br/> plt.gray()<br/> ax.get_xaxis().set_visible(False)<br/> ax.get_yaxis().set_visible(False)<br/> ax = plt.subplot(2, n, i + 1 + n)<br/> plt.imshow(DecodedDigits[i+10].reshape(28, 28))<br/> plt.gray()<br/> ax.get_xaxis().set_visible(False)<br/> ax.get_yaxis().set_visible(False)<br/>plt.show()</pre>
<p style="padding-left: 60px">The results are shown in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1005 image-border" src="assets/8aa1ffc4-1688-4590-9ec6-94b3f059c7f7.png" style="width:24.42em;height:5.92em;"/></p>
<p class="CDPAlignLeft CDPAlign">As you can see in the preceding output, the result is very close to the original, meaning that the model works well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>An autoencoder is a neural network whose purpose is to code its input into small dimensions and the result obtained so as to be able to reconstruct the input itself. Autoencoders are made up of a union of the following two subnets.</p>
<p>First, we have an encoder that calculates the following function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4f384c7f-ab17-490d-affa-84c1b17d111b.png" style="width:5.83em;height:1.83em;"/></p>
<p>Given an <em>x</em> input, the encoder encodes it in a z variable, which is also called a latent variable. <em>z</em> usually has much smaller dimensions than <em>x</em>.</p>
<p>Second, we have a decoder that calculates the following function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/defee6c0-e241-476b-bfdb-ad8bbf6ac0c3.png" style="width:6.33em;height:1.92em;"/></p>
<p>Since <em>z</em> is the code of <em>x</em> produced by the encoder, the decoder must decode it so that <em>x'</em> is similar to <em>x</em>. The training of autoencoders is intended to minimize the mean squared error between the input and the result.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Keras is a Python library that provides a simple and clean way to create a range of deep learning models. The Keras code was released under the MIT license. Keras has been structured based on austerity and simplicity, and it provides a programming model without ornaments to maximize readability. It allows neural networks to be expressed in a very modular way, considering models as a sequence or a single graph.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Refer to the official documentation of the<span> Keras library: <a href="https://keras.io/">https://keras.io/</a></span></li>
<li>Refer to <em>Keras 2.x Projects</em>, Giuseppe Ciaburro, Packt Publishing.</li>
</ul>


            </article>

            
        </section>
    </body></html>