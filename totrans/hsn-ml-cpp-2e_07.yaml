- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, the task of classification is that of dividing a set of
    observations (objects) into groups called **classes**, based on an analysis of
    their formal description. In **classification**, each observation (object) is
    assigned to a group or nominal category based on specific qualitative properties.
    Classification is a supervised task because it requires known classes for training
    samples. The labeling of a training set is usually done manually, with the involvement
    of specialists in the given field of study. It’s also notable that if classes
    are not initially defined, then there will be a problem with clustering. Furthermore,
    in the classification task, there may be more than two classes (multi-class),
    and each of the objects may belong to more than one class (intersecting).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss various approaches to solving a classification
    task with machine learning. We are going to look at some of the most well-known
    and widespread algorithms, which are logistic regression, **support vector machine**
    (**SVM**), and **k-nearest neighbors** (**kNN**). Logistic regression is one of
    the most straightforward algorithms based on linear regression and a special loss
    function. SVM is based on a concept of support vectors that helps to build a decision
    boundary to separate data. This approach can be effectively used with high-dimensional
    data. kNN has a simple implementation algorithm that uses the idea of data compactness.
    Also, we will show how the multi-class classification problem can be solved with
    the algorithms mentioned previously. We will implement program examples to see
    how to use these algorithms to solve the classification task with different C++
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of classification methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring various classification methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of using C++ libraries for dealing with the classification task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The required technologies and installations for this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `mlpack` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Dlib` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Flashlight library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A modern C++ compiler with C++20 support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CMake build system version >= 3.10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files for this chapter can be found at the following GitHub repo:
    [https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-Edition/tree/main/Chapter07](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-Edition/tree/main/Chapter07).'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of classification methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification is a fundamental task in **applied statistics**, **machine learning**,
    and **artificial intelligence** (**AI**). This is because classification is one
    of the most understandable and easy-to-interpret data analysis technologies, and
    classification rules can be formulated in a natural language. In machine learning,
    a classification task is solved using supervised algorithms because the classes
    are defined in advance, and the objects in the training set have class labels.
    Analytical models that solve a classification task are called **classifiers**.
  prefs: []
  type: TYPE_NORMAL
- en: Classification is the process of moving an object to a predetermined class based
    on its formalized features. Each object in this problem is usually represented
    as a vector in *N*-dimensional space. Each dimension in that space is a description
    of one of the features of the object.
  prefs: []
  type: TYPE_NORMAL
- en: We can formulate the classification task with mathematical notation. Let *X*
    denote the set of descriptions of objects, and *Y* be a finite set of names or
    class labels. There is an unknown objective function—namely, the mapping ![](img/B19849_Formula_0011.png),
    whose values are known only on the objects of the final training sample, ![](img/B19849_Formula_0021.png).
    So, we have to construct an ![](img/B19849_Formula_0031.png) algorithm, capable
    of classifying an ![](img/B19849_Formula_0041.png) arbitrary object. In mathematical
    statistics, classification problems are also called discriminant analysis problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The classification task is applicable to many areas, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trade**: The classification of customers and products allows a business to
    optimize marketing strategies, stimulate sales, and reduce costs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Telecommunications**: The classification of subscribers allows a business
    to appraise customer loyalty, and therefore develop loyalty programs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medicine and health care**: Assisting the diagnosis of disease by classifying
    the population into risk groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Banking**: The classification of customers is used for credit-scoring procedures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Classification can be solved by using the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kNN method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discriminant analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We looked into discriminant analysis in [*Chapter 6*](B19849_06.xhtml#_idTextAnchor301),
    *Dimensionality Reduction*, as an algorithm for dimensionality reduction, but
    most libraries provide an **application programming interface** (**API**) for
    working with the discriminant analysis algorithm as a classifier, too. We will
    discuss decision trees in [*Chapter 9*](B19849_09.xhtml#_idTextAnchor496), *Ensemble
    Learning*, focusing on algorithm ensembles. We will also discuss neural networks
    in the chapter that follows this: [*Chapter 10*](B19849_10.xhtml#_idTextAnchor539),
    *Neural Networks for* *Image Classification*.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ve discussed what the classification task is, let’s look at various classification
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring various classification methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, **deep learning** has become increasingly popular for classification
    tasks too, especially when dealing with complex and high-dimensional data such
    as images, audio, and text. Deep neural networks can learn hierarchical representations
    of data that allow them to perform accurate classification. In this chapter, we
    concentrate on more classical classification approaches because they are still
    applicable and usually require fewer computational resources. Specifically, we
    will discuss some of the classification methods such as logistic regression, **kernel
    ridge regression** (**KRR**), the kNN method, and SVM approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression determines the degree of dependence between the categorical
    dependent and one or more independent variables by using the logistic function.
    It aims to find the values of the coefficients for the input variables, as with
    linear regression. The difference, in the case of logistic regression, is that
    the output value is converted by using a non-linear (logistic) function. The logistic
    function has an S-shaped curve and converts any value to a number between `0`
    and `1`. This property is useful because we can apply the rule to the output of
    the logistic function to bind `0` and `1` to a class prediction. The following
    screenshot shows a logistic function graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Logistic function](img/B19849_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Logistic function
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the result of the function is less than `0.5`, then the output
    is `0`. Prediction is not just a simple answer (`+1` or `-1`) either, and we can
    interpret it as a probability of being classified as `+1`.
  prefs: []
  type: TYPE_NORMAL
- en: In many tasks, this interpretation is an essential business requirement. For
    example, in the task of credit-scoring, where logistic regression is traditionally
    used, the probability of a loan being defaulted on is a common prediction. As
    with the case of linear regression, logistic regression performs the task better
    if outliers and correlating variables are removed. The logistic regression model
    can be quickly trained and is well-suited for binary classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea of a linear classifier is that the feature space can be divided
    by a hyperplane into two half-spaces, in each of which one of the two values of
    the target class is predicted. If we can divide a feature space without errors,
    then the training set is called **linearly separable**. Logistic regression is
    a unique type of linear classifier, but it is able to predict the probability
    of ![](img/B19849_Formula_0051.png), attributing the example of ![](img/B19849_Formula_006.png)
    to the class *+*, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Consider the task of binary classification, with labels of the target class
    denoted by `+1` (positive examples) and `-1` (negative examples). We want to predict
    the probability of ![](img/B19849_Formula_0081.png); so, for now, we can build
    a linear forecast using the following optimization technique: ![](img/B19849_Formula_0091.png).
    So, how do we convert the resulting value into a probability whose limits are
    `[0, 1]`? This approach requires a specific function. In the logistic regression
    model, the specific function ![](img/B19849_Formula_010.png) is used for this.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s denote *P(X)* by the probability of the occurring event *X*. The probability
    odds ratio *OR(X)* is determined from ![](img/B19849_Formula_0111.png). This is
    the ratio of the probabilities of whether the event will occur or not. We can
    see that the probability and the odds ratio both contain the same information.
    However, while *P(X)* is in the range of `0` to `1`, *OR(X)* is in the range of
    `0` to ![](img/B19849_Formula_0121.png). If you calculate the logarithm of *OR(X)*
    (known as the **logarithm of the odds**, or the **logarithm of the probability
    ratio**), it is easy to see that the following applies: ![](img/B19849_Formula_0131.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the logistic function to predict the probability of ![](img/B19849_Formula_0051.png),
    it can be obtained from the probability ratio (for the time being, let’s assume
    we have the weights too) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, the logistic regression predicts the probability of classifying a sample
    to the "*+*" class as a sigmoid transformation of a linear combination of the
    model weights vector, as well as the sample’s features vector, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From the maximum likelihood principle, we can obtain an optimization problem
    that the logistic regression solves—namely, the minimization of the logistic loss
    function. For the "`-`" class, the probability is determined by a similar formula,
    as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The expressions for both classes can be combined into one, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the expression ![](img/B19849_Formula_019.png) is called the margin of
    classification of the ![](img/B19849_Formula_006.png) object. The classification
    margin can be understood as a model’s *confidence* in the object’s classification.
    An interpretation of this margin is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If the margin vector’s absolute value is large and positive, the class label
    is set correctly, and the object is far from the separating hyperplane. Such an
    object is therefore classified confidently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the margin is large (by modulo) but negative, then the class label is set
    incorrectly. The object is far from the separating hyperplane. Such an object
    is most likely an anomaly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the margin is small (by modulo), then the object is close to the separating
    hyperplane. In this case, the margin sign determines whether the object is correctly
    classified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the discrete case, the likelihood function ![](img/B19849_Formula_0212.png)
    can be interpreted as the probability that the sample *X*1 *, . . . , X*n is equal
    to *x*1 *, . . . , x*nin the given set of experiments. Furthermore, this probability
    depends on *θ*, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The maximum likelihood estimate ![](img/B19849_Formula_0231.png) for the unknown
    parameter ![](img/B19849_Formula_0241.png) is called the value of ![](img/B19849_Formula_0241.png),
    for which the function ![](img/B19849_Formula_0261.png) reaches its maximum (as
    a function of *θ*, with fixed ![](img/B19849_Formula_0271.png)), as illustrated
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can write out the likelihood of the sample—namely, the probability
    of observing the given vector ![](img/B19849_Formula_0291.png) in the sample ![](img/B19849_Formula_0301.png).
    We make one assumption—objects arise independently from a single distribution,
    as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0311.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s take the logarithm of this expression since the sum is much easier to
    optimize than the product, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the principle of maximizing the likelihood leads to a minimization
    of the expression, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This formula is a logistic loss function, summed over all objects of the training
    sample. Usually, it is a good idea to add some regularization to a model to deal
    with overfitting. **L2 regularization** of logistic regression is arranged in
    much the same way as for the ridge regression (**linear regression** with regularization).
    However, it is common to use the controlled variable decay parameter *C* that
    is used in SVM models, where it denotes soft margin parameter denotation. So,
    for logistic regression, *C* is equal to the inverse regularization coefficient
    ![](img/B19849_Formula_0341.png). The relationship between *C* and ![](img/B19849_Formula_0351.png)
    would be the following: lowering *C* would strengthen the regularization effect.
    Therefore, instead of the functional ![](img/B19849_Formula_0361.png), the following
    function should be minimized:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_037.jpg)'
  prefs: []
  type: TYPE_IMG
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: For this function minimization, we can apply different methods—for example,
    the method of least squares, or the **gradient descent** method. The vital issue
    with logistic regression is that it is generally a linear classifier, in order
    to deal with non-linear decision boundaries, which typically use polynomial features
    with original features as a basis for them. This approach was discussed in [*Chapter
    3*](B19849_03.xhtml#_idTextAnchor152) when we discussed polynomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: KRR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KRR combines linear ridge regression (linear regression and L2 norm regularization)
    with the kernel trick and can be used for classification problems. It learns a
    linear function in the higher-dimensional space produced by the chosen kernel
    and training data. For non-linear kernels, it learns a non-linear function in
    the original space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model learned by KRR is identical to the SVM model, but these approaches
    have the following differences:'
  prefs: []
  type: TYPE_NORMAL
- en: The KRR method uses squared error loss, while the SVM model uses insensitive
    loss or hinge loss for classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In contrast to the SVM method, the KRR training can be completed in closed form
    so that it can be trained faster for medium-sized datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learned KRR model is non-sparse and can be slower than the SVM model when
    it comes to prediction times
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite these differences, both approaches usually use L2 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: SVM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SVM method is a set of algorithms used for classification and regression
    analysis tasks. Considering that in an *N*-dimensional space, each object belongs
    to one of two classes, SVM generates an (*N-1*)-dimensional hyperplane to divide
    these points into two groups. It’s similar to an on-paper depiction of points
    of two different types that can be linearly divided. Furthermore, the SVM selects
    the hyperplane, which is characterized by the maximum distance from the nearest
    group elements.
  prefs: []
  type: TYPE_NORMAL
- en: The input data can be separated using various hyperplanes. The best hyperplane
    is a hyperplane with the maximum resulting separation and the maximum resulting
    difference between the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine the data points on the plane. In the following case, the separator
    is just a straight line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Point separation line](img/B19849_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Point separation line
  prefs: []
  type: TYPE_NORMAL
- en: Let’s draw distinct straight lines that divide the points into two sets. Then,
    choose a straight line as far as possible from the points, maximizing the distance
    from it to the nearest point on each side. If such a line exists, then it is called
    the maximum margin hyperplane. Intuitively, a good separation is achieved due
    to the hyperplane itself (which has the longest distance to the nearest point
    of the training sample of any class), since, in general, the bigger the distance,
    the smaller the classifier error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider learning samples given with the set ![](img/B19849_Formula_0391.png)
    consisting of ![](img/B19849_Formula_0401.png) objects with ![](img/B19849_Formula_119.png)
    parameters, where ![](img/B19849_Formula_0421.png) takes the values -1 or 1, thus
    defining the point’s classes. Each point ![](img/B19849_Formula_0431.png) is a
    vector of the dimension ![](img/B19849_Formula_119.png). Our task is to find the
    maximum margin hyperplane that separates the observations. We can use analytic
    geometry to define any hyperplane as a set of points ![](img/B19849_Formula_0431.png)
    that satisfy the condition, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_046.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_0471.png) and ![](img/B19849_Formula_0481.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the linear separating (discriminant) function is described by the equation
    *g(x)=0*. The distance from the point to the separating function *g(x)=0* (the
    distance from the point to the plane) is equal to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_049.jpg)'
  prefs: []
  type: TYPE_IMG
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_312.png) lies in the closure of the boundary that is
    ![](img/B19849_Formula_0512.png). The border, which is the width of the dividing
    strip, needs to be as large as possible. Considering that the closure of the boundary
    satisfies the condition ![](img/B19849_Formula_0521.png), then the distance from
    ![](img/B19849_Formula_0531.png) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_054.jpg)'
  prefs: []
  type: TYPE_IMG
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the width of the dividing strip is ![](img/B19849_Formula_0551.png).
    To exclude points from the dividing strip, we can write out the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_056.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s also introduce the index function ![](img/B19849_Formula_057.png) that
    shows to which class ![](img/B19849_Formula_058.png) belongs, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0591.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the task of choosing a separating function that generates a corridor
    of the greatest width can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_060.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The *J(w)* function was introduced with the assumption that ![](img/B19849_Formula_0612.png)
    for all ![](img/B19849_Formula_194.png). Since the objective function is quadratic,
    this problem has a unique solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the Kuhn-Tucker theorem, this condition is equivalent to the following
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_063.jpg)'
  prefs: []
  type: TYPE_IMG
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 'This is provided that ![](img/B19849_Formula_0641.png) and ![](img/B19849_Formula_0651.png),
    where ![](img/B19849_Formula_0661.png), are new variables. We can rewrite ![](img/B19849_Formula_0671.png)
    in the matrix form, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_068.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The *H* coefficients of the matrix can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_069.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Quadratic programming methods can solve the task ![](img/B19849_Formula_0701.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'After finding the optimal ![](img/B19849_Formula_0712.png) for every ![](img/B19849_Formula_0721.png),
    one of the two following conditions is fulfilled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0731.png) (*i* corresponds to a non-support vector)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B19849_Formula_0741.png) (*i* corresponds to a support vector)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'Then, ![](img/B19849_Formula_0751.png) can be found from the relation ![](img/B19849_Formula_0761.png),
    and the value of ![](img/B19849_Formula_077.png) can be determined, considering
    that for any ![](img/B19849_Formula_0781.png) and ![](img/B19849_Formula_0791.png),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_080.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, we can calculate w0 with the provided formula considering the provided conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can obtain the discriminant function, illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0812.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the summation is not carried out over all vectors, but only over the
    set *S*, which is the set of support vectors ![](img/B19849_Formula_0822.png).
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the described algorithm is realized only for linearly separable
    datasets, which, in itself, occurs rather infrequently. There are two approaches
    for working with linearly non-separable data.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of them is called a soft margin, which chooses a hyperplane that divides
    the training sample as purely (with minimal error) as possible, while at the same
    time maximizing the distance to the nearest point on the training dataset. For
    this, we have to introduce additional variables, ![](img/B19849_Formula_0831.png),
    which characterize the magnitude of the error on each object *x*i. Furthermore,
    we can introduce the penalty for the total error into the goal functional, like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_084.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B19849_Formula_0851.png) is a method tuning parameter that allows
    you to adjust the relationship between maximizing the width of the dividing strip
    and minimizing the total error. The value of the penalty ![](img/B19849_Formula_086.png)
    for the corresponding object *x*i depends on the location of the object *x*i relative
    to the dividing line. So, if *x*i lies on the opposite side of the discriminant
    function, then we can assume the value of the penalty ![](img/B19849_Formula_0871.png),
    if *x*i lies in the dividing strip, and comes from its class. The corresponding
    weight is, therefore, ![](img/B19849_Formula_0881.png). In the ideal case, we
    assume that ![](img/B19849_Formula_089.png). The resulting problem can then be
    rewritten as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_090.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that elements that are not an ideal case are involved in the minimization
    process too, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0911.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the constant *β* is the weight that takes into account the width of the
    strip. If *β* is small, then we can allow the algorithm to locate a relatively
    high number of elements in a non-ideal position (in the dividing strip). If *β*
    is vast, then we require the presence of a small number of elements in a non-ideal
    position (in the dividing strip). Unfortunately, the minimization problem is rather
    complicated due to the ![](img/B19849_Formula_0921.png) discontinuity. Instead,
    we can use the minimization of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_093.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This occurs under restrictions ![](img/B19849_Formula_0941.png), as illustrated
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_095.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Another idea of the SVM method in the case of the impossibility of a linear
    separation of classes is the transition to a space of higher dimension, in which
    such a separation is possible. While the original problem can be formulated in
    a finite-dimensional space, it often happens that the samples for discrimination
    are not linearly separable in this space. Therefore, it is suggested to map the
    original finite-dimensional space into a larger dimension space, which makes the
    separation much easier. To keep the computational load reasonable, the mappings
    used in support vector algorithms provide ease of calculating points in terms
    of variables in the original space, specifically in terms of the kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the function of the mapping ![](img/B19849_Formula_096.png) is selected
    to map the data of ![](img/B19849_Formula_0971.png) into a space of a higher dimension.
    Then, a non-linear discriminant function can be written in the form ![](img/B19849_Formula_0981.png).
    The idea of the method is to find the kernel function ![](img/B19849_Formula_0991.png)
    and maximize the objective function, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_100.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, to minimize computations, the direct mapping of data into a space of a
    higher dimension is not used. Instead, an approach called the kernel trick is
    used—that is, *K(x, y)*, which is a kernel matrix. The kernel trick is a method
    used in machine learning algorithms to transform non-linear data into a higher-dimensional
    space where it becomes linearly separable. This allows to use of linear algorithms
    to solve non-linear problems because they are often simpler and more computationally
    efficient (see [*Chapter 6*](B19849_06.xhtml#_idTextAnchor301) for the detailed
    explanation of the kernel trick).
  prefs: []
  type: TYPE_NORMAL
- en: In general, the more support vectors the method chooses, the better it generalizes.
    Any training example that does not constitute a support vector is correctly classified
    if it appears in the test set because the border between positive and negative
    examples is still in the same place. Therefore, the expected error rate of the
    support vector method is, as a rule, equal to the proportion of examples that
    are support vectors. As the number of measurements grows, this proportion also
    grows, so the method is not immune from the curse of dimensionality but it is
    more resistant to it than most algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth noting that the support vector method is sensitive to noise
    and data standardization.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the method of SVMs is not only limited to the classification task but
    can also be adapted for solving regression tasks. So, you can usually use the
    same SVM software implementation for solving classification and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: kNN method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The kNN is a popular classification method that is sometimes used in regression
    problems. It is one of the most natural approaches to classification. The essence
    of the method is to classify the current item by the most prevailing class of
    its neighbors. Formally, the basis of the method is the hypothesis of compactness:
    if the metric of the distance between the examples is clarified successfully,
    then similar examples are more likely to be in the same class. For example, if
    you don’t know what type of product to specify in the ad for a Bluetooth headset,
    you can find five similar headset ads. If four of them are categorized as *Accessories*
    and only one as *Hardware*, common sense will tell you that your ad should probably
    be in the *Accessories* category.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, to classify an object, you must perform the following operations
    sequentially:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the distance from the object to other objects in the training dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the *k* of training objects, with the minimal distance to the object
    that is classified.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the classifying object class to the class most often found among the nearest
    *k* neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we take the number of nearest neighbors *k = 1*, then the algorithm loses
    the ability to generalize (that is, to produce a correct result for data not previously
    encountered in the algorithm) because the new item is assigned to the closest
    class. If we set too high a value, then the algorithm may not reveal many local
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function for calculating the distance must meet the following rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_1012.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B19849_Formula_1021.png) only when *x = y*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_1031.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B19849_Formula_1041.png) in the case when the *x*, *y*, and *z* points
    don’t lie on one straight line'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this case, *x*, *y*, and *z* are feature vectors of compared objects. For
    ordered attribute values, Euclidean distance can be applied, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_1051.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, *n* is the number of attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For string variables that cannot be ordered, the difference function can be
    applied, which is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_1061.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When finding the distance, the importance of the attributes is sometimes taken
    into account. Usually, attribute relevance can be determined subjectively by an
    expert or analyst, and is based on their own experience, expertise, and problem
    interpretation. In this case, each *i*th square of the difference in the sum is
    multiplied by the coefficient *Z*i. For example, if the attribute *A* is three
    times more important than the attribute ![](img/B19849_Formula_1221.png) (![](img/B19849_Formula_1081.png),
    ![](img/B19849_Formula_109.png)), then the distance is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_110.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This technique is called **stretching the axes**, which reduces the classification
    error.
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice of class for the object of classification can also be different,
    and there are two main approaches to making this choice: *unweighted voting* and
    *weighted voting*.'
  prefs: []
  type: TYPE_NORMAL
- en: For unweighted voting, we determine how many objects have the right to vote
    in the classification task by specifying the *k* number. We identify such objects
    by their minimal distance to the new object. The individual distance to each object
    is no longer critical for voting. All have equal rights in a class definition.
    Each existing object votes for the class to which it belongs. We assign a class
    with the most votes to a new object. However, there may be a problem if several
    classes score an equal number of votes. Weighted voting removes this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the weighted vote, we also take into account the distance to the new
    object. The smaller the distance, the more significant the contribution of the
    vote. The votes for the class formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_111.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, ![](img/B19849_Formula_1122.png) is the square of the distance
    from the known object ![](img/B19849_Formula_1132.png) to the new object ![](img/B19849_Formula_1141.png),
    while ![](img/B19849_Formula_026.png) is the number of known objects of the class
    for which votes are calculated. `class` is the name of the class. The new object
    corresponds to the class with the most votes. In this case, the probability that
    several classes gain the same number of votes is much lower. When ![](img/B19849_Formula_116.png),
    the new object is assigned to the class of the nearest neighbor.
  prefs: []
  type: TYPE_NORMAL
- en: A notable feature of the kNN approach is its laziness. Laziness means that the
    calculations begin only at the moment of the classification. When using training
    samples with the kNN method, we don’t simply build the model but also do sample
    classification simultaneously. Note that the method of nearest neighbors is a
    well-studied approach (in machine learning, econometrics, and statistics, only
    linear regression is more well-known). For the method of nearest neighbors, there
    are quite a few crucial theorems that state that on *infinite* samples, kNN is
    the optimal classification method. The authors of the classic book *The Elements
    of Statistical Learning* consider kNN to be a theoretically ideal algorithm, the
    applicability of which is limited only by computational capabilities and the curse
    of dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'kNN is one of the simplest classification algorithms, so it is often ineffective
    in real-world tasks. The KNN algorithm has several disadvantages. Besides a low
    classification accuracy when we don’t have enough samples, the kNN classifier’s
    problem is the speed of classification: if there are *N* objects in the training
    set and the dimension of the space is *K*, then the number of operations for classifying
    a test sample can be estimated as ![](img/B19849_Formula_1171.png). The dataset
    used for the algorithm must be representative. The model cannot be *separated*
    from the data: to classify a new example, you need to use all the examples.'
  prefs: []
  type: TYPE_NORMAL
- en: The positive features include the fact that the algorithm is resistant to abnormal
    outliers since the probability of such a record falling into the number of kNN
    is small. If this happens, then the impact on the vote (uniquely weighted) with
    ![](img/B19849_Formula_1181.png) is also likely to be insignificant, and therefore,
    the impact on the classification result is also small. The program implementation
    of the algorithm is relatively simple, and the algorithm result is easily interpreted.
    Experts in applicable fields, therefore, understand the logic of the algorithm,
    based on finding similar objects. The ability to modify the algorithm by using
    the most appropriate combination of functions and metrics allows you to adjust
    the algorithm for a specific task.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the existing methods of multi-class classification are either based
    on binary classifiers or are reduced to them. The general idea of such an approach
    is to use a set of binary classifiers trained to separate different groups of
    objects from each other. With such a multi-class classification, various voting
    schemes for a set of binary classifiers are used.
  prefs: []
  type: TYPE_NORMAL
- en: In the **one-against-all** strategy for *N* classes, *N* classifiers are trained,
    each of which separates its class from all other classes. At the recognition stage,
    the unknown vector *X* is fed to all *N* classifiers. The membership of the vector
    *X* is determined by the classifier that gave the highest estimate. This approach
    can meet the problem of class imbalances when they arise. Even if the task of
    a multi-class classification is initially balanced (that is, it has the same number
    of training samples in each class), when training a binary classifier, the ratio
    of the number of samples in each binary problem increases with an increase in
    the number of classes, which, therefore significantly affects tasks with a notable
    number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: The **each-against-each** strategy allocates ![](img/B19849_Formula_1191.png)
    classifiers. These classifiers are trained to distinguish all possible pairs of
    classes of each other. For the input vector, each classifier gives an estimate
    of ![](img/B19849_Formula_120.png), reflecting membership in the classes ![](img/B19849_Formula_194.png)
    and ![](img/B19849_Formula_1222.png). The result is a class with a maximum sum
    ![](img/B19849_Formula_1232.png), where *g* is a monotonically non-decreasing
    function—for example, identical or logistic.
  prefs: []
  type: TYPE_NORMAL
- en: The **shooting tournament** strategy also involves training ![](img/B19849_Formula_1242.png)
    classifiers that distinguish all possible pairs of classes. Unlike the previous
    strategy, at the stage of classification of the vector *X*, we arrange a tournament
    between classes. We create a tournament tree, where each class has one opponent
    and only a winner can go to the next tournament stage. So, at each step, only
    one classifier determines the vector *X* class, then the *winning* class is used
    to determine the next classifier with the next pair of classes. The process is
    carried out until there is only one winning class left, which should be considered
    the result.
  prefs: []
  type: TYPE_NORMAL
- en: Some methods can produce multi-class classification immediately, without additional
    configuration and combinations. The kNN algorithms or neural networks can be considered
    examples of such methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, the logistic regression can be generalized for the multi-class case by
    using the softmax function. The softmax function is used to determine the probability
    that a sample belongs to a particular class, it looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_125.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *K* is the number of possible classes and the theta is a vector of learnable
    parameters. For the case when *K=2*, this expression reduces to the logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_126.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Replacing the vector difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_127.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'with a single parameter vector, we can see that the probability for the one
    class will be predicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_128.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the second class, it will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, these expressions are equal to the logistic regression we already
    saw.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have become familiar with some of the most widespread classification
    algorithms, let’s look at how to use them in different C++ libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of using C++ libraries for dealing with the classification task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s now see how to use the methods we’ve described for solving a classification
    task on artificial datasets, which we can see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Artificial datasets](img/B19849_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Artificial datasets
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, these datasets contain two and three different classes of objects,
    so it makes sense to use methods for multi-class classification because such tasks
    appear more often in real life; they can be easily reduced to binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: Classification is a supervised technique, so we usually have a training dataset,
    as well as new data for classification. To model this situation, we will use two
    datasets in our examples, one for training and one for testing. They come from
    the same distribution in one large dataset. However, the test set won’t be used
    for training; therefore, we can evaluate the accuracy metric and see how well
    models perform and generalize.
  prefs: []
  type: TYPE_NORMAL
- en: Using the mlpack library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we show how to use the `mlpack` library for solving the classification
    task. This library provides the implementation for the three main types of classification
    algorithms: logistic regression, softmax regression, and SVM.'
  prefs: []
  type: TYPE_NORMAL
- en: With softmax regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `mlpack` library implements multi-class logistic regression in the `SoftmaxRegression`
    class. Using this class is very simple. We have to initialize an object with the
    number of samples that will be used for training and the number of classes. Let’s
    say we have the following objects as training data and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can initialize an object of `SoftmaxRegression` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have the classifier object, we can train it and apply the classification
    function for some new data. The following code snippet shows how it can be done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Having the prediction vector we can visualize it with the technique we are
    using in this book that is based on the `plotcpp` library. Notice that the `Train`
    and `Classify` method requires the `size_t` type for the class labels. The following
    screenshot shows the results of applying the `mlpack` implementation of the softmax
    regression algorithm to our datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Softmax classification with mlpack](img/B19849_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Softmax classification with mlpack
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we have classification errors in the **Dataset 0**, **Dataset 1**,
    and **Dataset 2** datasets, and other datasets were classified almost correctly.
  prefs: []
  type: TYPE_NORMAL
- en: With SVMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `mlpack` library also has an implementation of the multi-class SVM algorithm
    in the `LinearSVM` class. The library provides mostly the same API for all classification
    algorithms so the initialization of the classifier object is mostly the same as
    in the previous example. The main difference is that you can use the constructor
    without parameters. So, the object initialization will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we train the classificator with the `Train` method and apply the `Classify`
    method for new data samples, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the results of applying the `mlpack` implementation
    of the SVM algorithm to our datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – SVM classification with mlpack](img/B19849_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – SVM classification with mlpack
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the results we got from the SVM method are pretty much the
    same as we got with the softmax regression.
  prefs: []
  type: TYPE_NORMAL
- en: With the linear regression algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `mlpack` library also implements the classic logistic regression algorithm
    in the `LogisticRegression` class. An object of this class can be applied to classify
    samples only into two classes. The usage API is the same as in the previous examples
    for the `mlpack` library. The typical application of this class will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the results of applying the two-class logistic
    regression to our datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Logistic regression classification with mlpack](img/B19849_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Logistic regression classification with mlpack
  prefs: []
  type: TYPE_NORMAL
- en: You can see that we only got reasonable classifications for **Dataset 3** and
    **Dataset 4** as they can be separated with a straight line. However, due to the
    two-class limitation, we were not able to get the correct results.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Dlib library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Dlib` library doesn’t have many classification algorithms. There are two
    that are most applicable: *KRR* and *SVM*. These methods are implemented as binary
    classifiers, but for multi-class classification, this library provides the `one_vs_one_trainer`
    class, which implements the voting strategy. Note that this class can use classifiers
    of different types so that you can combine the KRR and the SVM for one classification
    task. We can also specify which classifiers should be used for which distinct
    classes.'
  prefs: []
  type: TYPE_NORMAL
- en: With KRR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following code sample shows how to use the `Dlib` KRR algorithm implementation
    for the multi-class classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Firstly, we initialized the object of the `krr_trainer` class, and then we
    configured it with the instance of a kernel object. In this example, we used the
    `radial_basis_kernel` type for the kernel object, in order to deal with samples
    that can’t be linearly separated. After we obtained the binary classifier object,
    we initialized the instance of the `one_vs_one_trainer` class and added this classifier
    to its stack with the `set_trainer()` method. Then, we used the `train()` method
    for training our multi-class classifier. As with most of the algorithms in the
    `Dlib` library, this one assumes that the training samples and labels have the
    `std::vector` type, whereby each element has a `matrix` type. The `train()` method
    returns a decision function—namely, the object that behaves as a functor, which
    then takes a single sample and returns a classification label for it. This decision
    function is an object of the `one_vs_one_decision_function` type. The following
    piece of code demonstrates how we can use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: There is no explicit implementation for the accuracy metric in the `Dlib` library;
    so, in this example, accuracy is calculated directly as a ratio of correctly classified
    test samples against the total number of test samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the results of applying the `Dlib` implementation
    of the KRR algorithm to our datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – KRR classification with DLib](img/B19849_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – KRR classification with Dlib
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the KRR algorithm performed a correct classification on all datasets.
  prefs: []
  type: TYPE_NORMAL
- en: With SVM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following code sample shows how to use the `Dlib` SVM algorithm implementation
    for multi-class classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This sample shows that the `Dlib` library also has a unified API for using different
    algorithms, and the main difference from the previous example is the object of
    the binary classifier. For the SVM classification, we used an object of the `svm_nu_trainer`
    type, which was also configured with the kernel object of the `radial_basis_kernel`
    type.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the results of applying the `Dlib` implementation
    of the SVM algorithm to our datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – SVM classification with DLib](img/B19849_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – SVM classification with Dlib
  prefs: []
  type: TYPE_NORMAL
- en: You can see the `Dlib` implementation of the SVM algorithm also did correct
    classification on all datasets because of the `mlpack` implementation of the same
    algorithm made incorrect classification in some cases due to its linearity.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Flashlight library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Flashlight library doesn’t have any special classes for the classification
    algorithms. But using the linear algebra primitives and auto-gradient facilities
    of the library, we can implement the logistic regression algorithm from scratch.
    Also, to handle the datasets that are non-linearly separable, the kernel trick
    approach will be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: With logistic regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following example shows how to implement the two-class classification with
    the Flashlight library. Let’s define a function for training a linear classifier;
    it will have the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`train_x` and `train_y` are the training samples and their labels correspondingly.
    The function’s result is the learned parameters vector—in our case, defined with
    the `fl::Tensor` class. We are going to use the batched gradient descent algorithm
    to learn the parameters vector. So, we can use the Flashlight dataset types to
    simplify work with training data batches. The following code snippet shows how
    we can make a dataset object that will allow us to iterate over batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: At first, we defined the regular dataset object with the `fl::TensorDataset`
    type and then we used it to create the object of the `fl::BatchDataset` type that
    was initialized with the batch size value also.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to initialize the parameters vector that we will learn with the
    gradient descent, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we explicitly passed the `true` value as the last argument to enable
    the gradient calculation by the Flashlight autograd mechanism. Now, we are ready
    to define the training cycle with the predefined number of epochs. In each of
    the epochs, we will iterate over all batches in the dataset. So, such a cycle
    can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see two nested loops: the outer one for epochs and the inner one for
    batches. The `batch_dataset` object used in the `for` loop is compatible with
    C++ range based for loop construction, so it’s easily used to access the batches.
    Also, notice that we defined two variables, `x` and `y`, with the `fl::Variable`
    type as we did for weights. Usage of this type makes it possible to pass tensor
    values into the autograd mechanism. And for these variables, we didn’t configure
    the gradients calculation because they are not trainable parameters. Another important
    issue is that we used `fl::reshape` to make all tensor shapes compatible with
    the matrix multiplication that will be applied in the loss function calculation.
    The logistic regression loss function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_130.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the code, we can implement it with the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After we get the loss value, we can apply the gradient descent algorithm to
    correct the weights (parameters vector) according to the influence of the current
    training samples batch. The following code snippet shows how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the last step in the gradient zeroing was done to make it possible
    to learn something new from the next training sample and not mix gradients. At
    the end of the training cycle, the resulting parameters vector can be returned
    from the function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And the following sample shows how our training function can be used :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Having the learned parameter vector, we can use it to classify a new data sample
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You can see that we implemented the logistic function call that returns a result
    into the `p` variable. The value of this variable can be interpreted as the probability
    of the event that the sample belongs to a particular class. We introduced the
    `threshold` variable to check the probability. If it is greater than this threshold,
    then we classify the sample as it has a class of `1`; otherwise, it has a class
    of `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the results of applying Flashlight implementation
    of the logistic regression algorithm to our datasets with two classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Logistic regression classification with Flashlight](img/B19849_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Logistic regression classification with Flashlight
  prefs: []
  type: TYPE_NORMAL
- en: You can see that it fails to correctly classify **Dataset 0** and **Dataset
    1** with a non-linear class boundary but successfully classified **Dataset 4**
    as it is linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: With logistic regression and kernel trick
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To address the problem with non-linear class boundaries, we can apply the kernel
    trick. Let’s see how we can implement it in the Flashlight library with the Gaussian
    kernel. The idea is to move our data samples into higher dimension space where
    they can be linearly separable. The Gaussian kernel function looks like as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_1312.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To make our calculations more computationally effective, we can rewrite them
    in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_1322.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code sample shows this formula implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `make_kernel_matrix` function takes two matrices and applies the Gaussian
    kernel returning the single matrix. Let’s see how we can apply it to our problem.
    At first, we apply it to our training dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the function was called with the same `train_x` value for the two
    arguments. So, we moved our training dataset into the higher dimension space based
    on this training dataset. The gamma is a scaling hyperparameter that was configured
    manually in this example. Having this transformed dataset, we can train a classifier
    with the function that we created in the previous example as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to use these weights (parameters vector), we should apply the kernel
    to the new data samples in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that we used the reshaped new sample as the first argument and
    the training set tensor as the second argument. So, we transformed the new sample
    into the higher dimension space based on the original training data to preserve
    the same space properties. Then, we can apply the same classification procedure
    with a threshold as in the previous example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You can see that we just used the transformed weights tensor and transformed
    sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the results of applying the logistic regression
    with the kernel trick implementation to our two-class datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Logistic regression with kernel trick classification with Flashlight](img/B19849_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Logistic regression with kernel trick classification with Flashlight
  prefs: []
  type: TYPE_NORMAL
- en: You can see that with the kernel trick logistic regression successfully classified
    data with non-linear class boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed supervised machine learning approaches to solving
    classification tasks. These approaches use trained models to determine the class
    of an object according to its characteristics. We considered two methods of binary
    classification: logistic regression and SVMs. We looked at the approaches for
    the implementation of multi-class classification.'
  prefs: []
  type: TYPE_NORMAL
- en: We saw that working with non-linear data requires additional improvements in
    the algorithms and their tuning. Implementations of classification algorithms
    differ in terms of performance, as well as the amount of required memory and the
    amount of time required for learning. Therefore, the classification algorithm’s
    choice should be guided by a specific task and business requirements. Furthermore,
    their implementations in different libraries can produce different results, even
    for the same algorithm. Therefore, it makes sense to have several libraries for
    your software.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss recommender systems. We will see how they
    work, which algorithms exist for their implementation, and how to train and evaluate
    them. In the simplest sense, recommender systems are used to predict which objects
    (goods or services) are of interest to a user. Examples of such systems can be
    seen in many online stores such as Amazon or on streaming sites such as Netflix,
    which recommend new content based on your previous consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Logistic Regression—Detailed* *Overview*: [https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Understanding SVM algorithm from examples (along with code): [https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Understanding Support Vector Machines: A Primer: [https://appliedmachinelearning.wordpress.com/2017/03/09/understanding-support-vector-machines-a-primer/](https://appliedmachinelearning.wordpress.com/2017/03/09/understanding-support-vector-machines-a-primer/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Support Vector Machine: Kernel Trick; Mercer’s* *Theorem*: [https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d](https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SVMs with Kernel Trick (lecture): [https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf](https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Support Vector Machines—Kernels and the Kernel* *Trick*: [https://cogsys.uni-bamberg.de/teaching/ss06/hs_svm/slides/SVM_Seminarbericht_Hofmann.pdf](https://cogsys.uni-bamberg.de/teaching/ss06/hs_svm/slides/SVM_Seminarbericht_Hofmann.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Complete Guide to K-Nearest-Neighbors with Applications in Python and* *R*:
    [https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
