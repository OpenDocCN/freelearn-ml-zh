<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Evaluating Kernel Learning</h1>
                </header>
            
            <article>
                
<p>In machine learning, pattern finding is an area that is being explored to the hilt. There are many methods and algorithms that can drive this kind of work and analysis. However, in this chapter, we will try to focus on how kernels are making a significant difference to the whole machine learning outlook. The application of kernel learning doesn't have any boundaries: starting from a simple regression problem to a computer vision classification, it has made its presence felt everywhere. <strong>Support vector machine</strong> (<strong>SVM</strong>) is one of those algorithms that happens to make use of kernel learning.</p>
<p>In this chapter, we will be focusing on the following concepts:</p>
<ul>
<li>Concepts of vectors, linear separability, and hyperplanes</li>
<li>SVM</li>
<li>Kernel tricks</li>
<li>Gaussian process</li>
<li>Parameter optimization</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to vectors</h1>
                </header>
            
            <article>
                
<p>Before moving on to the core topic, <span><span>we</span></span> would like to build a foundation for getting there. Hence, this segment of the chapter is very important. It might look familiar to you and many of you will be cognizant about this. However, going through this channel will set the flow.</p>
<p><span>A vector is an object that has both a direction and magnitude. It is represented by an arrow and with a coordinate (<em>x</em>, <em>y</em>) in space, as shown in the following plot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/69d91204-36be-4e74-87ee-6f080307281d.png" style="width:24.42em;height:20.42em;"/></p>
<p>As shown in the preceding diagram, the vector OA has the coordinates <em>(4,3)</em>: </p>
<p class="CDPAlignCenter CDPAlign"><em>Vector OA= (4,3)</em></p>
<p>However, it is not sufficient to define a vector just by coordinates—we also need a direction. That means the direction from the <em>x</em> axis.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Magnitude of the vector</h1>
                </header>
            
            <article>
                
<p>The magnitude of the vector is also called the <strong>norm</strong>. It is represented by <em>||OA||</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b44d86d0-04f1-4c5d-abdb-4d64361bb275.png" style="width:25.58em;height:20.42em;"/></p>
<p>To find out magnitude of this vector, we can follow the Pythagorean theorem:</p>
<p class="CDPAlignCenter CDPAlign"><em>OA<sup>2 </sup>= OB<sup>2</sup> + AB<sup>2</sup></em></p>
<p class="CDPAlignCenter CDPAlign"><em>= 4<sup>2 </sup>+ 3<sup>2</sup> </em></p>
<p class="CDPAlignCenter CDPAlign"><em>= 16 + 9</em></p>
<p class="CDPAlignCenter CDPAlign"><em>= 25</em></p>
<p class="CDPAlignLeft CDPAlign">Hence:</p>
<p class="CDPAlignCenter CDPAlign"><em>OA = √25 = 5</em></p>
<p class="CDPAlignCenter CDPAlign"><em>||OA||= 5</em></p>
<p>So, if there is a vector <em>x = (x<sub>1,</sub>x<sub>2</sub>,....,x<sub>n</sub>)</em>:</p>
<p class="CDPAlignCenter CDPAlign"><em>||x||= x<sub>1</sub><sup>2 </sup>+ x<sub>2</sub><sup>2</sup>+........+x<sub>n</sub><sup>2</sup></em></p>
<p>And direction of this vector as:</p>
<p class="CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/416b0c98-2520-42ea-9f6f-72e6aff74934.png" style="width:13.92em;height:2.75em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dot product</h1>
                </header>
            
            <article>
                
<p>The dot product of two vectors returns a number that happens to be scalar. It is a representation of how two vectors are associated with each other.</p>
<p>Geometrically, the dot product of two vectors <em>x</em> and <em>y</em> would be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>x . y= ||x|| ||y|| cosθ</em></p>
<p><em>θ</em> is the angle between the vector <em>x</em> and <em>y</em>.</p>
<p>However, algebraically, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/92a6025d-116f-4815-a073-b003c343e241.png" style="width:36.75em;height:24.83em;"/></p>
<p>Geometrically, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><em>θ=β-α</em></p>
<p class="CDPAlignCenter CDPAlign"><em>cos<span>θ=cos(β-α)</span></em></p>
<p class="CDPAlignCenter CDPAlign"><em>cos<span>θ = cosβ cosα + sinβ sinα</span></em></p>
<p class="CDPAlignCenter CDPAlign"><em>cosθ = (x<sub>1</sub>/||x||) (y<sub>1</sub>/||y||) + (x<span>2</span>/||x||) (y<span>2</span>/||y||)</em></p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><em>||x||||y|| cosθ= x<sub>1</sub> y<sub>1 </sub>+ x<sub>2</sub>y<sub>2</sub></em></p>
<p class="CDPAlignCenter CDPAlign"><em>x . y = <span>x</span><sub>1</sub><span> y</span><sub>1 </sub><span>+ x</span><sub>2</sub><span>y</span><sub>2</sub></em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear separability</h1>
                </header>
            
            <article>
                
<p>Linear separability implies that if there are two classes then there will be a point, line, plane, or hyperplane that splits the input features in such a way that all points of one class are in one-half space and the second class is in the other half-space.</p>
<p>For example, here is a case of selling a house based on area and price. We have got a number of data points for that along with the class, which is house <strong>Sold</strong>/<strong>Not Sold</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-997 image-border" src="assets/c608dc6d-58f1-4548-b5d7-f3ba22fe1709.png" style="width:105.75em;height:39.17em;"/></p>
<p>In the preceding figure, all the <strong>N</strong>, are the class (event) of <strong>Not Sold</strong>, which has been derived based on the <strong>Price</strong> and <strong>Area</strong> of the house and all the instances of <strong>S</strong> represent the class of the house getting sold. The number of <strong>N</strong> and <strong>S</strong> represent the data points on which the class has been determined.</p>
<p>In the first diagram, <strong>N</strong> and <strong>S</strong> are quite close and happen to be more random, hence, it's difficult to have linear separability achieved as no matter how you try to separate two classes, at least one of them would be in the misclassified region. It implies that there won't be a correct possible line to separate the two. But the second diagram depicts datasets that can easily be separated based on given conditions.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Separation methodology changes based on the number of dimensions. If there is just one dimensional situation, we can have a point separating classes. Adding more dimensions will require a different arrangement to split the class. Once we have got a 2D situation, a line (as seen previously) will be required to separate it. Similarly, more than 2D will need a plane (a set of points) in order to separate the classes, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ffd8af10-e582-402b-8887-197f5abb2b79.png" style="width:35.92em;height:24.08em;"/></p>
<p class="mce-root">Separation method:</p>
<table border="1" style="border-collapse: collapse;width: 57.3045%">
<tbody>
<tr>
<td style="width: 10%"><strong>Number of dimensions</strong></td>
<td style="width: 10%"><strong>Separation method</strong></td>
</tr>
<tr>
<td style="width: 10%">1</td>
<td style="width: 10%">Point</td>
</tr>
<tr>
<td style="width: 10%">2</td>
<td style="width: 10%">Line</td>
</tr>
<tr>
<td style="width: 10%">3</td>
<td style="width: 10%">Plane</td>
</tr>
</tbody>
</table>
<p> </p>
<p>What if we have more than 3D? What do we do? What's the solution? Any guesses?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperplanes </h1>
                </header>
            
            <article>
                
<p>Many of you will have guessed it right. We use hyperplanes when it comes to more than 3D. We will define it using a bit of mathematics.</p>
<p>A linear equation looks like this: <em>y = ax + b</em> has got two variables, <em>x</em> and <em>y</em>, and a <em>y</em>-intercept, which is <em>b</em>. If we rename <em>y</em> as <em>x<sub>2</sub></em> and <em>x</em> as <em>x<sub>1</sub></em>, the equation comes out as <em>x<sub>2</sub>=ax<sub>1</sub> + b </em>which implies <em>ax<sub>1</sub> - x<sub>2</sub> + b=0</em>. If we define 2D vectors as <em>x= (x<sub>1</sub>,x<sub>2</sub>)</em> and <em>w=(a,-1)</em> and if we make use of the dot product, then the equation becomes <em>w.x + b = 0.</em> </p>
<div class="packt_infobox">Remember, <em>x.y = x<sub>1</sub>y<sub>1</sub> + x<sub>2</sub>y<sub>2</sub>.</em></div>
<p>So, a hyperplane is a set of points that satisfies the preceding equation. But how do we classify with the help of hyperplane?</p>
<p>We define a hypothesis function <em>h</em>:</p>
<p class="CDPAlignCenter CDPAlign"><em>h(x<sub>i</sub>) = +1 if w.x<sub>i</sub> + b ≥ 0</em></p>
<p class="CDPAlignCenter CDPAlign"><em>-1 if w.x<sub>i</sub> + b &lt; 0</em></p>
<p>This could be equivalent to the following:</p>
<p class="CDPAlignCenter CDPAlign"><em>h(x<sub>i</sub>)= sign(w.x<sub>i</sub> + b) </em></p>
<p class="CDPAlignLeft CDPAlign">It could also be equivalent to the following:</p>
<p class="CDPAlignCenter CDPAlign"><em>sign(w.x<sub>i</sub>) if (x<sub>0</sub>=1 and w<sub>0</sub>=b)</em></p>
<p>What it means is that it will use the position of <em>x</em> with respect to the hyperplane to predict a value for <em>y</em>. A data point on one side of the hyperplane gets a classification and a data point on other side of hyperplane gets another class.</p>
<p>Because it uses the equation of a hyperplane that happens to be the linear combination of the values, it is called a <strong>linear classifier</strong>. The shape of hyperplane is by <em>w</em> as it has elements as b and a responsible for the shape.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SVM</h1>
                </header>
            
            <article>
                
<p>Now we are ready to understand <span>SVMs</span>. SVM is an algorithm that enables us to make use of it for both classification and regression. Given a set of examples, it builds a model to assign a group of observations into one category and others into a second category. It is a non-probabilistic linear classifier. Training data being linearly separable is the key here. All the observations or training data are a representation of vectors that are mapped into a space and SVM tries to classify them by using a margin that has to be as wide as possible:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4d4cb88e-65ce-4483-81ea-5b640aa0c860.png" style="width:29.25em;height:25.58em;"/></p>
<p>Let's say there are two classes <strong>A</strong> and <strong>B</strong> as in the preceding screenshot.</p>
<p>And from the preceding section, we have learned the following:</p>
<p class="CDPAlignCenter CDPAlign"><em>g(x) = w. x + b</em></p>
<p>Where:</p>
<ul>
<li><em>w</em>: Weight vector that decides the orientation of the hyperplane</li>
<li><em>b</em>: Bias term that decides the position of the hyperplane in n-dimensional space by biasing it</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The preceding equation is also called a <strong>linear discriminant function</strong>. If there is a vector <em>x<sub>1</sub></em> that lies on the positive side of the hyperplane, the equation becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><em>g(x<sub>1</sub>)= w.x<sub>1</sub> +b &gt;0 </em></p>
<p>The equation will become the following:</p>
<p class="CDPAlignCenter CDPAlign"><em>g(x<sub>1</sub>)&lt;0</em></p>
<p class="CDPAlignLeft CDPAlign">If <em>x<sub>1</sub></em> lies on the positive side of the hyperplane.</p>
<p>What if <em>g(x<sub>1</sub>)=0</em>? Can you guess where <em>x<sub>1</sub></em> would be? Well, yes, it would be on the hyperplane, since our goal is to find out the class of the vector.</p>
<p>So, if <em>g(x<sub>1</sub>)&gt;0 =&gt; x<sub>1</sub></em> belongs to <strong>Class A</strong>, <em>g(x<sub>1</sub>)&lt;0 =&gt; x<sub>1</sub></em> belongs to <strong>Class B</strong>.</p>
<p>Here, it's evident that we can find out the classification by using the previous equation. But can you see the issue in it? Let's say the boundary line is like the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9adf98c4-d994-4397-89f9-a022df614a5d.png" style="width:28.25em;height:25.17em;"/></p>
<p>Even in the preceding scenario, we are able to classify those feature vectors here. But is it desirable? What can be seen here is that the boundary line or the classifier is close to the <strong>Class B</strong>. It implies that it brings in a large bias in the favor of <strong>Class A</strong> but penalizes <strong>Class B</strong>. As a result of that, due to any disturbances in the vectors close to the boundary, they might cross over and become part of <strong>Class A</strong>, which might not be correct. Hence, our goal is to find an optimal classifier that has got the widest margin, like what is shown in the following plot: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4d4cb88e-65ce-4483-81ea-5b640aa0c860.png" style="width:27.25em;height:23.83em;"/></p>
<p>Through SVM, we are attempting to create a boundary or hyperplane such that the distance from each of the feature vectors to the boundary is maximized so that any slight noise or disturbance won't cause the change in classification. So, in this scenario, if we try to bring in certain <em>y<sub>i</sub></em> which happens to be the class belonging to <em>xi,</em> we get the following: </p>
<p class="CDPAlignCenter CDPAlign"><em>y<sub>i</sub>= ± 1</em></p>
<p><em>y<sub>i</sub> (w.x<sub>i</sub> + b)</em> will always be greater than 0. <em>y<sub>i</sub>(w.x<sub>i</sub> + b) &gt;0</em> because when <em>x<sub>i </sub>∈ class A</em>, <em>w.x<sub>i</sub> +b&gt;0</em> then <em>y<sub>i</sub>&gt;0,</em> so the whole term will be positive. Also, if <em><span>x<sub>i </sub></span><span>∈ class B</span></em>, <em>w.x<sub>i</sub> + b&lt;0</em> then <em>y<sub>i</sub>&lt;0</em>, and it will make the term positive.</p>
<p>So, now if we have to redesign it, we say the following:</p>
<p><em>w.x<sub>i</sub> + b&gt; γ</em> where <span><em>γ</em> is the measure of the distance of hyperplane from <em>xi</em>.</span></p>
<p>And if there is a hyperplane <em>w.x + b = 0</em>, then the distance of point <em>x</em> from the preceding hyperplane is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em> <span>w.x + b/ ||w||</span></em></p>
<p>Hence, as mentioned previously:</p>
<p class="CDPAlignCenter CDPAlign"><em><span>w.x + b/ ||w|| ≥ γ</span></em></p>
<p class="CDPAlignCenter CDPAlign"><em>w.x + b<span> </span><span>≥ γ.||w||</span></em></p>
<p>On performing proper scaling, we can say the following:</p>
<p class="CDPAlignCenter CDPAlign"><em><span>w.x + b </span><span>≥ 1 (since γ.||w|| = 1)</span></em></p>
<p>It implies that if there is a classification to be arrived at based on the previous result, it follows this:</p>
<p class="CDPAlignCenter CDPAlign"><em><span>w.x + b </span><span>≥ 1 if x ∈ class A and</span></em></p>
<p class="CDPAlignCenter CDPAlign"><em>w.x + b ≤ -1 if<span> x </span><span>∈ class B</span></em></p>
<p>And now, again, if we bring in a class belonging to <em>y<sub>i</sub></em> here, the equation becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><em>yi (w.xi + b) <span>≥ 1</span></em></p>
<p>But, if <em><span>y<sub>i</sub> (w.x<sub>i</sub> + b) =</span></em><span><em> 1</em>, <em>x<sub>i</sub></em> is a support vector. Next, we will learn what a support vector is.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support vector</h1>
                </header>
            
            <article>
                
<p><span>We draw two boundary lines passing through feature vectors of one class closest to the feature vectors of another class. The center line of these boundary lines is the hyperplane we have been talking about. For example, for <strong>Class B</strong>, a boundary line is passing through <strong>p</strong> and <strong>q</strong> along the way and another boundary line through <strong>r</strong> and <strong>s</strong> because <strong>p</strong> and <strong>q</strong> are the closest to the feature vectors of <strong>Class B</strong> and so are <strong>r</strong> and <strong>s</strong>. These are called <strong>support vectors</strong>. We will understand now why these are called <strong>support vectors</strong>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/480ad417-ed82-49ab-ae3f-fa33bcc91538.png" style="width:21.00em;height:18.67em;"/></p>
<p>Let's say that if we try to remove one of the feature vectors that is not so close to the boundary line, we will not have an impact on the position or orientation of the hyperplane <span>because the hyperplane's position is decided by boundary lines crossing through vectors <strong>p</strong>, <strong>q</strong>, <strong>r</strong>, and <strong>s</strong></span>. And, since these are the points holding (supporting) the hyperplane together, they have been named support vectors.</p>
<p><span>So, this equation <em>y<sub>i</sub> (w.x<sub>i</sub> + b) =</em></span><span><em> 1</em> holds true when <em>x<sub>i</sub></em> is <strong>p</strong>, <strong>q</strong>, <strong>r</strong>, or <strong>s</strong>.</span></p>
<p>We will go back to the equation <span><em>w.x + b/ ||w|| ≥ γ</em>; here, we are trying to maximize <em>γ</em>, and in order to do so either we need to maximize b or minimize <em>||w||</em>.</span></p>
<p>Or we can say we have to minimize <em>w.w</em>. If we convert that into a function, <em>Φ(w) = w.w</em> has to be minimized. <span><em>Φ(w) =1/2( w.w)</em> (here 1/2 has been added for mathematical convenience).</span></p>
<p>So, the objective function of SVM becomes <span><em>Φ(w) =1/2( w.w)</em>, which has to be minimized </span>subject to constraints, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em><span>y<sub>i</sub> (w.x<sub>i</sub> + b) =</span><span> 1</span></em></p>
<p>Since it is a constrained optimization problem, it can be converted into an unconstrained optimization problem using the Lagrangian multiplier.</p>
<p>Hence, <em>L(w,b)= 1/2(w.w) - ∑ αi [yi(w.xi+b) - 1]</em> where <span>αi is the Lagrangian multiplier, </span><em>L(w,b)= <span>1/2(w.w) - </span><span>∑ </span><span>α<sub>i</sub> y<sub>i</sub> (w.x<sub>i</sub>) -∑ α<sub>i</sub> y<sub>i</sub> b + ∑ α<sub>i</sub></span></em>.</p>
<p>Let's find out <em>w</em> and <em>b</em> by using maxima and minima calculus:</p>
<p class="CDPAlignCenter CDPAlign"><em>δL/<span>δb = 0 </span></em></p>
<p>It results in<em> <span>∑ </span><span>α<sub>i</sub> y<sub>i</sub>=0, </span></em><em><span>δL/</span></em><span><em>δw = 0</em> would result in <em>∑ αi yi xi = w</em></span>.<span><em> </em></span>Now, putting these results back into the Lagrangian function yields the following:</p>
<p class="CDPAlignCenter CDPAlign"><em>L= <span>∑ </span><span>α<sub>i</sub> - 1/2 ∑ α<sub>i</sub> α<sub>j</sub> y<sub>i</sub> y<sub>j</sub> (x<sub>j</sub>.x<sub>i</sub>)</span></em></p>
<p>It means that if the value of <span><em>α<sub>i</sub></em> is very high then the corresponding <em>x</em></span>.<span><em> </em></span><span>There will be a lot of influence on the position of the hyperplane. Hence, for classification and for unknown feature vector <em>z</em>, the required equation would be the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><em>D(z) = Sign(<span> ∑ αi xi yi z + b)</span></em></p>
<p>If <em>D(z) &gt;0</em>, then <em>z</em> would belong to class A and if <em>D(z)&lt;0</em>, <em>z ∈ class B</em>. Let's try to perform a case study in Python:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/44b032dc-d35e-4103-8986-55a7e8d79b98.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kernel trick</h1>
                </header>
            
            <article>
                
<p>We have already seen that SVM works smoothly when it comes to having linear separable data. Just have a look at the following figure; it depicts that vectors are not linearly separable, but the noticeable part is that it is not being separable in 2D space:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c938db90-3030-49a0-bcbb-76090683fa47.png" style="width:23.33em;height:22.75em;"/></p>
<p>With a few adjustments, we can still make use of SVM here.</p>
<p>Transformation of a two-dimensional vector into a 3D vector or any other higher dimensional vector can set things right for us. The next step would be to train the SVM using a higher dimensional vector. But the question arises of how high in dimension we should go to transform the vector. What this means is if the transformation has to be a two-dimensional vector, or 3D or 4D or more. It actually depends on the which brings separability into the dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kernel</h1>
                </header>
            
            <article>
                
<p>A non-separable dataset like the one used previously is always a tough thing to deal with, however, there are ways to deal with it. One way is to set the vectors into higher dimensions through transformation. But, can we really do it when we have millions of data or vector in reckoning? It will take lots of computation and, also, time. That's where kernel to saves our day.</p>
<p>We have seen the following equation. In this, only the dot product of the training examples are responsible for making the model learn. Let's try to do a small exercise here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/81286c92-7e64-42d9-931f-29a4232387d9.png" style="width:22.92em;height:3.92em;"/></p>
<p>Let's take two vectors here:</p>
<pre>x1=[4,8] <br/>x2= [20,30]</pre>
<p>Now, build a transformation function that will help in transforming these 2D vectors into 3D.</p>
<p>The function to be used in order to transform is the following: </p>
<p class="CDPAlignCenter CDPAlign"><em>t(x1,x2)= (x1<sup>2</sup>,x1 x2 √2,x2<sup>2</sup>)</em></p>
<pre>#transformation from 2-D to 3-D vector<br/>def t(x): <br/>    return [x[0]**2, np.sqrt(2)*x[0]*x[1], x[1]**2]</pre>
<p>Now let's use this function:</p>
<pre>x1_3D= t(x1) <br/>x2_3D= t(x2)<br/><br/>print(np.dot(x1_3D,x2_3D))# the result is 102400</pre>
<p>But can't we do this without transforming the values. Kernel can help us in doing it:</p>
<pre>def kernel(a, b): <br/>    return a[0]**2 * b[0]**2 + 2*a[0]*b[0]*a[1]*b[1] + a[1]**2 * b[1]**2</pre>
<p>It's the time to use this <kbd>kernel</kbd> now:</p>
<pre>kernel(x1,x2) #the result is 102400</pre>
<p>Isn't it quite thrilling to see such an amazing result that is the same as before, without using transformation? So, kernel is a function that leads to the dot-product-like result in another space.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Back to Kernel trick</h1>
                </header>
            
            <article>
                
<p>So, now we have got a fair understanding of kernel and its importance. And, as discussed in the last section, the <kbd>kernel</kbd> function is:</p>
<p class="CDPAlignCenter CDPAlign"><em>K(x<sub>i</sub>,x<sub>j</sub>)= <span>x</span><sub>i </sub>. <span>x</span><sub>j</sub></em></p>
<p class="CDPAlignLeft CDPAlign">So, now the margin problem becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/81b41737-cd77-4e76-8316-b22acd9dc135.png" style="width:25.17em;height:3.25em;"/></p>
<p>This is subject to <em>0 ≤ α<sub>i</sub> </em><span><em>≤ C</em>, for any <em>i = 1, ..., m</em>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/46d3bf1e-49a9-4c32-8561-e063baab823b.png" style="width:6.67em;height:3.67em;"/></p>
<div class="packt_tip">Applying the kernel trick simply means replacing the dot product of two examples with a <kbd>kernel</kbd> function.</div>
<p>Now even the hypothesis function will change as well:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d1477ee3-964a-4d74-b35a-460418052ad5.png" style="width:16.83em;height:3.58em;"/></p>
<p>This function will be able to decide on and classify the categories. Also, since <em>S</em> denotes the set of support vectors, it implies that we need to compute the <kbd>kernel</kbd> function only on support vectors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kernel types</h1>
                </header>
            
            <article>
                
<p>We're going to explain the types of in this section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear kernel</h1>
                </header>
            
            <article>
                
<p>Let's say there are two vectors, <em>x</em><sub><em>1</em> </sub>and <em>x<sub>2</sub></em>, so the linear kernel can be defined by the following:</p>
<p class="CDPAlignCenter CDPAlign"><em>K(<span>x</span><sub>1,</sub> <span>x</span><sub>2</sub>)= <span>x</span><sub>1 . </sub><span>x</span><sub>2</sub></em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Polynomial kernel</h1>
                </header>
            
            <article>
                
<p><span>If there are two vectors, <em>x</em></span><em><sub>1</sub></em><span><em> </em>and <em>x</em></span><em><sub>2</sub></em><span>, the linear kernel can be defined by the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><em><span>K(</span><span>x</span><sub>1,</sub><span> </span><span>x</span><sub>2</sub><span>)= (</span><span>x</span><sub>1 . </sub><span>x</span><sub>2 </sub>+ c)<sup>d</sup></em></p>
<p>Where:</p>
<ul>
<li><em>c</em>: Constant</li>
<li><em>d</em>: Degree of polynomial:</li>
</ul>
<pre style="padding-left: 60px">def polynomial_kernel(x1, x2, degree, constant=0): <br/>    result = sum([x1[i] * x2[i] for i in range(len(x1))]) + constant <br/>    return pow(result, degree)</pre>
<p>If we use the same <kbd>x1</kbd> and <kbd>x2</kbd> as used previously, we get the following:</p>
<pre>x1= [4,8]<br/>x2=[20,30] <br/>polynomial_kernel(x1,x2,2,0)<br/># result would be 102400</pre>
<p>If we increase the degree of polynomial, we will try to get influenced by other vectors as the decision boundary becomes too complex and it will result in overfitting:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-998 image-border" src="assets/14b37f8d-a6d2-4f1e-8d95-e64b604952e2.png" style="width:38.17em;height:22.33em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-999 image-border" src="assets/4b52d85d-9734-4bf2-9432-c3ccccadc31c.png" style="width:23.08em;height:22.75em;"/></p>
<p>Polynomial kernel using degree as 6.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gaussian kernel</h1>
                </header>
            
            <article>
                
<p>The polynomial kernel has given us a good boundary line. But can we work with polynomial kernels all the time? Not in the following scenario:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1000 image-border" src="assets/d5af809b-ac44-4604-9bfa-888ecf1ea487.png" style="width:18.83em;height:16.75em;"/></p>
<p>The solution is a radial basis function or Gaussian kernel. It's nothing but the similarity function of the vectors to translate them into a high dimensional space or infinite dimensional space. Its value depends on the distance from the Gaussian kernel function, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>K(x,x<sup>'</sup>) = exp(-γ ||x-x'||<sup>2</sup>)</em></p>
<p> </p>
<p>Without loss of generality, let <img class="fm-editor-equation" src="assets/610290f7-d9ab-43ca-8211-b8623507ae1f.png" style="width:3.08em;height:1.67em;"/>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5a59facc-78d6-4edb-bbb9-2d3bd597f660.png" style="width:50.50em;height:14.92em;"/><br/>
<img class="fm-editor-equation" src="assets/e89bba32-a989-40f4-8517-c3dce9f75b45.png" style="width:26.42em;height:1.75em;"/></p>
<p>With the help of this RBF as a similarity function, all the feature vectors get calculated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SVM example and parameter optimization through grid search</h1>
                </header>
            
            <article>
                
<p>Here, we are taking a breast cancer dataset wherein we have classified according to whether the cancer is benign/malignant.</p>
<p>The following is for importing all the required libraries:</p>
<pre>import pandas as pd<br/>import numpy as np<br/>from sklearn import svm, datasets<br/>from sklearn.svm import SVC<br/>import matplotlib.pyplot as plt<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.metrics import classification_report<br/>from sklearn.utils import shuffle<br/>%matplotlib inline</pre>
<p>Now, let's load the breast cancer dataset:</p>
<pre>BC_Data = datasets.load_breast_cancer()</pre>
<p>The following allows us to check the details of the dataset:</p>
<pre>print(BC_Data.DESCR)</pre>
<p>This if for splitting the dataset into train and test:</p>
<pre>X_train, X_test, y_train, y_test = train_test_split(BC_Data.data, BC_Data.target, random_state=0)</pre>
<p>This is for setting the model with the linear kernel and finding out the accuracy:</p>
<pre>C= 1.0<br/>svm= SVC(kernel="linear",C=C)<br/>svm.fit(X_train, y_train)<br/>print('Accuracy-train dataset: {:.3f}'.format(svm.score(X_train,y_train)))<br/>print('Accuracy- test dataset: {:.3f}'.format(svm.score(X_test,y_test)))</pre>
<p>We get the accuracy output as shown:</p>
<pre><span>Accuracy-train dataset: 0.967<br/><br/></span>Accuracy- test dataset: 0.958</pre>
<p><span>Setting the model with the Gaussian/RBF kernel and accuracy is done like this:</span></p>
<pre>svm= SVC(kernel="rbf",C=C)<br/>svm.fit(X_train, y_train)<br/>print('Accuracy-train dataset: {:.3f}'.format(svm.score(X_train,y_train)))<br/>print('Accuracy- test dataset: {:.3f}'.format(svm.score(X_test,y_test)))</pre>
<p>The output can be seen as follows:</p>
<pre>Accuracy-train dataset: 1.000<br/><br/>Accuracy- test dataset: 0.629</pre>
<p>It's quite apparent that the model is overfitted. So, we will go for normalization:</p>
<pre>min_train = X_train.min(axis=0)<br/>range_train = (X_train - min_train).max(axis=0)<br/>X_train_scaled = (X_train - min_train)/range_train<br/>X_test_scaled = (X_test - min_train)/range_train</pre>
<p>This code is for setting up the model again:</p>
<pre>svm= SVC(kernel="rbf",C=C)<br/>svm.fit(X_train_scaled, y_train)<br/>print('Accuracy-train dataset: {:.3f}'.format(svm.score(X_train_scaled,y_train)))<br/>print('Accuracy test dataset: {:.3f}'.format(svm.score(X_test_scaled,y_test)))</pre>
<p>The following shows the output:</p>
<pre>Accuracy-train dataset: 0.948<br/><br/>Accuracy test dataset: 0.951</pre>
<p>Now, the overfitting issue cannot be seen any more. Let's move on to having an optimal result:</p>
<pre>parameters = [{'kernel': ['rbf'],<br/> 'gamma': [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5],<br/> 'C': [1, 10, 100, 1000]},<br/> {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]<br/>clf = GridSearchCV(SVC(decision_function_shape='ovr'), parameters, cv=5)<br/>clf.fit(X_train, y_train)<br/>print("Best parameters set found on development set:")<br/>print()<br/>print(clf.best_params_)<br/>print()<br/>print("Grid scores on training set:")<br/>print()<br/>means = clf.cv_results_['mean_test_score']<br/>stds = clf.cv_results_['std_test_score']<br/>for mean, std, params in zip(means, stds, clf.cv_results_['params']):<br/> print("%0.3f (+/-%0.03f) for %r"<br/> % (mean, std * 2, params))<br/>print()</pre>
<p><span><span>With the help of grid search, we get the optimal combination for <kbd>gamma</kbd>, <kbd>kernel</kbd>, and <kbd>C</kbd> as shown:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a0614335-6cba-4c29-9033-97af3f63bb68.png" style="width:36.25em;height:37.00em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>With the help of this, we can see and find out which combination of parameters is giving us the better result.</p>
<p>Here, the best combination turns out to be a linear kernel with a <kbd>C</kbd> value of <kbd>1</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we were introduced to vectors, magnitude of vector, and the dot product. We learned about SVMs that<span> can be used for both classification and regression. We studied support vectors and kernels and the different types of kernels. Lastly, we studied the SVM example and parameter optimization through grid search.</span></p>
<p>In the next chapter, we will learn about <span>performance in ensemble learning.</span></p>


            </article>

            
        </section>
    </body></html>