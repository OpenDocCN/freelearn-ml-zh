- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Understanding AutoML Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解AutoML算法
- en: All ML algorithms have a foundation in **computational statistics**. Computational
    statistics is the combination of statistics and computer science where computers
    are used to compute complex mathematics. This computation is the ML algorithm
    and the results that we get from it are the predictions. As engineers and scientists
    working in the field of ML, we are often expected to know the basic logic of ML
    algorithms. There are plenty of ML algorithms in the AI domain. All of them aim
    to solve different types of prediction problems. All of them also have their own
    set of pros and cons. Thus, it became the job of engineers and scientists to find
    the best ML algorithms that can solve a given prediction problem within the required
    constraints. This job, however, was eased with the invention of AutoML.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 所有机器学习算法都以**计算统计学**为基础。计算统计学是统计学与计算机科学的结合，其中计算机用于计算复杂的数学。这种计算就是机器学习算法，而我们从中得到的预测结果。作为机器学习领域的工程师和科学家，我们通常需要了解机器学习算法的基本逻辑。人工智能领域有大量的机器学习算法。它们都旨在解决不同类型的预测问题。它们各自也有自己的优缺点。因此，工程师和科学家的工作就是找到能够解决给定预测问题且在所需约束条件下表现最佳的机器学习算法。然而，随着AutoML的发明，这项工作变得容易多了。
- en: Despite AutoML taking over this huge responsibility of finding the best ML algorithm,
    it is still our job as engineers and scientists to verify and justify the selection
    of these algorithms. And to do that, a basic understanding of the ML algorithms
    is a must.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管AutoML承担了寻找最佳机器学习算法的巨大责任，但作为工程师和科学家，我们仍然需要验证和证明这些算法的选择。为此，对机器学习算法的基本理解是必不可少的。
- en: In this chapter, we shall explore and understand the various ML algorithms that
    H2O AutoML uses to train models. As mentioned previously, all ML algorithms have
    a heavy foundation in statistics. Statistics itself is a huge branch of mathematics
    and is too large to cover in a single chapter. Hence, for the sake of having a
    basic understanding of how ML algorithms work, we shall explore their inner workings
    conceptually with basic statistics rather than diving deep into the math.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨和理解H2O AutoML用于训练模型的各种机器学习算法。如前所述，所有机器学习算法都有深厚的统计学基础。统计学本身是数学的一个庞大分支，内容太多，无法在一章中涵盖。因此，为了对机器学习算法的工作原理有一个基本的理解，我们将从基本统计学概念出发，从概念上探讨它们的内部运作，而不是深入数学。
- en: Tip
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If you are interested in gaining more knowledge in the field of statistics,
    then the following link should be a good place to start: [https://online.stanford.edu/courses/xfds110-introduction-statistics](https://online.stanford.edu/courses/xfds110-introduction-statistics).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要在统计学领域获得更多知识，那么以下链接应该是一个不错的起点：[https://online.stanford.edu/courses/xfds110-introduction-statistics](https://online.stanford.edu/courses/xfds110-introduction-statistics)。
- en: First, we shall understand what the different types of ML algorithms are and
    then learn about the workings of these algorithms. We will do this by breaking
    them down into individual concepts, understanding them, and then building the
    algorithm back up to understand the big picture.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将了解不同类型的机器学习算法，然后学习这些算法的工作原理。我们将通过将它们分解为单个概念，理解它们，然后再将这些概念组合起来以理解整体来做到这一点。
- en: 'In this chapter, we are going to cover the following topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding the different types of ML algorithms
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解不同类型的机器学习算法
- en: Understanding the Generalized Linear Model algorithm
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解广义线性模型算法
- en: Understanding the Distributed Random Forest algorithm
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分布式随机森林算法
- en: Understanding the Gradient Boosting Machine algorithm
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解梯度提升机算法
- en: Understanding what is Deep Learning
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解深度学习是什么
- en: So, let’s begin our journey by understanding the different types of ML algorithms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们从理解不同类型的机器学习算法开始我们的旅程。
- en: Understanding the different types of ML algorithms
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解不同类型的机器学习算法
- en: ML algorithms are designed to solve a specific prediction problem. These prediction
    problems can be anything that can provide value if predicted accurately. The differentiating
    factor between various prediction problems is what value is to be predicted. Is
    it a simple yes or no value, a range of numbers, or a specific value from a list
    of potential values, probabilities, or semantics of a text? The field of ML is
    vast enough to cover the majority, if not all, of such problems in a wide variety
    of ways.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法旨在解决特定的预测问题。这些预测问题可以是任何在预测准确的情况下能提供价值的问题。不同预测问题之间的区别在于要预测的价值是什么。是简单的“是”或“否”值，一个数值范围，还是从潜在值列表中选择的一个特定值，或者是文本的概率或语义？机器学习领域足够广泛，可以以多种方式涵盖大多数，如果不是所有这样的问题。
- en: 'So, let’s start with understanding the different categories of prediction problems.
    They are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们从了解预测问题的不同类别开始。它们如下：
- en: '**Regression**: Regression analysis is a statistical process that aims to find
    the relationship between independent variables, also called features, and dependent
    variables, also called label or response variables, and use that relationship
    to predict future values.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：回归分析是一种统计过程，旨在找到独立变量（也称为特征）和因变量（也称为标签或响应变量）之间的关系，并使用这种关系来预测未来的值。'
- en: Regression problems are problems that aim to predict certain continuous numerical
    values – for example, predicting the price of a car given the car’s brand name,
    engine size, economy, and electronic features. In such a scenario, the car’s brand
    name, engine size, economy, and electronic features are the independent variables
    as their presence is independent of other values, while the car price is the dependent
    variable whose value is dependent on the other features. Also, the price of the
    car is a continuous value as it can numerically range anywhere from 0 to 100 million
    in dollars or any other currency.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题是指旨在预测某些连续数值的问题——例如，根据汽车的品牌名称、发动机大小、经济性和电子特性来预测汽车的价格。在这种情况下，汽车的品牌名称、发动机大小、经济性和电子特性是独立变量，因为它们的存在与其他值无关，而汽车价格是因变量，其值取决于其他特性。此外，汽车的价格是一个连续值，因为它可以在0到1亿美元或任何其他货币的数值范围内。
- en: '**Classification**: Classification is a statistical process that aims to categorize
    the label values depending on their relationship to the features into certain
    classes or categories.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：分类是一种统计过程，旨在根据标签值与特征之间的关系将它们分类到某些类别或类别中。'
- en: Classification problems are problems that aim to predict a certain set of values
    – for example, predicting if a person is likely to face heart disease, depending
    on their cholesterol level, weight, exercise levels, heart rate, and family history.
    Another example would be predicting the rating of a restaurant on Google reviews
    that ranges from 1-5 stars, depending on the location, food, ambience, and price.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题是指旨在预测一组特定值的问题——例如，根据一个人的胆固醇水平、体重、锻炼水平、心率和家庭病史来预测这个人是否可能面临心脏病。另一个例子是预测餐厅在谷歌评论上的评分，评分范围从1到5星，这取决于位置、食物、环境和价格。
- en: 'As you can see from these examples, classification problems can be either a
    *yes* or *no*, *true* or *false*, or *1* or *0* type of classification, or a specific
    set of classification values, such as those in the Google Review example, where
    the values can either be 1, 2, 3, 4, or 5\. Thus, classification problems can
    be further divided, as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这些例子中看到的，分类问题可以是“是”或“否”，“真”或“假”，“1”或“0”类型的分类，或者是一组特定的分类值，例如在谷歌评论示例中，值可以是1、2、3、4或5。因此，分类问题可以进一步分为以下几类：
- en: '**Binary Classification**: In this type of classification problem, the predicted
    values are binary, meaning they have only two values – that is, *yes* or *no*,
    *true* or *false*, *1* or *0*.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元分类**：在这种分类问题中，预测值是二元的，这意味着它们只有两个值——也就是说，“是”或“否”，“真”或“假”，“1”或“0”。'
- en: '**Multiclass/Polynomial Classification**: In this type of classification problem,
    the predicted values are non-binary, also called polynomial, in nature, meaning
    they have more than two sets of values. For example, classification by age, which
    involves whole numbers from 1 to 100, or classification by primary colors, which
    can be red, yellow, or blue.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类/多项式分类**：在这种分类问题中，预测值是非二元的，也称为多项式，这意味着它们有多于两组的值。例如，按年龄分类，涉及从1到100的整数，或者按基本颜色分类，可以是红色、黄色或蓝色。'
- en: '**Clustering**: Clustering is a statistical process that aims to group or divide
    certain data points in such a way that data points in a single group have similar
    characteristics that are different from data points in other groups.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：聚类是一种统计过程，旨在以某种方式将某些数据点分组或划分，使得单个组内的数据点具有与其他组数据点不同的相似特征。'
- en: Clustering problems are problems that aim to understand similarities within
    a set of values.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类问题**：聚类问题是旨在理解一组值内部相似性的问题。'
- en: 'For example, given a set of people who play video games with certain details,
    such as the hardware they use, the different games they play, and time spent playing
    those video games, you can categorize people by their favorite game genre. Clustering
    can be further divided as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定一组具有某些细节（如他们使用的硬件、他们玩的不同游戏以及他们玩这些视频游戏的时间）玩视频游戏的人，你可以根据他们最喜欢的游戏类型对人们进行分类。聚类可以进一步细分为以下几种：
- en: '**Hard Clustering**: In this type of clustering, all data points either belong
    to one or another cluster; they are mutually exclusive.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬聚类**：在这种类型的聚类中，所有数据点要么属于一个簇，要么属于另一个簇；它们是互斥的。'
- en: '**Soft Clustering**: In this type of clustering, rather than assigning a data
    point to a cluster, the probability that a data point might belong to a certain
    cluster is calculated. This opens the likelihood that a data point might belong
    to multiple clusters at the same time.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软聚类**：在这种类型的聚类中，不是将数据点分配给一个簇，而是计算数据点可能属于某个簇的概率。这增加了数据点可能同时属于多个簇的可能性。'
- en: '**Association**: Association is a statistical process that aims to find the
    probability that if event A happened, what is the likelihood that event B will
    happen too? Association problems are based on association rules, which are if-then
    statements that show the probability of a relationship between different data
    points.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关联**：关联是一种统计过程，旨在找出如果事件A发生了，事件B发生的可能性有多大？关联问题基于关联规则，这些规则是如果-那么陈述，显示了不同数据点之间关系的概率。'
- en: The most common example of the association problem is **Market Basket Analysis**.
    Market Basket Analysis is a prediction problem where, given a user buys a certain
    product A from the market, what is the probability of the user buying product
    B, which is related to product A?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 关联问题的最常见例子是**市场篮子分析**。市场篮子分析是一个预测问题，给定一个用户在市场上购买了一定产品A，那么用户购买与产品A相关的产品B的概率是多少？
- en: '**Optimization/Control**: **Control Theory**, **Optimal Control Theory**, or
    **Optimization Problems** is a branch of mathematics that deals with finding a
    certain combination of values that collectively optimize a dynamic system. **Machine
    Learning Control** (**MLC**) is a subfield in ML that aims to solve the Optimization
    Problem using ML. A good example of MLC is the implementation of ML to optimize
    traffic on roads using automated cars.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化/控制**：**控制理论**、**最优控制理论**或**优化问题**是数学的一个分支，它处理寻找一组值的组合，以优化动态系统。**机器学习控制**（**MLC**）是机器学习中的一个子领域，旨在使用机器学习来解决优化问题。MLC的一个好例子是将机器学习应用于优化道路上的交通，使用自动汽车。'
- en: 'Now that we understand the different types of prediction problems, let’s dive
    into understanding the different types of ML algorithms. The different types of
    ML algorithms are categorized as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了不同类型的预测问题，让我们深入了解不同类型的机器学习算法。不同类型的机器学习算法被分类如下：
- en: '**Supervised Learning**: Supervised learning is the ML task of mapping the
    relationship between the independent variables and dependent variables based on
    previously existing values that are labeled. Labeled data is data that contains
    information about which of its features are dependent and which features are independent.
    In supervised learning, we know which feature we want to predict and tag that
    feature as a label. The ML algorithm will use this information to map the relationships.
    Using this mapping, we predict the output for new input values. Another way of
    understanding this problem is that the previously existing values supervise the
    ML algorithm''s learning task.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：监督学习是根据先前存在的、已标记的值来映射自变量和因变量之间关系的机器学习任务。标记数据是包含有关其特征哪些是依赖的、哪些是独立的的信息的数据。在监督学习中，我们知道我们想要预测的特征，并将该特征标记为标签。机器学习算法将使用这些信息来映射关系。使用这种映射，我们预测新输入值的输出。另一种理解这个问题的方式是，先前存在的值监督机器学习算法的学习任务。'
- en: Supervised learning algorithms are often used to solve regression and classification
    problems.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法通常用于解决回归和分类问题。
- en: Some examples of supervised learning algorithms are **decision trees**, **linear
    regression**, and **neural networks**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法的一些例子包括**决策树**、**线性回归**和**神经网络**。
- en: '**Unsupervised learning**: As mentioned previously, supervised learning is
    the ML task of finding patterns and behaviors from data that is not tagged. In
    this case, we don’t know which feature we want to predict, or the feature we want
    to predict may not even be a part of the dataset. Unsupervised learning helps
    us predict potential repeating patterns and categorize the set of data using those
    patterns. Another way of understanding this problem is that there are no labeled
    values to supervise the ML algorithm learning task; the algorithm learns the patterns
    and behaviors on its own.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：如前所述，监督学习是从未标记的数据中寻找模式和行为的机器学习任务。在这种情况下，我们不知道我们想要预测哪个特征，或者我们想要预测的特征甚至可能不是数据集的一部分。无监督学习帮助我们预测潜在的重复模式，并使用这些模式对数据集进行分类。另一种理解这个问题的方式是，没有标记的值来监督机器学习算法的学习任务；算法独立地学习模式和行为了。'
- en: Unsupervised learning algorithms are often used to solve clustering and association
    problems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法通常用于解决聚类和关联问题。
- en: Some examples of unsupervised learning algorithms are **K-means clustering**
    and **association rule learning**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法的一些例子包括**K-means聚类**和**关联规则学习**。
- en: '**Semi-supervised learning**: Semi-supervised learning falls between supervised
    learning and unsupervised learning. It is the ML task of performing learning on
    a dataset that is partially labeled. It is used in scenarios where you have a
    small dataset that is labeled along with a large unlabeled dataset. In real-world
    scenarios, labeling large amounts of data is an expensive task as it requires
    a lot of experimentation and contextual information that is manually interpreted,
    while unlabeled data is relatively cheap to acquire. Semi-supervised learning
    often proves efficient in this case as it is good at assuming expected label values
    from unlabeled datasets while working as efficiently as any supervised learning
    algorithm.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半监督学习**：半监督学习介于监督学习和无监督学习之间。它是执行部分标记数据集上的机器学习任务的机器学习任务。它用于你有一个小部分标记的数据集和大量未标记数据集的场景。在现实场景中，标记大量数据是一个昂贵的任务，因为它需要大量的实验和需要人工解释的上下文信息，而未标记数据相对容易获取。在这种情况下，半监督学习通常证明是有效的，因为它擅长从未标记数据集中假设预期的标签值，同时像任何监督学习算法一样高效地工作。'
- en: Unsupervised learning algorithms are often used to solve clustering and classification
    problems.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法通常用于解决聚类和分类问题。
- en: Some examples of semi-supervised learning algorithms are **generative models**
    and **Laplacian regularization**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习算法的一些例子包括**生成模型**和**拉普拉斯正则化**。
- en: '**Reinforcement learning**: Reinforcement learning is an ML task that aims
    to identify the next correct logical action to take in a given environment to
    maximize the cumulative reward. In this type of learning, the accuracy of the
    prediction is calculated after the prediction is made using positive and/or negative
    reinforcement, which is again fed to the algorithm. This continuous learning of
    the environment eventually helps the algorithm find the best sequence of steps
    to take to maximize the reward, thus making the most accurate decision.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习**：强化学习是一个旨在在给定环境中识别下一步正确逻辑动作以最大化累积奖励的机器学习任务。在这种学习中，预测的准确性是在使用正强化和/或负强化后计算的，这些强化再次被输入到算法中。这种对环境的持续学习最终帮助算法找到最佳步骤序列以最大化奖励，从而做出最准确的决策。'
- en: Reinforcement learning is often used to solve a mix of regression, classification,
    and optimization problems.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习通常用于解决回归、分类和优化问题的混合。
- en: Some examples of reinforcement learning algorithms are **Monte Carlo Methods**,
    **Q-Learning**, and **Deep Q Network**.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法的一些例子包括**蒙特卡洛方法**、**Q学习**和**深度Q网络**。
- en: 'The AutoML technology, despite being mature enough to be used commercially,
    is still in its infancy compared to the vast developments in the field of ML.
    AutoML may be able to train the best predictive models in the shortest time using
    little to no human intervention, but its potential is currently limited to only
    supervised learning. The following diagram summarizes the various types of ML
    algorithms categorized under the different ML tasks:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管AutoML技术已经足够成熟，可以用于商业用途，但与机器学习领域的广泛发展相比，它仍然处于起步阶段。AutoML可能能够在最短的时间内使用最少或没有人为干预来训练最佳的预测模型，但它的潜力目前仅限于监督学习。以下图表总结了不同机器学习任务下分类的各种机器学习算法：
- en: '![Figure 5.1 – Types of ML problems and algorithms ](img/B17298_05_001.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – 机器学习问题的类型和算法](img/B17298_05_001.jpg)'
- en: Figure 5.1 – Types of ML problems and algorithms
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 机器学习问题的类型和算法
- en: Similarly, H2O’s AutoML also focuses on supervised learning and as such, you
    are often expected to have labeled data that you can feed to it.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，H2O的AutoML也专注于监督学习，因此你通常需要提供标签数据来供其使用。
- en: ML algorithms that perform unsupervised learning are often quite sophisticated
    compared to supervised learning algorithms as there is no ground truth to measure
    the performance of the model. This goes against the very nature of AutoML, which
    is very reliant on model performance measurements to automate training and hyperparameter
    tuning.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习算法相比，执行无监督学习的机器学习算法通常更为复杂。因为在这种情况下，没有真实数据来衡量模型的性能。这与AutoML的本质相悖，AutoML非常依赖于模型性能的测量来自动化训练和超参数调整。
- en: So, accordingly, H2O AutoML falls in the domain of supervised ML algorithms,
    where it trains several supervised ML algorithms to solve regression and classification
    problems and ranks them based on their performance. In this chapter, we shall
    focus on these ML algorithms and understand their functionality so that we are
    well equipped to understand, select, and justify the different models that H2O
    AutoML trains for a given prediction problem.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，相应地，H2O AutoML属于监督机器学习算法的范畴，它训练多个监督机器学习算法来解决回归和分类问题，并根据它们的性能进行排名。在本章中，我们将关注这些机器学习算法，了解它们的功能，以便我们能够充分理解、选择和证明H2O
    AutoML为特定预测问题训练的不同模型。
- en: 'With this understanding, let’s start with the first ML algorithm: Generalized
    Linear Model.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这种理解，让我们从第一个机器学习算法：广义线性模型开始。
- en: Understanding the Generalized Linear Model algorithm
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解广义线性模型算法
- en: '**Generalized Linear Model** (**GLM**), as its name suggests, is a flexible
    way of generalizing linear models. It was formulated by *John Nelder* and *Robert
    Wedderburn* as a way of combining various regression models into a single analysis
    with considerations given to different probability distributions. You can find
    their detailed paper (Nelder, J.A. and Wedderburn, R.W., 1972\. *Generalized linear
    models. Journal of the Royal Statistical Society: Series A (General), 135(3),
    pp.370-384*.) at [https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2344614](https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2344614).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**广义线性模型**（**GLM**），正如其名所示，是一种灵活的推广线性模型的方法。它是由*约翰·尼尔*和*罗伯特·韦德伯恩*提出的，作为一种将各种回归模型组合成一个分析的方法，同时考虑到不同的概率分布。你可以在[https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2344614](https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2344614)找到他们详细的论文（Nelder,
    J.A. and Wedderburn, R.W., 1972\. *广义线性模型. 皇家统计学会会刊：A系列（一般），135(3)，pp.370-384*）。'
- en: Now, you may be wondering what linear models are. Why do we need to generalize
    them? What benefit does it provide? These are relevant questions indeed and they
    are pretty easy to understand without diving too deep into the mathematics. Once
    we break down the logic, you will notice that the concept of GLM is pretty easy
    to understand.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能想知道什么是线性模型。为什么我们需要对它们进行推广？它提供了什么好处？这些问题确实很重要，而且不需要深入数学知识就可以很容易地理解。一旦我们分解了逻辑，你会发现广义线性模型的概念非常容易理解。
- en: So, let’s start by understanding the basics of linear regression.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们先从理解线性回归的基本概念开始。
- en: Introduction to linear regression
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归简介
- en: '**Linear regression** is probably one of the oldest statistical models, dating
    back to 200 years ago. It is an approach that maps the relationship between the
    dependent and independent variables linearly on a graph. What that means is that
    the relationship between the two variables can be completely explained by a straight
    line.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性回归**可能是最古老的统计模型之一，其历史可以追溯到200年前。它是一种在图上线性映射因变量和自变量之间关系的方法。这意味着两个变量之间的关系可以完全由一条直线来解释。'
- en: 'Consider the following example:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下例子：
- en: '![Figure 5.2 – Linear regression ](img/B17298_05_002.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – 线性回归](img/B17298_05_002.jpg)'
- en: Figure 5.2 – Linear regression
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 线性回归
- en: This example demonstrates the relationship between two variables. The height
    of a person, H, is an independent variable, while the weight of a person, W, is
    a dependent variable. The relationship between these two variables can easily
    be explained by a straight red line. The taller a person is, the more likely he
    or she will weigh more. Easy enough to understand.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了两个变量之间的关系。一个人的身高H是自变量，而一个人的体重W是因变量。这两个变量之间的关系可以很容易地用一条红色的直线来解释。一个人越高，他或她的体重越有可能更重。这很容易理解。
- en: 'Statistically, the general equation for any straight line, also called the
    **linear equation**, is as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 统计上，任何直线的通用方程，也称为**线性方程**，如下所示：
- en: '![](img/Formula_B17298__05_001.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17298__05_001.png)'
- en: 'Here, we have the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '*y* is a point on the *Y*-axis and indicates the dependent variable.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* 是 *Y* 轴上的一个点，表示因变量。'
- en: '*x* is a point on the *X*-axis and indicates the independent variable.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x* 是 *X* 轴上的一个点，表示自变量。'
- en: '*b**1* is the slope of the line, also called the gradient, and indicates how
    steep the line is. The bigger the gradient, the steeper the line.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b**1* 是直线的斜率，也称为梯度，表示直线的陡峭程度。梯度越大，直线越陡。'
- en: '*b**0* is a constant that indicates the point at which the line crosses the
    *Y*-axis.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b**0* 是一个常数，表示直线与 *Y* 轴相交的点。'
- en: During linear regression, the machine will map all the data points of the two
    variables on the graph and randomly place the line on the graph. Then, it will
    calculate the values of *y* by inserting the value of *x* from the data points
    in the graph into the linear equation and comparing the result with the respective
    *y* values from the data points. After that, it will calculate the magnitude of
    the error between the *y* value it calculated and the actual *y* value. This difference
    in values is what we call a **residual**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归过程中，机器将在图上映射两个变量的所有数据点，并在图上随机放置直线。然后，它将通过将图中的数据点的 *x* 值插入线性方程中来计算 *y* 的值，并将结果与数据点的相应
    *y* 值进行比较。之后，它将计算它计算的 *y* 值与实际 *y* 值之间的误差大小。这种值之间的差异就是我们所说的**残差**。
- en: 'The following diagram should help you understand what residuals are:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表应有助于你理解什么是残差：
- en: '![Figure 5.3 – Residuals in linear regression ](img/B17298_05_003.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – 线性回归中的残差](img/B17298_05_003.jpg)'
- en: Figure 5.3 – Residuals in linear regression
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 线性回归中的残差
- en: The machine will do this for all the data points and make a note of all the
    errors. It will then try to tweak the line by changing the values of b1 and b0,
    meaning changing the angle and position of the line on the graph, and repeating
    the process. It will do this until it minimizes the error.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 机器将为所有数据点做这件事，并记录所有误差。然后，它将通过改变b1和b0的值来尝试调整直线，这意味着改变图上直线的角度和位置，并重复这个过程。它将这样做，直到最小化误差。
- en: The values of b1 and b0 that generate the least amount of error are the most
    accurate linear relationship between the two variables. The equation with these
    values for b1 and b0 is the linear model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 产生最少误差的b1和b0的值是两个变量之间最准确的线性关系。具有这些b1和b0值的方程是线性模型。
- en: Now, say you want to predict how much a person would weigh if they were 180
    cm tall. Then, you use this same linear model equation with the b1 and b0 values,
    set *x* to 180, and calculate *y*, which will be the expected weight.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你想预测一个身高180厘米的人的体重。然后，你使用这个相同的线性模型方程，使用b1和b0的值，将 *x* 设置为180，并计算 *y*，这将是你预期的体重。
- en: 'Congratulations, you just performed ML in your mind without any computers and
    made predictions too! Actual ML works the same way, albeit with added complexities
    from complex algorithms. Linear regression doesn’t need to be restricted to just
    two variables – it can also work on multiple variables where there’s more than
    one independent variable. Such linear regression is called multiple or curvilinear
    regression. The equation of such a linear regression expands as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，你刚刚在心中进行了机器学习，没有任何计算机的帮助，并且做出了预测！实际的机器学习工作方式相同，尽管增加了复杂算法的复杂性。线性回归不仅限于两个变量——它还可以处理存在多个自变量的多个变量。这种线性回归称为多元或曲线回归。这种线性回归的方程如下扩展：
- en: '![](img/Formula_B17298__05_002.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17298__05_002.png)'
- en: In this equation, the additional variables – x1, x2, x3, and so on – are added
    with their own coefficients – b1, b2, and b3, respectively.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，附加的变量——x1、x2、x3等等——分别与它们自己的系数——b1、b2和b3相加。
- en: Feel free to explore these algorithms and the mathematics behind them if you
    are interested in the inner workings of linear regression.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对线性回归的内部工作原理感兴趣，不妨探索这些算法及其背后的数学。
- en: Understanding the assumptions of linear regression
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解线性回归的假设
- en: Linear regression, when training a model on a given dataset, works on certain
    assumptions about the data. One of these assumptions is the **normality of errors**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当在给定的数据集上训练模型时，线性回归基于对数据的某些假设。这些假设之一是**误差的正态性**。
- en: 'Before we understand what the normality of errors is, let’s quickly understand
    the concept of the **probability density function**. This is a mathematical expression
    that defines the probability distribution of discrete values – in other words,
    it is a mathematical expression that shows the probability of a sample value occurring
    from a given sample space. To understand this, refer to the following diagram:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们了解误差的正态性之前，让我们快速了解**概率密度函数**的概念。这是一个数学表达式，它定义了离散值的概率分布——换句话说，它是一个数学表达式，显示了从给定的样本空间中发生样本值的概率。为了理解这一点，请参考以下图表：
- en: '![Figure 5.4 – Probability distribution of values for two dice ](img/B17298_05_004.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4 – 两个骰子数值的概率分布](img/B17298_05_004.jpg)'
- en: Figure 5.4 – Probability distribution of values for two dice
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 两个骰子数值的概率分布
- en: 'The preceding diagram shows the distribution of probabilities of all the values
    that can occur when a pair of six-sided dice are thrown fairly and independently.
    There are different kinds of distributions. Some examples of commonly occurring
    distributions are as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了当公平且独立地掷一对六面骰子时，所有可能出现的数值的概率分布。存在不同类型的分布。以下是一些常见分布的例子：
- en: '![Figure 5.5 – Different types of distribution ](img/B17298_05_005.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5 – 不同类型的分布](img/B17298_05_005.jpg)'
- en: Figure 5.5 – Different types of distribution
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 不同类型的分布
- en: 'The normality of errors states that the residuals of the data must be normally
    distributed. A **normal distribution**, also called **Gaussian distribution**,
    is a probability density function that is symmetrical about the mean, where the
    values closest to the mean occur frequently, while those far from the mean rarely
    occur. The following diagram shows a normal distribution:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 误差的正态性表明，数据残差必须是正态分布的。**正态分布**，也称为**高斯分布**，是一个关于均值对称的概率密度函数，其中最接近均值的值出现频率较高，而远离均值的值很少出现。以下图表显示了正态分布：
- en: '![Figure 5.6 – Normal distribution  ](img/B17298_05_006.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – 正态分布](img/B17298_05_006.jpg)'
- en: Figure 5.6 – Normal distribution
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 正态分布
- en: Linear regression expects the residuals that get calculated to fall within a
    normal distribution. In our previous example of the expected weight for a height,
    there is bound to be some error between the predicted weight and the actual weight
    of a person with a certain height. However, the residuals or errors from the prediction
    will most likely fall within a normal distribution as there cannot be too many
    occurrences of people with an extreme difference between the expected weight and
    the predicted weight.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归期望计算出的残差落在正态分布范围内。在我们之前的关于预期体重与一定身高的人实际体重之间误差的例子中，预测体重和实际体重之间必然存在一些误差。然而，由于预期体重和预测体重之间极端差异的人出现的次数不会太多，因此预测的残差或误差很可能会落在正态分布范围内。
- en: 'Consider a scenario of people claiming health insurance payouts. The following
    diagram shows a sample of the linear regression graph for that dataset:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个人们申请健康保险赔付的场景。以下图表显示了该数据集的线性回归图样本：
- en: '![Figure 5.7 – Health insurance payout ](img/B17298_05_007.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – 健康保险赔付](img/B17298_05_007.jpg)'
- en: Figure 5.7 – Health insurance payout
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 健康保险赔付
- en: In the preceding diagram, you can see that the majority of people from various
    age groups did not claim health insurance. Some of them did and the cost of claims
    varied a lot. Some had minor issues costing *little*, while some had serious injuries
    and had to go through expensive surgeries.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，你可以看到来自各个年龄段的大多数人没有申请健康保险。其中一些人申请了，索赔费用差异很大。有些人有轻微的问题，花费*很少*，而有些人遭受了严重的伤害，不得不进行昂贵的手术。
- en: 'If you plot a linear regression line through this dataset, it will look as
    follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过这个数据集绘制线性回归线，它看起来如下所示：
- en: '![Figure 5.8 – Linear regression on health insurance payouts ](img/B17298_05_008.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – 健康保险赔付的线性回归](img/B17298_05_008.jpg)'
- en: Figure 5.8 – Linear regression on health insurance payouts
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 健康保险赔付的线性回归
- en: 'But now, if you calculate the residual errors from the expected and predicted
    value for all the data points, then the probability distribution of these residuals
    will not fall into a normal distribution. It will look as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，现在如果你从所有数据点的预期值和预测值中计算残差误差，那么这些残差的概率分布将不会落入正态分布。它看起来如下所示：
- en: '![Figure 5.9 – Residual distribution of health insurance payouts ](img/B17298_05_009.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 健康保险赔付的残差分布](img/B17298_05_009.jpg)'
- en: Figure 5.9 – Residual distribution of health insurance payouts
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 健康保险赔付的残差分布
- en: This is an inaccurate model as the expected value and the predicted values are
    not even close enough to round off or correct. So, what do you do for such a scenario,
    where the normality of errors assumption fails for the dataset? What if the distribution
    of the residuals is, say, Poisson instead of normal? How will the machine correct
    that?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不精确的模型，因为预期值和预测值甚至不够接近，以至于可以四舍五入或纠正。那么，对于这种错误假设对于数据集不成立的情况，你该怎么办？如果残差的分布是泊松分布而不是正态分布，机器将如何纠正？
- en: Well, the answer to this is that the distribution of residuals depends on the
    distribution of the dataset itself. If the values of the dependent variables are
    normally distributed, then the distribution of the residuals will also be normal.
    So, once we have identified which probability density function fits the dataset,
    we can use that function to train our linear model.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这个问题的答案是残差的分布取决于数据集本身的分布。如果因变量的值呈正态分布，那么残差的分布也将是正态的。因此，一旦我们确定了哪个概率密度函数适合数据集，我们就可以使用该函数来训练我们的线性模型。
- en: Depending on this function, there are specialized linear regression methods
    for every probability density function. If your distribution is **Poisson**, then
    you can use **Poisson regression**. If your data distribution is negative **binomial**,
    then you can use **negative binomial regression**.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个函数，为每个概率密度函数有专门的线性回归方法。如果你的分布是**泊松**，那么你可以使用**泊松回归**。如果你的数据分布是负**二项式**，那么你可以使用**负二项式回归**。
- en: Working with a Generalized Linear Model
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用广义线性模型（GLM）进行工作
- en: Now that we have covered the basics, let’s focus on understanding what GLM is.
    GLM is a way of pointing to all the regression methods that are specific to the
    type of probability distribution of the data. Technically, all the regression
    models are GLM, including our ordinary simple linear model. GLM just encapsulates
    them together and trains the appropriate regression model based on the probability
    distribution function.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了基础知识，让我们来了解什么是GLM。GLM是一种指向所有特定于数据类型概率分布的回归方法的方式。技术上讲，所有回归模型都是GLM，包括我们的普通简单线性模型。GLM只是将它们封装在一起，并根据概率分布函数训练适当的回归模型。
- en: The way GLM works is by using something called a link function in conjunction
    with a systematic component and the random variable.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: GLM的工作方式是使用所谓的连接函数，结合系统成分和随机变量。
- en: 'These are three components of GLM:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是广义线性模型（GLM）的三个组成部分：
- en: '**Systematic component**: Going back to the multi-variate linear equation,
    we have the following:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统成分**：回到多元线性方程，我们有以下内容：'
- en: '![](img/Formula_B17298__05_003.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17298__05_003.png)'
- en: Here, b1x1+ b2x2 + b3x3 + ……. + b0 is the systematic component. This is the
    function that links our data, also called predictors, with our predictions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '**Random Component**: This component refers to the probability distribution
    of the response variable. This will be whether the response variable is normally
    distributed or binomially distributed or any other form of distribution.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Link function**: A link function is a function that maps the non-linear relationship
    of data to a linear one. In other words, it bends the line of linear regression
    to represent the relationship of non-linear data more accurately. It is a link
    between the random and the systematic components. We can explain the equation
    with a link function mathematically as *Y = f*n*( b*1*x*1*+ b*2*x*2 *+ b*3*x*3
    *+ ……. + b*0 *)*, where *f*n is the link function that changes as per the distribution
    of the response variable.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The link function is different for different distributions. The following table
    shows the different link functions for different distributions:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '| **Distribution Type** | **Link Function** | **Name of Algorithm** |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| Normal | ![](img/Formula_B17298__05_004.png) | Linear model |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| Binomial | ![](img/Formula_B17298__05_005.png) | Logistic regression |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| Poisson | ![](img/Formula_B17298__05_006.png) | Poisson regression |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| Gamma | ![](img/Formula_B17298__05_007.png) | Gamma regression |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: Figure 5.10 – Link functions for different distribution types
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: When training GLM models, you have the option of selecting the value for the
    family hyperparameter. The family option specifies the probability distribution
    of your response column and the GLM training algorithm uses the appropriate link
    function during training.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'The values for the family hyperparameter are as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '**gaussian**: You should select this option if the response is a real integer
    number.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**binomial**: You should select this option if the response is categorical
    with two classes or binaries that could be either enums or integers.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**fractionalbinomial**: You should select this option if the response is numeric
    between 0 and 1.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ordinal**: You should select this option if the response is a categorical
    response with three or more classes.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**quasibinomial**: You should select this option if the response is numeric.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**multinomial**: You should select this option if the response is a categorical
    response with three or more classes that are of enum types.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**poisson**: You should select this option if the response is numeric and contains
    non-negative integers.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gamma**: You should select this option if the response is numeric and continuous
    and contains positive real integers.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tweedie**: You should select this option if the response is numeric and contains
    continuous real values and non-negative values.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negativebinomial**: You should select this option if the response is numeric
    and contains a non-negative integer.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AUTO**: This determines the family automatically for the user.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you may have guessed, H2O’s AutoML selects AUTO as the family type when training
    GLM models. The AutoML process handles this case of selecting the correct distribution
    family by understanding the distribution of the response variable in the dataset
    and applying the correct link function to train the GLM model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations, we have just looked into how the GLM algorithm works! GLM is
    a very powerful and flexible algorithm and H2O AutoML expertly configures its
    training so that it trains the most accurate and high-performance GLM model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s move on to the next ML algorithm that H2O trains: **Distributed
    Random Forest** (**DRF**).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Distributed Random Forest algorithm
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**DRF**, simply called **Random Forest**, is a very powerful supervised learning
    technique often used for classification and regression. The foundation of the
    DRF learning technique is based on **decision trees**, where a large number of
    decision trees are randomly created and used for predictions and their results
    are combined to get the final output. This randomness is used to minimize the
    bias and variance of all the individual decision trees. All the decision trees
    are collectively combined and called a forest, hence the name Random Forest.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: To get a deeper conceptual understanding of DRF, we need to understand the basic
    building block of DRF – that is, a decision tree.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to decision trees
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In very simple terms, a decision tree is just a set of *IF* conditions that
    either return a yes or a no answer based on data passed to it. The following diagram
    shows a simple example of a decision tree:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Simple decision tree ](img/B17298_05_010.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Simple decision tree
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram shows a basic decision tree. A decision tree consists
    of the following components:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '**Nodes**: Nodes are basically *IF* conditions that split the decision tree
    based on whether the condition was met or not.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Root Node**: The node on the top of the decision tree is called the root
    node.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leaf Node**: The nodes of the decision tree that do not branch out further
    are called leaf nodes, or simply leaves. The condition, in this case, is if the
    value of the data that’s passed to it is numeric, then the answer is the data
    is a number; if the data that’s passed to it is not numeric, then the answer will
    be the data is non-numeric. This is simple enough to understand.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As seen in *Figure 5.11*, the decision tree is based on a simple true or false
    question. Decision trees can also be based on mathematical conditions on numeric
    data. The following example shows a decision tree on numeric conditions:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Numerical decision tree ](img/B17298_05_011.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Numerical decision tree
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the root node computes whether the IQ number is greater than
    300 and decides if it is artificial intelligence or human intelligence.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees can be combined as well. They can form a complex set of decision-making
    conditions that rely on the results of previous decisions. Refer to the following
    example for a complex decision tree:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Complex decision tree ](img/B17298_05_012.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Complex decision tree
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we are trying to calculate if *you can go play outside*
    or *finish your ML studies*. This decision tree combines numeric data as well
    as the classification of data. When making predictions, the decision tree will
    start at the top and work its way down, making decisions on whether the data satisfies
    the conditions or not. The leaf nodes are the final potential results of the decision
    tree.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'With this knowledge in mind, let’s create a decision tree on a sample dataset.
    Refer to the following table for the sample dataset:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Sample dataset for creating a decision tree ](img/B17298_05_013.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – Sample dataset for creating a decision tree
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'The content of the aforementioned dataset is as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '**Chest Pain**: This column indicates if a patient suffers from chest pain.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Good Blood Circulation**: This column indicates if a patient has good blood
    circulation.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blocked Arteries**: This column indicates if a patient has any blocked arteries.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heart Disease**: This column indicates if the patient suffers from heart
    disease.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this scenario, we want to create a decision tree that uses Chest Pain, Good
    Blood Circulation, and Blocked Arteries features to predict whether a patient
    has heart disease. Now, when forming a decision tree, the first thing that we
    need to do is find the root node. So, what feature should we place at the top
    of the decision tree?
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by looking at how the Chest Pain feature alone fairs when predicting
    heart disease. We shall go through all the values in the dataset and map them
    to this decision tree while comparing the values in the Chest Pain column with
    those of heart disease. We shall keep track of these relationships in the decision
    tree. The decision tree for Chest Pain as the root node will be as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – Decision tree for the Chest Pain feature ](img/B17298_05_014.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – Decision tree for the Chest Pain feature
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Now, we do this for all the other features in the dataset. We create a decision
    tree for Good Blood Circulation and see how it fairs alone when making predictions
    for Heart Disease and keep a track of the comparison, repeating the same process
    for the Blocked Arteries status as well. If there are any missing values in the
    dataset, then we skip them. Ideally, you should not work with datasets that have
    missing values. We can use the techniques we learned about in [*Chapter 3*](B17298_03.xhtml#_idTextAnchor066),
    *Understanding Data Processing*, where we impute and handle missing dataset values.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following diagram, which shows the two decision trees that were
    created – one for **Patient Has Blocked Arteries** and another for **Patient Has
    Good Blood Circulation**:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Decision tree for the Blocked Arteries and Good Blood Circulation
    features ](img/B17298_05_015.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – Decision tree for the Blocked Arteries and Good Blood Circulation
    features
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have created a decision tree for all the features in the dataset,
    we can compare their results to find the pure feature. In the context of decision
    trees, a feature is said to be 100% impure when a node is split evenly, 50/50,
    and 100% pure when all of its data belongs to a single class. In our scenario,
    we don’t have any feature that is 100% pure. All of our features are impure to
    some degree. So, we need to find some way of finding the feature that is the purest.
    For that, we need a metric that can measure the purity of a decision tree.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of ways by which data scientists and engineers measure purity.
    The most common metric to measure impurity in decision trees is **Gini Impurity**.
    Gini Impurity is the measure of the likelihood that a new random instance of data
    will be incorrectly classified during classification.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Gini Impurity is calculated as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17298__05_008.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: 'Here, p1, p2 , p3 , p4 … are the probabilities of the various classifications
    for Heart Disease. In our scenario, we only have two classifications – either
    a yes or a no. Thus, for our scenario, the measure of impurity is as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17298__05_009.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: 'So, let’s calculate the Gini Impurity of all the decision trees we just created
    so that we can find the feature that is the purest. Gini Impurity for a decision
    tree with multiple leaf nodes is calculated by calculating the Gini Impurity of
    individual leaf nodes and then calculating the weighted average of all the impurity
    values to get the Gini Impurity of the decision tree as a whole. So, let’s start
    by calculating the Gini Impurity of the left leaf node of the Chest Pain decision
    tree and repeat this for the right leaf node:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17298__05_010.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: '![](img/Formula_B17298__05_011.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: 'The reason why we calculate the weighted average of the Gini Impurities is
    because the representation of the data is not equally divided between the two
    branches of the decision tree. The weighted average helps us offset this unequal
    distribution of the data values. Thus, we can calculate the Gini Impurity of the
    whole Chest Pain decision tree as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '*Gini Impurity (Chest Pain) = weighted average of the Gini Impurities of the
    leaf nodes*'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '*Gini Impurity (Chest Pain) =*'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '*(Total number of data inputs in the left leaf node / total number of rows)
    x Gini Impurity of the left leaf node*'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '*+*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '*(Total number of data inputs in the right leaf node / total number of rows)
    x Gini Impurity of the right leaf node*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '*= (144 / (144 + 159)) x 0.395 + (159 / (144 + 159)) x 0.364*'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.364*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The Gini Impurity of the Chest Pain decision tree is 0.364.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'We repeat this process for all the other feature decision trees as well. We
    should get the following results:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: The Gini Impurity of the Chest Pain decision tree is 0.364
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Gini Impurity of the Good Blood Circulation tree is 0.360
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Gini Impurity of the Blocked Arteries tree is 0.381
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing these values, we can infer that the Gini Impurity of the Good Blood
    Circulation feature has the lowest Gini Impurity, making it the purest feature
    in the dataset. So, we will use it as the root of our decision tree.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Referring to *Figures 5.12* and *5.13*, when we divided the patients by using
    the Good Blood Circulation feature, we were left with an impure distribution of
    the results on the left and right leaf nodes. So, each leaf node had a mix of
    results that showed with and without Heart Disease. Now, we need to figure out
    a way to separate the mix of results from the Good Blood Circulation feature using
    the remaining features – Chest Pain and Blocked Arteries.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: So, just as how we did previously, we shall use these mixed results and separate
    them using the other features and calculate the Gini Impurity value of those features.
    We shall choose the feature that is the purest and replace it at the given node
    for further classification.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'We shall repeat this process for the right branch as well. So, to simplify
    the selection of the decision tree nodes, we must do the following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the Gini Impurity score of all the remaining features for that node
    using the mixed results.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the one with the lowest impurity and replace it with the node.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the same process further down the decision tree with the remaining features.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continue replacing the nodes, so long as the classification lowers the Gini
    Impurity; otherwise, leave it as a leaf node.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, your final decision tree will be as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 – The final decision tree ](img/B17298_05_016.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 – The final decision tree
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: This decision tree is good for classification with true or false values. What
    if you had numerical data instead?
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating decision trees with numerical data is very easy and has almost the
    same steps as we do for true/false data. Consider Weight as a new feature; the
    data for the **Weight** column is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Dataset with a new feature, Weight, in kilograms ](img/B17298_05_017.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 – Dataset with a new feature, Weight, in kilograms
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'For this scenario, we must follow these steps:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Sort the data in ascending order. In our scenario, we shall sort the rows of
    the dataset with the Weight column from highest to lowest.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the average weights for all the adjacent rows:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Calculating the average of the subsequent row values ](img/B17298_05_018.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 – Calculating the average of the subsequent row values
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the Gini Impurity of all the averages we calculated:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.20 – Calculating the Gini Impurity of all the averages ](img/B17298_05_019.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: Figure 5.20 – Calculating the Gini Impurity of all the averages
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Identify and select the average feature value that gives us the least Gini Impurity
    value.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the selected feature value as a decision node in the decision tree.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Making predictions using decision trees is very easy. You will have data with
    values for chest pain, blocked arteries, good circulation, and weight and you
    will feed it to the decision tree model. The model will filter the values down
    the decision tree while calculating the node conditions and eventually arriving
    at the leaf node with the prediction value.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations – you have just understood the concept of decision trees! Despite
    decision trees being easy to understand and implement, they are not that good
    at solving real-life ML problems.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'There are certain drawbacks to using decision trees:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are very unstable. Any minor changes in the dataset can drastically
    alter the performance of the model and prediction results.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are inaccurate.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can get very complex for large datasets with a large number of features.
    Imagine a dataset with 1,000 features – the decision tree for this dataset will
    have a tree whose depth will be very large and its computation will be very resource-intensive.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To mitigate all these drawbacks, the Random Forest algorithm was developed,
    which builds on top of decision trees. With this knowledge, let’s move on to the
    next concept: Random Forest.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Random Forest
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Random Forest**, also called **Random Decision Forest**, is an ML method
    that builds a large number of decision trees during learning and groups, or ensembles,
    the results of the individual decision trees to make predictions. Random Forest
    is used to solve both classification and regression problems. For classification
    problems, the class value predicted by the majority of the decision trees is the
    predicted value. For regression problems, the mean or average prediction of the
    individual trees is calculated and returned as the prediction value.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'The Random Forest algorithm follows these steps for learning during training:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Create a bootstrapped dataset from the original dataset.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly select a subset of the data features.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start creating a decision tree using the selected subset of features, where
    the feature that splits the data the best is chosen as the root node.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a random subset of the other remaining features to further split the
    decision tree.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s understand this concept of Random Forest by creating one.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: We shall use the same dataset that we used to make our complex decision tree
    in *Figure 5.17*. The dataset is the same one we used to make our decision trees.
    To create a Random Forest, we need to create a bootstrapped version of the dataset.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: A bootstrapped dataset is a dataset that is created from the original dataset
    by randomly selecting rows from the dataset. The bootstrapped dataset is the same
    size as the original dataset and can also contain duplicate rows from the original
    dataset. There are plenty of inbuilt functions for creating a bootstrapped dataset
    and you can use any of them to create one.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following bootstrapped dataset:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 – Bootstrapping dataset ](img/B17298_05_020.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: Figure 5.21 – Bootstrapping dataset
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to create a decision tree from the bootstrapped dataset but
    using only a subset of the feature columns at each step. So, selecting all the
    features to be considered for the decision tree only lets you go with Good Blood
    Circulation and Blocked Arteries as features for the decision tree.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: We shall follow the same purity identification criteria to determine the root
    of the node. Let’s assume that for our experiment, Good Blood Circulation is the
    purest. Setting that as the root node, we shall now consider the remaining features
    to fill the next level of decision nodes. Just like we did previously, we shall
    randomly select two features from the remaining features and decide which feature
    should fit in the next decision node. We will build the tree as usual while considering
    the random subset of remaining variables at each step.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the tree we just made:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22 – First decision tree from the bootstrapped dataset ](img/B17298_05_021.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: Figure 5.22 – First decision tree from the bootstrapped dataset
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we repeat the same process while creating multiple decision trees and
    bootstrapping and selecting features from random trees. An ideal Random Forest
    will create hundreds of decision trees, as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.23 – Multiple decision trees from different bootstrapped datasets
    ](img/B17298_05_022.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: Figure 5.23 – Multiple decision trees from different bootstrapped datasets
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: This large variety of decision trees that were created with randomized implementation
    is what makes Random Forest more effective than a single decision tree.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have created our Random Forest, let’s see how we can use it to make
    predictions. To make predictions, you will have a row that contains data values
    for the different features and you want to predict whether that person has heart
    disease.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'You will pass this data down an individual decision tree in the Random Forest:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.24 – Predictions from the first decision tree in the Random Forest
    ](img/B17298_05_023.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: Figure 5.24 – Predictions from the first decision tree in the Random Forest
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision tree will predict the results based on its structure. We shall
    keep a track of the prediction made by this tree and continue passing the data
    down to the other trees, noting their predictions as well:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25 – Predictions from the other individual trees in the Random Forest
    ](img/B17298_05_024.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: Figure 5.25 – Predictions from the other individual trees in the Random Forest
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Once we get predictions from all the individual trees, we can find out which
    value got the most votes from all the decision trees. The prediction value with
    the most votes concludes the prediction for the Random Forest.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping the dataset and aggregating the prediction values of all the decision
    trees to make a decision is called **bagging**.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations – you have just understood the concept of Random Forest! Random
    Forest, despite being a very good ML algorithm with low bias and variance, still
    suffers from high computation requirements. Hence, H2O AutoML, instead of training
    Random Forest, trains an alternate version of Random Forest called **Extremely
    Randomized Trees** (**XRT**).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Extremely Randomized Trees
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **XRT** algorithm, also called **ExtraTrees**, is just like the ordinary
    Random Forest algorithm. However, there are two key differences between Random
    Forest and XRT, as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: In Random Forest, we use a bootstrapped dataset to train the individual decision
    trees. In XRT, we use the whole dataset to train the individual decision trees.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Random Forest, the decision nodes are split based on certain selection criteria
    such as the impurity metric or error rate when building the individual decision
    tree. In XRT, this process is completely randomized and the one with the best
    results is chosen.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s consider the same example we used to understand Random Forest to understand
    XRT. We have a dataset, as shown in *Figure 5.17*. Instead of bootstrapping the
    data, as we did in *Figure 5.20*, we shall use the dataset as-is.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Then, we start creating our decision trees by randomly selecting a subset of
    the features. In Random Forest, we used the purity criteria to decide which feature
    should be set as the root node of the decision tree. However, for XRT, we shall
    set the root node as well as the decision nodes of the decision tree randomly.
    Similarly, we shall create multiple decision trees like these with all the features
    randomly selected. This added randomness allows the algorithm to further reduce
    the variance of the model, at the expense of a slight increase in bias.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! We have just investigated how the XRT algorithm uses an extremely
    randomized forest of decision trees to make accurate regressions and classification
    predictions. Now, let’s understand how the GBM algorithm trains a classification
    model to classify data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Gradient Boosting Machine algorithm
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Gradient Boosting Machine** (**GBM**) is a forward learning ensemble ML algorithm
    that works on both classification as well as regression. The GBM model is an ensemble
    model just like the DRF algorithm in the sense that the GBM model, as a whole,
    is a combination of multiple weak learner models whose results are aggregated
    and presented as a GBM prediction. GBM works similarly to DRF in that it consists
    of multiple decision trees that are built in a sequence that sequentially minimizes
    the error.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: GBM can be used to predict continuous numerical values, as well as to classify
    data. If GBM is used to predict continuous numerical values, we say that we are
    using GBM for regression. If we are using GBM to classify data, then we say we
    are using GBM for classification.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: The GBM algorithm has a foundation on decision trees, just like DRF. However,
    how the decision trees are built is different compared to DRF.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to understand how the GBM algorithm works for regression.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Building a Gradient Boosting Machine
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We shall use the following sample dataset and understand how GBM works as we
    conceptually build the model. The following table contains a sample of the dataset:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26 – Sample dataset for GBM ](img/B17298_05_025.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: Figure 5.26 – Sample dataset for GBM
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an arbitrary dataset that we are using just for the sake of understanding
    how GBM will build its ML model. The contents of the dataset are as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '**Height**: This column indicates the height of the person in centimeters.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gender**: This column indicates the gender of the person.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Age**: This column indicates the age of the person.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weight**: This column indicates the weight of the person.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GBM, unlike DRF, starts creating its weak learner decision trees from leaf
    nodes instead of root nodes. The very first leaf node that it will create will
    be the average of all the values of the response variable. So, accordingly, the
    GBM algorithm will create the leaf node, as shown in the following diagram:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.27 – Calculating the leaf node using the column average ](img/B17298_05_026.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: Figure 5.27 – Calculating the leaf node using the column average
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: This leaf node alone can also be considered a decision tree. It acts like a
    prediction model that only predicts a constant value for any kind of input data.
    In this case, it’s the average value that we get from the response column. This
    is, as we expect, an incorrect way of making any predictions, but it is just the
    first step for GBM.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: The next thing GBM will do is create another decision tree based on the errors
    it observed from its initial leaf node predictions on the dataset. An error, as
    we discussed previously, is nothing but the difference between the observed weight
    and the predicted weight and is also called the residual. However, these residuals
    are different from the actual residuals that we will get from the complete GBM
    model. The residuals that we get from the weak learner decision trees of GBM are
    called **pseudo-residuals**, while those of the GBM model are the actual residuals.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: So, as mentioned previously, the GBM algorithm will calculate the pseudo-residuals
    of the first leaf node for all the data values in the dataset and create a special
    column that keeps track of these pseudo-residual values.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following diagram for a better understanding:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.28 – Dataset with pseudo-residuals ](img/B17298_05_027.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: Figure 5.28 – Dataset with pseudo-residuals
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Using these pseudo-residual values, the GBM algorithm then builds a decision
    tree using all the remaining features – that is, Height, Favorite Color, and Gender.
    The decision tree will look as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.29 – Decision tree using pseudo-residual values ](img/B17298_05_028.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: Figure 5.29 – Decision tree using pseudo-residual values
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this decision tree only has four leaf nodes, while the pseudo-residual
    values in the algorithm generated from the first tree are way more than four.
    This is because the GBM algorithm restricts the size of the decision trees it
    makes. For this scenario, we are only using four leaf nodes. Data scientists can
    control the size of the trees by passing the right hyperparameters when configuring
    the GBM algorithm. Ideally, for large datasets, you often use 8 to 32 leaf nodes.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the restriction of the leaf nodes in the decision trees, the decision
    tree ends up with multiple pseudo-residual values in the same leaf nodes. So,
    the GBM algorithm replaces them with their average to get one concrete number
    for a single leaf node. Accordingly, after calculating the averages, we will end
    up with a decision tree that looks as follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.30 – Decision tree using averaged pseudo-residual values ](img/B17298_05_029.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: Figure 5.30 – Decision tree using averaged pseudo-residual values
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Now, the algorithm combines the original leaf node with this new decision tree
    to make predictions on it. So, now, we have a value of 71.2 from the initial leaf
    node prediction. Then, after running the data down the decision tree, we get 16.8\.
    So, the predicted weight is the summation of both the predictions, which is 88\.
    This is also the observed weight.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: This is not correct as this is a case of overfitting. **Overfitting** is a modeling
    error where the model function is too fine-tuned to predict only the data values
    available in the dataset and not any other values outside the dataset. As a result,
    the model becomes useless for predicting any values that fall outside of the dataset.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to correct this, the GBM algorithm assigns a learning rate to all the weak
    learner decision trees that it trains. The **learning rate** is a hyperparameter
    that tunes the rate at which the model learns new information that can override
    the old information. The value of the learning rate ranges from 0 to 1\. By adding
    this learning rate to the predictions from the decision trees, the algorithm controls
    the influence of the decision tree’s predictions and slowly moves toward minimizing
    the error step by step. For our example, let’s assume that the learning rate is
    0.1\. So, accordingly, the predicted weight can be calculated as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.31 – Calculating the predicted weight ](img/B17298_05_030.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
- en: Figure 5.31 – Calculating the predicted weight
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the algorithm will plug in the learning rate for the predictions made
    by the decision tree and then calculate the predicted weight. Now, the predicted
    weight will be *62.1 + (0.1 x -14.2) = 60.68*.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 60.68 is not a very good prediction but it is still a better prediction than
    62.68, which is what the initial leaf node predicted. The incremental steps to
    minimize the errors are the right way to maintain low variance in predictions.
    A correct balance of learning rate is also important as too high a learning rate
    will offshoot the correction in the opposite direction, while too low a learning
    rate will lead to long computation time as the algorithm will take very small
    correction steps to reach the minimum error.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: To further correct the prediction value and minimize the error, the GBM algorithm
    will create another decision tree. For this, it will calculate the new pseudo-residual
    values from predictions made with the leaf node and the first decision tree and
    use these values to build the second decision tree.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how new pseudo-residual values are calculated:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.32 – Calculating new pseudo-residual values ](img/B17298_05_031.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: Figure 5.32 – Calculating new pseudo-residual values
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that the new pseudo-residual values that were generated are
    a lot closer to the actual values compared to the first pseudo-residual values.
    This indicates that the GBM model is slowly minimizing errors and improving its
    accuracy.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Moving on with the second decision tree, the algorithm uses the new pseudo residual
    values to create the second decision tree. Once created, it aggregates the tree,
    along with the learning rate, to the already existing leaf node and the first
    decision tree.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision trees can be different each time the GBM algorithm creates one.
    However, the learning rate stays common for all the trees. So, now, the prediction
    values will be the summation of the three components – the initial leaf node prediction
    value, the scaled value of the first decision tree prediction, and the scaled
    value of the second decision tree prediction. So, the prediction values will be
    as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.33 – GBM model with a second boosted decision tree ](img/B17298_05_032.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
- en: Figure 5.33 – GBM model with a second boosted decision tree
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'The GBM algorithm will repeat the same process, creating decision trees up
    to the specified number of trees or until adding decision trees stops improving
    the predictions. So, eventually, the GBM model will look as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.34 – Complete GBM model ](img/B17298_05_033.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
- en: Figure 5.34 – Complete GBM model
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! we have just explored how the GBM algorithm uses an ensemble
    of weak decision tree learners to make accurate regressions predictions.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Another algorithm that H2O AutoML uses, which builds on top of GBM, is the XGBoost
    algorithm. XGBoost stands for Extreme Gradient Boosting and implements a process
    called boosting that sometimes helps in training better-performing models. It
    is one of the most widely used ML algorithms in Kaggle competitions and has proven
    to be an amazing ML algorithm that can be used for both classification and regression.
    The mathematics behind how XGBoost works can be slightly difficult for users not
    well versed with statistics. However, it is highly recommended that you take the
    time and learn more about this algorithm. You can find more information about
    how H2O performs XGBoost training at [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.xhtml](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.xhtml).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble ML is a method of combining multiple ML models to obtain better prediction
    results compared to the performance of the models individually – just like how
    a combination of decision trees creates the Random Forest algorithm using bagging
    and how the GBM algorithm uses a combination of weak learners to minimize errors.
    Ensemble models take things one step further by finding the best combinations
    of prediction algorithms and using their combined performance to train a meta-learner
    that provides improved performance. This is done using a process called stacking.
    You can find more information about how H2O trains these stacked ensemble models
    at [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.xhtml#stacked-ensembles](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.xhtml#stacked-ensembles).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s learn how deep learning works and understand neural networks.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what is Deep Learning
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deep Learning** (**DL**) is a branch of ML that develops prediction models
    using **Artificial Neural Networks** (**ANNs**). ANNs, simply called **Neural
    Networks** (**NNs**), are computations that are loosely based on how human brains
    with neurons work to process information. ANNs consist of neurons, which are types
    of nodes that are interconnected with other neurons. These neurons transmit information
    among themselves; this gets processed down the NN to eventually arrive at a result.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: DL is one of the most powerful ML techniques and is used to train models that
    are highly configurable and can support predictions for large and complicated
    datasets. DL models can be supervised, semi-supervised, or unsupervised, depending
    on their configuration.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various types of ANNs:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '**Recurrent Neural Network** (**RNN**): RNN is a type of NN where the connections
    between the various neurons of the NN can form a directed or undirected graph.
    This type of network is cyclic since the outputs of the network are fed back to
    the start of the network and contribute to the next cycle of predictions. The
    following diagram shows an example of an RNN:'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.35 – RNN ](img/B17298_05_034.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
- en: Figure 5.35 – RNN
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the values from the last nodes in the NN are fed to the starting
    nodes of the network as inputs.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '**Feedforward NN**: A feedforward neural network is similar to an RNN, with
    the only difference being that the network of nodes does not form a cycle. The
    following diagram shows an example of a feedforward neural network:'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.36 – Feedforward neural network ](img/B17298_05_035.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
- en: Figure 5.36 – Feedforward neural network
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this type of NN is unidirectional. This is the simplest type
    of ANN. A feedforward neural network is also called **Deep Neural Network** (**DNN**).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O’s DL is based on a multi-layer free forward ANN. It is trained on **stochastic
    gradient descent** using **backpropagation**. There are plenty of different types
    of DNNs that H2O can train. They are as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-Layer Perceptron** (**MLP**): These types of DNNs are best suited for
    tabular data.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional Neural Networks** (**CNNs**): These types of DNNs are best
    suited for image data.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNNs**): These types of DNNs are best suited
    for sequential data such as voice data or time series data.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is recommended to use the default DNNs that H2O provides out of the box as
    configuring a DNN can be very difficult for non-experts. H2O has already preconfigured
    its implementation of DL to use the best type of DNNs for the given cases.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: With these basics in mind, let’s dive deeper into understanding how DL works.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Understanding neural networks
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**NNs** form the basis of DL. The workings of a NN are easy to understand:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: You feed the data into the input layer of the NN.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The nodes in the NN train themselves to recognize patterns and behaviors from
    the input data.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The NN then makes predictions based on the patterns and behaviors it learns
    during training.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The structure of an NN looks as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.37 – Structure of an NN ](img/B17298_05_036.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
- en: Figure 5.37 – Structure of an NN
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three essential components of an NN:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '**The input layer**: The input layer consists of multiple sets of neurons.
    These neurons are connected to the next layer of neurons, which reside in the
    hidden layer.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The hidden layer**: Within the hidden layer, there can be multiple layers
    of neurons, all of which are interconnected layer by layer.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The output layer**: The output layer is the final layer in the NN that makes
    the final calculations to compute the final prediction values in terms of probability.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The learning process of the NN can be broken down into two components:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward propagation**: As the name suggests, forward propagation is where
    the information flows from the input layer to the output layer through the middle
    layer. The following diagram shows forward propagation in action:'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.38 – Forward propagation in an NN ](img/B17298_05_037.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
- en: Figure 5.38 – Forward propagation in an NN
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: The neurons within the middle layer are connected via **channels**. These channels
    are assigned numerical values called **weights**. Weights determine how important
    the neuron is in terms of its value contributing to the overall prediction. The
    higher the value of the weight, the more important that node is when making predictions.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: The input values from the input layer are multiplied by these weights as they
    pass through the channels and their sum is sent as inputs to the neurons in the
    hidden layer. Each neuron in the hidden layer is associated with a numerical value
    called a **bias,** which is added to the input sum.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: This weighted value is then passed to a non-linear function called the activation
    function. The activation function is a function that decides if the particular
    neuron can pass its calculated weight value onto the next layer of the neuron
    or not, depending on the equation of the non-linear function. The bias is a scalar
    value that shifts the activation function either to the left or right of the graph
    for corrections.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: This flow of information continues to the next layer of neurons in the hidden
    layer, following the same process of multiplying the weight of the channels and
    passing the input to the next activation function of the node.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the output layer, the neuron with the highest value determines what
    the prediction value is, which is a form of probability.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '**Backpropagation**: Backpropagation works the same way as forward propagation
    except that it works in the reverse direction. Information is passed from the
    output layer to the input layer through the hidden layer in a reverse manner.
    The following diagram will give you a better understanding of this:'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.39 – Backpropagation in NN ](img/B17298_05_038.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
- en: Figure 5.39 – Backpropagation in NN
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: It may be counterintuitive to understand how backpropagation can work since
    it works in a reverse manner from the output to the input, but it is a concept
    that makes DL so powerful for ML. It is through backpropagation that NNs can learn
    by themselves.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: The way it does this is pretty simple. In backpropagation learning, the NN will
    calculate the magnitude of error between the expected value and the predicted
    value and evaluate its performance by mapping it onto a **loss function**. The
    loss function is a function that calculates the deviance between the predicted
    and expected values. This deviation value is the information that helps the NN
    adjust its biases and weights in the hidden layer to improve its performance and
    make better predictions.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations – we have just gotten a basic glimpse of how DL trains ML models!
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: DL is one of the most sophisticated fields in ML, as well as AI as a whole.
    It’s a vast area of ML specialization where data scientists spend a lot of time
    researching and understanding the problem statement and the data they are working
    with so that they can correctly tune their DL NN. The mathematics behind it is
    also very complex and as such deserves a dedicated book. So, if you are interested
    in mathematics and want to excel in the art of DL, feel free to explore the ML
    algorithm in depth as every step in understanding it will make you that much more
    of an expert ML engineer. You can find more information about how H2O performs
    DL training at [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.xhtml](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.xhtml).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we understood the different types of prediction problems and
    how various algorithms aim to solve them. Then, we understood how the different
    ML algorithms are categorized into supervised, unsupervised, semi-supervised,
    and reinforcement based on their method of learning from data. Once we had an
    understanding of the overall problem domain of ML, we understood that H2O AutoML
    trains only supervised learning ML algorithms and can solve prediction problems
    in this domain specifically.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Then, we understood which algorithms H2O AutoML trains starting with GLM. To
    understand GLM, we understood what linear regression is and how it works and what
    assumptions about the normal distribution of data it has to make to be effective.
    With these basics in mind, we understood how GLM is generalized to be effective,
    even if these assumptions of linear regression are met, which is a common case
    in real life.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Then, we learned about DRF. To understand DRF, we understood what decision trees
    are – that is, the basic building blocks of DRF. Then, we learned that multiple
    decision trees with their ensembled learnings are better ML models than a normal
    decision tree – that is how Random Forest works. Building on top of this, we learned
    how DRF adds more randomization in the form of XRT to make the algorithm all the
    more effective with low variance and bias.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: After that, we learned about GBM. We learned how GBM is similar to DRFs but
    that it has a slightly different way of learning. We understood how GBM sequentially
    builds decision trees and slowly minimizes error by learning from its residuals
    from previous decision tree prediction aggregates.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned what DL is. We understood how NNs are the building blocks
    of DL and their different types. We also understood how NNs perform backpropagation
    learning from its results and self-learn and improve the model by adjusting the
    weights and biases of the neurons in the middle layer.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: This chapter gave you a brief conceptual understanding of how the various ML
    algorithms are trained by H2O AutoML without diving too deep into the mathematics.
    However, ML enthusiasts who want to become experts in the field of ML and wish
    to work on complex ML problems are strongly encouraged to understand the math
    behind the wonderful world of ML algorithms. It is the culmination of years of
    research and effort by scientists and enthusiasts such as yourselves that we have
    the capability today to potentially predict the future with the help of machines.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we shall dive deep into understanding how you can understand
    if an ML model is performing optimally or not using different statistical measurements
    and other metrics that explain more about ML model performance.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
