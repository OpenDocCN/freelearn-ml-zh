["```py\nfrom sagemaker.tensorflow import TensorFlow\ntraining_job_name='<your-training-job-name>'\nestimator = TensorFlow.attach(training_job_name)\n```", "```py\n    transformer = estimator.transformer(instance_count=1, \n                                        instance_type='ml.c5.xlarge',\n                                        max_payload = 2, # MB\n                                        accept = 'application/jsonlines',\n                                        output_path = s3_output_location,\n                                        assemble_with = 'Line')\n    transformer.transform(test_data_s3, \n                          content_type='text/csv', \n                          split_type = 'Line', \n                          job_name = jobname,\n                          experiment_config = experiment_config)\n    ```", "```py\n    output = transformer.output_path\n    output_prefix = 'imdb_data/test_output'\n    !mkdir -p {output_prefix}\n    !aws s3 cp --recursive {output} {output_prefix}\n    !head {output_prefix}/{csv_test_filename}.out\n    {    \"predictions\": [[0.00371244829], [1.0], [1.0], [0.400452465], [1.0], [1.0], [0.163813606], [0.10115058], [0.793149233], [1.0], [1.0], [6.37737814e-14], [2.10463966e-08], [0.400452465], [1.0], [0.0], [1.0], [0.400452465], [2.65155926e-29], [4.04420768e-11], ……]}\n    ```", "```py\nresults=[]\nwith open(f'{output_prefix}/{csv_test_filename}.out', 'r') as f:\n    lines = f.readlines()\n    for line in lines:\n        print(line)\n        json_output = json.loads(line)\n        result = [float('%.3f'%(item)) for sublist in json_output['predictions'] \n                                       for item in sublist]\n        results += result\nprint(results)\n```", "```py\n    predictor = estimator.deploy(\n                     instance_type='ml.c5.xlarge',\n                     initial_instance_count=1)\n    ```", "```py\n    prediction=predictor.predict(x_test[data_index])\n    print(prediction)\n    {'predictions': [[1.80986511e-11]]}\n    ```", "```py\n    predictor.predict(x_test)\n    ```", "```py\n    sagemaker_client = sess.boto_session.client('sagemaker')\n    autoscaling_client = sess.boto_session.client('application-autoscaling')\n    ```", "```py\n    resource_id=f'endpoint/{endpoint_name}/variant/AllTraffic' \n    response = autoscaling_client.register_scalable_target(\n       ServiceNamespace='sagemaker',\n       ResourceId=resource_id,\n    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n       MinCapacity=1,\n       MaxCapacity=4)\n    ```", "```py\n    response = autoscaling_client.put_scaling_policy(\n       PolicyName='Invocations-ScalingPolicy',\n       ServiceNamespace='sagemaker',\n       ResourceId=resource_id, \n       ScalableDimension='sagemaker:variant:DesiredInstanceCount', \n       PolicyType='TargetTrackingScaling', \n       TargetTrackingScalingPolicyConfiguration={\n           'TargetValue': 4000.0, \n           'PredefinedMetricSpecification': {\n              'PredefinedMetricType': \n                 'SageMakerVariantInvocationsPerInstance'},\n            'ScaleInCooldown': 600, \n            'ScaleOutCooldown': 300})\n    ```", "```py\n    response = autoscaling_client.describe_scaling_policies(\n             ServiceNamespace='sagemaker')\n    for i in response['ScalingPolicies']:\n        print('')\n        print(i['PolicyName'])\n        print('')\n        if('TargetTrackingScalingPolicyConfiguration' in i):\n            print(i['TargetTrackingS calingPolicyConfiguration']) \n        else:\n            print(i['StepScalingPolicyConfiguration'])\n        print('')\n    Invocations-ScalingPolicy\n    {'TargetValue': 4000.0, 'PredefinedMetricSpecification': {'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'}, 'ScaleOutCooldown': 300, 'ScaleInCooldown': 600}\n    ```", "```py\n    df[[\"Int'l Plan\", \"VMail Plan\"]] = df[[\"Int'l Plan\", \"VMail Plan\"]].replace(to_replace=['yes', 'no'], value=[1, 0])\n    df['Churn?'] = df['Churn?'].replace(to_replace=['True.', 'False.'], value=[1, 0])\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    df_train, df_test = train_test_split(df_processed,\n             test_size=0.1, random_state=42, shuffle=True, \n             stratify=df_processed['State'])\n    ```", "```py\n    def launch_training_job(state, train_data_s3, val_data_s3):\n        ...\n        xgb = sagemaker.estimator.Estimator(image, role,\n              instance_count=train_instance_count,\n              instance_type=train_instance_type,\n              output_path=s3_output,\n              enable_sagemaker_metrics=True,\n              sagemaker_session=sess)\n        xgb.set_hyperparameters(\n              objective='binary:logistic',\n              num_round=20)\n\n        ...    \n        xgb.fit(inputs=data_channels, \n                job_name=jobname, \n                experiment_config=experiment_config, \n                wait=False)\n        return xgb\n    ```", "```py\n    dict_estimator = {}\n    for state in df_processed.State.unique()[:5]:\n        print(state)\n        output_dir = f's3://{bucket}/{prefix}/{local_prefix}/by_state'\n        df_state = df_train[df_train['State']==state].drop(labels='State', axis=1)\n        df_state_train, df_state_val = train_test_split(df_state, test_size=0.1, random_state=42, \n                                                        shuffle=True, stratify=df_state['Churn?'])\n\n        df_state_train.to_csv(f'{local_prefix}/churn_{state}_train.csv', index=False)\n        df_state_val.to_csv(f'{local_prefix}/churn_{state}_val.csv', index=False)\n        sagemaker.s3.S3Uploader.upload(f'{local_prefix}/churn_{state}_train.csv', output_dir)\n        sagemaker.s3.S3Uploader.upload(f'{local_prefix}/churn_{state}_val.csv', output_dir)\n\n        dict_estimator[state] = launch_training_job(state, out_train_csv_s3, out_val_csv_s3)\n        time.sleep(2)\n    ```", "```py\n    model_PA = dict_estimator['PA'].create_model(\n           role=role, image_uri=image)\n    mme = MultiDataModel(name=model_name,               \n           model_data_prefix=model_data_prefix,\n           model=model_PA,\n           sagemaker_session=sess)\n    ```", "```py\npredictor = mme.deploy(\n       initial_instance_count=hosting_instance_count, \n       instance_type=hosting_instance_type, \n       endpoint_name=endpoint_name,\n       serializer = CSVSerializer(),\n       deserializer = JSONDeserializer())\n```", "```py\n    for state, est in dict_estimator.items():\n        artifact_path = est.latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts']\n        model_name = f'{state}.tar.gz'\n        mme.add_model(model_data_source=artifact_path, \n                      model_data_path=model_name)\n    ```", "```py\nlist(mme.list_models())\n['MO.tar.gz', 'PA.tar.gz', 'SC.tar.gz', 'VA.tar.gz', 'WY.tar.gz']\n```", "```py\n    state='PA'\n    test_data=sample_test_data(state)\n    prediction = predictor.predict(data=test_data[0], \n                                   target_model=f'{state}.tar.gz')\n    ```", "```py\n    sagemaker_client = sess.boto_session.client('sagemaker')\n    autoscaling_client = sess.boto_session.client('application-autoscaling')\n    endpoint_name = '<endpoint-with-ml.c5-xlarge-instance>'\n    resource_id = f'endpoint/{endpoint_name}/variant/AllTraffic' \n    response = autoscaling_client.register_scalable_target(\n       ServiceNamespace='sagemaker',\n       ResourceId=resource_id,\n       ScalableDimension='sagemaker:variant:   DesiredInstanceCount',\n       MinCapacity=1,\n       MaxCapacity=1)\n    ```", "```py\n    %%sh --bg\n    export ENDPOINT_NAME='<endpoint-with-ml.c5-xlarge-instance>'\n    bind_port=5557\n    locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} & \n    locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} &\n    locust -f load_testing/locustfile.py --headless -u 500 -r 10 -t 60s \\\n           --print-stats --only-summary --loglevel ERROR \\\n           --autostart --autoquit 10 --master --expect-workers 2 --master-bind-port ${bind_port}\n    ```", "```py\n    from sagemaker.tensorflow import TensorFlow\n    training_job_name='<your-training-job-name>'\n    estimator = TensorFlow.attach(training_job_name)\n    predictor_c5_2xl = estimator.deploy(\n              initial_instance_count=1, \n              instance_type='ml.c5.2xlarge')\n    ```", "```py\n    export ENDPOINT_NAME='<endpoint-with-ml.c5-2xlarge-instance>'\n    ```", "```py\n    predictor_g4dn_xl = estimator.deploy(\n             initial_instance_count=1,        \n             instance_type='ml.g4dn.xlarge')\n    ```", "```py\n    endpoint_name = '<endpoint-with-ml.c5-xlarge-instance>'\n    resource_id=f'endpoint/{endpoint_name}/variant/AllTraffic'\n    response = autoscaling_client.register_scalable_target(\n       ServiceNamespace='sagemaker',\n       ResourceId=resource_id,\n       ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n       MinCapacity=1,\n       MaxCapacity=4)\n    ```"]