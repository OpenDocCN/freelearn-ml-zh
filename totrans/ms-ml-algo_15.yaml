- en: Advanced Policy Estimation Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will continue our exploration of the world of **Reinforcement
    Learning** (**RL**), focusing our attention on complex algorithms that can be
    employed to solve difficult problems. As this is still the introductory part of
    RL (the whole topic is extremely large), the structure of the chapter is based
    on many practical examples that can be used as a basis to work on more complex
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that will be discussed in this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: TD(λ) algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Action-Critic TD(0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SARSA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q-learning with a simple visual input and a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TD(λ) algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we introduced the temporal difference strategy, and
    we discussed a simple example called TD(0). In the case of TD(0), the discounted
    reward is approximated by using a one-step backup. Hence, if the agent performs
    an action *a[t]* in the state *s[t]*, and the transition to the state *s[t][+1]*
    is observed, the approximation becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa44f3c3-5483-4558-a869-23c89e357ce9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the task is episodic (as in many real-life scenarios) and has *T(e[i])*
    steps, the complete backup for the episode *e[i]* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99c8410c-cb8c-4dd6-8873-2a1648151699.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous expression ends when the MDP process reaches an absorbing state;
    therefore, *R[t]* is the actual value of the discounted reward. The difference
    between TD(0) and this choice is clear: in the first case, we can update the value
    function after each transition, whereas with a complete backup, we need to wait
    for the end of the episode. We can say that this method (which is called Monte
    Carlo, because it''s based on the idea of averaging the overall reward of an entire
    sequence) is exactly the opposite of TD(0); therefore, it''s reasonable to think
    about an intermediate solution, based on *k*-step backups. In particular, our
    goal is to find an online algorithm that can exploit the backups once they are
    available.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s imagine a sequence of four steps. The agent is in the first state and
    observes a transition; at this point, only a one-step backup is possible, and
    it''s a good idea to update the value function in order to improve the convergence
    speed. After the second transition, the agent can use a two-step backup; however,
    it can also consider the first one-step backup in addition to the newer, longer
    one. So, we have two approximations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b661dfb8-7ffc-4f7f-a4fc-45b5503b9c5b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Which of the preceding is the most reliable? Obviously, the second one depends
    on the first one (in particular, when the value function is almost stabilized),
    and so on until the end of the episode. Hence, the most common strategy is to
    employ a weighted average that assigns a different level of importance to each
    backup (assuming the longest backup has *k* steps):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48597f36-c27f-4ee8-919c-dd9aa6a04017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Watkins (in *Learning from Delayed Rewards*, *Watkins C.I.C.H.*, *Ph.D. Thesis*,
    *University of Cambridge*, *1989*) proved that this approach (with or without
    averaging) has the fundamental property of reducing the absolute error of the
    expected *R[t]^k*, with respect to the optimal value function, *V(s; π)*. In fact,
    he proved that the following inequality holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0fda46a-9ff2-4faf-a08b-b5303e14a459.png)'
  prefs: []
  type: TYPE_IMG
- en: As *γ* is bounded between 0 and 1, the right-hand side is always smaller than
    the maximum absolute error *V(t) - V(s;π)*, where *V(s)* is the value of a state
    during an episode. Therefore, the expected discounted return of a *k*-step backup
    (or of a combination of different backups) yields a more accurate estimation of
    the optimal value function if the policy is chosen to be greedy with respect to
    it. This is not surprising, as a longer backup incorporates more actual returns,
    but the importance of this theorem resides in its validity when an average of
    different *k*-step backups are employed. In other words, it provides us with the
    mathematical proof that an intuitive approach actually converges, and it can also
    effectively improve both the convergence speed and the final accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, managing *k* coefficients is generally problematic, and in many cases,
    useless. The main idea behind TD(λ) is to employ a single factor, *λ*, that can
    be tuned in order to meet specific requirements. The theoretical analysis (or
    *forward view*, as referred to by Sutton and Barto) is based, in a general case,
    on an exponentially decaying average. If we consider a geometric series with *λ*
    bounded between 0 and 1 (exclusive), we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e979110-e2a8-4991-8e52-26b61805f81b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, we can consider the averaged discounted return *R[t]^((λ))* with infinite
    backups as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fae3add0-ff56-49ca-9f3c-1549039fb66f.png)'
  prefs: []
  type: TYPE_IMG
- en: Before defining the finite case, it's helpful to understand how *R[t]^((λ))*
    was built. As *λ* is bounded between 0 and 1, the factors decay proportionally
    to *λ*, so the first backup has the highest impact, and all of the subsequent
    ones have smaller and smaller influences on the estimation. This means that, in
    general, we are assuming that the estimation of *R[t]* has more importance to
    the *immediate* backups (which become more and more precise), and we exploit the
    longer ones only to improve the estimated value. Now, it should be clear that *λ*
    = 0 is equivalent to TD(0), because only the one-step backup remains in the sum
    (remember that *0⁰ = 1*), while higher values involve all of the remaining backups. Let's
    now consider an episode *e[i]* whose length is *T(e[i])*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conventionally, if the agent reached an absorbing state at *t = T(e[i])*, all
    of the remaining *t*+*i* returns are equal to *R*[*t* ](this is straightforward,
    as all of the possible rewards have already been collected); therefore, we can
    truncate *R[t]^((λ))*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe22f4da-626c-4928-ab5c-6d920feedfba.png)'
  prefs: []
  type: TYPE_IMG
- en: The first term of the previous expression involves all of the non-terminal states,
    while the second is equal to *R[t]* discounted proportionally to the distance
    between the first time step and the final state. Again, if *λ* = 0, we obtain
    TD(0), but we are now also authorized to consider *λ* = *1* (because the sum is
    always extended to a finite number of elements). When λ = 1, we obtain *R[t]^((λ))*
    = *R[t]*, which means that we need to wait until the end of the episode to get
    the actual discounted reward. As explained previously, this method is normally
    not a first-choice solution, because when the episodes are very long, the agent
    selects the actions with a value function that is not up to date in the majority
    of cases. Therefore, *TD(λ)* is normally employed with *λ* values less than 1,
    in order to obtain the advantage of an online update, together with a correction
    based on the new states. To achieve this goal without looking at the future (we
    want to update *V(s)* as soon as new pieces of information are available), we
    need to introduce the concept of *eligibility trace**e(s)* (sometimes, in the
    context of computational neuroscience, *e(s)* is also called *stimulus trace*).
  prefs: []
  type: TYPE_NORMAL
- en: An eligibility trace for a state *s* is a function of time that returns the
    weight (greater than 0) of the specific state. Let's imagine a sequence, *s[1],
    s[2], ..., s[n]*, and consider a state, *s[i]*. After a backup *V(s[i])* is updated,
    the agent continues its exploration. When is a new update of *s[i]* (given longer
    backups) important? If *s[i]* is not visited anymore, the effect of longer backups
    must be smaller and smaller, and *s[i]* is said to not be eligible for changes
    in *V(s)*. This is a consequence of the previous assumption that shorter backups
    must generally have higher importance. So, if *s[i]* is an initial state (or is
    immediately after the initial state) and the agent moves to other states, the
    effect of *s[i]* must decay. Conversely, if *s[i]* is revisited, it means that
    the previous estimation of *V(s[i])* is probably wrong, and hence *s[i]* is eligible
    for a change. (To better understand this concept, imagine a sequence, *s[1], s[2],
    s[1], ...*. It's clear that when the agent is in *s[1]*, as well as in *s[2]*,
    it cannot select the right action; therefore, it's necessary to reevaluate *V(s)*
    until the agent is able to move forward.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common strategy (which is also discussed in *Reinforcement Learning*, *Sutton
    R. S.*,*‎ Barto A. G.*, *The MIT Press*) is to define the eligibility traces in
    a recursive fashion. After each time step, *e[t](s)* decays by a factor equal
    to *γλ* (to meet the requirement imposed by the forward view); but, when the state
    *s* is revisited, *e[t](s)* is also increased by 1 (*e[t](s)* =* γλe[t-1](s)*
    + *1*). In this way, we impose a jump in the trend of *e(s)* whenever we desire
    to emphasize its impact. However, as *e(s)* decays independently of the jumps,
    the states that are visited and revisited later have a lower impact than the ones
    that are revisited very soon. The reason for this choice is very intuitive: the
    importance of a state revisited after a long sequence is clearly lower than the
    importance of a state that is revisited after a few steps. In fact, the estimation
    of *R[t]* is obviously wrong if the agent moves back and forth between two states
    at the beginning of the episode, but the error becomes less significant when the
    agent revisits a state after having explored other areas. For example, a policy
    can allow an initial phase in order to reach a partial goal, and then it can force
    the agent to move back to reach a terminal state.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploiting the eligibility traces, *TD(λ)* can achieve a very fast convergence
    in more complex environments, with a trade-off between a one-step TD method and
    a Monte Carlo one (which is normally avoided). At this point, the reader might
    wonder if we are sure about the convergence, and luckily, the answer is positive.
    Dayan proved (in* The convergence of TD (λ) for General λ*, *Dayan P.*, *Machine
    Learning 8*, *3–4/1992*) that *TD(λ)* converges for a generic *λ* with only a
    few specific assumptions and the fundamental condition that the policy is GLIE.
    The proof is very technical, and it''s beyond the scope of this book; however,
    the most important assumptions (which are generally met) are:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Markov Decision Process** (**MDP**) has absorbing states (in other words,
    all of the episodes end in a finite number of steps).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the transition probabilities are not-null (all states can be visited
    an infinite number of times).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first condition is obvious, the absence of absorbing states yields infinite
    explorations, which are not compatible with a TD method (sometimes it's possible
    to prematurely end an episode, but this can either be unacceptable (in some contexts)
    or a sub-optimal choice (in many others)). Moreover, Sutton and Barto (in the
    aforementioned book) proved that TD(λ) is equivalent to employing the weighted
    average of discounted return approximations, but without the constraint of looking
    ahead in the future (which is clearly impossible).
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete TD(λ) algorithm (with an optional forced termination of the episode)
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: Set an initial deterministic random policy, *π(s)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial value array, *V(s) = 0 ∀ s ∈ S*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial eligibility trace array, *e(s) = 0 **∀ s ∈ S*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the number of episodes, *N*[*episodes*]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a maximum number of steps per episode, *N*[*max*]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a constant, α (α = *0.1*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a constant, γ (γ = *0.9*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a constant, λ (λ = *0.5*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a counter, *e* = *0*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i* = *1* to *N[episodes]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an empty state list, *L*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the initial state, *s[i]*, and append *s[i]* to *L*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: While *s[j]* is non-terminal and *e < N[max:]*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*e += 1*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the action, *a[t] = **π(s[i])*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the transition, *(a[t], s[i]) → (s[j], r[ij])*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the TD error as *TD[error] = r[ij] + γV(s[j]) - V(s[i])*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Increment the eligibility trace, *e(s[i]) += 1.0*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *s* in *L*:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the value, *V(s) += α · TD[error] · e(s)*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the eligibility trace, *e(s) *= γλ*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *s[i]* = *s*[*j*]
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Append *s[j]* to *L*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the policy to be greedy with respect to the value function, *π(s) = argmax[a] Q(s,
    a)*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The reader can better understand the logic of this algorithm by considering
    the TD error and its back-propagation. Even if this is only a comparison, it's
    possible to imagine the behavior of TD(λ) as similar to the **Stochastic Gradient
    Descent** (**SGD**) algorithms employed to train a neural network. In fact, the
    error is propagated to the previous states (analogous to the lower layers of an
    MLP) and affects them proportionally to their importance, which is defined by
    their eligibility traces. Hence, a state with a higher eligibility trace can be
    considered more responsible for the error; therefore, the corresponding value
    must be corrected proportionally. This isn't a formal explanation, but it can
    simplify comprehension of the dynamics without an excessive loss of rigor.
  prefs: []
  type: TYPE_NORMAL
- en: TD(λ) in a more complex Checkerboard environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we want to test the TD(λ) algorithm with a slightly more complex
    tunnel environment. In fact, together with the absorbing states, we will also
    consider some intermediate positive states, which can be imagined as *checkpoints*.
    An agent should learn the optimal path from any cell to the final state, trying
    to pass through the highest number of checkpoints possible. Let''s start by defining
    the new structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The reward structure is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45b37fed-90f7-4956-8996-7738cbbfb0a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Reward schema in the new tunnel environment
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can proceed to initialize all of the constants (in particular,
    we have chosen *λ = 0.6*, which is an intermediate solution that guarantees an
    awareness close to a Monte Carlo method, without compromising the learning speed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As in Python, the keyword `lambda` is reserved; we used the truncated expression
    `lambd` to declare the constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we want to start from a random cell, we need to repeat the same procedure
    presented in the previous chapter; but, in this case, we are also including the
    checkpoint states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now define the `episode()` function, which implements a complete TD(λ)
    cycle. As we don''t want the agent to roam around trying to pass through the checkpoints
    an infinite number of times, we have decided to reduce the reward during the exploration,
    to incentivize the agent to pass through only the necessary checkpoints—trying,
    at the same time, to reach the final state as soon as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `is_final()` and `policy_selection()` functions are the same ones defined
    in the previous chapter, and need no explanation. Even if it''s not really necessary,
    we have decided to implement a forced termination after a number of steps, equal
    to `max_steps`. This is helpful at the beginning because as the policy is not *ε*-greedy,
    the agent can remain stuck in a looping exploration that never ends. We can now
    train the model for a fixed number of episodes (alternatively, it''s possible
    to stop the process when the value array doesn''t change anymore):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `episode()` function returns the total rewards; therefore, it''s useful
    to check how the agent learning process evolved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08409634-f125-445e-baef-8f952ac52c77.png)'
  prefs: []
  type: TYPE_IMG
- en: Total rewards achieved by the agent
  prefs: []
  type: TYPE_NORMAL
- en: 'At the beginning (for about 500 episodes), the agent employs an unacceptable
    policy that yields very negative total rewards. However, in about 1,000 iterations,
    the algorithm reaches an optimal policy that is only slightly improved by the
    following episodes. The oscillations are due to the different starting points;
    however, the total rewards are never negative, and as the checkpoint weights decay,
    this is a positive signal, indicating that the agent reaches the final positive
    state. To have a confirmation of this hypothesis, we can plot the learned value
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/939757a7-49a4-4c84-8b7e-1d8b10fdeaf7.png)'
  prefs: []
  type: TYPE_IMG
- en: Final value matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'The values are coherent with our initial analysis; in fact, they tend to be
    higher when the cell is close to a checkpoint, but, at the same time, the global
    configuration (considering a policy greedy with respect to *V(s)*) forces the
    agent to reach the ending state whose surrounding values are the highest. The
    last step is checking the actual policy, with a particular focus on the checkpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f326296-76bc-4afa-9773-9b73d6ac581e.png)'
  prefs: []
  type: TYPE_IMG
- en: Final policy
  prefs: []
  type: TYPE_NORMAL
- en: As it's possible to observe, the agent tries to pass through the checkpoints,
    but when it's close to the final state, it (correctly) prefers to end the episode
    as soon as possible. I invite the reader to repeat the experiment using different
    values for the constant *λ*, and changing the environment dynamics for the checkpoints.
    What happens if their values remain the same? Is it possible to improve the policy
    with a higher *λ*?
  prefs: []
  type: TYPE_NORMAL
- en: It's important to remember that, as we are extensively using random values,
    successive experiments can yield different results due to different initial conditions.
    However, the algorithm should always converge to an optimal policy when the number
    of episodes is high enough.
  prefs: []
  type: TYPE_NORMAL
- en: Actor-Critic TD(0) in the checkerboard environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we want to employ an alternative algorithm called *Actor-Critic*,
    together with TD(0). In this method, the agent is split into two components, a
    Critic, which is responsible for evaluating the quality of the value estimation,
    and an actor, which selects and performs an action. As pointed out by Dayan (in
    *Theoretical Neuroscience*, *Dayan P*., *Abbott L*. *F*., *The MIT Press*), the
    dynamics of an Actor-Critic approach are similar to the interleaving policy evaluation
    and policy improvement steps. In fact, the knowledge of the Critic is obtained
    through an iterative process, and its initial evaluations are normally sub-optimal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The structural schema is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ddd636c-72f0-4b0d-a647-c56bc60322dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Actor-Critic schema
  prefs: []
  type: TYPE_NORMAL
- en: 'In this particular case, it''s preferable to employ a *ε*-greedy soft policy,
    based on the softmax function. The model stores a matrix (or an approximating
    function) called *policy importance*, where each entry *p[i](s, a)* is a value
    representing the preference for a specific action in a certain state. The actual
    stochastic policy is obtained by applying the softmax with a simple trick to increase
    the numerical stability when the exponentials become very large:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2711708-7293-46b6-a984-5519dc987c4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After performing the action *a* in the state *s[i]* and observing the transition
    to the state *s[j]* with a reward *r[ij]*, the Critic evaluates the TD error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/412ea371-91ba-4bb1-9eff-9ed4517c265b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If *V(s[i]) < r[ij] + γV(s[j])*, the transition is considered positive, because
    the value is increasing. Conversely, when *V(s[i]) > r[ij] + γV(s[j]**)*, the
    Critic evaluates the action as negative, because the previous value was higher
    than the new estimation. A more general approach is based on the concept of *advantage*,
    which is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/484d36a2-70f6-4ee6-931d-624bf0f71188.png)'
  prefs: []
  type: TYPE_IMG
- en: Normally, one of the terms from the previous expression can be approximated.
    In our case, we cannot compute the *Q* function directly; hence, we approximate
    it with the term *r[ij] + γV(s[j])*. It's clear that the role of the advantage
    is analogous to the one of the TD error (which is an approximation) and must represent
    the confirmation that an action in a certain state is a good or bad choice. An
    analysis of all **advantage Actor-Critic** (**A3C**) algorithms (in other words,
    improvements of the standard *policy gradient* algorithm) is beyond the scope
    of this book. However, the reader can find some helpful pieces of information
    in *High-Dimensional Continuous Control Using Generalized Advantage Estimation*, *Schulman
    J*., *Moritz P*., *Levine S*., *Jordan M*. *I*., *Abbeel P*., *ICLR 2016.*
  prefs: []
  type: TYPE_NORMAL
- en: Of course, an Actor-Critic correction is not sufficient. To improve the policy,
    it's necessary to employ a standard algorithm (such as TD(0), TD(λ), or least
    square regression, which can be implemented using a neural network) in order to
    learn the correct value function, *V(s)*. As for many other algorithms, this process
    can converge only after a sufficiently high number of iterations, which must be
    exploited to visit the states many times, experimenting with all possible actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, with a TD(0) approach, the first step after evaluating the TD error
    is updating *V(s)* using the rule defined in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c695ed0f-e214-4e0d-8e35-f3e2b1727564.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second step is more pragmatic; in fact, the main role of the Critic is
    actually to criticize every action, deciding when it''s better to increase or
    decrease the probability of selecting it again in a certain state. This goal can
    be achieved by simply updating the policy importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63fdf0cc-74d2-4e4e-b770-ccf7aa03529a.png)'
  prefs: []
  type: TYPE_IMG
- en: The role of the learning rate *ρ* is extremely important; in fact, incorrect
    values (in other words, values that are too high) can yield initial wrong corrections
    that may compromise the convergence. It's essential to not forget that the value
    function is almost completely unknown at the beginning, and therefore the Critic
    has no chance to increase the right probability with awareness. For this reason,
    I always suggest to start with very small value (*ρ = 0.001*) and increase it
    only if the convergence speed of the algorithm is effectively improved.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the policy is based on the softmax function, after a Critic update, the
    values will always be renormalized, resulting in an actual probability distribution.
    After an adequately large number of iterations, with the right choice of both *ρ*
    and *γ*, the model is able to learn both a stochastic policy and a value function.
    Therefore, it''s possible to employ the trained agent by always selecting the
    action with the highest probability (which corresponds to an implicitly greedy
    behavior):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7046f44f-1702-4122-b3b1-3154667a5506.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now apply this algorithm to the tunnel environment. The first step is
    defining the constants (as we are looking for a long sighted agent, we are setting
    the discount factor *γ = 0.99*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we need to define the policy importance array, and a function
    to generate the softmax policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The functions needed to implement a single training step are very straightforward,
    and the reader should already be familiar with their structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can train the model with 50,000 iterations, and 30,000 explorative
    ones (with a linear decay of the exploration factor):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting greedy policy is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c061246-8480-4b13-ba41-f488adbcd9e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Final greedy policy
  prefs: []
  type: TYPE_NORMAL
- en: The final greedy policy is consistent with the objective, and the agent always
    reaches the final positive state by avoiding the wells. This kind of algorithm
    can appear more complex than necessary; however, in complex situations, it turns
    out to be extremely effective. In fact, the learning process can be dramatically
    improved, thanks to the fast corrections performed by the Critic. Moreover, the
    author has noticed that the Actor-Critic is more robust to wrong (or noisy) evaluations.
    As the policy is learned separately, the effect of small variations in *V(s)*
    cannot easily change the probabilities *π(s, a)* (in particular, when an action
    is generally much *stronger* than the others). On the other hand, as discussed
    previously, it's necessary to avoid a premature convergence in order to let the
    algorithm modify the importance/probabilities, without an excessive number of
    iterations. The right trade-off can be found only after a complete analysis of
    each specific scenario, and unfortunately, there are no general rules that work
    in every case. My suggestion is to test various configurations, starting with
    small values (and, for example, a discount factor of *γ ∈ [0.7, 0.9]*), evaluating
    the total reward achieved after the same exploration period.
  prefs: []
  type: TYPE_NORMAL
- en: 'Complex deep learning models (such as asynchronous A3C; see *Asynchronous Methods
    for Deep Reinforcement Learning*, *Mnih V*., *Puigdomènech Badia A*., *Mirza M*., *Graves
    A*., *Lillicrap T*. *P*., *Harley T*., *Silver D*., *Kavukcuoglu K*., *arXiv:1602.01783
    [cs.LG]* for further information) are based on a single network that outputs both
    the softmax policy (whose actions are generally proportional to their probability)
    and the value. Instead of employing an explicitly *ε*-greedy soft policy, it''s
    possible to add a *maximum-entropy constraint* to the global cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21653b7e-8e87-40d4-9800-65b36bdbe1ba.png)'
  prefs: []
  type: TYPE_IMG
- en: As the entropy is at the maximum when all of the actions have the same probability,
    this constraint (with an appropriate weight) forces the algorithm to increase
    the exploration probability until an action becomes dominant and there's no more
    need to avoid a greedy selection. This is a sound and easy way to employ an *adaptive ε-greedy
    policy*, because as the model works with each state separately, the states where
    the uncertainty is very low can become greedy; it's possible to automatically
    keep a high entropy whenever it's necessary to continue the exploration, in order
    to maximize the reward.
  prefs: []
  type: TYPE_NORMAL
- en: The effect of double correction, together with a maximum-entropy constraint,
    improves the convergence speed of the model, encourages the exploration during
    the initial iterations, and yields very high final accuracy. I invite the reader
    to implement this variant with other scenarios and algorithms. In particular,
    at the end of this chapter, we are going to experiment with an algorithm based
    on a neural network. As the example is pretty simple, I suggest using Tensorflow
    to create a small network based on the Actor-Critic approach. The reader can employ
    a *mean squared error* loss for the value and softmax cross entropy for the policy.
    Once the models work successfully with our toy examples, it will be possible to
    start working with more complex scenarios (like the ones proposed in OpenAI Gym
    at [https://gym.openai.com/](https://gym.openai.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: SARSA algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**SARSA** (whose name is derived from the sequence *state-action-reward-state-action*)
    is a natural extension of TD(0) to the estimation of the *Q* function. Its standard
    formulation (which is sometimes called one-step SARSA, or SARSA(0), for the same
    reasons explained in the previous chapter) is based on a single next reward, *r[t+1]*,
    which is obtained by executing the action *a[t]* in the state *s[t]*. The temporal
    difference computation is based on the following update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/539d7ddc-fad5-4e37-bd39-5d52684fa197.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The equation is equivalent to TD(0), and if the policy is chosen to be GLIE,
    it has been proven (in *Convergence Results for Single-Step On-Policy Reinforcement-Learning
    Algorithms*, *Singh S.*,* Jaakkola T.*,* Littman M. L.*,* Szepesvári C.*,* Machine
    Learning*, *39/2000*) that SARSA converges to an optimal policy, *π^(opt)(s)*,
    with the probability 1, when all couples (state, action) are experienced an infinite
    number of times. This means that if the policy is updated to be greedy with respect
    to the current value function induced by *Q*, it holds that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ba03afc-b4f4-4b09-8055-2e3910d0588a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The same result is valid for the *Q* function. In particular, the most important
    conditions required by the proof are:'
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate, *α ∈ [0, 1]*, with the constraints *Σ**α = ∞* and *Σα² <
    ∞*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variance of the rewards must be finite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first condition is particularly important when *α* is a function of the
    state and the time step; however, in many cases, it is a constant bounded between
    0 and 1, and hence, *Σα² = ∞*. A common way to solve this problem (above all when
    a large number of iterations are required) is to let the learning rate decay (in
    other words, exponentially) during the training process. Instead, to mitigate
    the effect of very large rewards, it's possible to clip them in a suitable range
    (*[-1, 1]*). In many cases, it's not necessary to employ these strategies, but
    in more complex scenarios, they can become crucial in order to ensure the convergence
    of the algorithm. Moreover, as pointed out in the previous chapter, these kinds
    of algorithms need a long exploration phase before starting to stabilize the policy.
    The most common strategy is to employ a *ε*-greedy policy, with a temporal decay
    of the exploration factor. During the first iterations, the agent must explore
    without caring about the returns of the actions. In this way, it's possible to
    assess the actual values before the beginning of a final refining phase characterized
    by a purely greedy exploration, based on a more precise approximation of *V(s)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete SARSA(0) algorithm (with an optional forced termination of the
    episode) is:'
  prefs: []
  type: TYPE_NORMAL
- en: Set an initial deterministic random policy, *π(s)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial value array, *Q(s, a) = 0 ∀ s ∈ S* and *∀ a ∈ A*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the number of episodes, *N*[*episodes*]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a maximum number of steps per episode, *N*[*max*]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a constant, *α* (*α = 0.1*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a constant, *γ* (*γ** = 0.9*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set an initial exploration factor, *ε^((0))* (*ε^((0)) = 1.0*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a policy to let the exploration factor *ε* decay (linear or exponential)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a counter, *e = 0*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i = 1* to *N[episodes]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the initial state, *s*[*i*]
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *s[j]* is non-terminal and *e < N[max]*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*e += 1*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the action, *a[t] = **π(s[i])*, with an exploration factor *ε*^(*(e)*)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the transition, *(a[t], s[i]) → (s[j], r[ij])*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the action, *a[t+1] = π(s[j]**)*, with an exploration factor *ε*^(*(e)*)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the *Q(s[t], a[t])* function (if *s[j]* is terminal, set *Q(s[t+1], a[t+1]**)
    = 0*)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *s[i] = s*[*j*]
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The concept of eligibility trace can also be extended to SARSA (and other TD
    methods); however, that is beyond the scope of this book. A reader who is interested
    can find all of the algorithms (together with their mathematical formulations)
    in *Sutton R. S.,‎ Barto A. G., Reinforcement Learning, A Bradford Book*.
  prefs: []
  type: TYPE_NORMAL
- en: SARSA in the checkerboard environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now test the SARSA algorithm in the original tunnel environment (all
    of the elements that are not redefined are the same as the previous chapter).
    The first step is defining the *Q(s, a)* array and the constants employed in the
    training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As we want to employ a *ε*-greedy policy, we can set the starting point to
    `(0, 0)`, forcing the agent to reach the positive final state. We can now define
    the functions needed to perform a training step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `select_action()` function has been designed to select a random action
    with the probability *ε*, and a greedy one with respect to *Q(s, a)*, with the
    probability *1 - **ε*. The `sarsa_step()` function is straightforward, and executes
    a complete episode updating the *Q(s, a)* (that''s why this is an online algorithm).
    At this point, it''s possible to train the model for 20,000 episodes and employ
    a linear decay for *ε* during the first 15,000 episodes (when t > 15,000, *ε*
    is set equal to 0 in order to employ a purely greedy policy):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, let''s check the learned values (considering that the policy is greedy,
    we''re going to plot *V(s)* *= max[a] Q(s, a)*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f28d7a1-70b6-4a99-973d-2e12cf37714f.png)'
  prefs: []
  type: TYPE_IMG
- en: Final value matrix (as *V(s) = max[a] Q(s, a)*)
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, the Q function has been learned in a consistent way, and we can
    get a confirmation plotting the resulting policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7736ad03-4864-455a-b93d-832fb9048459.png)'
  prefs: []
  type: TYPE_IMG
- en: Final policy
  prefs: []
  type: TYPE_NORMAL
- en: The policy is coherent with the initial objective, and the agent avoids all
    negative absorbing states, always trying to move towards the final positive state.
    However, some paths seem longer than expected. As an exercise, I invite the reader
    to retrain the model for a larger number of iterations, adjusting the exploration
    period. Moreover, *is it possible to improve the model by increasing (or decreasing)
    the discount factor γ?* Remember that *γ → 0* leads to a short-sighted agent,
    which is able to select actions only considering the immediate reward, while*γ →
    1* forces the agent to take into account a larger number of future rewards. This
    particular example is based on a long environment, because the agent always starts
    from *(0, 0)* and must reach the farthest point; therefore, all intermediate states
    have less importance, and it's helpful to look at the future to pick the optimal
    actions. Using random starts can surely improve the policy for all initial states,
    but it's interesting to investigate how different *γ* values can affect the decisions;
    hence, I suggest repeating the experiment in order to evaluate the various configurations
    and increase awareness about the different factors that are involved in a TD algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This algorithm was proposed by Watkins (in *Learning from delayed rewards*,
    *Watkins C.I.C.H.*,* Ph.D. Thesis*, *University of Cambridge*, *1989*; and further
    analyzed in *Watkins C.I.C.H.*, *Dayan P.*, *Technical Note Q-Learning*, *Machine
    Learning 8*, *1992*) as a more efficient alternative to SARSA. The main feature
    of *Q-learning* is that the TD update rule is immediately greedy with respect
    to the *Q(s[t+1], a)* function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d568224d-83a3-42bf-85c1-5b5ae44789d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The key idea is to compare the current *Q(s[t], a[t])* value with the maximum
    *Q* value achievable when the agent is in the successor state. In fact, as the
    policy must be GLIE, the convergence speed can be increased by avoiding wrong
    estimations due to the selection of a *Q* value that won''t be associated with
    the final action. By choosing the maximum *Q* value, the algorithm will move towards
    the optimal solution faster than SARSA, and also, the convergence proof is less
    restrictive. In fact, Watkins and Dayan (in the aforementioned papers) proved
    that, if *|r[i]| < R*, the learning rate *α ∈ [0, 1[* (in this case, α must be
    always smaller than 1) with the same constraints imposed for SARSA (*Σα = ∞* and *Σα² <
    ∞*), then the estimated *Q* function converges with probability 1 to the optimal
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/825eda8d-d7a9-4362-815d-f39440a16f79.png)'
  prefs: []
  type: TYPE_IMG
- en: As discussed for SARSA, the conditions on the rewards and the learning rate
    can be managed by employing a clipping function and a temporal decay, respectively.
    In almost all deep Q-learning applications, these are extremely important factors
    to guarantee the convergence; therefore, I invite the reader to consider them
    whenever the training process isn't able to converge to an acceptable solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete Q-learning algorithm (with an optional forced termination of the
    episode) is:'
  prefs: []
  type: TYPE_NORMAL
- en: Set an initial deterministic random policy, *π(s)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial value array, *Q(s, a) = 0 ∀ s ∈ S* and *∀ a ∈ A*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the number of episodes, *N*[*episodes*]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a maximum number of steps per episode, *N*[*max*]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a constant, *α* (*α = 0.1*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a constant, *γ* (*γ** = 0.9*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set an initial exploration factor, *ε^((0))* (*ε^((0)) = 1.0*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a policy to let the exploration factor *ε* decay (linear or exponential)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a counter, *e = 0*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i = 1* to *N[episodes]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the initial state, *s*[*i*]
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *s[j]* is non-terminal and *e < N[max]*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*e += 1*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the action, *a[t] = **π(s[i])*, with an exploration factor *ε*^(*(e)*)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the transition (*a[t], s[i]*) → (*s[j], r[ij]*)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the action, *a[t+1] = π(s[j]**)*, with an exploration factor *ε*^(*(e)*)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the *Q(s[t], a[t])* function (if *s[j]* is terminal, set *Q(s[t+1], a[t+1]**)
    = 0*) using *max[a] Q(s[t+1], a)*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *s[i] = s*[*j*]
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Q-learning in the checkerboard environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s repeat the previous experiment with the Q-learning algorithm. As all
    of the constants are the same (as well as the choice of a *ε*-greedy policy and
    the starting point set to *(0, 0)*), we can directly define the function that
    implements the training for a single episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train the model for 5,000 iterations, with 3,500 explorative ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting value matrix (defined as in the SARSA experiment) is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b87746f0-69ad-4765-9ce2-70d810465dcc.png)'
  prefs: []
  type: TYPE_IMG
- en: Final value matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, the learned *Q* function (and obviously, also the greedy *V(s)*) is
    coherent with the initial objective (in particular, considering the starting point
    set to *(0, 0)*), and the resulting policy can immediately confirm this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/098a6096-5934-4fb8-9601-78c6f2bbec35.png)'
  prefs: []
  type: TYPE_IMG
- en: Final policy
  prefs: []
  type: TYPE_NORMAL
- en: The behavior of Q-learning is not very different from SARSA (even if the convergence
    is faster), and some initial states are not perfectly managed. This is a consequence
    of our choice; therefore, I invite the reader to repeat the exercise using random
    starts and comparing the training speed of Q-learning and SARSA.
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning using a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we want to test the Q-learning algorithm using a smaller checkerboard
    environment and a neural network (with Keras). The main difference from the previous
    examples is that now, the state is represented by a screenshot of the current
    configuration; hence, the model has to learn how to associate a value with each
    input image and action. This isn''t actual deep Q-learning (which is based on
    Deep Convolutional Networks, and requires more complex environments that we cannot
    discuss in this book), but it shows how such a model can learn an optimal policy
    with the same input provided to a human being. In order to reduce the training
    time, we are considering a square checkerboard environment, with four negative
    absorbing states and a positive final one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'A graphical representation of the rewards is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8795c029-0797-4dd0-ba56-81f67b20ff0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Rewards in the smaller checkerboard environment
  prefs: []
  type: TYPE_NORMAL
- en: 'As we want to provide the network with a graphical input, we need to define
    a function to create a matrix representing the tunnel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `reset_tunnel()` function sets all values equal to 0, except for (which
    is marked with `-1`) and the final state (defined by `0.5`). The position of the
    agent (defined with the value `1`) is directly managed by the training function.
    At this point, we can create and compile our neural network. As the problem is
    not very complex, we are employing an MLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The input is a flattened array, while the output is the *Q* function (all of
    the values corresponding to each action). The network is trained using RMSprop
    and a mean squared error loss function (our goal is to reduce the MSE between
    the actual value and the prediction). In order to train and query the network,
    it''s helpful to create two dedicated functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The behavior of these functions is straightforward. The only element that may
    be new to the reader is the use of the `train_on_batch()` method. Contrary to
    `fit()`, this function allows us to perform a single training step, given a batch
    of input-output couples (in our case, we always have a single couple). As our
    goal is finding an optimal path to the final state, starting from every possible
    cell, we are going to employ random starts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can define the functions needed to perform a single training step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `q_step_neural_network()` function is very similar to the one defined in
    the previous example. The only difference is the management of the visual state.
    Every time there''s a transition, the value `1.0` (denoting the agent) is moved
    from the old position to the new one, and the value of the previous cell is reset
    to its default (saved in the `prev_value` variable). Another secondary difference
    is the absence of *α* because there''s already a learning rate set in the SGD
    algorithm, and it doesn''t make sense to add another parameter to the model. We
    can now train the model for 10,000 iterations, with 7,500 explorative ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'When the training process has finished, we can analyze the total rewards, in
    order to understand whether the network has successfully learned the *Q* functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae1981ff-5be7-44eb-a44e-cce3327b5944.png)'
  prefs: []
  type: TYPE_IMG
- en: Total rewards obtained by the neural network Q-learning algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s clear that the model is working well, because after the exploration period,
    the total reward becomes stationary around `4`, with small oscillations due to
    the different path lengths (however, the final plot can be different because of
    the internal random state employed by Keras). To see a confirmation, let''s generate
    the trajectories for all of the possible initial states, using the greedy policy
    (equivalent to *ε = 0*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Twelve random trajectories are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1324c22-d2fc-442e-9242-04366531da33.png)'
  prefs: []
  type: TYPE_IMG
- en: Twelve trajectories generated using the greedy policy
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent always follows the optimal policy, independent from the initial state,
    and never ends up in a well. Even if the example is quite simple, it''s helpful
    to introduce the reader to the concept of deep Q-learning (for further details,
    the reader can check the introductory paper, *Deep Reinforcement Learning*: *An
    Overview*, *Li Y.*, *arXiv:1701.07274 [cs.LG]*).'
  prefs: []
  type: TYPE_NORMAL
- en: In a general case, the environment can be a more complex game (like Atari or
    Sega), and the number of possible actions is very limited. Moreover, there's no
    possibility to employ random starts, but it's generally a good practice to skip
    a number of initial frames, in order to avoid a bias to the estimator. Clearly,
    the network must be more complex (involving convolutions to better learn the geometric
    dependencies), and the number of iterations must be extremely large. Many other
    tricks and specific algorithms can be employed in order to speed up the convergence,
    but due to a lack of space, they are beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: However, the general process and its logic are almost the same, and it's not
    difficult to understand why some strategies are preferable, and how the accuracy
    can be improved. As an exercise, I invite the reader to create more complex environments,
    with or without checkpoints and stochastic rewards. It's not surprising to see
    how the model will be able to easily learn the dynamics with a sufficiently large
    number of episodes. Moreover, as suggested in the Actor-Critic section, it's a
    good idea to use Tensorflow to implement such a model, comparing the performances
    with Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we presented the natural evolution of TD(0), based on an average
    of backups with different lengths. The algorithm, called TD(λ), is extremely powerful,
    and it assures a faster convergence than TD(0), with only a few (non-restrictive)
    conditions. We also showed how to implement the Actor-Critic method with TD(0),
    in order to learn about both a stochastic policy and a value function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In further sections, we discussed two methods based on the estimation of the
    *Q* function: SARSA and Q-learning. They are very similar, but the latter has
    a greedy approach, and its performance (in particular, the training speed) results
    in it being superior to SARSA. The Q-learning algorithm is one of the most important
    models for the latest developments. In fact, it was the first RL approach employed
    with a Deep Convolutional Network to solve complex environments (like Atari games).
    For this reason, we also presented a simple example, based on an MLP that processes
    a visual input and outputs the *Q* values for each action.'
  prefs: []
  type: TYPE_NORMAL
- en: The world of RL is extremely fascinating, and hundreds of researchers work every
    day to improve algorithms and solve more and more complex problems. I invite the
    reader to check the references in order to find useful resources that can be exploited
    to obtain a deeper understanding of the models and their developments. Moreover,
    I suggest reading the blog posts written by the Google DeepMind team, which is
    one of the pioneers in the field of deep RL. I also suggest searching for the
    papers freely available on *arXiv*.
  prefs: []
  type: TYPE_NORMAL
- en: I'm happy to end this book with this topic, because I believe that RL can provide
    new and more powerful tools that will dramatically change our lives!
  prefs: []
  type: TYPE_NORMAL
