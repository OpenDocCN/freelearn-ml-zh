- en: '*Chapter 4*: Encoding, Transforming, and Scaling Features'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*：编码、转换和缩放特征'
- en: The first three chapters of this book focused on data cleaning, exploration,
    and how to identify missing values and outliers. The next few chapters will delve
    heavily into feature engineering, starting, in this chapter, with techniques to
    encode, transform, and scale data to improve the performance of machine learning
    models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的前三章重点介绍了数据清洗、探索以及如何识别缺失值和异常值。接下来的几章将深入探讨特征工程，本章将从编码、转换和缩放数据以提高机器学习模型性能的技术开始。
- en: Typically, machine learning algorithms require some form of encoding of variables.
    Additionally, our models often perform better with scaling so that features with
    higher variability do not overwhelm the optimization. We will show you how to
    use different scaling techniques when your features have dramatically different
    ranges.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，机器学习算法需要以某种形式对变量进行编码。此外，我们的模型在缩放后通常表现更好，这样具有更高变异性的特征就不会压倒优化过程。我们将向您展示如何在使用特征范围差异很大的情况下使用不同的缩放技术。
- en: 'Specifically, in this chapter, we will explore the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中，我们将探讨以下主要主题：
- en: Creating training datasets and avoiding data leakage
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建训练数据集并避免数据泄露
- en: Identifying irrelevant or redundant observations to be removed
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别要删除的不相关或冗余观察结果
- en: Encoding categorical features
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码分类特征
- en: Encoding features with medium or high cardinality
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用中等或高基数编码特征
- en: Transforming features
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换特征
- en: Binning features
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分箱特征
- en: Scaling features
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征缩放
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will work extensively with the `feature-engine` and `category_encoders`
    packages alongside the `sklearn` library. You can use `pip` to install these packages
    with `pip install feature-engine`, `pip install` `category_encoders`, and `pip
    install scikit-learn`. The code in this chapter uses version 0.24.2 of `sklearn`,
    version 1.1.2 of `feature-engine`, and version 2.2.2 of `category_encoders`. Note
    that either `pip install feature-engine` or `pip install feature_engine` will
    work.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将与`feature-engine`和`category_encoders`包以及`sklearn`库进行大量工作。您可以使用`pip`安装这些包，命令为`pip
    install feature-engine`，`pip install category_encoders`，以及`pip install scikit-learn`。本章中的代码使用了`sklearn`的0.24.2版本，`feature-engine`的1.1.2版本，以及`category_encoders`的2.2.2版本。请注意，无论是`pip
    install feature-engine`还是`pip install feature_engine`都可以工作。
- en: All of the code for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning/tree/main/4.%20PruningEncodingandRescalingFeatures](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning/tree/main/4.%20PruningEncodingandRescalingFeatures).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码都可以在GitHub上找到，链接为[https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning/tree/main/4.%20PruningEncodingandRescalingFeatures](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning/tree/main/4.%20PruningEncodingandRescalingFeatures)。
- en: Creating training datasets and avoiding data leakage
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建训练数据集并避免数据泄露
- en: One of the biggest threats to the performance of our models is data leakage.
    **Data leakage** occurs whenever our models are informed by data that is not in
    the training dataset. Sometimes, we inadvertently assist our model training with
    information that cannot be gleaned from the training data alone and end up with
    an overly rosy assessment of our model's accuracy.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型性能的最大威胁之一是数据泄露。**数据泄露**发生在我们的模型被训练数据集中没有的数据所告知的情况下。有时，我们无意中用无法仅从训练数据中获取的信息帮助我们的模型训练，最终导致我们对模型准确性的评估过于乐观。
- en: Data scientists do not really intend for this to happen, hence the term *leakage*.
    This is not a *don't do it* kind of discussion. We all know not to do it. This
    is more of a *which steps should I take to avoid the problem?* discussion. It
    is actually quite easy to have some data leakage unless we develop routines to
    prevent it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家并不真的希望这种情况发生，因此有了“泄露”这个术语。这不是一种“不要这样做”的讨论。我们都知道不要这样做。这更像是一种“我应该采取哪些步骤来避免这个问题？”的讨论。实际上，除非我们制定预防措施，否则很容易出现数据泄露。
- en: For example, if we have missing values for a feature, we might impute the mean
    across the whole dataset for those values. However, in order to validate our model,
    we subsequently split our data into training and testing datasets. We would then
    have accidentally introduced data leakage into our training dataset since the
    information from the full dataset (that is, the global mean) would have been used.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个特征的缺失值，我们可能会使用整个数据集的平均值来插补这些值。然而，为了验证我们的模型，我们随后将数据分为训练和测试数据集。这样我们就会意外地将来自完整数据集（即全局平均值）的信息引入到训练数据集中。
- en: One of the practices that data scientists have adopted to avoid this is to establish
    separate training and testing datasets as close to the beginning of the analysis
    as possible. This can become a little more complicated with validation techniques
    such as cross-validation, but in the following chapters, we will go over how to
    avoid data leakage in a variety of situations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家为了避免这种情况采取的一种做法是在分析开始尽可能早地建立单独的训练和测试数据集。在交叉验证等验证技术中，这可能会变得稍微复杂一些，但在接下来的章节中，我们将介绍如何在各种情况下避免数据泄露。
- en: We can use scikit-learn to create training and testing DataFrames for the National
    Longitudinal Survey of Youth data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用scikit-learn为**国家纵向青年调查**数据创建训练和测试 DataFrame。
- en: Note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The **National Longitudinal Survey** (**NLS**) of Youth is conducted by the
    United States Bureau of Labor Statistics. This survey started with a cohort of
    individuals in 1997 who were born between 1980 and 1985, with annual follow-ups
    each year through to 2017\. For this section, I pulled 89 variables on grades,
    employment, income, and attitudes toward the government from the hundreds of data
    items within the survey. Separate files for SPSS, Stata, and SAS can be downloaded
    from the repository. The NLS data can be downloaded for public use from [https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**国家纵向青年调查**（**NLS**）由美国劳工统计局进行。这项调查始于1997年，调查对象为1980年至1985年间出生的一批人，每年进行一次年度跟踪调查，直至2017年。在本节中，我从调查中的数百个数据项中提取了89个关于成绩、就业、收入和对政府态度的变量。可以从存储库下载SPSS、Stata和SAS的单独文件。NLS数据可以从[https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search)下载供公众使用。'
- en: 'Let''s start creating the DataFrame:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始创建 DataFrame：
- en: 'First, we import the `train_test_split` module from `sklearn` and load the
    NLS data:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从`sklearn`导入`train_test_split`模块并加载NLS数据：
- en: '[PRE0]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we can create training and testing DataFrames for the features (`X_train`
    and `X_test`) and the targets (`y_train` and `y_test`). In this example, `wageincome`
    is the target variable. We set the `test_size` parameter to `0.3` to leave 30%
    of the observations for testing. Note that we will only work with the **Scholastic
    Assessment Test** (**SAT**) and **grade point average** (**GPA**) data from the
    NLS:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以为特征（`X_train`和`X_test`）和目标（`y_train`和`y_test`）创建训练和测试 DataFrame。在本例中，`wageincome`是目标变量。我们将`test_size`参数设置为`0.3`，以保留30%的观测值用于测试。请注意，我们只将使用NLS中的**学术能力评估测试**（**SAT**）和**平均成绩点**（**GPA**）数据：
- en: '[PRE1]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s take a look at the training DataFrames created with `train_test_split`.
    We get the expected number of observations, 6,288, which is 70% of the total number
    of observations in the NLS DataFrame of 8,984:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看使用`train_test_split`创建的训练 DataFrame。我们得到了预期的观测数，6,288，这是NLS DataFrame中8,984个观测总数的70%：
- en: '[PRE2]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Additionally, let''s look at the testing DataFrames. We get 30% of the total
    number of observations, as expected:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，让我们看看测试 DataFrame。我们得到了预期的30%的观测总数：
- en: '[PRE3]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will use scikit-learn's `test_train_split` to create separate training and
    testing DataFrames in the rest of this chapter. We will introduce more complicated
    strategies for constructing testing datasets for validation in [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078),
    *Preparing for Model Evaluation*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的其余部分使用scikit-learn的`test_train_split`来创建单独的训练和测试 DataFrame。我们将在[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)
    *准备模型评估*中介绍构建验证测试数据集的更复杂策略。
- en: Next, we begin our feature engineering work by removing features that are obviously
    unhelpful. This is because they have the same data as another feature or there
    is no variation in the responses.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们开始我们的特征工程工作，通过移除明显无用的特征。这是因为它们与另一个特征具有相同的数据，或者响应中没有变化。
- en: Removing redundant or unhelpful features
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除冗余或无用的特征
- en: During the process of data cleaning and manipulation, we often end up with data
    that is no longer meaningful. Perhaps we subsetted data based on a single feature
    value, and we have retained that feature even though it now has the same value
    for all observations. Or, for the subset of the data that we are using, two features
    have the same value. Ideally, we catch those redundancies during our data cleaning.
    However, if we do not catch them during that process, we can use the open source
    `feature-engine` package to help us.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据清洗和处理的过程中，我们经常会得到不再有意义的数据。也许我们根据单个特征值对数据进行子集划分，尽管现在所有观测值都具有相同的值，我们仍然保留了该特征。或者，对于我们所使用的数据子集，两个特征具有相同的值。理想情况下，我们在数据清洗过程中捕捉到这些冗余。然而，如果我们在这个过程中没有捕捉到它们，我们可以使用开源的`feature-engine`包来帮助我们。
- en: Additionally, there might be features that are so highly correlated that it
    is very unlikely that we could build a model that could use all of them effectively.
    `feature-engine` has a method, `DropCorrelatedFeatures`, that makes it easy to
    remove a feature when it is highly correlated with another feature.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可能存在高度相关的特征，我们几乎不可能构建一个能够有效使用所有这些特征的模型。`feature-engine`有一个名为`DropCorrelatedFeatures`的方法，它使得在特征高度相关时移除特征变得容易。
- en: In this section, we will work with land temperature data, along with the NLS
    data. Note that we will only load temperature data for Poland here.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将处理陆地温度数据，以及NLS数据。请注意，我们在这里只加载波兰的温度数据。
- en: Data Note
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据备注
- en: The land temperature dataset contains the average temperature readings (in Celsius)
    in 2019 from over 12,000 stations across the world, though the majority of the
    stations are in the United States. The raw data was retrieved from the Global
    Historical Climatology Network integrated database. It has been made available
    for public use by the United States National Oceanic and Atmospheric Administration
    at [https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4](https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 陆地温度数据集包含了2019年来自全球超过12,000个站点的平均温度读数（以摄氏度为单位），尽管大多数站点位于美国。原始数据是从全球历史气候学网络集成数据库中检索的。它已经由美国国家海洋和大气管理局在[https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4](https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4)上提供给公众使用。
- en: 'Let''s start removing redundant and unhelpful features:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始移除冗余和无用的特征：
- en: 'Let''s import the modules we need from `feature_engine` and `sklearn`, and
    load the NLS data and temperature data for Poland. The data from Poland was pulled
    from a larger dataset of 12,000 weather stations across the world. We use `dropna`
    to drop observations with any missing data:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从`feature_engine`和`sklearn`模块中导入所需的模块，并加载波兰的NLS数据和温度数据。波兰的数据是从全球12,000个气象站的大数据集中提取的。我们使用`dropna`来删除任何缺失数据的观测值：
- en: '[PRE4]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we create training and testing DataFrames, as we did in the previous
    section:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建训练和测试DataFrame，就像我们在上一节中所做的那样：
- en: '[PRE5]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can use the pandas `corr` method to see how these features are correlated:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用pandas的`corr`方法来查看这些特征之间的相关性：
- en: '[PRE6]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, `gpaoverall` is highly correlated with `gpascience`, `gpaenglish`, and
    `gpamath`. The `corr` method returns the Pearson coefficients by default. This
    is fine when we can assume a linear relationship between the features. However,
    when this assumption does not make sense, we should consider requesting Spearman
    coefficients instead. We can do that by passing `spearman` to the method parameter
    of `corr`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`gpaoverall`与`gpascience`、`gpaenglish`和`gpamath`高度相关。`corr`方法默认返回皮尔逊相关系数。当我们假设特征之间存在线性关系时，这是可以的。然而，当这个假设没有意义时，我们应该考虑请求Spearman相关系数。我们可以通过将`spearman`传递给`corr`方法的参数来实现这一点。
- en: 'Let''s drop features that have a correlation higher than 0.75 with another
    feature. We pass 0.75 to the `threshold` parameter of `DropCorrelatedFeatures`,
    indicating that we want to use Pearson coefficients and that we want to evaluate
    all the features by setting the variables to `None`. We use the `fit` method on
    the training data and then transform both the training and testing data. The `info`
    method shows that the resulting training DataFrame (`X_train_tr`) has all of the
    features except `gpaoverall`, which has correlations of 0.793 and 0.844 with `gpascience`
    and `gpaenglish`, respectively (`DropCorrelatedFeatures` will evaluate from left
    to right, so if `gpamath` and `gpaoverall` are highly correlated, it will drop
    `gpaoverall`. If `gpaoverall` had been to the left of `gpamath`, it would have
    dropped `gpamath`):'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们删除与另一个特征相关性高于0.75的特征。我们将0.75传递给`DropCorrelatedFeatures`的`threshold`参数，表示我们想要使用皮尔逊相关系数，并且我们想要通过将变量设置为`None`来评估所有特征。我们在训练数据上使用`fit`方法，然后转换训练和测试数据。`info`方法显示，结果训练DataFrame（`X_train_tr`）除了`gpaoverall`以外的所有特征，`gpaoverall`与`gpascience`和`gpaenglish`的相关性分别为0.793和0.844（`DropCorrelatedFeatures`将从左到右进行评估，因此如果`gpamath`和`gpaoverall`高度相关，它将删除`gpaoverall`。如果`gpaoverall`在`gpamath`左侧，它将删除`gpamath`）：
- en: '[PRE7]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Typically, we would evaluate a feature more carefully before deciding to drop
    it. However, there are times when feature selection is part of a pipeline, and
    we need to automate the process. This can be done with `DropCorrelatedFeatures`
    since all of the `feature_engine` methods can be brought into a scikit-learn pipeline.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们在决定删除特征之前会更仔细地评估特征。然而，有时特征选择是管道的一部分，我们需要自动化这个过程。这可以通过`DropCorrelatedFeatures`来实现，因为所有的`feature_engine`方法都可以被纳入scikit-learn管道。
- en: 'Now, let''s create training and testing DataFrames from the land temperature
    data for Poland. The value of `year` is the same for all observations, as is the
    value for `country`. Additionally, the value for `latabs` is the same as it is
    for `latitude` for each observation:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们从波兰的陆地温度数据中创建训练和测试DataFrame。`year`的值对所有观测值都是相同的，`country`的值也是如此。此外，对于每个观测值，`latabs`的值与`latitude`相同：
- en: '[PRE8]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s drop features with the same values throughout the training dataset.
    Notice that `year` and `country` are removed after the transform:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们删除在整个训练数据集中具有相同值的特征。注意，在转换后删除了`year`和`country`：
- en: '[PRE9]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s drop features that have the same values as other features. In this case,
    the transform drops `latitude`, which has the same values as `latabs`:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们删除具有与其他特征相同值的特征。在这种情况下，转换删除了`latitude`，因为它与`latabs`具有相同的值：
- en: '[PRE10]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This fixes some obvious problems with our features in the NLS data and the land
    temperature data for Poland. We dropped `gpaoverall` from a DataFrame that has
    the other GPA features because it is highly correlated with them. Additionally,
    we removed redundant data, dropping features with the same value throughout the
    DataFrame and features that duplicate the values of another feature.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这解决了NLS数据中我们的特征和波兰陆地温度数据中的一些明显问题。我们从包含其他GPA特征的DataFrame中删除了`gpaoverall`，因为它与它们高度相关。此外，我们删除了冗余数据，删除了在整个DataFrame中具有相同值的特征以及重复另一个特征值的特征。
- en: 'The rest of this chapter explores somewhat messier feature engineering challenges:
    encoding, transforming, binning, and scaling.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分探讨了某些较为混乱的特征工程挑战：编码、转换、分箱和缩放。
- en: Encoding categorical features
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对分类特征进行编码
- en: There are several reasons why we might need to encode features before using
    them in most machine learning algorithms. First, these algorithms typically require
    numeric data. Second, when a categorical feature *is* represented with numbers,
    for example, 1 for female and 2 for male, we need to encode the values so that
    they are recognized as categorical. Third, the feature might actually be ordinal,
    with a discrete number of values that represent some meaningful ranking. Our models
    need to capture that ranking. Finally, a categorical feature might have a large
    number of values (known as high cardinality), and we might want our encoding to
    collapse categories.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能需要在使用大多数机器学习算法之前对特征进行编码的几个原因。首先，这些算法通常需要数值数据。其次，当一个分类特征*是*用数字表示时，例如，女性为1，男性为2，我们需要对这些值进行编码，以便它们被识别为分类数据。第三，该特征实际上可能是有序的，具有代表某些有意义排名的离散数值。我们的模型需要捕捉这种排名。最后，一个分类特征可能具有大量值（称为高基数），我们可能希望我们的编码能够合并类别。
- en: We can handle the encoding of features with a limited number of values, say
    15 or less, with one-hot encoding. In this section, we will, first, go over one-hot
    encoding and then discuss ordinal encoding. We will look at strategies for handling
    categorical features with high cardinality in the next section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用独热编码来处理具有有限值的特征，例如 15 或更少。在本节中，我们首先将介绍独热编码，然后讨论顺序编码。在下一节中，我们将探讨处理具有高基数类别特征的战略。
- en: One-hot encoding
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独热编码
- en: 'One-hot encoding a feature creates a binary vector for each value of that feature.
    So, if a feature, called *letter*, has three unique values, *A*, *B*, and *C*,
    one-hot encoding creates three binary vectors to represent those values. The first
    binary vector, which we can call *letter_A,* has 1 whenever *letter* has a value
    of *A*, and 0 when it is *B* or *C*. *letter_B* and *letter_C* would be coded
    similarly. The transformed features, *letter_A*, *letter_B*, and *letter_C*, are
    often referred to as **dummy variables**. *Figure 4.1* illustrates one-hot encoding:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对特征进行独热编码会为该特征的每个值创建一个二进制向量。因此，如果一个名为 *letter* 的特征有三个唯一值，*A*，*B* 和 *C*，独热编码会创建三个二进制向量来表示这些值。第一个二进制向量，我们可以称之为
    *letter_A*，当 *letter* 的值为 *A* 时为 1，而当它是 *B* 或 *C* 时为 0。*letter_B* 和 *letter_C*
    的编码方式类似。转换后的特征，*letter_A*，*letter_B* 和 *letter_C*，通常被称为**虚拟变量**。*图 4.1* 展示了独热编码：
- en: '![Figure 4.1 – The one-hot encoding of a categorical feature ](img/Figure_1.1_B17978.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 类别特征的独热编码](img/Figure_1.1_B17978.jpg)'
- en: Figure 4.1 – The one-hot encoding of a categorical feature
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 类别特征的独热编码
- en: 'A number of features from the NLS data are appropriate for one-hot encoding.
    In the following code blocks, we encode some of those features:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: NLS 数据中的许多特征适合进行独热编码。在下面的代码块中，我们将对其中一些特征进行编码：
- en: 'Let''s start by importing the `OneHotEncoder` module from `feature_engine`
    and loading the data. Additionally, we import the `OrdinalEncoder` module from
    scikit-learn since we will use it later:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入 `feature_engine` 中的 `OneHotEncoder` 模块并加载数据开始。此外，我们还从 scikit-learn 中导入
    `OrdinalEncoder` 模块，因为我们稍后会使用它：
- en: '[PRE11]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we create training and testing DataFrames for the NLS data:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们为 NLS 数据创建训练和测试 DataFrame：
- en: '[PRE12]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'One option we have for the encoding is the pandas `get_dummies` method. We
    can use it to indicate that we want to convert the `gender` and `maritalstatus`
    features. `get_dummies` gives us a dummy variable for each value of `gender` and
    `maritalstatus`. For example, `gender` has the values of `Female` and `Male`.
    `get_dummies` creates a feature, `gender_Female`, which is 1 when `gender` is
    `Female` and 0 when `gender` is `Male`. When `gender` is `Male`, `gender_Male`
    is 1 and `gender_Female` is 0\. This is a tried-and-true method of doing this
    type of encoding and has served statisticians well for many years:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们用于编码的一个选项是 pandas 的 `get_dummies` 方法。我们可以用它来指示我们想要转换 `gender` 和 `maritalstatus`
    特征。`get_dummies` 为 `gender` 和 `maritalstatus` 的每个值提供一个虚拟变量。例如，`gender` 有 `Female`
    和 `Male` 的值。`get_dummies` 创建一个特征，`gender_Female`，当 `gender` 为 `Female` 时为 1，而当
    `gender` 为 `Male` 时为 0。当 `gender` 为 `Male` 时，`gender_Male` 为 1 而 `gender_Female`
    为 0。这是一个经过验证的方法来进行此类编码，并且多年来一直为统计学家提供了良好的服务：
- en: '[PRE13]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We are not saving the DataFrame created by `get_dummies` because, later in this
    section, we will be using a different technique to do the encoding.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有保存由 `get_dummies` 创建的 DataFrame，因为在本节的后面部分，我们将使用不同的技术进行编码。
- en: Typically, we create *k-1* dummy variables for *k* unique values for a feature.
    So, if `gender` has two values in our data, we only need to create one dummy variable.
    If we know the value for `gender_Female`, we also know the value of `gender_Male`;
    therefore, the latter variable is redundant. Similarly, we know the value of `maritalstatus_Divorced`
    if we know the values of the other `maritalstatus` dummies. Creating a redundancy
    in this way is inelegantly referred to as the **dummy variable trap**. To avoid
    this problem, we drop one dummy from each group.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们为特征的 *k* 个唯一值创建 *k-1* 个虚拟变量。因此，如果 `gender` 在我们的数据中有两个值，我们只需要创建一个虚拟变量。如果我们知道
    `gender_Female` 的值，我们也知道 `gender_Male` 的值；因此，后者变量是冗余的。同样，如果我们知道其他 `maritalstatus`
    虚拟变量的值，我们也知道 `maritalstatus_Divorced` 的值。以这种方式创建冗余被不优雅地称为**虚拟变量陷阱**。为了避免这个问题，我们从每个组中删除一个虚拟变量。
- en: Note
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For some machine learning algorithms, such as linear regression, dropping one
    dummy variable is actually required. In estimating the parameters of a linear
    model, the matrix is inverted. If our model has an intercept, and all dummy variables
    are included, the matrix cannot be inverted.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些机器学习算法，例如线性回归，实际上需要删除一个虚拟变量。在估计线性模型的参数时，矩阵会被求逆。如果我们的模型有截距，并且所有虚拟变量都被包含在内，那么矩阵就无法求逆。
- en: 'We can set the `get_dummies` `drop_first` parameter to `True` to drop the first
    dummy from each group:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将`get_dummies`的`drop_first`参数设置为`True`以从每个组中删除第一个虚拟变量：
- en: '[PRE14]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: An alternative to `get_dummies` is the one-hot encoder in either `sklearn` or
    `feature_engine`. These one-hot encoders have the advantage that they can be easily
    brought into a machine learning pipeline, and they can persist information gathered
    from the training dataset to the testing dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_dummies`的一个替代方案是`sklearn`或`feature_engine`中的one-hot编码器。这些one-hot编码器有优势，它们可以轻松地集成到机器学习流程中，并且可以将从训练数据集中收集到的信息持久化到测试数据集中。'
- en: 'Let''s use the `OneHotEncoder` module from `feature_engine` to do the encoding.
    We set `drop_last` to `True` to drop one of the dummies from each group. We fit
    the encoding to the training data and then transform both the training and testing
    data:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用`feature_engine`中的`OneHotEncoder`模块来进行编码。我们将`drop_last`设置为`True`以从每个组中删除一个虚拟变量。我们将编码拟合到训练数据，然后转换训练数据和测试数据：
- en: '[PRE15]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This demonstrates that one-hot encoding is a fairly straightforward way to prepare
    nominal data for a machine learning algorithm. But what if our categorical features
    are ordinal, rather than nominal? In that case, we need to use ordinal encoding.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明one-hot编码是准备名义数据供机器学习算法使用的一种相当直接的方法。但如果我们分类特征是有序的而不是名义的，那会怎样？在这种情况下，我们需要使用有序编码。
- en: Ordinal encoding
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有序编码
- en: Categorical features can be either nominal or ordinal, as discussed in [*Chapter
    1*](B17978_01_ePub.xhtml#_idTextAnchor014), *Examining the Distribution of Features
    and Targets*. Gender and marital status are nominal. Their values do not imply
    order. For example, "never married" is not a higher value than "divorced."
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在[*第一章*](B17978_01_ePub.xhtml#_idTextAnchor014)中讨论的，分类特征可以是名义的或有序的。性别和婚姻状况是名义的。它们的值不表示顺序。例如，“未婚”并不比“离婚”的值高。
- en: However, when a categorical feature is ordinal, we want the encoding to capture
    the ranking of the values. For example, if we have a feature that has the values
    of low, medium, and high, one-hot encoding would lose this ordering. Instead,
    a transformed feature with the values of 1, 2, and 3 for low, medium, and high,
    respectively, would be better. We can accomplish this with ordinal encoding.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当一个分类特征是有序的时，我们希望编码能够捕捉到值的排序。例如，如果我们有一个具有低、中、高值的特征，one-hot编码会丢失这个排序。相反，一个具有低、中、高分别为1、2、3的转换特征会更好。我们可以通过有序编码来实现这一点。
- en: 'The college enrollment feature on the NLS dataset can be considered an ordinal
    feature. The values range from *1\. Not enrolled* to *3\. 4-year college*. We
    should use ordinal encoding to prepare it for modeling. We will do that next:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: NLS数据集中的大学入学特征可以被认为是有序特征。其值范围从*1. 未入学*到*3. 四年制大学*。我们应该使用有序编码来为建模做准备。我们将在下一步做这件事：
- en: 'We can use the `OrdinalEncoder` module of `sklearn` to encode the college enrollment
    for 1999 feature. First, let''s take a look at the values of `colenroct99` prior
    to encoding. The values are strings, but there is an implied order:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`sklearn`的`OrdinalEncoder`模块来对1999年的大学入学特征进行编码。首先，让我们看一下编码前的`colenroct99`的值。这些值是字符串，但存在隐含的顺序：
- en: '[PRE16]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can tell the `OrdinalEncoder` module to rank the values in the same order
    by passing the preceding array into the `categories` parameter. Then, we can use
    `fit_transform` to transform the college enrollment field, `colenroct99`. (The
    `fit_transform` method of the `sklearn` `OrdinalEncoder` module returns a NumPy
    array, so we need to use the pandas DataFrame method to create a DataFrame.) Finally,
    we join the encoded features with the other features from the training data:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过将前面的数组传递给`categories`参数来告诉`OrdinalEncoder`模块按相同的顺序对值进行排序。然后，我们可以使用`fit_transform`来转换大学入学字段`colenroct99`。（`sklearn`的`OrdinalEncoder`模块的`fit_transform`方法返回一个NumPy数组，因此我们需要使用pandas
    DataFrame方法来创建一个DataFrame。）最后，我们将编码后的特征与训练数据中的其他特征合并：
- en: '[PRE17]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s take a look at a few observations of the resulting DataFrame. Additionally,
    we should compare the counts of the original college enrollment feature to the
    transformed feature:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看结果 DataFrame 的几个观察结果。此外，我们还应该比较原始大学入学特征计数与转换特征计数：
- en: '[PRE18]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The ordinal encoding replaces the initial values for `colenroct99` with numbers
    from 0 to 2\. It is now in a form that is consumable by many machine learning
    models, and we have retained the meaningful ranking information.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 序列编码将 `colenroct99` 的初始值替换为从 0 到 2 的数字。现在它以许多机器学习模型可消费的形式存在，并且我们保留了有意义的排名信息。
- en: Note
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Ordinal encoding is appropriate for non-linear models such as decision trees.
    It might not make sense in a linear regression model because that would assume
    that the distance between values was equally meaningful across the whole distribution.
    In this example, that would assume that the increase from 0 to 1 (that is, from
    no enrollment to 2-year enrollment) is the same thing as the increase from 1 to
    2 (that is, from 2-year enrollment to 4-year enrollment).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 序列编码适用于非线性模型，如决策树。在线性回归模型中可能没有意义，因为这会假设在整个分布中值之间的距离具有同等意义。在本例中，这会假设从 0 到 1 的增加（即从无入学到
    2 年入学）与从 1 到 2 的增加（即从 2 年入学到 4 年入学）是同一件事。
- en: One-hot encoding and ordinal encoding are relatively straightforward approaches
    to engineering categorical features. It can be more complicated to deal with categorical
    features when there are many more unique values. In the next section, we will
    go over a couple of techniques for handling those features.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: One-hot 编码和序列编码是工程化分类特征的相对直接的方法。当有更多唯一值时，处理分类特征可能更复杂。在下一节中，我们将介绍处理这些特征的一些技术。
- en: Encoding categorical features with medium or high cardinality
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对中等或高基数的分类特征进行编码
- en: When we are working with a categorical feature that has many unique values,
    say 10 or more, it can be impractical to create a dummy variable for each value.
    When there is high cardinality, that is, a very large number of unique values,
    there might be too few observations with certain values to provide much information
    for our models. At the extreme, with an ID variable, there is just one observation
    for each value.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理具有许多唯一值的分类特征时，例如 10 个或更多，为每个值创建虚拟变量可能是不切实际的。当基数高，即具有非常多的唯一值时，某些值可能观察到的样本太少，无法为我们提供很多信息。在极端情况下，对于
    ID 变量，每个值只有一个观察值。
- en: 'There are a couple of ways in which to handle medium or high cardinality. One
    way is to create dummies for the top *k* categories and group the remaining values
    into an *other* category. Another way is to use feature hashing, also known as
    the hashing trick. In this section, we will explore both strategies. We will be
    using the COVID-19 dataset for this example:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 处理中等或高基数的方法有几个。一种方法是为前 *k* 个类别创建虚拟变量，并将剩余的值组合到一个 *其他* 类别中。另一种方法是使用特征哈希，也称为哈希技巧。在本节中，我们将探讨这两种策略。我们将使用
    COVID-19 数据集作为示例：
- en: 'Let''s create training and testing DataFrames from COVID-19 data, and import
    the `feature_engine` and `category_encoders` libraries:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从 COVID-19 数据中创建训练和测试 DataFrame，并导入 `feature_engine` 和 `category_encoders`
    库：
- en: '[PRE19]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The feature region has 16 unique values, the first 6 of which have counts of
    10 or more:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 特征区域有 16 个唯一值，其中前 6 个的计数为 10 或更多：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can use the `OneHotEncoder` module from `feature_engine` again to encode
    the `region` feature. This time, we use the `top_categories` parameter to indicate
    that we only want to create dummies for the top six category values. Any values
    that do not fall into the top six will have a 0 for all of the dummies:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以再次使用 `feature_engine` 中的 `OneHotEncoder` 模块来编码 `region` 特征。这次，我们使用 `top_categories`
    参数来指示我们只想为前六个类别值创建虚拟变量。任何不属于前六个的值都将为所有虚拟变量设置 0：
- en: '[PRE21]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: An alternative approach to one-hot encoding, when a categorical feature has
    many unique values, is to use **feature hashing**.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当分类特征具有许多唯一值时，一种替代 one-hot 编码的方法是使用 **特征哈希**。
- en: Feature hashing
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征哈希
- en: Feature hashing maps a large number of unique feature values to a smaller number
    of dummy variables. We can specify the number of dummy variables to create. However,
    collisions are possible; that is, some feature values might map to the same dummy
    variable combination. The number of collisions increases as we decrease the number
    of requested dummy variables.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 特征哈希将大量的唯一特征值映射到更少的虚拟变量。我们可以指定要创建的虚拟变量的数量。然而，可能出现冲突；也就是说，一些特征值可能映射到相同的虚拟变量组合。随着我们减少请求的虚拟变量数量，冲突的数量会增加。
- en: 'We can use `HashingEncoder` from `category_encoders` to do feature hashing.
    We use `n_components` to indicate that we want six dummy variables (we copy the
    `region` feature before we do the transform so that we can compare the original
    values to the new dummies):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `category_encoders` 中的 `HashingEncoder` 进行特征哈希。我们使用 `n_components` 来表示我们想要六个虚拟变量（我们在变换之前复制了
    `region` 特征，这样我们就可以将原始值与新的虚拟变量进行比较）：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Unfortunately, this gives us a large number of collisions. For example, Caribbean,
    Central Africa, East Africa, and North Africa all get the same dummy variable
    values. In this case at least, using one-hot encoding and specifying the number
    of categories, as we did in the last section, was a better solution.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这给我们带来了大量的冲突。例如，加勒比海、中非、东非和北非都得到了相同的虚拟变量值。在这种情况下，至少使用独热编码并指定类别数量，就像我们在上一节中所做的那样，是一个更好的解决方案。
- en: 'In the previous two sections, we covered common encoding strategies: one-hot
    encoding, ordinal encoding, and feature hashing. Almost all of our categorical
    features will require some kind of encoding before we can use them in a model.
    However, sometimes, we need to alter our features in other ways, including with
    transformations, binning, and scaling. In the next three sections, we will consider
    the reasons why we might need to alter our features in these ways and explore
    tools for doing that.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两节中，我们介绍了常见的编码策略：独热编码、顺序编码和特征哈希。我们的大部分分类特征在使用模型之前都需要进行某种形式的编码。然而，有时我们需要以其他方式修改我们的特征，包括变换、分箱和缩放。在接下来的三个部分中，我们将考虑我们可能需要以这种方式修改特征的原因，并探讨实现这些修改的工具。
- en: Using mathematical transformations
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数学变换
- en: 'Sometimes, we want to use features that do not have a Gaussian distribution
    with a machine learning algorithm that assumes our features are distributed in
    that way. When that happens, we either need to change our minds about which algorithm
    to use (for example, we could choose KNN rather than linear regression) or transform
    our features so that they approximate a Gaussian distribution. In this section,
    we will go over a couple of strategies for doing the latter:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们希望使用不具有高斯分布的特征，而机器学习算法假设我们的特征是以这种方式分布的。当这种情况发生时，我们可能需要改变我们关于使用哪种算法的想法（例如，我们可以选择
    KNN 而不是线性回归）或者变换我们的特征，使它们近似于高斯分布。在本节中，我们将介绍几种实现后者的策略：
- en: 'We start by importing the transformation module from `feature_engine`, `train_test_split`
    from `sklearn`, and `stats` from `scipy`. Additionally, we create training and
    testing DataFrames with the COVID-19 data:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从 `feature_engine` 导入变换模块，从 `sklearn` 导入 `train_test_split`，从 `scipy` 导入
    `stats`。此外，我们使用 COVID-19 数据创建训练和测试 DataFrame：
- en: '[PRE46]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s take a look at how the total number of cases by country is distributed.
    We should also calculate the skew:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看按国家划分的病例总数是如何分布的。我们还应该计算偏斜：
- en: '[PRE47]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This produces the following histogram:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下直方图：
- en: '![Figure 4.2 – A histogram of the total number of COVID cases ](img/Figure_1.2_B17978.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – COVID 病例总数的直方图](img/Figure_1.2_B17978.jpg)'
- en: Figure 4.2 – A histogram of the total number of COVID cases
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – COVID 病例总数的直方图
- en: This illustrates the very high skew for the total number of cases. In fact,
    it looks log-normal, which is not surprising given the large number of very low
    values and several very high values.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了病例总数的极高偏斜。实际上，它看起来是对数正态分布的，考虑到有大量非常低的值和几个非常高的值，这并不令人惊讶。
- en: Note
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information about the measures of skew and kurtosis, please refer to
    [*Chapter 1*](B17978_01_ePub.xhtml#_idTextAnchor014), *Examining the Distribution
    of Features and Targets*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有关偏斜和峰度的度量方法更多信息，请参阅[*第 1 章*](B17978_01_ePub.xhtml#_idTextAnchor014)，*检查特征和目标分布*。
- en: 'Let''s try a log transformation. All we need to do to get `feature_engine`
    to do the transformation is call `LogTranformer` and pass the feature or features
    that we would like to transform:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试对数变换。我们只需要调用`LogTranformer`并传递我们想要变换的特征或特征即可：
- en: '[PRE48]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This produces the following histogram:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下直方图：
- en: '![Figure 4.3 – A histogram of the total number of COVID cases with log transformation
    ](img/Figure_1.3_B17978.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – 对数变换后的总COVID病例数直方图](img/Figure_1.3_B17978.jpg)'
- en: Figure 4.3 – A histogram of the total number of COVID cases with log transformation
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – 对数变换后的总COVID病例数直方图
- en: Effectively, log transformations increase variability at the lower end of the
    distribution and decrease variability at the upper end. This produces a more symmetrical
    distribution. This is because the slope of the logarithmic function is steeper
    for smaller values than for larger ones.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，对数变换会增加分布下端的变异性，并减少上端的变异性。这会产生一个更对称的分布。这是因为对数函数的斜率对于较小的值比较大的值更陡峭。
- en: 'This is definitely a big improvement, but there is now some negative skew.
    Perhaps a Box-Cox transformation will yield better results. Let''s try that:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这确实是一个很大的改进，但现在有一些负偏斜。也许Box-Cox变换会产生更好的结果。让我们试试：
- en: '[PRE49]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This produces the following plot:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 4.4 – A histogram of the total number of COVID cases with a Box-Cox
    transformation ](img/Figure_1.4_B17978.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – Box-Cox变换后的总COVID病例数直方图](img/Figure_1.4_B17978.jpg)'
- en: Figure 4.4 – A histogram of the total number of COVID cases with a Box-Cox transformation
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – Box-Cox变换后的总COVID病例数直方图
- en: 'Box-Cox transformations identify a value for lambda between -5 and 5 that generates
    a distribution that is closest to normal. It uses the following equation for the
    transformation:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Box-Cox变换确定一个介于-5和5之间的lambda值，该值生成一个与正态分布最接近的分布。它使用以下方程进行变换：
- en: '![](img/B17978_04_001.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_04_001.png)'
- en: or
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '![](img/B17978_04_002.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_04_002.png)'
- en: 'Here, ![](img/B17978_04_003.png) is our transformed feature. Just for fun,
    let''s see the value of the lambda that was used to transform `total_cases`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![图片](img/B17978_04_003.png)是我们的变换特征。为了好玩，让我们看看用于变换`total_cases`的lambda值：
- en: '[PRE50]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The lambda for the Box-Cox transformation is `0.104`. For comparison, the lambda
    for a feature with a Gaussian distribution would be 1.000, meaning that no transformation
    would be necessary.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Box-Cox变换的lambda值为`0.104`。相比之下，具有高斯分布的特征的lambda值为1.000，这意味着不需要进行变换。
- en: Now that our transformed total cases feature looks good, we can build a model
    with it as the target. Additionally, we can set up our pipeline to restore values
    to their original scaling when we make predictions. `feature_engine` has a number
    of other transformations that are implemented similarly to the log and Box-Cox
    transformations.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转换后的总病例特征看起来很好，我们可以用它作为目标来构建模型。此外，我们可以在预测时设置我们的管道以将值恢复到原始缩放。`feature_engine`有其他一些变换，它们的实现方式类似于对数和Box-Cox变换。
- en: Feature binning
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征分箱
- en: 'Sometimes, we will want to convert a continuous feature into a categorical
    feature. The process of creating *k* equally spaced intervals from the minimum
    to the maximum value of a distribution is called **binning** or, the somewhat
    less-friendly term, **discretization**. Binning can address several important
    issues with a feature: skew, excessive kurtosis, and the presence of outliers.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们可能希望将一个连续特征转换为分类特征。从分布的最小值到最大值创建*k*个等间隔区间的过程称为**分箱**，或者，不那么友好的术语，**离散化**。分箱可以解决特征的一些重要问题：偏斜、过度峰度和异常值的存在。
- en: Equal-width and equal-frequency binning
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 等宽和等频分箱
- en: 'Binning might be a good choice with the COVID case data. Let''s try that (this
    might also be useful with other variables in the dataset, including total deaths
    and population, but we will only work with total cases for now. `total_cases`
    is the target variable in the following code, so it is a column – the only column
    – on the `y_train` DataFrame):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在COVID病例数据中，分箱可能是一个不错的选择。让我们试试（这可能在数据集中的其他变量中也很有用，包括总死亡人数和人口，但我们现在只处理总病例数。“total_cases”是以下代码中的目标变量，因此它是一个列——`y_train`
    DataFrame上的唯一列）：
- en: 'First, we need to import `EqualFrequencyDiscretiser` and `EqualWidthDiscretiser`
    from `feature_engine`. Additionally, we need to create training and testing DataFrames
    from the COVID data:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要从`feature_engine`导入`EqualFrequencyDiscretiser`和`EqualWidthDiscretiser`。此外，我们还需要从COVID数据创建训练集和测试集DataFrame：
- en: '[PRE52]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We can use the pandas `qcut` method, and its `q` parameter, to create 10 bins
    of relatively equal frequency:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用pandas的`qcut`方法和其`q`参数来创建10个相对等频的箱子：
- en: '[PRE53]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can accomplish the same thing with `EqualFrequencyDiscretiser`. First, we
    define a function to run the transformation. The function takes a `feature_engine`
    transformation and the training and testing DataFrames. It returns the transformed
    DataFrames (it is not necessary to define a function, but it makes sense here
    since we will repeat these steps later):'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`EqualFrequencyDiscretiser`实现相同的功能。首先，我们定义一个函数来运行转换。该函数接受一个`feature_engine`转换和训练集和测试集DataFrame。它返回转换后的DataFrame（定义函数不是必需的，但在这里这样做是有意义的，因为我们稍后会重复这些步骤）：
- en: '[PRE54]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, we create an `EqualFrequencyDiscretiser` transformer and call the `runtransform`
    function that we just created:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个`EqualFrequencyDiscretiser`转换器，并调用我们刚刚创建的`runtransform`函数：
- en: '[PRE55]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: This gives us the same results as `qcut`, but it has the advantage of being
    easier to bring into a machine learning pipeline since we are using `feature_engine`
    to produce it. The equal-frequency binning addresses both the skew and outlier
    problems.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了与`qcut`相同的结果，但它有一个优点，即更容易将其引入机器学习流程，因为我们使用`feature_engine`来生成它。等频分箱解决了偏斜和异常值问题。
- en: Note
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will explore machine learning pipelines in detail in this book, starting
    with [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing for Model
    Evaluation*. Here, the key point is that feature engine transformers can be a
    part of a pipeline that includes other `sklearn`-compatible transformers, even
    ones we construct ourselves.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书中详细探讨机器学习流程，从[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)，*准备模型评估*开始。在这里，关键点是特征引擎转换器可以是包含其他`sklearn`兼容转换器的流程的一部分，甚至包括我们自己构建的。
- en: '`EqualWidthDiscretiser` works similarly:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`EqualWidthDiscretiser`的工作方式类似：'
- en: '[PRE56]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: This is a far less successful transformation. Almost all of the values are at
    the bottom of the distribution in the data prior to the binning, so it is not
    surprising that equal-width binning would have the same problem. It results in
    only 4 bins, even though we requested 10.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个远不如成功的转换。在分箱之前的数据中，几乎所有值都位于分布的底部，因此等宽分箱会产生相同的问题并不令人惊讶。它只产生了4个箱子，尽管我们请求了10个。
- en: 'Let''s examine the range of each bin. Here, we can see that the equal-width
    binner is not even able to construct equal-width bins because of the small number
    of observations at the top of the distribution:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查每个箱子的范围。在这里，我们可以看到由于分布顶部的观察值数量很少，等宽分箱器甚至无法构建等宽箱子：
- en: '[PRE57]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Although in this case, equal-width binning was a bad choice, there are many
    times when it makes sense. It can be useful when data is more uniformly distributed
    or when the equal widths make sense substantively.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这种情况下，等宽分箱是一个糟糕的选择，但很多时候它是有意义的。当数据分布更均匀或等宽在实质上有意义时，它可能很有用。
- en: K-means binning
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means分箱
- en: Another option is to use *k*-means clustering to determine the bins. The *k*-means
    algorithm randomly selects *k* data points as centers of clusters and then assigns
    the other data points to the closest cluster. The mean of each cluster is computed,
    and the data points are reassigned to the nearest new cluster. This process is
    repeated until the optimal centers are found.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是使用k-means聚类来确定箱子。k-means算法随机选择k个数据点作为聚类的中心，然后将其他数据点分配到最近的聚类。计算每个聚类的平均值，并将数据点重新分配到最近的新的聚类。这个过程重复进行，直到找到最佳中心。
- en: 'When *k*-means is used for binning, all data points in the same cluster will
    have the same ordinal value:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用k-means进行分箱时，同一聚类中的所有数据点将具有相同的序数值：
- en: 'We can use scikit-learn''s `KBinsDiscretizer` to create bins with the COVID
    cases data:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用scikit-learn的`KBinsDiscretizer`使用COVID病例数据创建箱子：
- en: '[PRE58]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Let''s compare the skew and kurtosis of the original total cases variable to
    that of the binned variable. Recall that we would expect a skew of 0 and a kurtosis
    near 3 for a variable with a Gaussian distribution. The distribution of the binned
    variable is much closer to Gaussian:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们比较原始总病例变量的偏斜和峰度与分箱变量的偏斜和峰度。回想一下，对于一个具有高斯分布的变量，我们预计偏斜为0，峰度接近3。分箱变量的分布与高斯分布非常接近：
- en: '[PRE59]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Binning can help us to address skew, kurtosis, and outliers in our data. However,
    it does mask much of the variation in the feature and reduces its explanatory
    potential. Often, some form of scaling, such as min-max or z-score, is a better
    option. Let's examine feature scaling next.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 分箱可以帮助我们解决数据中的偏斜、峰度和异常值。然而，它确实掩盖了特征中的大部分变化，并减少了其解释潜力。通常，某种形式的缩放，如最小-最大或 z 分数，是一个更好的选择。让我们接下来检查特征缩放。
- en: Feature scaling
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征缩放
- en: Often, the features we want to use in our model are on very different scales.
    Put simply, the distance between the minimum and maximum values, or the range,
    varies substantially across possible features. For example, in the COVID-19 data,
    the total cases feature goes from 1 to almost 34 million, while aged 65 or older
    goes from 9 to 27 (the number represents the percentage of the population).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们想在模型中使用的特征在非常不同的尺度上。简单来说，最小值和最大值之间的距离，或者说范围，在可能的特征之间有很大的变化。例如，在 COVID-19
    数据中，总病例特征从 1 到近 3400 万，而 65 岁及以上的人口从 9% 到 27%（数字代表人口百分比）。
- en: Having features on very different scales impacts many machine learning algorithms.
    For example, KNN models often use Euclidean distance, and features with greater
    ranges will have a greater influence on the model. Scaling can address this problem.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 特征尺度差异很大会影响许多机器学习算法。例如，KNN 模型通常使用欧几里得距离，范围更大的特征将对模型产生更大的影响。缩放可以解决这个问题。
- en: 'In this section, we will go over two popular approaches to scaling: **min-max
    scaling** and **standard** (or **z-score**) scaling. Min-max scaling replaces
    each value with its location in the range. More precisely, the following happens:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍两种流行的缩放方法：**最小-最大缩放**和**标准**（或**z 分数**）缩放。最小-最大缩放将每个值替换为其在范围内的位置。更确切地说，以下情况发生：
- en: '![](img/B17978_04_004.png) = ![](img/B17978_04_005.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_04_004.png) = ![](img/B17978_04_005.png)'
- en: Here, ![](img/B17978_04_006.png) is the min-max score, ![](img/B17978_04_007.png)
    is the value for the ![](img/B17978_04_008.png) observation of the ![](img/B17978_04_009.png)
    feature, and ![](img/B17978_04_010.png) and ![](img/B17978_04_011.png) are the
    minimum and maximum values of the ![](img/B17978_04_012.png) feature.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17978_04_006.png) 是最小-最大分数，![](img/B17978_04_007.png) 是 ![](img/B17978_04_008.png)
    观测的 ![](img/B17978_04_009.png) 特征的值，而 ![](img/B17978_04_010.png) 和 ![](img/B17978_04_011.png)
    是该 ![](img/B17978_04_012.png) 特征的最小值和最大值。
- en: 'Standard scaling normalizes the feature values around a mean of 0\. Those who
    studied undergraduate statistics will recognize it as the z-score. Specifically,
    it is as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 标准缩放将特征值标准化到均值为 0 的周围。那些学习过本科统计学的人会将其识别为 z 分数。具体来说，如下所示：
- en: '![](img/B17978_04_013.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_04_013.png)'
- en: Here, ![](img/B17978_04_014.png) is the value for the ![](img/B17978_04_015.png)
    observation of the ![](img/B17978_04_009.png) feature, ![](img/B17978_04_017.png)
    is the mean for feature ![](img/B17978_04_018.png), and ![](img/B17978_04_019.png)
    is the standard deviation for that feature.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17978_04_014.png) 是 ![](img/B17978_04_009.png) 特征的 ![](img/B17978_04_015.png)
    观测的值，![](img/B17978_04_017.png) 是特征 ![](img/B17978_04_018.png) 的均值，而 ![](img/B17978_04_019.png)
    是该特征的标准差。
- en: 'We can use scikit-learn''s preprocessing module to get the min-max and standard
    scalers:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 scikit-learn 的预处理模块来获取最小-最大和标准缩放器：
- en: 'We start by importing the preprocessing module and creating training and testing
    DataFrames from the COVID-19 data:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入预处理模块，并从 COVID-19 数据中创建训练和测试 DataFrame：
- en: '[PRE60]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now, we can run the min-max scaler. The `fit_transform` method for `sklearn`
    will return a `numpy` array. We convert it into a pandas DataFrame using the columns
    and index from the training DataFrame. Notice how all features now have values
    between 0 and 1:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以运行最小-最大缩放器。`sklearn` 的 `fit_transform` 方法将返回一个 `numpy` 数组。我们使用训练 DataFrame
    的列和索引将其转换为 pandas DataFrame。注意，现在所有特征现在都有介于 0 和 1 之间的值：
- en: '[PRE61]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We run the standard scaler in the same manner:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们以相同的方式运行标准缩放器：
- en: '[PRE62]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'If we have outliers in our data, robust scaling might be a good option. Robust
    scaling subtracts the median from each value of a variable and divides that value
    by the interquartile range. So, each value is as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据中有异常值，鲁棒缩放可能是一个不错的选择。鲁棒缩放从变量的每个值中减去中位数，并将该值除以四分位距。因此，每个值如下所示：
- en: '![](img/B17978_04_020.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_04_020.png)'
- en: Here, ![](img/B17978_04_021.png) is the value of the ![](img/B17978_04_022.png)
    feature, and ![](img/B17978_04_023.png), ![](img/B17978_04_024.png),and ![](img/B17978_04_025.png)
    are the median, third, and first quantiles of the ![](img/B17978_04_026.png) feature.
    Robust scaling is less sensitive to extreme values since it does not use the mean
    or variance.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17978_04_021.png)是![](img/B17978_04_022.png)特征的值，而![](img/B17978_04_023.png)、![](img/B17978_04_024.png)和![](img/B17978_04_025.png)分别是![](img/B17978_04_026.png)特征的均值、第三四分位数和第一四分位数。由于鲁棒缩放不使用均值或方差，因此它对极端值不太敏感。
- en: 'We can use scikit-learn''s `RobustScaler` module to do robust scaling:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用scikit-learn的`RobustScaler`模块来进行鲁棒缩放：
- en: '[PRE63]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: We use feature scaling with most machine learning algorithms. Although it is
    not often required, it yields noticeably better results. Min-max scaling and standard
    scaling are popular scaling techniques, but there are times when robust scaling
    might be the better option.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在大多数机器学习算法中使用特征缩放。尽管它不是经常必需的，但它会产生明显更好的结果。最小-最大缩放和标准缩放是流行的缩放技术，但在某些情况下，鲁棒缩放可能是更好的选择。
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered a wide range of feature engineering techniques.
    We used tools to drop redundant or highly correlated features. We explored the
    most common kinds of encoding – one-hot encoding, ordinal encoding, and hashing
    encoding. Following this, we used transformations to improve the distribution
    of our features. Finally, we used common binning and scaling approaches to address
    skew, kurtosis, and outliers, and to adjust for features with widely different
    ranges.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了广泛的特征工程技术。我们使用了工具来删除冗余或高度相关的特征。我们探讨了最常见的编码类型——独热编码、顺序编码和哈希编码。在此之后，我们使用了转换来改善我们特征的分布。最后，我们使用了常见的分箱和缩放方法来解决偏斜、峰度和异常值，以及调整具有广泛不同范围的特性。
- en: Some of the techniques we discussed in this chapter are required for most machine
    learning models. We almost always need to encode our features for algorithms in
    order to understand them correctly. For example, most algorithms cannot make sense
    of *female* or *male* values or know not to treat ZIP codes as ordinal. Although
    not typically necessary, scaling is often a very good idea when we have features
    with vastly different ranges. When we are using algorithms that assume a Gaussian
    distribution of our features, some form of transformation might be required for
    our features to be consistent with that assumption.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们讨论的一些技术对于大多数机器学习模型是必需的。我们几乎总是需要为算法编码我们的特征以便正确理解它们。例如，大多数算法无法理解*女性*或*男性*值，或者不知道不要将邮编视为有序值。虽然通常不是必需的，但当我们的特征具有非常不同的范围时，缩放通常是一个非常不错的想法。当我们使用假设特征具有高斯分布的算法时，可能需要对特征进行某种形式的转换，以便与该假设保持一致。
- en: We now have a good sense of how our features are distributed, have imputed missing
    values, and have done some feature engineering where necessary. We are now prepared
    to begin perhaps the most interesting and meaningful part of the model building
    process – feature selection.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对特征的分布有了很好的了解，已经填充了缺失值，并在必要时进行了一些特征工程。我们现在准备开始模型构建过程中最有趣和最有意义的一部分——特征选择。
- en: In the next chapter, we will examine key feature selection tasks, building on
    the feature cleaning, exploration, and engineering work that we have done so far.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将检查关键的特征选择任务，这些任务建立在到目前为止我们所做的特征清洗、探索和工程工作之上。
