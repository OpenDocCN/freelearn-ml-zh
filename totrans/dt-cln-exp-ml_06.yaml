- en: '*Chapter 4*: Encoding, Transforming, and Scaling Features'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first three chapters of this book focused on data cleaning, exploration,
    and how to identify missing values and outliers. The next few chapters will delve
    heavily into feature engineering, starting, in this chapter, with techniques to
    encode, transform, and scale data to improve the performance of machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, machine learning algorithms require some form of encoding of variables.
    Additionally, our models often perform better with scaling so that features with
    higher variability do not overwhelm the optimization. We will show you how to
    use different scaling techniques when your features have dramatically different
    ranges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, in this chapter, we will explore the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating training datasets and avoiding data leakage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying irrelevant or redundant observations to be removed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding categorical features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding features with medium or high cardinality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binning features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will work extensively with the `feature-engine` and `category_encoders`
    packages alongside the `sklearn` library. You can use `pip` to install these packages
    with `pip install feature-engine`, `pip install` `category_encoders`, and `pip
    install scikit-learn`. The code in this chapter uses version 0.24.2 of `sklearn`,
    version 1.1.2 of `feature-engine`, and version 2.2.2 of `category_encoders`. Note
    that either `pip install feature-engine` or `pip install feature_engine` will
    work.
  prefs: []
  type: TYPE_NORMAL
- en: All of the code for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning/tree/main/4.%20PruningEncodingandRescalingFeatures](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning/tree/main/4.%20PruningEncodingandRescalingFeatures).
  prefs: []
  type: TYPE_NORMAL
- en: Creating training datasets and avoiding data leakage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the biggest threats to the performance of our models is data leakage.
    **Data leakage** occurs whenever our models are informed by data that is not in
    the training dataset. Sometimes, we inadvertently assist our model training with
    information that cannot be gleaned from the training data alone and end up with
    an overly rosy assessment of our model's accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists do not really intend for this to happen, hence the term *leakage*.
    This is not a *don't do it* kind of discussion. We all know not to do it. This
    is more of a *which steps should I take to avoid the problem?* discussion. It
    is actually quite easy to have some data leakage unless we develop routines to
    prevent it.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have missing values for a feature, we might impute the mean
    across the whole dataset for those values. However, in order to validate our model,
    we subsequently split our data into training and testing datasets. We would then
    have accidentally introduced data leakage into our training dataset since the
    information from the full dataset (that is, the global mean) would have been used.
  prefs: []
  type: TYPE_NORMAL
- en: One of the practices that data scientists have adopted to avoid this is to establish
    separate training and testing datasets as close to the beginning of the analysis
    as possible. This can become a little more complicated with validation techniques
    such as cross-validation, but in the following chapters, we will go over how to
    avoid data leakage in a variety of situations.
  prefs: []
  type: TYPE_NORMAL
- en: We can use scikit-learn to create training and testing DataFrames for the National
    Longitudinal Survey of Youth data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The **National Longitudinal Survey** (**NLS**) of Youth is conducted by the
    United States Bureau of Labor Statistics. This survey started with a cohort of
    individuals in 1997 who were born between 1980 and 1985, with annual follow-ups
    each year through to 2017\. For this section, I pulled 89 variables on grades,
    employment, income, and attitudes toward the government from the hundreds of data
    items within the survey. Separate files for SPSS, Stata, and SAS can be downloaded
    from the repository. The NLS data can be downloaded for public use from [https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start creating the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the `train_test_split` module from `sklearn` and load the
    NLS data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we can create training and testing DataFrames for the features (`X_train`
    and `X_test`) and the targets (`y_train` and `y_test`). In this example, `wageincome`
    is the target variable. We set the `test_size` parameter to `0.3` to leave 30%
    of the observations for testing. Note that we will only work with the **Scholastic
    Assessment Test** (**SAT**) and **grade point average** (**GPA**) data from the
    NLS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a look at the training DataFrames created with `train_test_split`.
    We get the expected number of observations, 6,288, which is 70% of the total number
    of observations in the NLS DataFrame of 8,984:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Additionally, let''s look at the testing DataFrames. We get 30% of the total
    number of observations, as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will use scikit-learn's `test_train_split` to create separate training and
    testing DataFrames in the rest of this chapter. We will introduce more complicated
    strategies for constructing testing datasets for validation in [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078),
    *Preparing for Model Evaluation*.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we begin our feature engineering work by removing features that are obviously
    unhelpful. This is because they have the same data as another feature or there
    is no variation in the responses.
  prefs: []
  type: TYPE_NORMAL
- en: Removing redundant or unhelpful features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the process of data cleaning and manipulation, we often end up with data
    that is no longer meaningful. Perhaps we subsetted data based on a single feature
    value, and we have retained that feature even though it now has the same value
    for all observations. Or, for the subset of the data that we are using, two features
    have the same value. Ideally, we catch those redundancies during our data cleaning.
    However, if we do not catch them during that process, we can use the open source
    `feature-engine` package to help us.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there might be features that are so highly correlated that it
    is very unlikely that we could build a model that could use all of them effectively.
    `feature-engine` has a method, `DropCorrelatedFeatures`, that makes it easy to
    remove a feature when it is highly correlated with another feature.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will work with land temperature data, along with the NLS
    data. Note that we will only load temperature data for Poland here.
  prefs: []
  type: TYPE_NORMAL
- en: Data Note
  prefs: []
  type: TYPE_NORMAL
- en: The land temperature dataset contains the average temperature readings (in Celsius)
    in 2019 from over 12,000 stations across the world, though the majority of the
    stations are in the United States. The raw data was retrieved from the Global
    Historical Climatology Network integrated database. It has been made available
    for public use by the United States National Oceanic and Atmospheric Administration
    at [https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4](https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start removing redundant and unhelpful features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import the modules we need from `feature_engine` and `sklearn`, and
    load the NLS data and temperature data for Poland. The data from Poland was pulled
    from a larger dataset of 12,000 weather stations across the world. We use `dropna`
    to drop observations with any missing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create training and testing DataFrames, as we did in the previous
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the pandas `corr` method to see how these features are correlated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `gpaoverall` is highly correlated with `gpascience`, `gpaenglish`, and
    `gpamath`. The `corr` method returns the Pearson coefficients by default. This
    is fine when we can assume a linear relationship between the features. However,
    when this assumption does not make sense, we should consider requesting Spearman
    coefficients instead. We can do that by passing `spearman` to the method parameter
    of `corr`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s drop features that have a correlation higher than 0.75 with another
    feature. We pass 0.75 to the `threshold` parameter of `DropCorrelatedFeatures`,
    indicating that we want to use Pearson coefficients and that we want to evaluate
    all the features by setting the variables to `None`. We use the `fit` method on
    the training data and then transform both the training and testing data. The `info`
    method shows that the resulting training DataFrame (`X_train_tr`) has all of the
    features except `gpaoverall`, which has correlations of 0.793 and 0.844 with `gpascience`
    and `gpaenglish`, respectively (`DropCorrelatedFeatures` will evaluate from left
    to right, so if `gpamath` and `gpaoverall` are highly correlated, it will drop
    `gpaoverall`. If `gpaoverall` had been to the left of `gpamath`, it would have
    dropped `gpamath`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Typically, we would evaluate a feature more carefully before deciding to drop
    it. However, there are times when feature selection is part of a pipeline, and
    we need to automate the process. This can be done with `DropCorrelatedFeatures`
    since all of the `feature_engine` methods can be brought into a scikit-learn pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create training and testing DataFrames from the land temperature
    data for Poland. The value of `year` is the same for all observations, as is the
    value for `country`. Additionally, the value for `latabs` is the same as it is
    for `latitude` for each observation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s drop features with the same values throughout the training dataset.
    Notice that `year` and `country` are removed after the transform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s drop features that have the same values as other features. In this case,
    the transform drops `latitude`, which has the same values as `latabs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This fixes some obvious problems with our features in the NLS data and the land
    temperature data for Poland. We dropped `gpaoverall` from a DataFrame that has
    the other GPA features because it is highly correlated with them. Additionally,
    we removed redundant data, dropping features with the same value throughout the
    DataFrame and features that duplicate the values of another feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of this chapter explores somewhat messier feature engineering challenges:
    encoding, transforming, binning, and scaling.'
  prefs: []
  type: TYPE_NORMAL
- en: Encoding categorical features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several reasons why we might need to encode features before using
    them in most machine learning algorithms. First, these algorithms typically require
    numeric data. Second, when a categorical feature *is* represented with numbers,
    for example, 1 for female and 2 for male, we need to encode the values so that
    they are recognized as categorical. Third, the feature might actually be ordinal,
    with a discrete number of values that represent some meaningful ranking. Our models
    need to capture that ranking. Finally, a categorical feature might have a large
    number of values (known as high cardinality), and we might want our encoding to
    collapse categories.
  prefs: []
  type: TYPE_NORMAL
- en: We can handle the encoding of features with a limited number of values, say
    15 or less, with one-hot encoding. In this section, we will, first, go over one-hot
    encoding and then discuss ordinal encoding. We will look at strategies for handling
    categorical features with high cardinality in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One-hot encoding a feature creates a binary vector for each value of that feature.
    So, if a feature, called *letter*, has three unique values, *A*, *B*, and *C*,
    one-hot encoding creates three binary vectors to represent those values. The first
    binary vector, which we can call *letter_A,* has 1 whenever *letter* has a value
    of *A*, and 0 when it is *B* or *C*. *letter_B* and *letter_C* would be coded
    similarly. The transformed features, *letter_A*, *letter_B*, and *letter_C*, are
    often referred to as **dummy variables**. *Figure 4.1* illustrates one-hot encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The one-hot encoding of a categorical feature ](img/Figure_1.1_B17978.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – The one-hot encoding of a categorical feature
  prefs: []
  type: TYPE_NORMAL
- en: 'A number of features from the NLS data are appropriate for one-hot encoding.
    In the following code blocks, we encode some of those features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the `OneHotEncoder` module from `feature_engine`
    and loading the data. Additionally, we import the `OrdinalEncoder` module from
    scikit-learn since we will use it later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create training and testing DataFrames for the NLS data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'One option we have for the encoding is the pandas `get_dummies` method. We
    can use it to indicate that we want to convert the `gender` and `maritalstatus`
    features. `get_dummies` gives us a dummy variable for each value of `gender` and
    `maritalstatus`. For example, `gender` has the values of `Female` and `Male`.
    `get_dummies` creates a feature, `gender_Female`, which is 1 when `gender` is
    `Female` and 0 when `gender` is `Male`. When `gender` is `Male`, `gender_Male`
    is 1 and `gender_Female` is 0\. This is a tried-and-true method of doing this
    type of encoding and has served statisticians well for many years:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are not saving the DataFrame created by `get_dummies` because, later in this
    section, we will be using a different technique to do the encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, we create *k-1* dummy variables for *k* unique values for a feature.
    So, if `gender` has two values in our data, we only need to create one dummy variable.
    If we know the value for `gender_Female`, we also know the value of `gender_Male`;
    therefore, the latter variable is redundant. Similarly, we know the value of `maritalstatus_Divorced`
    if we know the values of the other `maritalstatus` dummies. Creating a redundancy
    in this way is inelegantly referred to as the **dummy variable trap**. To avoid
    this problem, we drop one dummy from each group.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For some machine learning algorithms, such as linear regression, dropping one
    dummy variable is actually required. In estimating the parameters of a linear
    model, the matrix is inverted. If our model has an intercept, and all dummy variables
    are included, the matrix cannot be inverted.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can set the `get_dummies` `drop_first` parameter to `True` to drop the first
    dummy from each group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: An alternative to `get_dummies` is the one-hot encoder in either `sklearn` or
    `feature_engine`. These one-hot encoders have the advantage that they can be easily
    brought into a machine learning pipeline, and they can persist information gathered
    from the training dataset to the testing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the `OneHotEncoder` module from `feature_engine` to do the encoding.
    We set `drop_last` to `True` to drop one of the dummies from each group. We fit
    the encoding to the training data and then transform both the training and testing
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This demonstrates that one-hot encoding is a fairly straightforward way to prepare
    nominal data for a machine learning algorithm. But what if our categorical features
    are ordinal, rather than nominal? In that case, we need to use ordinal encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Ordinal encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Categorical features can be either nominal or ordinal, as discussed in [*Chapter
    1*](B17978_01_ePub.xhtml#_idTextAnchor014), *Examining the Distribution of Features
    and Targets*. Gender and marital status are nominal. Their values do not imply
    order. For example, "never married" is not a higher value than "divorced."
  prefs: []
  type: TYPE_NORMAL
- en: However, when a categorical feature is ordinal, we want the encoding to capture
    the ranking of the values. For example, if we have a feature that has the values
    of low, medium, and high, one-hot encoding would lose this ordering. Instead,
    a transformed feature with the values of 1, 2, and 3 for low, medium, and high,
    respectively, would be better. We can accomplish this with ordinal encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'The college enrollment feature on the NLS dataset can be considered an ordinal
    feature. The values range from *1\. Not enrolled* to *3\. 4-year college*. We
    should use ordinal encoding to prepare it for modeling. We will do that next:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `OrdinalEncoder` module of `sklearn` to encode the college enrollment
    for 1999 feature. First, let''s take a look at the values of `colenroct99` prior
    to encoding. The values are strings, but there is an implied order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can tell the `OrdinalEncoder` module to rank the values in the same order
    by passing the preceding array into the `categories` parameter. Then, we can use
    `fit_transform` to transform the college enrollment field, `colenroct99`. (The
    `fit_transform` method of the `sklearn` `OrdinalEncoder` module returns a NumPy
    array, so we need to use the pandas DataFrame method to create a DataFrame.) Finally,
    we join the encoded features with the other features from the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a look at a few observations of the resulting DataFrame. Additionally,
    we should compare the counts of the original college enrollment feature to the
    transformed feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The ordinal encoding replaces the initial values for `colenroct99` with numbers
    from 0 to 2\. It is now in a form that is consumable by many machine learning
    models, and we have retained the meaningful ranking information.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Ordinal encoding is appropriate for non-linear models such as decision trees.
    It might not make sense in a linear regression model because that would assume
    that the distance between values was equally meaningful across the whole distribution.
    In this example, that would assume that the increase from 0 to 1 (that is, from
    no enrollment to 2-year enrollment) is the same thing as the increase from 1 to
    2 (that is, from 2-year enrollment to 4-year enrollment).
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding and ordinal encoding are relatively straightforward approaches
    to engineering categorical features. It can be more complicated to deal with categorical
    features when there are many more unique values. In the next section, we will
    go over a couple of techniques for handling those features.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding categorical features with medium or high cardinality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we are working with a categorical feature that has many unique values,
    say 10 or more, it can be impractical to create a dummy variable for each value.
    When there is high cardinality, that is, a very large number of unique values,
    there might be too few observations with certain values to provide much information
    for our models. At the extreme, with an ID variable, there is just one observation
    for each value.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of ways in which to handle medium or high cardinality. One
    way is to create dummies for the top *k* categories and group the remaining values
    into an *other* category. Another way is to use feature hashing, also known as
    the hashing trick. In this section, we will explore both strategies. We will be
    using the COVID-19 dataset for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create training and testing DataFrames from COVID-19 data, and import
    the `feature_engine` and `category_encoders` libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The feature region has 16 unique values, the first 6 of which have counts of
    10 or more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `OneHotEncoder` module from `feature_engine` again to encode
    the `region` feature. This time, we use the `top_categories` parameter to indicate
    that we only want to create dummies for the top six category values. Any values
    that do not fall into the top six will have a 0 for all of the dummies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: An alternative approach to one-hot encoding, when a categorical feature has
    many unique values, is to use **feature hashing**.
  prefs: []
  type: TYPE_NORMAL
- en: Feature hashing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature hashing maps a large number of unique feature values to a smaller number
    of dummy variables. We can specify the number of dummy variables to create. However,
    collisions are possible; that is, some feature values might map to the same dummy
    variable combination. The number of collisions increases as we decrease the number
    of requested dummy variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `HashingEncoder` from `category_encoders` to do feature hashing.
    We use `n_components` to indicate that we want six dummy variables (we copy the
    `region` feature before we do the transform so that we can compare the original
    values to the new dummies):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, this gives us a large number of collisions. For example, Caribbean,
    Central Africa, East Africa, and North Africa all get the same dummy variable
    values. In this case at least, using one-hot encoding and specifying the number
    of categories, as we did in the last section, was a better solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous two sections, we covered common encoding strategies: one-hot
    encoding, ordinal encoding, and feature hashing. Almost all of our categorical
    features will require some kind of encoding before we can use them in a model.
    However, sometimes, we need to alter our features in other ways, including with
    transformations, binning, and scaling. In the next three sections, we will consider
    the reasons why we might need to alter our features in these ways and explore
    tools for doing that.'
  prefs: []
  type: TYPE_NORMAL
- en: Using mathematical transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, we want to use features that do not have a Gaussian distribution
    with a machine learning algorithm that assumes our features are distributed in
    that way. When that happens, we either need to change our minds about which algorithm
    to use (for example, we could choose KNN rather than linear regression) or transform
    our features so that they approximate a Gaussian distribution. In this section,
    we will go over a couple of strategies for doing the latter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the transformation module from `feature_engine`, `train_test_split`
    from `sklearn`, and `stats` from `scipy`. Additionally, we create training and
    testing DataFrames with the COVID-19 data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a look at how the total number of cases by country is distributed.
    We should also calculate the skew:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – A histogram of the total number of COVID cases ](img/Figure_1.2_B17978.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – A histogram of the total number of COVID cases
  prefs: []
  type: TYPE_NORMAL
- en: This illustrates the very high skew for the total number of cases. In fact,
    it looks log-normal, which is not surprising given the large number of very low
    values and several very high values.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the measures of skew and kurtosis, please refer to
    [*Chapter 1*](B17978_01_ePub.xhtml#_idTextAnchor014), *Examining the Distribution
    of Features and Targets*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try a log transformation. All we need to do to get `feature_engine`
    to do the transformation is call `LogTranformer` and pass the feature or features
    that we would like to transform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – A histogram of the total number of COVID cases with log transformation
    ](img/Figure_1.3_B17978.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – A histogram of the total number of COVID cases with log transformation
  prefs: []
  type: TYPE_NORMAL
- en: Effectively, log transformations increase variability at the lower end of the
    distribution and decrease variability at the upper end. This produces a more symmetrical
    distribution. This is because the slope of the logarithmic function is steeper
    for smaller values than for larger ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is definitely a big improvement, but there is now some negative skew.
    Perhaps a Box-Cox transformation will yield better results. Let''s try that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – A histogram of the total number of COVID cases with a Box-Cox
    transformation ](img/Figure_1.4_B17978.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – A histogram of the total number of COVID cases with a Box-Cox transformation
  prefs: []
  type: TYPE_NORMAL
- en: 'Box-Cox transformations identify a value for lambda between -5 and 5 that generates
    a distribution that is closest to normal. It uses the following equation for the
    transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_04_001.png)'
  prefs: []
  type: TYPE_IMG
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_04_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B17978_04_003.png) is our transformed feature. Just for fun,
    let''s see the value of the lambda that was used to transform `total_cases`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The lambda for the Box-Cox transformation is `0.104`. For comparison, the lambda
    for a feature with a Gaussian distribution would be 1.000, meaning that no transformation
    would be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Now that our transformed total cases feature looks good, we can build a model
    with it as the target. Additionally, we can set up our pipeline to restore values
    to their original scaling when we make predictions. `feature_engine` has a number
    of other transformations that are implemented similarly to the log and Box-Cox
    transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Feature binning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, we will want to convert a continuous feature into a categorical
    feature. The process of creating *k* equally spaced intervals from the minimum
    to the maximum value of a distribution is called **binning** or, the somewhat
    less-friendly term, **discretization**. Binning can address several important
    issues with a feature: skew, excessive kurtosis, and the presence of outliers.'
  prefs: []
  type: TYPE_NORMAL
- en: Equal-width and equal-frequency binning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Binning might be a good choice with the COVID case data. Let''s try that (this
    might also be useful with other variables in the dataset, including total deaths
    and population, but we will only work with total cases for now. `total_cases`
    is the target variable in the following code, so it is a column – the only column
    – on the `y_train` DataFrame):'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import `EqualFrequencyDiscretiser` and `EqualWidthDiscretiser`
    from `feature_engine`. Additionally, we need to create training and testing DataFrames
    from the COVID data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the pandas `qcut` method, and its `q` parameter, to create 10 bins
    of relatively equal frequency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can accomplish the same thing with `EqualFrequencyDiscretiser`. First, we
    define a function to run the transformation. The function takes a `feature_engine`
    transformation and the training and testing DataFrames. It returns the transformed
    DataFrames (it is not necessary to define a function, but it makes sense here
    since we will repeat these steps later):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create an `EqualFrequencyDiscretiser` transformer and call the `runtransform`
    function that we just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This gives us the same results as `qcut`, but it has the advantage of being
    easier to bring into a machine learning pipeline since we are using `feature_engine`
    to produce it. The equal-frequency binning addresses both the skew and outlier
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will explore machine learning pipelines in detail in this book, starting
    with [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing for Model
    Evaluation*. Here, the key point is that feature engine transformers can be a
    part of a pipeline that includes other `sklearn`-compatible transformers, even
    ones we construct ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: '`EqualWidthDiscretiser` works similarly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is a far less successful transformation. Almost all of the values are at
    the bottom of the distribution in the data prior to the binning, so it is not
    surprising that equal-width binning would have the same problem. It results in
    only 4 bins, even though we requested 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s examine the range of each bin. Here, we can see that the equal-width
    binner is not even able to construct equal-width bins because of the small number
    of observations at the top of the distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Although in this case, equal-width binning was a bad choice, there are many
    times when it makes sense. It can be useful when data is more uniformly distributed
    or when the equal widths make sense substantively.
  prefs: []
  type: TYPE_NORMAL
- en: K-means binning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another option is to use *k*-means clustering to determine the bins. The *k*-means
    algorithm randomly selects *k* data points as centers of clusters and then assigns
    the other data points to the closest cluster. The mean of each cluster is computed,
    and the data points are reassigned to the nearest new cluster. This process is
    repeated until the optimal centers are found.
  prefs: []
  type: TYPE_NORMAL
- en: 'When *k*-means is used for binning, all data points in the same cluster will
    have the same ordinal value:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use scikit-learn''s `KBinsDiscretizer` to create bins with the COVID
    cases data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s compare the skew and kurtosis of the original total cases variable to
    that of the binned variable. Recall that we would expect a skew of 0 and a kurtosis
    near 3 for a variable with a Gaussian distribution. The distribution of the binned
    variable is much closer to Gaussian:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Binning can help us to address skew, kurtosis, and outliers in our data. However,
    it does mask much of the variation in the feature and reduces its explanatory
    potential. Often, some form of scaling, such as min-max or z-score, is a better
    option. Let's examine feature scaling next.
  prefs: []
  type: TYPE_NORMAL
- en: Feature scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, the features we want to use in our model are on very different scales.
    Put simply, the distance between the minimum and maximum values, or the range,
    varies substantially across possible features. For example, in the COVID-19 data,
    the total cases feature goes from 1 to almost 34 million, while aged 65 or older
    goes from 9 to 27 (the number represents the percentage of the population).
  prefs: []
  type: TYPE_NORMAL
- en: Having features on very different scales impacts many machine learning algorithms.
    For example, KNN models often use Euclidean distance, and features with greater
    ranges will have a greater influence on the model. Scaling can address this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will go over two popular approaches to scaling: **min-max
    scaling** and **standard** (or **z-score**) scaling. Min-max scaling replaces
    each value with its location in the range. More precisely, the following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_04_004.png) = ![](img/B17978_04_005.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17978_04_006.png) is the min-max score, ![](img/B17978_04_007.png)
    is the value for the ![](img/B17978_04_008.png) observation of the ![](img/B17978_04_009.png)
    feature, and ![](img/B17978_04_010.png) and ![](img/B17978_04_011.png) are the
    minimum and maximum values of the ![](img/B17978_04_012.png) feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Standard scaling normalizes the feature values around a mean of 0\. Those who
    studied undergraduate statistics will recognize it as the z-score. Specifically,
    it is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_04_013.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17978_04_014.png) is the value for the ![](img/B17978_04_015.png)
    observation of the ![](img/B17978_04_009.png) feature, ![](img/B17978_04_017.png)
    is the mean for feature ![](img/B17978_04_018.png), and ![](img/B17978_04_019.png)
    is the standard deviation for that feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use scikit-learn''s preprocessing module to get the min-max and standard
    scalers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the preprocessing module and creating training and testing
    DataFrames from the COVID-19 data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can run the min-max scaler. The `fit_transform` method for `sklearn`
    will return a `numpy` array. We convert it into a pandas DataFrame using the columns
    and index from the training DataFrame. Notice how all features now have values
    between 0 and 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We run the standard scaler in the same manner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we have outliers in our data, robust scaling might be a good option. Robust
    scaling subtracts the median from each value of a variable and divides that value
    by the interquartile range. So, each value is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_04_020.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17978_04_021.png) is the value of the ![](img/B17978_04_022.png)
    feature, and ![](img/B17978_04_023.png), ![](img/B17978_04_024.png),and ![](img/B17978_04_025.png)
    are the median, third, and first quantiles of the ![](img/B17978_04_026.png) feature.
    Robust scaling is less sensitive to extreme values since it does not use the mean
    or variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use scikit-learn''s `RobustScaler` module to do robust scaling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use feature scaling with most machine learning algorithms. Although it is
    not often required, it yields noticeably better results. Min-max scaling and standard
    scaling are popular scaling techniques, but there are times when robust scaling
    might be the better option.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered a wide range of feature engineering techniques.
    We used tools to drop redundant or highly correlated features. We explored the
    most common kinds of encoding – one-hot encoding, ordinal encoding, and hashing
    encoding. Following this, we used transformations to improve the distribution
    of our features. Finally, we used common binning and scaling approaches to address
    skew, kurtosis, and outliers, and to adjust for features with widely different
    ranges.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the techniques we discussed in this chapter are required for most machine
    learning models. We almost always need to encode our features for algorithms in
    order to understand them correctly. For example, most algorithms cannot make sense
    of *female* or *male* values or know not to treat ZIP codes as ordinal. Although
    not typically necessary, scaling is often a very good idea when we have features
    with vastly different ranges. When we are using algorithms that assume a Gaussian
    distribution of our features, some form of transformation might be required for
    our features to be consistent with that assumption.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a good sense of how our features are distributed, have imputed missing
    values, and have done some feature engineering where necessary. We are now prepared
    to begin perhaps the most interesting and meaningful part of the model building
    process – feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will examine key feature selection tasks, building on
    the feature cleaning, exploration, and engineering work that we have done so far.
  prefs: []
  type: TYPE_NORMAL
