<html><head></head><body>
		<div id="_idContainer587">
			<h1 class="chapter-number" id="_idParaDest-153"><a id="_idTextAnchor383"/>7</h1>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor384"/>Classification</h1>
			<p>In machine learning, the task of classification is that of dividing a set of observations (objects) into groups called <strong class="bold">classes</strong>, based <a id="_idIndexMarker808"/>on an analysis of their formal description. In <strong class="bold">classification</strong>, each<a id="_idIndexMarker809"/> observation (object) is assigned to a group or nominal category based on specific qualitative properties. Classification is a supervised task because it requires known classes for training samples. The labeling of a training set is usually done manually, with the involvement of specialists in the given field of study. It’s also notable that if classes are not initially defined, then there will be a problem with clustering. Furthermore, in the classification task, there may be more than two classes (multi-class), and each of the objects may belong to more than one <span class="No-Break">class (intersecting).</span></p>
			<p>In this chapter, we will discuss various approaches to solving a classification task with machine learning. We are going to look at some of the <a id="_idIndexMarker810"/>most well-known and widespread algorithms, which <a id="_idIndexMarker811"/>are logistic regression, <strong class="bold">support vector machine</strong> (<strong class="bold">SVM</strong>), and <strong class="bold">k-nearest neighbors</strong> (<strong class="bold">kNN</strong>). Logistic regression <a id="_idIndexMarker812"/>is one of the most straightforward algorithms based on linear regression and a special loss function. SVM is based on a concept of support vectors that helps to build a decision boundary to separate data. This approach can be effectively used with high-dimensional data. kNN has a simple implementation algorithm that uses the idea of data compactness. Also, we will show how the multi-class classification problem can be solved with the algorithms mentioned previously. We will implement program examples to see how to use these algorithms to solve the classification task with different <span class="No-Break">C++ libraries.</span></p>
			<p>The following topics are covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>An overview of <span class="No-Break">classification methods</span></li>
				<li>Exploring various <span class="No-Break">classification methods</span></li>
				<li>Examples of using C++ libraries for dealing with the <span class="No-Break">classification<a id="_idTextAnchor385"/> <a id="_idTextAnchor386"/>task</span></li>
			</ul>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor387"/>Technical requirements</h1>
			<p>The required technologies and installations for this chapter include <span class="No-Break">the following:</span></p>
			<ul>
				<li>The <span class="No-Break"><strong class="source-inline">mlpack</strong></span><span class="No-Break"> library</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">Dlib</strong></span><span class="No-Break"> library</span></li>
				<li>The <span class="No-Break">Flashlight library</span></li>
				<li>A modern C++ compiler with <span class="No-Break">C++20 support</span></li>
				<li>CMake build system version &gt;= <span class="No-Break">3.10</span></li>
			</ul>
			<p>The code files for this chapter can be found at the following GitHub <span class="No-Break">repo: </span><a href="https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-Edition/tree/main/Chapter07"><span class="No-Break">https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-Edition/tree/main/Chap<span id="_idTextAnchor388"/>te<span id="_idTextAnchor389"/>r07</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor390"/>An overview of classification methods</h1>
			<p>Classification<a id="_idIndexMarker813"/> is a<a id="_idIndexMarker814"/> fundamental task in <strong class="bold">applied statistics</strong>, <strong class="bold">machine learning</strong>, and <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>). This is because classification is one of the <a id="_idIndexMarker815"/>most understandable and easy-to-interpret data analysis <a id="_idIndexMarker816"/>technologies, and classification rules can be formulated in a natural language. In machine learning, a classification task is solved using supervised algorithms because the classes are defined in advance, and the objects in the training set have class labels. Analytical models that solve a classification task are<a id="_idIndexMarker817"/> <span class="No-Break">called </span><span class="No-Break"><strong class="bold">classifiers</strong></span><span class="No-Break">.</span></p>
			<p>Classification is the process of moving an object to a predetermined class based on its formalized features. Each object in this problem is usually represented as a vector in <em class="italic">N</em>-dimensional space. Each dimension in that space is a description of one of the features of <span class="No-Break">the object.</span></p>
			<p>We can formulate the classification task with mathematical notation. Let <em class="italic">X</em> denote the set of descriptions of objects, and <em class="italic">Y</em> be a finite set of names or class labels. There is an unknown objective function—namely, the mapping <img alt="" role="presentation" src="image/B19849_Formula_0011.png"/>, whose values are known only on the objects of the final training sample, <img alt="" role="presentation" src="image/B19849_Formula_0021.png"/>. So, we have to construct an <img alt="" role="presentation" src="image/B19849_Formula_0031.png"/> algorithm, capable of classifying an <img alt="" role="presentation" src="image/B19849_Formula_0041.png"/> arbitrary object. In mathematical statistics, classification problems<a id="_idIndexMarker818"/> are also called discriminant <span class="No-Break">analysis problems.</span></p>
			<p>The classification task is applicable<a id="_idIndexMarker819"/> to many areas, including <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Trade</strong>: The classification of customers and products allows a business to optimize marketing strategies, stimulate sales, and <span class="No-Break">reduce costs</span></li>
				<li><strong class="bold">Telecommunications</strong>: The classification of subscribers allows a business to appraise customer loyalty, and therefore develop <span class="No-Break">loyalty programs</span></li>
				<li><strong class="bold">Medicine and health care</strong>: Assisting the diagnosis of disease by classifying the population into <span class="No-Break">risk groups</span></li>
				<li><strong class="bold">Banking</strong>: The classification of customers is used for <span class="No-Break">credit-scoring procedures</span></li>
			</ul>
			<p>Classification can be solved by <a id="_idIndexMarker820"/>using the <span class="No-Break">following methods:</span></p>
			<ul>
				<li><span class="No-Break">Logistic regression</span></li>
				<li>The <span class="No-Break">kNN method</span></li>
				<li><span class="No-Break">SVM</span></li>
				<li><span class="No-Break">Discriminant analysis</span></li>
				<li><span class="No-Break">Decision trees</span></li>
				<li><span class="No-Break">Neural networks</span></li>
			</ul>
			<p>We looked into discriminant analysis in <a href="B19849_06.xhtml#_idTextAnchor301"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Dimensionality Reduction</em>, as an algorithm for dimensionality reduction, but most libraries provide an <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) for<a id="_idIndexMarker821"/> working with the discriminant analysis algorithm as a classifier, too. We will discuss decision trees in <a href="B19849_09.xhtml#_idTextAnchor496"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Ensemble Learning</em>, focusing on algorithm ensembles. We will also discuss neural networks in the chapter that follows this: <a href="B19849_10.xhtml#_idTextAnchor539"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Neural Networks for </em><span class="No-Break"><em class="italic">Image Classification</em></span><span class="No-Break">.</span></p>
			<p>Now we’ve discussed what the classification task is, let’s look at various <span class="No-Break">classificatio<a id="_idTextAnchor391"/>n<a id="_idTextAnchor392"/> methods.</span></p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor393"/>Exploring various classification methods</h1>
			<p>Nowadays, <strong class="bold">deep learning</strong> has <a id="_idIndexMarker822"/>become increasingly popular for classification tasks too, especially when dealing with complex and high-dimensional data such as images, audio, and text. Deep neural networks can learn hierarchical representations of data that allow them to perform accurate classification. In this chapter, we concentrate on more classical classification approaches because they are still applicable and usually require fewer computational resources. Specifically, we will discuss some of the classification methods<a id="_idIndexMarker823"/> such as logistic regression, <strong class="bold">kernel ridge regression</strong> (<strong class="bold">KRR</strong>), the kNN method, and <span class="No-Break">SVM <a id="_idTextAnchor394"/><a id="_idTextAnchor395"/>approaches.</span></p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor396"/>Logistic regression</h2>
			<p>Logistic regression<a id="_idIndexMarker824"/> determines<a id="_idIndexMarker825"/> the degree of dependence between the categorical dependent and one or more independent variables by using the logistic function. It aims to find the values of the coefficients for the input variables, as with linear regression. The difference, in the case of logistic regression, is that the output value is converted by using a non-linear (logistic) function. The logistic function has an S-shaped curve and converts any value to a number between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. This property is useful because we can apply the rule to the output of the logistic function to bind <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> to a class prediction. The following screenshot shows a logistic <span class="No-Break">function graph:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer450">
					<img alt="Figure 7.1 – Logistic function" src="image/B19849_07_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Logistic function</p>
			<p>For example, if the<a id="_idIndexMarker826"/> result of the function is less than <strong class="source-inline">0.5</strong>, then the output is <strong class="source-inline">0</strong>. Prediction<a id="_idIndexMarker827"/> is not just a simple answer (<strong class="source-inline">+1</strong> or <strong class="source-inline">-1</strong>) either, and we can interpret it as a probability of being classified <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">+1</strong></span><span class="No-Break">.</span></p>
			<p>In many tasks, this interpretation is an essential business requirement. For example, in the task of credit-scoring, where logistic regression is traditionally used, the probability of a loan being defaulted on is a common prediction. As with the case of linear regression, logistic regression performs the task better if outliers and correlating variables are removed. The logistic regression model can be quickly trained and is well-suited for binary <span class="No-Break">classification problems.</span></p>
			<p>The basic<a id="_idIndexMarker828"/> idea of a linear classifier is that the feature space <a id="_idIndexMarker829"/>can be divided by a hyperplane into two half-spaces, in each of which one of the two values of the target class is predicted. If we can divide a feature space without errors, then the training set is called <strong class="bold">linearly separable</strong>. Logistic regression<a id="_idIndexMarker830"/> is a unique type of linear classifier, but it is able to predict the probability of <img alt="" role="presentation" src="image/B19849_Formula_0051.png"/>, attributing the example of <img alt="" role="presentation" src="image/B19849_Formula_006.png"/> to the class <em class="italic">+</em>, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer453">
					<img alt="" role="presentation" src="image/B19849_Formula_007.jpg"/>
				</div>
			</div>
			<p>Consider the task of binary classification, with labels of the target class denoted by <strong class="source-inline">+1</strong> (positive examples) and <strong class="source-inline">-1</strong> (negative examples). We want to predict the probability of <img alt="" role="presentation" src="image/B19849_Formula_0081.png"/>; so, for now, we can build a linear forecast using the following optimization technique: <img alt="" role="presentation" src="image/B19849_Formula_0091.png"/>. So, how do we convert the resulting value into a probability whose limits are <strong class="source-inline">[0, 1]</strong>? This approach requires a specific function. In the logistic regression model, the specific function <img alt="" role="presentation" src="image/B19849_Formula_010.png"/> is used <span class="No-Break">for this.</span></p>
			<p>Let’s denote <em class="italic">P(X)</em> by the probability of the occurring event <em class="italic">X</em>. The probability odds ratio <em class="italic">OR(X)</em> is determined from <img alt="" role="presentation" src="image/B19849_Formula_0111.png"/>. This is the ratio of the probabilities of whether the event will occur or not. We can see that the probability and the odds ratio both contain the same information. However, while <em class="italic">P(X)</em> is in the range of <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>, <em class="italic">OR(X)</em> is in the range of <strong class="source-inline">0</strong> to <img alt="" role="presentation" src="image/B19849_Formula_0121.png"/>. If you calculate the logarithm of <em class="italic">OR(X)</em> (known <a id="_idIndexMarker831"/>as the <strong class="bold">logarithm of the odds</strong>, or<a id="_idIndexMarker832"/> the <strong class="bold">logarithm of the probability ratio</strong>), it is easy to see that the following <span class="No-Break">applies: <img alt="" role="presentation" src="image/B19849_Formula_0131.png"/>.</span></p>
			<p>Using the logistic function to predict the probability of <img alt="" role="presentation" src="image/B19849_Formula_0051.png"/>, it can be obtained from the probability ratio (for the time being, let’s assume we have the weights too) <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer461">
					<img alt="" role="presentation" src="image/B19849_Formula_015.jpg"/>
				</div>
			</div>
			<p>So, the logistic regression predicts the probability of classifying a sample to the "<em class="italic">+</em>" class as a sigmoid transformation of a linear combination of the model weights vector, as well as the sample’s features vector, <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer462">
					<img alt="" role="presentation" src="image/B19849_Formula_016.jpg"/>
				</div>
			</div>
			<p>From the maximum likelihood principle, we can obtain an optimization problem that the logistic regression solves—namely, the minimization of the logistic loss function. For the "<strong class="source-inline">-</strong>" class, the probability is determined by a similar formula, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer463">
					<img alt="" role="presentation" src="image/B19849_Formula_017.jpg"/>
				</div>
			</div>
			<p>The expressions for both classes can be combined into one, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer464">
					<img alt="" role="presentation" src="image/B19849_Formula_018.jpg"/>
				</div>
			</div>
			<p>Here, the expression <img alt="" role="presentation" src="image/B19849_Formula_019.png"/> is called the margin of classification of the <img alt="" role="presentation" src="image/B19849_Formula_006.png"/> object. The<a id="_idIndexMarker833"/> classification margin can be understood as a <a id="_idIndexMarker834"/>model’s <em class="italic">confidence</em> in the object’s classification. An interpretation of this margin is <span class="No-Break">as follows:</span></p>
			<ul>
				<li>If the margin vector’s absolute value is large and positive, the class label is set correctly, and the object is far from the separating hyperplane. Such an object is therefore <span class="No-Break">classified confidently.</span></li>
				<li>If the margin is large (by modulo) but negative, then the class label is set incorrectly. The object is far from the separating hyperplane. Such an object is most likely <span class="No-Break">an anomaly.</span></li>
				<li>If the margin is small (by modulo), then the object is close to the separating hyperplane. In this case, the margin sign determines whether the object is <span class="No-Break">correctly classified.</span></li>
			</ul>
			<p>In the discrete case, the likelihood function <img alt="" role="presentation" src="image/B19849_Formula_0212.png"/> can be interpreted as the probability that the sample <em class="italic">X</em><span class="subscript">1</span><em class="italic"> , . . . , X</em><span class="subscript">n</span> is equal to <em class="italic">x</em><span class="subscript">1</span><em class="italic"> , . . . , x</em><span class="subscript">n</span><em class="italic"> </em>in the given set of experiments. Furthermore, this probability depends on <em class="italic">θ</em>, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer468">
					<img alt="" role="presentation" src="image/B19849_Formula_022.jpg"/>
				</div>
			</div>
			<p>The maximum likelihood estimate <img alt="" role="presentation" src="image/B19849_Formula_0231.png"/> for the unknown parameter <img alt="" role="presentation" src="image/B19849_Formula_0241.png"/> is called the value of <img alt="" role="presentation" src="image/B19849_Formula_0241.png"/>, for which the function <img alt="" role="presentation" src="image/B19849_Formula_0261.png"/> reaches its maximum (as a function of <em class="italic">θ</em>, with fixed <img alt="" role="presentation" src="image/B19849_Formula_0271.png"/>), as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer474">
					<img alt="" role="presentation" src="image/B19849_Formula_028.jpg"/>
				</div>
			</div>
			<p>Now, we can write out the likelihood of the sample—namely, the probability of observing the given vector <img alt="" role="presentation" src="image/B19849_Formula_0291.png"/> in the sample <img alt="" role="presentation" src="image/B19849_Formula_0301.png"/>. We make one assumption—objects arise independently from a single distribution, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer477">
					<img alt="" role="presentation" src="image/B19849_Formula_0311.jpg"/>
				</div>
			</div>
			<p>Let’s take the<a id="_idIndexMarker835"/> logarithm of this expression since <a id="_idIndexMarker836"/>the sum is much easier to optimize than the product, <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer478">
					<img alt="" role="presentation" src="image/B19849_Formula_032.jpg"/>
				</div>
			</div>
			<p>In this case, the principle of maximizing the likelihood leads to a minimization of the expression, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer479">
					<img alt="" role="presentation" src="image/B19849_Formula_033.jpg"/>
				</div>
			</div>
			<p>This formula<a id="_idIndexMarker837"/> is a logistic loss function, summed over all <a id="_idIndexMarker838"/>objects of the training sample. Usually, it is a good idea to add some regularization to a model to deal with overfitting. <strong class="bold">L2 regularization</strong> of<a id="_idIndexMarker839"/> logistic regression is arranged in much the same way as for the ridge regression (<strong class="bold">linear regression</strong> with<a id="_idIndexMarker840"/> regularization). However, it is common to use the controlled variable decay parameter <em class="italic">C</em> that is used in SVM models, where it denotes soft margin parameter denotation. So, for logistic regression, <em class="italic">C</em> is equal to the inverse regularization coefficient <img alt="" role="presentation" src="image/B19849_Formula_0341.png"/>. The relationship between <em class="italic">C</em> and <img alt="" role="presentation" src="image/B19849_Formula_0351.png"/> would be the following: lowering <em class="italic">C</em> would strengthen the regularization effect. Therefore, instead of the functional <img alt="" role="presentation" src="image/B19849_Formula_0361.png"/>, the following function should <span class="No-Break">be minimized:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer483">
					<img alt="" role="presentation" src="image/B19849_Formula_037.jpg"/>
				</div>
			</div>
			<p class="IMG---Figure">.</p>
			<p>For this function minimization, we can apply different methods—for example, the method of least squares, or <a id="_idIndexMarker841"/>the <strong class="bold">gradient descent</strong> method. The vital issue with logistic regression is that it is generally a linear classifier, in order to deal with non-linear decision boundaries, which typically use polynomial features with original features as a basis for them. This approach was discussed in <a href="B19849_03.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> when we discussed <span class="No-Break">polynomial regression.</span></p>
			<h2 id="_idParaDest-159">KRR<a id="_idTextAnchor397"/></h2>
			<p><a id="_idTextAnchor398"/>KRR <a id="_idIndexMarker842"/>combines linear<a id="_idIndexMarker843"/> ridge regression (linear regression and L2 norm regularization) with the kernel trick and can be used for classification problems. It learns a linear function in the higher-dimensional space produced by the chosen kernel and training data. For non-linear kernels, it learns a non-linear function in the <span class="No-Break">original space.</span></p>
			<p>The model learned by KRR is identical to the SVM model, but these approaches have the <span class="No-Break">following differences:</span></p>
			<ul>
				<li>The KRR method uses squared error loss, while the SVM model uses insensitive loss or hinge loss <span class="No-Break">for classification</span></li>
				<li>In contrast to the SVM method, the KRR training can be completed in closed form so that it can be trained faster for <span class="No-Break">medium-sized datasets</span></li>
				<li>The <a id="_idIndexMarker844"/>learned KRR model is non-sparse and can be slower than the <a id="_idIndexMarker845"/>SVM model when it comes to <span class="No-Break">prediction times</span></li>
			</ul>
			<p>Despite these differences, both approaches usually use <span class="No-Break">L2 regularization.</span><a id="_idTextAnchor399"/></p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor400"/>SVM</h2>
			<p>The SVM method is a<a id="_idIndexMarker846"/> set of algorithms used for classification and <a id="_idIndexMarker847"/>regression analysis tasks. Considering that in an <em class="italic">N</em>-dimensional space, each object belongs to one of two classes, SVM generates an (<em class="italic">N-1</em>)-dimensional hyperplane to divide these points into two groups. It’s similar to an on-paper depiction of points of two different types that can be linearly divided. Furthermore, the SVM selects the hyperplane, which is characterized by the maximum distance from the nearest <span class="No-Break">group elements.</span></p>
			<p>The input data can be separated using various hyperplanes. The best hyperplane is a hyperplane with the maximum resulting separation and the maximum resulting difference between the <span class="No-Break">two classes.</span></p>
			<p>Imagine the data points on the plane. In the following case, the separator is just a <span class="No-Break">straight line:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer484">
					<img alt="Figure 7.2 – Point separation line" src="image/B19849_07_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Point separation line</p>
			<p>Let’s draw <a id="_idIndexMarker848"/>distinct straight lines that divide the points into two <a id="_idIndexMarker849"/>sets. Then, choose a straight line as far as possible from the points, maximizing the distance from it to the nearest point on each side. If such a line exists, then it is called the maximum margin hyperplane. Intuitively, a good separation is achieved due to the hyperplane itself (which has the longest distance to the nearest point of the training sample of any class), since, in general, the bigger the distance, the smaller the <span class="No-Break">classifier error.</span></p>
			<p>Consider learning samples given with the set <img alt="" role="presentation" src="image/B19849_Formula_0391.png"/> consisting of <img alt="" role="presentation" src="image/B19849_Formula_0401.png"/> objects with <img alt="" role="presentation" src="image/B19849_Formula_119.png"/> parameters, where <img alt="" role="presentation" src="image/B19849_Formula_0421.png"/> takes the values -1 or 1, thus defining the point’s classes. Each point <img alt="" role="presentation" src="image/B19849_Formula_0431.png"/> is a vector of the dimension <img alt="" role="presentation" src="image/B19849_Formula_119.png"/>. Our task is to find the maximum margin hyperplane that separates the observations. We can use analytic geometry to define any hyperplane as a set of points <img alt="" role="presentation" src="image/B19849_Formula_0431.png"/> that satisfy the condition, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer492">
					<img alt="" role="presentation" src="image/B19849_Formula_046.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_0471.png"/> <span class="No-Break">and <img alt="" role="presentation" src="image/B19849_Formula_0481.png"/></span><span class="No-Break">.</span></p>
			<p>Thus, the linear separating (discriminant) function is described by the equation <em class="italic">g(x)=0</em>. The distance from the point to the separating function <em class="italic">g(x)=0</em> (the distance from the point to the plane) is equal to <span class="No-Break">the following:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer495">
					<img alt="" role="presentation" src="image/B19849_Formula_049.jpg"/>
				</div>
			</div>
			<p class="IMG---Figure">.</p>
			<p><img alt="" role="presentation" src="image/B19849_Formula_312.png"/> lies in the <a id="_idIndexMarker850"/>closure of the boundary that is <img alt="" role="presentation" src="image/B19849_Formula_0512.png"/>. The <a id="_idIndexMarker851"/>border, which is the width of the dividing strip, needs to be as large as possible. Considering that the closure of the boundary satisfies the condition <img alt="" role="presentation" src="image/B19849_Formula_0521.png"/>, then the distance from <img alt="" role="presentation" src="image/B19849_Formula_0531.png"/> is <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer500">
					<img alt="" role="presentation" src="image/B19849_Formula_054.jpg"/>
				</div>
			</div>
			<p class="IMG---Figure">.</p>
			<p>Thus, the width of the dividing strip is <img alt="" role="presentation" src="image/B19849_Formula_0551.png"/>. To exclude points from the dividing strip, we can write out the <span class="No-Break">following conditions:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer502">
					<img alt="" role="presentation" src="image/B19849_Formula_056.jpg"/>
				</div>
			</div>
			<p>Let’s also introduce the index function <img alt="" role="presentation" src="image/B19849_Formula_057.png"/> that shows to which class <img alt="" role="presentation" src="image/B19849_Formula_058.png"/> belongs, <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer505">
					<img alt="" role="presentation" src="image/B19849_Formula_0591.jpg"/>
				</div>
			</div>
			<p>Thus, the task of choosing a separating function that generates a corridor of the greatest width can be written <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer506">
					<img alt="" role="presentation" src="image/B19849_Formula_060.jpg"/>
				</div>
			</div>
			<p>The <em class="italic">J(w)</em> function was introduced with the assumption that <img alt="" role="presentation" src="image/B19849_Formula_0612.png"/> for all <img alt="" role="presentation" src="image/B19849_Formula_194.png"/>. Since the objective function is quadratic, this problem has a <span class="No-Break">unique solution.</span></p>
			<p>According to the Kuhn-Tucker theorem, this condition is equivalent to the <span class="No-Break">following problem:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer509">
					<img alt="" role="presentation" src="image/B19849_Formula_063.jpg"/>
				</div>
			</div>
			<p class="IMG---Figure">.</p>
			<p>This is provided that <img alt="" role="presentation" src="image/B19849_Formula_0641.png"/> and <img alt="" role="presentation" src="image/B19849_Formula_0651.png"/>, where <img alt="" role="presentation" src="image/B19849_Formula_0661.png"/>, are new variables. We can rewrite <img alt="" role="presentation" src="image/B19849_Formula_0671.png"/> in the matrix form, <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer514">
					<img alt="" role="presentation" src="image/B19849_Formula_068.jpg"/>
				</div>
			</div>
			<p>The <em class="italic">H</em> coefficients <a id="_idIndexMarker852"/>of the matrix can <a id="_idIndexMarker853"/>be calculated as follows: </p>
			<div>
				<div class="IMG---Figure" id="_idContainer515">
					<img alt="" role="presentation" src="image/B19849_Formula_069.jpg"/>
				</div>
			</div>
			<p>Quadratic programming methods can solve the <span class="No-Break">task <img alt="" role="presentation" src="image/B19849_Formula_0701.png"/>.</span></p>
			<p>After finding the optimal <img alt="" role="presentation" src="image/B19849_Formula_0712.png"/> for every <img alt="" role="presentation" src="image/B19849_Formula_0721.png"/>, one of the two following conditions <span class="No-Break">is fulfilled:</span></p>
			<ul>
				<li><img alt="" role="presentation" src="image/B19849_Formula_0731.png"/> (<em class="italic">i</em> corresponds to a <span class="No-Break">non-support vector)</span></li>
				<li><img alt="" role="presentation" src="image/B19849_Formula_0741.png"/> (<em class="italic">i</em> corresponds to a <span class="No-Break">support vector)</span></li>
			</ul>
			<p>Then, <img alt="" role="presentation" src="image/B19849_Formula_0751.png"/> can be found from the relation <img alt="" role="presentation" src="image/B19849_Formula_0761.png"/>, and the value of <img alt="" role="presentation" src="image/B19849_Formula_077.png"/> can be determined, considering that for any <img alt="" role="presentation" src="image/B19849_Formula_0781.png"/> and <img alt="" role="presentation" src="image/B19849_Formula_0791.png"/>, <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer526">
					<img alt="" role="presentation" src="image/B19849_Formula_080.jpg"/>
				</div>
			</div>
			<p>So, we can calculate w<span class="subscript">0</span> with the provided formula considering the <span class="No-Break">provided conditions.</span></p>
			<p>Finally, we can obtain the discriminant function, <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer527">
					<img alt="" role="presentation" src="image/B19849_Formula_0812.jpg"/>
				</div>
			</div>
			<p>Note that the summation is not carried out over all vectors, but only over the set <em class="italic">S</em>, which is the set of support <span class="No-Break">vectors <img alt="" role="presentation" src="image/B19849_Formula_0822.png"/>.</span></p>
			<p>Unfortunately, the described algorithm is realized only for linearly separable datasets, which, in itself, occurs rather infrequently. There are two approaches for working with linearly <span class="No-Break">non-separable data.</span></p>
			<p>One of them is <a id="_idIndexMarker854"/>called a soft margin, which <a id="_idIndexMarker855"/>chooses a hyperplane that<a id="_idIndexMarker856"/> divides the training sample as purely (with minimal error) as possible, while at the same time maximizing the distance to the nearest point on the training dataset. For this, we have to introduce additional variables, <img alt="" role="presentation" src="image/B19849_Formula_0831.png"/>, which characterize the magnitude of the error on each object <em class="italic">x</em><span class="subscript">i</span>. Furthermore, we can introduce the penalty for the total error into the goal functional, <span class="No-Break">like this:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer530">
					<img alt="" role="presentation" src="image/B19849_Formula_084.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_0851.png"/> is a<a id="_idIndexMarker857"/> method tuning parameter that allows you to adjust the relationship <a id="_idIndexMarker858"/>between maximizing the width of the dividing strip and minimizing the total error. The value of the penalty <img alt="" role="presentation" src="image/B19849_Formula_086.png"/> for the corresponding object <em class="italic">x</em><span class="subscript">i</span> depends on the location of the object <em class="italic">x</em><span class="subscript">i</span> relative to the dividing line. So, if <em class="italic">x</em><span class="subscript">i</span> lies on the opposite side of the discriminant function, then we can assume the value of the penalty <img alt="" role="presentation" src="image/B19849_Formula_0871.png"/>, if <em class="italic">x</em><span class="subscript">i</span> lies in the dividing strip, and comes from its class. The corresponding weight is, therefore, <img alt="" role="presentation" src="image/B19849_Formula_0881.png"/>. In the ideal case, we assume that <img alt="" role="presentation" src="image/B19849_Formula_089.png"/>. The resulting problem can then be rewritten <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer536">
					<img alt="" role="presentation" src="image/B19849_Formula_090.jpg"/>
				</div>
			</div>
			<p>Notice that elements that are not an ideal case are involved in the minimization process too, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer537">
					<img alt="" role="presentation" src="image/B19849_Formula_0911.jpg"/>
				</div>
			</div>
			<p>Here, the constant <em class="italic">β</em> is the weight that takes into account the width of the strip. If <em class="italic">β</em> is small, then we can allow the algorithm to locate a relatively high number of elements in a non-ideal position (in the dividing strip). If <em class="italic">β</em> is vast, then we require the presence of a small number of elements in a non-ideal position (in the dividing strip). Unfortunately, the minimization problem is rather complicated due to the <img alt="" role="presentation" src="image/B19849_Formula_0921.png"/> discontinuity. Instead, we can use the minimization of <span class="No-Break">the following:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer539">
					<img alt="" role="presentation" src="image/B19849_Formula_093.jpg"/>
				</div>
			</div>
			<p>This occurs under restrictions <img alt="" role="presentation" src="image/B19849_Formula_0941.png"/>, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer541">
					<img alt="" role="presentation" src="image/B19849_Formula_095.jpg"/>
				</div>
			</div>
			<p>Another idea of the SVM method in the case of the impossibility of a linear separation of classes is the transition to a space of higher dimension, in which such a separation is possible. While the original problem can be formulated in a finite-dimensional space, it often happens that the samples for discrimination are not linearly separable in this space. Therefore, it is suggested to map the original finite-dimensional space into a larger dimension space, which makes the separation much easier. To keep the computational load reasonable, the mappings used in support vector algorithms provide ease of calculating points in terms of variables in the original space, specifically in terms of the <span class="No-Break">kernel function.</span></p>
			<p>First, the<a id="_idIndexMarker859"/> function of the mapping <img alt="" role="presentation" src="image/B19849_Formula_096.png"/> is selected to map the data of <img alt="" role="presentation" src="image/B19849_Formula_0971.png"/> into a space<a id="_idIndexMarker860"/> of a higher dimension. Then, a non-linear discriminant function can be written in the form <img alt="" role="presentation" src="image/B19849_Formula_0981.png"/>. The idea of the method is to find the kernel function <img alt="" role="presentation" src="image/B19849_Formula_0991.png"/> and maximize the objective function, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer546">
					<img alt="" role="presentation" src="image/B19849_Formula_100.jpg"/>
				</div>
			</div>
			<p>Here, to minimize computations, the direct mapping of data into a space of a higher dimension is not used. Instead, an approach called the kernel trick is used—that is, <em class="italic">K(x, y)</em>, which is a kernel matrix. The kernel trick is a method used in machine learning algorithms to transform non-linear data into a higher-dimensional space where it becomes linearly separable. This allows to use of linear algorithms to solve non-linear problems because they are often simpler and more computationally efficient (see <a href="B19849_06.xhtml#_idTextAnchor301"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> for the detailed explanation of the <span class="No-Break">kernel trick).</span></p>
			<p>In general, the more support vectors the method chooses, the better it generalizes. Any training example that does not constitute a support vector is correctly classified if it appears in the test set because the border between positive and negative examples is still in the same place. Therefore, the expected error rate of the support vector method is, as a rule, equal to the proportion of examples that are support vectors. As the number of measurements grows, this proportion also grows, so the method is not immune from the curse of dimensionality but it is more resistant to it than <span class="No-Break">most algorithms.</span></p>
			<p>It is also worth noting that the support vector method is sensitive to noise and <span class="No-Break">data standardization.</span></p>
			<p>Also, the method of <a id="_idIndexMarker861"/>SVMs is not only limited to the classification task but <a id="_idIndexMarker862"/>can also be adapted for solving regression tasks. So, you can usually use the same SVM software implementation for solving classification and <span class="No-Break">regression tasks.</span></p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor401"/>kNN method</h2>
			<p>The kNN<a id="_idIndexMarker863"/> is a popular classificati<a id="_idTextAnchor402"/>o<a id="_idTextAnchor403"/>n method that is sometimes used in<a id="_idIndexMarker864"/> regression problems. It is one of the most natural approaches to classification. The essence of the method is to classify the current item by the most prevailing class of its neighbors. Formally, the basis of the method is the hypothesis of compactness: if the metric of the distance between the examples is clarified successfully, then similar examples are more likely to be in the same class. For example, if you don’t know what type of product to specify in the ad for a Bluetooth headset, you can find five similar headset ads. If four of them are categorized as <em class="italic">Accessories</em> and only one as <em class="italic">Hardware</em>, common sense will tell you that your ad should probably be in the <span class="No-Break"><em class="italic">Accessories</em></span><span class="No-Break"> category.</span></p>
			<p>In general, to classify an object, you must perform the following <span class="No-Break">operations sequentially:</span></p>
			<ol>
				<li>Calculate the distance from the object to other objects in the <span class="No-Break">training dataset.</span></li>
				<li>Select the <em class="italic">k</em> of training objects, with the minimal distance to the object that <span class="No-Break">is classified.</span></li>
				<li>Set the classifying object class to the class most often found among the nearest <span class="No-Break"><em class="italic">k</em></span><span class="No-Break"> neighbors.</span></li>
			</ol>
			<p>If we take the number of nearest neighbors <em class="italic">k = 1</em>, then the algorithm loses the ability to generalize (that is, to produce a correct result for data not previously encountered in the algorithm) because the new item is assigned to the closest class. If we set too high a value, then the algorithm may not reveal many <span class="No-Break">local features.</span></p>
			<p>The function for calculating the distance must meet the <span class="No-Break">following rules:</span></p>
			<ul>
				<li><img alt="" role="presentation" src="image/B19849_Formula_1012.png"/></li>
				<li><img alt="" role="presentation" src="image/B19849_Formula_1021.png"/> only when <em class="italic">x = y</em></li>
				<li><img alt="" role="presentation" src="image/B19849_Formula_1031.png"/></li>
				<li><img alt="" role="presentation" src="image/B19849_Formula_1041.png"/> in the case when the <em class="italic">x</em>, <em class="italic">y</em>, and <em class="italic">z</em> points don’t lie on one <span class="No-Break">straight line</span></li>
			</ul>
			<p>In this case, <em class="italic">x</em>, <em class="italic">y</em>, and <em class="italic">z</em> are feature vectors of compared objects. For ordered attribute values, Euclidean distance can be applied, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer551">
					<img alt="" role="presentation" src="image/B19849_Formula_1051.jpg"/>
				</div>
			</div>
			<p>In this case, <em class="italic">n</em> is the number <span class="No-Break">of attributes.</span></p>
			<p>For string <a id="_idIndexMarker865"/>variables that cannot be ordered, the difference <a id="_idIndexMarker866"/>function can be applied, which is set <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer552">
					<img alt="" role="presentation" src="image/B19849_Formula_1061.jpg"/>
				</div>
			</div>
			<p>When finding the distance, the importance of the attributes is sometimes taken into account. Usually, attribute relevance can be determined subjectively by an expert or analyst, and is based on their own experience, expertise, and problem interpretation. In this case, each <em class="italic">i</em><span class="superscript">th</span> square of the difference in the sum is multiplied by the coefficient <em class="italic">Z</em><span class="subscript">i</span>. For example, if the attribute <em class="italic">A</em> is three times more important than the attribute <img alt="" role="presentation" src="image/B19849_Formula_1221.png"/> (<img alt="" role="presentation" src="image/B19849_Formula_1081.png"/>, <img alt="" role="presentation" src="image/B19849_Formula_109.png"/>), then the distance is calculated <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer556">
					<img alt="" role="presentation" src="image/B19849_Formula_110.jpg"/>
				</div>
			</div>
			<p>This technique is <a id="_idIndexMarker867"/>called <strong class="bold">stretching the axes</strong>, which reduces the <span class="No-Break">classification error.</span></p>
			<p>The choice of class for the object of classification can also be different, and there are two main approaches to making this choice: <em class="italic">unweighted voting</em> and <span class="No-Break"><em class="italic">weighted voting</em></span><span class="No-Break">.</span></p>
			<p>For unweighted voting, we determine how many objects have the right to vote in the classification task by specifying the <em class="italic">k</em> number. We identify such objects by their minimal distance to the new object. The individual distance to each object is no longer critical for voting. All have equal rights in a class definition. Each existing object votes for the class to which it belongs. We assign a class with the most votes to a new object. However, there may be a problem if several classes score an equal number of votes. Weighted voting removes <span class="No-Break">this problem.</span></p>
			<p>During the weighted vote, we also take into account the distance to the new object. The smaller the distance, the more significant the contribution of the vote. The votes for the class formula is <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer557">
					<img alt="" role="presentation" src="image/B19849_Formula_111.jpg"/>
				</div>
			</div>
			<p>In this case, <img alt="" role="presentation" src="image/B19849_Formula_1122.png"/> is the square of the distance from the known object <img alt="" role="presentation" src="image/B19849_Formula_1132.png"/> to the new object <img alt="" role="presentation" src="image/B19849_Formula_1141.png"/>, while <img alt="" role="presentation" src="image/B19849_Formula_026.png"/> is the number of known objects of the class for which votes are calculated. <strong class="source-inline">class</strong> is the name of the class. The new object corresponds to the class with the most votes. In this case, the probability that several classes gain the same number of votes is much lower. When <img alt="" role="presentation" src="image/B19849_Formula_116.png"/>, the new object is assigned to the class of the <span class="No-Break">nearest neighbor.</span></p>
			<p>A notable<a id="_idIndexMarker868"/> feature of the <a id="_idIndexMarker869"/>kNN approach is its laziness. Laziness means that the calculations begin only at the moment of the classification. When using training samples with the kNN method, we don’t simply build the model but also do sample classification simultaneously. Note that the method of nearest neighbors is a well-studied approach (in machine learning, econometrics, and statistics, only linear regression is more well-known). For the method of nearest neighbors, there are quite a few crucial theorems that state that on <em class="italic">infinite</em> samples, kNN is the optimal classification method. The authors of the classic book <em class="italic">The Elements of Statistical Learning</em> consider kNN to be a theoretically ideal algorithm, the applicability of which is limited only by computational capabilities and the curse <span class="No-Break">of dimensionality.</span></p>
			<p>kNN is one<a id="_idIndexMarker870"/> of the simplest classification algorithms, so it is often <a id="_idIndexMarker871"/>ineffective in real-world tasks. The KNN algorithm has several disadvantages. Besides a low classification accuracy when we don’t have enough samples, the kNN classifier’s problem is the speed of classification: if there are <em class="italic">N</em> objects in the training set and the dimension of the space is <em class="italic">K</em>, then the number of operations for classifying a test sample can be estimated as <img alt="" role="presentation" src="image/B19849_Formula_1171.png"/>. The dataset used for the algorithm must be representative. The model cannot be <em class="italic">separated</em> from the data: to classify a new example, you need to use all <span class="No-Break">the examples.</span></p>
			<p>The positive features include the fact that the algorithm is resistant to abnormal outliers since the probability of such a record falling into the number of kNN is small. If this happens, then the impact on the vote (uniquely weighted) with <img alt="" role="presentation" src="image/B19849_Formula_1181.png"/> is also likely to be insignificant, and therefore, the impact on the classification result is also small. The program implementation of the algorithm is relatively simple, and the algorithm result is easily interpreted. Experts in applicable fields, therefore, understand the logic of the algorithm, based on finding similar objects. The ability to modify the algorithm by using the most appropriate combination of functions and metrics allows you to adjust the algorithm for a <span class="No-Break">specific task.</span></p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor404"/>Multi-class classification</h2>
			<p>Most of the existing method<a id="_idTextAnchor405"/>s<a id="_idTextAnchor406"/> of <a id="_idIndexMarker872"/>multi-class classification<a id="_idIndexMarker873"/> are either based on binary classifiers or are reduced to them. The general idea of such an approach is to use a set of binary classifiers trained to separate different groups of objects from each other. With such a multi-class classification, various voting schemes for a set of binary classifiers <span class="No-Break">are used.</span></p>
			<p>In the <strong class="bold">one-against-all</strong> strategy<a id="_idIndexMarker874"/> for <em class="italic">N</em> classes, <em class="italic">N</em> classifiers are trained, each of which separates its class from all other classes. At the recognition stage, the unknown vector <em class="italic">X</em> is fed to all <em class="italic">N</em> classifiers. The membership of the vector <em class="italic">X</em> is determined by the classifier that gave the highest estimate. This approach can meet the problem of class imbalances when they arise. Even if the task of a multi-class classification is initially balanced (that is, it has the same number of training samples in each class), when training a binary classifier, the ratio of the number of samples in each binary problem increases with an increase in the number of classes, which, therefore significantly affects tasks with a notable number <span class="No-Break">of classes.</span></p>
			<p>The <strong class="bold">each-against-each</strong> strategy<a id="_idIndexMarker875"/> allocates <img alt="" role="presentation" src="image/B19849_Formula_1191.png"/> classifiers. These classifiers are trained to distinguish all possible pairs of classes of each other. For the input vector, each classifier gives an estimate <span class="No-Break">of <img alt="" role="presentation" src="image/B19849_Formula_120.png"/></span><span class="No-Break">,</span>
 reflecting membership in the classes <img alt="" role="presentation" src="image/B19849_Formula_194.png"/> and <img alt="" role="presentation" src="image/B19849_Formula_1222.png"/>. The result is a class with a maximum sum <img alt="" role="presentation" src="image/B19849_Formula_1232.png"/>, where <em class="italic">g</em> is a monotonically non-decreasing function—for example, identical <span class="No-Break">or logistic.</span></p>
			<p>The <strong class="bold">shooting tournament</strong> strategy<a id="_idIndexMarker876"/> also involves training <img alt="" role="presentation" src="image/B19849_Formula_1242.png"/> classifiers that distinguish all possible pairs of classes. Unlike the previous strategy, at the stage of classification of the vector <em class="italic">X</em>, we arrange a tournament between classes. We create a tournament tree, where each class has one opponent and only a winner can go to the next tournament stage. So, at each step, only one classifier determines the vector <em class="italic">X</em> class, then the <em class="italic">winning</em> class is used to determine the next classifier with the next pair of classes. The process is carried out until there is only one winning class left, which should be considered <span class="No-Break">the result.</span></p>
			<p>Some methods can produce multi-class classification immediately, without additional configuration and combinations. The kNN algorithms or neural networks can be considered examples of <span class="No-Break">such methods.</span></p>
			<p>Also, the<a id="_idIndexMarker877"/> logistic regression can be generalized for the <a id="_idIndexMarker878"/>multi-class case by using the softmax function. The softmax function is used to determine the probability that a sample belongs to a particular class, it looks <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer571">
					<img alt="" role="presentation" src="image/B19849_Formula_125.jpg"/>
				</div>
			</div>
			<p>Here, <em class="italic">K</em> is the number of possible classes and the theta is a vector of learnable parameters. For the case when <em class="italic">K=2</em>, this expression reduces to the <span class="No-Break">logistic regression:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer572">
					<img alt="" role="presentation" src="image/B19849_Formula_126.jpg"/>
				</div>
			</div>
			<p>Replacing the <span class="No-Break">vector difference:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer573">
					<img alt="" role="presentation" src="image/B19849_Formula_127.jpg"/>
				</div>
			</div>
			<p>with a single parameter vector, we can see that the probability for the one class will be predicted <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer574">
					<img alt="" role="presentation" src="image/B19849_Formula_128.jpg"/>
				</div>
			</div>
			<p>For the <a id="_idIndexMarker879"/>second class, it <a id="_idIndexMarker880"/>will be <span class="No-Break">the following:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer575">
					<img alt="" role="presentation" src="image/B19849_Formula_129.jpg"/>
				</div>
			</div>
			<p>As you can see, these expressions are equal to the logistic regression we <span class="No-Break">already saw.</span></p>
			<p>Now we have become familiar with some of the most widespread classification algorithms, let’s look at how to use them in different <span class="No-Break">C++ libraries.</span></p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor407"/>Examples of using C++ libraries for dealing with the cla<a id="_idTextAnchor408"/>s<a id="_idTextAnchor409"/>sification task</h1>
			<p>Let’s now see how to use the methods we’ve described for solving a classification task on artificial datasets, which we can<a id="_idIndexMarker881"/> see in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer576">
					<img alt="Figure 7.3 – Artificial datasets" src="image/B19849_07_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Artificial datasets</p>
			<p>As we can see, these<a id="_idIndexMarker882"/> datasets contain two and three different classes of objects, so it makes sense to use methods for multi-class classification because such tasks appear more often in real life; they can be easily reduced to <span class="No-Break">binary classification.</span></p>
			<p>Classification is a<a id="_idIndexMarker883"/> supervised technique, so we usually have a training dataset, as well as new data for classification. To model this situation, we will use two datasets in our examples, one for training and one for testing. They come from the same distribution in one large dataset. However, the test set won’t be used for training; therefore, we can evaluate the accuracy metric and see how well models perform <span class="No-Break">and generalize.</span></p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor410"/>Using the mlpack library</h2>
			<p>In this section, we show how <a id="_idTextAnchor411"/>t<a id="_idTextAnchor412"/>o <a id="_idIndexMarker884"/>use the <strong class="source-inline">mlpack</strong> library for solving the classification task. This library provides the implementation for the three main types of classification algorithms: logistic regression, softmax regression, <span class="No-Break">and SVM.</span></p>
			<h3>With softmax regression</h3>
			<p>The <strong class="source-inline">mlpack</strong> library <a id="_idIndexMarker885"/>implement<a id="_idTextAnchor413"/>s<a id="_idTextAnchor414"/> multi-class logistic <a id="_idIndexMarker886"/>regression in the <strong class="source-inline">SoftmaxRegression</strong> class. Using this class is very simple. We have to initialize an object with the number of samples that will be used for training and the number of classes. Let’s say we have the following objects as training data <span class="No-Break">and labels:</span></p>
			<pre class="source-code">
using namespace mlpack;
size_t num_classes;
arma::mat train_input;
arma::Row&lt;size_t&gt; train_labels;</pre>			<p>Then we can initialize an object of <strong class="source-inline">SoftmaxRegression</strong> <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
SoftmaxRegression smr(train_input.n_cols, num_classes);</pre>			<p>After we have the classifier object, we can train it and apply the classification function for some new data. The following code snippet shows how it can <span class="No-Break">be done:</span></p>
			<pre class="source-code">
smr.Train(train_input, train_labels, num_classes);
arma::Row&lt;size_t&gt; predictions;
smr.Classify(test_input, predictions);</pre>			<p>Having the <a id="_idIndexMarker887"/>prediction vector we can visualize it with <a id="_idIndexMarker888"/>the technique we are using in this book that is based on the <strong class="source-inline">plotcpp</strong> library. Notice that the <strong class="source-inline">Train</strong> and <strong class="source-inline">Classify</strong> method requires the <strong class="source-inline">size_t</strong> type for the class labels. The following screenshot shows the results of applying the <strong class="source-inline">mlpack</strong> implementation of the softmax regression algorithm to <span class="No-Break">our datasets:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer577">
					<img alt="Figure 7.4 – Softmax classification with mlpack" src="image/B19849_07_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Softmax classification with mlpack</p>
			<p>Notice that we have classification errors in the <strong class="bold">Dataset 0</strong>, <strong class="bold">Dataset 1</strong>, and <strong class="bold">Dataset 2</strong> datasets, and other datasets were classified <span class="No-Break">almost correctly.</span><a id="_idTextAnchor415"/></p>
			<h3><a id="_idTextAnchor416"/>With SVMs</h3>
			<p>The <strong class="source-inline">mlpack</strong> library also <a id="_idIndexMarker889"/>has an implementation<a id="_idIndexMarker890"/> of the multi-class SVM algorithm in the <strong class="source-inline">LinearSVM</strong> class. The library provides mostly the same API for all classification algorithms so the initialization of the classifier object is mostly the same as in the previous example. The main difference is that you can use the constructor without parameters. So, the object initialization will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
mlpack::LinearSVM&lt;&gt; lsvm;</pre>			<p>Then, we train the classificator with the <strong class="source-inline">Train</strong> method and apply the <strong class="source-inline">Classify</strong> method for new data samples, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
lsvm.Train(train_input, train_labels, num_classes);
arma::Row&lt;size_t&gt; predictions;
lsvm.Classify(test_input, predictions);</pre>			<p>The following screenshot shows the results of applying the <strong class="source-inline">mlpack</strong> implementation of the SVM algorithm to <span class="No-Break">our datasets:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer578">
					<img alt="Figure 7.5 – SVM classification with mlpack" src="image/B19849_07_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – SVM classification with mlpack</p>
			<p>You can <a id="_idIndexMarker891"/>see that the results we got from <a id="_idIndexMarker892"/>the SVM method are pretty much the same as we got with the <span class="No-Break">softmax regressio<a id="_idTextAnchor417"/>n<a id="_idTextAnchor418"/>.</span></p>
			<h3>With the linear regression algorithm</h3>
			<p>The <strong class="source-inline">mlpack</strong> library also <a id="_idIndexMarker893"/>implements the classic <a id="_idIndexMarker894"/>logistic regression algorithm in the <strong class="source-inline">LogisticRegression</strong> class. An object of this class can be applied to classify samples only into two classes. The usage API is the same as in the previous examples for the <strong class="source-inline">mlpack</strong> library. The typical application of this class will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
using namespace mlpack;
LogisticRegression&lt;&gt; lr;
lr.Train(train_input, train_labels);
arma::Row&lt;size_t&gt; predictions;
lr.Classify(test_input, predictions);</pre>			<p>The following<a id="_idIndexMarker895"/> screenshot shows the results of<a id="_idIndexMarker896"/> applying the two-class logistic regression to <span class="No-Break">our datasets:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer579">
					<img alt="Figure 7.6 – Logistic regression classification with mlpack" src="image/B19849_07_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Logistic regression classification with mlpack</p>
			<p>You can <a id="_idIndexMarker897"/>see that we only got reasonable <a id="_idIndexMarker898"/>classifications for <strong class="bold">Dataset 3</strong> and <strong class="bold">Dataset 4</strong> as they can be separated with a straight lin<a id="_idTextAnchor419"/>e. However, due to the two-class limitation, we were not able to get the <span class="No-Break">correct results<a id="_idTextAnchor420"/>.</span></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor421"/>Using the Dlib library</h2>
			<p>The <strong class="source-inline">Dlib</strong> library doesn’t have many<a id="_idIndexMarker899"/> classification algorithms. There are two that are most applicable: <em class="italic">KRR</em> and <em class="italic">SVM</em>. These methods are implemented as binary classifiers, but for multi-class classification, this library provides the <strong class="source-inline">one_vs_one_trainer</strong> class, which implements the voting strategy. Note that this class can use classifiers of different types so that you can combine the KRR and the SVM for one classification task. We can also specify which classifiers should be used for which <span class="No-Break">distinct class<a id="_idTextAnchor422"/>e<a id="_idTextAnchor423"/>s.</span></p>
			<h3>With KRR</h3>
			<p>The following <a id="_idIndexMarker900"/>code sample <a id="_idIndexMarker901"/>shows how to use the <strong class="source-inline">Dlib</strong> KRR algorithm implementation for the <span class="No-Break">multi-class classification:</span></p>
			<pre class="source-code">
void KRRClassification(const Samples&amp; samples,
                       const Labels&amp; labels,
                       const Samples&amp; test_samples,
                       const Labels&amp; test_labels) {
  using OVOtrainer = one_vs_one_trainer&lt;
    any_trainer&lt;SampleType&gt;&gt;;
  using KernelType = radial_basis_kernel&lt;SampleType&gt;;
  krr_trainer&lt;KernelType&gt; krr_trainer;
  krr_trainer.set_kernel(KernelType(0.1));
  OVOtrainer trainer;
  trainer.set_trainer(krr_trainer);
  one_vs_one_decision_function&lt;OVOtrainer&gt; df =
      trainer.train(samples, labels);
  // process results and estimate accuracy
  DataType accuracy = 0;
  for (size_t i = 0; i != test_samples.size(); i++) {
    auto vec = test_samples[i];
    auto class_idx = static_cast&lt;size_t&gt;(df(vec));
    if (static_cast&lt;size_t&gt;(test_labels[i]) == class_idx)
        ++accuracy;
    // ...
  }
  accuracy /= test_samples.size();
}</pre>			<p>Firstly, we<a id="_idIndexMarker902"/> initialized the object of the <strong class="source-inline">krr_trainer</strong> class, and <a id="_idIndexMarker903"/>then we configured it with the instance of a kernel object. In this example, we used the <strong class="source-inline">radial_basis_kernel</strong> type for the kernel object, in order to deal with samples that can’t be linearly separated. After we obtained the binary classifier object, we initialized the instance of the <strong class="source-inline">one_vs_one_trainer</strong> class and added this classifier to its stack with the <strong class="source-inline">set_trainer()</strong> method. Then, we used the <strong class="source-inline">train()</strong> method for training our multi-class classifier. As with most of the algorithms in the <strong class="source-inline">Dlib</strong> library, this one assumes that the training samples and labels have the <strong class="source-inline">std::vector</strong> type, whereby each element has a <strong class="source-inline">matrix</strong> type. The <strong class="source-inline">train()</strong> method returns a decision function—namely, the object that behaves as a functor, which then takes a single sample and returns a classification label for it. This decision function is an object of the <strong class="source-inline">one_vs_one_decision_function</strong> type. The following piece of code demonstrates how we can <span class="No-Break">use it:</span></p>
			<pre class="source-code">
        auto vec = test_samples[i];
        auto class_idx = static_cast&lt;size_t&gt;(df(vec));</pre>			<p>There is no explicit implementation for the accuracy metric in the <strong class="source-inline">Dlib</strong> library; so, in this example, accuracy is calculated directly as a ratio of correctly classified test samples against the total number of <span class="No-Break">test samples.</span></p>
			<p>The following screenshot shows the results of applying the <strong class="source-inline">Dlib</strong> implementation of the KRR algorithm to <span class="No-Break">our datasets:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer580">
					<img alt="Figure 7.7 – KRR classification with DLib" src="image/B19849_07_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – KRR classification with Dlib</p>
			<p>Notice that the <a id="_idIndexMarker904"/>KRR algorithm performed a correct classification on <span class="No-Break">all </span><span class="No-Break"><a id="_idIndexMarker905"/></span><span class="No-Break">d<a id="_idTextAnchor424"/>a<a id="_idTextAnchor425"/>tasets.</span></p>
			<h3>With SVM</h3>
			<p>The <a id="_idIndexMarker906"/>following code sample shows <a id="_idIndexMarker907"/>how to use the <strong class="source-inline">Dlib</strong> SVM algorithm implementation for <span class="No-Break">multi-class classification:</span></p>
			<pre class="source-code">
void SVMClassification(const Samples&amp; samples,
                       const Labels&amp; labels,
                       const Samples&amp; test_samples,
                       const Labels&amp; test_labels) {
  using OVOtrainer = one_vs_one_trainer&lt;
    any_trainer&lt;SampleType&gt;&gt;;
  using KernelType = radial_basis_kernel&lt;SampleType&gt;;
  svm_nu_trainer&lt;KernelType&gt; svm_trainer;
  svm_trainer.set_kernel(KernelType(0.1));
  OVOtrainer trainer;
  trainer.set_trainer(svm_trainer);
  one_vs_one_decision_function&lt;OVOtrainer&gt; df =
    trainer.train(samples, labels);
  // process results and estimate accuracy
  DataType accuracy = 0;
  for (size_t i = 0; i != test_samples.size(); i++) {
    auto vec = test_samples[i];
    auto class_idx = static_cast&lt;size_t&gt;(df(vec));
    if (static_cast&lt;size_t&gt;(test_labels[i]) == class_idx)
        ++accuracy;
    // ...
  }
  accuracy /= test_samples.size();
}</pre>			<p>This<a id="_idIndexMarker908"/> sample shows that the <strong class="source-inline">Dlib</strong> library<a id="_idIndexMarker909"/> also has a unified API for using different algorithms, and the main difference from the previous example is the object of the binary classifier. For the SVM classification, we used an object of the <strong class="source-inline">svm_nu_trainer</strong> type, which was also configured with the kernel object of the <span class="No-Break"><strong class="source-inline">radial_basis_kernel</strong></span><span class="No-Break"> type.</span></p>
			<p>The following screenshot shows the results of applying the <strong class="source-inline">Dlib</strong> implementation of the SVM algorithm to <span class="No-Break">our datasets:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer581">
					<img alt="Figure 7.8 – SVM classification with DLib" src="image/B19849_07_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – SVM classification with Dlib</p>
			<p>You can<a id="_idIndexMarker910"/> see the <strong class="source-inline">Dlib</strong> implementation<a id="_idIndexMarker911"/> of the SVM algorithm also did correct classification on all datasets because of <a id="_idIndexMarker912"/>the <strong class="bold">radial basis function</strong> (<strong class="bold">RBF</strong>) kernel trick usage. Remember that the <strong class="source-inline">mlpack</strong> implementation of the same algorithm made incorrect classification in some cases due to <a id="_idTextAnchor426"/><span class="No-Break">i<a id="_idTextAnchor427"/>ts linearity.</span></p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor428"/>Using the Flashlight library</h2>
			<p>The Flas<a id="_idTextAnchor429"/>hlight library<a id="_idIndexMarker913"/> doesn’t have any special classes for the classification algorithms. But using the linear algebra primitives and auto-gradient facilities of the library, we can implement the logistic regression algorithm from scratch. Also, to handle the datasets that are non-linearly separable, the kernel trick approach will <span class="No-Break">b<a id="_idTextAnchor430"/>e implemented.</span></p>
			<h3>With logistic regression</h3>
			<p>The following example shows<a id="_idIndexMarker914"/> how to implement the <a id="_idIndexMarker915"/>two-class classification with the Flashlight library. Let’s define a function for training a linear classifier; it will have the <span class="No-Break">following signature:</span></p>
			<pre class="source-code">
fl::Tensor train_linear_classifier(
  const fl::Tensor&amp; train_x,
  const fl::Tensor&amp; train_y, float learning_rate);</pre>			<p><strong class="source-inline">train_x</strong> and <strong class="source-inline">train_y</strong> are the training samples and their labels correspondingly. The function’s result is the learned parameters vector—in our case, defined with the <strong class="source-inline">fl::Tensor</strong> class. We are going to use the batched gradient descent algorithm to learn the parameters vector. So, we can use the Flashlight dataset types to simplify work with training data batches. The following code snippet shows how we can make a dataset object that will allow us to iterate <span class="No-Break">over batches:</span></p>
			<pre class="source-code">
std::vector&lt;fl::Tensor&gt; fields{train_x, train_y};
auto dataset = std::make_shared&lt;fl::TensorDataset&gt;(fields);
int batch_size = 8;
auto batch_dataset = std::make_shared&lt;fl::BatchDataset&gt;(
  dataset, batch_size);</pre>			<p>At first, we defined the regular dataset object with the <strong class="source-inline">fl::TensorDataset</strong> type and then we used it to create the object of the <strong class="source-inline">fl::BatchDataset</strong> type that was initialized with the batch size <span class="No-Break">value also.</span></p>
			<p>Next, we need to initialize the parameters vector that we will learn with the gradient descent, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
auto weights = fl::Variable(fl::rand({
  train_x.shape().dim(0), 1}), /*calcGrad=*/true);</pre>			<p>Notice that we explicitly<a id="_idIndexMarker916"/> passed the <strong class="source-inline">true</strong> value as the last argument<a id="_idIndexMarker917"/> to enable the gradient calculation by the Flashlight autograd mechanism. Now, we are ready to define the training cycle with the predefined number of epochs. In each of the epochs, we will iterate over all batches in the dataset. So, such a cycle can be defined <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
int num_epochs = 100;
for (int e = 1; e &lt;= num_epochs; ++e) {
  fl::Tensor epoch_error = fl::fromScalar(0);
  for (auto&amp; batch : *batch_dataset) {
    auto x = fl::Variable(batch[0], /*calcGrad=*/false);
    auto y = fl::Variable(
      fl::reshape(batch[1], {1, batch[1].shape().dim(0)}), 
      /*calcGrad=*/false);
  }
}</pre>			<p>You can see two nested loops: the outer one for epochs and the inner one for batches. The <strong class="source-inline">batch_dataset</strong> object used in the <strong class="source-inline">for</strong> loop is compatible with C++ range based for loop construction, so it’s easily used to access the batches. Also, notice that we defined two variables, <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong>, with the <strong class="source-inline">fl::Variable</strong> type as we did for weights. Usage of this type makes it possible to pass tensor values into the autograd mechanism. And for these variables, we didn’t configure the gradients calculation because they are not trainable parameters. Another important issue is that we used <strong class="source-inline">fl::reshape</strong> to make all tensor shapes compatible with the matrix multiplication that will be applied in the loss function calculation. The logistic regression loss function looks <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer582">
					<img alt="" role="presentation" src="image/B19849_Formula_130.jpg"/>
				</div>
			</div>
			<p>In the code, we can implement it with the <span class="No-Break">following lines:</span></p>
			<pre class="source-code">
auto z = fl::matmul(fl::transpose(weights), x);
auto loss = fl::sum(fl::log(1 + fl::exp(-1 * y * z)), /*axes=*/{1});</pre>			<p>After we get the<a id="_idIndexMarker918"/> loss value, we can apply the gradient<a id="_idIndexMarker919"/> descent algorithm to correct the weights (parameters vector) according to the influence of the current training samples batch. The following code snippet shows how to <span class="No-Break">do it:</span></p>
			<pre class="source-code">
loss.backward();
weights.tensor() -= learning_rate *weights.grad().tensor();
weights.zeroGrad();</pre>			<p>Notice that the last step in the gradient zeroing was done to make it possible to learn something new from the next training sample and not mix gradients. At the end of the training cycle, the resulting parameters vector can be returned from the function <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
return weights.tensor();</pre>			<p>And the following sample shows how our training function can be <span class="No-Break">used :</span></p>
			<pre class="source-code">
fl::Tensor train_x;
fl::Tensor train_y;
auto weights = train_linear_classifier(
  train_x, train_y, /*learning_rate=*/0.1f);</pre>			<p>Having the learned parameter vector, we can use it to classify a new data sample <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
fl::Tensor sample;
constexpr float threshold = 0.5;
auto p = fl::sigmoid(fl::matmul(fl::transpose(weights), sample));
if (p.scalar&lt;float&gt;() &gt; threshold)
  return 1;
else
  return 0;</pre>			<p>You can see that we<a id="_idIndexMarker920"/> implemented the logistic function call <a id="_idIndexMarker921"/>that returns a result into the <strong class="source-inline">p</strong> variable. The value of this variable can be interpreted as the probability of the event that the sample belongs to a particular class. We introduced the <strong class="source-inline">threshold</strong> variable to check the probability. If it is greater than this threshold, then we classify the sample as it has a class of <strong class="source-inline">1</strong>; otherwise, it has a class <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">0</strong></span><span class="No-Break">.</span></p>
			<p>The following screenshot shows the results of applying Flashlight implementation of the logistic regression algorithm to our datasets with <span class="No-Break">two classes:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer583">
					<img alt="Figure 7.9 – Logistic regression classification with Flashlight" src="image/B19849_07_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – Logistic regression classification with Flashlight</p>
			<p>You can see that <a id="_idIndexMarker922"/>it fails to correctly<a id="_idIndexMarker923"/> classify <strong class="bold">Dataset 0</strong> and <strong class="bold">Dataset 1</strong> with a non-linear class boundary but successfully classified <strong class="bold">Dataset 4</strong> as it is <span class="No-Break">linearly separable<a id="_idTextAnchor431"/>.</span></p>
			<h3>With logistic regression and kernel trick</h3>
			<p>To address the problem with<a id="_idIndexMarker924"/> non-linear class boundaries, we can apply the kernel trick. Let’s see how we can implement it in the<a id="_idIndexMarker925"/> Flashlight library with the Gaussian kernel. The idea is to move our data samples into higher dimension space where they can be linearly separable. The Gaussian kernel function looks like <span class="No-Break">as follows:</span></p>
			<p class="IMG---Figure"><img alt="" role="presentation" src="image/B19849_Formula_1312.png"/></p>
			<p>To make our calculations more computationally effective, we can rewrite them in the <span class="No-Break">following way:</span></p>
			<p class="IMG---Figure"><img alt="" role="presentation" src="image/B19849_Formula_1322.png"/></p>
			<p>The following code sample shows this <span class="No-Break">formula implementation:</span></p>
			<pre class="source-code">
fl::Tensor make_kernel_matrix(const fl::Tensor&amp; x,
                              const fl::Tensor&amp; y, float gamma) {
  auto x_norm = fl::sum(fl::power(x, 2), /*axes=*/{-1});
  x_norm = fl::reshape(x_norm, {x_norm.dim(0), 1});
  auto y_norm = fl::sum(fl::power(y, 2), /*axes=*/{-1});
  y_norm = fl::reshape(y_norm, {1, y_norm.dim(0)});
  auto k = fl::exp(-gamma * (x_norm + y_norm -
                             2 * fl::matmul(fl::transpose(x), y)));
  return k;
}</pre>			<p>The <strong class="source-inline">make_kernel_matrix</strong> function takes two matrices and applies the Gaussian kernel returning the single matrix. Let’s see how we can apply it to our problem. At first, we apply it to our training dataset <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
constexpr float rbf_gamma = 100.f;
auto kx = make_kernel_matrix(train_x, train_x, rbf_gamma);</pre>			<p>Notice that <a id="_idIndexMarker926"/>the function was<a id="_idIndexMarker927"/> called with the same <strong class="source-inline">train_x</strong> value for the two arguments. So, we moved our training dataset into the higher dimension space based on this training dataset. The gamma is a scaling hyperparameter that was configured manually in this example. Having this transformed dataset, we can train a classifier with the function that we created in the previous example <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
auto kweights = train_linear_classifier(kx, train_y, learning_rate);</pre>			<p>Then, to use these weights (parameters vector), we should apply the kernel to the new data samples in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
fl::Tensor sample;
auto k_sample = make_kernel_matrix(fl::reshape(sample, {
  sample.dim(0), 1}), train_x, rbf_gamma);</pre>			<p>You can see<a id="_idIndexMarker928"/> that we used the<a id="_idIndexMarker929"/> reshaped new sample as the first argument and the training set tensor as the second argument. So, we transformed the new sample into the higher dimension space based on the original training data to preserve the same space properties. Then, we can apply the same classification procedure with a threshold as in the previous example, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
constexpr float threshold = 0.5;
auto p = fl::sigmoid(fl::matmul(fl::transpose(kweights),
         fl::transpose(k_sample)));
if (p.scalar&lt;float&gt;() &gt; threshold)
  return 1;
else
  return 0;</pre>			<p>You can see that we just used the transformed weights tensor and <span class="No-Break">transformed sample.</span></p>
			<p>The following screenshot shows the results of applying the logistic regression with the kernel trick implementation to our <span class="No-Break">two-class datasets:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer586">
					<img alt="Figure 7.10 – Logistic regression with kernel trick classification with Flashlight" src="image/B19849_07_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – Logistic regression with kernel trick classification with Flashlight</p>
			<p>You can <a id="_idIndexMarker930"/>see that with the kernel trick<a id="_idIndexMarker931"/> logistic regression successfully classified data with non-linear <span class="No-Break">class boundarie<a id="_idTextAnchor432"/>s<a id="_idTextAnchor433"/>.</span></p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor434"/>Summary</h1>
			<p>In this chapter, we discussed supervised machine learning approaches to solving classification tasks. These approaches use trained models to determine the class of an object according to its characteristics. We considered two methods of binary classification: logistic regression and SVMs. We looked at the approaches for the implementation of <span class="No-Break">multi-class classification.</span></p>
			<p>We saw that working with non-linear data requires additional improvements in the algorithms and their tuning. Implementations of classification algorithms differ in terms of performance, as well as the amount of required memory and the amount of time required for learning. Therefore, the classification algorithm’s choice should be guided by a specific task and business requirements. Furthermore, their implementations in different libraries can produce different results, even for the same algorithm. Therefore, it makes sense to have several libraries for <span class="No-Break">your software.</span></p>
			<p>In the next chapter, we will discuss recommender systems. We will see how they work, which algorithms exist for their implementation, and how to train and evaluate them. In the simplest sense, recommender systems are used to predict which objects (goods or services) are of interest to a user. Examples of such systems can be seen in many online stores such as Amazon or on streaming sites such as Netflix, which recommend new content based on your <span class="No-Break">previous consumptio<a id="_idTextAnchor435"/>n<a id="_idTextAnchor436"/>.</span></p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor437"/>Further reading</h1>
			<ul>
				<li><em class="italic">Logistic Regression—Detailed </em><span class="No-Break"><em class="italic">Overview</em></span><span class="No-Break">: </span><a href="https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc"><span class="No-Break">https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc</span></a></li>
				<li>Understanding SVM algorithm from examples (along with <span class="No-Break">code): </span><a href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/"><span class="No-Break">https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/</span></a></li>
				<li>Understanding Support Vector Machines: A <span class="No-Break">Primer: </span><a href="https://appliedmachinelearning.wordpress.com/2017/03/09/understanding-support-vector-machines-a-primer/"><span class="No-Break">https://appliedmachinelearning.wordpress.com/2017/03/09/understanding-support-vector-machines-a-primer/</span></a></li>
				<li><em class="italic">Support Vector Machine: Kernel Trick; Mercer’s </em><span class="No-Break"><em class="italic">Theorem</em></span><span class="No-Break">: </span><a href="https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d"><span class="No-Break">https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d</span></a></li>
				<li>SVMs with Kernel Trick (<span class="No-Break">lecture): </span><a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf"><span class="No-Break">https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf</span></a></li>
				<li><em class="italic">Support Vector Machines—Kernels and the Kernel </em><span class="No-Break"><em class="italic">Trick</em></span><span class="No-Break">: </span><a href="https://cogsys.uni-bamberg.de/teaching/ss06/hs_svm/slides/SVM_Seminarbericht_Hofmann.pdf"><span class="No-Break">https://cogsys.uni-bamberg.de/teaching/ss06/hs_svm/slides/SVM_Seminarbericht_Hofmann.pdf</span></a></li>
				<li><em class="italic">A Complete Guide to K-Nearest-Neighbors with Applications in Python and </em><span class="No-Break"><em class="italic">R</em></span><span class="No-Break">: </span><a href="https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor"><span class="No-Break">https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor</span></a></li>
			</ul>
		</div>
	</body></html>