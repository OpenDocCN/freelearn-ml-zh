- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Best Practices for Applying Synthetic Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Synthetic data indeed has many advantages and has been successfully and extensively
    utilized recently in various domains and applications. However, many general issues
    limit the usability of synthetic data. In this chapter, you will learn about these
    issues that present a bottleneck for synthetic data. Then, we will delve into
    domain-related issues that make deploying synthetic data even more challenging.
    You will explore these issues in various fields, such as healthcare, finance,
    and self-driving cars. Following this, you will be introduced to an excellent
    set of good practices that improve the usability of synthetic data in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Unveiling the challenges of generating and utilizing synthetic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain-specific issues limiting the usability of synthetic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for the effective utilization of synthetic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unveiling the challenges of generating and utilizing synthetic data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will understand the main common issues usually seen across
    different domains that limit the benefits and usability of synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can roughly categorize these limiting factors into four main categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Domain gap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy, security, and validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trust and credibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They can be represented as shown in *Figure 13**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Main factors that limit the usability of synthetic data in
    practice](img/B18494_13_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – Main factors that limit the usability of synthetic data in practice
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s delve into each of these categories in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Domain gap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While neural networks are very successful at learning hidden patterns, correlations,
    and structures in large datasets, they can suffer from the domain gap problem.
    **Domain gap** usually refers to the difference between the source and target
    domains’ data. The source domain refers to the training data’s domain on which
    the ML model was trained. On the other hand, the target domain refers to the domain
    on which the model will be tested, evaluated, or used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: In many scenarios, you may achieve excellent results on one dataset but dramatically
    unsatisfactory performance on another dataset. Both datasets can be real, synthetic,
    or a mixture of both. However, we focus here on the synthetic source domain and
    real target domain as it is usually the main and most frequent setup. Training
    your **Machine Learning** (**ML**) models on a large-scale synthetic dataset and
    achieving excellent performance in the synthetic domain may not necessarily guarantee
    the same performance on a real dataset. Thus, it is usually recommended to validate
    your ML model on a dataset collected from your target domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s dig deeper into the main reasons behind the domain gap between synthetic
    and real domains. In general, we can identify the following principal factors:'
  prefs: []
  type: TYPE_NORMAL
- en: Lack of realism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributional differences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of noise and artifacts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of realism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using a simulator, game engine, or generative model may generate appealing and
    semirealistic synthetic data but not exactly realistic data. Synthetic data generators
    cannot capture all the details of the complex real world. It is not necessary
    for ML models to be trained on data that captures all real-world nuances. However,
    it needs to be trained on synthetic data that captures and reflects the essential
    and auxiliary task-relevant details. It is crucial to acknowledge the substantial
    disparity between both scenarios. Additionally, identifying what is relevant to
    your ML task may not be a straightforward process. Thus, in general, if you do
    not train your ML model on sufficiently realistic data, you will end up with a
    domain gap between your training data and the real world. Thus, your model may
    not perform well in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Distributional differences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The essence of the training process of **d****eep learning** (**DL**) and ML
    models is learning how to make associations between input features and output
    labels. In other words, for a classification problem, the ML model learns to link
    certain patterns in pixels colors and locations with the target class label. Thus,
    when the ML model receives a new image, it may correctly classify it based on
    the patterns that the ML model learned in the training stage. As you can see,
    there is a clear assumption that data distributions between source and target
    domains are identical or close to each other. If the synthetic data distribution
    is not sufficiently close to the real one, this will make the learned associations,
    patterns, and correlations from synthetic data not applicable to real data.
  prefs: []
  type: TYPE_NORMAL
- en: For demonstration, let us imagine a scenario that captures the main idea, although
    it may not reflect the exact reality. If you trained your ML model to do a cats-dogs
    classification task on synthetic labeled images collected from the *The Secret
    Life of Pets* animated movie, you would not expect your model to perform well
    on real data because of the distributional differences problem. For example, the
    dimensions of the cats and dogs, colors, variations, and densities concerning
    other objects in the scenes in this movie may not match the ones in the real dataset
    even though they still may look partially realistic. It is essential to recognize
    that the issue here is not photorealism but distributional differences.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of noise and artifacts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While synthetic data may be generated to approximate or represent real data,
    it is usually extremely challenging to model noise and artifacts (anomalies and
    imperfections) of complex real-world data. The real world is full of imperfections,
    anomalies, noise, and artifacts. This is due to many reasons, such as random event
    occurrences, interactions and emergence among complex processes, limitations and
    errors in sensors and measurement procedures, and even errors because of human
    intervention. Thus, synthetic data may successfully present the central portion
    of the distribution, but it may fail to capture anomalies, outliers, and artifacts.
    Therefore, when the ML model does not observe these scenarios in the training
    process, it will simply fail when it encounters similar situations in the real
    world.
  prefs: []
  type: TYPE_NORMAL
- en: For more details about how to bridge the gap between synthetic and real domains,
    please refer to [*Chapter 14*](B18494_14.xhtml#_idTextAnchor230).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore the next primary category that restricts the usability of
    synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: Data representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training data is usually collected or generated to be a proxy of the real world.
    The human element is always present in this process. Thus, to some extent, our
    decisions, assumptions, and biases are explicitly or implicitly reflected in how
    we choose to represent the real world for the ML model. Nevertheless, while it
    is an issue with real data, it is more vital and problematic with synthetic data,
    as we will see next.
  prefs: []
  type: TYPE_NORMAL
- en: Biases and distortions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we know, one of the main methods of generating synthetic data is using generative
    models, which are trained on real data (for more information, please refer to
    [*Chapter 7*](B18494_07.xhtml#_idTextAnchor120)). If the generative model is trained
    on biased and distorted real data, it will also generate biased data. Then, when
    the ML models are trained on this data, the decisions will also be biased. Thus,
    it is very important to thoroughly comprehend and focus on the quality and procedures
    of our decision-making methodology and assumptions as we generate synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: Limited coverage and overfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second main issue under this category is synthetic data diversity. Imagine
    you want to build a 3D virtual world using a game engine to generate synthetic
    data for a semantic segmentation problem. For your synthetic data to be useful,
    you need to diversify scene elements such as 3D models, materials, textures, lighting,
    and camera parameters. Otherwise, your ML model will overfit to a few variations
    and will fail to generalize well when tested on real data. It should be noted
    that diversifying these scene elements requires more 3D assets to buy or design,
    more work and effort, more budget to spend, and more engineers, designers, and
    programmers.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of context
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike real data that is generated by real-world processes, synthetic data is
    generated artificially by algorithms or systems. Thus, it lacks contextual information
    that can be essential for learning the phenomenon or task under consideration.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say we have created a system to generate synthetic data for
    the cats-dogs classification problem. Indeed, we can generate thousands of labeled
    cat and dog images under various attributes, such as lighting conditions, backgrounds,
    and weather conditions. However, what is vital and much harder to capture with
    synthetic data is context – in other words, where, when, and how dogs and cats
    usually appear in the real world. For example, we can usually see them in parks,
    streets, and residential areas. On the other hand, it is unlikely to see them
    in hospitals and laboratories. Thus, as you can see, if we are not fully aware
    of the context of the problem, we may end up generating synthetic training data
    that lacks context. In this simple scenario, it may be easy to understand the
    context, but in other scenarios, the context may not be clear and straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy, security, and validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw earlier, one of the main issues with real data is privacy (for more
    information, please refer to [*Chapter 3*](B18494_03.xhtml#_idTextAnchor049)).
    Unfortunately, even with synthetic data, it is still a concern.
  prefs: []
  type: TYPE_NORMAL
- en: There is usually a trade-off between data usefulness and privacy when dealing
    with sensitive data. Synthetic data generators for certain problems in fields
    such as healthcare and finance are usually trained on sensitive data. Thus, there
    is a chance that the generated synthetic data may reveal sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: While generating the synthetic data is the main challenge, there are still other
    tasks to be performed before deploying synthetic data for your ML problem. Synthetic
    data needs to be evaluated and validated. Therefore, an effective risk assessment
    procedure should be performed to ensure that synthetic data is anonymous and still
    represents the phenomenon under consideration. As privacy attacks evolve, synthetic
    data generation procedures need to be assessed and monitored continuously to ensure
    that the generated synthetic data does not breach regulations or disclose sensitive
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore another interesting factor, which is associated with the
    sociology of customers.
  prefs: []
  type: TYPE_NORMAL
- en: Trust and credibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML in general is still a new, emerging field and synthetic data has only been
    utilized recently. Thus, it requires time for companies, customers, and ML practitioners
    to understand and trust synthetic data. Let’s discuss the main two elements under
    this category that usually limit the usability of synthetic data in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Consumer skepticism and lack of familiarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As companies have just started to deploy more synthetic data-based ML solutions,
    customers have also started to question the usability of this new approach. One
    of the main reasons behind this is their misunderstanding of or unfamiliarity
    with synthetic data generation approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Perception of artificiality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Synthetic data is not collected from the real world. Rather, it is generated
    artificially. Its synthetic nature causes customers to question its usability
    and genuineness. Thus, they may question and not trust this new data source or
    any ML solution based on it.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have understood the key general issues that limit the usability
    of synthetic data, let’s examine a wide range of domain-specific issues commonly
    seen in certain fields, such as healthcare and finance.
  prefs: []
  type: TYPE_NORMAL
- en: "Domain-specific issues limiting the usability of \Lsynthetic data"
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to general issues that limit the usability of synthetic data in
    practice, there are also domain-specific issues related to that. In this section,
    we explore these common domain-specific issues limiting the usability of synthetic
    data. Let’s study synthetic data usability issues in the following three fields:
    healthcare, finance, and autonomous cars.'
  prefs: []
  type: TYPE_NORMAL
- en: Healthcare
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML in healthcare requires large-scale training data. Usually, the data is unstructured,
    comes from different sensors and sources, is longitudinal (data collected over
    a long period), is highly imbalanced, and contains sensitive information. The
    illnesses and diseases that patients suffer from are diverse and complex and depend
    on a multitude of factors, such as genes, geographic location, medical conditions,
    and occupation. Thus, to generate useful synthetic training data in the healthcare
    field, domain experts are usually needed to assess the quality of the generated
    training data and the validity of the assumptions made by ML engineers. For more
    information, please refer to *Amplifying Domain Expertise in Clinical Data* *Pipelines*
    ([https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7677017](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7677017)).
  prefs: []
  type: TYPE_NORMAL
- en: Finance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This field is usually associated with rapid changes, being influenced by a huge
    number of factors and elements that are usually very hard to predict, such as
    politics, regulations, competitions, new technologies, and natural catastrophes.
    Thus, it is not easy to generate synthetic data that takes into consideration
    the dynamics of the market and other factors. Consequently, applying domain knowledge
    to the synthetic generation pipeline may significantly improve the usability of
    the generated synthetic data for this field. For more details, please refer to
    *Expectations, competencies and domain knowledge in data- and machine-driven*
    *finance* ([https://www.tandfonline.com/doi/full/10.1080/03085147.2023.2216601](https://www.tandfonline.com/doi/full/10.1080/03085147.2023.2216601)).
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous cars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simulating 3D virtual worlds is a hard task. However, what is more challenging
    is simulating drivers’ and pedestrians’ behaviors. In the real world, human behaviors
    are complex, hard to anticipate, and highly dependent on the environment and situation.
    For example, drivers and pedestrians may not obey traffic regulations and rules
    in the events of natural disasters and evacuations. Generating synthetic data
    that incorporates and anticipates similar scenarios is very complex and not easy
    to achieve. Additionally, simulators usually need to make many assumptions to
    simplify computations. However, the consequences of these assumptions may not
    always be clear and may cause ML models to fail in critical and rare situations.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s learn some best practices to unlock the full potential of synthetic
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for the effective utilization of synthetic data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn about some common good practices that can improve
    the usability of your synthetic data-based ML solution in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Understand the problem**: Before you start deploying synthetic data, you
    need to understand what the problem with your ML model and data is and why the
    available real datasets are not suitable. Do not jump directly to the synthetic
    data solution if you are not fully aware of the problem and the limitations of
    the available real data-based solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understand the synthetic data generation pipeline**: We should not consider
    the synthetic data generation pipeline as a black box. However, we need a good
    understanding of the generation process to avoid biases and artifacts. For example,
    suppose we are generating synthetic data for an application to flag fraudulent
    transactions. If our synthetic data generator often generates the majority of
    fraudulent transactions with certain attributes, such as a transaction amount
    between 10K and 12K and the transaction location being some specific country,
    our ML model, trained on this biased data, will tend to mistakenly identify any
    transaction with these attributes to be fraudulent regardless of other crucial
    attributes! As expected, this will make our ML model perform poorly in practice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diversity, variability, and realism**: For synthetic data to be useful in
    practice, it should usually be diverse, rich, and realistic and match the distribution
    of real dataset counterparts. Please refer to *Diversity in Machine Learning*
    ([https://arxiv.org/pdf/1807.01477.pdf](https://arxiv.org/pdf/1807.01477.pdf))
    and *Enhancing Photorealism Enhancement* ([https://arxiv.org/abs/2105.04619](https://arxiv.org/abs/2105.04619)).
    It is always recommended that you analyze the available real data (if any) and
    identify the key variabilities and properties that you wish your synthetic dataset
    to address.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuously validate and evaluate**: You should always and frequently compare
    and assess the quality of the generated synthetic data to ensure that the data
    generation pipeline is working as expected. For example, if you are working with
    sensitive data, you should continuously assess the generated synthetic data to
    ensure that it does not disclose any sensitive information and to ensure a high-quality
    anonymization procedure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combine synthetic with real data**: It is often suggested to combine synthetic
    with real data to achieve the best results. Training on a mixture of both or pre-training
    on synthetic data and fine-tuning on real data are well-known approaches to improve
    the usability of synthetic data. Please refer to *Semantic Segmentation under
    Adverse Conditions: A Weather and Nighttime-aware Synthetic Data-based Approach*
    ([https://bmvc2022.mpi-inf.mpg.de/0977.pdf](https://bmvc2022.mpi-inf.mpg.de/0977.pdf))
    and *Using synthetic data for person tracking under adverse weather* *conditions*
    ([https://doi.org/10.1016/j.imavis.2021.104187](https://doi.org/10.1016/j.imavis.2021.104187)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise and anomalies**: One of the main common issues is ignoring or underestimating
    the benefits of outliers and rare scenarios when generating synthetic data. Try
    to always include these circumstances as they are essential to ensure that your
    ML model does not fail in these situations in the real world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the primary challenges of deploying synthetic
    data. Then, we delved into domain-specific issues. We learned why synthetic data
    is inherently challenging, especially in fields such as healthcare and finance.
    Finally, we explored a list of best practices to improve the usability of your
    synthetic data in practice. Next, we will focus in more detail on enhancing and
    improving synthetic data usability through synthetic-to-real domain adaptation
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Part 5:Current Challenges and Future Perspectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you learn about a well-known issue that usually limits the usability
    of synthetic data. You will learn about the domain gap problem and why diversity
    and photorealism are some of the main challenges toward generating useful and
    large-scale synthetic data in practice. You will learn about various approaches
    to bridge the domain gap and improve the diversity and photorealism of your synthetic
    data. Then, we will recap the benefits of synthetic data-based solutions, challenges,
    and limitations. Finally, we will highlight some interesting future perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B18494_14.xhtml#_idTextAnchor230), *Synthetic-to-Real Domain
    Adaptation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 15*](B18494_15.xhtml#_idTextAnchor247), *Diversity Issues in Synthetic
    Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 16*](B18494_16.xhtml#_idTextAnchor269), *Photorealism in Computer
    Vision*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 17*](B18494_17.xhtml#_idTextAnchor287), *Conclusion*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
