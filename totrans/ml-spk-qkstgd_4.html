<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Supervised Learning Using Apache Spark</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will develop, test, and evaluate supervised machine learning models applied to a variety of real-world use cases using Python, Apache Spark, and its machine learning library, <kbd>MLlib</kbd>. Specifically, we will train, test, and interpret the following types of supervised machine learning models:</p>
<ul>
<li>Univariate linear regression</li>
<li>Multivariate linear regression</li>
<li>Logistic regression</li>
<li>Classification and regression trees</li>
<li>Random forests</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear regression</h1>
                </header>
            
            <article>
                
<p>The first supervised learning model that we will study is that of linear regression. Formally, linear regression models the relationship between a <em>dependent</em> variable using a set of one or more <em>independent</em> variables. The resulting model can then be used to predict the numerical value of the <em>dependent</em> variable. But what does this mean in practice? Well, let's look at our first real-world use case to make sense of this.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Case study – predicting bike sharing demand</h1>
                </header>
            
            <article>
                
<p>Bike sharing schemes have become very popular across the world over the last decade or so as people seek a convenient means to travel within busy cities while limiting their carbon footprint and helping to reduce road congestion. If you are unfamiliar with bike sharing systems, they are very simple; people rent a bike from certain locations in a city and thereafter return that bike to either the same or another location once they have finished their journey. In this example, we will be examining whether we can predict the daily demand for bike sharing systems given the weather on a particular day!</p>
<div class="packt_infobox">The dataset that we will be using has been derived from the <strong>University of California's</strong> (<strong>UCI</strong>) machine learning repository found at <a href="https://archive.ics.uci.edu/ml/index.php">https://archive.ics.uci.edu/ml/index.php</a>. The specific bike sharing dataset that we will use, available from both the GitHub repository accompanying this book and from <a href="https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset">https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset</a>, has been cited by Fanaee-T, Hadi, and Gama, Joao, 'Event labeling combining ensemble detectors and background knowledge,' Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg.</div>
<p>If you open <kbd>bike-sharing-data/day.csv</kbd><em> </em><span>in any text editor</span>, from either the GitHub repository accompanying this book or from UCI's machine learning repository, you will find bike sharing data aggregated on a daily basis over 731 days using the following schema:</p>
<table style="border-color: #000000;border-collapse: collapse" border="1">
<tbody>
<tr>
<td style="padding: 5px">
<p><strong>Column name</strong></p>
</td>
<td style="padding: 5px">
<p><strong>Data type</strong></p>
</td>
<td style="padding: 5px">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>instant</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Integer</kbd></p>
</td>
<td style="padding: 5px">
<p>Unique record identifier (primary key)</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>dteday</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Date</kbd></p>
</td>
<td style="padding: 5px">
<p>Date</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>season</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Integer</kbd></p>
</td>
<td style="padding: 5px">
<p>Season (1 – spring, 2 <span>–</span> summer, 3 <span>–</span> fall, 4 <span>–</span> winter)</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>yr</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Integer</kbd></p>
</td>
<td style="padding: 5px">
<p>Year</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>mnth</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Integer</kbd></p>
</td>
<td style="padding: 5px">
<p>Month</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>holiday</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Integer</kbd></p>
</td>
<td style="padding: 5px">
<p>Day is a holiday or not</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>weekday</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Integer</kbd></p>
</td>
<td style="padding: 5px">
<p>Day of the week</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>workingday</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Integer</kbd></p>
</td>
<td style="padding: 5px">
<p>1 <span>–</span> neither a weekend nor a holiday, 0 <span>–</span> otherwise</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>weathersit</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Integer</kbd></p>
</td>
<td style="padding: 5px">
<p>1 <span>–</span> clear, 2 <span>–</span> mist, 3 <span>–</span> light snow, 4 <span>–</span> heavy rain</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>temp</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Double</kbd></p>
</td>
<td style="padding: 5px">
<p>Normalized temperature in Celsius</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>atemp</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Double</kbd></p>
</td>
<td style="padding: 5px">
<p>Normalized feeling temperature in Celsius</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>hum</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Double</kbd></p>
</td>
<td style="padding: 5px">
<p>Normalized humidity</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>windspeed</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Double</kbd></p>
</td>
<td style="padding: 5px">
<p>Normalized wind speed</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>casual</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Integer</kbd></p>
</td>
<td style="padding: 5px">
<p>Count of casual users for that day</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>registered</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Integer</kbd></p>
</td>
<td style="padding: 5px">
<p>Count of registered users for that day</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>cnt</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Integer</kbd></p>
</td>
<td style="padding: 5px">
<p>Count of total bike renters that day</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="mce-root">Using this dataset, can we predict the total bike renters for a given day (<em>cnt</em>) given the weather patterns for that particular day? In this case, <em>cnt</em> is the <em>dependent</em> variable that we wish to predict based on a set of <em>independent</em> variables that we shall choose from.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Univariate linear regression</h1>
                </header>
            
            <article>
                
<p>Univariate (or single-variable) linear regression refers to a linear regression model where we use only one independent variable <em>x</em> to learn a <em>linear</em> function that maps <em>x</em> to our dependent variable <em>y:</em></p>
<p style="padding-left: 180px" class="CDPAlignLeft CDPAlign"><img src="Images/e16cc5f8-50c1-4e2f-b9c8-7cfcfb1aa96f.png" style="width:11.25em;height:1.67em;" width="1550" height="230"/></p>
<p>In the preceding equation, we have the following:</p>
<ul>
<li><em>y<sup>i</sup></em> represents the <em>dependent</em> variable (cnt) for the <em>i<sup>th</sup></em> observation</li>
<li><em>x<sup>i</sup></em> represents the single <em>independent</em> variable for the <em>i<sup>th</sup></em> observation</li>
<li>ε<sup><em>i</em></sup> represents the <em>error</em> term for the <em>i<sup>th</sup></em> observation</li>
<li><em>β<sub>0</sub></em> is the intercept coefficient</li>
<li><em>β<sub>1</sub></em> is the regression coefficient for the single independent variable</li>
</ul>
<p>Since, in general form, a univariate linear regression model is a linear function, we can easily plot this on a scatter graph where the x-axis represents the single independent variable, and the y-axis represents the dependent variable that we are trying to predict. <em>Figure 4.1</em> illustrates the scatter plot generated when we plot normalized feeling temperature (independent variable) against total daily bike renters (dependent variable):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-562 image-border" src="Images/12c53f09-7f55-47e1-b8e7-210d12522132.png" style="width:48.50em;height:28.00em;" width="1096" height="633"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4.1: Normalized temperature against total daily bike renters</div>
<p>By analyzing <em>Figure 4.1</em>, you will see that there seems to be a general positive linear trend between the normalized feeling temperature (<strong>atemp</strong>) and the total daily biker renters (<strong>cnt</strong>). However, you will also see that our blue trend line, which is the visual representation of our univariate linear regression function, is not perfect, <span>in other words, </span>not all of our data points fit exactly on this line. In the real world, it is extremely rare to have a perfect model; in other words, all predictive models will make some mistakes. The goal therefore is to minimize the number of mistakes our models make so that we may have confidence in the predictions that they provide.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Residuals</h1>
                </header>
            
            <article>
                
<p>The errors (or mistakes) that our model makes are called error terms or <em>residuals</em>, and are denoted in our univariate linear regression equation by ε<sup><em>i</em></sup>. Our goal therefore is to choose regression coefficients for the independent variables (in our case <em>β<sub>1</sub></em>) that minimize these residuals. To compute the <em>i<sup>th</sup></em> residual, we can simply subtract the predicted value from the actual value, as illustrated in <em>Figure 4.1</em>. To quantify the quality of our regression line, and hence our regression model, we can use a metric called the <strong>Sum of Squared Errors</strong> (<strong>SSE</strong>), which is simply the sum of all squared residuals, as follows:</p>
<p style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="Images/18a90520-c676-47aa-8d27-6ca44ac2dbf9.png" style="width:15.92em;height:1.50em;" width="2570" height="240"/></p>
<p>A smaller SSE implies a better fit. However, SSE as a metric to quantify the quality of our regression model has its limitations. SSE scales with the number of data points <em>N</em>, which means that if we doubled the number of data points, the SSE may be twice as large, which may lead you to believe that the model is twice as bad, which is not the case! We therefore require other means to quantify the quality of our model.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Root mean square error</h1>
                </header>
            
            <article>
                
<p>The <strong>root mean square error</strong> (<strong>RMSE</strong>) is the square root of the SSE divided by the total number of data points <em>N</em>, as follows:</p>
<p style="padding-left: 210px" class="CDPAlignLeft CDPAlign"><img src="Images/dff36623-b263-4ab0-8f39-9c455faf0dc6.png" style="width:7.83em;height:2.17em;" width="1370" height="380"/></p>
<p>The RMSE tends to be used more often as a means to quantify the quality of a linear regression model, since its units are the same as the dependent variable, and is normalized by N.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">R-squared</h1>
                </header>
            
            <article>
                
<p>Another metric that provides an error measure of a linear regression model is called the R<sup><em>2</em></sup> (R-squared) metric. The R<sup>2</sup> metric represents the proportion of <em>variance</em> in the dependent variable explained by the independent variable(s). The equation for calculating R<sup>2</sup> is as follows:</p>
<p style="padding-left: 210px" class="CDPAlignLeft CDPAlign"><img src="Images/8baa308e-d8df-4565-bcc7-2b0b5bd2d866.png" style="width:7.33em;height:1.83em;" width="1130" height="280"/></p>
<p>In this equation, SST refers to the <strong>Total Sum of Squares</strong>, which is just the SSE from the overall mean (as illustrated in <em>Figure 4.1</em> by the red horizontal line, which is often used as a <strong>baseline</strong> model). An R<sup>2</sup> value of 0 implies a linear regression model that provides no improvement over the baseline model (in other words, SSE = SST). An R<sup>2</sup> value of 1 implies a perfect predictive linear regression model (<span>in other words,</span> SSE = 0). The aim therefore is to get an R<sup>2</sup> value as close as possible to 1.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Univariate linear regression in Apache Spark</h1>
                </header>
            
            <article>
                
<p>Returning to our case study, let's develop a univariate linear regression model in Apache Spark using its machine learning library, <kbd>MLlib</kbd>, in order to predict the total daily bike renters using our bike sharing dataset:</p>
<div class="packt_infobox">The following sub-sections describe each of the pertinent cells in the corresponding Jupyter Notebook for this use case, entitled <kbd>chp04-01-univariate-linear-regression.ipynb</kbd>, and which may be found in the GitHub repository accompanying this book.</div>
<ol>
<li>First, we i<span>mport the required Python dependencies, including </span><kbd>pandas</kbd><span> (Python data analysis library), </span><kbd>matplotlib</kbd><span> (Python plotting library), and </span><kbd>pyspark</kbd><span> (Apache Spark Python API). By using the <kbd>%matplotlib</kbd> magic function, any plots that we generate will automatically be rendered within the Jupyter Notebook cell output:</span></li>
</ol>
<pre style="padding-left: 60px">%matplotlib inline<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>import findspark<br/>findspark.init()<br/>from pyspark import SparkContext, SparkConf<br/>from pyspark.sql import SQLContext<br/>from pyspark.ml.feature import VectorAssembler<br/>from pyspark.ml.regression import LinearRegression<br/>from pyspark.ml.evaluation import RegressionEvaluator</pre>
<ol start="2">
<li><span>Before we instantiate a Spark context, it is generally a good idea to load a sample of any pertinent dataset into </span><kbd>pandas</kbd><span> so that we may identify any trends or patterns before developing a predictive model. Here, we use the <kbd>pandas</kbd> library to load the </span>entire<span> CSV into a <kbd>pandas</kbd> DataFrame called</span> <kbd>bike_sharing_raw_df</kbd><span> (since it is a very small dataset anyway):</span></li>
</ol>
<pre style="padding-left: 60px">bike_sharing_raw_df = pd.read_csv('&lt;Path to CSV file&gt;', <br/>   delimiter = '&lt;delimiter character&gt;')<br/>bike_sharing_raw_df.head()</pre>
<ol start="3">
<li><span>In cells 3.1 to 3.4, we use the </span><kbd>matplotlib</kbd><span> library to plot various independent variables (<kbd>temp</kbd>, <kbd>atemp</kbd>, <kbd>hum</kbd>, and <kbd>windspeed</kbd>) against the dependent variable (<kbd>cnt</kbd>):</span></li>
</ol>
<pre style="padding-left: 60px">bike_sharing_raw_df.plot.scatter(x = '&lt;Independent Variable&gt;', <br/>   y = '&lt;Dependent Variable&gt;')</pre>
<p style="padding-left: 60px"><span>As you can see in <em>Figure 4.2</em>, there is a general positive linear relationship between the normalized temperatures (<kbd>temp</kbd> and <kbd>atemp</kbd>) and the total daily bike renters (cnt). However, there is no such obvious trend when using humidity and wind speed as our independent variables. Therefore, we will proceed to develop a univariate linear regression model using normalized feeling temperature (<kbd>atemp</kbd>) as our single independent variable, with total daily bike renters (<kbd>cnt</kbd>) being our dependent variable:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-563 image-border" src="Images/5415139f-e95c-4d00-a697-a67b1968a7df.png" style="width:27.00em;height:18.00em;" width="802" height="535"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4.2: Bike sharing scatter plot</div>
<ol start="4">
<li><span>In order to develop a Spark application, we need to first instantiate a Spark context (as described in <a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem</em>) to connect to our local Apache Spark cluster. We also instantiate a Spark <kbd>SQLContext</kbd> for the structured processing of our dataset:</span></li>
</ol>
<pre style="padding-left: 60px">conf = SparkConf().setMaster("spark://192.168.56.10:7077")<br/>   .setAppName("Univariate Linear Regression - Bike Sharing")<br/>sc = SparkContext(conf=conf)<br/>sqlContext = SQLContext(sc)</pre>
<ol start="5">
<li><span>We can now load our CSV dataset into a Spark DataFrame (see <a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem</em>) called</span> <kbd>bike_sharing_df</kbd><span>. We use the </span><kbd>SQLContext</kbd><span> previously defined and we tell Spark to use the first row as the header row and to infer the schema data types:</span></li>
</ol>
<pre style="padding-left: 60px">bike_sharing_df = sqlContext.read<br/>   .format('com.databricks.spark.csv')<br/>   .options(header = 'true', inferschema = 'true')<br/>   .load('Path to CSV file')<br/>bike_sharing_df.head(10)<br/>bike_sharing_df.printSchema()</pre>
<ol start="6">
<li><span>Before developing a predictive model, it is also a good idea to generate standard statistical metrics for a dataset so as to gain additional insights. Here, we generate the row count for the DataFrame, as well as calculating the mean average, standard deviation, and the minimum and maximum for each column. We achieve this using the </span><kbd>describe()</kbd><span> method for a Spark DataFrame as follows:</span></li>
</ol>
<pre style="padding-left: 60px">bike_sharing_df.describe().toPandas().transpose()</pre>
<ol start="7">
<li><span>We now demonstrate how to plot a dataset using a Spark DataFrame as an input. In this case, we simply convert the Spark DataFrame into a </span><kbd>pandas</kbd> <span>DataFrame before plotting as before (note that for very large datasets, it is recommended to use a representative sample of the dataset for plotting purposes):</span></li>
</ol>
<pre style="padding-left: 60px">bike_sharing_df.toPandas().plot.scatter(x='atemp', y='cnt')</pre>
<ol start="8">
<li><span>Now that we have finished our exploratory analysis, we can start developing our univariate linear regression model! First, we need to convert our independent variable (<kbd>atemp</kbd>) into a </span><em>numerical feature vector</em><span> (see <a href="a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml" target="_blank">Chapter 3</a>, <em>Artificial Intelligence and Machine Learning</em>). We can achieve this using MLlib's </span><kbd>VectorAssembler</kbd>,<span> which will take one or more feature columns, convert them into feature vectors, and store those feature vectors in an output column, which, in this example, is called <kbd>features</kbd>:</span></li>
</ol>
<pre style="padding-left: 60px">univariate_feature_column = 'atemp'<br/>univariate_label_column = 'cnt'<br/>vector_assembler = VectorAssembler(<br/>   inputCols = [univariate_feature_column], <br/>   outputCol = 'features')</pre>
<p style="padding-left: 60px"><span>We then apply the <kbd>VectorAssembler</kbd> <em>transformer</em> (see <a href="a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml" target="_blank">Chapter 3</a>, <em>Artificial Intelligence and Machine Learning</em>) to the raw dataset and identify the column that contains our label (in this case, our dependent variable </span><kbd>cnt</kbd><span>). The output is a new Spark DataFrame called</span> <kbd>bike_sharing_features_df</kbd><span> containing our independent numerical feature vectors (<kbd>atemp</kbd>) mapped to a known label (<kbd>cnt</kbd>):</span></p>
<pre style="padding-left: 60px">bike_sharing_features_df = vector_assembler<br/>   .transform(bike_sharing_df)<br/>   .select(['features', univariate_label_column])<br/>bike_sharing_features_df.head(10)</pre>
<ol start="9">
<li><span>As per supervised learning models in general, we need a </span><em>training</em><span> dataset to train our model in order to learn the mapping function, and a </span><em>test</em><span> dataset in order to evaluate the performance of our model. We can randomly split our raw labeled feature vector DataFrame using the </span><kbd>randomSplit()</kbd><span> method and a </span>seed<span>, which is used to initialize the random generator, and which can be any number you like. Note that if you use a different seed, you will get a different random split between your training and test dataset, which means that you may get slightly different coefficients for your final linear regression model:</span></li>
</ol>
<pre style="padding-left: 60px">train_df, test_df = bike_sharing_features_df<br/>   .randomSplit([0.75, 0.25], seed=12345)<br/>train_df.count(), test_df.count()</pre>
<p style="padding-left: 60px"><span>In our case, 75% of the original rows will form our training DataFrame called</span> <kbd>train_df</kbd><span>, with the remaining 25% forming our test DataFrame called</span> <kbd>test_df</kbd><span>, while using a <kbd>seed</kbd> of <kbd>12345</kbd>.</span></p>
<ol start="10">
<li><span>We are now ready to train our univariate linear regression model! We achieve this by using <kbd>MLlib</kbd>'s </span><kbd>LinearRegression</kbd><span> </span>estimator<span> (see <a href="a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml" target="_blank">Chapter 3</a>, <em>Artificial Intelligence and Machine Learning</em>) and passing it the name of the column containing our independent numerical feature vectors (in our case, called <kbd>features</kbd>) and the name of the column containing our labels (in our case, called <kbd>cnt</kbd>). We then apply the </span><kbd>fit()</kbd><span> method to train our model and output a linear regression </span><em>transformer</em><span> which, in our case, is called</span> <kbd>linear_regression_model</kbd><span>:</span></li>
</ol>
<pre style="padding-left: 60px">linear_regression = LinearRegression(featuresCol = 'features', <br/>   labelCol = univariate_label_column)<br/>linear_regression_model = linear_regression.fit(train_df)</pre>
<ol start="11">
<li><span>Before we evaluate our trained univariate linear regression model on the test DataFrame, let's generate some summary statistics for it. The transformer model exposes a series of statistics, including model coefficients (in other words,</span><span> </span><em>β<sub>1</sub></em><span> </span><span>in our case), the intercept coefficient</span><span> </span><em>β<sub>0</sub></em><span>, the error metrics RMSE and R</span><sup>2</sup><span><span>, and the set of residuals for each data point. </span></span>In our case, we have the following:<br/>
<br/>
<ul>
<li>β<sub>0</sub><span> </span>= 829.62</li>
<li>β<sub>1</sub><span> </span>= 7733.75</li>
<li>RMSE = 1490.12</li>
<li>R<sup>2</sup><span> </span>= 0.42</li>
</ul>
</li>
</ol>
<pre style="padding-left: 60px">print("Model Coefficients: " + <br/>   str(linear_regression_model.coefficients))<br/>print("Intercept: " + str(linear_regression_model.intercept))<br/>training_summary = linear_regression_model.summary<br/>print("RMSE: %f" % training_summary.rootMeanSquaredError)<br/>print("R-SQUARED: %f" % training_summary.r2)<br/>print("TRAINING DATASET DESCRIPTIVE SUMMARY: ")<br/>train_df.describe().show()<br/>print("TRAINING DATASET RESIDUALS: ")<br/>training_summary.residuals.show()</pre>
<p style="padding-left: 60px">Therefore, our trained univariate linear regression model has learned the following function in order to be able to predict our dependent variable<span> </span><em>y</em><span> </span>(total daily bike renters) using a single independent variable<span> </span><em>x</em><span> </span>(normalized feeling temperature):</p>
<p style="padding-left: 60px"><em>y = 829.62 + 7733.75x</em></p>
<ol start="12">
<li><span>Let's now apply our trained model to our test DataFrame in order to evaluate its performance on test data. Here, we apply our trained linear regression model transformer to the test DataFrame using the </span><kbd>transform()</kbd><span> method in order to make predictions. For example, our model predicts a total daily bike rental count of 1742 given a normalized feeling temperature of 0.11793. The actual total daily bike rental count was 1416 (an error of 326):</span></li>
</ol>
<pre style="padding-left: 60px">test_linear_regression_predictions_df = <br/>   linear_regression_model.transform(test_df)<br/>test_linear_regression_predictions_df<br/>   .select("prediction", univariate_label_column, "features")<br/>   .show(10)</pre>
<ol start="13">
<li><span>We now compute the same RMSE and R</span><sup>2</sup><span> error metrics, but based on the performance of our model on the </span><em>test </em><span>DataFrame. In our case, these are 1534.51 (RMSE) and 0.34 (R</span><sup>2</sup><span>) respectively, calculated using <kbd>MLlib</kbd>'s </span><kbd>RegressionEvaluator</kbd>. <span>So, in our case, our trained model actually performs more poorly on the test dataset:</span></li>
</ol>
<pre style="padding-left: 60px">linear_regression_evaluator_rmse = RegressionEvaluator(<br/>   predictionCol = "prediction", <br/>   labelCol = univariate_label_column, metricName = "rmse")<br/>linear_regression_evaluator_r2 = RegressionEvaluator(<br/>   predictionCol = "prediction", <br/>   labelCol = univariate_label_column, metricName = "r2")<br/>print("RMSE on Test Data = %g" % linear_regression_evaluator_rmse<br/>   .evaluate(test_linear_regression_predictions_df))<br/>print("R-SQUARED on Test Data = %g" % <br/>   linear_regression_evaluator_r2<br/>   .evaluate(test_linear_regression_predictions_df))</pre>
<ol start="14">
<li><span>Note that we can generate the same metrics but using the </span><kbd>evaluate()</kbd><span> method of the linear regression model, as shown in the following code block:</span></li>
</ol>
<pre style="padding-left: 60px">test_summary = linear_regression_model.evaluate(test_df)<br/>print("RMSE on Test Data = %g" % test_summary.rootMeanSquaredError)<br/>print("R-SQUARED on Test Data = %g" % test_summary.r2)</pre>
<ol start="15">
<li><span>Finally, we terminate our Spark application by stopping the Spark context:</span></li>
</ol>
<pre style="padding-left: 60px">sc.stop()</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Multivariate linear regression</h1>
                </header>
            
            <article>
                
<p>Our univariate linear regression model actually performed relatively poorly on both the training and test datasets, with R<sup>2</sup> values of 0.42 on the training dataset and 0.34 on the test dataset respectively. Is there any way we can take advantage of the other independent variables available in our raw dataset to increase the predictive quality of our model?</p>
<p>Multivariate (or multiple) linear regression extends univariate linear regression by allowing us to utilize more than one independent variable, in this case <em>K</em> independent variables, as follows:</p>
<p style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="Images/92ac9e48-9e36-422a-b81b-5edbb280f7c0.png" style="width:23.67em;height:1.92em;" width="3100" height="250"/></p>
<p>As before, we have our dependent variable <em>y<sup>i</sup></em> (for the <em>i<sup>th</sup></em> observation), an intercept coefficient <em>β<sub>0</sub>,</em> and our residuals ε<sup><em>i</em></sup>. But we also now have <em>k</em> independent variables, each with their own regression coefficient, <em>β<sub>k</sub></em>. The goal, as before, is to derive coefficients that minimize the amount of error that our model makes. The problem now though is how to choose which subset of independent variables to use in order to train our multivariate linear regression model. Adding more independent variables increases the complexity of models in general and, hence, the data storage and processing requirements of underlying processing platforms. Furthermore, models that are too complex tend to cause <strong>overfitting</strong>, whereby the model achieves better performance (<span>in other words,</span> a higher <em>R<sup>2</sup></em> metric) on the training dataset used to train the model than on new data that it has not seen before.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Correlation</h1>
                </header>
            
            <article>
                
<p>Correlation is a metric that measures the linear relationship between two variables, and helps us to decide which independent variables to include in our model:</p>
<ul>
<li>+1 implies a perfect positive linear relationship</li>
<li>0 implies no linear relationship</li>
<li>-1 implies a perfect negative linear relationship</li>
</ul>
<p>When two variables have an <em>absolute</em> value of correlation close to 1, then these two variables are said to be "highly correlated".</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Multivariate linear regression in Apache Spark</h1>
                </header>
            
            <article>
                
<p>Returning to our case study, let's now develop a multivariate linear regression model in order to predict the total daily bike renters using our bike sharing dataset and a subset of independent variables:</p>
<div class="packt_infobox"><span>The following sub-sections describe each of the pertinent cells in the corresponding Jupyter Notebook for this use case, entitled</span> <kbd>chp04-02-multivariate-linear-regression.ipynb</kbd><span>, and which may be found in the GitHub repository accompanying this book. Note that for the sake of brevity, we will skip those cells that perform the same functions as seen previously.</span></div>
<ol>
<li><span>First, let's demonstrate how we can use Spark to calculate the correlation value between our dependent variable, <kbd>cnt</kbd>, and each independent variable in our DataFrame. We achieve this by iterating over each column in our raw Spark DataFrame and using the </span><kbd>stat.corr()</kbd><span> method as follows:</span></li>
</ol>
<pre style="padding-left: 60px">independent_variables = ['season', 'yr', 'mnth', 'holiday', <br/>   'weekday', 'workingday', 'weathersit', 'temp', 'atemp', <br/>   'hum', 'windspeed']<br/>dependent_variable = ['cnt']<br/>bike_sharing_df = bike_sharing_df.select( independent_variables + <br/>   dependent_variable )<br/>for i in bike_sharing_df.columns:<br/>   print( "Correlation to CNT for ", <br/>      i, bike_sharing_df.stat.corr('cnt', i))</pre>
<p style="padding-left: 60px"><span>The resultant correlation matrix shows that the independent variables—</span><kbd>season</kbd><span>, </span><kbd>yr</kbd><span>, </span><kbd>mnth</kbd><span>, </span><kbd>temp</kbd><span> , and </span><kbd>atemp</kbd><span>, exhibit significant positive correlation with our dependent variable </span><kbd>cnt</kbd>. <span>We will therefore proceed to train a multivariate linear regression model using this subset of independent variables.</span></p>
<ol start="2">
<li><span>As seen previously, we can apply a </span><kbd>VectorAssembler</kbd><span> in order to generate numerical feature vector representations of our collection of independent variables along with the </span><kbd>cnt</kbd><span> label. The syntax is identical to that seen previously, but this time we pass multiple columns to the <kbd>VectorAssembler</kbd> representing the columns containing our independent variables:</span></li>
</ol>
<pre style="padding-left: 60px">multivariate_feature_columns = ['season', 'yr', 'mnth', <br/>   'temp', 'atemp']<br/>multivariate_label_column = 'cnt'<br/>vector_assembler = VectorAssembler(inputCols = <br/>   multivariate_feature_columns, outputCol = 'features')<br/>bike_sharing_features_df = vector_assembler<br/>   .transform(bike_sharing_df)<br/>   .select(['features', multivariate_label_column])</pre>
<ol start="3">
<li>We are now ready to generate our respective training and test datasets using the <kbd>randomSplit</kbd> method via the DataFrame API:</li>
</ol>
<pre style="padding-left: 60px">train_df, test_df = bike_sharing_features_df<br/>   .randomSplit([0.75, 0.25], seed=12345)<br/>train_df.count(), test_df.count()</pre>
<ol start="4">
<li>We can now train our multivariate linear regression model using the same <kbd>LinearRegression</kbd> estimator that we used in our univariate linear regression model:</li>
</ol>
<pre style="padding-left: 60px">linear_regression = LinearRegression(featuresCol = 'features', <br/>   labelCol = multivariate_label_column)<br/>linear_regression_model = linear_regression.fit(train_df)</pre>
<ol start="5">
<li><span>After splitting our original DataFrame into a training and test DataFrame respectively, and applying the same</span><span> </span><em>LinearRegression</em><span> </span><span><span>estimator to the training DataFrame, we now have a trained multivariate linear regression model with the following summary training statistics (as can be seen in cell 8 of this Jupyter Notebook):<br/>
<br/></span></span>
<ul>
<li>β<sub>0</sub><span> </span>= -389.94, β<sub>1</sub><span> </span>= 526.05, β<sub>2</sub><span> </span>= 2058.85, β<sub>3</sub><span> </span>= -51.90, β<sub>4</sub><span> </span>= 2408.66, β<sub>5</sub><span> </span>= 3502.94</li>
<li>RMSE = 1008.50</li>
<li>R<sup>2</sup><span> </span>= 0.73</li>
</ul>
</li>
</ol>
<p style="padding-left: 60px">Therefore, our trained multivariate linear regression model has learned the following function in order to be able to predict our dependent variable<span> </span><em>y</em><span> </span>(total daily bike renters) using a set of independent variables<span> </span><em>x<sub>k</sub></em><span> </span>(season, year, month, normalized temperature, and normalized feeling temperature):</p>
<p style="padding-left: 60px"><em>y = -389.94 + 526.05x<sub>1</sub><span> </span>+ 2058.85x<sub>2</sub><span> </span>- 51.90x<sub>3</sub><span> </span>+ 2408.66x<sub>4</sub><span> </span>+ 3502.94x<sub>5</sub></em></p>
<p style="padding-left: 60px">Furthermore, our trained multivariate linear regression model actually performs even better on the test dataset with a test RMSE of 964.60 and a test R<sup>2</sup><span> of</span> 0.74.</p>
<p>To finish our discussion of multivariate linear regression models, note that our training R<sup>2</sup> metric will always either increase or stay the same as more independent variables are added. However, a better training R<sup>2</sup> metric does not always imply a better test R<sup>2</sup> metric—in fact, a test R<sup>2</sup> metric can even be negative, meaning that it performs worse on the test dataset than the baseline model (which can never be the case for the training R<sup>2</sup> metric). The goal therefore is to be able to develop a model that works well for both the training and test datasets.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Logistic regression</h1>
                </header>
            
            <article>
                
<p>We have seen how linear regression models allows us to predict a numerical outcome. Logistic regression models, however, allow us to predict a <em>categorical</em> outcome by predicting the probability that an outcome is true.</p>
<p>As with linear regression, in logistic regression models, we also have a dependent variable <em>y</em> and a set of independent variables <em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>, …, <em>x<sub>k</sub></em>. In logistic regression however, we want to learn a function that provides the probability that <em>y = 1</em> (<span>in other words,</span> that the outcome variable is true) given this set of independent variables, as follows:</p>
<p style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="Images/fcd811e4-d51a-4d40-b1b2-3c4669141f9c.png" style="width:24.17em;height:2.83em;" width="2560" height="300"/></p>
<p>This function is called the <strong>Logistic Response</strong> function, and provides a number between 0 and 1, representing the probability that the outcome-dependent variable is true, as illustrated in <em>Figure 4.3</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-564 image-border" src="Images/6fcc1d21-faf7-44d6-9ed3-8776dc37c6fe.png" style="width:20.83em;height:19.08em;" width="710" height="646"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4.3: Logistic response function</div>
<p>Positive coefficient values β<sub>k</sub> increase the probability that y = 1, and negative coefficient values decrease the probability that y = 1. Our goal, therefore, when developing logistic regression models, is to choose coefficients that predict a high probability when y = 1, but predict a low probability when y = 0.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Threshold value</h1>
                </header>
            
            <article>
                
<p>We now know that logistic regression models provide us with the probability that the outcome variable is true, that is to say, y = 1. However, in real-world use cases, we need to make <em>decisions</em>, not just deliver probabilities. Often, we make binary predictions, such as Yes/No, Good/Bad, and Go/Stop. A threshold value (<em>t</em>) allows us to make these decisions based on probabilities as follows:</p>
<ul>
<li>If P(y=1) &gt;= t, then we predict y = 1</li>
<li>If P(y=1) &lt; t, then we predict y = 0</li>
</ul>
<p>The challenge now is how to choose a suitable value of <em>t</em>. In fact, what does <em>suitable</em> mean in this context?</p>
<p>In real-world use cases, some types of error are better than others. Imagine that you were a doctor and were testing a large group of patients for a particular disease using logistic regression. In this case, the outcome <em>y=1</em> would be a patient carrying the disease (therefore y=0 would be a patient not carrying the disease), and, hence, our model would provide P(y=1) for a given person. In this example, it is better to detect as many patients potentially carrying the disease as possible, even if it means misclassifying some patients as carrying the disease who subsequently turn out not to. In this case, we select a smaller threshold value. If we select a large threshold value, however, we would detect those patients that almost certainly have the disease, but we would misclassify a large number of patients as not carrying the disease when, in actual fact, they do, which would be a much worse scenario!</p>
<p>In general therefore, when using logistic regression models, we can make two types of error:</p>
<ul>
<li>We predict y=1 (disease), but the actual outcome is y=0 (healthy)</li>
<li>We predict y=0 (healthy), but the actual outcome is y=1 (disease)</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Confusion matrix</h1>
                </header>
            
            <article>
                
<p>A confusion (or classification) matrix can help us qualify what threshold value to use by comparing the predicted outcomes against the actual outcomes as follows:</p>
<table style="border-color: #000000;border-collapse: collapse" border="1">
<tbody>
<tr>
<td style="padding: 5px"/>
<td style="padding: 5px">
<p><strong>Predict y=0 (healthy)</strong></p>
</td>
<td style="padding: 5px">
<p><strong>Predict y=1 (disease)</strong></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><strong>Actual y=0 (healthy)</strong></p>
</td>
<td style="padding: 5px">
<p><strong>True negatives</strong> (<strong>TN</strong>)</p>
</td>
<td style="padding: 5px">
<p><strong>False positives</strong> (<strong>FP</strong>)</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><strong>Actual y=1 (disease)</strong></p>
</td>
<td style="padding: 5px">
<p><strong>False negatives</strong> (<strong>FN</strong>)</p>
</td>
<td style="padding: 5px">
<p><strong>True positives</strong> (<strong>TP</strong>)</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>By generating a confusion matrix, it allows us to quantify the accuracy of our model based on a given threshold value by using the following series of metrics:</p>
<ul>
<li>N = number of observations</li>
<li>Overall accuracy = (TN + TP) / N</li>
<li>Overall error rate = (FP + FN) / N</li>
<li>Sensitivity (True Positive Rate) = TP / (TP + FN)</li>
<li>Specificity (True Negative Rate) = TN / (TN + FP)</li>
<li>False positive error rate = FP / (TN + FP)</li>
<li>False negative error rate = FN / (TP + FN)</li>
</ul>
<p>Logistic regression models with a higher threshold value will have a lower sensitivity and higher specificity. Models with a lower threshold value will have a higher sensitivity and lower specificity. The choice of threshold value therefore depends on the type of error that is "better" for your particular use case. In use cases where there is genuinely no preference, for example, political leaning of Conservative/Non-Conservative, then you should choose a threshold value of 0.5 that will predict the most likely outcome.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Receiver operator characteristic curve</h1>
                </header>
            
            <article>
                
<p>To further assist us in choosing a threshold value in a more visual way, we can generate a <strong>receiver operator characteristic</strong> (<strong>ROC</strong>) curve. An ROC curve plots the <strong>false positive error rate</strong> (<strong>FPR</strong>) against the <strong>true positive rate</strong> (<strong>TPR</strong>, or sensitivity) for every threshold value between 0 and 1, as illustrated in <em>Figure 4.4</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-565 image-border" src="Images/9366a343-9e08-481b-b1f5-b0b81bfb0c53.png" style="width:40.08em;height:32.33em;" width="1077" height="870"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4.4: ROC curve</div>
<p>As illustrated in <em>Figure 4.4</em>, using a threshold value of 0 means that you will catch ALL cases of y=1 (disease), but you will also incorrectly label all cases of y=0 (healthy) as y=1 (disease) as well, scaring a lot of healthy people! However using a threshold value of 1 means that you will NOT catch ANY cases of y=1 (disease), leaving a lot of people untreated, but you will correctly label all cases of y=0 (healthy). The benefit of plotting an ROC curve therefore is that it helps you to see the trade-off for <em>every</em> threshold value, and ultimately helps you to make a decision as to which threshold value to use for your given use case.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Area under the ROC curve</h1>
                </header>
            
            <article>
                
<p>As a means of quantifying the quality of the predictions made by a logistic regression model, we can calculate the <strong>Area under the ROC curve</strong> (<strong>AUC</strong>), as illustrated in <em>Figure 4.5</em>. The AUC measures the proportion of time that the model predicts correctly, with an AUC value of 1 (maximum), implying a perfect model, <span>in other words,</span> our model predicts correctly 100% of the time, and an AUC value of 0.5 (minimum), implying our model predicts correctly 50% of the time, analogous to just guessing:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-566 image-border" src="Images/4b08363c-6b44-4144-af01-3b21dc3b8799.png" style="width:30.75em;height:30.58em;" width="854" height="849"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.5: Area under the ROC curve</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Case study – predicting breast cancer</h1>
                </header>
            
            <article>
                
<p>Let's now apply logistic regression to a very important real-world use case; predicting patients who may have breast cancer. Approximately 1 in 8 women are diagnosed with breast cancer during their lifetime (with the disease also affecting men), resulting in the premature deaths of hundreds of thousands of women annually across the world. In fact, it is projected that over 2 million new cases of breast cancer will have been reported worldwide by the end of 2018 alone. Various factors are known to increase the risk of breast cancer, including age, weight, family history, and previous diagnoses.</p>
<p>Using a dataset of quantitative predictors, along with a binary dependent variable indicating the presence or absence of breast cancer, we will train a logistic regression model to predict the probability of whether a given patient is healthy (y=1) or has the biomarkers of breast cancer (y=0).</p>
<div class="packt_infobox">The dataset that we will use has again been derived from the University of California's (UCI) machine learning repository. The specific breast cancer dataset, available from both the GitHub repository accompanying this book and from <a href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra">https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra</a>, has been cited by [Patricio, 2018] Patrício, M., Pereira, J., Crisóstomo, J., Matafome, P., Gomes, M., Seiça, R., and Caramelo, F. (2018). Using Resistin, glucose, age, and BMI to predict the presence of breast cancer. BMC Cancer, 18(1).</div>
<p>If you open <kbd>breast-cancer-data/dataR2.csv</kbd><em> </em><span>in any text editor</span>, from either the GitHub repository accompanying this book or from UCI's machine learning repository, you will find breast cancer data that employs the following schema:</p>
<table style="border-color: #000000;border-collapse: collapse" border="1">
<tbody>
<tr>
<td style="padding: 5px">
<p><strong>Column name</strong></p>
</td>
<td style="padding: 5px">
<p><strong>Data type</strong></p>
</td>
<td style="padding: 5px">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>Age</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Integer</kbd></p>
</td>
<td style="padding: 5px">
<p>Age of patient</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>BMI</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Double</kbd></p>
</td>
<td style="padding: 5px">
<p>Body mass index (kg/m<sup>2</sup>)</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>Glucose</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Double</kbd></p>
</td>
<td style="padding: 5px">
<p>Blood glucose level (mg/dL)</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>Insulin</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Double</kbd></p>
</td>
<td style="padding: 5px">
<p>Insulin level (µU/mL)</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>HOMA</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Double</kbd></p>
</td>
<td style="padding: 5px">
<p>Homeostatic Model Assessment (HOMA) – used to assess β-cell function and insulin sensitivity</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>Leptin</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Double</kbd></p>
</td>
<td style="padding: 5px">
<p>Hormone used to regulate energy expenditure (ng/mL)</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>Adiponectin</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Double</kbd></p>
</td>
<td style="padding: 5px">
<p>Protein hormone used to regulate glucose levels (µg/mL)</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>Resistin</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Double</kbd></p>
</td>
<td style="padding: 5px">
<p>Hormone that causes insulin resistance (ng/mL)</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>MCP.1</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Double</kbd></p>
</td>
<td style="padding: 5px">
<p>Protein to aid recovery from injury and infection (pg/dL)</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>Classification</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Integer</kbd></p>
</td>
<td style="padding: 5px">
<p>1 = Healthy patient as part of a control group, 2 = patient with breast cancer</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>Using this dataset, can we develop a logistic regression model that calculates the probability of a given patient being healthy (in other words, y=1) and thereafter apply a threshold value to make a predictive decision?</p>
<div class="packt_infobox"><span>The following sub-sections describe each of the pertinent cells in the corresponding Jupyter Notebook for this use case, entitled</span> <kbd>chp04-03-logistic-regression.ipynb</kbd><span>, and which may be found in the GitHub repository accompanying this book. Note that, for the sake of brevity, we will skip those cells that perform the same functions as seen previously.</span></div>
<ol>
<li><span>After loading our breast cancer CSV file, we first identify the column that will act as our label, that is to say,</span> <kbd>Classification</kbd><span>. Since the values in this column are either 1 (healthy) or 2 (breast cancer patient), we will apply a </span><kbd>StringIndexer</kbd><span> to this column to identify and index all the possible categories. The result is that a label of 1 corresponds to a healthy patient, and a label of 0 corresponds to a breast cancer patient:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">indexer = StringIndexer(inputCol = "Classification", <br/>   outputCol = "label").fit(breast_cancer_df)<br/>breast_cancer_df = indexer.transform(breast_cancer_df)</pre></div>
<ol start="2">
<li><span>I</span><span>n our case, we will use all the raw quantitative columns [</span><kbd>Age</kbd>, <kbd>BMI</kbd>, <kbd>Glucose</kbd>, <kbd>Insulin</kbd>, <kbd>HOMA</kbd>, <kbd>Leptin</kbd>, <kbd>Adiponectin</kbd>, <kbd>Resistin</kbd>, and <kbd>MCP.1</kbd><span>] as independent variables in order to generate numerical feature vectors for our model. Again, we can use the</span> <kbd>VectorAssembler</kbd><span> of <kbd>MLlib</kbd> to achieve this:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">feature_columns = ['Age', 'BMI', 'Glucose', 'Insulin', 'HOMA', <br/>   'Leptin', 'Adiponectin', 'Resistin', 'MCP_1']<br/>label_column = 'label'<br/>vector_assembler = VectorAssembler(inputCols = feature_columns, <br/>   outputCol = 'features')</pre>
<pre style="padding-left: 60px">breast_cancer_features_df = vector_assembler<br/>   .transform(breast_cancer_df)<br/>   .select(['features', label_column])</pre></div>
<ol start="3">
<li><span>After generating training and test DataFrames respectively, we apply the</span> <kbd>LogisticRegression</kbd><span> estimator of <kbd>MLlib</kbd> to train a </span><kbd>LogisticRegression</kbd><span> model transformer:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">logistic_regression = LogisticRegression(featuresCol = 'features', <br/>   labelCol = label_column)<br/>logistic_regression_model = logistic_regression.fit(train_df)</pre></div>
<ol start="4">
<li><span>We then use our trained logistic regression model to make predictions on the test DataFrame, using the </span><kbd>transform()</kbd><span> method of our logistic regression model transformer. This results in a new DataFrame with the columns</span> <kbd>rawPrediction</kbd><span>,</span> <kbd>prediction</kbd><span>, and</span> <kbd>probability</kbd><span> appended to it. The probability of y=1, in other words, P(y=1), is contained within the</span> <kbd>probability</kbd><span> column, and the overall predictive decision using a default threshold value of t=0.5 is contained within the</span> <kbd>prediction</kbd><span> column:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">test_logistic_regression_predictions_df = logistic_regression_model<br/>   .transform(test_df)<br/>test_logistic_regression_predictions_df.select("probability", <br/>   "rawPrediction", "prediction", label_column, "features").show()</pre></div>
<ol start="5">
<li><span>To quantify the quality of our trained logistic regression model, we can plot an ROC curve and calculate the AUC metric. The ROC curve is generated using the </span><kbd>matplotlib</kbd><span> library, given the <strong>false positive rate</strong> (<strong>FPR</strong>) and <strong>true positive rate</strong> (<strong>TPR</strong>), as exposed by evaluating our trained logistic regression model on the test DataFrame. We can then use <kbd>MLlib</kbd>'s </span><kbd>BinaryClassificationEvaluator</kbd><span> to calculate the AUC metric as follows:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">test_summary = logistic_regression_model.evaluate(test_df)<br/>roc = test_summary.roc.toPandas()<br/>plt.plot(roc['FPR'],roc['TPR'])<br/>plt.ylabel('False Positive Rate')<br/>plt.xlabel('True Positive Rate')<br/>plt.title('ROC Curve')<br/>plt.show()<br/>evaluator_roc_area = BinaryClassificationEvaluator(<br/>   rawPredictionCol = "rawPrediction", labelCol = label_column, <br/>   metricName = "areaUnderROC")<br/>print("Area Under ROC Curve on Test Data = %g" %<br/>   evaluator_roc_area.evaluate(<br/>   test_logistic_regression_predictions_df))<br/><br/><strong>Area Under ROC Curve on Test Data = 0.859375<br/></strong></pre></div>
<p style="padding-left: 60px">The resultant ROC curve, generated using the <kbd>matplotlib</kbd> library, is illustrated in <em>Figure 4.6</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-567 image-border" src="Images/d2252ec8-5665-4f13-b5ca-f39a89803285.png" style="width:25.83em;height:18.75em;" width="794" height="574"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 4.6: ROC curve rendered using</span> <kbd>matplotlib</kbd></div>
<ol start="6">
<li><span>One method of generating a confusion matrix based on the test dataset predictions is to simply filter the test predictions' DataFrame based on cases where the predicted outcome equals, and does not equal, the actual outcome and thereafter count the number of records post-filter:</span></li>
</ol>
<pre style="padding-left: 60px">N = test_logistic_regression_predictions_df.count()<br/>true_positives = test_logistic_regression_predictions_df<br/>   .filter( col("prediction") == 1.0 )<br/>   .filter( col("label") == 1.0 ).count()<br/>true_negatives = test_logistic_regression_predictions_df<br/>   .filter( col("prediction") == 0.0 )<br/>   .filter( col("label") == 0.0 ).count()<br/>false_positives = test_logistic_regression_predictions_df<br/>   .filter( col("prediction") == 1.0 )<br/>   .filter( col("label") == 0.0 ).count()<br/>false_negatives = test_logistic_regression_predictions_df<br/>   .filter( col("prediction") == 0.0 )<br/>   .filter( col("label") == 1.0 ).count()</pre>
<ol start="7">
<li><span>Alternatively, we can use MLlib's RDD API (which is in maintenance mode as of Spark 2.0) to automatically generate the confusion matrix by converting the test predictions' DataFrame into an RDD (see <a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem</em>), and thereafter passing it to the </span><kbd>MulticlassMetrics</kbd><span> evaluation abstraction:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">predictions_and_label = test_logistic_regression_predictions_df<br/>   .select("prediction", "label").rdd<br/>metrics = MulticlassMetrics(predictions_and_label)<br/>print(metrics.confusionMatrix())</pre></div>
<p>The confusion matrix for our logistic regression model, using a default threshold value of 0.5, is as follows:</p>
<table style="border-color: #000000;border-collapse: collapse" border="1">
<tbody>
<tr>
<td style="padding: 5px">
<p> </p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>Predict y=0 (breast cancer)</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>Predict y=1 (healthy)</strong></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><strong>Actual y=0</strong></p>
<p><strong>(breast cancer)</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>10</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><strong>Actual y=1</strong></p>
<p><strong>(healthy)</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>8</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We can interpret this confusion matrix as follows. Out of a total of 28 observations, our model exhibits the following properties:</p>
<ul>
<li>Correctly labeling 10 cases of breast cancer that actually are breast cancer</li>
<li>Correctly labeling 8 healthy patients that actually are healthy patients</li>
<li>Incorrectly labeling 6 patients as healthy when they actually have breast cancer</li>
<li>Incorrectly labeling 4 patients as having breast cancer when they are actually healthy patients</li>
<li>Overall accuracy = 64%</li>
<li>Overall error rate = 36%</li>
<li>Sensitivity = 67%</li>
<li>Specificity = 63%</li>
</ul>
<p class="mce-root">To improve our logistic regression model, we must, of course, include many more observations. Furthermore, the AUC metric for our model is 0.86, which is quite high. However, bear in mind that the AUC is a measure of accuracy, taking into account all possible threshold values, while the preceding confusion matrix only takes into account a single threshold value (in this case 0.5). As an extension exercise, generate confusion matrices for a range of threshold values to see how this affects our final classifications!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classification and Regression Trees</h1>
                </header>
            
            <article>
                
<p>We have seen how linear regression models allow us to predict a numerical outcome, and how logistic regression models allow us to predict a categorical outcome. However, both of these models assume a <em>linear</em> relationship between variables. <strong>Classification and Regression Trees</strong> (<strong>CART</strong>) overcome this problem by generating <strong>Decision Trees</strong>, which are also much easier to interpret compared to the supervised learning models we have seen so far. These decision trees can then be traversed to come to a final decision, where the outcome can either be numerical (regression trees) or categorical (classification trees). A simple classification tree used by a mortgage lender is illustrated in <em>Figure 4.7</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-568 image-border" src="Images/13ca1e7c-ba16-47ce-952c-472c29d1c620.png" style="width:33.50em;height:33.50em;" width="1158" height="1160"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4.7: Simple classification tree used by a mortgage lender</div>
<p>When traversing decision trees, start at the top. Thereafter, traverse left for yes, or positive responses, and traverse right for no, or negative responses. Once you reach the end of a branch, the leaf nodes describe the final outcome.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Case study – predicting political affiliation</h1>
                </header>
            
            <article>
                
<p>For our next use case, we will use congressional voting records from the US House of Representatives to build a classification tree in order to predict whether a given congressman or woman is a Republican or a Democrat.</p>
<div class="packt_infobox">The specific congressional voting dataset that we will use is available from both the GitHub repository accompanying this book and UCI's machine learning repository at <a href="https://archive.ics.uci.edu/ml/datasets/congressional+voting+records">https://archive.ics.uci.edu/ml/datasets/congressional+voting+records</a>. It has been cited by Dua, D., and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.</div>
<p>If you open <kbd>congressional-voting-data/house-votes-84.data</kbd> in any text editor of your choosing, from either the GitHub repository accompanying this book or from UCI's machine learning repository, you will find 435 congressional voting records, of which 267 belong to Democrats and 168 belong to Republicans. The first column contains the label string, <span>in other words, </span>Democrat or Republican, and the subsequent columns indicate how the congressman or woman in question voted on particular key issues at the time (y = for, n = against, ? = neither for nor against), such as an anti-satellite weapons test ban and a reduction in funding to a synthetic fuels corporation. Let's now develop a classification tree in order to predict the political affiliation of a given congressman or woman based on their voting records:</p>
<div class="packt_infobox"><span>The following sub-sections describe each of the pertinent cells in the corresponding Jupyter Notebook for this use case, entitled <kbd>chp04-04-classification-regression-trees.ipynb</kbd>, and which may be found in the GitHub repository accompanying this book. Note that for the sake of brevity, we will skip those cells that perform the same functions as seen previously.</span></div>
<ol>
<li><span>Since our raw data file has no header row, we need to explicitly define its schema before we can load it into a Spark DataFrame, as follows:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">schema = StructType([<br/>   StructField("party", StringType()),<br/>   StructField("handicapped_infants", StringType()),<br/>   StructField("water_project_cost_sharing", StringType()),<br/>   ...<br/>])</pre></div>
<ol start="2">
<li><span>Since all of our columns, both the label and all the independent variables, are string-based data types, we need to apply a </span><em>StringIndexer</em><span> to them (as we did when developing our logistic regression model) in order to identify and index all possible categories for each column before generating numerical feature vectors. However, since we have multiple columns that we need to index, it is more efficient to build a</span> <em><span>p</span>ipeline</em><span>. A pipeline is a list of data and/or machine learning transformation stages to be applied to a Spark DataFrame. In our case, each stage in our pipeline will be the indexing of a different column as follows:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">categorical_columns = ['handicapped_infants', <br/>   'water_project_cost_sharing', ...]<br/>pipeline_stages = []<br/>for categorial_column in categorical_columns:<br/>   string_indexer = StringIndexer(inputCol = categorial_column, <br/>      outputCol = categorial_column + 'Index')<br/>   encoder = OneHotEncoderEstimator(<br/>      inputCols = [string_indexer.getOutputCol()], <br/>      outputCols = [categorial_column + "classVec"])<br/>   pipeline_stages += [string_indexer, encoder]<br/><br/>label_string_idx = StringIndexer(inputCol = 'party', <br/>   outputCol = 'label')<br/>pipeline_stages += [label_string_idx]<br/>vector_assembler_inputs = [c + "classVec" for c <br/>   in categorical_columns]<br/>vector_assembler = VectorAssembler(<br/>   inputCols = vector_assembler_inputs, <br/>   outputCol = "features")<br/>pipeline_stages += [vector_assembler]</pre></div>
<ol start="3">
<li>Next, <span>we instantiate our pipeline by passing to it the list of stages that we generated in the previous cell. We then execute our pipeline on the raw Spark DataFrame using the </span><kbd>fit()</kbd><span> method, before proceeding to generate our numerical feature vectors using </span><kbd>VectorAssembler</kbd><span> as before:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">pipeline = Pipeline(stages = pipeline_stages)<br/>pipeline_model = pipeline.fit(congressional_voting_df)<br/>label_column = 'label'<br/>congressional_voting_features_df = pipeline_model<br/>   .transform(congressional_voting_df)<br/>   .select(['features', label_column, 'party'])<br/>pd.DataFrame(congressional_voting_features_df.take(5), columns=congressional_voting_features_df.columns).transpose()</pre></div>
<ol start="4">
<li><span>We are now ready to train our classification tree! To achieve this, we can use MLlib's </span><kbd>DecisionTreeClassifier</kbd><span> estimator to train a decision tree on our training dataset as follows:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">decision_tree = DecisionTreeClassifier(featuresCol = 'features', <br/>   labelCol = label_column)<br/>decision_tree_model = decision_tree.fit(train_df)</pre></div>
<ol start="5">
<li><span>After training our classification tree, we will evaluate its performance on the test DataFrame. As with logistic regression, we can use the AUC metric as a measure of the proportion of time that the model predicts correctly. In our case, our model has an AUC metric of 0.91, which is very high:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">evaluator_roc_area = BinaryClassificationEvaluator(<br/>   rawPredictionCol = "rawPrediction", labelCol = label_column, <br/>   metricName = "areaUnderROC")<br/>print("Area Under ROC Curve on Test Data = %g" % evaluator_roc_area.evaluate(test_decision_tree_predictions_df))</pre></div>
<ol start="6">
<li><span>Ideally, we would like to visualize our classification tree. Unfortunately, there is not yet any direct method in which to render a Spark decision tree without using third-party tools such as </span><a href="https://github.com/julioasotodv/spark-tree-plotting">https://github.com/julioasotodv/spark-tree-plotting</a><span>. However, we can render a text-based decision tree by invoking the </span><kbd>toDebugString</kbd><span> method on our trained classification tree model, as follows:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">print(str(decision_tree_model.toDebugString))</pre></div>
<p>With an AUC value of 0.91, we can say that our classification tree model performs very well on the test data and is very good at predicting the political affiliation of congressmen and women based on their voting records. In fact, it classifies correctly 91% of the time across all threshold values!</p>
<p>Note that a CART model also generates probabilities, just like a logistic regression model. Therefore, we use a threshold value (default 0.5) in order to convert these probabilities into decisions, or classifications as in our example. There is, however, an added layer of complexity when it comes to training CART models—how do we control the number of splits in our decision tree? One method is to set a lower limit for the number of training data points to put into each subset or bucket. In <kbd>MLlib</kbd>, this value is tuneable, via the <kbd>minInstancesPerNode</kbd> parameter, which is accessible when training our <kbd>DecisionTreeClassifier</kbd>. The smaller this value, the more splits that will be generated.</p>
<p>However, if it is too small, then overfitting will occur. Conversely, if it is too large, then our CART model will be too simple with a low level of accuracy. We will discuss how to select an appropriate value during our introduction to random forests next. Note that <kbd>MLlib</kbd> also exposes other configurable parameters, including <kbd>maxDepth</kbd> (the maximum depth of the tree) and <kbd>maxBins</kbd>, but note that the larger a tree becomes in terms of splits and depth, the more computationally expensive it is to compute and traverse. To learn more about the tuneable parameters available to a <kbd>DecisionTreeClassifier</kbd>, please visit <a href="https://spark.apache.org/docs/latest/ml-classification-regression.html">https://spark.apache.org/docs/latest/ml-classification-regression.html</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Random forests</h1>
                </header>
            
            <article>
                
<p>One method of improving the accuracy of CART models is to build multiple decision trees, not just the one. In random forests, we do just that—a large number of CART trees are generated and thereafter, each tree in the forest votes on the outcome, with the majority outcome taken as the final prediction.</p>
<p>To generate a random forest, a process known as bootstrapping is employed whereby the training data for each tree making up the forest is selected randomly with replacement. Therefore, each individual tree will be trained using a different subset of independent variables and, hence, different training data.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">K-Fold cross validation</h1>
                </header>
            
            <article>
                
<p>Let's now return to the challenge of choosing an appropriate lower-bound bucket size for an individual decision tree. This challenge is particularly pertinent when training a random forest since the computational complexity increases with the number of trees in the forest. To choose an appropriate minimum bucket size, we can employ a process known as K-Fold cross validation, the steps of which are as follows:</p>
<ul>
<li>Split a given training dataset into K subsets or "folds" of equal size.</li>
<li>(K - 1) folds are then used to train the model, with the remaining fold, called the validation set, used to test the model and make predictions for each lower-bound bucket size value under consideration.</li>
<li>This process is then repeated for all possible training and test fold combinations, resulting in the generation of multiple trained models that have been tested on each fold for every lower-bound bucket size value under consideration.</li>
</ul>
<ul>
<li>For each lower-bound bucket size value under consideration, and for each fold, calculate the accuracy of the model on that combination pair.</li>
<li>Finally, for each fold, plot the calculated accuracy of the model against each lower-bound bucket size value, as illustrated in <em>Figure 4.8</em>:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-569 image-border" src="Images/0c3d28a9-caf1-4562-993a-3d1749eaa725.png" style="width:34.25em;height:30.67em;" width="983" height="880"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4.8: Typical K-Fold cross-validation output</div>
<p>As illustrated in <em>Figure 4.8</em>, choosing a small lower-bound bucket size value results in lower accuracy as a result of the model overfitting the training data. Conversely, choosing a large lower-bound bucket size value also results in lower accuracy as the model is too simple. Therefore, in our case, we would choose a lower-bound bucket size value of around 4 or 5, since the average accuracy of the model seems to be maximized in that region (as illustrated by the dashed circle in <em>Figure 4.8</em>).</p>
<p>Returning to our Jupyter Notebook, <kbd>chp04-04-classification-regression-trees.ipynb</kbd>, let's now train a random forest model using the same congressional voting dataset to see whether it results in a better performing model compared to our single classification tree that we developed previously:</p>
<ol>
<li><span>To build a random forest, we can use <kbd>MLlib</kbd>'s </span><kbd>RandomForestClassifier</kbd><span> estimator to train a random forest on our training dataset, specifying the minimum number of instances each child must have after a split via the <kbd>minInstancesPerNode</kbd> parameter, as follows:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">random_forest = RandomForestClassifier(featuresCol = 'features', <br/>   labelCol = label_column, minInstancesPerNode = 5)<br/>random_forest_model = random_forest.fit(train_df)</pre></div>
<ol start="2">
<li><span>We can now evaluate the performance of our trained random forest model on our test dataset by computing the AUC metric using the same </span><kbd>BinaryClassificationEvaluator</kbd><span> as follows:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">test_random_forest_predictions_df = random_forest_model<br/>   .transform(test_df)<br/>evaluator_rf_roc_area = BinaryClassificationEvaluator(<br/>   rawPredictionCol = "rawPrediction", labelCol = label_column,<br/>   metricName = "areaUnderROC")<br/>print("Area Under ROC Curve on Test Data = %g" % evaluator_rf_roc_area.evaluate(test_random_forest_predictions_df))</pre></div>
<p>Our trained random forest model has an AUC value of 0.97, meaning that it is more accurate in predicting political affiliation based on historical voting records than our single classification tree!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have developed, tested, and evaluated various supervised machine learning models in Apache Spark using a wide variety of real-world use cases, from predicting breast cancer to predicting political affiliation based on historical voting records.</p>
<p>In the next chapter, we will develop, test, and evaluate unsupervised machine learning models!</p>


            </article>

            
        </section>
    </div>



  </body></html>