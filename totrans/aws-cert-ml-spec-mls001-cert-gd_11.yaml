- en: '*Chapter 8*: Evaluating and Optimizing Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is now time to learn how to evaluate and optimize machine learning models.
    During the process of modeling, or even after model completion, you might want
    to understand how your model is performing. Each type of model has its own set
    of metrics that can be used to evaluate performance, and that is what we are going
    to study in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from model evaluation, as a data scientist, you might also need to improve
    your model's performance by tuning the hyperparameters of your algorithm. We will
    take a look at some nuances of this modeling task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing model evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alright, let's do it!
  prefs: []
  type: TYPE_NORMAL
- en: Introducing model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several different scenarios in which we might want to evaluate model
    performance, some of them are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: You are creating a model and testing different approaches and/or algorithms.
    Therefore, you need to compare these models to select the best one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have just completed your model and you need to document your work, which
    includes specifying the model's performance metrics that you have reached out
    to during the modeling phase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your model is running in a production environment and you need to track its
    performance. If you encounter model drift, then you might want to retrain the
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The term **model drift** is used to refer to the problem of model deterioration.
    When you are building a machine learning model, you must use data to train the
    algorithm. This set of data is known as training data, and it reflects the business
    rules at a particular point in time. If these business rules change over time,
    your model will probably fail to adapt to that change. This is because it was
    trained on top of another dataset, which was reflecting another business scenario.
    To solve this problem, you must retrain the model so that it can consider the
    rules of the new business scenario. Reinforcement learning systems might not suffer
    from this issue since they can adapt to the new data by themselves.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We perform model evaluations by designing a testing approach. We have learned
    about holdout validation and cross-validation before. However, both testing approaches
    share the same requirement: they need a metric in order to evaluate performance.'
  prefs: []
  type: TYPE_NORMAL
- en: This metric is specific to the problem domain, for example, there are specific
    metrics for regression models, classification models, clustering, natural language
    processing, and more. Therefore, during the design of your testing approach, you
    have to consider what type of model you are building in order to define the evaluation
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will take a look at the most important metrics
    and concepts that you should know to evaluate your models.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating classification models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification models are one of the most traditional classes of problems that
    you might face, either during the exam or during your journey as a data scientist.
    A very important artifact that you might want to generate during the classification
    model evaluation is known as a **confusion matrix**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A confusion matrix compares your model predictions against the real values
    of each class under evaluation. *Figure 8.1* shows what a confusion matrix looks
    like in a binary classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – A confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_08_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – A confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'We find the following components in a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TP**: This is the number of **True Positive** cases. Here, we have to count
    the number of cases that have been predicted as true and are, indeed, true. For
    example, in a fraud detection system, this would be the number of fraudulent transactions
    that were correctly predicted as fraud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TN**: This is the number of **True Negative** cases. Here, we have to count
    the number of cases that have been predicted as false and are, indeed, false.
    For example, in a fraud detection system, this would be the number of non-fraudulent
    transactions that were correctly predicted as not fraud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FN**: This is the number of **False Negative** cases. Here, we have to count
    the number of cases that have been predicted as false but are, instead, true.
    For example, in a fraud detection system, this would be the number of fraudulent
    transactions that were wrongly predicted as not fraud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FP**: This is the number of **False Positive** cases. Here, we have to count
    the number of cases that have been predicted as true but are, instead, false.
    For example, in a fraud detection system, this would be the number of non-fraudulent
    transactions that were wrongly predicted as fraud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a perfect scenario, your confusion matrix will have only true positive and
    true negative cases, which means that your model has an accuracy of 100%. In practical
    terms, if that type of scenario occurs, you should be skeptical instead of happy
    since it is expected that your model will contain errors. If your model does not
    contain errors, you are likely to be suffering from overfitting issues, so be
    careful.
  prefs: []
  type: TYPE_NORMAL
- en: Once false negatives and false positives are expected, the most that you can
    do is to prioritize one of them. For example, you can reduce the number of false
    negatives by increasing the number of false positives and vice versa. This is
    known as the precision versus recall trade-off. Let's take a look at these metrics
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting metrics from a confusion matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The simplest metric that we can extract from a confusion matrix is known as
    **accuracy**. Accuracy is given by the number of true positives plus true negatives
    over the total number of cases. In *Figure 8.2*, we have filled out all the components
    of the confusion matrix for the sake of this demonstration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – A confusion matrix filled with some examples'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_08_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – A confusion matrix filled with some examples
  prefs: []
  type: TYPE_NORMAL
- en: According to *Figure 8.2*, the accuracy would be (100 + 90) / 210, which is
    equal to 0.90\. There is a common issue that occurs when utilizing an accuracy
    metric, which is related to the balance of each class. Problems with highly imbalanced
    classes, such as 99% of positive cases and 1% of negative cases, will impact the
    accuracy score and make it useless.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if your training data has 99% of positive cases (the majority class),
    your model is likely to correctly classify most of the positive cases but go badly
    in the classification of negative cases (the minority class). The accuracy will
    be very high (due to the correctness of the classification of the positive cases),
    regardless of the bad results in the minority class classification.
  prefs: []
  type: TYPE_NORMAL
- en: The point is that on highly imbalanced problems, we usually have more interest
    in correctly classifying the minority class, not the majority class. That's the
    case on most fraud detection systems, for example, where the minority class corresponds
    to fraudulent cases. For imbalanced problems, you should look for other types
    of metrics, which we will cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Another important metric that we can extract from a confusion matrix is known
    as **recall**, which is the number of true positives over the number of true positives
    plus false negatives. In other words, recall is given by the number of true positive
    and overall positive cases. Recall is also known as **sensitivity**.
  prefs: []
  type: TYPE_NORMAL
- en: According to *Figure 8.2*, recall is given by 100 / 112, which is equal to 0.89\.
    **Precision**, on the other hand, is given by the number of true positives over
    the number of true positives plus false positives. In other words, precision is
    given by the number of true positive and overall predicted positive cases. Precision
    is also known as **positive predictive power**.
  prefs: []
  type: TYPE_NORMAL
- en: According to *Figure 8.2*, precision is given by 100 / 108, which is equal to
    0.93\. In general, we can increase precision by the cost of decrease recall and
    vice versa. There is another model evaluation artifact in which we can play around
    with this precision versus recall trade-off. It is known as a **precision-recall
    curve**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision-recall curves summarize the precision versus recall trade-off by
    using different probability thresholds. For example, the default threshold is
    0.5, where any prediction above 0.5 will be considered as true; otherwise, it
    is false. You can change the default threshold according to your need so that
    you can prioritize recall or precision. *Figure 8.3* shows an example of a precision-recall
    curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – A precision-recall curve'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_08_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – A precision-recall curve
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 8.3*, increasing the precision will reduce the amount
    of recall and vice versa. *Figure 8.3* shows the precision/recall for each threshold
    for a gradient boosting model (as shown by the orange line) compared to a no-skill
    model (as shown by the blue dashed line). A perfect model will approximate the
    curve to the point (1,1), forming a squared corner in the top right-hand side
    of the chart.
  prefs: []
  type: TYPE_NORMAL
- en: Another visual analysis we can do on top of confusion matrixes is known as a
    **Receiver Operating Characteristic** (**ROC**) curve. ROC curves summarize the
    trade-off between the **true positive rate** and the **false positive rate** according
    to different thresholds, as in the precision-recall curve.
  prefs: []
  type: TYPE_NORMAL
- en: You already know about the true positive rate, or sensitivity, which is the
    same as what we have just learned in the precision-recall curve. The other dimension
    of an ROC curve is the **false positive rate**, which is the number of false positives
    over the number of false positives plus true negatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'In literature, you might find the false positive rate referred to as inverted
    **specificity**, represented by *1 – specificity*. Specificity is given as the
    number of true negatives over the number of true negatives plus false positives.
    Furthermore, false-positive rates or inverted specificity are the same. *Figure
    8.4* shows what an ROC curve looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – An ROC curve'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_08_04.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – An ROC curve
  prefs: []
  type: TYPE_NORMAL
- en: A perfect model will approximate the curve to the point (0,1), forming a squared
    corner in the top left-hand side of the chart. The orange line represents the
    trade-off between the true positive rate and the false positive rate of a gradient
    boosting classifier. The dashed blue line represents a no-skill model, which cannot
    predict the classes properly.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, you can use ROC curves for fairly balanced datasets and precision-recall
    curves for moderate to imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing precision and recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes, we might want to use a metric that summarizes precision and recall,
    instead of prioritizing one over the other. Two very popular metrics can be used
    to summarize precision and recall: **F1 score** and **Area Under Curve** (**AUC**).'
  prefs: []
  type: TYPE_NORMAL
- en: The F1 score, also known as **F-measure**, computes the harmonic mean of precision
    and recall. AUC summarizes the approximation of the area under the precision-recall
    curve.
  prefs: []
  type: TYPE_NORMAL
- en: That brings us to the end of this section on classification metrics. Let's now
    take a look at the evaluation metrics for regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression models are quite different from classification models since the outcome
    of the model is a continuous number. Therefore, the metrics around regression
    models aim to monitor the difference between real and predicted values.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to check the difference between a predicted value (*yhat*)
    and its actual value (*y*) is by performing a simple subtraction operation, where
    the error will be equal to the absolute value of *yhat – y*. This metric is known
    as the **Mean Absolute Error** (**MAE**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we usually have to evaluate the error of each prediction, *i*, we have
    to take the mean value of the errors. The following formula shows how this error
    can be formally defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Sometimes, you might want to penalize bigger errors over smaller errors. To
    achieve this, you can use another metric, which is known as the **Mean Squared
    Error** (**MSE**). MSE will square each error and return the mean value.
  prefs: []
  type: TYPE_NORMAL
- en: 'By squaring errors, MSE will penalize the bigger ones. The following formula
    shows how MSE can be formally defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16735_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There is a potential interpretation problem with MSE. Since it has to compute
    the squared error, it might be difficult to interpret the final results from a
    business perspective. The **Root Mean Squared Error** (**RMSE**) works around
    this interpretation issue, by taking the square root of MSE. Here is the RMSE
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: RMSE is probably the most used metric for regression models since it can either
    penalize bigger errors, yet still be easily interpreted.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring other regression metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many more metrics that are suitable for regression problems aside
    from the ones that we have just learned. We will not be able to cover most of
    them here, but there are a few more metrics that might be important for you to
    know.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of these metrics is known as the **Mean Absolute Percentage Error** (**MAPE**).
    As the name suggests, MAPE will compute the absolute percentage error of each
    prediction and then take the average value. The following formula shows how this
    metric is computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16735_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: MAPE is broadly used on forecasting models since it is very simple to interpret,
    and it provides a very good sense of how far (or close) the predictions are from
    the actual values (in terms of a percentage).
  prefs: []
  type: TYPE_NORMAL
- en: We have now completed this section on regression metrics. Next, we will talk
    about model optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Model optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you know, understanding evaluation metrics is very important in order to
    measure your model's performance and document your work. In the same way, when
    we want to optimize our current models, evaluating metrics also plays a very important
    role in defining the baseline performance that we want to challenge.
  prefs: []
  type: TYPE_NORMAL
- en: The process of model optimization consists of finding the best configuration
    (also known as hyperparameters) of the machine learning algorithm for a particular
    data distribution. We don't want to find hyperparameters that overfit the training
    data in the same way that we don't want to find hyperparameters that underfit
    the training data.
  prefs: []
  type: TYPE_NORMAL
- en: You learned about overfitting and underfitting in [*Chapter 1*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014),
    *Machine Learning Fundamentals*. In the same chapter, you also learned how to
    avoid these two types of modeling issues.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn about some techniques that you can use to find
    the best configuration for a particular algorithm and dataset. You can combine
    these techniques of model optimization with other methods, such as cross-validation,
    to find the best set of hyperparameters for your model and avoid fitting issues.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Always remember that you don't want to optimize your algorithm to the underlying
    training data but to the data distribution behind the training data. This is so
    that your model will work in the training data as well as in the production data
    (that is, the data that has never been exposed to your model during the training
    process). A machine learning model that works only in the training data is useless.
    That's why combining model-tuning techniques (such as the ones we will learn about
    next) with sampling techniques (such as cross-validation) makes all the difference
    when it comes to creating a good model.
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Grid search** is probably the most popular method for model optimization.
    It consists of testing different combinations of the algorithm and selecting the
    best one. Here, we have two important points that we need to pay attention to:'
  prefs: []
  type: TYPE_NORMAL
- en: How to define the best one?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many combinations should we test?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best model is defined based on an evaluation metric. In other words, you
    have to first define which metric you are going to use to evaluate the model's
    performance. Secondly, you have to define how you are going to evaluate the model.
    Usually, we use cross-validation to evaluate the model on multiple datasets that
    have never been used for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of the number of combinations, this is the most challenging part when
    playing with grid search. Each hyperparameter of an algorithm may have multiple
    or, sometimes, infinite possibilities of values. If you consider that an algorithm
    will usually have multiple hyperparameters, this becomes a function with quadratic
    cost, where the number of unique combinations to test (also known as a model for
    testing) is given as *the number of values of hyperparameter a * the number of
    values of hyperparameter b * the number of values of hyperparameter i*. *Figure
    8.5* shows how you could potentially set a grid search configuration for a decision
    tree model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Grid search configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_08_Table_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – Grid search configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 8.5*, there are three hyperparameters: **Criterion**, **Max depth**,
    and **Min samples leaf**. Each of these hyperparameters has a list of values for
    testing: 2, 3, and 3 values, respectively. That means, by the end of the grid
    search process, we will have tested 18 models (2 * 3 * 3), where only the best
    one will be selected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you might have noticed, all the different combinations of those three hyperparameters
    will be tested, for example, consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Criterion = Gini, Max depth = 2, Min samples leaf = 10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Criterion = Gini, Max depth = 5, Min samples leaf = 10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Criterion = Gini, Max depth = 10, Min samples leaf = 10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other questions that you might be wondering could include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Considering that a particular algorithm might have several hyperparameters,
    which ones should I tune?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering that a particular hyperparameter might accept infinite values, which
    values should I test?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are good questions and grid search will not give you a straight answer
    for them. Instead, this is closer to an empirical process, where you have to test
    as much as you need to achieve your target performance.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Of course, grid search cannot guarantee that you will come up with your target
    performance. That depends on the algorithm and the training data.
  prefs: []
  type: TYPE_NORMAL
- en: A common practice, though, is to define the values for testing by using a **linear
    space** or **log space**, where you can manually set the limits of the hyperparameter
    you want to test and the number of values for testing. Then, the intermediate
    values will be drawn by a linear or log function.
  prefs: []
  type: TYPE_NORMAL
- en: As you might imagine, grid search can take a long time to run. A number of alternative
    methods have been proposed to work around this time issue. **Random search** is
    one of them, where the list of values for testing is randomly selected from the
    search space.
  prefs: []
  type: TYPE_NORMAL
- en: Another method that has gained rapid adoption across the industry is known as
    **Bayesian optimization**. Algorithm optimizations, such as **gradient descent**,
    try to find what is called the **global minima**, by calculating derivatives of
    the cost function. Global minima are the points where you find the algorithm configuration
    with the least associated cost.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian optimization is useful when calculating derivatives is not an option.
    So, we can use the **Bayes theorem**, a probabilistic approach, to find the global
    minima using the smallest number of steps.
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, Bayesian optimization will start testing the entire search
    space to find the most promising set of optimal hyperparameters. Then, it will
    perform more tests specifically in the place where the global minima are likely
    to be.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the main metrics for model evaluation. We
    first started with the metrics for classification problems and then we moved on
    to the metrics for regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of classification metrics, you have been introduced to the well-known
    confusion matrix, which is probably the most important artifact to perform a model
    evaluation on classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from knowing what true positive, true negative, false positive, and false
    negative are, we have learned how to combine these components to extract other
    metrics, such as accuracy, precision, recall, the F1 score, and AUC.
  prefs: []
  type: TYPE_NORMAL
- en: We went even deeper and learned about ROC curves, as well as precision-recall
    curves. We learned that we can use ROC curves to evaluate fairly balanced datasets
    and precision-recall curves for moderate to imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: By the way, when you are dealing with imbalanced datasets, remember that using
    accuracy might not be a good idea.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of regression metrics, we learned that the most popular ones, and the
    most likely to be present in the *AWS Machine Learning Specialty* exam, are MAE,
    MSE, RMSE, and MAPE. Make sure you know the basics of each of them before taking
    the exam.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned about methods for hyperparameter optimization, where grid
    search and Bayesian optimization are the primary ones. In the next chapter, we
    will take a look at SageMaker and learn how it can be used for modeling. But first,
    let's take a moment to practice these questions on model evaluation and model
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are working as a data scientist for a pharmaceutical company. You are collaborating
    with other teammates to create a machine learning model to classify certain types
    of diseases on image exams. The company wants to prioritize the assertiveness
    rate of positive cases, even if they have to wrongly return false negatives. Which
    type of metric would you use to optimize the underlying model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Recall
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Precision
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. R-squared
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. RMSE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this scenario, the company prefers to have a higher probability to be right
    on positive outcomes at the cost of wrongly classifying some positive cases as
    negative. Technically, they prefer to increase precision at the cost of reducing
    recall.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are working as a data scientist for a pharmaceutical company. You are collaborating
    with other teammates to create a machine learning model to classify certain types
    of diseases on image exams. The company wants to prioritize the capture of positive
    cases, even if they have to wrongly return false positives. Which type of metric
    would you use to optimize the underlying model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Recall
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Precision
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. R-squared
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. RMSE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this scenario, the company prefers to find most of the positive cases at
    the cost of wrongly classifying some negative cases as positive. Technically,
    they prefer to increase recall at the cost of reducing precision.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are working in a fraud identification system, where one of the components
    is a classification model. You want to check the model's performance. Which of
    the following metrics could be used and why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Accuracy. Since fraudulent system datasets are naturally unbalanced, this
    metric is good to take into consideration the assertiveness of both positive and
    negative classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Precision. Since fraudulent system datasets are naturally unbalanced, this
    metric is good to take into consideration the assertiveness of both positive and
    negative classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. Recall. Since fraudulent system datasets are naturally unbalanced, this metric
    is good to take into consideration the assertiveness of both positive and negative
    classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. The F1 score. Since fraudulent system datasets are naturally unbalanced,
    this metric is good to take into consideration the assertiveness of both positive
    and negative classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Option "d" is the only one that matches the explanation of the proposed metric
    and provides a valid measure to the problem. Accuracy cannot be used in this problem
    due to the unbalanced issue. Precision and recall could be potentially used together
    to provide a quality view of the problem, but there is no such option in the list
    of answers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are building a machine learning model to predict house prices. You have
    approached the problem as a regression model. Which of the following metrics are
    not applicable for regression models? (Select all correct answers.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Recall
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Precision
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. MAPE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. RMSE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Recall and precision are applicable for classification problems; that's why
    they are the correct answers. On the other hand, MAPE and RMSE are applicable
    for regression models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following metrics help us to penalize bigger errors on regression
    models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Recall
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Precision
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. MAPE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. RMSE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RMSE computes the squared error of each prediction. Then, it takes the squared
    root of the MSE. By computing the squared error, RMSE will penalize bigger errors
    over smaller errors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are working as a data scientist for a financial services company and you
    have created a regression model to predict credit utilization. If you decide to
    include more features in the model, what will happen to R-squared and Adjusted
    R-squared?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Adjusted R-squared will increase, whereas R-squared can either increase or
    decrease.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. R-squared will decrease, whereas Adjusted R-squared can either increase or
    decrease.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. R-squared will increase, whereas Adjusted R-squared can either increase or
    decrease.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. Adjusted R-squared will decrease, whereas R-squared can either increase or
    decrease.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: R-squared will increase since the extra information will help the model to capture
    more variance in the data. However, Adjusted R-squared can either increase or
    decrease, depending on the gain of adding the extra variable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following metrics will compute the percentage of errors instead
    of absolute errors?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Recall
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Precision
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. MAPE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. RMSE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: MAPE is applicable for regression models and it will compute the error as a
    percentage number.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are the lead data scientist of the company. Your team wants to optimize
    a model that is no longer performing well in production. The team has decided
    to use grid search to retrain the hyperparameters; however, the process is taking
    a long time and does not complete. Which approach could you take to speed up the
    process of tuning and still maximize your chances of finding a better model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Reduce the search space to speed up the training process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Use Bayesian optimization instead of grid search.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Increase the search space to speed up the training process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) None of the above.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Reducing the search space of grid search will help to speed up the process of
    tuning, but you will test fewer models. This will reduce your chances of finding
    the best model for the problem. Increasing the search space will increase the
    time for tuning. Option "b" is the most resealable one since Bayesian optimization
    can focus on the most important search space, potentially reducing the time for
    processing and increasing your chances of finding the best model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are using grid search to tune a machine learning model. During the tuning
    process, you obtain good performance metrics. However, when you execute the model
    in production, the model performance is not acceptable. You have to troubleshoot
    the problem. Which of the following options are valid reasons for this issue?
    (Select all correct answers.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) You are tuning and evaluating the model in the training data, which is causing
    overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The production data does not have the same distribution as the training data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) You are not using cross-validation in the training process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) You are not tuning the right hyperparameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can't tune and test the model in the same dataset at the risk of overfitting
    it. That's why option "a" is correct. If the production data does not follow the
    same distribution of the training data, the model will not work, so option "b"
    is also correct. Option "c" is not valid because cross-validation is not mandatory
    for model evaluation. Option "d" would be correct if you find bad results in the
    training data, not in the production data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are working for a global financial company. Your team has created a binary
    classification model to identify fraudulent transactions. The model has been put
    into production and is automatically flagging fraudulent transactions and sending
    them for further screening. The operation team is complaining that this model
    is blocking too many transactions and they would prefer to flag a smaller number
    of transactions. According to the preceding scenario, what is the expectation
    of the operation team?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) They want to calibrate the model threshold at 0.5.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) They want to prioritize precision over recall.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) They want to prioritize recall over precision.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) They want to use F-measure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We always have to match model usage with business goals and capacity. In this
    scenario, the model is flagging a lot of potentially fraudulent transactions,
    but there isn't a big enough human workforce to evaluate all of those blocked
    transactions. Furthermore, what makes more sense is "calibrating" the model to
    the real business scenario, where it will flag fewer (but more likely) fraudulent
    cases for further screening.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
