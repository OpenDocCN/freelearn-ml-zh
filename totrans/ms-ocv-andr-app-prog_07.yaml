- en: Chapter 7. Bringing Your Apps to Life with OpenCV Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With so much data around us, we need better systems and applications to process
    it and extract relevant information out of it. A field of computer science that
    deals with this is **machine learning**. In this chapter, we will take a look
    at the different machine learning techniques that can be used to exploit all the
    data around us and build smart applications that can deal with unencountered situations
    or scenarios, without any form of human intervention.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In the recent years, computer vision and machine learning have formed a strong
    synergy between them, thus enabling technologies that have helped build some extremely
    efficient and useful applications. Humanoids, robotic arms, and assembly lines
    are some of the examples where computer vision and machine learning find applications.
    Developers and researchers are now trying to exploit mobile platforms and build
    light-weight applications that can be used by common people. In the following
    section, we will build an application for **Optical Character Recognition** (**OCR**)
    using standard OpenCV and Android APIs. Toward the end, we will revisit the Sudoku
    solving application that we started developing in [Chapter 2](ch02.html "Chapter 2. Detecting
    Basic Features in Images"), *Detecting Basic Features in Images*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: We will understand the machine learning techniques alongside building applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Optical Character Recognition
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Optical Character Recognition** (**OCR**) is one of the favorite topics of
    research in computer vision and machine learning. There are a lot of efficient
    off-the-shelf implementations and algorithms readily available for OCR, but for
    better understanding of the concepts, we will build our own OCR Android application.
    Before we get down to writing the code for our application, let''s take some time
    to take a look at the different character recognition techniques and how they
    work. In this chapter, we will use two standard machine learning techniques: **k-nearest
    neighbors** (**KNN**) and **Support Vector Machines** (**SVM**), while building
    our applications.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this chapter is to build a real-time digit recognition application.
    The application will have a live camera output being displayed on the mobile screen
    and as soon as the camera captures a digit, we will recognize the digit.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: OCR using k-nearest neighbors
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: k-nearest neighbors is the one of the simplest algorithms used for supervised
    classification. In KNN, we give the training dataset and their corresponding labels
    as input. An n-dimensional space is created (where *n* is the length of each training
    data) and every training data is plotted as a point in it. While classification,
    we plot the data to be classified in the same n-dimensional space, and calculate
    the distance of that point from every other point in the space. The distance computed
    is used to find an appropriate class for the testing data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a step–by-step explanation of the working of the algorithm:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Choose a user-defined value of *k*.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the training data along with their classes in the form of a row vector.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the input query (data to be classified) and calculate the distance from
    it to every other row vector in the training data (meaning of distance is explained
    in the following box).
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort all the row vectors in the ascending order of their distance (calculated
    in the previous step) from the query data.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, from the first *k* sorted row vectors, choose the class (training labels),
    which has the majority of row vectors as the predicted class.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Distance between vectors**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'In Euclidean Space, we define the distance between two vectors as:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![OCR using k-nearest neighbors](img/B02052_07_03.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: where, *xi* and *yi* are the *i^(th)* dimension values of the two vectors *x*
    and *y* respectively. *n* is the length of the training vectors (*x* and *y* in
    our case). The algorithm does not put any restriction on the type of distance
    we can use. Some other types of distances that we can use are **Manhattan distance**,
    Maximum distance and the likes. Refer to [http://en.wikipedia.org/wiki/Distance](http://en.wikipedia.org/wiki/Distance)
    for some other definitions of distance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Simple enough! How do we use it with image data? For us to be able to use KNN
    on the image data, we need to convert the training images to some sort of a row
    vector.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Consider a 10x10 grayscale image of any digit from one to nine. The easiest
    and the fastest way to get a feature vector from the 10x10 image is to convert
    it into a 1x100 row vector. This can be done by appending the rows in the image
    one after another. This way, we can convert all the images in our training set
    to row vectors for later use in the KNN classifier.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it easier for us to build the digit recognition application, we will
    break it down into smaller parts listed as follows and finally use them together:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Making a camera application
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling the training data
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing the captured digit
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making a camera application
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will begin by building a simple camera application that displays the camera
    output on the screen, as we did in [Chapter 4](ch04.html "Chapter 4. Drilling
    Deeper into Object Detection – Using Cascade Classifiers"), *Drilling Deeper into
    Object Detection – Using Cascade Classifiers*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Create a new Android project in Eclipse (or Android Studio)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize OpenCV in the project (refer to [Chapter 1](ch01.html "Chapter 1. Applying
    Effects to Images"), *Applying Effects to Images*).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add `JavaCameraView` to the main activity using the following code snippet:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the camera is in place, draw a square on the screen that will help the
    user to localize the digit that he/she wants to recognize. The user will point
    to the digit and try to bring it within the square drawn on the screen (as shown
    in Figure 1). Copy the following piece of code in the Mat `onCameraFrame()` function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this code, we take the size of the frame captured by the mobile camera and
    draw a 400x400 (can vary according to the screen size) white rectangle in the
    center of the image (as shown in Figure 1). That's it. The camera application
    is ready. Next, is handling the training data in the application.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![Making a camera application](img/B02052_07_01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Screenshot of the camera application
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Handling the training data
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the trickiest part of the application. Training data plays a crucial
    role in any machine learning application. The amount of data that such applications
    deal with is usually in the order of few megabytes. This may not be a concern
    for a normal desktop application, but for a mobile application (because of resource
    constraints), even handling around 50 megabytes can lead to performance issues,
    if not done properly. The code needs to be concise, to the point, and should have
    minimum memory leaks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: For this application, we will make use of a publically available handwritten
    digits dataset —MNIST, to train the KNN classifier.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The MNIST database ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/))
    of handwritten digits available from this page, has a training set of 60,000 examples
    and a test set of 10,000 examples. It is a subset of a larger set available from
    MNIST. The digits have been size-normalized and centered in a fixed-size image.
    (Text taken from Prof. Yann LeCun's web page, which is available at [http://yann.lecun.com](http://yann.lecun.com).)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'First, download the MNIST training data using the following links:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Training images at [http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz](http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training labels at [http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz](http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the downloaded files and transfer them to an Android phone (make sure
    you have around 60 MB of free space available).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Getting back to our application, create a new `DigitRecognizer` class that will
    handle all the tasks related to digit recognition, including loading the dataset
    into the application, training the classifier, and finally, recognizing the digit.
    Add a new Java class to the project and name it `DigitRecognizer`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: So, we already have the training images and training labels stored in the phone.
    We need to load the data into the application. For this, all we have to do is
    read the data from these files and make them compatible with OpenCV's API.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a new function void `ReadMNISTData()` to the `DigitRecognizer` class created
    earlier. This function will read the MNIST dataset and store it in the form of
    a Mat (OpenCV''s class to store images). Read the dataset in two parts: first,
    the training images and then the training labels.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'In `ReadMNISTData()`, create a new `File` object that will store the path to
    the phone''s SD card (as shown in the following code). In case the file is in
    the phone''s internal memory, skip this step and provide an absolute path of the
    file that we wish to use later in the code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After doing this, create another `File` object that will point to the exact
    file that we want to read in our application, and an `InputStreamReader` object
    that will help us in reading the file:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, `images_path` is the absolute path of the `train-images-idx3-ubyte.idx3`
    training images file.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we continue with the code, we need to understand how images are stored
    in the file. Here is the description of the contents of the training images file:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Pixels are organized row-wise. Pixel values are 0 to 255, where 0 represents
    the background (white) and 255 represents the foreground (black).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'With this information, we can continue writing the code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code, first, we read the first 16 bytes of the file, which
    stores the number of images, height, and width of the images (refer to the aforementioned
    table describing the contents of the file). Using the `ByteBuffer` class, we get
    four integers from the 16 bytes by combining four bytes, one each for an integer.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: In OpenCV, KNN's implementation requires us to pass all the feature vectors
    using the Mat class. Every training image needs to be converted to a row vector
    that will form one row of the Mat object, which will be passed to the KNN classifier.
    For example, if we have 5,000 training images each with dimensions 20x20, we will
    need a Mat object with dimensions 5000x400 that can be passed to OpenCV's KNN
    training function. Confused? Continue reading!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Take a 20x20 image from the training dataset and convert it to a 1x400 vector
    by appending rows one after another. Do this for all the images. At the end, we
    will have 5,000 such 1x400 vectors. Now, create a new Mat object with dimensions
    5000x400, and each row of this new Mat object will be the 1x400 vector that we
    got just now by resizing the original images in the dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what the preceding piece of code intends to do. First, read all the
    pixels in an image using the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, `px_count` is the total number of pixels in a training image and `image`
    is a row vector that stores the image. As explained earlier, we need to copy these
    row vectors to a Mat object (`training_images` refers to the Mat object that will
    be used to store these training images). Copy the `image` row vector to `training_images`,
    as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Training data is in place. We now need their corresponding labels. As we did
    for training images, their corresponding labels (label values are from 0 to 9)
    can be read in the same way. The contents of the `labels` file are arranged in
    the following way:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here is the code to read the labels:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The basis of the preceding code is similar to the code used for reading images.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'We have successfully loaded the training data in our application. At this point,
    you can use some diagnostic tools of Android to check the memory usage of the
    application. An important point that you need to take care of is to not duplicate
    the data. Doing this will increase the amount of memory consumed, which can affect
    the performance of your application as well as other applications running on your
    phone. Pass the `training_images` and `training_labels` Mat objects to OpenCV''s
    KNN classifier object:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The KNN classifier is ready. We are now ready to classify the data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing digits
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the final part of the application. Here, we use the frames that are
    captured from the camera as an input to the classifier and allow the classifier
    to predict the digit in the frame.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, add a new function void `FindMatch()` to the `DigitRecognizer`
    class created in the previous section as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note: Images in the training dataset are 28x28 binary images.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: The camera output is not directly usable. We need to preprocess the images to
    bring them as close as possible to the images in the training dataset for our
    classifier to give accurate results.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps (preferably in the same order) to make the camera
    output usable by the KNN classifier:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Dilate the image to make the digit more prominent in the image and reduce any
    background noise.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Resize the image to 28x28\. The training images are also of this dimension.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the image to a grayscale image.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform adaptive threshold on the image to get a binary image.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the parameters used in the code here are subject to lighting conditions.
    You are requested to tweak these parameters to suit their environment for best
    results.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'After following these steps, we will have a test that needs to go into the
    KNN classifier that we trained in the previous section. Before this can happen,
    there is one more thing that needs to be done to the test image—transforming the
    image to a row vector (remember the transformations we did to training images?).
    Convert the 28x28 test image to a 1x784 row vector. Use the following piece of
    code to transform the image:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, pass the transformed `test` image to the KNN classifier and store
    the result in the 1x1 Mat object `results`. The last two parameters in the `find_nearest`
    function are optional:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'One last thing, how and when do we call the `FindMatch` function? Since we
    are building a real-time digit recognition application, we need to perform the
    matching operation on every output frame of the camera. Because of this, we need
    to call this function in `onCameraFrame()` in the main activity class. The function
    should finally look like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We take the RGBA image of the camera output and extract the part of the image
    enclosed by the rectangle that we drew on the screen before. We want the user
    to bring the digit within the rectangle for it to be successfully recognized.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our application is written for landscape mode (set in the `AndroidManifest.xml`
    file) but we use it in the portrait mode, we need to transpose the test image
    before we can run the recognition algorithm. Hence, run this command:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We have successfully created a real-time digit recognition application. Let's
    take a look at another machine learning technique that can be used in recognizing
    digits.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: OCR using Support Vector Machines
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Support Vector Machines** (**SVMs**) are supervised learning algorithms that
    are commonly used for classification and regression. In SVMs, the training data
    is divided into different regions using infinite hyperplanes, and each region
    represents a class. To test data, we plot the point in the same space as the training
    points and using the hyperplanes compute the region where the test point lies.
    SVMs are useful when dealing with high-dimensional data.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For details on SVMs, you can refer to [http://www.support-vector-machines.org/](http://www.support-vector-machines.org/).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn how to use SVMs for digit recognition. As in
    KNN, to train an SVM, we will directly use the training images without any image
    manipulations or detecting any extra features.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of directly using the training images, it is possible to extract some
    features from the images and use those features as training data for the SVM.
    One of the OpenCV tutorials implemented in Python follows a different path. Here,
    they first deskew the image using affine transformations, then compute Histogram
    of Orientation Gradients. These HoG features are used to train the SVM. The reason
    why we are not following the same path is because of the cost of computation involved
    in computing affine transformations and HoG.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Only slight modifications are involved in using SVM instead of KNN in the application
    that we built in the previous section. The basic camera application and handling
    training data remains as is. The only modification that has to happen is in the
    digit recognition part where we train the classifier.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'In `ReadMNISTData()` function, instead of creating a KNN classifier object,
    we will create an SVM object. Remove the following lines where we declared and
    initialized a KNN object:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, replace them with the following lines (declaring and initializing an SVM
    object):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The SVM classifier is now ready. The next step for the KNN classifier is to
    pass a test image to the classifier and check the result. For this, we need to
    modify the `FindMatch()` function. Replace the line that uses KNN for classification
    with an appropriate line which uses SVM.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An optimization that the users can incorporate in the preceding application
    is that they can save the trained classifier in a file on the device. This will
    save time in training the classifier again and again.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following command:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We need to replace the preceding command with the following command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: That's all. Our application is ready. We can run the application, check for
    the results, and probably compare which algorithm runs better under what condition.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Solving a Sudoku puzzle
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Remember the Sudoku puzzle project in [Chapter 2](ch02.html "Chapter 2. Detecting
    Basic Features in Images"), *Detecting Basic Features in Images*? Now is the perfect
    time to revisit this project and see whether we can use anything that we learnt
    in this chapter to complete this application. So, in [Chapter 2](ch02.html "Chapter 2. Detecting
    Basic Features in Images"), *Detecting Basic Features in Images*, we had successfully
    detected the Sudoku puzzle. Only two things were left in that application: recognizing
    digits and solving the Sudoku puzzle.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing digits in the puzzle
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's pick up from where we left in [Chapter 2](ch02.html "Chapter 2. Detecting
    Basic Features in Images"), *Detecting Basic Features in Images*. After detecting
    the grid successfully, we need to further break down the grid into 81 small squares.
    There are many possible ways of doing this, but here, we will look at only three
    techniques.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: First, the easiest of all is to draw nine equally spaced vertical and horizontal
    lines each on the image, and assume the digits to be placed within the boxes made
    by these lines.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![Recognizing digits in the puzzle](img/B02052_07_02.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Vertical and horizontal lines drawn on a Sudoku grid
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The second way is using Hough lines. Apply Hough lines on the Sudoku grid and
    store all the lines that are returned. Ideally, nine vertical and nine horizontal
    lines should be returned but chances of this happening are very bleak, unless
    you have a very good camera and perfect lighting conditions. There will be missing
    or incomplete lines that will reduce the application's performance or may lead
    to false results.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The third way is using corner detection. Run any corner detection algorithm
    and get all the corners in the image. These corners represent the vertices of
    the boxes enclosing the digits. Once you have all the corners, you can join four
    corners to form a box and extract that part of the image.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: The previously mentioned techniques may not always guarantee perfect results.
    Different techniques may perform better, depending on the surroundings and the
    kind of camera being used.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract all 89 images using any of the previously mentioned technique, and
    pass them through a pretrained digit classifier—SVM or KNN (as seen in the previous
    sections). Done! Take the output of the classifier and make a 9x9 integer matrix
    in your code, and fill it up with the corresponding digits recognized from the
    grid. So now we have the grid with us. Use any brute force or Artificial Intelligence
    algorithm to get the correct solution of the Sudoku puzzle. Different algorithms
    that can be used are as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Backtracking
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Genetic algorithms
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sudoku as a constraint problem
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to [http://en.wikipedia.org/wiki/Sudoku_solving_algorithms](http://en.wikipedia.org/wiki/Sudoku_solving_algorithms)
    for a detailed explanation on these algorithms.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 有关这些算法的详细解释，请参阅[http://en.wikipedia.org/wiki/Sudoku_solving_algorithms](http://en.wikipedia.org/wiki/Sudoku_solving_algorithms)。
- en: Summary
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at how to make an application intelligent by incorporating
    machine learning into them. We looked at Support Vector Machines and KNNs, and
    how we can use them to build applications that can learn patterns in user entered
    data. Till now we have covered many computer vision algorithms and their implementations
    in detail. In the next chapter, we will take a look at some commonly faced errors
    while building such applications, and some best practices that will help you make
    the applications more efficient.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何通过将机器学习融入其中来使应用程序变得智能。我们讨论了支持向量机（Support Vector Machines）和KNN（K-Nearest
    Neighbors），以及我们如何使用它们来构建能够从用户输入数据中学习模式的应用程序。到目前为止，我们已经详细介绍了许多计算机视觉算法及其实现。在下一章中，我们将探讨在构建此类应用程序时常见的一些错误，以及一些有助于使应用程序更高效的最佳实践。
