<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Topic Modeling and Sentiment Analysis in NLP</h1>
                </header>
            
            <article>
                
<p>In this chapter, we're going to introduce some common topic modeling methods, discussing some applications. Topic modeling is a very important NLP section and its purpose is to extract semantic pieces of information out of a corpus of documents. We're going to discuss <strong>latent semantic analysis</strong>, one of most famous methods; it's based on the same philosophy already discussed for model-based recommendation systems. We'll also discuss its probabilistic variant, PLSA, which is aimed at building a latent factor probability model without any assumption of prior distributions. On the other hand, the <strong>Latent Dirichlet Allocation</strong> is a similar approach that assumes a prior Dirichlet distribution for latent variables. In the last section, we're going to discuss sentiment analysis with a concrete example based on a Twitter dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Topic modeling</h1>
                </header>
            
            <article>
                
<p>The main goal of topic modeling in natural language processing is to analyze a corpus in order to identify common topics among documents. In this context, even if we talk about semantics, this concept has a particular meaning, driven by a very important assumption. A topic derives from the usage of particular terms in the same document and it is confirmed by the multiplicity of different documents where the first condition is true.</p>
<p>In other words, we don't consider a human-oriented semantics but a statistical modeling that works with meaningful documents (this guarantees that the usage of terms is aimed to express a particular concept and, therefore, there's a human semantic purpose behind them). For this reason, the starting point of all our methods is an occurrence matrix, normally defined as a document-term matrix (we have already discussed count vectorizing and tf-idf in <a href="d276f3b9-d2e3-4b98-b144-645f7ba08d36.xhtml">Chapter 12</a>, <em>Introduction to NLP</em>):</p>
<div class="CDPAlignCenter CDPAlign"><img height="73" width="471" src="assets/63977317-5b6c-4654-8148-96aef3833a58.png"/></div>
<p class="CDPAlignLeft CDPAlign">In many papers, this matrix is transposed (it's a term-document one); however, scikit-learn produces document-term matrices, and, to avoid confusion, we are going to consider this structure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Latent semantic analysis</h1>
                </header>
            
            <article>
                
<p>The idea behind latent semantic analysis is factorizing <em>M<sub>dw</sub></em> so as to extract a set of latent variables (this means that we can assume their existence but they cannot be observed directly) that work as connectors between the document and terms. As discussed in <a href="9c4b1425-663c-4995-8faf-c0823d854ef1.xhtml">Chapter 11</a>, <em>Introduction to Recommendation Systems</em>, a very common decomposition method is SVD:</p>
<div class="CDPAlignCenter CDPAlign"><img height="37" width="367" src="assets/804f7c06-5366-4973-bd3f-42eb8fe4d3fb.png"/></div>
<p class="CDPAlignLeft CDPAlign">However, we're not interested in a full decomposition; we are interested only in the subspace defined by the top <em>k</em> singular values:</p>
<div class="CDPAlignCenter CDPAlign"><img height="32" width="95" src="assets/76837ca8-9a75-4c09-ae12-098c4a101123.png"/></div>
<p class="CDPAlignLeft CDPAlign">This approximation has the reputation of being the best one considering the Frobenius norm, so it guarantees a very high level of accuracy. When applying it to a document-term matrix, we obtain the following decomposition:</p>
<div class="CDPAlignCenter CDPAlign"><img height="67" width="413" src="assets/ddb9caa0-938b-4570-8aaa-06399d54d44e.png"/></div>
<p class="CDPAlignLeft CDPAlign">Or, in a more compact way:</p>
<div class="CDPAlignCenter CDPAlign"><img height="40" width="145" src="assets/6376620d-c7ab-44a5-91e2-3ad7caf2f569.png"/></div>
<p class="CDPAlignLeft CDPAlign">Here, the first matrix defines a relationship among documents and <em>k</em> latent variables, and the second a relationship among <em>k</em> latent variables and words. Considering the structure of the original matrix and what is explained at the beginning of this chapter, we can consider the latent variables as <strong>topics</strong> that define a subspace where the documents are projected. A generic document can now be defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="59" width="124" src="assets/beda378a-ddfc-428e-a7e3-088a9dd30cdc.png"/></div>
<p class="CDPAlignLeft CDPAlign">Furthermore, each topic becomes a linear combination of words. As the weight of many words is close to zero, we can decide to take only the top <em>r</em> words to define a topic; therefore, we get:</p>
<div class="CDPAlignCenter CDPAlign"><img height="60" width="92" src="assets/8a768187-1982-4cfe-ae6a-1beb2f94e2bb.png"/></div>
<p class="CDPAlignLeft CDPAlign">Here, each <em>h<sub>ji</sub></em> is obtained after sorting the columns of <em>M<sub>twk</sub></em>. To better understand the process, let's show a complete example based on a subset of Brown corpus (500 documents from the <kbd>news</kbd><span> category</span><span>):</span></p>
<pre class="CDPAlignLeft CDPAlign"><strong>from nltk.corpus import brown</strong><br/><br/><strong>&gt;&gt;&gt; sentences = brown.sents(categories=['news'])[0:500]</strong><br/><strong>&gt;&gt;&gt; corpus = []</strong><br/><br/><strong>&gt;&gt;&gt; for s in sentences:</strong><br/><strong>&gt;&gt;&gt;   corpus.append(' '.join(s))</strong></pre>
<p>After defining the corpus, we need to tokenize and vectorize using a tf-idf approach:</p>
<pre><strong>from sklearn.feature_extraction.text import TfidfVectorizer</strong><br/><br/><strong>&gt;&gt;&gt; vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', norm='l2', sublinear_tf=True)</strong><br/><strong>&gt;&gt;&gt; Xc = vectorizer.fit_transform(corpus).todense()</strong></pre>
<p>Now it's possible to apply an SVD to the <kbd>Xc</kbd> <span>matrix</span><span> </span><span>(remember that in SciPy, the</span> <kbd>V</kbd><span> </span><span>matrix</span><span> </span><span>is already transposed):</span></p>
<pre><strong>from scipy.linalg import svd</strong><br/><br/><strong>&gt;&gt;&gt; U, s, V = svd(Xc, full_matrices=False)</strong></pre>
<p>As the corpus is not very small, it's useful to set the parameter <kbd>full_matrices=False</kbd> to save computational time. We assume we have two topics, so we can extract our sub-matrices:</p>
<pre><strong>import numpy as np<br/><br/>&gt;&gt;&gt; rank = 2</strong><br/><br/><strong>&gt;&gt;&gt; Uk = U[:, 0:rank]</strong><br/><strong>&gt;&gt;&gt; sk = np.diag(s)[0:rank, 0:rank]</strong><br/><strong>&gt;&gt;&gt; Vk = V[0:rank, :]</strong></pre>
<p>If we want to analyze the top 10 words per topic, we need to consider that:</p>
<div class="CDPAlignCenter CDPAlign"><img height="33" width="80" src="assets/a1f988ce-2dc8-4c50-b703-e004e3bcec7d.png"/></div>
<p class="CDPAlignLeft CDPAlign">Therefore, we can obtain the most significant words per topic after sorting the matrix using the <kbd>get_feature_names()</kbd> <span>method</span><span> </span><span>provided by the vectorizers:</span></p>
<pre class="CDPAlignLeft CDPAlign"><strong>&gt;&gt;&gt; Mtwks = np.argsort(Vk, axis=1)[::-1]</strong><br/><br/><strong>&gt;&gt;&gt; for t in range(rank):</strong><br/><strong>&gt;&gt;&gt;   print('\nTopic ' + str(t))</strong><br/><strong>&gt;&gt;&gt;     for i in range(10):</strong><br/><strong>&gt;&gt;&gt;        print(vectorizer.get_feature_names()[Mtwks[t, i]])</strong><br/><br/><strong>Topic 0<br/>said<br/>mr<br/>city<br/>hawksley<br/>president<br/>year<br/>time<br/>council<br/>election<br/>federal<br/><br/>Topic 1<br/>plainfield<br/>wasn<br/>copy<br/>released<br/>absence<br/>africa<br/>clash<br/>exacerbated<br/>facing<br/>difficulties</strong></pre>
<p><span>In this case, we're considering only non-negative values in the matrix <kbd>Vk</kbd>; however, as a topic is a</span> mixture<span> of words, the negative components should </span><span>also</span><span> </span><span>be taken into account. In this case, we need to sort the absolute values of</span> <kbd>Vk</kbd><span>:</span></p>
<pre class="CDPAlignLeft CDPAlign"><strong>&gt;&gt;&gt; Mtwks = np.argsort(np.abs(Vk), axis=1)[::-1]</strong></pre>
<p>If we want to analyze how a document is represented in this sub-space, we must use:</p>
<div class="CDPAlignCenter CDPAlign"><img height="43" width="103" src="assets/3dc27e85-a78a-4495-887f-69f2ada30f94.png"/></div>
<p class="CDPAlignLeft CDPAlign">Let's consider, for example, the first document of our corpus:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>&gt;&gt;&gt; print(corpus[0])</strong><br/><strong>The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .</strong><br/><br/><strong>&gt;&gt;&gt; Mdtk = Uk.dot(sk)</strong><br/><br/><strong>&gt;&gt;&gt; print('d0 = %.2f*t1 + %.2f*t2' % (Mdtk[0][0], Mdtk[0][1]))</strong><br/><strong>d0 = 0.15*t1 + -0.12*t2</strong></pre>
<p>As we are working in a bidimensional space, it's interesting to plot all the points corresponding to each document:</p>
<div class="CDPAlignCenter CDPAlign"><img height="314" width="508" class="image-border" src="assets/22cc8148-a648-41a4-9d21-35bcc85deb1e.png"/></div>
<p class="CDPAlignLeft CDPAlign">In the previous figure, we can see that many documents are correlated, with a small group of outliers. This is probably due to the fact that our choice of two topics is restrictive. If we repeat the same experiment using two Brown corpus categories (<kbd>news</kbd> and <kbd>fiction</kbd>), we observe a different behavior:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>sentences = brown.sents(categories=['news', 'fiction'])</strong><br/><strong>corpus = []</strong><br/><br/><strong>for s in sentences:</strong><br/><strong> corpus.append(' '.join(s))</strong></pre>
<p>I don't repeat the remaining calculations because they are similar. (The only difference is that our corpus is now quite bigger and this leads to a longer computational time. For this reason, we're going to discuss an alternative, which is much faster.) Plotting the points corresponding to the documents, <span>we</span><span> </span><span>now get:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="312" width="500" class="image-border" src="assets/2b2c24ea-f6a0-4df6-aab5-6b57d85ccfb6.png"/></div>
<p class="CDPAlignLeft CDPAlign">Now it's easier to distinguish two groups, which are almost orthogonal (meaning that many documents belong to <span>only</span><span> </span><span>one category). I suggest repeating this experiment with different corpora and ranks. Unfortunately, it's impossible to plot more than three dimensions, but it's always possible to check whether the sub-space describes the underlying semantics </span><span>correctly</span><span> </span><span>using only numerical computations.</span></p>
<p class="CDPAlignLeft CDPAlign">As anticipated, the standard SciPy SVD implementation can be really slow when the occurrence matrix is huge; however, scikit-learn provides a truncated SVD implementation, <kbd>TruncatedSVD</kbd>, that works only with the sub-space. The result is much faster (it can directly manage sparse matrices too). Let's repeat the previous experiments (with a complete corpus) using this class:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>from sklearn.decomposition import TruncatedSVD</strong><br/><br/><strong>&gt;&gt;&gt; tsvd = TruncatedSVD(n_components=rank)</strong><br/><strong>&gt;&gt;&gt; Xt = tsvd.fit_transform(Xc)</strong></pre>
<p>Through the <kbd>n_components</kbd> <span>parameter,</span><span> </span><span>it's possible to set the desired rank, discarding the remaining parts of the matrices. After fitting the model, we get the document-topic matrix</span> <em>M<sub>dtk</sub></em> <span>directly as the output of the method</span> <kbd>fit_transform()</kbd><span>, while the topic-word matrix</span> <em>M<sub>twk</sub></em> <span>can be accessed using the instance variable</span> <kbd>components_</kbd><span>:</span></p>
<pre><strong>&gt;&gt;&gt; Mtws = np.argsort(tsvd.components_, axis=1)[::-1]</strong><br/><br/><strong>&gt;&gt;&gt; for t in range(rank):</strong><br/><strong>&gt;&gt;&gt;    print('\nTopic ' + str(t))</strong><br/><strong>&gt;&gt;&gt;       for i in range(10):</strong><br/><strong>&gt;&gt;&gt;          print(vectorizer.get_feature_names()[Mwts[t, i]])</strong><br/><br/><strong>Topic 0</strong><br/><strong>said</strong><br/><strong>rector</strong><br/><strong>hans</strong><br/><strong>aloud</strong><br/><strong>liston</strong><br/><strong>nonsense</strong><br/><strong>leave</strong><br/><strong>whiskey</strong><br/><strong>chicken</strong><br/><strong>fat</strong><br/><br/><strong>Topic 1</strong><br/><strong>bong</strong><br/><strong>varnessa</strong><br/><strong>schoolboy</strong><br/><strong>kaboom</strong><br/><strong>keeeerist</strong><br/><strong>aggravated</strong><br/><strong>jealous</strong><br/><strong>hides</strong><br/><strong>mayonnaise</strong><br/><strong>fowl</strong></pre>
<p>The reader can verify how much faster this process can be; therefore, I suggest using a standard SVD implementation only when it's needed to have access to the full matrices. Unfortunately, as is also written in the documentation, this method is very sensitive to the algorithm and the random state. It also suffers from a phenomenon called <strong>sign indeterminacy</strong>, which means that the signs of all components can change if a different random seed is used. I suggest you declare:</p>
<pre><strong>import numpy as np</strong><br/><br/><strong>np.random.seed(1234)</strong></pre>
<p>Do this with a fixed seed at the beginning of every file (even Jupyter notebooks) to be sure that it's possible to repeat the calculations and always <span>obtain</span><span> </span><span>the same result.</span></p>
<p>Moreover, I advise repeating this experiment using <strong>non-negative matrix factorization</strong>, as described in <a href="7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml">Chapter 3</a>, <em>Feature Selection and Feature Engineering</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Probabilistic latent semantic analysis</h1>
                </header>
            
            <article>
                
<p>The previous model was based on a deterministic approach, but it's also possible to define a probabilistic model over the space determined by documents and words. In this case, we're not making any assumption about Apriori probabilities (this will be done in the next approach), and we're going to determine the parameters that maximize the log-likelihood of our model. In particular, consider the plate notation (if you want to know more about this technique, read <a href="https://en.wikipedia.org/wiki/Plate_notation">https://en.wikipedia.org/wiki/Plate_notation</a>) shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/4f6fddca-41f4-419f-8b5d-d88135beab30.png"/></div>
<p class="CDPAlignLeft CDPAlign">We assume we have a corpus of <em>m</em> documents and each of them is composed of <em>n</em> words (both elements are observed and therefore represented as gray circles); however, we also assume the presence of a limited set of <em>k</em> common latent factors (topics) that link a document with a group of words (as they are not observed, the circle is white). As already written, we cannot observe them directly, but we're allowed to assume their existence.</p>
<p class="CDPAlignLeft CDPAlign">The joint probability to find a document with a particular word is:</p>
<div class="CDPAlignCenter CDPAlign"><img height="27" width="144" src="assets/bf99ba35-7325-46ac-9759-c0226c94c541.png"/> </div>
<p class="CDPAlignLeft CDPAlign">Therefore, after introducing the latent factors, the conditional probability to find a word in a specific document can be written as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="61" width="203" src="assets/5f8ece24-825d-4ef1-9491-d0c10caaf96c.png"/> </div>
<p class="CDPAlignLeft CDPAlign">The initial joint probability <em>P(d, w)</em> can be also expressed using the latent factors:</p>
<div class="CDPAlignCenter CDPAlign"><img height="53" width="215" src="assets/4f5761ae-c0d7-42b5-ba13-add58068af28.png"/></div>
<p class="CDPAlignLeft CDPAlign">This includes the prior probability <em>P(t)</em>. As we don't want to work with it, it's preferable to use the expression <em>P(w|d)</em>. To determine the two conditional probability distributions, a common approach is the <strong>expectation-maximization</strong> (<strong>EM</strong>) strategy. A full description can be found in Hofmann T., <em>Unsupervised Learning by Probabilistic Latent Semantic Analysis</em>, Machine Learning 42, 177-196, 2001, Kluwer Academic Publishers<em>. </em>In this context, we show only the final results without any proof.</p>
<p class="CDPAlignLeft CDPAlign">The log-likelihood can be written as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="51" width="217" src="assets/8006da53-20d2-4e60-ae2e-0fdcac43e2e9.png"/></div>
<p class="CDPAlignLeft CDPAlign">Which becomes:</p>
<div class="CDPAlignCenter CDPAlign"><img height="53" width="440" src="assets/d7bcae69-f621-4ce8-8774-0e4120b69411.png"/></div>
<p class="CDPAlignLeft CDPAlign"><em>M<sub>dw</sub></em> is an occurrence matrix (normally obtained with a count vectorizer) and <em>M<sub>dw</sub>(d, w)</em> is the frequency of the word <em>w</em> in document <em>d</em>. For simplicity, we are going to approximate it by excluding the first term (which doesn't depend on <em>t<sub>k</sub></em>):</p>
<div class="CDPAlignCenter CDPAlign"><img height="50" width="304" src="assets/00ab1edb-b077-4522-999a-e6bec52b90fb.png"/></div>
<p class="CDPAlignLeft CDPAlign">Moreover, it's useful to introduce the conditional probability <em>P(t|d,w)</em>, which is the probability of a topic given a document and a word. <span>The EM algorithm maximizes the expected complete log-likelihood under the posterior probability <em>P(t|d,w)</em>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="56" width="389" src="assets/47ae99fd-dbcf-462e-9d4d-fed24934c75b.png"/></div>
<p class="CDPAlignLeft CDPAlign">The <strong>E</strong> phase of the algorithm can be expressed as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="51" width="207" src="assets/9b106abc-630e-42f9-a621-29265ad5e83e.png"/></div>
<p class="CDPAlignLeft CDPAlign">It must be extended to all topics, words, and documents and must be normalized with the sum per topic in order to <span>always</span><span> </span><span>have consistent probabilities.</span></p>
<p class="CDPAlignLeft CDPAlign">The <strong>M</strong> phase is split into two computations:</p>
<div class="CDPAlignCenter CDPAlign"><img height="91" width="273" src="assets/8ee6088d-59d4-461b-9e79-cbe4a96b7958.png"/></div>
<p class="CDPAlignLeft CDPAlign">Also in this case, the calculations must be extended to all topics, words, and documents. But in the first case, we sum by document and normalize by summing by word and document, while in the second, we sum by word and normalize by the length of the document.</p>
<p class="CDPAlignLeft CDPAlign">The algorithm must be iterated until the log-likelihood stops increasing its magnitude. Unfortunately, scikit-learn doesn't provide a PLSA implementation (maybe because the next strategy, LDA, is considered much more powerful and efficient), so we need to write some code from scratch. Let's start by defining a small subset of the Brown corpus, taking 10 sentences from the <kbd>editorial</kbd> category and 10 from the <kbd>fiction</kbd> one:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>&gt;&gt;&gt; sentences_1 = brown.sents(categories=['editorial'])[0:10]</strong><br/><strong>&gt;&gt;&gt; sentences_2 = brown.sents(categories=['fiction'])[0:10]</strong><br/><strong>&gt;&gt;&gt; corpus = []</strong><br/><br/><strong>&gt;&gt;&gt; for s in sentences_1 + sentences_2:</strong><br/><strong>&gt;&gt;&gt;    corpus.append(' '.join(s))</strong></pre>
<p>Now we can vectorize using the <kbd>CountVectorizer</kbd> class:</p>
<pre><strong>import numpy as np</strong><br/><br/><strong>from sklearn.feature_extraction.text import CountVectorizer</strong><br/><br/><strong>&gt;&gt;&gt; cv = CountVectorizer(strip_accents='unicode', stop_words='english')</strong><br/><strong>&gt;&gt;&gt; Xc = np.array(cv.fit_transform(corpus).todense())</strong></pre>
<p>At this point, we can define the rank (we choose 2 for simplicity), two constants that will be used later, and the matrices to hold the probabilities <em>P(t|d)</em>, <em>P(w|t)</em>, and <em>P(t|d,w)</em>:</p>
<pre><strong>&gt;&gt;&gt; rank = 2</strong><br/><strong>&gt;&gt;&gt; alpha_1 = 1000.0</strong><br/><strong>&gt;&gt;&gt; alpha_2 = 10.0</strong><br/><br/><strong>&gt;&gt;&gt; Ptd = np.random.uniform(0.0, 1.0, size=(len(corpus), rank))</strong><br/><strong>&gt;&gt;&gt; Pwt = np.random.uniform(0.0, 1.0, size=(rank, len(cv.vocabulary_)))</strong><br/><strong>&gt;&gt;&gt; Ptdw = np.zeros(shape=(len(cv.vocabulary_), len(corpus), rank))</strong><br/><br/><strong>&gt;&gt;&gt; for d in range(len(corpus)):</strong><br/><strong>&gt;&gt;&gt;    nf = np.sum(Ptd[d, :])</strong><br/><strong>&gt;&gt;&gt;    for t in range(rank):</strong><br/><strong>&gt;&gt;&gt;       Ptd[d, t] /= nf</strong><br/><br/><strong>&gt;&gt;&gt; for t in range(rank):</strong><br/><strong>&gt;&gt;&gt;    nf = np.sum(Pwt[t, :])</strong><br/><strong>&gt;&gt;&gt;    for w in range(len(cv.vocabulary_)):</strong><br/><strong>&gt;&gt;&gt;       Pwt[t, w] /= nf</strong></pre>
<p>The two matrices <em>P(t|d)</em><span>,</span> <em>P(w|t)</em> must be normalized so as to be coherent with the algorithm; the other one is initialized to zero. Now we can define the log-likelihood function:</p>
<pre><strong>&gt;&gt;&gt; def log_likelihood():</strong><br/><strong>&gt;&gt;&gt;    value = 0.0</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;    for d in range(len(corpus)):</strong><br/><strong>&gt;&gt;&gt;       for w in range(len(cv.vocabulary_)):</strong><br/><strong>&gt;&gt;&gt;          real_topic_value = 0.0</strong><br/><strong>&gt;&gt;&gt;</strong><br/><strong>&gt;&gt;&gt;          for t in range(rank):</strong><br/><strong>&gt;&gt;&gt;             real_topic_value += Ptd[d, t] * Pwt[t, w]</strong><br/><strong>&gt;&gt;&gt;</strong><br/><strong>&gt;&gt;&gt;          if real_topic_value &gt; 0.0:</strong><br/><strong>&gt;&gt;&gt;             value += Xc[d, w] * np.log(real_topic_value)</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;    return value</strong></pre>
<p>And finally the expectation-maximization functions:</p>
<pre><strong>&gt;&gt;&gt; def expectation():</strong><br/><strong>&gt;&gt;&gt;    global Ptd, Pwt, Ptdw</strong><br/><strong>&gt;&gt;&gt;</strong><br/><strong>&gt;&gt;&gt;    for d in range(len(corpus)):</strong><br/><strong>&gt;&gt;&gt;       for w in range(len(cv.vocabulary_)):</strong><br/><strong>&gt;&gt;&gt;          nf = 0.0</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;          for t in range(rank):</strong><br/><strong>&gt;&gt;&gt;             Ptdw[w, d, t] = Ptd[d, t] * Pwt[t, w]</strong><br/><strong>&gt;&gt;&gt;             nf += Ptdw[w, d, t]</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;          Ptdw[w, d, :] = (Ptdw[w, d, :] / nf) if nf != 0.0 else 0.0</strong></pre>
<p>In the preceding function, when the normalization factor is 0, the probability <em>P(t|w, d)</em> is set to 0.0 for each topic:</p>
<pre><strong>&gt;&gt;&gt; def maximization():</strong><br/><strong>&gt;&gt;&gt;    global Ptd, Pwt, Ptdw</strong><br/><strong>&gt;&gt;&gt;</strong><br/><strong>&gt;&gt;&gt;    for t in range(rank):</strong><br/><strong>&gt;&gt;&gt;       nf = 0.0</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;       for d in range(len(corpus)):</strong><br/><strong>&gt;&gt;&gt;          ps = 0.0</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;          for w in range(len(cv.vocabulary_)):</strong><br/><strong>&gt;&gt;&gt;             ps += Xc[d, w] * Ptdw[w, d, t]</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;          Pwt[t, w] = ps</strong><br/><strong>&gt;&gt;&gt;          nf += Pwt[t, w]</strong><br/><strong>&gt;&gt;&gt;</strong><br/><strong>&gt;&gt;&gt;       Pwt[:, w] /= nf if nf != 0.0 else alpha_1</strong><br/><strong>&gt;&gt;&gt;</strong><br/><strong>&gt;&gt;&gt;    for d in range(len(corpus)):</strong><br/><strong>&gt;&gt;&gt;       for t in range(rank):</strong><br/><strong>&gt;&gt;&gt;          ps = 0.0</strong><br/><strong>&gt;&gt;&gt;          nf = 0.0</strong><br/><strong>&gt;&gt;&gt;</strong><br/><strong>&gt;&gt;&gt;          for w in range(len(cv.vocabulary_)):</strong><br/><strong>&gt;&gt;&gt;             ps += Xc[d, w] * Ptdw[w, d, t]</strong><br/><strong>&gt;&gt;&gt;             nf += Xc[d, w]</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;          Ptd[d, t] = ps / (nf if nf != 0.0 else alpha_2)</strong></pre>
<p>The constants <kbd>alpha_1</kbd> and <kbd>alpha_2</kbd> are used when a normalization factor becomes 0. In that case, it can be useful to assign the probability a small value; therefore we divided the numerator for those constants. I suggest trying with different values so as to tune up the algorithm for different tasks.</p>
<p>At this point, we can try our algorithm with a limited number of iterations:</p>
<pre><strong>&gt;&gt;&gt; print('Initial Log-Likelihood: %f' % log_likelihood())</strong><br/><br/><strong>&gt;&gt;&gt; for i in range(50):</strong><br/><strong>&gt;&gt;&gt;    expectation()</strong><br/><strong>&gt;&gt;&gt;    maximization()</strong><br/><strong>&gt;&gt;&gt;    print('Step %d - Log-Likelihood: %f' % (i, log_likelihood()))<br/><br/>Initial Log-Likelihood: -1242.878549<br/>Step 0 - Log-Likelihood: -1240.160748<br/>Step 1 - Log-Likelihood: -1237.584194<br/>Step 2 - Log-Likelihood: -1236.009227<br/>Step 3 - Log-Likelihood: -1234.993974<br/>Step 4 - Log-Likelihood: -1234.318545<br/>Step 5 - Log-Likelihood: -1233.864516<br/>Step 6 - Log-Likelihood: -1233.559474<br/>Step 7 - Log-Likelihood: -1233.355097<br/>Step 8 - Log-Likelihood: -1233.218306<br/>Step 9 - Log-Likelihood: -1233.126583<br/>Step 10 - Log-Likelihood: -1233.064804<br/>Step 11 - Log-Likelihood: -1233.022915<br/>Step 12 - Log-Likelihood: -1232.994274<br/>Step 13 - Log-Likelihood: -1232.974501<br/>Step 14 - Log-Likelihood: -1232.960704<br/>Step 15 - Log-Likelihood: -1232.950965</strong><br/><strong>...</strong></pre>
<p>It's possible to verify the convergence after the 30th step. At this point, we can check the top five words per topic considering the <em>P(w|t)</em> conditional distribution sorted in descending mode per topic weight:</p>
<pre><strong>&gt;&gt;&gt; Pwts = np.argsort(Pwt, axis=1)[::-1]</strong><br/><br/><strong>&gt;&gt;&gt; for t in range(rank):</strong><br/><strong>&gt;&gt;&gt;    print('\nTopic ' + str(t))</strong><br/><strong>&gt;&gt;&gt;       for i in range(5):</strong><br/><strong>&gt;&gt;&gt;          print(cv.get_feature_names()[Pwts[t, i]])</strong><br/><br/><strong>Topic 0</strong><br/><strong>years</strong><br/><strong>questions</strong><br/><strong>south</strong><br/><strong>reform</strong><br/><strong>social</strong><br/><br/><strong>Topic 1</strong><br/><strong>convened</strong><br/><strong>maintenance</strong><br/><strong>penal</strong><br/><strong>year</strong><br/><strong>legislators</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Latent Dirichlet Allocation</h1>
                </header>
            
            <article>
                
<p>In the previous method, we didn't make any assumptions about the topic prior to distribution and this can result in a limitation because the algorithm isn't driven by any real-world intuition. LDA, instead, is based on the idea that a topic is characterized by a small ensemble of important words and normally a document doesn't cover many topics. For this reason, the main assumption is that the prior topic distribution is a symmetric <strong>Dirichlet </strong>one. The probability density function is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="65" width="328" src="assets/5f47820a-b6e1-4601-8fd1-793649391f08.png"/></div>
<p class="CDPAlignLeft CDPAlign">If the concentration parameter alpha is below 1.0, the distribution will be sparse as desired. This allows us to model topic-document and topic-word distributions, which will always be concentrated on a few values. In this way we can avoid the following:</p>
<ul>
<li>The topic mixture assigned to a document could becoming flat (many topics with similar weight)</li>
<li>The structure of a topic considering the word ensemble could becoming similar to a background (in fact, only a limited number of words must be important; otherwise the semantic boundaries fade out).</li>
</ul>
<p>Using the plate notation, we can represent the relationship among documents, topics, and words as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="269" width="504" class="image-border" src="assets/2658f9fb-fac1-4aaf-b84c-1a9f2759b2a3.png"/></div>
<p class="CDPAlignLeft CDPAlign">In the previous figure, alpha is the Dirichlet parameter for the topic-document distribution, while gamma has the same role for the topic-word distribution. Theta, instead, is the topic distribution for a specific document, while beta is the topic distribution for a specific word.</p>
<p class="CDPAlignLeft CDPAlign">If we have a corpus of <em>m</em> documents and a vocabulary of <em>n</em> words (each document has <em>n<sub>i</sub></em> words) and we assume to have <em>k</em> different topics, the generative algorithm can be described with the following steps:</p>
<ul>
<li>For each document, draw a sample (a topic mixture) from the topic-document distribution:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="31" width="169" src="assets/d3a4ad0a-5c17-479b-aeb6-9cf7084ab843.png"/></div>
<ul>
<li>For each topic, draw a sample from the from the topic-word distribution:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="28" width="157" src="assets/f56a7a90-aecd-472b-8236-c6b732851b3d.png"/></div>
<p class="CDPAlignLeft CDPAlign">Both parameters must be estimated. At this point, considering the occurrence matrix <em>M<sub>dw</sub></em> and the notation <em>z<sub>mn</sub></em> to define the topic assigned to the <em>n</em>-th word in the <em>m</em>-th document, we can iterate over documents (index <em>d</em>) and words (index <em>w</em>):</p>
<ul>
<li>A topic for document <em>d</em> and word <em>w</em> is chosen according to:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="25" width="172" src="assets/4d93ad6e-8bc5-4974-9ccb-5a178835345d.png"/></div>
<ul>
<li>A word is chosen according to:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="28" width="175" src="assets/b3ab9e42-ce8b-4269-a511-597a753c15fc.png"/></div>
<p class="CDPAlignLeft CDPAlign">In both cases, a categorical distribution is a one-trial multinomial one. A complete description of how the parameters are estimated is quite complex and it's beyond the scope of this book; however, the main problem is finding the distribution of latent variables:</p>
<div class="CDPAlignCenter CDPAlign"><img height="39" width="211" src="assets/d339ef3f-7607-4030-bc4d-355da52b46e7.png"/></div>
<p class="CDPAlignLeft CDPAlign">The reader can find a lot more information in Blei D., Ng A., Jordan M., <em>Latent Dirichlet Allocation</em>, Journal of Machine Learning Research, 3, (2003) 993-1022<em>. </em>However, a very important difference between LDA and PLSA is about the generative ability of LDA, which allows working with unseen documents. In fact, the PLSA training process finds the optimal parameters <em>p(t|d)</em> only for the corpus, while LDA adopts random variables. It's possible to understand this concept by defining the probability of theta (a topic mixture) as joint with a set of topics and a set of words, and conditioned to the model parameters:</p>
<div class="CDPAlignCenter CDPAlign"> <img height="47" width="297" src="assets/c5691c82-1945-47db-95cb-fe698757a9a6.png"/></div>
<p class="CDPAlignLeft CDPAlign">As shown in the previously mentioned paper, the probability of a document (a set of words) conditioned to the model parameters, can be obtained by integration:</p>
<div class="CDPAlignCenter CDPAlign"><img height="67" width="356" src="assets/32d90868-9ebe-4524-ba35-28207ace2197.png"/></div>
<p class="CDPAlignLeft CDPAlign">This expression shows the difference between PLSA and LDA. Once learned <em>p(t|d)</em>, PLSA cannot generalize, while LDA, sampling from the random variables, can always find a suitable topic mixture for an unseen document.</p>
<p class="CDPAlignLeft CDPAlign">scikit-learn provides a full LDA implementation through the class <kbd>LatentDirichletAllocation</kbd>. We're going to use it with a bigger dataset (4,000 documents) built from a subset of the Brown corpus:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>&gt;&gt;&gt; sentences_1 = brown.sents(categories=['reviews'])[0:1000]</strong><br/><strong>&gt;&gt;&gt; sentences_2 = brown.sents(categories=['government'])[0:1000]</strong><br/><strong>&gt;&gt;&gt; sentences_3 = brown.sents(categories=['fiction'])[0:1000]</strong><br/><strong>&gt;&gt;&gt; sentences_4 = brown.sents(categories=['news'])[0:1000]</strong><br/><strong>&gt;&gt;&gt; corpus = []</strong><br/><br/><strong>&gt;&gt;&gt; for s in sentences_1 + sentences_2 + sentences_3 + sentences_4:</strong><br/><strong>&gt;&gt;&gt;    corpus.append(' '.join(s))</strong></pre>
<p>Now we can vectorize, define, and train our LDA model by assuming that we have eight main topics:</p>
<pre><strong>from sklearn.decomposition import LatentDirichletAllocation</strong><br/><br/><strong>&gt;&gt;&gt; cv = CountVectorizer(strip_accents='unicode', stop_words='english', analyzer='word', token_pattern='[a-z]+')</strong><br/><strong>&gt;&gt;&gt; Xc = cv.fit_transform(corpus)</strong><br/><br/><strong>&gt;&gt;&gt; lda = LatentDirichletAllocation(n_topics=8, learning_method='online', max_iter=25)</strong><br/><strong>&gt;&gt;&gt; Xl = lda.fit_transform(Xc)</strong></pre>
<p>In <kbd>CountVectorizer</kbd>, we added a regular expression to filter the tokens through the parameter <kbd>token_pattern</kbd>. This is useful as we are not using a full tokenizer and, in the corpus, there are also many numbers that we want to filter out. The class <kbd>LatentDirichletAllocation</kbd> allows us to specify the learning method (through <kbd>learning_method</kbd>), which can be either batch or online. We have chosen online because it's faster; however, both methods adopt variational Bayes to learn the parameters. The former adopts the whole dataset, while the latter works with mini-batches. The online option will be removed in the 0.20 release; therefore, you can see a deprecation warning when using it now. Both theta and beta Dirichlet parameters can be specified through <kbd>doc_topic_prior</kbd> (theta) and <kbd>topic_word_prior</kbd> (beta). The default value (adopted by us too) is 1.0 / <kbd>n_topics</kbd> . It's important to keep both values small and, in particular, less than 1.0 in order to encourage sparseness. The maximum number of iterations (<kbd>max_iter</kbd>) and other learning-related parameters can be applied by reading the built-in documentation or visiting <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html">http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html</a>.</p>
<p>Now we can test our model by extracting the top five keywords per topic. Just like <kbd>TruncatedSVD</kbd>, the topic-word distribution results are stored in the instance variable <kbd>components_</kbd>:</p>
<pre><strong>&gt;&gt;&gt; Mwts_lda = np.argsort(lda.components_, axis=1)[::-1]</strong><br/><br/><strong>&gt;&gt;&gt; for t in range(8):</strong><br/><strong>&gt;&gt;&gt;    print('\nTopic ' + str(t))</strong><br/><strong>&gt;&gt;&gt;       for i in range(5):</strong><br/><strong>&gt;&gt;&gt;          print(cv.get_feature_names()[Mwts_lda[t, i]])</strong><br/><br/><strong>Topic 0</strong><br/><strong>code</strong><br/><strong>cadenza</strong><br/><strong>unlocks</strong><br/><strong>ophthalmic</strong><br/><strong>quo</strong><br/><br/><strong>Topic 1</strong><br/><strong>countless</strong><br/><strong>harnick</strong><br/><strong>leni</strong><br/><strong>addle</strong><br/><strong>chivalry</strong><br/><br/><strong>Topic 2</strong><br/><strong>evasive</strong><br/><strong>errant</strong><br/><strong>tum</strong><br/><strong>rum</strong><br/><strong>orations</strong><br/><br/><strong>Topic 3</strong><br/><strong>grigory</strong><br/><strong>tum</strong><br/><strong>absurdity</strong><br/><strong>tarantara</strong><br/><strong>suitably</strong><br/><br/><strong>Topic 4</strong><br/><strong>seventeenth</strong><br/><strong>conant</strong><br/><strong>chivalrous</strong><br/><strong>janitsch</strong><br/><strong>knight</strong><br/><br/><strong>Topic 5</strong><br/><strong>hypocrites</strong><br/><strong>errantry</strong><br/><strong>adventures</strong><br/><strong>knight</strong><br/><strong>errant</strong><br/><br/><strong>Topic 6</strong><br/><strong>counter</strong><br/><strong>rogues</strong><br/><strong>tum</strong><br/><strong>lassus</strong><br/><strong>wars</strong><br/><br/><strong>Topic 7</strong><br/><strong>pitch</strong><br/><strong>cards</strong><br/><strong>cynicism</strong><br/><strong>silences</strong><br/><strong>shrewd</strong></pre>
<p>There are some repetitions, probably due to the composition of some topics, and the reader can try different prior parameters to observe the changes. It's possible to do an experiment to check whether the model works correctly.</p>
<p>Let's consider two documents:</p>
<pre><strong>&gt;&gt;&gt; print(corpus[0])</strong><br/><strong>It is not news that Nathan Milstein is a wizard of the violin .</strong><br/><br/><strong>&gt;&gt;&gt; print(corpus[2500])</strong><br/><strong>The children had nowhere to go and no place to play , not even sidewalks .</strong></pre>
<p>They are quite different and so are their topic distributions:</p>
<pre><strong>&gt;&gt;&gt; print(Xl[0])</strong><br/><strong>[ 0.85412134 0.02083335 0.02083335 0.02083335 0.02083335 0.02083677</strong><br/><strong> 0.02087515 0.02083335]</strong><br/><br/><strong>&gt;&gt;&gt; print(Xl[2500])</strong><br/><strong>[ 0.22499749 0.02500001 0.22500135 0.02500221 0.025 0.02500219</strong><br/><strong> 0.02500001 0.42499674]</strong></pre>
<p>We have a dominant topic <kbd>(0.85t<sub>0</sub>)</kbd> for the first document and a mixture <kbd>(0.22t<sub>0</sub> + 0.22t<sub>2 </sub>+ 0.42t<sub>7</sub>)</kbd> for the second one. Now let's consider the concatenation of both documents:</p>
<pre><strong>&gt;&gt;&gt; test_doc = corpus[0] + ' ' + corpus[2500]</strong><br/><strong>&gt;&gt;&gt; y_test = lda.transform(cv.transform([test_doc]))</strong><br/><br/><strong>&gt;&gt;&gt; print(y_test)</strong><br/><strong>[[ 0.61242771 0.01250001 0.11251451 0.0125011 0.01250001 0.01250278</strong><br/><strong> 0.01251778 0.21253611]]</strong></pre>
<p>In the resulting document, as expected, the mixture has changed: <kbd>0.61t<sub><span>0</span></sub> + 0.11t<sub>2</sub> + 0.21t<sub>7</sub></kbd>. In other words, the algorithm introduced the previously dominant topic 5 (which is now stronger) by weakening both topic 2 and topic 7. This is reasonable, because the length of the first document is less than the second one, and therefore topic 5 cannot completely cancel the other topics out.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sentiment analysis</h1>
                </header>
            
            <article>
                
<p>One the most widespread applications of <span>NLP</span><span> i</span><span>s sentiment analysis of short texts (tweets, posts, comments, reviews, and so on). From a marketing viewpoint, it's very important to understand the semantics of these pieces of information in terms of the sentiment expressed. As you can understand, this task can be very easy when the comment is precise and contains only a set of positive/negative words, but it becomes more complex when in the same sentence there are different propositions that can conflict with each other. For example,</span> <em>I loved that hotel. It was a wonderful experience</em><span> is clearly a positive comment, while</span> <em>The hotel is good, however, the restaurant was bad and, even if the waiters were kind, I had to fight with a receptionist to have another pillow</em><span>. In this case, the situation is more difficult to manage, because there are both positive and negative elements, resulting in a neutral review. For this reason, many applications aren't based on a binary decision but admit intermediate levels (at least one to express the neutrality).</span></p>
<p>These kind of problems are normally supervised (as we're going to do), but there are also cheaper and more complex solutions. The simplest way to evaluate the sentiment is to look for particular keywords. This dictionary-based approach is fast and, together with a good stemmer, can immediately mark positive and negative documents. On the flip side, it doesn't consider the relationship among terms and cannot learn how to weight the different components. For example, <em>Lovely day, bad mood</em> will result in a neutral (+1, -1), while with a supervised approach it's possible to make the model learn that <em>mood</em> is very important and <em>bad mood</em> will normally drive to a negative sentiment. Other approaches (much more complex) are based on topic modeling (you can now understand how to apply LSA or LDA to determine the underlying topics in terms of positivity or negativity); however, they need further steps to use topic-word and topic-document distributions. It can be helpful in the real semantics of a comment, where, for example, a positive adjective is normally used together with other similar components (like verbs). Say, <em>Lovely hotel, I'm surely coming back</em>. In this case (if the number of samples is big enough), a topic can emerge from the combination of words such as <em>lovely</em> or <em>amazing</em> and (positive) verbs such as <em>returning</em> or <em>coming back</em>.</p>
<p>An alternative is to consider the topic distribution of positive and negative documents and work with a supervised approach in the topic sub-space. Other approaches include deep-learning techniques (such as Word2Vec or Doc2Vec) and are based on the idea of generating a vectorial space where similar words are close to each other, in order to easily manage synonyms. For example, if the training set contains the sentence <em>Lovely hotel</em> but it doesn't contain <em>Wonderful hotel</em>, a Word2Vec model can learn from other examples that <em>lovely</em> and <em>wonderful</em> are very close; therefore the new document <em>Wonderful hotel</em> is immediately classified using the knowledge provided by the first comment. An introduction to this technique, together with some technical papers, can be found at <a href="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a>.</p>
<p>Let's now consider our example, which is based on a subset of the <em>Twitter Sentiment Analysis Training Corpus</em> dataset. In order to speed up the process, we have limited the experiment to 1,00,000 tweets. After downloading the file (see the box at the end of this paragraph), it's necessary to parse it (using the UTF-8 encoding):</p>
<pre><strong>&gt;&gt;&gt; dataset = 'dataset.csv'</strong><br/><br/><strong>&gt;&gt;&gt; corpus = []</strong><br/><strong>&gt;&gt;&gt; labels = []</strong><br/><br/><strong>&gt;&gt;&gt; with open(dataset, 'r', encoding='utf-8') as df:</strong><br/><strong>&gt;&gt;&gt;    for i, line in enumerate(df):</strong><br/><strong>&gt;&gt;&gt;    if i == 0:</strong><br/><strong>&gt;&gt;&gt;       continue</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;    parts = line.strip().split(',')</strong><br/><strong>&gt;&gt;&gt;    labels.append(float(parts[1].strip()))</strong><br/><strong>&gt;&gt;&gt;    corpus.append(parts[3].strip())</strong></pre>
<p>The <kbd>dataset</kbd> <span>variable</span><span> </span><span>must contain the full path to the CSV file. This procedure reads all the lines skipping the first one (which is the header), and stores each tweet as a new list entry in the</span> <kbd>corpus</kbd><span> </span><span>variable,</span><span> </span><span>and the corresponding sentiment (which is binary, 0 or 1) in the</span> <kbd>labels</kbd><span> variable</span><span>. At this point, we proceed as usual, tokenizing, vectorizing, and preparing the training and test sets:</span></p>
<pre><strong>from nltk.tokenize import RegexpTokenizer</strong><br/><strong>from nltk.corpus import stopwords</strong><br/><strong>from nltk.stem.lancaster import LancasterStemmer</strong><br/><br/><strong>from sklearn.feature_extraction.text import TfidfVectorizer</strong><br/><strong>from sklearn.model_selection import train_test_split</strong><br/><br/><strong>&gt;&gt;&gt; rt = RegexpTokenizer('[a-zA-Z0-9\.]+')</strong><br/><strong>&gt;&gt;&gt; ls = LancasterStemmer()</strong><br/><strong>&gt;&gt;&gt; sw = set(stopwords.words('english'))</strong><br/><br/><strong>&gt;&gt;&gt; def tokenizer(sentence):</strong><br/><strong>&gt;&gt;&gt;    tokens = rt.tokenize(sentence)</strong><br/><strong>&gt;&gt;&gt;    return [ls.stem(t.lower()) for t in tokens if t not in sw]</strong><br/><br/><strong>&gt;&gt;&gt; tfv = TfidfVectorizer(tokenizer=tokenizer, sublinear_tf=True, ngram_range=(1, 2), norm='l2')</strong><br/><strong>&gt;&gt;&gt; X = tfv.fit_transform(corpus[0:100000])</strong><br/><strong>&gt;&gt;&gt; Y = np.array(labels[0:100000])</strong><br/><br/><strong>&gt;&gt;&gt; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)</strong></pre>
<p>We have chosen to include dots together with letters and numbers in the <kbd>RegexpTokenizer</kbd> instance because they are useful for expressing particular emotions. Moreover, the n-gram range has been set to (1, 2), so we include bigrams (the reader can try with trigrams too). At this point, we can train a random forest:</p>
<pre><strong>from sklearn.ensemble import RandomForestClassifier</strong><br/><br/><strong>import multiprocessing</strong><br/><br/><strong>&gt;&gt;&gt; rf = RandomForestClassifier(n_estimators=20, n_jobs=multiprocessing.cpu_count())</strong><br/><strong>&gt;&gt;&gt; rf.fit(X_train, Y_train)</strong></pre>
<p>Now we can produce some metrics to evaluate the model:</p>
<pre><strong>from sklearn.metrics import precision_score, recall_score<br/><br/>&gt;&gt;&gt; print('Precision: %.3f' % precision_score(Y_test, rf.predict(X_test)))<br/>Precision: 0.720</strong><br/><br/><strong>&gt;&gt;&gt; print('Recall: %.3f' % recall_score(Y_test, rf.predict(X_test)))<br/>Recall: 0.784<br/></strong></pre>
<p>The performances are not excellent (it's possible to achieve better accuracies using Word2Vec); however, they are acceptable for many tasks. In particular, a 78% recall means that the number of false negatives is about 20% and it can be useful when using sentiment analysis for an automatic processing task (in many cases, the risk threshold to auto-publish a negative review is quite a bit lower, and, therefore, a better solution must be employed). The performances can be also confirmed by the corresponding ROC curve:</p>
<div class="CDPAlignCenter CDPAlign"><img height="417" width="409" class="image-border" src="assets/d74b2df8-0327-4453-bd56-9633db59673e.png"/></div>
<div class="packt_infobox"><span>The <em>Twitter Sentiment Analysis Training Corpus</em> dataset (as a CSV file) used in the example can be downloaded from <a href="http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip">http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip</a>. Considering the amount of data, the training process can be very long (even taking hours on slower machines).</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">VADER sentiment analysis with NLTK</h1>
                </header>
            
            <article>
                
<p>For the English language, NLTK provides an already trained model called <strong>VADER</strong> (<span><strong>Valence Aware Dictionary and sEntiment Reasoner</strong>) that works in a slightly different way and adopts a rule engine together with a lexicon to infer the sentiment intensity of a piece of text. More information and details can be found in </span>Hutto C.J., Gilbert E., <em>VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text</em>, AAAI, 2014<em>.</em></p>
<p><span>The NLTK version uses the</span> <span class="s1"><kbd>SentimentIntensityAnalyzer</kbd> </span><span>class </span><span>and can immediately be used to have a polarity sentiment measure made up of four components:</span></p>
<ul>
<li>Positive factor</li>
<li>Negative factor</li>
<li>Neutral factor</li>
<li>Compound factor</li>
</ul>
<p>The first three don't need any explanation, while the last one is a particular measure (a normalized overall score), which is computed as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="51" width="266" src="assets/2db0e565-d2f1-453e-a70d-698edba300ee.png"/></div>
<p class="CDPAlignLeft CDPAlign">Here, <em>Sentiment(w<sub>i</sub>)</em> is the score valence of the word <em>w<sub>i</sub></em> and alpha is a normalization coefficient that should approximate the maximum expected value (the default value set in NLTK is 15). The usage of this class is immediate, as the following snippet can confirm:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>from nltk.sentiment.vader import SentimentIntensityAnalyzer<br/><br/>&gt;&gt;&gt; text = 'This is a very interesting and quite powerful sentiment analyzer'<br/><br/>&gt;&gt;&gt; vader = SentimentIntensityAnalyzer()<br/>&gt;&gt;&gt; print(vader.polarity_scores(text))<br/><span class="s1">{'neg': 0.0, 'neu': 0.535, 'pos': 0.465, 'compound': 0.7258}</span> </strong></pre>
<div class="packt_infobox">The NLTK Vader implementation uses the library Twython for some functionalities. Even though it's not necessary, in order to avoid a warning, it's possible to install it using pip (<kbd>pip install twython</kbd>).</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Hofmann T., <em>Unsupervised Learning by Probabilistic Latent Semantic Analysis</em>, Machine Learning 42, 177-196, 2001, Kluwer Academic Publishers.</li>
<li>Blei D., Ng A., Jordan M., <em>Latent Dirichlet Allocation, Journal of Machine Learning Research</em>, 3, (2003) 993-1022.</li>
<li>Hutto C.J., Gilbert E., <em>VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text</em>, AAAI, 2014.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced topic modeling. We discussed latent semantic analysis based on truncated SVD, probabilistic latent semantic analysis (which aims to build a model without assumptions about latent factor prior probabilities), and latent Dirichlet allocation, which outperformed the previous method and is based on the assumption that the latent factor has a sparse prior Dirichlet distribution. This means that a document normally covers only a limited number of topics and a topic is characterized only by a few important words.</p>
<p>In the last section, we discussed sentiment analysis of documents, which is aimed at determining whether a piece of text expresses a positive or negative feeling. In order to show a feasible solution, we built a classifier based on an NLP pipeline and a random forest with average performances that can be used in many real-life situations.</p>
<p>In the next chapter, we're going to briefly introduce deep learning, together with the TensorFlow framework. As this topic alone requires a dedicated book, our goal is to define the main concepts with some practical examples. If the reader wants to have further information, at the end of the chapter, a complete reference list will be provided.</p>


            </article>

            
        </section>
    </body></html>