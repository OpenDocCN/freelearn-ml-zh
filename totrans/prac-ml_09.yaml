- en: Chapter 9. Bayesian learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will go back to covering an important, statistical-based
    method of learning called the Bayesian method learning, and in particular, the
    Naïve Bayes algorithm among others. The statistical models generally have an explicit
    probability model, which reveals the probability of an instance belonging to a
    particular class rather than just classification while solving a classification
    problem. Before taking a deep dive into the Bayesian learning, you will learn
    some important concepts under statistics such as probability distribution and
    the Bayes theorem which is the heart of Bayesian learning.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian learning is a supervised learning technique where the goal is to build
    a model of the distribution of class labels that have a concrete definition of
    the target attribute. Naïve Bayes is based on applying Bayes' theorem with the
    *naïve* assumption of independence between each and every pair of features.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn the basics and advanced concepts of this technique and get hands-on
    implementation guidance in using Apache Mahout, R, Julia, Apache Spark, and Python
    to implement the means - clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts different learning models covered in this book,
    and the technique highlighted will be dealt with in detail in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian learning](img/B03980_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The topics listed here are covered in depth in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of Bayesian statistics and core principles or concepts of probability,
    distribution, and other relevant statistical measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayes' theorem and its mechanics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep dive into the Naïve Bayes algorithm and variations of Naïve Bayes classifiers
    like multinomial and Bernoulli classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A detailed explanation of some real-world problems or use cases that the Bayesian
    learning technique can address
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample implementation using Apache Mahout, R, Apache Spark, Julia, and Python
    (scikit-learn) libraries and modules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Under supervised learning techniques, the learning models that are categorized
    under statistical methods are instance-based learning methods and the Bayesian
    learning method. Before we understand the Bayesian learning method, we will first
    cover an overview of concepts of probabilistic modeling and Bayesian statistics
    that are relevant in the context of Machine learning. The core concepts of statistics
    are very deep, and what will be covered in the next few sections is primarily
    focused on equipping you with a basic understanding of the dynamic and diverse
    field of probabilistic Machine learning, which is sufficient to interpret the
    functioning of the Bayesian learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: Statistician's thinking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The objective of statisticians is to answer questions asked by people from various
    domains using data. The typical engineering methods use some subjective/objective
    methods that do not require data to answer the questions. But, statisticians always
    look at the data to answer questions. They also incorporate variability (the probability
    that measurements taken on the exact quantity at two different times will slightly
    differ) in all their models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example: *was M.F. Hussain a good painter?* One method of answering
    this question measures the paintings based on some accepted norms (by the person
    or community) of the quality of paintings. The answer in such a case may be based
    on creative expression, color usage, form, and shape. *I believe M.F. Hussain
    is a good painter.* In this case, this response can be fairly subjective (which
    means that the response you get from one person can be very different from the
    response you get from another). The statistician''s method of answering this is
    very different. They first collect the data from a sample of people who are considered
    experts in assessing the quality of paintings (university professors of art, other
    artists, art collectors, and more). Then, after analyzing the data, they will
    come up with a conclusion such as: "75% of the university professors of arts,
    83% of the professional artists, and 96% of the art collectors from the data of
    3000 participants of the survey (with equal number of participants from each category)
    opined that Mr. M.F. Hussain is a good painter". Hence, it can be stated that
    he is considered a good painter by most. Very obviously, this is a very objective
    measure.'
  prefs: []
  type: TYPE_NORMAL
- en: Important terms and definitions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the essential parameters and concepts that are used to assess
    and understand the data. They are explained as definitions in some cases and with
    examples and formulae in others. They are classified as "vocabulary" and "statistical
    quantities". You will come across some of these terms in the next sections of
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Term | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Population** | This is the universe of data. Typically, statisticians want
    to make a prediction about a group of objects (Indians, galaxies, countries, and
    more). All the members of the group are called the population. |'
  prefs: []
  type: TYPE_TB
- en: '| **Sample** | Most of the times, it is not feasible to work on the entire
    population. So, statisticians collect a representative sample from the population
    and do all their calculations on them. The subset of the population that is chosen
    for the analysis is called a **sample**. It is always cheaper to compile the sample
    compared to the population or census. There are several techniques to collect
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stratified sampling**: This is defined as the process of dividing the members
    of the population into homogeneous subgroups before sampling. Each subgroup should
    be mutually exclusive, and every element of the population should be assigned
    to a subgroup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster sampling**: This method of sampling ensure n unique clusters where
    each cluster has elements with no repetition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sample size** | This is an obvious dilemma that every statistician has
    been through. How big should be the size of the sample? The bigger the sample,
    the higher will be the accuracy. However, the cost of collection and analysis
    also rise accordingly. So, the challenge is to find an optimum sample size where
    the results are accurate, and the costs are lower. |'
  prefs: []
  type: TYPE_TB
- en: '| **Sampling Bias** | Bias is a systematic error that impacts the outcome in
    some way. Sampling bias is a consistent error that arises due to the sample selection.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Variable** | It is one of the measurements of the sample or population.
    If we are taking all the members of a class, then their age, academic background,
    gender, height, and so on, become the variables. Some variables are independent.
    This means they do not depend on any other variable. Some are dependent. |'
  prefs: []
  type: TYPE_TB
- en: '| **Randomness** | An event is called random if its outcome is uncertain before
    it happens. An example of a random event is the value of the price of gold tomorrow
    afternoon at 1 P.M. |'
  prefs: []
  type: TYPE_TB
- en: '| **Mean** | It is equal to the sum of all the values in the sample divided
    by the total number of observations in the sample. |'
  prefs: []
  type: TYPE_TB
- en: '| **Median** | Median is a midpoint value between the lowest and highest value
    of a data set. This is also called the second quartile (designated Q2) = cuts
    data set in half = 50^(th) percentile. If there is no exact midpoint (that is,
    the observations in the sample are even), then the median is the average of the
    two points in the middle. |'
  prefs: []
  type: TYPE_TB
- en: '| **Mode** | This is the most frequently occurring value of the variable. A
    data can be unimodal (single mode), or multimodal (frequent multiple values).
    If the data obeys normal distribution (about which you will learn later), the
    mode is obtained using the empirical formula:*mean – mode = 3 x (mean - median)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Standard deviation** | It is an average measure of how much each measurement
    in the sample deviates from the mean. Standard deviation is also called the standard
    deviation of the mean.![Important terms and definitions](img/B03980_09_15.jpg)
    |'
  prefs: []
  type: TYPE_TB
- en: Probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we start understanding the probability, let's first look at why we need
    to consider uncertainty in the first place. Any real-life action is always associated
    with the uncertainty of the result or the outcome. Let's take some examples; will
    I be able to catch the train on time today? Will the sales of our top-selling
    product continue to be in the top position this quarter? If I toss a coin, will
    I get a heads or tails? Will I be able to go to the airport in *t* minutes?
  prefs: []
  type: TYPE_NORMAL
- en: 'There can be many sources of uncertainty:'
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty due to lack of knowledge, as a result of insufficient data, incomplete
    analysis, and inaccurate measurements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, uncertainty can also be due to complexity, as a result of incomplete
    processing conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the real world, we need to use probabilities and uncertainties to summarize
    our lack of knowledge and ability to predict an outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Let's elaborate on the last previous example.
  prefs: []
  type: TYPE_NORMAL
- en: Can I go to the airport in 25 minutes? There could be many problems, such as
    incomplete observations on the road conditions, noisy sensors (traffic reports),
    or uncertainty in action, say a flat tire or complexity in modeling the traffic.
    To predict the outcome, there should definitely be some assumptions made, and
    we need to deal with uncertainty in a principled way; this is called **probability**.
    In short, probability is a study of randomness and uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: In probability, an experiment is something that can be repeated and has uncertainty
    in the result. A single outcome of an experiment is referred to as a single event,
    and an event is a collection of outcomes. A **sample space** probability is a
    list of all the possible outcomes of an experiment.
  prefs: []
  type: TYPE_NORMAL
- en: The probability of the event *E* is represented as *P(E)* and is defined as
    the likelihood of this event occurring.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Probability of an Event P(E) = the number of ways an event can happen /
    the number of possible outcomes
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, for a coin that is tossed, there are two possibilities: heads
    or tails.'
  prefs: []
  type: TYPE_NORMAL
- en: The probability of heads is *P(H) = ½ = 0.5*
  prefs: []
  type: TYPE_NORMAL
- en: When a dice is thrown, there are six possibilities, which are 1, 2, 3, 4, 5,
    and 6.
  prefs: []
  type: TYPE_NORMAL
- en: The probability of 1 is *P(1) = 1/6 = 0.16667*
  prefs: []
  type: TYPE_NORMAL
- en: The probability of rolling any event, *E*, *P(E)*, must be between *0* and *1*
    (inclusive).
  prefs: []
  type: TYPE_NORMAL
- en: 0 ≤ P(E) ≤ 1
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of *0* for probability indicates that an event is impossible, and
    the value of *1* indicates the certainty of the event. If there are *n* events,
    then the summation of the probability of each event is *1*. This can be represented
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: If *S = {e1, e2, ….en}* then *P(e1) +P(e2)+…P(en) = 1*
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways to determine the probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classical method**: This is the method that we used to define probability
    in the previous section. This method requires equally likely outcomes. So, if
    an experiment has equally likely *n* events and there are *m* possibilities, the
    event *E* can then occur.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(E) = the number of ways the event E can occur / the number of possible outcomes
    = m/n.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For example, a bag of chocolates contains five brown covered chocolates, six
    yellow covered chocolates, two red covered chocolates, eight orange covered chocolates,
    two blue covered chocolates, and seven green covered chocolates. Suppose that
    a candy is randomly selected. What is the probability of a candy being brown?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P (B) = 5/30*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Empirical method**: The empirical method of probability computation is also
    called relative frequency, as this formula requires the number of times an experiment
    is repeated. This method defines the probability of the event *E*, which is the
    number of times an event is observed over the total number of times the experiment
    is repeated. The basis on which the probability is computed in this case is either
    observations or experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(E) = Frequency of E / the number of trials of the experiment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For example, we want to compute the probability of a grad student to pick medicine
    as their major. We pick, let''s say, a sample of 200 students and 55 of them pick
    medicine as majors, then:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(someone picking medicine) = 55/200 = 0.275*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Subjective method**: This method of probability uses some fair and computed,
    or educated assumptions. It usually describes an individual''s perception of the
    likelihood of an event to occur. This means the individual''s degree of belief
    in the event is considered, and thus can be biased. For example, there is a 40%
    probability that the physics professor would not turn up to take the class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of events
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Events can be mutually exclusive, independent, or dependent in nature.
  prefs: []
  type: TYPE_NORMAL
- en: Mutually exclusive or disjoint events
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Mutually exclusive events are the events that cannot happen at the same time.
    In short, the probability of the two events occurring at the same time is *0*.
    *P(1)* and *P(5)*. When a dice is rolled, there are mutually exclusive events.
    A Venn diagram representation of mutually exclusive events is depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mutually exclusive or disjoint events](img/B03980_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For mutually exclusive events A and B the Addition rule is:'
  prefs: []
  type: TYPE_NORMAL
- en: P(A or B) = P(A) + P(B)
  prefs: []
  type: TYPE_NORMAL
- en: 'For mutually exclusive events A and B the Multiplication rule is:'
  prefs: []
  type: TYPE_NORMAL
- en: P(A and B) = P(A) x P(B)
  prefs: []
  type: TYPE_NORMAL
- en: Independent events
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If the outcome of one event does not impact the outcome of another event, the
    two events are called independent events. For example, event A is that it rained
    on Sunday, and event B is the car having a flat tire. These two events are not
    related and the probability of one does not impact the other. An independent event
    can be mutually exclusive but not vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplication rule in the case of independent events A and B is:'
  prefs: []
  type: TYPE_NORMAL
- en: P(A and B) = P(A) x P(B)
  prefs: []
  type: TYPE_NORMAL
- en: Dependent events
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Dependent events are the events where the occurrence of one event can influence
    the occurrence of another event. For example, a student who takes English as their
    first major can take political science as the second major. The Venn representation
    of dependent events is depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dependent events](img/B03980_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Addition rule for dependent event A and B is:'
  prefs: []
  type: TYPE_NORMAL
- en: P(A or B) = P(A) + P(B) – P(A and B)
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplication rule for dependent event A and B is:'
  prefs: []
  type: TYPE_NORMAL
- en: P(A and B) = P(A) x P(B)
  prefs: []
  type: TYPE_NORMAL
- en: Types of probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will take a look at the different types of probabilities,
    which are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prior and posterior probability**: Prior probability is the probability that
    an event E occurs without any prior information or knowledge of any assumptions
    in the problem context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's take an example. If your friend was travelling by air and you were asked
    if they have a man or a woman as their neighbor, as the basis formula of probability
    works, there is a 0.5 (50%) probability that it can be a man and a 0.5 (50%) probability
    that it can be a woman. These values can change when more information is provided,
    and the probability that is measured then is called the posterior probability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Conditional probability**: Conditional probability is defined as the probability
    that an event occurs, given another event already occurred. *P(B|A)* is interpreted
    as the probability of event B, given event A.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, let's compute the probability that a person will be hit by a car
    while walking on the road. Let *H* be a discrete random variable describing the
    probability of a person being hit by a car, taking the hit as 1 and not as 0.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let *L* be a discrete random variable describing the probability of the cross
    traffic''s traffic light state at a given moment, taking one from *{red, yellow,
    green}*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(L=red) = 0.7,*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(L=yellow) = 0.1,*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(L=green) = 0.2.*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(H=1|L=R) = 0.99,*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(H|L=Y) = 0.9 and*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(H|L=G) = 0.2.*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using the conditional probability formulae, we get the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(H=1 and L=R) = P(L=R)*P(H|L=R) = 0.693;*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(H=1 and L=Y) = 0.1*0.9 = 0.09*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similarly, if the probability of getting hit while red is on is 0.99, the probability
    of not getting hit is 0.01\. So, *P(H=0|L=R) = 0.01*. From these, we can compute
    the probability of *H=0* and *L=R*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Joint probability**: Joint probability is the probability of two or more
    things happening together. In a two variable case, *f(x,y|θ)* is the joint probability
    distribution, where *f* is the probability of *x* and *y* together as a pair,
    given the distribution parameters—*θ*. For discrete random variables, the joint
    probability mass function is:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(X and Y) = P(X).P(Y|X) =P(Y).P(X|Y)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You already saw this while studying the conditional probability. Since these
    are probabilities, we have the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Types of probability](img/B03980_09_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Marginal probability**: Marginal probability is represented by *f(x|θ)* where
    *f* is the probability density of *x* for all the possible values of *y*, given
    the distribution parameters—*θ*. The marginal probability in a random distribution
    is determined from the joint distribution of *x* and *y* by summing over all the
    values of *y*. In a continuous distribution, it is determined by integrating over
    all the values of *y*. This is called **integrating out** the variable *y*. For
    discrete random variables, the marginal probability mass function can be written
    as *P(X = x)*. This is as follows:![Types of probability](img/B03980_09_17.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the above equation, *P(X = x,Y = y)* is the joint distribution of *X* and
    *Y*, and *P(X = x|Y = y)* is the conditional distribution of *X*, given *Y*. The
    variable *Y* is marginalized out. These bivariate marginal and joint probabilities
    for discrete random variables are often displayed as two-way tables (as illustrated
    next). We will show the computations in a worked out problem in the next section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For example, suppose two dices are rolled, and the sequence of scores *(X1,
    X2)* is recorded. Let *Y=X1+X2* and *Z=X1−X2* denote the sum and difference of
    the scores respectively. Find the probability density function of *(Y, Z)*. Find
    the probability density function of *Y*. Find the probability density function
    of *Z*. Are *Y* and *Z* independent?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Assuming that *X1* and *X2* are independent, they can take 36 possibilities,
    as shown in the table here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Types of probability](img/B03980_09_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Let''s now construct the joint, marginal, and conditional table. In this, we
    will have values of *Z* as rows and *Y* as columns. *Y* varies from 2 to 12 and
    *Z* varies from -5 to 5\. We can fill all the conditional distributions just by
    counting. For example, take *Z=-1*; we see that this happens when *Y=3, 5, 7,
    9, 11*. We also note that the probability of each one of them (say, the conditional
    probability that *Z=-1*, given *Y=3*) is *1/36*. We can fill the table like this
    for all the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Types of probability](img/B03980_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, the bottom row is the marginal distribution of *Y*. The right-most column
    is the marginal distribution of *Z*. The total table is the joint distribution.
    Clearly, they are dependent.
  prefs: []
  type: TYPE_NORMAL
- en: Distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Distributions are either discrete or continuous probability distributions,
    depending on whether they define probabilities associated with discrete variables
    or continuous variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution](img/B03980_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will cover a few of the previously mentioned distributions here.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, our major emphasis is on modeling and describing a given property
    of the data. To understand how crucial this skill is, let''s look at a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: A bank wants to look at the amount of cash withdrawn per transaction in an ATM
    machine over a period of time to determine the limits of transaction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A retailer wants to understand the number of broken toys that he is getting
    in every shipment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A manufacturer wants to understand how the diameter of a probe is varying between
    various manufacturing cycles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pharmaceutical company wants to understand how the blood pressures of millions
    of patients are impacted by its new drug
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all these cases, we need to come up with some precise quantitative description
    of how the observed quantity is behaving. This section is all about this. Anyway,
    intuitively, what do you think are the qualities that you would like to measure
    to gain an understanding?
  prefs: []
  type: TYPE_NORMAL
- en: What are all the values that a given variable is taking?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the probability of taking a given value and what values have the highest
    probability?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the mean/median, and how much is the variance?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a value, can we tell how many observations fall into it and how many fall
    away from it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we give a range of values where we can tell 90% of the data lies?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actually, if we can answer these questions, and more importantly if we develop
    a technique to describe such quantities, we are more or less unstoppable as far
    as this property is considered!
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two prime observations to be made here. First, a property when distributed
    the way it is has all the qualities it takes to be a random variable (knowing
    one value of the quantity does not help us know the next value). Then, if we know
    the probability mass function or the distribution function of this random variable,
    we can compute all the previous matter. This is why it is so important to understand
    mathematics. In general, we follow (for that matter, almost anybody interested
    in analyzing the data that follows) a systematic process in describing a quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: We will first understand the random variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will compute the probability mass (or distribution) function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we will predict the all-important parameters (mean and variance).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we will check with experimental data to see how good our approximations
    are.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, the number of vans that have been requested for rental at a car
    rental agency during a 50-day period is identified in the following table. The
    observed frequencies have been converted into probabilities for this 50-day period
    in the last column of the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution](img/B03980_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The expected value is 5.66 vans, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution](img/B03980_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, variance computation is given next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution](img/B03980_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The standard deviation is a square root of variance and is equal to 1.32 vans.
    Let's systematically analyze various distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Bernoulli distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the simplest distribution that one can think of. Many a times, a property
    takes only discrete values; like a coin toss, a roll of the dice, the gender of
    people, and so on. Even if they are not exactly discrete, we can transform them
    by binning in some cases. For example, when we look at the net worth of individuals,
    we can redivide them as rich and poor (**discrete quantity**) based on the exact
    wealth they have (**continuous quantity**). Let's say that the probability of
    the property taking a given value is *p* (of course, the probability of it not
    taking is *(1-p)*). If we collect the large sample sufficiently, then how does
    the dataset look? Well, there will be some positives (where the variable took
    the value) and negatives (where the variable does not take the value). Assume
    that we denote positive with 1 and negative with 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The mean = weighted average of probabilities = 1*p +0*(1-p) = p
  prefs: []
  type: TYPE_NORMAL
- en: Binomial distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is an extension of the Bernoulli idea. Let''s take a specific example.
    You are working in a population bureau and have the data of all the families in
    a state. Let''s say you want to identify the probability of having two male children
    in families that have exactly two children. As you can see, a family can have
    two children in only four different ways: MM, MF, FM, and FF. As we consider having
    a male child as the event of interest, then the probability that there are only
    male children is *0.25 (1/4)*. The probability of there being one male child is
    *0.5 (0.25+0.25) (1/4+1/4),* and no male child is *0.25 (1/4)*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if you look at 100 families, what is the probability that 20 families have
    exactly two male children? We will come to the solution later. Let''s extend the
    argument to find the probability of having all the males in families with three
    children: The total possibilities are FFF, FFM, FMF, FMM, MFM, MMF, MFF, and MMM
    (eight total possibilities). The probability for all three to be male is *1/8*.
    The probability for two of the three being male is *3/8*. The probability for
    one of the three to be male is *3/8*. The probability for none to be male is *1/8*.
    Note that the total probability of all the events is always equal to 1.'
  prefs: []
  type: TYPE_NORMAL
- en: Poisson probability distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s try to extend the Binomial theorem to infinite trials, but with
    a catch. The examples that we have taken (coin toss and more) have an interesting
    property. The probability of the event occurring in a trial does not change even
    if we increase the number of trials. However, there are a great number of examples,
    whereas the number of trials (or its equivalent) increases, the corresponding
    probability of the event decreases. So, we need to reduce the time interval to
    zero, or the number of observations to infinity to ensure that we see only a single
    success or failure in any trial. In this limiting case, the probability that we
    see *r* successes in *n* observations can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Poisson probability distribution](img/B03980_09_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The probability distribution of a Poisson random variable *X* is as given below.
    This considers representing the number of successes occurring in a given time
    interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Poisson probability distribution](img/B03980_09_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *r* is the *r*^(th) trial and *λ* = a mean number of successes in the
    given time interval or the region of space.
  prefs: []
  type: TYPE_NORMAL
- en: Exponential distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s now look at the Poisson example and ask ourselves a different question.
    What is the probability that the inspector does not see the first car until *t*
    hours? In this case, it may not be relevant, but when we work on the failure of
    a component, it makes sense to understand what time the probability of not seeing
    the failure is high. So, let''s say the sighting of the car (or first failure)
    follows the Poisson process. Then, let''s define *L*, a random variable that is
    the probability that the inspector will not see the first car until time *t* as
    the time before the first sighting of the car. From the Poisson distribution,
    the probability that she will not see the first car in 1 hour is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exponential distribution](img/B03980_09_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The probability that that she will not see a car in the second hour also is
    the same, and the probability that she will not see the car in *t* hours is *e*^(−λt)
    *(e*^(−λ) * *e*^(−λ) **…times)*. The probability that she will see the car in
    the first *t* hours then is *1-e*^(-λt).
  prefs: []
  type: TYPE_NORMAL
- en: 'The applications of exponential distribution are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Time to the first failure in a Poisson process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance of the dispersion of seeds from the parent plant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The expected lifetime of an organism, ignoring the aging process (where the
    end occurs due to accidents, infections, and more)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normal distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Normal distribution is a very widely used class of continuous distribution.
    It is also often called the bell curve because the graph of its probability density
    resembles a bell. Most of the real-life data such as weights, heights, and more
    (particularly when there are large collections) can be well approximated by a
    normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Normal distribution](img/B03980_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we know the values of the heights, the number of samples that have this
    value can be mathematically described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Normal distribution](img/B03980_09_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, σ is the standard deviation and µ is the mean. To describe a normal distribution,
    we just need to know two concepts (average and SD).
  prefs: []
  type: TYPE_NORMAL
- en: 'Every normal curve adheres to the following *rule*:'
  prefs: []
  type: TYPE_NORMAL
- en: About 68% of the area under the curve falls within one standard deviation of
    the mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About 95% of the area under the curve falls within two standard deviations of
    the mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About 99.7% of the area under the curve falls within three standard deviations
    of the mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collectively, these points are known as the **empirical rule** or the **68-95-99.7
    rule**.
  prefs: []
  type: TYPE_NORMAL
- en: Relationship between the distributions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While we know that more or less everything converges to a normal distribution,
    it is best to understand where each one fits. The following chart helps in this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Relationship between the distributions](img/B03980_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Bayes' theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we go into the Bayes' theorem, we mentioned at the beginning of this
    chapter what is at the Bayesian learning is the Bayes theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with an example. Assume that there are two bowls of nuts; the first
    bowl contains 30 cashew nuts and 10 pistachios and the second bowl contains 20
    of each. Let's choose one bowl randomly and pick a nut with eyes closed. The nut
    is cashew. Now, what is the probability that the bowl chosen is the first bowl?
    This is a conditional probability.
  prefs: []
  type: TYPE_NORMAL
- en: So, *p(Bowl 1|cashew)* or the probability that it is bowl 1, given the nut is
    cashew, is not an easy and obvious one to crack.
  prefs: []
  type: TYPE_NORMAL
- en: If the question was to put the other way, *p(cashew|bowl1)* or the probability
    that the nut is cashew, given bowl 1 is easy, *p(cashew|Bowl 1) = ¾*.
  prefs: []
  type: TYPE_NORMAL
- en: As we know, *p(cashew|Bowl 1)* is not the same as *p(Bowl 1|cashew),* but we
    can use one value to get another value, and this is what Bayes' theorem is all
    about.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step of defining the Bayes'' theorem conjunction is commutative;
    following are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (A and B) =p (B and A),*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Further, the probability of A and B is the probability of A and the probability
    of B, given A:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (A and B) = p (A) p (B|A)*, similarly'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (B and A) = p (B) p (A|B)*'
  prefs: []
  type: TYPE_NORMAL
- en: so,
  prefs: []
  type: TYPE_NORMAL
- en: '*p (A) p (B|A) = p (B) p (A|B)* and'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayes'' theorem](img/B03980_09_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: And that's Bayes' theorem!
  prefs: []
  type: TYPE_NORMAL
- en: It might not be very obvious, but it is a very powerful definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s apply this to solve the previous *nut* problem to find *p(bowl1
    cashew)*, and we can derive it if we can get *p(cashew|bowl 1)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (bowl1 cashew) = (p(bowl1) p(cashew|bowl1)) / p (cashew)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (bowl1) = ½*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (cashew|bowl1) = ¾*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (cashew) = total cashews / total nuts (between bowl1 and bowl2) = 50/80
    = 5/8*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting it together, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (bowl1 cashew) = ((1/2) (3/4))/(5/8)= 3/5 = 0.6*'
  prefs: []
  type: TYPE_NORMAL
- en: The additional aspect that needs to be considered now is how to feature in the
    changes that come over time as the new data comes in. This way, the probability
    of a hypothesis can be measured in the context of the data at a given point in
    time. This is called the diachronic interpretation of the Bayes' theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the restating Bayes'' theorem with the hypothesis (*H*) for the
    given data (*D*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayes'' theorem](img/B03980_09_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*p (H)* is the probability of the hypothesis *H* before seeing the data *D*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (D)* is the probability of data *D* under any hypothesis, which is usually
    constant.'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (H|D)* is the probability of the hypothesis *H* after seeing the data *D*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (D|H)* is the probability of data *D* given the hypothesis *H*.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*p (H)* is called prior probability; *p (H|D)* is posterior probability; *p
    (D|H)* is the likelihood; and *p (D)* is the evidence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayes'' theorem](img/B03980_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Naïve Bayes classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look at the Naïve Bayes classifiers and how they are
    used to solve the classification problems. The Naïve Bayes classifier technique
    is based on the Bayes' theorem and assumes the predictors to be independent, which
    means knowing the value of one attribute does influence the value of any other
    attribute. The independence assumption is what makes Naïve Bayes *naïve*.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes classifiers are easy to build, do not involve any iterative process,
    and work very well with large datasets. Despite its simplicity, Naïve Bayes is
    known to have often outperformed other classification methods.
  prefs: []
  type: TYPE_NORMAL
- en: We need to compute the probability of an assumption given a class.
  prefs: []
  type: TYPE_NORMAL
- en: That is, *P(x*[1]*, x*[2]*, ….x*[n|y]*)*. Obviously, there are multiple pieces
    of evidence represented by *x*[1], *x*[2], *….x*[n].
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we start with an assumption that *x*[1], *x*[2], *….x*[n] are conditionally
    independent, given *y*. Another simple way of defining this is that we need to
    predict an outcome given multiple evidence as against a single evidence. To simplify,
    we uncouple these multiple pieces of evidence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Outcome|Multiple Evidence) = [P(Evidence1|Outcome) x P(Evidence2|outcome)
    x ... x P(EvidenceN|outcome)] x P(Outcome) / P(Multiple Evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Outcome|Evidence) = P(Likelihood of Evidence) x Prior probability of outcome
    / P(Evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: In order to apply Naïve Bayes to predict an outcome, the previously mentioned
    formula will need to be run for every outcome. Just run this formula for each
    possible outcome, and in the case of a classification problem, the outcome will
    be a class. We will look at the famous fruit problem to help you understand this
    easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given any three important characteristics of a fruit, we will need to predict
    what fruit it is. To simplify the case, let''s take three attributes—long, sweet,
    and yellow; and three classes of fruit—banana, orange, and others. Let there be
    1,000 data points in the training set, and this is how the available information
    looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Long | Not long | Sweet | Not sweet | Yellow | Not yellow | Total
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Banana** | 400 | 100 | 350 | 150 | 450 | 50 | 500 |'
  prefs: []
  type: TYPE_TB
- en: '| **Orange** | 0 | 300 | 150 | 150 | 300 | 0 | 300 |'
  prefs: []
  type: TYPE_TB
- en: '| **Others** | 100 | 100 | 150 | 50 | 50 | 150 | 200 |'
  prefs: []
  type: TYPE_TB
- en: '| **Total** | 500 | 500 | 650 | 350 | 800 | 200 | 1000 |'
  prefs: []
  type: TYPE_TB
- en: 'Some derived values/prior probabilities from the previous table are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Probability of Class
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Banana)= 0.5 (500/1000)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Orange)= 0.3*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Others) = 0.2*'
  prefs: []
  type: TYPE_NORMAL
- en: Probability of Evidence
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Long)= 0.5*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Sweet)= 0.65*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Yellow) = 0.8*'
  prefs: []
  type: TYPE_NORMAL
- en: Probability of Likelihood
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Long|Banana) = 0.8*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Long/Orange) = 0 P(Yellow/Other Fruit) =50/200 = 0.25*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Not Yellow|Other Fruit)= 0.75*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, given a fruit, let''s classify it based on attributes. First, we run probability
    for each of the three outcomes, take the highest probability, and then classify
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Banana|/Long, Sweet and Yellow) = p (Long|Banana) x p (Sweet|Banana) x
    p (Yellow|Banana) x p (banana) /p (Long) xp (Sweet) x. p (Yellow)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Banana||Long, Sweet and Yellow) =0.8 x 0.7 x 0.9 x 0.5 / p (evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Banana||Long, Sweet and Yellow) =0.252/ p (evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Orange||Long, Sweet and Yellow) = 0*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Other Fruit/Long, Sweet and Yellow) = p (Long/Other fruit) x p (Sweet/Other
    fruit) x p (Yellow/Other fruit) x p (Other Fruit)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= (100/200 x 150/200 x 50/150 x 200/1000) / p (evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.01875/ p (evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: With the largest margin of *0.252 >> 0.01875*, we can now classify this Sweet/Long/Yellow
    fruit as likely to be a *Banana*.
  prefs: []
  type: TYPE_NORMAL
- en: As Naïve Bayes assumes a gaussian distribution for each of the features, it
    is also called the Gaussian Naïve Bayes classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes is particularly good when there is missing data. In the next sections,
    let's look at different types of Naïve Bayes classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Multinomial Naïve Bayes classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we have seen in the previous section, Naïve Bayes assumes independence of
    the model against the distribution for a feature. In the case of a multinomial
    Naïve Bayes, the *p(x*[i]*|y)* is a multinomial distribution; in short, a multinomial
    distribution is assumed for each of the features. The case that fits this variant
    is that of a document where we need to compute the word count. A simple algorithm
    of multinomial Naïve Bayes is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multinomial Naïve Bayes classifier](img/B03980_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Bernoulli Naïve Bayes classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Bernoulli Naïve Bayes classifier attaches a Boolean indicator to a word
    as one if it belongs to a document under examination and zero if it does not.
    The focus of this variation is that it considers the count of occurrence or non-occurrence
    of a word in a specific document under consideration. The non-occurrence of a
    word is an important value as it is used in the computation of the conditional
    probabilities of the occurrence of a word. The Bernoulli Naïve Bayes algorithm
    is detailed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Bernoulli Naïve Bayes classifier](img/B03980_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '|   | Multinomial Naïve Bayes | Bernoulli Naïve Bayes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Model Variable** | Here, a token is generated andchecked for occurrence
    in a position | Here, a document is generatedand checked for occurrence in a document
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Document** | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_23.jpg)
    | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_24.jpg)![The Bernoulli
    Naïve Bayes classifier](img/B03980_09_25.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| **Estimation of the parameter** | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_26.jpg)
    | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_27.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| **Rule** | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_28.jpg)
    | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_29.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| **Occurrences** | This considers multiple occurrences | This considers single
    occurrences |'
  prefs: []
  type: TYPE_TB
- en: '| **Size of the document** | Large documents are handled | Good with smaller
    documents |'
  prefs: []
  type: TYPE_TB
- en: '| **Features** | This supports handling more features | This is good with lesser
    features |'
  prefs: []
  type: TYPE_TB
- en: '| **Estimation of a term** | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_30.jpg)
    | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_31.jpg) |'
  prefs: []
  type: TYPE_TB
- en: Implementing Naïve Bayes algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the source code provided for this chapter for implementing Naïve Bayes
    classifier (source code path `.../chapter9/...` under each of the folders for
    the technology).
  prefs: []
  type: TYPE_NORMAL
- en: Using Mahout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../mahout/chapter9/naivebayesexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../r/chapter9/naivebayesexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../spark/chapter9/naivebayesexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../python-scikit-learn/chapter9/naivebayesexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Julia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../julia/chapter9/naivebayesexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned Bayesian Machine learning and how to implement
    Naïve Bayes classifiers association rule-based learning with Mahout, R, Python,
    Julia, and Spark. Additionally, we covered all the core concepts of statistics,
    starting from basic nomenclature to various distributions. We have covered the
    Bayes' theorem in depth with examples to understand how to apply it to the real-world
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be covering the regression-based learning techniques
    and in specific, the implementation for linear and logistic regression.
  prefs: []
  type: TYPE_NORMAL
