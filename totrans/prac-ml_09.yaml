- en: Chapter 9. Bayesian learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 贝叶斯学习
- en: In this chapter, we will go back to covering an important, statistical-based
    method of learning called the Bayesian method learning, and in particular, the
    Naïve Bayes algorithm among others. The statistical models generally have an explicit
    probability model, which reveals the probability of an instance belonging to a
    particular class rather than just classification while solving a classification
    problem. Before taking a deep dive into the Bayesian learning, you will learn
    some important concepts under statistics such as probability distribution and
    the Bayes theorem which is the heart of Bayesian learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回顾一个重要的基于统计的学习方法，称为贝叶斯学习方法，特别是其中的朴素贝叶斯算法等。统计模型通常具有一个明确的概率模型，它揭示了实例属于特定类别的概率，而不仅仅是分类，在解决分类问题时。在深入探讨贝叶斯学习之前，你将学习一些统计学中的重要概念，如概率分布和贝叶斯定理，这是贝叶斯学习的核心。
- en: Bayesian learning is a supervised learning technique where the goal is to build
    a model of the distribution of class labels that have a concrete definition of
    the target attribute. Naïve Bayes is based on applying Bayes' theorem with the
    *naïve* assumption of independence between each and every pair of features.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯学习是一种监督学习技术，其目标是构建一个具有明确目标属性定义的类别标签分布模型。朴素贝叶斯基于应用贝叶斯定理，并假设每个特征对之间都是**朴素**的独立性。
- en: You will learn the basics and advanced concepts of this technique and get hands-on
    implementation guidance in using Apache Mahout, R, Julia, Apache Spark, and Python
    to implement the means - clustering algorithm.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习该技术的基本和高级概念，并获得使用Apache Mahout、R、Julia、Apache Spark和Python实现均值聚类算法的实践指导。
- en: 'The following figure depicts different learning models covered in this book,
    and the technique highlighted will be dealt with in detail in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了本书中涵盖的不同学习模型，本章将详细处理突出显示的技术：
- en: '![Bayesian learning](img/B03980_09_01.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯学习](img/B03980_09_01.jpg)'
- en: 'The topics listed here are covered in depth in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将深入探讨以下主题：
- en: An overview of Bayesian statistics and core principles or concepts of probability,
    distribution, and other relevant statistical measures
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯统计概述及其核心原理或概念，如概率、分布以及其他相关统计度量
- en: Bayes' theorem and its mechanics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯定理及其机制
- en: Deep dive into the Naïve Bayes algorithm and variations of Naïve Bayes classifiers
    like multinomial and Bernoulli classifiers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入探讨朴素贝叶斯算法及其变体，如多项式和伯努利分类器
- en: A detailed explanation of some real-world problems or use cases that the Bayesian
    learning technique can address
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对贝叶斯学习技术可以解决的某些现实世界问题或用例的详细解释
- en: Sample implementation using Apache Mahout, R, Apache Spark, Julia, and Python
    (scikit-learn) libraries and modules
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Mahout、R、Apache Spark、Julia和Python（scikit-learn）库和模块的示例实现
- en: Bayesian learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯学习
- en: Under supervised learning techniques, the learning models that are categorized
    under statistical methods are instance-based learning methods and the Bayesian
    learning method. Before we understand the Bayesian learning method, we will first
    cover an overview of concepts of probabilistic modeling and Bayesian statistics
    that are relevant in the context of Machine learning. The core concepts of statistics
    are very deep, and what will be covered in the next few sections is primarily
    focused on equipping you with a basic understanding of the dynamic and diverse
    field of probabilistic Machine learning, which is sufficient to interpret the
    functioning of the Bayesian learning methods.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习技术中，归类于统计方法的模型包括基于实例的学习方法和贝叶斯学习方法。在我们理解贝叶斯学习方法之前，我们将首先概述与机器学习相关的概率建模和贝叶斯统计概念。统计学的基本概念非常深入，接下来几节将主要关注为你提供概率机器学习动态和多样性的基本理解，这对于解释贝叶斯学习方法的运作是足够的。
- en: Statistician's thinking
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计学家的思维方式
- en: The objective of statisticians is to answer questions asked by people from various
    domains using data. The typical engineering methods use some subjective/objective
    methods that do not require data to answer the questions. But, statisticians always
    look at the data to answer questions. They also incorporate variability (the probability
    that measurements taken on the exact quantity at two different times will slightly
    differ) in all their models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example: *was M.F. Hussain a good painter?* One method of answering
    this question measures the paintings based on some accepted norms (by the person
    or community) of the quality of paintings. The answer in such a case may be based
    on creative expression, color usage, form, and shape. *I believe M.F. Hussain
    is a good painter.* In this case, this response can be fairly subjective (which
    means that the response you get from one person can be very different from the
    response you get from another). The statistician''s method of answering this is
    very different. They first collect the data from a sample of people who are considered
    experts in assessing the quality of paintings (university professors of art, other
    artists, art collectors, and more). Then, after analyzing the data, they will
    come up with a conclusion such as: "75% of the university professors of arts,
    83% of the professional artists, and 96% of the art collectors from the data of
    3000 participants of the survey (with equal number of participants from each category)
    opined that Mr. M.F. Hussain is a good painter". Hence, it can be stated that
    he is considered a good painter by most. Very obviously, this is a very objective
    measure.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Important terms and definitions
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the essential parameters and concepts that are used to assess
    and understand the data. They are explained as definitions in some cases and with
    examples and formulae in others. They are classified as "vocabulary" and "statistical
    quantities". You will come across some of these terms in the next sections of
    this chapter:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '| Term | Definition |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
- en: '| **Population** | This is the universe of data. Typically, statisticians want
    to make a prediction about a group of objects (Indians, galaxies, countries, and
    more). All the members of the group are called the population. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
- en: '| **Sample** | Most of the times, it is not feasible to work on the entire
    population. So, statisticians collect a representative sample from the population
    and do all their calculations on them. The subset of the population that is chosen
    for the analysis is called a **sample**. It is always cheaper to compile the sample
    compared to the population or census. There are several techniques to collect
    samples:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '**Stratified sampling**: This is defined as the process of dividing the members
    of the population into homogeneous subgroups before sampling. Each subgroup should
    be mutually exclusive, and every element of the population should be assigned
    to a subgroup.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster sampling**: This method of sampling ensure n unique clusters where
    each cluster has elements with no repetition.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sample size** | This is an obvious dilemma that every statistician has
    been through. How big should be the size of the sample? The bigger the sample,
    the higher will be the accuracy. However, the cost of collection and analysis
    also rise accordingly. So, the challenge is to find an optimum sample size where
    the results are accurate, and the costs are lower. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| **Sampling Bias** | Bias is a systematic error that impacts the outcome in
    some way. Sampling bias is a consistent error that arises due to the sample selection.
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| **Variable** | It is one of the measurements of the sample or population.
    If we are taking all the members of a class, then their age, academic background,
    gender, height, and so on, become the variables. Some variables are independent.
    This means they do not depend on any other variable. Some are dependent. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| **Randomness** | An event is called random if its outcome is uncertain before
    it happens. An example of a random event is the value of the price of gold tomorrow
    afternoon at 1 P.M. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| **Mean** | It is equal to the sum of all the values in the sample divided
    by the total number of observations in the sample. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| **Median** | Median is a midpoint value between the lowest and highest value
    of a data set. This is also called the second quartile (designated Q2) = cuts
    data set in half = 50^(th) percentile. If there is no exact midpoint (that is,
    the observations in the sample are even), then the median is the average of the
    two points in the middle. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| **Mode** | This is the most frequently occurring value of the variable. A
    data can be unimodal (single mode), or multimodal (frequent multiple values).
    If the data obeys normal distribution (about which you will learn later), the
    mode is obtained using the empirical formula:*mean – mode = 3 x (mean - median)*
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| **Standard deviation** | It is an average measure of how much each measurement
    in the sample deviates from the mean. Standard deviation is also called the standard
    deviation of the mean.![Important terms and definitions](img/B03980_09_15.jpg)
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: Probability
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we start understanding the probability, let's first look at why we need
    to consider uncertainty in the first place. Any real-life action is always associated
    with the uncertainty of the result or the outcome. Let's take some examples; will
    I be able to catch the train on time today? Will the sales of our top-selling
    product continue to be in the top position this quarter? If I toss a coin, will
    I get a heads or tails? Will I be able to go to the airport in *t* minutes?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'There can be many sources of uncertainty:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty due to lack of knowledge, as a result of insufficient data, incomplete
    analysis, and inaccurate measurements
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, uncertainty can also be due to complexity, as a result of incomplete
    processing conditions
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the real world, we need to use probabilities and uncertainties to summarize
    our lack of knowledge and ability to predict an outcome.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Let's elaborate on the last previous example.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Can I go to the airport in 25 minutes? There could be many problems, such as
    incomplete observations on the road conditions, noisy sensors (traffic reports),
    or uncertainty in action, say a flat tire or complexity in modeling the traffic.
    To predict the outcome, there should definitely be some assumptions made, and
    we need to deal with uncertainty in a principled way; this is called **probability**.
    In short, probability is a study of randomness and uncertainty.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: In probability, an experiment is something that can be repeated and has uncertainty
    in the result. A single outcome of an experiment is referred to as a single event,
    and an event is a collection of outcomes. A **sample space** probability is a
    list of all the possible outcomes of an experiment.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: The probability of the event *E* is represented as *P(E)* and is defined as
    the likelihood of this event occurring.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Probability of an Event P(E) = the number of ways an event can happen /
    the number of possible outcomes
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, for a coin that is tossed, there are two possibilities: heads
    or tails.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The probability of heads is *P(H) = ½ = 0.5*
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: When a dice is thrown, there are six possibilities, which are 1, 2, 3, 4, 5,
    and 6.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: The probability of 1 is *P(1) = 1/6 = 0.16667*
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: The probability of rolling any event, *E*, *P(E)*, must be between *0* and *1*
    (inclusive).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 0 ≤ P(E) ≤ 1
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of *0* for probability indicates that an event is impossible, and
    the value of *1* indicates the certainty of the event. If there are *n* events,
    then the summation of the probability of each event is *1*. This can be represented
    as:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: If *S = {e1, e2, ….en}* then *P(e1) +P(e2)+…P(en) = 1*
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways to determine the probability:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '**Classical method**: This is the method that we used to define probability
    in the previous section. This method requires equally likely outcomes. So, if
    an experiment has equally likely *n* events and there are *m* possibilities, the
    event *E* can then occur.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(E) = the number of ways the event E can occur / the number of possible outcomes
    = m/n.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For example, a bag of chocolates contains five brown covered chocolates, six
    yellow covered chocolates, two red covered chocolates, eight orange covered chocolates,
    two blue covered chocolates, and seven green covered chocolates. Suppose that
    a candy is randomly selected. What is the probability of a candy being brown?
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P (B) = 5/30*'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Empirical method**: The empirical method of probability computation is also
    called relative frequency, as this formula requires the number of times an experiment
    is repeated. This method defines the probability of the event *E*, which is the
    number of times an event is observed over the total number of times the experiment
    is repeated. The basis on which the probability is computed in this case is either
    observations or experiences.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(E) = Frequency of E / the number of trials of the experiment.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For example, we want to compute the probability of a grad student to pick medicine
    as their major. We pick, let''s say, a sample of 200 students and 55 of them pick
    medicine as majors, then:'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(someone picking medicine) = 55/200 = 0.275*'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Subjective method**: This method of probability uses some fair and computed,
    or educated assumptions. It usually describes an individual''s perception of the
    likelihood of an event to occur. This means the individual''s degree of belief
    in the event is considered, and thus can be biased. For example, there is a 40%
    probability that the physics professor would not turn up to take the class.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of events
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Events can be mutually exclusive, independent, or dependent in nature.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Mutually exclusive or disjoint events
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Mutually exclusive events are the events that cannot happen at the same time.
    In short, the probability of the two events occurring at the same time is *0*.
    *P(1)* and *P(5)*. When a dice is rolled, there are mutually exclusive events.
    A Venn diagram representation of mutually exclusive events is depicted here:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![Mutually exclusive or disjoint events](img/B03980_09_02.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: 'For mutually exclusive events A and B the Addition rule is:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: P(A or B) = P(A) + P(B)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'For mutually exclusive events A and B the Multiplication rule is:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: P(A and B) = P(A) x P(B)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Independent events
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If the outcome of one event does not impact the outcome of another event, the
    two events are called independent events. For example, event A is that it rained
    on Sunday, and event B is the car having a flat tire. These two events are not
    related and the probability of one does not impact the other. An independent event
    can be mutually exclusive but not vice versa.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplication rule in the case of independent events A and B is:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: P(A and B) = P(A) x P(B)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Dependent events
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Dependent events are the events where the occurrence of one event can influence
    the occurrence of another event. For example, a student who takes English as their
    first major can take political science as the second major. The Venn representation
    of dependent events is depicted here:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![Dependent events](img/B03980_09_03.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: 'Addition rule for dependent event A and B is:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: P(A or B) = P(A) + P(B) – P(A and B)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplication rule for dependent event A and B is:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: P(A and B) = P(A) x P(B)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Types of probability
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will take a look at the different types of probabilities,
    which are listed as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '**Prior and posterior probability**: Prior probability is the probability that
    an event E occurs without any prior information or knowledge of any assumptions
    in the problem context.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's take an example. If your friend was travelling by air and you were asked
    if they have a man or a woman as their neighbor, as the basis formula of probability
    works, there is a 0.5 (50%) probability that it can be a man and a 0.5 (50%) probability
    that it can be a woman. These values can change when more information is provided,
    and the probability that is measured then is called the posterior probability.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Conditional probability**: Conditional probability is defined as the probability
    that an event occurs, given another event already occurred. *P(B|A)* is interpreted
    as the probability of event B, given event A.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, let's compute the probability that a person will be hit by a car
    while walking on the road. Let *H* be a discrete random variable describing the
    probability of a person being hit by a car, taking the hit as 1 and not as 0.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let *L* be a discrete random variable describing the probability of the cross
    traffic''s traffic light state at a given moment, taking one from *{red, yellow,
    green}*:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(L=red) = 0.7,*'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(L=yellow) = 0.1,*'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(L=green) = 0.2.*'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(H=1|L=R) = 0.99,*'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(H|L=Y) = 0.9 and*'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(H|L=G) = 0.2.*'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using the conditional probability formulae, we get the following:'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(H=1 and L=R) = P(L=R)*P(H|L=R) = 0.693;*'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*P(H=1 and L=Y) = 0.1*0.9 = 0.09*'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similarly, if the probability of getting hit while red is on is 0.99, the probability
    of not getting hit is 0.01\. So, *P(H=0|L=R) = 0.01*. From these, we can compute
    the probability of *H=0* and *L=R*.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Joint probability**: Joint probability is the probability of two or more
    things happening together. In a two variable case, *f(x,y|θ)* is the joint probability
    distribution, where *f* is the probability of *x* and *y* together as a pair,
    given the distribution parameters—*θ*. For discrete random variables, the joint
    probability mass function is:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(X and Y) = P(X).P(Y|X) =P(Y).P(X|Y)
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You already saw this while studying the conditional probability. Since these
    are probabilities, we have the following:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Types of probability](img/B03980_09_16.jpg)'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Marginal probability**: Marginal probability is represented by *f(x|θ)* where
    *f* is the probability density of *x* for all the possible values of *y*, given
    the distribution parameters—*θ*. The marginal probability in a random distribution
    is determined from the joint distribution of *x* and *y* by summing over all the
    values of *y*. In a continuous distribution, it is determined by integrating over
    all the values of *y*. This is called **integrating out** the variable *y*. For
    discrete random variables, the marginal probability mass function can be written
    as *P(X = x)*. This is as follows:![Types of probability](img/B03980_09_17.jpg)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the above equation, *P(X = x,Y = y)* is the joint distribution of *X* and
    *Y*, and *P(X = x|Y = y)* is the conditional distribution of *X*, given *Y*. The
    variable *Y* is marginalized out. These bivariate marginal and joint probabilities
    for discrete random variables are often displayed as two-way tables (as illustrated
    next). We will show the computations in a worked out problem in the next section.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For example, suppose two dices are rolled, and the sequence of scores *(X1,
    X2)* is recorded. Let *Y=X1+X2* and *Z=X1−X2* denote the sum and difference of
    the scores respectively. Find the probability density function of *(Y, Z)*. Find
    the probability density function of *Y*. Find the probability density function
    of *Z*. Are *Y* and *Z* independent?
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Assuming that *X1* and *X2* are independent, they can take 36 possibilities,
    as shown in the table here:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Types of probability](img/B03980_09_04.jpg)'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Let''s now construct the joint, marginal, and conditional table. In this, we
    will have values of *Z* as rows and *Y* as columns. *Y* varies from 2 to 12 and
    *Z* varies from -5 to 5\. We can fill all the conditional distributions just by
    counting. For example, take *Z=-1*; we see that this happens when *Y=3, 5, 7,
    9, 11*. We also note that the probability of each one of them (say, the conditional
    probability that *Z=-1*, given *Y=3*) is *1/36*. We can fill the table like this
    for all the values:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![Types of probability](img/B03980_09_05.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: So, the bottom row is the marginal distribution of *Y*. The right-most column
    is the marginal distribution of *Z*. The total table is the joint distribution.
    Clearly, they are dependent.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Distribution
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Distributions are either discrete or continuous probability distributions,
    depending on whether they define probabilities associated with discrete variables
    or continuous variables:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution](img/B03980_09_06.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: We will cover a few of the previously mentioned distributions here.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, our major emphasis is on modeling and describing a given property
    of the data. To understand how crucial this skill is, let''s look at a few examples:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: A bank wants to look at the amount of cash withdrawn per transaction in an ATM
    machine over a period of time to determine the limits of transaction
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A retailer wants to understand the number of broken toys that he is getting
    in every shipment
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A manufacturer wants to understand how the diameter of a probe is varying between
    various manufacturing cycles
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pharmaceutical company wants to understand how the blood pressures of millions
    of patients are impacted by its new drug
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all these cases, we need to come up with some precise quantitative description
    of how the observed quantity is behaving. This section is all about this. Anyway,
    intuitively, what do you think are the qualities that you would like to measure
    to gain an understanding?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: What are all the values that a given variable is taking?
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the probability of taking a given value and what values have the highest
    probability?
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the mean/median, and how much is the variance?
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a value, can we tell how many observations fall into it and how many fall
    away from it?
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we give a range of values where we can tell 90% of the data lies?
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actually, if we can answer these questions, and more importantly if we develop
    a technique to describe such quantities, we are more or less unstoppable as far
    as this property is considered!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two prime observations to be made here. First, a property when distributed
    the way it is has all the qualities it takes to be a random variable (knowing
    one value of the quantity does not help us know the next value). Then, if we know
    the probability mass function or the distribution function of this random variable,
    we can compute all the previous matter. This is why it is so important to understand
    mathematics. In general, we follow (for that matter, almost anybody interested
    in analyzing the data that follows) a systematic process in describing a quantity:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: We will first understand the random variable.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will compute the probability mass (or distribution) function.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we will predict the all-important parameters (mean and variance).
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we will check with experimental data to see how good our approximations
    are.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, the number of vans that have been requested for rental at a car
    rental agency during a 50-day period is identified in the following table. The
    observed frequencies have been converted into probabilities for this 50-day period
    in the last column of the table:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution](img/B03980_09_07.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: 'The expected value is 5.66 vans, as shown here:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution](img/B03980_09_08.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, variance computation is given next:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution](img/B03980_09_09.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: The standard deviation is a square root of variance and is equal to 1.32 vans.
    Let's systematically analyze various distributions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Bernoulli distribution
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the simplest distribution that one can think of. Many a times, a property
    takes only discrete values; like a coin toss, a roll of the dice, the gender of
    people, and so on. Even if they are not exactly discrete, we can transform them
    by binning in some cases. For example, when we look at the net worth of individuals,
    we can redivide them as rich and poor (**discrete quantity**) based on the exact
    wealth they have (**continuous quantity**). Let's say that the probability of
    the property taking a given value is *p* (of course, the probability of it not
    taking is *(1-p)*). If we collect the large sample sufficiently, then how does
    the dataset look? Well, there will be some positives (where the variable took
    the value) and negatives (where the variable does not take the value). Assume
    that we denote positive with 1 and negative with 0.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we have the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The mean = weighted average of probabilities = 1*p +0*(1-p) = p
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Binomial distribution
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is an extension of the Bernoulli idea. Let''s take a specific example.
    You are working in a population bureau and have the data of all the families in
    a state. Let''s say you want to identify the probability of having two male children
    in families that have exactly two children. As you can see, a family can have
    two children in only four different ways: MM, MF, FM, and FF. As we consider having
    a male child as the event of interest, then the probability that there are only
    male children is *0.25 (1/4)*. The probability of there being one male child is
    *0.5 (0.25+0.25) (1/4+1/4),* and no male child is *0.25 (1/4)*.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if you look at 100 families, what is the probability that 20 families have
    exactly two male children? We will come to the solution later. Let''s extend the
    argument to find the probability of having all the males in families with three
    children: The total possibilities are FFF, FFM, FMF, FMM, MFM, MMF, MFF, and MMM
    (eight total possibilities). The probability for all three to be male is *1/8*.
    The probability for two of the three being male is *3/8*. The probability for
    one of the three to be male is *3/8*. The probability for none to be male is *1/8*.
    Note that the total probability of all the events is always equal to 1.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Poisson probability distribution
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s try to extend the Binomial theorem to infinite trials, but with
    a catch. The examples that we have taken (coin toss and more) have an interesting
    property. The probability of the event occurring in a trial does not change even
    if we increase the number of trials. However, there are a great number of examples,
    whereas the number of trials (or its equivalent) increases, the corresponding
    probability of the event decreases. So, we need to reduce the time interval to
    zero, or the number of observations to infinity to ensure that we see only a single
    success or failure in any trial. In this limiting case, the probability that we
    see *r* successes in *n* observations can be computed as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Poisson probability distribution](img/B03980_09_18.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'The probability distribution of a Poisson random variable *X* is as given below.
    This considers representing the number of successes occurring in a given time
    interval:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![Poisson probability distribution](img/B03980_09_19.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: Here, *r* is the *r*^(th) trial and *λ* = a mean number of successes in the
    given time interval or the region of space.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Exponential distribution
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s now look at the Poisson example and ask ourselves a different question.
    What is the probability that the inspector does not see the first car until *t*
    hours? In this case, it may not be relevant, but when we work on the failure of
    a component, it makes sense to understand what time the probability of not seeing
    the failure is high. So, let''s say the sighting of the car (or first failure)
    follows the Poisson process. Then, let''s define *L*, a random variable that is
    the probability that the inspector will not see the first car until time *t* as
    the time before the first sighting of the car. From the Poisson distribution,
    the probability that she will not see the first car in 1 hour is as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![Exponential distribution](img/B03980_09_20.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: The probability that that she will not see a car in the second hour also is
    the same, and the probability that she will not see the car in *t* hours is *e*^(−λt)
    *(e*^(−λ) * *e*^(−λ) **…times)*. The probability that she will see the car in
    the first *t* hours then is *1-e*^(-λt).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'The applications of exponential distribution are as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Time to the first failure in a Poisson process
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance of the dispersion of seeds from the parent plant
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The expected lifetime of an organism, ignoring the aging process (where the
    end occurs due to accidents, infections, and more)
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normal distribution
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Normal distribution is a very widely used class of continuous distribution.
    It is also often called the bell curve because the graph of its probability density
    resembles a bell. Most of the real-life data such as weights, heights, and more
    (particularly when there are large collections) can be well approximated by a
    normal distribution.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![Normal distribution](img/B03980_09_10.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: 'Once we know the values of the heights, the number of samples that have this
    value can be mathematically described as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Normal distribution](img/B03980_09_21.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Here, σ is the standard deviation and µ is the mean. To describe a normal distribution,
    we just need to know two concepts (average and SD).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Every normal curve adheres to the following *rule*:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: About 68% of the area under the curve falls within one standard deviation of
    the mean
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About 95% of the area under the curve falls within two standard deviations of
    the mean
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About 99.7% of the area under the curve falls within three standard deviations
    of the mean
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collectively, these points are known as the **empirical rule** or the **68-95-99.7
    rule**.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Relationship between the distributions
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While we know that more or less everything converges to a normal distribution,
    it is best to understand where each one fits. The following chart helps in this:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![Relationship between the distributions](img/B03980_09_11.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: Bayes' theorem
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we go into the Bayes' theorem, we mentioned at the beginning of this
    chapter what is at the Bayesian learning is the Bayes theorem.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with an example. Assume that there are two bowls of nuts; the first
    bowl contains 30 cashew nuts and 10 pistachios and the second bowl contains 20
    of each. Let's choose one bowl randomly and pick a nut with eyes closed. The nut
    is cashew. Now, what is the probability that the bowl chosen is the first bowl?
    This is a conditional probability.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: So, *p(Bowl 1|cashew)* or the probability that it is bowl 1, given the nut is
    cashew, is not an easy and obvious one to crack.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: If the question was to put the other way, *p(cashew|bowl1)* or the probability
    that the nut is cashew, given bowl 1 is easy, *p(cashew|Bowl 1) = ¾*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: As we know, *p(cashew|Bowl 1)* is not the same as *p(Bowl 1|cashew),* but we
    can use one value to get another value, and this is what Bayes' theorem is all
    about.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step of defining the Bayes'' theorem conjunction is commutative;
    following are the steps:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '*p (A and B) =p (B and A),*'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Further, the probability of A and B is the probability of A and the probability
    of B, given A:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '*p (A and B) = p (A) p (B|A)*, similarly'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '*p (B and A) = p (B) p (A|B)*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: so,
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '*p (A) p (B|A) = p (B) p (A|B)* and'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayes'' theorem](img/B03980_09_32.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: And that's Bayes' theorem!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: It might not be very obvious, but it is a very powerful definition.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s apply this to solve the previous *nut* problem to find *p(bowl1
    cashew)*, and we can derive it if we can get *p(cashew|bowl 1)*:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '*p (bowl1 cashew) = (p(bowl1) p(cashew|bowl1)) / p (cashew)*'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '*p (bowl1) = ½*'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '*p (cashew|bowl1) = ¾*'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '*p (cashew) = total cashews / total nuts (between bowl1 and bowl2) = 50/80
    = 5/8*'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting it together, we have the following:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '*p (bowl1 cashew) = ((1/2) (3/4))/(5/8)= 3/5 = 0.6*'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The additional aspect that needs to be considered now is how to feature in the
    changes that come over time as the new data comes in. This way, the probability
    of a hypothesis can be measured in the context of the data at a given point in
    time. This is called the diachronic interpretation of the Bayes' theorem.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the restating Bayes'' theorem with the hypothesis (*H*) for the
    given data (*D*):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayes'' theorem](img/B03980_09_22.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: '*p (H)* is the probability of the hypothesis *H* before seeing the data *D*.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '*p (D)* is the probability of data *D* under any hypothesis, which is usually
    constant.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '*p (H|D)* is the probability of the hypothesis *H* after seeing the data *D*.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '*p (D|H)* is the probability of data *D* given the hypothesis *H*.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*p (H)* is called prior probability; *p (H|D)* is posterior probability; *p
    (D|H)* is the likelihood; and *p (D)* is the evidence:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayes'' theorem](img/B03980_09_12.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Naïve Bayes classifier
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look at the Naïve Bayes classifiers and how they are
    used to solve the classification problems. The Naïve Bayes classifier technique
    is based on the Bayes' theorem and assumes the predictors to be independent, which
    means knowing the value of one attribute does influence the value of any other
    attribute. The independence assumption is what makes Naïve Bayes *naïve*.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes classifiers are easy to build, do not involve any iterative process,
    and work very well with large datasets. Despite its simplicity, Naïve Bayes is
    known to have often outperformed other classification methods.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: We need to compute the probability of an assumption given a class.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: That is, *P(x*[1]*, x*[2]*, ….x*[n|y]*)*. Obviously, there are multiple pieces
    of evidence represented by *x*[1], *x*[2], *….x*[n].
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we start with an assumption that *x*[1], *x*[2], *….x*[n] are conditionally
    independent, given *y*. Another simple way of defining this is that we need to
    predict an outcome given multiple evidence as against a single evidence. To simplify,
    we uncouple these multiple pieces of evidence:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Outcome|Multiple Evidence) = [P(Evidence1|Outcome) x P(Evidence2|outcome)
    x ... x P(EvidenceN|outcome)] x P(Outcome) / P(Multiple Evidence)*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also written as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Outcome|Evidence) = P(Likelihood of Evidence) x Prior probability of outcome
    / P(Evidence)*'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: In order to apply Naïve Bayes to predict an outcome, the previously mentioned
    formula will need to be run for every outcome. Just run this formula for each
    possible outcome, and in the case of a classification problem, the outcome will
    be a class. We will look at the famous fruit problem to help you understand this
    easily.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Given any three important characteristics of a fruit, we will need to predict
    what fruit it is. To simplify the case, let''s take three attributes—long, sweet,
    and yellow; and three classes of fruit—banana, orange, and others. Let there be
    1,000 data points in the training set, and this is how the available information
    looks like:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Long | Not long | Sweet | Not sweet | Yellow | Not yellow | Total
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| **Banana** | 400 | 100 | 350 | 150 | 450 | 50 | 500 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| **Orange** | 0 | 300 | 150 | 150 | 300 | 0 | 300 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| **Others** | 100 | 100 | 150 | 50 | 50 | 150 | 200 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| **Total** | 500 | 500 | 650 | 350 | 800 | 200 | 1000 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: 'Some derived values/prior probabilities from the previous table are as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Probability of Class
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Banana)= 0.5 (500/1000)*'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Orange)= 0.3*'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Others) = 0.2*'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Probability of Evidence
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Long)= 0.5*'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Sweet)= 0.65*'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Yellow) = 0.8*'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Probability of Likelihood
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Long|Banana) = 0.8*'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Long/Orange) = 0 P(Yellow/Other Fruit) =50/200 = 0.25*'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Not Yellow|Other Fruit)= 0.75*'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, given a fruit, let''s classify it based on attributes. First, we run probability
    for each of the three outcomes, take the highest probability, and then classify
    it:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Banana|/Long, Sweet and Yellow) = p (Long|Banana) x p (Sweet|Banana) x
    p (Yellow|Banana) x p (banana) /p (Long) xp (Sweet) x. p (Yellow)*'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Banana||Long, Sweet and Yellow) =0.8 x 0.7 x 0.9 x 0.5 / p (evidence)*'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Banana||Long, Sweet and Yellow) =0.252/ p (evidence)*'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Orange||Long, Sweet and Yellow) = 0*'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '*p (Other Fruit/Long, Sweet and Yellow) = p (Long/Other fruit) x p (Sweet/Other
    fruit) x p (Yellow/Other fruit) x p (Other Fruit)*'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '*= (100/200 x 150/200 x 50/150 x 200/1000) / p (evidence)*'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.01875/ p (evidence)*'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: With the largest margin of *0.252 >> 0.01875*, we can now classify this Sweet/Long/Yellow
    fruit as likely to be a *Banana*.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: As Naïve Bayes assumes a gaussian distribution for each of the features, it
    is also called the Gaussian Naïve Bayes classifier.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes is particularly good when there is missing data. In the next sections,
    let's look at different types of Naïve Bayes classifiers.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Multinomial Naïve Bayes classifier
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we have seen in the previous section, Naïve Bayes assumes independence of
    the model against the distribution for a feature. In the case of a multinomial
    Naïve Bayes, the *p(x*[i]*|y)* is a multinomial distribution; in short, a multinomial
    distribution is assumed for each of the features. The case that fits this variant
    is that of a document where we need to compute the word count. A simple algorithm
    of multinomial Naïve Bayes is given here:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![Multinomial Naïve Bayes classifier](img/B03980_09_13.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: The Bernoulli Naïve Bayes classifier
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Bernoulli Naïve Bayes classifier attaches a Boolean indicator to a word
    as one if it belongs to a document under examination and zero if it does not.
    The focus of this variation is that it considers the count of occurrence or non-occurrence
    of a word in a specific document under consideration. The non-occurrence of a
    word is an important value as it is used in the computation of the conditional
    probabilities of the occurrence of a word. The Bernoulli Naïve Bayes algorithm
    is detailed here:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![The Bernoulli Naïve Bayes classifier](img/B03980_09_14.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: '|   | Multinomial Naïve Bayes | Bernoulli Naïve Bayes |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: '| **Model Variable** | Here, a token is generated andchecked for occurrence
    in a position | Here, a document is generatedand checked for occurrence in a document
    |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: '| **Document** | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_23.jpg)
    | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_24.jpg)![The Bernoulli
    Naïve Bayes classifier](img/B03980_09_25.jpg) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| **Estimation of the parameter** | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_26.jpg)
    | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_27.jpg) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| **Rule** | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_28.jpg)
    | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_29.jpg) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| **Occurrences** | This considers multiple occurrences | This considers single
    occurrences |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| **Size of the document** | Large documents are handled | Good with smaller
    documents |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: '| **Features** | This supports handling more features | This is good with lesser
    features |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '| **Estimation of a term** | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_30.jpg)
    | ![The Bernoulli Naïve Bayes classifier](img/B03980_09_31.jpg) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
- en: Implementing Naïve Bayes algorithm
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the source code provided for this chapter for implementing Naïve Bayes
    classifier (source code path `.../chapter9/...` under each of the folders for
    the technology).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Using Mahout
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../mahout/chapter9/naivebayesexample/`.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Using R
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../r/chapter9/naivebayesexample/`.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../spark/chapter9/naivebayesexample/`.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Using scikit-learn
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../python-scikit-learn/chapter9/naivebayesexample/`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Using Julia
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../julia/chapter9/naivebayesexample/`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned Bayesian Machine learning and how to implement
    Naïve Bayes classifiers association rule-based learning with Mahout, R, Python,
    Julia, and Spark. Additionally, we covered all the core concepts of statistics,
    starting from basic nomenclature to various distributions. We have covered the
    Bayes' theorem in depth with examples to understand how to apply it to the real-world
    problems.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be covering the regression-based learning techniques
    and in specific, the implementation for linear and logistic regression.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
