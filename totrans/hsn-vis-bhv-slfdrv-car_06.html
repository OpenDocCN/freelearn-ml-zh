<html><head></head><body><div><div><h1 id="_idParaDest-89"><a id="_idTextAnchor091"/><em class="italic">Chapter 4</em>: Deep Learning with Neural Networks</h1>
			<p>This chapter is an introduction to neural networks with Keras. If you have already worked with MNIST or CIFAR-10 image classification datasets, feel free to skip it. But if you have never trained a neural network, this chapter might have some surprises in store for you.</p>
			<p>This chapter is quite practical, to give you very quickly something to play with, and we will skip as much theory as reasonably possible and learn how to recognize handwritten numbers (composed of one single digit) with high precision. The theory behind what we do here, and more, will be covered in the next chapter.</p>
			<p>We will cover the following topics:</p>
			<ul>
				<li>Machine learning</li>
				<li>Neural networks and their parameters</li>
				<li>Convolutional neural networks </li>
				<li>Keras, a deep learning framework</li>
				<li>The MNIST dataset</li>
				<li>How to build and train a neural network</li>
				<li>The CIFAR-10 dataset</li>
			</ul>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor092"/>Technical requirements</h1>
			<p>For the instructions and code in this chapter, you need the following:</p>
			<ul>
				<li>Python 3.7</li>
				<li>NumPy</li>
				<li>Matplotlib</li>
				<li>TensorFlow</li>
				<li>Keras</li>
				<li>The OpenCV-Python module</li>
				<li>A GPU (recommended)</li>
			</ul>
			<p>The code for the book can be found here:</p>
			<p><a href="https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter4">https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter4</a></p>
			<p>The Code in Action videos for this chapter can be found here:</p>
			<p><a href="https://bit.ly/3jfOoWi">https://bit.ly/3jfOoWi</a></p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor093"/>Understanding machine learning and neural networks</h1>
			<p>According <a id="_idIndexMarker257"/>to Wikipedia, <strong class="bold">machine learning</strong> is <em class="italic">"the study of computer algorithms that improve automatically through experience</em>.<em class="italic">"</em></p>
			<p>What that means in practice, at least for what concerns us, is that the algorithm itself is only moderately important, and what is critical is the data that we feed to this algorithm so that it can learn: we need to <strong class="bold">train</strong> our algorithm. Putting it in another way, we can use the same algorithm in many different situations as long as we provide the proper data for the task at hand.</p>
			<p>For example, during this chapter, we will develop a neural network that is able to recognize handwritten numbers between 0 and 9; most likely, the exact same neural network could be used to recognize 10 letters, and with trivial modifications, it could recognize all letters or even different objects. In fact, we will reuse it basically as it is to recognize 10 objects.</p>
			<p>This is totally different from <em class="italic">normal programming</em>, where different tasks usually require different code; to improve a result, we need to improve the code, and we might not need data at all for an algorithm to be usable (with real data).</p>
			<p>That said, this does not mean that the result of a neural network is always good so long as good data is fed to it: difficult tasks require more advanced neural networks to perform well.</p>
			<p>To be clear, while the algorithm (meaning the neural network model) is less important than the code in traditional programming, it is still important if you want to get very good results. In fact, with the wrong architecture, your neural network might not be able to learn at all.</p>
			<p>Neural networks<a id="_idIndexMarker258"/> are only one of the tools that you can use to develop machine learning models, but this is what we will focus on. The accuracy of deep learning is usually quite high, and you might find that applications where less-accurate machine learning techniques are used are heavily constrained by the amount of data and the cost of processing it.</p>
			<p><strong class="bold">Deep learning</strong> can<a id="_idIndexMarker259"/> be considered a subset of machine learning, where the computation is performed by several computation layers, which is the <em class="italic">deep</em> part of the name. From a practical point of view, deep learning is achieved using neural networks.</p>
			<p>That brings us to the question: what exactly is a neural network?</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor094"/>Neural networks</h2>
			<p>Neural networks <a id="_idIndexMarker260"/>are somewhat inspired by our brains: a neuron in our brain is a "computational node" that is connected to other neurons. When performing a computation, each of our brain's neurons "senses" the excited state of the neurons that it is connected to and uses these external states to compute its own state. A neuron in a neural network (a perceptron) basically does the same, but here is more or<a id="_idIndexMarker261"/> less where the similarities end. To be clear, a perceptron is not a simulation of a neuron, but it is just inspired by it.</p>
			<p>The following is a mini neural network, with its neurons:</p>
			<div><div><img src="img/Figure_4.1_B16322.jpg" alt="Figure 4.1 – A neural network" width="1650" height="732"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – A neural network</p>
			<p>The first layer is the input (for example, the pixels of your image) and the output layer is the result (for example, your classification). The hidden layer is where the computation happens. Normally, you have more hidden layers, not just one. Every input can also be called <a id="_idIndexMarker262"/>a feature, and in the case of an RGB image, a feature is usually a single channel of a pixel.</p>
			<p>In feedforward neural networks, the<a id="_idIndexMarker263"/> neurons of a layer are connected only to neurons of the layer before and of the following layers:</p>
			<div><div><img src="img/Figure_4.2_B16322.jpg" alt="Figure 4.2 – A neural network" width="1650" height="465"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – A neural network</p>
			<p>But what exactly is a neuron?</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor095"/>Neurons</h2>
			<p>A neuron<a id="_idIndexMarker264"/> is a computation node that produces an output given some input. As for what these inputs and outputs are – well, it depends. We will come back to this point later.</p>
			<p>The following is a representation of the typical neuron of a neural network:</p>
			<div><div><img src="img/Figure_4.3_B16322.jpg" alt="Figure 4.3 – Schematic of a single neuron of a neural network. ©2016 Wikimedia Commons" width="1581" height="523"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – Schematic of a single neuron of a neural network. ©2016 Wikimedia Commons</p>
			<p>This needs some explanation. The <a id="_idIndexMarker265"/>computation performed by a neuron can be divided into two parts:</p>
			<ul>
				<li>The transfer function<a id="_idIndexMarker266"/> computes the sum of every input multiplied by its weight (just a number); what this means is that the state of the neuron depends on the state of its input neurons, but different neurons provide a different contribution. This is just a linear operation:</li>
			</ul>
			<div><div><img src="img/Formula_04_001.jpg" alt="" width="1650" height="141"/>
				</div>
			</div>
			<ul>
				<li>The activation function<a id="_idIndexMarker267"/> is applied to the result of the transfer function, and it should be a <strong class="bold">non-linear</strong> operation, typically with a threshold. A function that we will use often, because of its <a id="_idIndexMarker268"/>performance, is called <strong class="bold">Rectified Linear Unit (ReLU)</strong>.</li>
			</ul>
			<div><div><img src="img/Figure_4.4_B16322.jpg" alt="Figure 4.4 – Two activation functions: Softplus and ReLU" width="1018" height="611"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – Two activation functions: Softplus and ReLU</p>
			<p>There is usually also a <strong class="bold">bias</strong>, a value <a id="_idIndexMarker269"/>used to shift the activation function.</p>
			<p>The combination of a linear function with a non-linear function is non-linear, while the combination of two linear functions is still linear. This is very important because it means that if the activation was linear, then the output of the neurons will be linear, and the combination of different layers would be linear as well. So, the whole neural network would be linear and therefore equivalent to a single layer.</p>
			<p>Introducing a non-linear operation in the activation allows a network to compute non-linear functions that become more and more complex as the number of layers grows. This is one of the reasons why the most sophisticated neural networks can literally have hundreds of layers.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor096"/>Parameters</h2>
			<p>The bias and the weights<a id="_idIndexMarker270"/> are called <strong class="bold">parameters</strong>, because they are not fixed but need to change based on the task at hand. We do this during the <strong class="bold">training</strong> phase. To be clear, the whole purpose of the training phase is to find the best possible value for these parameters for our task.</p>
			<p>This has profound implications, as it means that the same neural network, with different parameters, can solve different problems – very different problems. The trick, of course, is to find the best values (or one approximation) for these parameters. If you are wondering how many<a id="_idIndexMarker271"/> parameters a typical neural network can have, the answer is millions. Luckily, this process, called training, can be automated.</p>
			<p>An alternative way to imagine a neural network is to consider it as a gigantic system of equations, and the training phase is an attempt to find an approximate solution to it.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor097"/>The success of deep learning</h2>
			<p>You've<a id="_idIndexMarker272"/> probably noticed that deep learning has seen explosive growth in the past few years, but neural networks are really nothing new. I remember trying to write a neural network (and failing miserably!) more than 20 years ago, after reading a book about it. In fact, they date back to 1965, with some theories being even 20 years older than that.</p>
			<p>Many years ago, they were dismissed basically as a curiosity, as they were too computationally demanding to be practical.</p>
			<p>However, fast forward some decades, and deep learning is the new black, thanks to some critical advances:</p>
			<ul>
				<li>Computers are much faster and have much more RAM available.</li>
				<li>GPUs can be used to make computations even faster.</li>
				<li>There are many datasets easily available on the internet to train neural networks.</li>
				<li>There are now plenty of tutorials and online courses dedicated to deep learning.</li>
				<li>There are several good open source libraries for neural networks.</li>
				<li>Architectures have become better and more efficient.</li>
			</ul>
			<p>It's the perfect storm to make neural networks much more appealing, and there are many applications that seem to be waiting for deep learning, such as voice assistants and, of course, self-driving cars.</p>
			<p>There is a special type of neural network that is particularly good at understanding the content of images, and we will pay great attention to them: convolutional neural networks.</p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor098"/>Learning about convolutional neural networks</h1>
			<p>If you look at a classical neural network, you can see that the first layer is composed of inputs, standing on a line. This is not only a graphical representation: for a classical neural network, an input is an input, and it should be independent of the other ones. This is probably fine if you are trying to predict the price of an apartment based on size, ZIP code, and floor number, but it does not seem optimal for an image, where pixels have neighbors and it seems intuitive that keeping this proximity information is important.</p>
			<p><strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>) solve<a id="_idIndexMarker273"/> exactly this problem, and it turns out that not only can they process images efficiently, but they can also be used with success for natural language processing.</p>
			<p>A CNN is a neural network that has at least one convolutional layer, which is inspired by the visual cortex of animals, where individual neurons respond only to stimuli in a small area of the field of vision. Let's see what convolutions really are.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor099"/>Convolutions</h2>
			<p>Convolutions<a id="_idIndexMarker274"/> are based on the concept of the <strong class="bold">kernel</strong>, a matrix that you apply to some pixels to get a single new pixel. Kernels <a id="_idIndexMarker275"/>can be used for edge detection or to apply filters to an image, and you normally have the option to define your kernel in image processing programs, if you wish to do so. The following is a 3x3 identity kernel that replicates an image as it is, and we are applying it to a small image:</p>
			<div><div><img src="img/Figure_4.5_B16322.jpg" alt="Figure 4.5 – Part of an image, a 3x3 identity kernel, and the result" width="1606" height="159"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – Part of an image, a 3x3 identity kernel, and the result</p>
			<p>Just imagine putting a pixel behind each element of the kernel and multiplying them together, then adding the results to get the value of the new pixel; clearly, you would get a zero for each pixel except the central one, which would be unchanged. This kernel preserves the value of the pixel in the middle and discards all the others. If you slide this convolution kernel over the whole picture, you will get the original image back:</p>
			<div><div><img src="img/Figure_4.6_B16322.jpg" alt="Figure 4.6 – Identity convolution – just copying an image" width="654" height="142"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – Identity convolution – just copying an image</p>
			<p>You can see<a id="_idIndexMarker276"/> that as the convolution kernel slides over the image, the pixels are replicated unchanged. You can also see that the resolution is reduced, as we use <em class="italic">valid</em> padding. </p>
			<p>This is another example:</p>
			<div><div><img src="img/Figure_4.7_B16322.jpg" alt="Figure 4.7 – Part of an image, a 3x3 kernel, and the result" width="906" height="106"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7 – Part of an image, a 3x3 kernel, and the result</p>
			<p>Other kernels can be more interesting than the identity kernel. The following kernel (on the left) can detect edges, as seen on the right:</p>
			<div><div><img src="img/Figure_4.8_B16322.jpg" alt="Figure 4.8 – Kernel detecting edges" width="894" height="310"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 – Kernel detecting edges</p>
			<p>If you are curious about kernels, please go ahead with OpenCV and have some fun:</p>
			<pre>img = cv2.imread("test.jpg")kernel = np.array(([-1, -1, -1], [-1, 8, -1], [-1, -1, -1]))dst = cv2.filter2D(img,-1,kernel)cv2.imshow("Kernel", cv2.hconcat([img, dst]))cv2.waitKey(0)</pre>
			<p>Kernels don't need to be 3x3; they can be bigger.</p>
			<p>If you imagine <a id="_idIndexMarker277"/>starting with the first pixel of the image, you might ask what happens then, as there are no pixels above it or to its left. If you position the top-left corner of a kernel on the top-left pixel of an image, you will lose one pixel on each side of the image, because you can think of it as the kernel <em class="italic">emitting a pixe</em>l from the center. This is not always a problem, because when designing a neural network, you might want the image to get smaller and smaller after each layer.</p>
			<p>An alternative is to use padding – for example, pretending that there are black pixels around the image.</p>
			<p>The good news is that you don't need to find the values of the kernels; the CNN will find them for you during the training phase.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor100"/>Why are convolutions so great?</h2>
			<p>Convolutions <a id="_idIndexMarker278"/>have some great advantages. As we have already said, they preserve the proximity of pixels:</p>
			<div><div><img src="img/Figure_4.9_B16322.jpg" alt="Figure 4.9 – A convolution layer, in yellow, versus a dense layer, in green" width="832" height="133"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9 – A convolution layer, in yellow, versus a dense layer, in green</p>
			<p>As you can see from the previous figure, the convolution knows the topology of the image and can know, for example, that the pixel with the number 43 is right next to the pixel with the number 42, is below the pixel with the number 33, and is above the pixel with the number 53. The dense layer in the same figure does not have this information and might think that the pixel with 43 and the pixel with 51 are close to each other. Not only that, but it does not even know whether the resolution is 3x3, 9x1, or 1x9. Intuitively knowing the topology of pixels is an advantage.</p>
			<p>An additional important advantage is that they are computationally efficient.</p>
			<p>Another great characteristic of convolutions is that they are very good at recognizing patterns, such as diagonal lines or circles. You might say that they can only do so at a small scale, which is true, but you can combine multiple convolutions to detect patterns at different scales, and they can be surprisingly good at that.</p>
			<p>They are also able to detect patterns in different parts of an image.</p>
			<p>All these characteristics make them great to work with images, and it is not surprising that they are used so much for object detection.</p>
			<p>Enough theory for now. Let's get our hands dirty and write our first neural network.</p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor101"/>Getting started with Keras and TensorFlow</h1>
			<p>There are many libraries dedicated to deep learning, and we will be using Keras, a Python library that uses multiple backends; we will be using TensorFlow as a backend. While the code is specific to Keras, the principles can apply to any other libraries.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor102"/>Requirements</h2>
			<p>Before starting, you need to<a id="_idIndexMarker279"/> install at least TensorFlow<a id="_idIndexMarker280"/> and Keras, using <code>pip</code>:</p>
			<pre>pip install tensorflow
pip install keras</pre>
			<p>We are using TensorFlow 2.2, which has integrated GPU support, but if you are using TensorFlow version 1.15 or older, you need to install a separate package to take advantage of a GPU:</p>
			<pre>pip install tensorflow-gpu</pre>
			<p>I would recommend using the most recent versions of both TensorFlow and Keras.</p>
			<p>Before starting, let's make sure that everything is in order. You probably want to use a GPU, to speed up training. Unfortunately, getting TensorFlow to use your GPU is not necessarily straightforward; for example, it is very picky with the version of CUDA: if it says CUDA 10.1, it really means it – it is not going to work with 10.0 or with 10.2. Hopefully, this will not affect your games much.</p>
			<p>To print the version of TensorFlow, you can use this code:</p>
			<pre>import tensorflow as tf
print("TensorFlow:", tf.__version__)
print("TensorFlow Git:", tf.version.GIT_VERSION)</pre>
			<p>On my computer, that prints this:</p>
			<pre>TensorFlow: 2.1.0
TensorFlow Git: v2.1.0-rc2-17-ge5bf8de410</pre>
			<p>To check the GPU support, you can use this code:</p>
			<pre>print("CUDA ON" if tf.test.is_built_with_cuda() else "CUDA OFF")print("GPU ON" if tf.test.is_gpu_available() else "GPU OFF")</pre>
			<p>If everything is fine, you should see <code>CUDA ON</code>, meaning that your version of TensorFlow has been built with CUDA support, and <code>GPU ON</code>, meaning that TensorFlow is able to use your GPU.</p>
			<p>If your GPU is not NVIDIA, it might require some more work, but it should be possible to configure TensorFlow to run on AMD graphics cards, using ROCm.</p>
			<p>Now that you have correctly installed all the software, it is time to use it on our first neural network. Our first task will be to recognize handwritten digits, using a dataset called MNIST.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor103"/>Detecting MNIST handwritten digits</h1>
			<p>When you design a<a id="_idIndexMarker281"/> neural network, you usually start with a problem that you want to solve, and you might start with a design that you know performs well on a similar task. You need a dataset, basically as big a dataset as you can get. There is not really a rule on that, but we can say that the minimum to train your own neural network might be something around 3,000 images, but nowadays world-class CNNs are trained using literally millions of pictures.</p>
			<p>Our first task is to detect handwritten digits, a classical task for CNNs. There is a dataset for that, the MNIST dataset (copyright of Yann LeCun and Corinna Cortes), and it is conveniently present in Keras. MNIST detection is an easy task, so we will achieve good results.</p>
			<p>Loading the dataset is easy:</p>
			<pre>from keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = np.reshape(x_train, np.append(x_train.shape, (1)))
x_test = np.reshape(x_test, np.append(x_test.shape, (1)))</pre>
			<p><code>reshape</code> just reinterprets the shape from (60000, 28, 28) to (60000, 28, 28, 1), because Keras needs four dimensions.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor104"/>What did we just load?</h2>
			<p>The <code>load_data()</code> method<a id="_idIndexMarker282"/> returns four things:</p>
			<ul>
				<li><code>x_train</code>: The images used for training</li>
				<li><code>y_train</code>: The labels used for training (that is, the correct numbers for each of the handwritten digits)</li>
				<li><code>x_test</code>: The images used for testing</li>
				<li><code>y_test</code>: The labels used for testing (that is, the correct numbers for each of the handwritten digits)</li>
			</ul>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor105"/>Training samples and labels</h2>
			<p>Let's print the dimensions of the <a id="_idIndexMarker283"/>training samples (<code>x</code>) and of the labels (<code>y</code>):</p>
			<pre>print('X Train', x_train.shape, ' - X Test', x_test.shape)print('Y Train', y_train.shape, ' - Y Test', y_test.shape)</pre>
			<p>It should print something like this:</p>
			<pre>X Train (60000, 28, 28, 1)  - X Test (10000, 28, 28, 1)Y Train (60000,)  - Y Test (10000,)</pre>
			<p>The x variable represents the input for the CNN, which means that x contains all our images divided into two sets, one for training and one for testing:</p>
			<ul>
				<li><code>x_train</code> contains 60,000 images intended for training, each with 28x28 pixels and in grayscale (one channel).</li>
				<li><code>x_test</code> contains 10,000 images intended for testing, each with 28x28 pixels and in grayscale (one channel).</li>
			</ul>
			<p>As you can see, the training and testing images have the same resolution and the same number of channels.</p>
			<p>The <code>y</code> variable represents the<a id="_idIndexMarker284"/> expected output of the CNN, also called the label. For many datasets, somebody manually labels all the images to say what they are. If the dataset is artificial, labeling might be automated:</p>
			<ul>
				<li><code>y_train</code> is composed of 60,000 numbers belonging to 10 classes, from 0 to 9.</li>
				<li><code>y_test</code> is composed of 10,000 numbers belonging to 10 classes, from 0 to 9.</li>
			</ul>
			<p>For each image, we have one label.</p>
			<p>Generally speaking, a neural network can have more than one output, and every output is a number. In the case of a classification task, such as MNIST, the output is a single integer number. In this case, we are particularly lucky, because the output value is actually the number we are interested in (for example, 0 means the number 0, and 1 means the number 1). Usually, you need to convert the number to a label (for example, 0 -&gt; cat, 1 -&gt; dog, and 2 -&gt; duck).</p>
			<p>To be precise, our CNN will not output one integer result from 0 to 9, but 10 floating-point numbers, and the position of the highest one will be the label (for example, if the output in position 3 is the highest value, then the output will be 3). We will discuss this more in the next chapter.</p>
			<p>To better understand MNIST, let's see five samples from the training dataset and five samples from the testing dataset:</p>
			<div><div><img src="img/Figure_4.10_B16322.jpg" alt="Figure 4.10 – MNIST training and testing dataset samples. Copyright of Yann LeCun and Corinna Cortes" width="791" height="48"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10 – MNIST training and testing dataset samples. Copyright of Yann LeCun and Corinna Cortes</p>
			<p>As you might suspect, the corresponding labels of those images are as follows:</p>
			<ul>
				<li>5, 0, 4, 1, and 9 for the training samples (<code>y_train</code>)</li>
				<li>7, 2, 1, 0, and 4 for the testing samples (<code>y_test</code>)</li>
			</ul>
			<p>We should also resize the samples so that instead of being in the 0-255 range, they are in the 0-1 range, as that helps the neural network to achieve better results:</p>
			<pre>x_train = x_train.astype('float32')x_test = x_test.astype('float32')x_train /= 255 x_test /= 255</pre>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor106"/>One-hot encoding</h2>
			<p>The labels cannot be used <a id="_idIndexMarker285"/>directly but need to be converted to a vector using <em class="italic">one-hot encoding</em>. As the name implies, you get a vector where only one element<a id="_idIndexMarker286"/> is hot (for example, its value is <code>1</code>) while all the other elements are cold (for example, their value is <code>0</code>). The hot element represents the position of the label, in a vector including all the possible positions. An example should make it easier to understand.</p>
			<p>In the case of MINST, you have 10 labels: 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. The one-hot encoding would therefore use 10 items. This is the encoding of the first three items:</p>
			<ul>
				<li><code>0 ==&gt; 1 0 0 0 0 0 0 0 0 0</code></li>
				<li><code>1 ==&gt; 0 1 0 0 0 0 0 0 0 0</code></li>
				<li><code>2 ==&gt; 0 0 1 0 0 0 0 0 0 0</code></li>
			</ul>
			<p>If you have three labels, dog, cat, and fish, your one-hot encoding would be as follows:</p>
			<ul>
				<li><code>Dog  ==&gt; 1 0 0</code></li>
				<li><code>Cat  ==&gt; 0 1 0</code></li>
				<li><code>Fish ==&gt; 0 0 1</code></li>
			</ul>
			<p>Keras provides a handy function for that, <code>to_categorical()</code>, which accepts the list of labels to transform and the total number of labels:</p>
			<pre>print("One hot encoding: ", keras.utils.to_categorical([0, 1, 2], 10))</pre>
			<p>If your labels are not numeric, you can use <code>index()</code> to get access to the index of the specified label and use it to call <code>to_categorical()</code>:</p>
			<pre>labels = ['Dog', 'Cat', 'Fish']
print("One hot encoding 'Cat': ", keras.utils.to_categorical(labels.index('Cat'), 10))</pre>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor107"/>Training and testing datasets</h2>
			<p>The <code>x</code> variable <a id="_idIndexMarker287"/>contains the images, but why do we have both <code>x_train</code> and <code>x_test</code>?</p>
			<p>We will explain <a id="_idIndexMarker288"/>everything in detail in the next chapter, but for now let's just say that Keras needs two datasets: one to train the neural network and one that is used to tune the hyperparameters and to evaluate the performance of the neural network.</p>
			<p>It is a bit like having a teacher first explaining things to you, then interrogating you, analyzing your answers to explain better what you did not understand.</p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor108"/>Defining the model of the neural network</h1>
			<p>Now we want to write our<a id="_idIndexMarker289"/> neural network, which we can call our model, and train it. We know that it should use convolutions, but we don't know much more than that. Let's take inspiration from an old but very influential CNN: <strong class="bold">LeNet</strong>.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor109"/>LeNet</h2>
			<p>LeNet<a id="_idIndexMarker290"/> was one of the first CNNs. Dating back to 1998, it's pretty small and simple for today's standards. But it is enough for this task.</p>
			<p>This is its architecture:</p>
			<div><div><img src="img/Figure_4.11_B16322.jpg" alt="Figure 4.11 – LeNet" width="1650" height="497"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11 – LeNet</p>
			<p>LeNet accepts 32x32 images and has the following layers:</p>
			<ul>
				<li>The first layer is<a id="_idIndexMarker291"/> composed of six 5x5 convolutions, emitting images of 28x28 pixels.</li>
				<li>The second layer subsamples the image (for example, computing the average of four pixels), emitting images of 14x14 pixels.</li>
				<li>The third layer is composed of 16 5x5 convolutions, emitting images of 10x10 pixels.</li>
				<li>The fourth layer subsamples the image (for example, computing the average of four pixels), emitting images of 5x5 pixels.</li>
				<li>The fifth layer is a fully connected dense layer (that is, all the neurons of the previous layer are connected to all the neurons of this layer) of 120 neurons.</li>
				<li>The sixth layer is a fully connected dense layer of 84 neurons.</li>
				<li>The seventh and last layer is the output, a fully connected dense layer of 10 neurons, because we need to classify the images into 10 classes, for the 10 digits.</li>
			</ul>
			<p>We are not trying to recreate LeNet precisely, and our input images are a bit smaller, but we will keep it as a reference.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor110"/>The code</h2>
			<p>The first step is defining which type <a id="_idIndexMarker292"/>of neural network we are creating, which in Keras usually is <code>Sequential</code>:</p>
			<pre>model = Sequential()</pre>
			<p>Now we can add the first convolutional layer:</p>
			<pre>model.add(Conv2D(filters=6, kernel_size=(5, 5),
   activation='relu', padding='same',
   input_shape=x_train.shape[1:]))</pre>
			<p>It accepts the following parameters:</p>
			<ul>
				<li>Six filters, so that we will get six kernels, which means six convolutions.</li>
				<li>Kernel size 5x5.</li>
				<li>ReLU activation.</li>
				<li><code>same</code> padding (for example, using black pixels around the image), to not reduce too much the size of the image too early, and to be closer to LeNet.</li>
				<li><code>input_shape</code> contains the shape of the images.</li>
			</ul>
			<p>Then, we add subsampling using <code>Max Pooling (default size=2x2)</code>, which emits the value of the pixel with the maximum <strong class="bold">activation</strong> (for example, with the maximum value):</p>
			<pre>model.add(MaxPooling2D())</pre>
			<p>Then, we can add the <a id="_idIndexMarker293"/>next convolutional layer and the next max pooling layer:</p>
			<pre>model.add(Conv2D(filters=16, kernel_size=(5,5), activation='relu'))
model.add(MaxPooling2D())</pre>
			<p>And then we can add the dense layers:</p>
			<pre>model.add(Flatten())model.add(Dense(units=120, activation='relu'))model.add(Dense(units=84, activation='relu'))model.add(Dense(units=num_classes, activation='softmax'))</pre>
			<p><code>Flatten()</code> is used to flatten the 2D outputs of the convolutional layer into a single row of outputs (1D), which is required by the dense layer. Just to be clear, for our use case, the input of a convolutional filter is a grayscale image, and the output is another grayscale image.</p>
			<p>The last activation, <code>softmax</code>, converts the prediction into a probability, for convenience, and the output with the highest probability will represent the label that the neural network associates to the image.</p>
			<p>That's it: just a few lines of code to build a CNN that can recognize handwritten digits. I challenge you to do the same without machine learning!</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor111"/>The architecture</h2>
			<p>Even if our model<a id="_idIndexMarker294"/> definition is pretty straightforward, it can be useful to visualize it and see whether, for example, the dimensions are as expected.</p>
			<p>Keras has a very useful function for that – <code>summary()</code>:</p>
			<pre>model.summary()</pre>
			<p>And this is the result:</p>
			<pre>_______________________________________________________________
Layer (type)                 Output Shape              Param #   
===============================================================
conv2d_1 (Conv2D)            (None, 28, 28, 6)         156       
_______________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 14, 14, 6)         0         
_______________________________________________________________
conv2d_2 (Conv2D)            (None, 10, 10, 16)        2416      
_______________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 5, 5, 16)          0         
_______________________________________________________________
flatten_1 (Flatten)          (None, 400)               0         
_______________________________________________________________
dense_1 (Dense)              (None, 120)               48120     
_______________________________________________________________
dense_2 (Dense)              (None, 84)                10164     
_______________________________________________________________
dense_3 (Dense)              (None, 10)                850       
===============================================================
Total params: 61,706
Trainable params: 61,706
Non-trainable params: 0</pre>
			<p>This is very interesting. Firstly, we can see that the dimensions of the output of the convolutional layers are the same as for LeNet: 28x28 and 10x10. This is not necessarily important; it just means that the network is dimensioned as we were expecting.</p>
			<p>We can also see that the order of the layers is correct. What is interesting is the third value on each row: the number of parameters. The parameters are the variables that the neural network needs to figure out to actually learn something. They are the variables of our huge <a id="_idIndexMarker295"/>system of equations.</p>
			<p>In the case of a fully connected dense layer, the number of parameters is obtained by multiplying the number of neurons of the previous layer, plus one, by the number of neurons of the current layer. If you remember the image of a neuron, there was one weight for each neuron it was connected to, so it is kind of intuitive that each of them is a trainable parameter. In addition, there is a parameter for the threshold (bias) of the activation. In the last layer, we therefore have the following:</p>
			<ul>
				<li>84 inputs ==&gt; 84 weights + 1 bias ==&gt; 85 parameters</li>
				<li>10 outputs</li>
				<li>85 x 10 ==&gt; 850 parameters</li>
			</ul>
			<p>In the case of a convolutional layer, the number of parameters is given by the area of the kernel plus one, the bias of the activation. In the first layer, we have the following:</p>
			<ul>
				<li>5x5 kernel ==&gt; 25 + 1 bias ==&gt; 26 parameters</li>
				<li>6 filters</li>
				<li>26 x 6 ==&gt; 156 parameters</li>
			</ul>
			<p>As you can see, our network has 61,706 parameters. While this might seem like a lot, it's not uncommon for a neural network to have millions of them. How do they impact the training? As a first approximation, we can say that having more parameters enables our network to learn more<a id="_idIndexMarker296"/> things, but at the same time, it slows it down and increases the size of the model and the amount of memory it uses. Don't become obsessed with the number of parameters, because not all of them are created equal, but keep an eye on them, in case there is some layer that's using too many. You can see that dense layers tend to use many parameters, and in our case, they hold more than 95% of the parameters.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor112"/>Training a neural network</h2>
			<p>Now that we have our<a id="_idIndexMarker297"/> neural network, we need to train it. We will talk more about training in the next chapter, but as the name suggests, training is the phase where the neural network <em class="italic">studies</em> the training dataset and actually learns it. As for how well it learns – that depends.</p>
			<p>For the sake of quickly explaining the concepts, we will do an improper comparison with a student trying to learn a book for an exam:</p>
			<ul>
				<li>The book is the training dataset that the student needs to learn.</li>
				<li>Every time that the student reads the whole book is called an epoch. A student might want to read the book more than once, and it is very common for neural networks to do the same and train for more than one epoch.</li>
				<li>The optimizer is like somebody who asks the student questions from an exercise book (the validation dataset; though, in our example, we are going to use the test dataset for validation) to see how well the student is learning. One key difference is that the neural network does not learn from the validation dataset. We will see in the next chapter why this is very good.</li>
				<li>To track their progress and learn in less time, the student can ask the optimizer to ask questions after a certain number of pages; that number of pages would be the batch size.</li>
			</ul>
			<p>The first thing to do is to configure the model, using <code>compile()</code>:</p>
			<pre>model.compile(loss=categorical_crossentropy, optimizer=Adam(),    metrics=['accuracy'])</pre>
			<p>Keras has a variety of loss functions that you can use. <code>loss</code> is basically a measure of how distant the result of your model is from the ideal output. For classification tasks, we can use <code>categorical_crossentropy</code> as a loss function. <code>optimizer</code> is the algorithm used to train the neural network. If you imagine the neural network as a giant system of equations, the optimizer is the one that figures out how to change the parameters to improve<a id="_idIndexMarker298"/> the result. We will use <code>metrics</code> is just some values computed during the training, but they are not used by the optimizer; they are just provided to you as a reference.</p>
			<p>We can now run the training, which might take a couple of minutes, and it will print the progress that is being made:</p>
			<pre>history = model.fit(x_train, y_train, batch_size=16,    epochs=5, validation_data=(x_test, y_test), shuffle=True)</pre>
			<p>We need to provide several parameters:</p>
			<ul>
				<li><code>x_train</code>: The training images.</li>
				<li><code>y_train</code>: The training labels.</li>
				<li><code>batch_size</code>: The default is 32, and usually it's worth trying powers of 2, from 16 to 256; the batch size affects both speed and accuracy.</li>
				<li><code>epochs</code>: The number of times that the CNN will go through the dataset.</li>
				<li><code>validation_data</code>: As we've already said, we are using the test dataset for validation.</li>
				<li><code>shuffle</code>: If we want to shuffle the training data before each epoch, which usually we want to.</li>
			</ul>
			<p>The result of the training is <code>history</code>, which contains a lot of useful information:</p>
			<pre>print("Min Loss:", min(history.history['loss']))
print("Min Val. Loss:", min(history.history['val_loss']))
print("Max Accuracy:", max(history.history['accuracy']))
print("Max Val. Accuracy:", max(history.history['val_accuracy']))</pre>
			<p>We are talking about minimum<a id="_idIndexMarker299"/> and maximum because these values are measured during each epoch, and do not necessarily progress always toward an improvement.</p>
			<p>Let's go through what we have here:</p>
			<ul>
				<li>The minimum loss is a measure of how close we come to the ideal output in the training dataset, or how well the neural network learned the training dataset. In general, we want this value to be as small as possible.</li>
				<li>The minimum validation loss is how close we come to the ideal output in the validation dataset, or how well the neural network can do with the validation dataset after training. This is probably the most important value, as it is what we are trying to minimize, so we want this value to be as small as possible.</li>
				<li>The maximum accuracy is the maximum percentage of correct answers (predictions) that our CNN can give using the training dataset. For the student example from earlier, it would tell them how well they had memorized the book. Knowing the book by heart is not bad by itself – it is actually desirable – but the goal is not to memorize the book, but to learn from it. While we expect this value to be as high as possible, it can be misleading.</li>
				<li>The maximum validation accuracy is the maximum percentage of correct answers (predictions) that our CNN can give using the validation dataset. For the student example from earlier, it would tell them how well they had really learned the content of the book, so that they can answer questions that might not be present in the book. This will be an indication of how well our neural network can perform in real life.</li>
			</ul>
			<p>This is the result of our CNN:</p>
			<pre>Min Loss: 0.054635716760404344
Min Validation Loss: 0.05480437679834067
Max Accuracy: 0.9842833
Max Validation Accuracy: 0.9835000038146973</pre>
			<p>On your computer, it will probably be slightly different, and in fact it should change a bit every time that you run it.</p>
			<p>We can see that the losses are close to zero, which is good. Both the accuracy and the validation accuracy are almost 98.5%, which in general is very good.</p>
			<p>We can also plot the evolution<a id="_idIndexMarker300"/> over time of these parameters:</p>
			<pre>plt.plot(history_object.history['loss'])
plt.plot(history_object.history['val_loss'])
plt.plot(history_object.history['accuracy'])
plt.plot(history_object.history['val_accuracy'])
plt.title('model mean squared error loss')
plt.ylabel('mean squared error loss')
plt.xlabel('epoch')
plt.legend(['T loss', 'V loss', 'T acc', 'V acc'], loc='upper left')
plt.show()</pre>
			<p>This is the result:</p>
			<div><div><img src="img/Figure_4.12_B16322.jpg" alt="Figure 4.12 – Plot of validation and accuracy over time for MNIST" width="493" height="386"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.12 – Plot of validation and accurac<a id="_idTextAnchor113"/>y over time for MNIST</p>
			<p>Both the<a id="_idIndexMarker301"/> accuracy and the loss are very good after the first epoch and keep improving.</p>
			<p>So far so good. Maybe you think that this was easy. But MNIST is a simple dataset. Let's try CIFAR-10.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor114"/>CIFAR-10</h2>
			<p>To use CIFAR-10, we can just<a id="_idIndexMarker302"/> ask Keras to use a different dataset:</p>
			<pre>(x_train, y_train), (x_test, y_test) = cifar10.load_data()</pre>
			<p>CIFAR-10 is a more difficult dataset. It contains 32x32 RGB images, containing 10 types of objects:</p>
			<pre>X Train (50000, 32, 32, 3)  - X Test (10000, 32, 32, 3)
Y Train (50000, 1)  - Y Test (10000, 1)</pre>
			<p>It looks similar to MNIST.</p>
			<p>In the code on GitHub, to use CIFAR 10, you can simply change the <code>use_mnist</code> variable to <code>False</code>:</p>
			<pre>use_mnist = False</pre>
			<p>You don't need to change anything else in the code, apart from removing the <code>reshape()</code> call because CIFAR-10 uses RGB images and, as a result, it already has three dimensions: width, height, and channels. Keras will adapt the model to the different dimensions and channels, and the<a id="_idIndexMarker303"/> neural network will just learn a new dataset!</p>
			<p>Let's see the new model:</p>
			<pre>_______________________________________________________________
Layer (type)                 Output Shape              Param #   
===============================================================
conv2d_1 (Conv2D)            (None, 32, 32, 6)         456       
_______________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 16, 16, 6)         0         
_______________________________________________________________
conv2d_2 (Conv2D)            (None, 12, 12, 16)        2416      
_______________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 6, 6, 16)          0         
_______________________________________________________________
flatten_1 (Flatten)          (None, 576)               0         
_______________________________________________________________
dense_1 (Dense)              (None, 120)               69240     
_______________________________________________________________
dense_2 (Dense)              (None, 84)                10164     
_______________________________________________________________
dense_3 (Dense)              (None, 10)                850       
===============================================================
Total params: 83,126
Trainable params: 83,126
Non-trainable params: 0</pre>
			<p>The model is a bit bigger, because the <a id="_idIndexMarker304"/>images are slightly bigger and in RGB format. Let's see how it performs:</p>
			<pre>Min Loss: 1.2048443819999695
Min Validation Loss: 1.2831668125152589
Max Accuracy: 0.57608
Max Validation Accuracy: 0.5572999715805054</pre>
			<p>This is not very good: the loss is high and the validation accuracy is only around 55%.</p>
			<p>The next graph is quite important, and you will see it many times, so please take some time to familiarize yourself with it. The following graph shows the evolution of the loss (we use mean squared error) and of the accuracy for each epoch, over time, for our model. On the <em class="italic">X</em> axis, you see the number of epochs, and then there are four lines:</p>
			<ul>
				<li><code>T loss</code>: The training loss</li>
				<li><code>V loss</code>: The validation loss</li>
				<li><code>T acc</code>: The training accuracy</li>
				<li><code>V acc</code>: The validation accuracy:</li>
			</ul>
			<div><div><img src="img/Figure_4.13_B16322.jpg" alt="Figure 4.13 – Plot of validation and accuracy over time for CIFAR-10" width="587" height="360"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.13 – Plot of validation and accuracy over time for CIFAR-10</p>
			<p>We can see that the loss is going down, but it<a id="_idIndexMarker305"/> has not reached a minimum yet, so it probably means more epochs can be beneficial. The accuracy is low and stays low, probably because the model does not have enough parameters.</p>
			<p>Let's see the result with 12 epochs:</p>
			<pre>Min Loss: 1.011266466407776
Min Validation Loss: 1.3062725918769837
Max Accuracy: 0.6473
Max Validation Accuracy: 0.5583999752998352</pre>
			<p>The good news: the loss went down and the accuracy improved. The bad news: the validation loss and validation accuracy did not improve. In practice, our network is learning the training dataset by heart, but it cannot generalize, and therefore it does not perform well on the validation dataset.</p>
			<p>Let's try to also significantly increase the size of the network:</p>
			<pre>model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]))
model.add(AveragePooling2D())
model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu'))
model.add(AveragePooling2D())
model.add(Flatten())
model.add(Dense(units=512, activation='relu'))
model.add(Dense(units=256, activation='relu'))
model.add(Dense(units=num_classes, activation = 'softmax'))</pre>
			<p>That gives us <a id="_idIndexMarker306"/>this new model:</p>
			<pre>_______________________________________________________________
Layer (type)                 Output Shape              Param #   
===============================================================
conv2d_1 (Conv2D)            (None, 30, 30, 64)        1792      
_______________________________________________________________
average_pooling2d_1 (Average (None, 15, 15, 64)        0         
_______________________________________________________________
conv2d_2 (Conv2D)            (None, 13, 13, 256)       147712    
_______________________________________________________________
average_pooling2d_2 (Average (None, 6, 6, 256)         0         
_______________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_______________________________________________________________
dense_1 (Dense)              (None, 512)               4719104   
_______________________________________________________________
dense_2 (Dense)              (None, 256)               131328    
_______________________________________________________________
dense_3 (Dense)              (None, 10)                2570      
===============================================================
Total params: 5,002,506
Trainable params: 5,002,506
Non-trainable params: 0</pre>
			<p>Wow: we jumped from 83,000 to 5,000,000 parameters! That first dense layer is getting big...</p>
			<p>Let's see <a id="_idIndexMarker307"/>whether we can see some improvements:</p>
			<pre>Min Loss: 0.23179266978245228
Min Validation Loss: 1.0802633233070373
Max Accuracy: 0.92804
Max Validation Accuracy: 0.65829998254776</pre>
			<p>Now all the values have improved; however, while the training accuracy is now above 90%, the validation accuracy is just 65%:</p>
			<div><div><img src="img/Figure_4.14_B16322.jpg" alt="Figure 4.14 – Plot of validation and accuracy over time for CIFAR-10" width="948" height="582"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.14 – Plot of validation and accuracy over time for CIFAR-10</p>
			<p>We see something a<a id="_idIndexMarker308"/> bit worrying: while the training loss goes down over time, the validation loss goes up. This situation is called overfitting, and it means that the network is not good at generalizing. It also means that we used way too many epochs for nothing.</p>
			<p>Not only that, but if we saved the model at the end, we would not be saving the best one. If you are wondering whether there is a way to save the best model (for example, with the lowest validation loss), then the answer is yes – Keras can do it:</p>
			<pre>checkpoint = ModelCheckpoint('cifar-10.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)</pre>
			<p>Here we are telling Keras to do the following:</p>
			<ul>
				<li>Save the model with the name <code>'cifar-10.h5'</code>.</li>
				<li>Monitor the validation loss.</li>
				<li>Select the model based on the minimum loss (for example, save only if the validation loss decreases).</li>
				<li>Save only the best model.</li>
			</ul>
			<p>You can pass the <code>checkpoint</code> object to <code>model.fit()</code>:</p>
			<pre>history_object = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True, callbacks=[checkpoint])</pre>
			<p>This helps, but the<a id="_idIndexMarker309"/> model is not good enough. We need something radically better.</p>
			<p>In the next chapter, we will learn many things that will hopefully help us to get some better results. Plus, in <a href="B16322_06_Final_JM_ePUB.xhtml#_idTextAnchor142"><em class="italic">Chapter 6</em></a>, <em class="italic">Improving Your Neural Network</em>, we will apply this knowledge, and more, to improve the results. Now, if you want, you can spend some time trying to tune and improve the network: you can change its size, add filters and layers, and see how it performs.</p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor115"/>Summary</h1>
			<p>This has been a dense chapter! We discussed machine learning in general and deep learning in particular. We talked about neural networks and how convolutions can be used to make faster and more accurate neural networks, leveraging the knowledge of pixel proximity. We learned about weights, bias, and parameters, and how the goal of the training phase is to optimize all these parameters to learn the task at hand.</p>
			<p>After verifying the installation of Keras and TensorFlow, we described MNIST, and we instructed Keras to build a network similar to LeNet, to achieve more than 98% accuracy on this dataset, meaning that we can now easily recognize handwritten digits. Then, we saw that the same model does not perform well in CIFAR-10, despite increasing the number of epochs and the size of the network.</p>
			<p>In the next chapter, we will study in depth many of the concepts that we introduced here, with the final goal, to be completed by <a href="B16322_06_Final_JM_ePUB.xhtml#_idTextAnchor142"><em class="italic">Chapter 6</em></a>, <em class="italic">Improving Your Neural Network</em>, of learning how to train a neural network.</p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor116"/>Questions</h1>
			<p>After reading this chapter, you should be able to answer the following questions:</p>
			<ol>
				<li value="1">What is a perceptron?</li>
				<li>Can you name an optimizer that tends to perform well in many tasks?</li>
				<li>What is a convolution?</li>
				<li>What is a CNN?</li>
				<li>What is a dense layer?</li>
				<li>What does the <code>Flatten()</code> layer do?</li>
				<li>Which backend have we been using for Keras?</li>
				<li>What is the name of one of the first CNNs?</li>
			</ol>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor117"/>Further reading</h1>
			<ul>
				<li>The original LeNet paper: <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf</a></li>
				<li>MNIST: <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></li>
				<li>CNNs: <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53</a></li>
			</ul>
		</div>
	</div>



  </body></html>