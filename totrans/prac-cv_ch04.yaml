- en: What is a Feature?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, our main focus was filtering an image and applying
    different transformations on it. These are good techniques to analyze images but
    are not sufficient for the majority of computer vision tasks. For example, if
    we were to make a product detector for a shopping store, computing only edges
    may not be enough to say whether the image is of an orange or an apple. On the
    other hand, if a person is given the same task, it is very intuitive to differentiate
    between an orange and an apple. This is because of the fact that human perception
    combines several features, such as texture, color, surface, shape, reflections,
    and so on,  to distinguish between one object with another. This motivates to
    look for more details that relates to complex features of objects. These complex
    features can then be used in high level image vision tasks like image recognition,
    search, and so on. There are, however, cases where someone just walks straight
    into a glass wall, which is due not being able to find enough features to say
    whether it is free space or glass.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will first begin with an explanation features and its importance
    in computer vision. Later in the chapter, we will different types of features
    extractors like Harris Corner Detector, FAST keypoint detectors, ORB features
    detectors. The visualization of the keypoints using each of them are also described
    using OpenCV. Lastly, the effectiveness of ORB features is shown with two similar
    applications. We will also see a brief discussion on black box features.
  prefs: []
  type: TYPE_NORMAL
- en: Features use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following are some of the generic applications that are popular in computer
    vision:'
  prefs: []
  type: TYPE_NORMAL
- en: We have two images and we would like to quantify whether these images match
    each other. Assuming a comparison metric, we say that the image matches when our
    comparison metric value is greater than a threshold.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In another example, we have a large database of images, and for a new image,
    we want to perform an operation similar to matching. Instead of recomputing everything
    for every image, we can store a smaller, easier to search and robust enough to
    match, representation of images. This is often referred to as a feature vector
    of the image. Once a new image comes, we extract similar representation for the
    new image and search for the nearest match among the previously generated database.
    This representation is usually formulated in terms of features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, in the case of finding an object, we have a small image of an object or
    a region called a **template**. The goal is to check whether an image has this
    template. This would require matching key points from the template against the
    given sample image. If the match value is greater than a threshold, we can say
    the sample image has a region similar to the given template. To further enhance
    our finding, we can also show where in the sample image lies our template image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, a computer vision system needs to learn several features that describes
    an object such that it is quite easy to distinguish from other objects.
  prefs: []
  type: TYPE_NORMAL
- en: When we design software to do image matching or object detection in images,
    the basic pipeline for detection is formulated from a machine learning perspective.
    This means that we take a set of images, extract significant information, learn
    our model and use the learned model on new images to detect similar objects. In
    this section, we will explore more on this.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, an image matching procedure looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to extract robust features from a given image. This involves
    searching through the whole image for possible features and then thresholding
    them. There are several techniques for the selection of features such as SIFT[3],
    SURF[4], FAST[5], BRIEF[6], ORB detectors[2], and so on. The feature extracted,
    in some cases, needs to be converted into a more descriptive form such that it
    is learnt by the model or can be stored for re-reading.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of feature matching, we are given a sample image and we would like
    to see whether this matches a reference image. After feature detection and extraction,
    as shown previously, a distance metric is formed to compute the distance between
    features of a sample with respect to the features of reference. If this distance
    is less than the threshold, we can say the two images are similar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For feature tracking, we omit previously explained feature matching steps. Instead
    of globally matching features, the focus is more on neighborhood matching. This
    is used in cases such as image stabilization, object tracking, or motion detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets and libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use `OpenCV` library for performing feature detection
    and matching. The plots are generated using `matplotlib`. We will be using custom
    images to show the results of various algorithms. However, the code provided here
    should work on webcam or other custom images too.
  prefs: []
  type: TYPE_NORMAL
- en: Why are features important?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Features play a major role in creating good quality computer vision systems.
    One of the first features we can think of is **pixels**. In order to create a
    comparison tool, we use an average of squared distance between the pixel values
    of two images. These, however, are not robust because rarely will you see two
    images that are exactly the same. There is always some camera movement and illumination
    changes between images, and computing a difference between pixel values will be
    giving out large values even when the images are quite similar.
  prefs: []
  type: TYPE_NORMAL
- en: There are, however, other kinds of features that take into account local and
    global properties of an image. The local properties are referred to as image statistics
    around the neighborhood of the image, while global refers to considering overall
    image statistics. Since both local, and global properties of an image provide
    significant information about an image, computing features that can capture these
    will make them more robust and accurate in applications.
  prefs: []
  type: TYPE_NORMAL
- en: The most basic form of feature detector is point features. In applications such
    as panorama creation on our smartphones, each image is stitched with the corresponding
    previous image. This stitching of image requires correct orientation of an image
    overlapped at pixel level accuracy. Computing corresponding pixels between two
    images requires pixel matching.
  prefs: []
  type: TYPE_NORMAL
- en: Harris Corner Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start feature point detection using the Harris Corner Detection[1] technique.
    In this, we begin with choosing a matrix, termed a **window**, which is small
    in size as compared to the image size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea is to first overlay chosen window on the input image and observe
    only the overlayed region from the input image. This window is later shifted over
    the image and the new overlayed region is observed. In this process, there arise
    three different cases:'
  prefs: []
  type: TYPE_NORMAL
- en: If there is a flat surface, then we won't be able to see any change in the window
    region irrespective of the direction of movement of the window. This is because
    there is no edge or corner in the window region.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our second case, the window is overlayed on edge in the image and shifted.
    If the window moves along the direction of the edge, we will not be able to see
    any changes in the window. While, if the window is moved in any other direction,
    we can easily observe changes in the window region.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, if the window is overlayed on a corner in the image and is shifted,
    where the corner is an intersection of two edges, in most of the cases, we will
    be able to observe the changes in the window region.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Harris Corner Detection uses this property in terms of a score function. Mathematically,
    it is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57db2e62-ffbc-41f7-8bc3-c5c706c99c02.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *w* is a window, *u* and *v* are the shift and *I* is image pixel value.
    The output *E* is the objective function and maximizing this with respect to *u*
    and *v* results in corner pixels in the image *I*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Harris Corner Detection score value will show whether there is an edge,
    corner, or flat surface. An example of Harris Corners of different kinds of images
    is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26e4734c-afa1-48e7-b2b6-afa9e56c4236.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous figure, the upper row has input images, while the bottom row
    has detected corners. These corners are shown with small gray pixels values corresponding
    to the location in the input image.  In order to generate an image of corners
    for a given colored image, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We can generate different number of corners for an image by changing the parameters
    such as covariance matrix block size, neighbourhood kernel size and Harris score
    parameter. In the next section, we will see more robust feature detectors.
  prefs: []
  type: TYPE_NORMAL
- en: FAST features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many features detectors are not useful for real-time applications such as  a
    robot with a camera is moving on the streets. Any delay caused may decrease the
    functionality of the robot or complete system failure. Features detection is not
    the only part of the robot system but if this effects the runtime, it can cause
    significant overhead on other tasks to make it work real time.
  prefs: []
  type: TYPE_NORMAL
- en: '**FAST** (**Features from Accelerated Segment Test**)[5], was introduced by
    Edward Rosten and Tom Drummond in 2006. The algorithm uses pixel neighborhood
    to compute key points in an image. The algorithm for FAST feature detection is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting point candidate pixel **(i,j)** is selected with an intensity
    *I (i,j)*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9d6d678d-ac93-4f05-a4ab-41e561fa4109.png)'
  prefs: []
  type: TYPE_IMG
- en: In a circle of 16 pixels, given a threshold *t,* estimate *n* adjoining points
    which are brighter than pixel *(i,j)* intensity by a threshold *t* or darker than
    *(i,j)* pixel intensity by a threshold *t*. This will become *n* pixels which
    are either less than *(I(i,j) + t)* or greater than *(I(i,j) - t)*. This *n* was
    chosen as 12.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a high-speed test, only four pixels (as shown in the figure) at 1, 9, 5,
    and 13 are looked at. The intensity value of at least three pixels of these decides
    whether the center pixel *p* is a corner. If these values are either greater than
    the *(I(i,j) + t)* or less than *(I(i,j) - t)* then the center pixel is considered
    a corner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In OpenCV , the steps to compute FAST features are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize detector using `cv2.FastFeatureDetector_create()`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setup threshold parameters for filtering detections
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setup flag if non-maximal suppression to be used for clearing neighbourhood
    regions of repeated detections
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detect keypoints and plot them on the input image
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following figure, there are plots of FAST corners (in small circles)
    on the input image with varying threshold values. Depending on the image, a different
    choice of thresholds produce different  number of key feature points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a69b4bbf-cc5e-4cfd-b77f-18597c86bb15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To generate each image in the previous figure, use the following code by changing
    the threshold values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows variations of the same detector across different
    images with varying thresholds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56b8c648-f7fa-42ef-81b9-1966529dc528.png)'
  prefs: []
  type: TYPE_IMG
- en: This shows that choice of parameters is quite crucial for different images.
    Though a common threshold value may not work for all image, a good approximation
    can be used depending on the similarity of images.
  prefs: []
  type: TYPE_NORMAL
- en: ORB features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using previously described corner detectors are fast to compute, however in
    matching two images, it is difficult to select which two image corners are matched
    for corresponding pixels. An additional information that describes properties
    a corner is required. A combination of detected keypoints, such as corners, and
    corresponding descriptors makes comparing images more efficient and robust.
  prefs: []
  type: TYPE_NORMAL
- en: 'ORB features detection[2] features were described by Ethan Rublee et al. in
    2011 and have since been one of the popular features in various applications.
    This combines two algorithms: FAST feature detector with an orientation component
    and BRIEF Descriptors, hence the name **Oriented FAST and Rotated BRIEF** (**ORB**). The
    major advantage of using ORB features is the speed of detections while maintaining
    robust detections. This makes them useful for several real-time applications like
    robotics vision system, smartphone apps, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we have already seen FAST feature detectors, we will further
    continue describing BRIEF descriptor and finally build on ORB detector.
  prefs: []
  type: TYPE_NORMAL
- en: FAST feature limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FAST Features as described in the previous section computes corner in the image
    using neighborhood pixels. By creating a comparison test along the circular region
    around a pixel, features are computed rapidly. FAST features are quite efficient
    for real-time applications; these do not produce rotation information of the features.
    This causes a limitation if we are looking for orientation invariant features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In ORB,  FAST features are used with orientation information as well. Using
    a circular radius of 9 pixels, a vector between computed intensity centroid and
    center of the corner is used to describe orientation at the given corner. This
    intensity centroid for a given patch is computed as follows :'
  prefs: []
  type: TYPE_NORMAL
- en: 'For an image I and a patch window, compute moments using:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/14ad6e32-f91b-4e8d-8d22-676b7c221fe5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using previous moments, intensity centroid of given patch is given as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ff053273-7b12-4d6f-ac57-9f88240e707d.png)'
  prefs: []
  type: TYPE_IMG
- en: Since, we already know the center *O* of the patch, a vector joining ![](img/29e62981-0826-45d3-8e36-d9b721011edb.png) is
    the orientation of the patch.  In further sections, we will see an overall implementation
    of ORB feature detectors which uses this method.
  prefs: []
  type: TYPE_NORMAL
- en: BRIEF Descriptors and their limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The popular feature descriptors like SIFT or SURF outputs large vectors of
    dimensions 128 and 64 respectively. In applications such as image search, it is
    quite likely that the features are stored and searched for features rather than
    the original image. This becomes computationally complex and memory may be inefficient
    if the number of images reaches a few hundred thousand. In such cases, simple
    dimensionality reduction is an added step and may reduce overall efficiency. The
    descriptor proposed by Michael Calonder and their co-authors. in *BRIEF: Binary
    Robust Independent Elementary Features*[6] resolves issues by consuming less memory.'
  prefs: []
  type: TYPE_NORMAL
- en: BRIEF computes differences of intensities in a small patch of an image and represents
    it as a binary string. This not only makes it faster but also the descriptor preserves
    good accuracy. However, there is no feature detector in BRIEF but combining it
    with FAST detectors makes it efficient.
  prefs: []
  type: TYPE_NORMAL
- en: ORB features using OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following code uses ORB features implementation in `OpenCV`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a three-step process, which is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First create an ORB object and update parameter values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Detect keypoints from previously created ORB object:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, compute descriptors for each keypoints detected:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The overall code for  ORB keypoints detections and descriptor extractor is
    given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'An example of generated keypoints is as shown in the following figure (in circles):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2acb8135-7ca7-4144-88d6-94cd87f58a27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the following figure, different images produce different
    feature points for various shapes of objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24e634de-267e-4f7f-b768-c79e2b5c4de7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to plot previous shown figures with different keypoints, we can use
    both `OpenCV` and `Matplotlib` as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we saw formulation of ORB features that not only combines robust
    features, but also provides descriptors for easier comparison to other features.
    This is a strong formulation of feature detector, however explicitly designing
    a feature detector for different task will require efficient choice of parameters
    such as patch size for FAST detector, BRIEF descriptor parameters etc. For a non-expert,
    setting these parameters may be quite cumbersome task. In following section, we
    will begin with discussion on black box features and its importance in creating
    computer vision systems.
  prefs: []
  type: TYPE_NORMAL
- en: The black box feature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The features we discussed previously are highly dependent on an image to image
    basis. Some of the challenges observed in detecting features are:'
  prefs: []
  type: TYPE_NORMAL
- en: In case of illumination changes, such as nighttime image or daylight images
    there would be a significant difference in pixel intensity values as well as neighborhood
    regions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As object orientation changes, keypoint descriptor changes significantly. In
    order to match corresponding features, a proper choice of descriptor parameters
    is required
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to these challenges, several parameters used here need to be tuned by experts.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, a lot has been happening with neural networks in the field
    of computer vision. The popularity of them has risen due to higher accuracy and
    less hand-tuned parameters. We can call them black box features—though the term
    black refers only to the way they are designed. In a majority of these model deployments,
    the parameters are learned through training and require the least supervision
    of parameters setting. The black box modeling feature detection helps in getting
    better features by learning over a dataset of images. This dataset consists of
    possible different variations  of images, as a result the learnt detector can
    extract better features even in wide variation of image types. We will study these
    feature detectors in the next chapter as CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Application – find your object in an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most common application for using features is given an object, find the
    best possible match for it in the image. This is often referred to as **template
    matching**, where the object at hand is a usually small window called a **template**
    and goal is to compute the best-matched features from this template to a target
    image. There exist several solutions to this, but for the sake of understanding,
    we will use ORB features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using ORB features, we can do feature matching in a brute force way as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute features in each image (template and target).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each feature in a template, compare all the features in the target detected
    previously. The criterion is set using a matching score.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the feature pair passes the criterion, then they are considered a match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Draw matches to visualize.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a pre-requisite, we will follow previously shown codes for extracting features
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Once we have keypoints and descriptors from each of the images, we can use them
    to compare and match.
  prefs: []
  type: TYPE_NORMAL
- en: 'Matching keypoints between two images is a two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create desired kind of matcher specifying the distance metric to be used. Here
    we will use Brute-Force Matching with Hamming distance:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Using descriptors for keypoints from each image, perform matching as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we will show overall Brute-Force method of matching
    keypoints from one image to another using corresponding descriptors only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following figure, the features from the template are matched to the
    original image. To show the effectiveness of matching, only the best matches are
    shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aeb9b00d-4645-4f3f-9f2c-0ebed4b2656a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous feature matching image is created using the following code, where
    we use a sample template image to match to a large image of the same object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Applications – is it similar?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this application, we would like to see if they are similar using previously
    described feature detectors. For that, we use a similar approach as previously
    mentioned. The first step is computing feature keypoints and descriptors for each
    image. Using these performs matching between one image and another. If there are
    a sufficient number of matches, we can comfortably say that the two images are
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the prerequisites,  we use the same ORB keypoint and descriptor extractor
    but added downsampling of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the previously computed keypoints and descriptors, the matching is done
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of an example are as shown in the following figure, where inputs
    are same objects with different viewpoints. The correct matches are shown as with
    connecting lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3983685d-067f-473f-bf48-634dd55bc183.png)'
  prefs: []
  type: TYPE_IMG
- en: In this section, we saw two similar approaches for image matching using ORB
    keypoints and a Brute-Force matcher. The matching can be further enhanced by using
    more faster algorithms like approximate neighborhood matches. The effect of faster
    matching is mostly seen in the cases where a large number of features keypoints
    are extracted.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we began discussion features and its importance in computer
    vision applications. Harris Corner Detector is used to detect corners where runtime
    is of utmost importance. These can run on embedded devices with high speeds. Extending
    over to more complex detectors, we saw FAST features and in combination with BRIEF
    descriptors, ORB features can be formed. These are robust for different scales
    as well as rotations. Finally, we saw the application of feature matching using
    ORB features and a use of pyramid downsampling.
  prefs: []
  type: TYPE_NORMAL
- en: The discussion on black box features will continue in the next chapter with
    the introduction of neural networks and especially CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Harris Chris, and Mike Stephens. *A combined corner and edge detector*. In Alvey
    vision conference, vol. 15, no. 50, pp. 10-5244\. 1988.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rublee Ethan, Vincent Rabaud, Kurt Konolige, and Gary Bradski. *ORB: An efficient
    alternative to SIFT or SURF*. In Computer Vision (ICCV), 2011 IEEE international
    conference on, pp. 2564-2571\. IEEE, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lowe David G. *Object recognition from local scale-invariant features*. In Computer
    vision, 1999\. The proceedings of the seventh IEEE international conference on,
    vol. 2, pp. 1150-1157\. IEEE, 1999.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bay Herbert, Tinne Tuytelaars, and Luc Van Gool. *Surf: Speeded up robust features*. Computer
    vision–ECCV 2006(2006): 404-417.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosten Edward, and Tom Drummond. *Machine learning for high-speed corner detection*. Computer
    Vision–ECCV 2006(2006): 430-443.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calonder Michael, Vincent Lepetit, Christoph Strecha, and Pascal Fua. *Brief:
    Binary robust independent elementary features*. Computer Vision–ECCV 2010 (2010):
    778-792.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
