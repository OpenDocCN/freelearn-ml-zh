["```py\n# import the Iris dataset from scikit-learn\nfrom sklearn.datasets import load_iris\n# import our plotting module\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# load the Iris dataset\niris = load_iris()\n```", "```py\n# create X and y variables to hold features and response column\niris_X, iris_y = iris.data, iris.target\n```", "```py\n# the names of the flower we are trying to predict.\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='|S10')\n```", "```py\n# Names of the features\niris.feature_names\n\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n```", "```py\n# for labelling\nlabel_dict = {i: k for i, k in enumerate(iris.target_names)}\n# {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n\ndef plot(X, y, title, x_label, y_label):\n ax = plt.subplot(111)\n for label,marker,color in zip(\n range(3),('^', 's', 'o'),('blue', 'red', 'green')):\n\n plt.scatter(x=X[:,0].real[y == label],\n y=X[:,1].real[y == label],\n color=color,\n alpha=0.5,\n label=label_dict[label]\n )\n\n plt.xlabel(x_label)\n plt.ylabel(y_label)\n\n leg = plt.legend(loc='upper right', fancybox=True)\n leg.get_frame().set_alpha(0.5)\n plt.title(title)\n\nplot(iris_X, iris_y, \"Original Iris Data\", \"sepal length (cm)\", \"sepal width (cm)\")\n```", "```py\n# Calculate a PCA manually\n\n# import numpy\nimport numpy as np\n\n# calculate the mean vector\nmean_vector = iris_X.mean(axis=0)\nprint mean_vector\n[ 5.84333333  3.054       3.75866667  1.19866667]\n\n# calculate the covariance matrix\ncov_mat = np.cov((iris_X-mean_vector).T)\nprint cov_mat.shape\n(4, 4)\n```", "```py\n# calculate the eigenvectors and eigenvalues of our covariance matrix of the iris dataset\neig_val_cov, eig_vec_cov = np.linalg.eig(cov_mat)\n\n# Print the eigen vectors and corresponding eigenvalues\n# in order of descending eigenvalues\nfor i in range(len(eig_val_cov)):\n eigvec_cov = eig_vec_cov[:,i]\n print 'Eigenvector {}: \\n{}'.format(i+1, eigvec_cov)\n print 'Eigenvalue {} from covariance matrix: {}'.format(i+1, eig_val_cov[i])\n print 30 * '-'\n\nEigenvector 1: \n[ 0.36158968 -0.08226889  0.85657211  0.35884393]\nEigenvalue 1 from covariance matrix: 4.22484076832\n------------------------------\nEigenvector 2: \n[-0.65653988 -0.72971237  0.1757674   0.07470647]\nEigenvalue 2 from covariance matrix: 0.242243571628\n------------------------------\nEigenvector 3: \n[-0.58099728  0.59641809  0.07252408  0.54906091]\nEigenvalue 3 from covariance matrix: 0.0785239080942\n------------------------------\nEigenvector 4: \n[ 0.31725455 -0.32409435 -0.47971899  0.75112056]\nEigenvalue 4 from covariance matrix: 0.023683027126\n------------------------------\n```", "```py\n# the percentages of the variance captured by each eigenvalue\n# is equal to the eigenvalue of that components divided by\n# the sum of all eigen values\n\nexplained_variance_ratio = eig_val_cov/eig_val_cov.sum()\nexplained_variance_ratio\n\narray([ 0.92461621,  0.05301557,  0.01718514,  0.00518309])\n```", "```py\n# Scree Plot\n\nplt.plot(np.cumsum(explained_variance_ratio))\nplt.title('Scree Plot')\nplt.xlabel('Principal Component (k)')\nplt.ylabel('% of Variance Explained <= k')\n```", "```py\n# store the top two eigenvectors in a variable\ntop_2_eigenvectors = eig_vec_cov[:,:2].T\n\n# show the transpose so that each row is a principal component, we have two rows == two components\ntop_2_eigenvectors\n\narray([[ 0.36158968, -0.08226889,  0.85657211,  0.35884393],\n       [-0.65653988, -0.72971237,  0.1757674 ,  0.07470647]])\n```", "```py\n# to transform our data from having shape (150, 4) to (150, 2)\n# we will multiply the matrices of our data and our eigen vectors together\n\nnp.dot(iris_X, top_2_eigenvectors.T)[:5,]\n\narray([[ 2.82713597, -5.64133105],\n       [ 2.79595248, -5.14516688],\n       [ 2.62152356, -5.17737812],\n       [ 2.7649059 , -5.00359942],\n       [ 2.78275012, -5.64864829]])\n```", "```py\n# scikit-learn's version of PCA\nfrom sklearn.decomposition import PCA\n```", "```py\n# Like any other sklearn module, we first instantiate the class\npca = PCA(n_components=2)\n```", "```py\n# fit the PCA to our data\npca.fit(iris_X)\n```", "```py\npca.components_\n\narray([[ 0.36158968, -0.08226889,  0.85657211,  0.35884393],\n       [ 0.65653988,  0.72971237, -0.1757674 , -0.07470647]])\n\n# note that the second column is the negative of the manual process\n# this is because eignevectors can be positive or negative\n# It should have little to no effect on our machine learning pipelines\n```", "```py\npca.transform(iris_X)[:5,]\n\narray([[-2.68420713,  0.32660731],\n       [-2.71539062, -0.16955685],\n       [-2.88981954, -0.13734561],\n       [-2.7464372 , -0.31112432],\n       [-2.72859298,  0.33392456]])\n\n# sklearn PCA centers the data first while transforming, so these numbers won't match our manual process.\n```", "```py\n# manually centering our data to match scikit-learn's implementation of PCA\nnp.dot(iris_X-mean_vector, top_2_eigenvectors.T)[:5,]\n\narray([[-2.68420713, -0.32660731],\n       [-2.71539062,  0.16955685],\n       [-2.88981954,  0.13734561],\n       [-2.7464372 ,  0.31112432],\n       [-2.72859298, -0.33392456]])\n```", "```py\n# Plot the original and projected data\nplot(iris_X, iris_y, \"Original Iris Data\", \"sepal length (cm)\", \"sepal width (cm)\")\nplt.show()\nplot(pca.transform(iris_X), iris_y, \"Iris: Data projected onto first two PCA components\", \"PCA1\", \"PCA2\")\n```", "```py\n# percentage of variance in data explained by each component\n# same as what we calculated earlier\npca.explained_variance_ratio_\n\narray([ 0.92461621,  0.05301557])\n```", "```py\n# show how pca attempts to eliminate dependence between columns\n\n# show the correlation matrix of the original dataset\nnp.corrcoef(iris_X.T)\n\narray([[ 1\\.        , -0.10936925,  0.87175416,  0.81795363],\n       [-0.10936925,  1\\.        , -0.4205161 , -0.35654409],\n       [ 0.87175416, -0.4205161 ,  1\\.        ,  0.9627571 ],\n       [ 0.81795363, -0.35654409,  0.9627571 ,  1\\.        ]])\n```", "```py\n# correlation coefficients above the diagonal\n np.corrcoef(iris_X.T)[[0, 0, 0, 1, 1], [1, 2, 3, 2, 3]]\n\n array([-0.10936925, 0.87175416, 0.81795363, -0.4205161 , -0.35654409])\n```", "```py\n# average correlation of original iris dataset.\n np.corrcoef(iris_X.T)[[0, 0, 0, 1, 1], [1, 2, 3, 2, 3]].mean()\n\n 0.16065567094168495\n```", "```py\n# capture all four principal components\n full_pca = PCA(n_components=4)\n\n # fit our PCA to the iris dataset\n full_pca.fit(iris_X)\n```", "```py\n pca_iris = full_pca.transform(iris_X)\n # average correlation of PCAed iris dataset.\n np.corrcoef(pca_iris.T)[[0, 0, 0, 1, 1], [1, 2, 3, 2, 3]].mean()\n # VERY close to 0 because columns are independent from one another\n # This is an important consequence of performing an eigenvalue decomposition\n\n 7.2640855025557061e-17 # very close to 0\n```", "```py\n# import our scaling module\n from sklearn.preprocessing import StandardScaler\n # center our data, not a full scaling\n X_centered = StandardScaler(with_std=False).fit_transform(iris_X)\n\n X_centered[:5,]\n\n array([[-0.74333333, 0.446 , -2.35866667, -0.99866667], [-0.94333333, -0.054 , -2.35866667, -0.99866667], [-1.14333333, 0.146 , -2.45866667, -0.99866667], [-1.24333333, 0.046 , -2.25866667, -0.99866667], [-0.84333333, 0.546 , -2.35866667, -0.99866667]])\n```", "```py\n# Plot our centered data\n plot(X_centered, iris_y, \"Iris: Data Centered\", \"sepal length (cm)\", \"sepal width (cm)\")\n```", "```py\n# fit our PCA (with n_components still set to 2) on our centered data\n pca.fit(X_centered)\n```", "```py\n# same components as before\n pca.components_\n\n array([[ 0.36158968, -0.08226889, 0.85657211, 0.35884393], [ 0.65653988, 0.72971237, -0.1757674 , -0.07470647]])\n```", "```py\n# same projection when data are centered because PCA does this automatically\n pca.transform(X_centered)[:5,]\n\n array([[-2.68420713, 0.32660731], [-2.71539062, -0.16955685], [-2.88981954, -0.13734561], [-2.7464372 , -0.31112432], [-2.72859298, 0.33392456]])\n```", "```py\n# Plot PCA projection of centered data, same as previous PCA projected data\n plot(pca.transform(X_centered), iris_y, \"Iris: Data projected onto first two PCA components with centered data\", \"PCA1\", \"PCA2\")\n\n```", "```py\n # percentage of variance in data explained by each component\n pca.explained_variance_ratio_\n\n array([ 0.92461621, 0.05301557])\n```", "```py\n# doing a normal z score scaling\n X_scaled = StandardScaler().fit_transform(iris_X)\n\n # Plot scaled data\n plot(X_scaled, iris_y, \"Iris: Data Scaled\", \"sepal length (cm)\", \"sepal width (cm)\")\n```", "```py\n# fit our 2-dimensional PCA on our scaled data\n pca.fit(X_scaled)\n\n # different components as cenetered data\n pca.components_\n\n array([[ 0.52237162, -0.26335492, 0.58125401, 0.56561105], [ 0.37231836, 0.92555649, 0.02109478, 0.06541577]])\n```", "```py\n# different projection when data are scaled\n pca.transform(X_scaled)[:5,]\n\n array([[-2.26454173, 0.5057039 ], [-2.0864255 , -0.65540473], [-2.36795045, -0.31847731], [-2.30419716, -0.57536771], [-2.38877749, 0.6747674 ]])\n```", "```py\n# percentage of variance in data explained by each component\n pca.explained_variance_ratio_\n\n array([ 0.72770452, 0.23030523])\n```", "```py\n# Plot PCA projection of scaled data\n plot(pca.transform(X_scaled), iris_y, \"Iris: Data projected onto first two PCA components\", \"PCA1\", \"PCA2\")\n```", "```py\n# how to interpret and use components\n pca.components_ # a 2 x 4 matrix\n\n array([[ 0.52237162, -0.26335492, 0.58125401, 0.56561105], [ 0.37231836, 0.92555649, 0.02109478, 0.06541577]])\n```", "```py\n# Multiply original matrix (150 x 4) by components transposed (4 x 2) to get new columns (150 x 2)\n np.dot(X_scaled, pca.components_.T)[:5,]\n\n array([[-2.26454173, 0.5057039 ], [-2.0864255 , -0.65540473], [-2.36795045, -0.31847731], [-2.30419716, -0.57536771], [-2.38877749, 0.6747674 ]])\n```", "```py\n# extract the first row of our scaled data\n first_scaled_flower = X_scaled[0]\n\n # extract the two PC's\n first_Pc = pca.components_[0]\n second_Pc = pca.components_[1]\n\n first_scaled_flower.shape # (4,)\n print first_scaled_flower # array([-0.90068117, 1.03205722, -1.3412724 , -1.31297673])\n\n # same result as the first row of our matrix multiplication\n np.dot(first_scaled_flower, first_Pc), np.dot(first_scaled_flower, second_Pc)\n\n (-2.2645417283949003, 0.50570390277378274)\n```", "```py\n# This is how the transform method works in pca\n pca.transform(X_scaled)[:5,]\n\n array([[-2.26454173, 0.5057039 ], [-2.0864255 , -0.65540473], [-2.36795045, -0.31847731], [-2.30419716, -0.57536771], [-2.38877749, 0.6747674 ]])\n```", "```py\n[ 0.52237162, -0.26335492, 0.58125401, 0.56561105]\n```", "```py\n[-0.90068117, 1.03205722, -1.3412724 , -1.31297673]\n```", "```py\n# cut out last two columns of the original iris dataset\n iris_2_dim = iris_X[:,2:4]\n\n # center the data\n iris_2_dim = iris_2_dim - iris_2_dim.mean(axis=0)\n\n plot(iris_2_dim, iris_y, \"Iris: Only 2 dimensions\", \"sepal length\", \"sepal width\")\n```", "```py\n# instantiate a PCA of 2 components\n twodim_pca = PCA(n_components=2)\n\n # fit and transform our truncated iris data\n iris_2_dim_transformed = twodim_pca.fit_transform(iris_2_dim)\n\n plot(iris_2_dim_transformed, iris_y, \"Iris: PCA performed on only 2 dimensions\", \"PCA1\", \"PCA2\")\n```", "```py\n# This code is graphing both the original iris data and the projected version of it using PCA.\n # Moreover, on each graph, the principal components are graphed as vectors on the data themselves\n # The longer of the arrows is meant to describe the first principal component and\n # the shorter of the arrows describes the second principal component\n def draw_vector(v0, v1, ax):\n arrowprops=dict(arrowstyle='->',linewidth=2,\n shrinkA=0, shrinkB=0)\n ax.annotate('', v1, v0, arrowprops=arrowprops)\n\n fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\n # plot data\n ax[0].scatter(iris_2_dim[:, 0], iris_2_dim[:, 1], alpha=0.2)\n for length, vector in zip(twodim_pca.explained_variance_, twodim_pca.components_):\n v = vector * np.sqrt(length) # elongdate vector to match up to explained_variance\n draw_vector(twodim_pca.mean_,\n twodim_pca.mean_ + v, ax=ax[0])\n ax[0].set(xlabel='x', ylabel='y', title='Original Iris Dataset',\n xlim=(-3, 3), ylim=(-2, 2))\n\n ax[1].scatter(iris_2_dim_transformed[:, 0], iris_2_dim_transformed[:, 1], alpha=0.2)\n for length, vector in zip(twodim_pca.explained_variance_, twodim_pca.components_):\n transformed_component = twodim_pca.transform([vector])[0] # transform components to new coordinate system\n v = transformed_component * np.sqrt(length) # elongdate vector to match up to explained_variance\n draw_vector(iris_2_dim_transformed.mean(axis=0),\n iris_2_dim_transformed.mean(axis=0) + v, ax=ax[1])\n ax[1].set(xlabel='component 1', ylabel='component 2',\n title='Projected Data',\n xlim=(-3, 3), ylim=(-1, 1))\n\n```", "```py\n# calculate the mean for each class\n # to do this we will separate the iris dataset into three dataframes\n # one for each flower, then we will take one's mean columnwise\n mean_vectors = []\n for cl in [0, 1, 2]:\n class_mean_vector = np.mean(iris_X[iris_y==cl], axis=0)\n mean_vectors.append(class_mean_vector)\n print label_dict[cl], class_mean_vector\n\n setosa [ 5.006 3.418 1.464 0.244]\n versicolor [ 5.936 2.77 4.26 1.326]\n virginica [ 6.588 2.974 5.552 2.026]\n```", "```py\n# Calculate within-class scatter matrix\n S_W = np.zeros((4,4))\n # for each flower\n for cl,mv in zip([0, 1, 2], mean_vectors):\n # scatter matrix for every class, starts with all 0's\n class_sc_mat = np.zeros((4,4))\n # for each row that describes the specific flower\n for row in iris_X[iris_y == cl]:\n # make column vectors\n row, mv = row.reshape(4,1), mv.reshape(4,1)\n # this is a 4x4 matrix\n class_sc_mat += (row-mv).dot((row-mv).T)\n # sum class scatter matrices\n S_W += class_sc_mat\n\n S_W\n\n array([[ 38.9562, 13.683 , 24.614 , 5.6556], [ 13.683 , 17.035 , 8.12 , 4.9132], [ 24.614 , 8.12 , 27.22 , 6.2536], [ 5.6556, 4.9132, 6.2536, 6.1756]])\n\n # calculate the between-class scatter matrix\n\n # mean of entire dataset\n overall_mean = np.mean(iris_X, axis=0).reshape(4,1)\n\n # will eventually become between class scatter matrix\n S_B = np.zeros((4,4))\n for i,mean_vec in enumerate(mean_vectors):\n # number of flowers in each species\n n = iris_X[iris_y==i,:].shape[0]\n # make column vector for each specied\n mean_vec = mean_vec.reshape(4,1)\n S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n\n S_B\n\n array([[ 63.2121, -19.534 , 165.1647, 71.3631], [ -19.534 , 10.9776, -56.0552, -22.4924], [ 165.1647, -56.0552, 436.6437, 186.9081], [ 71.3631, -22.4924, 186.9081, 80.6041]])\n```", "```py\n# calculate eigenvalues and eigenvectors of S−1W x SB\n eig_vals, eig_vecs = np.linalg.eig(np.dot(np.linalg.inv(S_W), S_B))\n eig_vecs = eig_vecs.real\n eig_vals = eig_vals.real\n\n for i in range(len(eig_vals)):\n eigvec_sc = eig_vecs[:,i]\n print 'Eigenvector {}: {}'.format(i+1, eigvec_sc)\n print 'Eigenvalue {:}: {}'.format(i+1, eig_vals[i])\n print\n\n Eigenvector 1: [-0.2049 -0.3871 0.5465 0.7138]\n Eigenvalue 1: 32.2719577997 Eigenvector 2: [ 0.009 0.589 -0.2543 0.767 ] Eigenvalue 2: 0.27756686384 Eigenvector 3: [ 0.2771 -0.3863 -0.4388 0.6644] Eigenvalue 3: -6.73276389619e-16 . # basically 0 Eigenvector 4: [ 0.2771 -0.3863 -0.4388 0.6644] Eigenvalue 4: -6.73276389619e-16 . # basically 0\n```", "```py\n# keep the top two linear discriminants\n linear_discriminants = eig_vecs.T[:2]\n\n linear_discriminants\n\n array([[-0.2049, -0.3871, 0.5465, 0.7138], [ 0.009 , 0.589 , -0.2543, 0.767 ]])\n```", "```py\n#explained variance ratios\n eig_vals / eig_vals.sum()\n\n array([ .99147, .0085275, -2.0685e-17, -2.0685e-17])\n```", "```py\n# LDA projected data\n lda_iris_projection = np.dot(iris_X, linear_discriminants.T)\n lda_iris_projection[:5,]\n\n plot(lda_iris_projection, iris_y, \"LDA Projection\", \"LDA1\", \"LDA2\")\n```", "```py\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n```", "```py\n# instantiate the LDA module\n lda = LinearDiscriminantAnalysis(n_components=2)\n\n # fit and transform our original iris data\n X_lda_iris = lda.fit_transform(iris_X, iris_y)\n\n # plot the projected data\n plot(X_lda_iris, iris_y, \"LDA Projection\", \"LDA1\", \"LDA2\")\n```", "```py\n# essentially the same as pca.components_, but transposed (4x2 instead of 2x4)\n lda.scalings_\n\n array([[ 0.81926852, 0.03285975], [ 1.5478732 , 2.15471106], [-2.18494056, -0.93024679], [-2.85385002, 2.8060046 ]])\n\n # same as manual calculations\n lda.explained_variance_ratio_\n array([ 0.9915, 0.0085])\n```", "```py\n# show that the sklearn components are just a scalar multiplication from the manual components we calculateda\n for manual_component, sklearn_component in zip(eig_vecs.T[:2], lda.scalings_.T):\n print sklearn_component / manual_component\n\n [-3.9982 -3.9982 -3.9982 -3.9982] [ 3.6583 3.6583 3.6583 3.6583]\n```", "```py\n# fit our LDA to scaled data\n X_lda_iris = lda.fit_transform(X_scaled, iris_y)\n\n lda.scalings_ # different scalings when data are scaled\n\n array([[ 0.67614337, 0.0271192 ], [ 0.66890811, 0.93115101], [-3.84228173, -1.63586613], [-2.17067434, 2.13428251]])\n```", "```py\n# fit our LDA to our truncated iris dataset\n iris_2_dim_transformed_lda = lda.fit_transform(iris_2_dim, iris_y)\n```", "```py\n# project data\n iris_2_dim_transformed_lda[:5,]\n\n array([[-6.04248571, 0.07027756], [-6.04248571, 0.07027756], [-6.19690803, 0.28598813], [-5.88806338, -0.14543302], [-6.04248571, 0.07027756]])\n```", "```py\n# different notation\n components = lda.scalings_.T # transposing gives same style as PCA. We want rows to be components\n\n print components\n [[ 1.54422328 2.40338224] [-2.15710573 5.02431491]]\n\n np.dot(iris_2_dim, components.T)[:5,] # same as transform method\n\n array([[-6.04248571, 0.07027756], [-6.04248571, 0.07027756], [-6.19690803, 0.28598813], [-5.88806338, -0.14543302], [-6.04248571, 0.07027756]])\n```", "```py\n# original features are highly correlated\nnp.corrcoef(iris_2_dim.T)\n array([[ 1\\. , 0.9627571],\n [ 0.9627571, 1\\. ]])\n\n # new LDA features are highly uncorrelated, like in PCA\n np.corrcoef(iris_2_dim_transformed_lda.T)\n array([[ 1.00000000e+00, 1.03227536e-15], [ 1.03227536e-15, 1.00000000e+00]])\n```", "```py\n# This code is graphing both the original iris data and the projected version of it using LDA.\n # Moreover, on each graph, the scalings of the LDA are graphed as vectors on the data themselves\n # The longer of the arrows is meant to describe the first scaling vector and\n # the shorter of the arrows describes the second scaling vector\n def draw_vector(v0, v1, ax):\n arrowprops=dict(arrowstyle='->',\n linewidth=2,\n shrinkA=0, shrinkB=0)\n ax.annotate('', v1, v0, arrowprops=arrowprops)\n\n fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\n # plot data\n ax[0].scatter(iris_2_dim[:, 0], iris_2_dim[:, 1], alpha=0.2)\n for length, vector in zip(lda.explained_variance_ratio_, components):\n v = vector * .5\n draw_vector(lda.xbar_, lda.xbar_ + v, ax=ax[0]) # lda.xbar_ is equivalent to pca.mean_\n ax[0].axis('equal')\n ax[0].set(xlabel='x', ylabel='y', title='Original Iris Dataset',\n xlim=(-3, 3), ylim=(-3, 3))\n\n ax[1].scatter(iris_2_dim_transformed_lda[:, 0], iris_2_dim_transformed_lda[:, 1], alpha=0.2)\n for length, vector in zip(lda.explained_variance_ratio_, components):\n transformed_component = lda.transform([vector])[0]\n v = transformed_component * .1\n draw_vector(iris_2_dim_transformed_lda.mean(axis=0), iris_2_dim_transformed_lda.mean(axis=0) + v, ax=ax[1])\n ax[1].axis('equal')\n ax[1].set(xlabel='lda component 1', ylabel='lda component 2',\n title='Linear Discriminant Analysis Projected Data',\n xlim=(-10, 10), ylim=(-3, 3))\n```", "```py\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\n```", "```py\n# Create a PCA module to keep a single component\n single_pca = PCA(n_components=1)\n\n # Create a LDA module to keep a single component\n single_lda = LinearDiscriminantAnalysis(n_components=1)\n\n # Instantiate a KNN model\n knn = KNeighborsClassifier(n_neighbors=3)\n```", "```py\n# run a cross validation on the KNN without any feature transformation\n knn_average = cross_val_score(knn, iris_X, iris_y).mean()\n\n # This is a baseline accuracy. If we did nothing, KNN on its own achieves a 98% accuracy\n knn_average\n\n 0.98039215686274517\n```", "```py\nlda_pipeline = Pipeline([('lda', single_lda), ('knn', knn)])\n lda_average = cross_val_score(lda_pipeline, iris_X, iris_y).mean()\n\n # better prediction accuracy than PCA by a good amount, but not as good as original\n lda_average\n\n 0.9673202614379085\n```", "```py\n# create a pipeline that performs PCA\n pca_pipeline = Pipeline([('pca', single_pca), ('knn', knn)])\n\n pca_average = cross_val_score(pca_pipeline, iris_X, iris_y).mean()\n\n pca_average\n\n 0.8941993464052288\n```", "```py\n# try LDA with 2 components\n lda_pipeline = Pipeline([('lda', LinearDiscriminantAnalysis(n_components=2)),\n ('knn', knn)])\n\n lda_average = cross_val_score(lda_pipeline, iris_X, iris_y).mean()\n\n # Just as good as using original data\n lda_average\n\n 0.98039215686274517\n```", "```py\n# compare our feature transformation tools to a feature selection tool\n from sklearn.feature_selection import SelectKBest\n # try all possible values for k, excluding keeping all columns\n for k in [1, 2, 3]:\n # make the pipeline\n select_pipeline = Pipeline([('select', SelectKBest(k=k)), ('knn', knn)])\n # cross validate the pipeline\n select_average = cross_val_score(select_pipeline, iris_X, iris_y).mean()\n print k, \"best feature has accuracy:\", select_average\n\n # LDA is even better than the best selectkbest\n 1 best feature has accuracy: 0.953839869281 2 best feature has accuracy: 0.960784313725 3 best feature has accuracy: 0.97385620915\n```", "```py\ndef get_best_model_and_accuracy(model, params, X, y):\n    grid = GridSearchCV(model, # the model to grid search\n                        params, # the parameter set to try\n                        error_score=0.) # if a parameter set raises an error, continue and set the performance as a big, fat 0\n\n    grid.fit(X, y) # fit the model and parameters\n    # our classical metric for performance\n    print \"Best Accuracy: {}\".format(grid.best_score_)\n    # the best parameters that caused the best accuracy\n    print \"Best Parameters: {}\".format(grid.best_params_)\n    # the average time it took a model to fit to the data (in seconds)\n    avg_time_fit = round(grid.cv_results_['mean_fit_time'].mean(), 3)\n    print \"Average Time to Fit (s): {}\".format(avg_time_fit)\n    # the average time it took a model to predict out of sample data (in seconds)\n    # this metric gives us insight into how this model will perform in real-time analysis\n    print \"Average Time to Score (s): {}\".format(round(grid.cv_results_['mean_score_time'].mean(), 3))\n```", "```py\n\nfrom sklearn.model_selection import GridSearchCV\niris_params = {\n     'preprocessing__scale__with_std': [True, False],\n     'preprocessing__scale__with_mean': [True, False],\n     'preprocessing__pca__n_components':[1, 2, 3, 4], # according to scikit-learn docs, max allowed n_components for LDA is number of classes - 1\n     'preprocessing__lda__n_components':[1, 2],\n\n     'clf__n_neighbors': range(1, 9)\n}\n# make a larger pipeline\npreprocessing = Pipeline([('scale', StandardScaler()), ('pca', PCA()), ('lda', LinearDiscriminantAnalysis())])\n\niris_pipeline = Pipeline(steps=[('preprocessing', preprocessing),('clf', KNeighborsClassifier())])\n\nget_best_model_and_accuracy(iris_pipeline, iris_params, iris_X, iris_y)\n\nBest Accuracy: 0.986666666667 Best Parameters: {'preprocessing__scale__with_std': False, 'preprocessing__pca__n_components': 3, 'preprocessing__scale__with_mean': True, 'preprocessing__lda__n_components': 2, 'clf__n_neighbors': 3} Average Time to Fit (s): 0.002 Average Time to Score (s): 0.001\n```"]