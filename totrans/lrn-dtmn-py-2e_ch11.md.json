["```py\nimport os\ndata_folder = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"cifar-10-batches-py\") \nbatch1_filename = os.path.join(data_folder, \"data_batch_1\")\n\n```", "```py\nimport pickle\n# Bugfix thanks to: http://stackoverflow.com/questions/11305790/pickle-incompatability-of-numpy-arrays-between-python-2-and-3 \ndef unpickle(filename): \n    with open(filename, 'rb') as fo: \n        return pickle.load(fo, encoding='latin1')\n\n```", "```py\nbatch1 = unpickle(batch1_filename)\n\n```", "```py\nimage_index = 100 \nimage = batch1['data'][image_index]\n\n```", "```py\nimage = image.reshape((32,32, 3), order='F') \nimport numpy as np \nimage = np.rot90(image, -1)\n\n```", "```py\n%matplotlib inline\n\nfrom matplotlib import pyplot as plt \nplt.imshow(image)\n\n```", "```py\nimport tensorflow as tf\n\n# Define the parameters of the equation as constant values\na = tf.constant(5.0)\nb = tf.constant(4.5)\nc = tf.constant(3.0)\n\n# Define the variable x, which lets its value be changed\nx = tf.Variable(0., name='x')  # Default of 0.0\n\n# Define the output y, which is an operation on a, b, c and x\ny = (a * x ** 2) + (b * x) + c\n\n```", "```py\nfrom IPython.display import clear_output, Image, display, HTML\n\ndef strip_consts(graph_def, max_const_size=32):\n    \"\"\"Strip large constant values from graph_def.\"\"\"\n    strip_def = tf.GraphDef()\n    for n0 in graph_def.node:\n        n = strip_def.node.add() \n        n.MergeFrom(n0)\n        if n.op == 'Const':\n            tensor = n.attr['value'].tensor\n            size = len(tensor.tensor_content)\n            if size > max_const_size:\n                tensor.tensor_content = \"<stripped %d bytes>\"%size\n    return strip_def\n\ndef show_graph(graph_def, max_const_size=32):\n    \"\"\"Visualize TensorFlow graph.\"\"\"\n    if hasattr(graph_def, 'as_graph_def'):\n        graph_def = graph_def.as_graph_def()\n    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n    code = \"\"\"\n        <script>\n          function load() {{\n            document.getElementById(\"{id}\").pbtxt = {data};\n          }}\n        </script>\n        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n        <div style=\"height:600px\">\n          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n        </div>\n    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n\n    iframe = \"\"\"\n        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n    \"\"\".format(code.replace('\"', '&quot;'))\n    display(HTML(iframe))\n\n```", "```py\nshow_graph(tf.get_default_graph().as_graph_def())\n\n```", "```py\nmodel = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(model)\n    result = session.run(y)\nprint(result)\n\n```", "```py\nmodel = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(model)\n    session.run(x.assign(10))\n    result = session.run(y)\nprint(result)\n\n```", "```py\nimport numpy as np\nfrom sklearn.datasets import load_iris \niris = load_iris() \nX = iris.data.astype(np.float32) \ny_true = iris.target.astype(np.int32)\n\n```", "```py\nfrom sklearn.preprocessing import OneHotEncoder\n\ny_onehot = OneHotEncoder().fit_transform(y_true.reshape(-1, 1))\ny_onehot = y_onehot.astype(np.int64).todense()\n\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_onehot, random_state=14)\n\n```", "```py\ninput_layer_size, hidden_layer_size, output_layer_size = 4, 6, 3\n\n```", "```py\nfrom keras.layers import Dense\nhidden_layer = Dense(output_dim=hidden_layer_size, input_dim=input_layer_size, activation='relu')\noutput_layer = Dense(output_layer_size, activation='sigmoid')\n\n```", "```py\nfrom keras.models import Sequential\nmodel = Sequential(layers=[hidden_layer, output_layer])\n\n```", "```py\nmodel.compile(loss='mean_squared_error',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n```", "```py\nhistory = model.fit(X_train, y_train)\n\n```", "```py\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nplt.plot(history.epoch, history.history['loss'])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\n```", "```py\nfrom sklearn.metrics import classification_report\ny_pred = model.predict_classes(X_test)\nprint(classification_report(y_true=y_test.argmax(axis=1), y_pred=y_pred))\n\n```", "```py\nhistory = model.fit(X_train, y_train, nb_epoch=1000, verbose=False)\n\n```", "```py\ny_pred = model.predict_classes(X_test)\nprint(classification_report(y_true=y_test.argmax(axis=1), y_pred=y_pred))\n\n```", "```py\nimport numpy as np \nfrom PIL import Image, ImageDraw, ImageFont \nfrom skimage import transform as tf\n\ndef create_captcha(text, shear=0, size=(100, 30), scale=1):\n    im = Image.new(\"L\", size, \"black\")\n    draw = ImageDraw.Draw(im)\n    font = ImageFont.truetype(r\"bretan/Coval-Black.otf\", 22) \n    draw.text((0, 0), text, fill=1, font=font)\n    image = np.array(im)\n    affine_tf = tf.AffineTransform(shear=shear)\n    image = tf.warp(image, affine_tf)\n    image = image / image.max()\n    shape = image.shape\n    # Apply scale\n    shapex, shapey = (shape[0] * scale, shape[1] * scale)\n    image = tf.resize(image, (shapex, shapey))\n    return image\n\nfrom skimage.measure import label, regionprops\nfrom skimage.filters import threshold_otsu\nfrom skimage.morphology import closing, square\n\ndef segment_image(image):\n    # label will find subimages of connected non-black pixels\n    labeled_image = label(image>0.2, connectivity=1, background=0)\n    subimages = []\n    # regionprops splits up the subimages\n    for region in regionprops(labeled_image):\n        # Extract the subimage\n        start_x, start_y, end_x, end_y = region.bbox\n        subimages.append(image[start_x:end_x,start_y:end_y])\n    if len(subimages) == 0:\n        # No subimages found, so return the entire image\n        return [image,]\n    return subimages\n\nfrom sklearn.utils import check_random_state\nrandom_state = check_random_state(14) \nletters = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\nassert len(letters) == 26\nshear_values = np.arange(0, 0.8, 0.05)\nscale_values = np.arange(0.9, 1.1, 0.1)\n\ndef generate_sample(random_state=None): \n    random_state = check_random_state(random_state) \n    letter = random_state.choice(letters) \n    shear = random_state.choice(shear_values)\n    scale = random_state.choice(scale_values)\n    return create_captcha(letter, shear=shear, size=(30, 30), scale=scale), letters.index(letter)\n\ndataset, targets = zip(*(generate_sample(random_state) for i in range(1000)))\ndataset = np.array([tf.resize(segment_image(sample)[0], (20, 20)) for sample in dataset])\ndataset = np.array(dataset, dtype='float') \ntargets = np.array(targets)\n\nfrom sklearn.preprocessing import OneHotEncoder \nonehot = OneHotEncoder() \ny = onehot.fit_transform(targets.reshape(targets.shape[0],1))\ny = y.todense()\n\nX = dataset.reshape((dataset.shape[0], dataset.shape[1] * dataset.shape[2]))\n\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9)\n\n```", "```py\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nhidden_layer = Dense(100, input_dim=X_train.shape[1])\noutput_layer = Dense(y_train.shape[1])\n# Create the model\nmodel = Sequential(layers=[hidden_layer, output_layer])\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n\n```", "```py\nmodel.fit(X_train, y_train, nb_epoch=1000, verbose=False)\ny_pred = model.predict(X_test)\n\n```", "```py\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_pred=y_pred.argmax(axis=1),\ny_true=y_test.argmax(axis=1)))\n\n```", "```py\nssh -i <certificante_name>.pem ubuntu@<server_ip_address>\n\n```", "```py\n$ cd~/\n\n$ nano chapter11script.py\n\n```", "```py\n$ wget http://openfontlibrary.org/assets/downloads/bretan/680bc56bbeeca95353ede363a3744fdf/bretan.zip\n\n$ sudo apt-get install unzip\n\n$ unzip -p bretan.zip\n\n```", "```py\n$ python chapter11script.py\n\n```", "```py\nimport os\nimport numpy as np \n\ndata_folder = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"cifar-10-batches-py\")\n\nbatches = [] \nfor i in range(1, 6):\n    batch_filename = os.path.join(data_folder, \"data_batch_{}\".format(i))\n    batches.append(unpickle(batch_filename)) \n    break\n\n```", "```py\nX = np.vstack([batch['data'] for batch in batches])\n\n```", "```py\nX = np.array(X) / X.max() \nX = X.astype(np.float32)\n\n```", "```py\nfrom keras.utils import np_utils\ny = np.hstack(batch['labels'] for batch in batches).flatten()\nnb_classes = len(np.unique(y))\ny = np_utils.to_categorical(y, nb_classes)\n\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n```", "```py\nX_train = X_train.reshape(-1, 3, 32, 32)\nX_test = X_test.reshape(-1, 3, 32, 32)\nn_samples, d, h, w = X_train.shape  # Obtain dataset dimensions\n# Convert to floats and ensure data is normalised.\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n\n```", "```py\nfrom keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D\nconv1 = Convolution2D(32, 3, 3, input_shape=(d, h, w), activation='relu')\npool1 = MaxPooling2D()\nconv2 = Convolution2D(64, 2, 2, activation='relu')\npool2 = MaxPooling2D()\nconv3 = Convolution2D(128, 2, 2, activation='relu')\npool3 = MaxPooling2D()\nflatten = Flatten()\nhidden4 = Dense(500, activation='relu')\nhidden5 = Dense(500, activation='relu')\noutput = Dense(nb_classes, activation='softmax')\nlayers = [conv1, pool1,\n          conv2, pool2,\n          conv3, pool3,\n          flatten, hidden4, hidden5,\n          output]\n\n```", "```py\nmodel = Sequential(layers=layers)\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\nimport tensorflow as tf\nhistory = model.fit(X_train, y_train, nb_epoch=25, verbose=True,\nvalidation_data=(X_test, y_test),batch_size=1000))\n\n```", "```py\ny_pred = model.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_pred=y_pred.argmax(axis=1),\n y_true=y_test.argmax(axis=1)))\n\n```", "```py\n$ wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n\n```", "```py\n$ mkdir Data\n\n$ tar -zxf cifar-10-python.tar.gz -C Data\n\n```", "```py\n$ python3 chapter11cifar.py\n\n```", "```py\n0.8497\n\n```"]