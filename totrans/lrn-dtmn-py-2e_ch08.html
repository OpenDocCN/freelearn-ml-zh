<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Beating CAPTCHAs with Neural Networks</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Beating CAPTCHAs with Neural Networks</h1>
            </header>

            <article>
                
<p>Images pose interesting and difficult challenges for data miners. Until recently, only small amounts of progress were made with analyzing images for extracting information. However recently, such as with the progress made on self-driving cars, significant advances have been made in a very short time-frame. The latest research is providing algorithms that can understand images for commercial surveillance, self-driving vehicles, and person identification.</p>
<p>There is lots of raw data in an image, and the standard method for encoding images - pixels - isn't that informative by itself. Images and photos can be blurry, too close to the targets, too dark, too light, scaled, cropped, skewed, or any other of a variety of problems that cause havoc for a computer system trying to extract useful information. Neural networks can combine these lower level features into higher level patterns that are more able to generalize and deal with these issues.</p>
<p>In this chapter, we look at extracting text data from images by using neural networks for predicting each letter in the CAPTCHA. CAPTCHAs are images designed to be easy for humans to solve and hard for a computer to solve, as per the acronym: <strong>Completely Automated Public Turing test to tell Computers and Humans Apart</strong>. Many websites use them for registration and commenting systems to stop automated programs&#160;flooding their site with fake accounts and spam comments.</p>
<p>These tests help stop programs (bots) using websites, such as a bot intent on automatically signing up new people to a website. We play the part of such a spammer, trying to get around a CAPTCHA-protected system for posting messages to an online forum. The website is protected by a CAPTCHA, meaning we can't post unless we pass the test.</p>
<p>The topics covered in this chapter include:</p>
<ul>
<li>Neural networks</li>
<li>Creating our own dataset of CAPTCHAs and letters</li>
<li>The scikit-image library for working with image data</li>
<li>Extracting basic features from images</li>
<li>Using neural networks for larger-scale classification tasks</li>
<li>Improving performance using postprocessing</li>
<li><span>Artificial neural networks</span></li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Artificial neural networks</h1>
            </header>

            <article>
                
<p>Neural networks are a class of algorithm that was originally designed based on the way that human brains work. However, modern advances are generally based on mathematics rather than biological insights. A neural network is a collection of neurons that are connected together. Each neuron is a simple function of its inputs, which are combined using some function to generate an output:</p>
<div class="CDPAlignCenter CDPAlign"><img height="98" width="185" class="image-border" src="images/B06162_08_01.jpg"/></div>
<p>The functions that define a neuron's processing can be any standard function, such as a linear combination of the inputs, and is called the <strong>activation function</strong>. For the commonly used learning algorithms to work, we need the activation function to be <em>derivable</em> and&#160; <em>smooth</em>. A frequently used activation function is the <strong>logistic function</strong>, which is defined by the following equation (<em>k</em> is often simply 1, <em>x</em> is the inputs into the neuron, and L is normally 1, that is, the maximum value of the function):</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/B06162_08_02.png"/></div>
<p>The value of this graph, from -6 to +6, is shown below.&#160;The red lines indicate that the value is 0.5 when <em>x</em> is zero, but the function quickly climbs to 1.0 as x increases, and quickly drops to -1.0 when x decreases.</p>
<div class="CDPAlignCenter CDPAlign"><img height="284" width="420" class="image-border" src="images/B06162_08_03.png"/></div>
<p>Each individual neuron receives its inputs and then computes the output based on these values. Neural networks can be considered as a collection of these neurons connected together, and they can be very powerful for data mining applications. The combinations of these neurons, how they fit together, and how they combine to learn a model are one of the most powerful concepts in machine learning.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">An introduction to neural networks</h1>
            </header>

            <article>
                
<p>For data mining applications, the arrangement of neurons is usually in <strong>layers</strong>. The first layer is called the <strong>input layer</strong>&#160;and takes its input from samples in the data. The outputs of each of these neurons are computed and then passed along to the neurons in the next layer. This is called a <strong>feed-forward neural network</strong>. We will refer to these simply as <strong>neural networks</strong> for this chapter, as they are the most common type used and the only type used in this chapter. There are other types of neural networks too that are used for different applications. We will see another type of network in <a href="lrn-dtmn-py-2e_ch11.html">Chapter 11</a><em>, Object Detection in Images Using Deep Neural Networks</em>.</p>
<p>The outputs of one layer become the inputs of the next layer, continuing until we reach the final layer: the&#160;<strong>output layer</strong>. These outputs represent the predictions of the neural network as the classification. Any layer of neurons between the input layer and the output layer is referred to as a&#160;<strong>hidden layer</strong>, as they learn a representation of the data not intuitively interpretable by humans. Most neural networks have at least three layers, although most modern applications use networks with many more layers than that.</p>
<div class="CDPAlignCenter CDPAlign"><img height="140" width="180" class="image-border" src="images/B06162_08_04.jpg"/></div>
<p>Typically, we consider fully connected layers. The outputs of each neuron in a layer go to all neurons in the next layer. While we do define a fully connected network, many of the weights will be set to zero during the training process, effectively removing these links. Additionally, many of these weights might retain very small values, even after training.</p>
<p>In addition to being one of the conceptually simpler forms for neural networks, fully connected neural networks are also simpler and more efficient to program than other connection patterns.</p>
<div class="packt_infobox">See <a href="lrn-dtmn-py-2e_ch11.html">Chapter 11</a>, <em>Object Detection in images using Deep Neural Networks</em>,&#160; for an investigation into different types of neural networks, including layers built specifically for image processing.</div>
<p>As the function of the neurons is normally the logistic function, and the neurons are fully connected to the next layer, the parameters for building and training a neural network must be other factors.</p>
<ul>
<li>The first factor for neural networks is in the building phase: the size and shape of the neural network. This includes how many layers the neural network has and how many neurons it has in each hidden layer (the size of the input and output layers is usually dictated by the dataset).</li>
<li>The second parameter for neural networks is determined in the training phase: the weight of the connections between neurons. When one neuron connects to another, this connection has an associated weight that is multiplied by the signal (the output of the first neuron). If the connection has a weight of 0.8, the neuron is activated, and it outputs a value of 1, the resulting input to the next neuron is 0.8. If the first neuron is not activated and has a value of 0, this stays at 0.</li>
</ul>
<p>The combination of an appropriately sized network and well-trained weights determines how accurate the neural network can be when making classifications. The word <em>appropriately</em> in the previous sentence also doesn't necessarily mean bigger, as neural networks that are too large can take a long time to train and can more easily over-fit the training data.</p>
<div class="packt_infobox">Weights can be set randomly to start with&#160;but are then updated during the training phase. Setting weights to zero is normally not a good idea, as all neurons in the network act similarly to begin with! Having randomly set weights gives each neuron a different&#160;<em>role</em> in the learning process that can be improved with training.</div>
<p>A neural network in this configuration is a classifier that can then be used to predict the target of a data sample based on the inputs, much like the classification algorithms we have used in previous chapters. But first, we need a dataset to train and test with.</p>
<div class="packt_tip packt_infobox">Neural networks are, by a margin, the biggest area of advancement in data mining in recent years. This might make you think:&#160;<em>Why bother learning any other type of classification algorithm?</em> While neural networks are state of the art in pretty much every domain (at least, right now), the reason to learn other classifiers is that neural networks often require larger amounts of data to work well, and they take a long time to learn. If you don't have <strong>big data</strong>, you will probably get better results from another algorithm.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Creating the dataset</h1>
            </header>

            <article>
                
<p>In this chapter, to spice up things a little, let us take on the role of the bad guy. We want to create a program that can beat CAPTCHAs, allowing our comment spam program to advertise on someone's website. It should be noted that our CAPTCHAs will be a little easier than those used on the web today and that spamming isn't a very nice thing to do.</p>
<div class="packt_tip">We play the bad guy today, but please <em>don't</em> use this against real world sites. One reason to "play the bad guy" is to help improve the security of our website, by looking for issues with it.</div>
<p>Our experiment will simplify a CAPTCHA to be individual English words of four letters only, as shown in the following image:</p>
<div class="CDPAlignCenter CDPAlign"><img height="103" width="335" src="images/B06162_08_05.png"/></div>
<p>Our goal will be to create a program that can recover the word from images like this. To do this, we will use four steps:</p>
<ol>
<li>Break the image into individual letters.</li>
<li>Classify each individual letter.</li>
<li>Recombine the letters to form a word.</li>
<li>Rank words with a dictionary to try to fix errors.</li>
</ol>
<div class="packt_tip">Our CAPTCHA-busting algorithm will make the following assumptions. First, the word will be a whole and valid four-character English word (in fact, we use the same dictionary for creating and busting CAPTCHAs). Second, the word will only contain uppercase letters. No symbols, numbers, or spaces will be used.</div>
<p>We are going to make the problem slightly harder than simply identifying letters, by performing a shear transform to the text, along with varying rates of shearing&#160;and scaling.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Drawing basic CAPTCHAs</h1>
            </header>

            <article>
                
<p>Before we can start classifying CAPTCHAs, we first need a dataset to learn from. In this section, we will be generating our own data to perform the data mining on.</p>
<div class="packt_tip">In more real-world applications, you'll be wanting to use an existing CAPTCHA service to generate the data, but for our purposes in this chapter, our own data will be sufficient. One of the issues that can arise is that we code in our assumptions around how the data works when we create the dataset ourselves, and then carry those same assumptions over to our data mining training.</div>
<p>Our goal here is to draw an image with a word on it, along with a shear transform. We are going to use the PIL library to draw our CAPTCHAs and the <kbd>scikit-image</kbd> library to perform the shear transform. The <kbd>scikit-image</kbd> library can read images in a NumPy array format that PIL can export to, allowing us to use both libraries.</p>
<div class="packt_tip">Both PIL and scikit-image can be installed via Anaconda. However, I recommend getting PIL through its replacement called <strong>pillow</strong>:<br/>
<span class="packt_screen">conda install pillow scikit-image</span></div>
<p>First, we import the necessary libraries and modules. We import NumPy and the Image drawing functions as follows:</p>
<pre>import numpy as np <br/>from PIL import Image, ImageDraw, ImageFont <br/>from skimage import transform as tf
</pre>
<p>Then we create our base function for generating CAPTCHAs. This function takes a word and a shear value (which is normally between 0 and 0.5) to return an image in a NumPy array format. We allow the user to set the size of the resulting image, as we will use this function for single-letter training samples as well:</p>
<pre>def create_captcha(text, shear=0, size=(100, 30), scale=1):<br/>    im = Image.new("L", size, "black")<br/>    draw = ImageDraw.Draw(im)<br/>    font = ImageFont.truetype(r"bretan/Coval-Black.otf", 22) <br/>    draw.text((0, 0), text, fill=1, font=font)<br/>    image = np.array(im)<br/>    affine_tf = tf.AffineTransform(shear=shear)<br/>    image = tf.warp(image, affine_tf)<br/>    image = image / image.max()<br/>    # Apply scale<br/>    shape = image.shape<br/>    shapex, shapey = (int(shape[0] * scale), int(shape[1] * scale))<br/>    image = tf.resize(image, (shapex, shapey))<br/>    return image
</pre>
<p>In this function, we create a new image using L for the format, which means black-and-white pixels only, and create an instance of the <kbd>ImageDraw</kbd> class. This allows us to draw on this image using PIL. We then load the font, draw the text, and perform a <kbd>scikit-image</kbd> shear transform on it.&#160;</p>
<div class="packt_infobox">You can get the Coval font I used from the Open Font Library at:<br/>
<a href="http://openfontlibrary.org/en/font/bretan">http://openfontlibrary.org/en/font/bretan</a><br/>
Download the <kbd>.zip</kbd> file and extract the <kbd>Coval-Black.otf</kbd> file into the same directory as your Notebook.</div>
<p>From here, we can now generate images quite easily and use <kbd>pyplot</kbd> to display them. First, we use our inline display for the matplotlib graphs and import <kbd>pyplot</kbd>. The code is as follows:</p>
<pre>%matplotlib inline<br/>from matplotlib import pyplot as plt<br/>image = create_captcha("GENE", shear=0.5, scale=0.6)<br/>plt.imshow(image, cmap='Greys')
</pre>
<p>The result is the image shown at the start of this section: our CAPTCHA. Here are some other examples with different shear and scale values:</p>
<pre>image = create_captcha("SEND", shear=0.1, scale=1.0)<br/>plt.imshow(image, cmap='Greys')
</pre>
<div class="CDPAlignCenter CDPAlign"><img height="149" width="394" src="images/B06162_08_12-2-e1493021528419.png"/></div>
<pre>image = create_captcha("BARK", shear=0.8, scale=1.0)<br/>plt.imshow(image, cmap='Greys')
</pre>
<div class="CDPAlignCenter CDPAlign"><img height="144" width="393" src="images/B06162_08_13-e1493021566785.png"/></div>
<p>Here is a variant scaled to <kbd>1.5</kbd> sized. While it looks similar to the BONE image above, note the <em>x</em>-axis and <em>y</em>-axis values are larger:</p>
<pre>image = create_captcha("WOOF", shear=0.25, scale=1.5)<br/>plt.imshow(image, cmap='Greys')
</pre>
<div class="CDPAlignCenter CDPAlign"><img height="151" width="413" src="images/B06162_08_14-e1493021653291.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Splitting the image into individual letters</h1>
            </header>

            <article>
                
<p>Our CAPTCHAs are words. Instead of building a classifier that can identify the thousands and thousands of possible words, we will break the problem down into a smaller problem: predicting letters.</p>
<div class="packt_infobox">Our experiment is in English, and all uppercase, meaning we have 26 classes to predict from for each letter. If you try these experiments in other languages, keep in mind the number of output classes will have to change.</div>
<p>The first step in our algorithm for beating these CAPTCHAs involves segmenting the word to discover each of the letters within it. To do this, we are going to create a function that finds contiguous sections of black pixels in the image and extract them as subimages. These are (or at least should be) our letters. The <kbd>scikit-image</kbd> function has tools for performing these operations.</p>
<p>Our function will take an image, and return a list of sub-images, where each sub-image is a letter from the original word in the image.&#160;The first thing we need to do is to detect where each letter is. To do this, we will use the label function in <kbd>scikit-image</kbd>, which finds connected sets of pixels that have the same value. This has analogies to our connected component discovery in <a href="lrn-dtmn-py-2e_ch07.html">Chapter 7</a><em>, Follow Recommendations Using Graph Mining</em>.</p>
<pre>from skimage.measure import label, regionprops<br/><br/>def segment_image(image):<br/>    # label will find subimages of connected non-black pixels<br/>    labeled_image = label(image&gt;0.2, connectivity=1, background=0)<br/>    subimages = []<br/>    # regionprops splits up the subimages<br/>    for region in regionprops(labeled_image):<br/>        # Extract the subimage<br/>        start_x, start_y, end_x, end_y = region.bbox<br/>        subimages.append(image[start_x:end_x,start_y:end_y])<br/>        if len(subimages) == 0:<br/>            # No subimages found, so return the entire image<br/>            return [image,]<br/>    return subimages
</pre>
<p>We can then get the subimages from the example CAPTCHA using this function:</p>
<pre>subimages = segment_image(image)
</pre>
<p>We can also view each of these subimages:</p>
<pre>f, axes = plt.subplots(1, len(subimages), figsize=(10, 3)) <br/>for i in range(len(subimages)): <br/>    axes[i].imshow(subimages[i], cmap="gray")
</pre>
<p>The result will look something like this:</p>
<div class="CDPAlignCenter CDPAlign"><img height="117" width="473" src="images/B06162_08_11.jpg"/></div>
<p>As you can see, our image segmentation does a reasonable job, but the results are still quite messy, with bits of previous letters showing. This is fine, and almost preferable. While training on data with regular noise makes our training worse, training on data with random noise can actually make it better. One reason is that the underlying data mining model learns the important aspects, namely the non-noise parts instead of specific noise inherent in the training data set. It is a fine line between too much and too little noise, and this can be hard to properly model. Testing on validation sets is a good way to ensure your training is improving.</p>
<p>One important note is that this code is not consistent in finding letters. Lower shear values typically result in accurately segmented images. For example, here is the code to segment the WOOF example from above:</p>
<pre>image = create_captcha("WOOF", shear=0.25, scale=1.5)<br/>subimages = segment_image(image)<br/>f, axes = plt.subplots(1, len(subimages), figsize=(10, 3), sharey=True) <br/>for i in range(len(subimages)): <br/>    axes[i].imshow(subimages[i], cmap="gray")
</pre>
<div class="CDPAlignCenter CDPAlign"><img height="165" width="475" src="images/B06162_08_15-e1493021829708.png"/></div>
<p>In contrast, higher shear values are not segmented&#160;properly. For example, here is the BARK example from before:</p>
<div class="CDPAlignCenter CDPAlign"><img height="176" width="587" src="images/B06162_08_16.png"/></div>
<p>Notice the large overlap caused by the square segmentation. One suggestion for an improvement on this chapter's code is to improve our segmentation by finding non-square segments.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Creating a training dataset</h1>
            </header>

            <article>
                
<p>Using the functions we have already defined, we can now create a dataset of letters, each with different shear values. From this, we will train a neural network to recognize each letter from the image.</p>
<p>We first set up our random state and an array that holds the options for letters, shear values and scale values that we will randomly select from. There isn't much surprise here, but if you haven't used NumPy's <kbd>arange</kbd> function before, it is similar to Python's <kbd>range</kbd> function—except this one works with NumPy arrays and allows the step to be a float. The code is as follows:</p>
<pre>from sklearn.utils import check_random_state<br/>random_state = check_random_state(14) <br/>letters = list("ABCDEFGHIJKLMNOPQRSTUVWXYZ") <br/>shear_values = np.arange(0, 0.5, 0.05)<br/>scale_values = np.arange(0.5, 1.5, 0.1)
</pre>
<p>We then create a function (for generating a single sample in our training dataset) that randomly selects a letter, a shear value, and a scale value selected from the available options.</p>
<pre>def generate_sample(random_state=None): <br/>    random_state = check_random_state(random_state) <br/>    letter = random_state.choice(letters) <br/>    shear = random_state.choice(shear_values)<br/>    scale = random_state.choice(scale_values)<br/>    # We use 30,30 as the image size to ensure we get all the text in the image<br/>    return create_captcha(letter, shear=shear, size=(30, 30), scale=scale), letters.index(letter)
</pre>
<p>We return the image of the letter, along with the target value representing the letter in the image. Our classes will be 0 for A, 1 for B, 2 for C, and so on.</p>
<p>Outside the function block, we can now call this code to generate a new sample and then show it using <kbd>pyplot</kbd>:</p>
<pre>image, target = generate_sample(random_state) <br/>plt.imshow(image, cmap="Greys") <br/>print("The target for this image is: {0}".format(target))
</pre>
<p>The resulting image has just a single letter, with a random shear and random scale value.</p>
<div class="CDPAlignCenter CDPAlign"><img height="214" width="215" src="images/B06162_08_17-e1493023909718.png"/></div>
<p>We can now generate all of our data by calling this several thousand times. We then put the data into NumPy arrays, as they are easier to work with than lists. The code is as follows:</p>
<pre>dataset, targets = zip(*(generate_sample(random_state) for i in range(1000))) <br/>dataset = np.array([tf.resize(segment_image(sample)[0], (20, 20)) for sample in dataset])<br/>dataset = np.array(dataset, dtype='float') <br/>targets = np.array(targets)
</pre>
<p>Our targets are integer values between 0 and 26, with each representing a letter of the alphabet. Neural networks don't usually support multiple values from a single neuron, instead preferring to have multiple outputs, each with values 0 or 1. We&#160;perform one-hot-encoding of the targets, giving us a target array that has 26 outputs per sample, using values near 1 if that letter is likely and near 0 otherwise. The code is as follows:</p>
<pre>from sklearn.preprocessing import OneHotEncoder <br/>onehot = OneHotEncoder() <br/>y = onehot.fit_transform(targets.reshape(targets.shape[0],1))
</pre>
<p>From this output, we know that our neural network's output layer will have 26 neurons. The goal of the neural network is to determine which of these neurons to fire, based on a given input--the pixels that compose the image.</p>
<p>The library we are going to use doesn't support sparse arrays, so we need to turn our sparse matrix into a dense NumPy array. The code is as follows:</p>
<pre>y = y.todense()<br/>X = dataset.reshape((dataset.shape[0], dataset.shape[1] * dataset.shape[2]))
</pre>
<p>Finally, we perform a train/test split to later evaluate our data:</p>
<pre>from sklearn.cross_validation import train_test_split <br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9)
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Training and classifying</h1>
            </header>

            <article>
                
<p>We are now going to build a neural network that will take an image as input and try to predict which (single) letter is in the image.</p>
<p>We will use the training set of single letters we created earlier. The dataset itself is quite simple. We have a 20-by-20-pixel image, each pixel 1 (black) or 0 (white). These represent the 400 features that we will use as inputs into the neural network. The outputs will be 26 values between 0 and 1, where higher values indicate a higher likelihood that the associated letter (the first neuron is A, the second is B, and so on) is the letter represented by the input image.</p>
<p>We are going to use the scikit-learn's <kbd>MLPClassifier</kbd> for our neural network in this chapter.</p>
<div class="packt_infobox">You will need a recent version of <kbd>scikit-learn</kbd> to use MLPClassifier. If the below import statement fails, try again after updating scikit-learn. You can do this using the following Anaconda command:<br/>
&#160;<kbd>conda update scikit-learn</kbd></div>
<p>As for other <kbd>scikit-learn</kbd> classifiers, we import the model type and create a new one. The constructor below specifies that we create one hidden layer with 100 nodes in it. The size of the input and output layers is determined at training time:</p>
<pre>from sklearn.neural_network import MLPClassifier<br/>clf = MLPClassifier(hidden_layer_sizes=(100,), random_state=14)
</pre>
<p>To see the internal parameters of the neural network, we can use the <kbd>get_params()</kbd> function. This function exists on all <kbd>scikit-learn</kbd> models. Here is the output from the above model. Many of these parameters can improve training or the speed of training. For example, increasing the learning rate will train the model faster, at the risk of missing optimal values:</p>
<pre>{'activation': 'relu',<br/> 'alpha': 0.0001,<br/> 'batch_size': 'auto',<br/> 'beta_1': 0.9,<br/> 'beta_2': 0.999,<br/> 'early_stopping': False,<br/> 'epsilon': 1e-08,<br/> 'hidden_layer_sizes': (100,),<br/> 'learning_rate': 'constant',<br/> 'learning_rate_init': 0.001,<br/> 'max_iter': 200,<br/> 'momentum': 0.9,<br/> 'nesterovs_momentum': True,<br/> 'power_t': 0.5,<br/> 'random_state': 14,<br/> 'shuffle': True,<br/> 'solver': 'adam',<br/> 'tol': 0.0001,<br/> 'validation_fraction': 0.1,<br/> 'verbose': False,<br/> 'warm_start': False}
</pre>
<p>Next, we fit our model using the standard scikit-learn interface:</p>
<pre>clf.fit(X_train, y_train)
</pre>
<p>Our model has now learned weights between each of the layers. We can view those weights by examining <kbd>clf.coefs_</kbd>, which is a list of NumPy arrays that join each of the layers. For example, the weights between the input layer with 400 neurons (from each of our pixels) to the hidden layer with 100 neurons (a parameter we set), can be obtained using <kbd>clf.coefs_[0]</kbd>. In addition, the weights between the hidden layer and the output layer (with 26 neurons) can be obtained using <kbd>clf.coefs_[1]</kbd>. These weights, together with the parameters above, wholly define our trained network.</p>
<p>We can now use that trained network to predict our test dataset:</p>
<pre>y_pred = clf.predict(X_test)
</pre>
<p>Finally, we evaluate the results:</p>
<pre>from sklearn.metrics import f1_score<br/>f1_score(y_pred=y_pred, y_true=y_test, average='macro')
</pre>
<p>The result is 0.96, which is pretty impressive. This version of the F1 score is based on the macro-average, which computes the individual F1 score for each class, and then averages them without considering the size of each class.</p>
<p>To examine these individual class results, we can view the classification report:</p>
<pre>from sklearn.metrics import classification_report<br/>print(classification_report(y_pred=y_pred, y_true=y_test))
</pre>
<p>The results from my experiment are shown here:</p>
<pre>             precision    recall  f1-score   support<br/><br/>          0       1.00      1.00      1.00         5<br/>          1       1.00      1.00      1.00         3<br/>          2       1.00      1.00      1.00         3<br/>          3       1.00      1.00      1.00         8<br/>          4       1.00      1.00      1.00         2<br/>          5       1.00      1.00      1.00         4<br/>          6       1.00      1.00      1.00         2<br/>          7       1.00      1.00      1.00         2<br/>          8       1.00      1.00      1.00         7<br/>          9       1.00      1.00      1.00         1<br/>         10       1.00      1.00      1.00         3<br/>         11       1.00      1.00      1.00         4<br/>         12       1.00      0.75      0.86         4<br/>         13       1.00      1.00      1.00         5<br/>         14       1.00      1.00      1.00         4<br/>         15       1.00      1.00      1.00         3<br/>         16       1.00      1.00      1.00         3<br/>         17       1.00      1.00      1.00         7<br/>         18       1.00      1.00      1.00         5<br/>         19       1.00      1.00      1.00         5<br/>         20       1.00      1.00      1.00         3<br/>         21       1.00      1.00      1.00         5<br/>         22       1.00      1.00      1.00         2<br/>         23       1.00      1.00      1.00         4<br/>         24       1.00      1.00      1.00         2<br/>         25       1.00      1.00      1.00         4<br/><br/>avg / total       1.00      0.99      0.99       100
</pre>
<p>The final <kbd>f1-score</kbd> for this report is shown on the bottom right, the second last number - 0.99. This is the micro-average, where the <kbd>f1-score</kbd> is computed for each sample and then the mean is computed. This form makes more sense for relatively similar class sizes, while the macro-average makes more sense for imbalanced classes.</p>
<p>Pretty simple from an API perspective, as <kbd>scikit-learn</kbd> hides all of the complexity. However what actually happened in the backend? How do we train a neural network?</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Back-propagation </h1>
            </header>

            <article>
                
<p>Training a neural network is specifically focused on the following things.</p>
<ul>
<li>The first is the size and shape of the network - how many layers, what sized layers and what error functions they use. While types of neural networks exists that can alter their size and shape, the most common type, a feed-forward neural network, rarely has this capability. Instead, its size is fixed at initialization time, which in this chapter is 400 neurons in the first layer, 100 in the hidden layer and 26 in the final layer. Training for the shape is usually the job of a meta-algorithm that trains a set of neural networks and determines which is the most effective, outside of training the networks themselves.</li>
<li>The second part of training a neural network is to alter the weights between neurons. In a standard neural network, nodes from one layer are attached to nodes of the next layer by edges with a specific weight. These can be initialized randomly (although several smarter methods do exist such as autoencoders), but need to be adjusted to allow the network to <em>learn</em> the relationship between training samples and training classes.</li>
</ul>
<p>This adjusting of weights was one of the key issues holding back very-early neural networks, before an algorithm called <strong>back propagation</strong> was developed to solve the issue.</p>
<p>The <strong>back propagation</strong> (<strong>backprop</strong>) algorithm is a way of assigning blame to each neuron for incorrect predictions. First, we consider the usage of a neural network, where we feed a sample into the input layer and see which of the output layer's neurons fire, as&#160;<em>forward propagation</em>. Back propagation goes backwards from the output layer to the input layer, assigning blame to each weight in the network, in proportion to the effect that weight has on any errors that the network makes.</p>
<p>The amount&#160;of change is based on two aspects:</p>
<ul>
<li>Neuron activation</li>
<li>The gradient of the activation function</li>
</ul>
<p>The first is the degree to which the neuron was <em>activated</em>. Neurons that fire with high (absolute) values are considered to have a great impact on the result, while those that fired with small (absolute) values have a low impact on the result. Due to this, weights around neurons that fire with high values are changed more than those around small values.</p>
<p>The second aspect to the amount that weights change is proportional to the <em>gradient of the activation function</em>. Many neural networks you use will have the same activation function for all neurons, but there are lots of situations where it makes sense to have different activation functions for different layers of neurons (or more rarely, different activation functions in the same layer). The gradient of the activation function, combined with the activation of the neuron, and the error assigned to that neuron, together form the amount that the weights are changed.</p>
<div class="packt_tip">I've skipped over the maths involved in back propagation, as the focus of this book is on practical usage. As you increase your usage of neural networks, it pays to know more about what goes on inside the algorithm. I recommend looking into the details of the back-prop algorithm, which can be understood with some basic knowledge of gradients and derivatives.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Predicting words</h1>
            </header>

            <article>
                
<p>Now that we have a classifier for predicting individual letters, we now move onto the next step in our plan - predicting words. To do this, we want to predict each letter from each of these segments, and put those predictions together to form the predicted word from a given CAPTCHA.</p>
<p>Our function will accept a CAPTCHA and the trained neural network, and it will return the predicted word:</p>
<pre>def predict_captcha(captcha_image, neural_network):<br/>    subimages = segment_image(captcha_image)<br/>    # Perform the same transformations we did for our training data<br/>    dataset = np.array([np.resize(subimage, (20, 20)) for subimage in subimages])<br/>    X_test = dataset.reshape((dataset.shape[0], dataset.shape[1] * dataset.shape[2]))<br/>    # Use predict_proba and argmax to get the most likely prediction<br/>    y_pred = neural_network.predict_proba(X_test)<br/>    predictions = np.argmax(y_pred, axis=1)<br/><br/>    # Convert predictions to letters<br/>    predicted_word = str.join("", [letters[prediction] for prediction in predictions])<br/>    return predicted_word
</pre>
<p>We can now test on a word using the following code. Try different words and see what sorts of errors you get, but keep in mind that our neural network only knows about capital letters:</p>
<pre>word = "GENE"<br/>captcha = create_captcha(word, shear=0.2) <br/>print(predict_captcha(captcha, clf))<br/>plt.imshow(captcha, cmap="Greys") 
</pre>
<p>We can codify this into a function, allowing us to perform predictions more easily:</p>
<pre>def test_prediction(word, net, shear=0.2, scale=1):<br/>    captcha = create_captcha(word, shear=shear, scale=scale, size=(len(word) * 25, 30))<br/>    prediction = predict_captcha(captcha, net) <br/>    return word == prediction, word, prediction
</pre>
<p>The returned results specify whether the prediction is correct, the original word, and the predicted word. This code correctly predicts the word GENE, but makes mistakes with other words. How accurate is it? To test, we will create a dataset with a whole bunch of four-letter English words from NLTK. The code is as follows:</p>
<pre>from nltk.corpus import words
</pre>
<div class="packt_tip">Install NLTK using Anaconda: <span class="packt_screen">conda install nltk</span><br/>
After installation, and before using it in code, you will need to download the corpus using:<br/>
<span class="packt_screen">python -c "import nltk; nltk.download('words')"</span></div>
<p>The words instance here is actually a corpus object, so we need to call <kbd>words()</kbd> on it to extract the individual words from this corpus. We also filter to get only four-letter words from this list:</p>
<pre>valid_words = set([word.upper() for word in words.words() if len(word) == 4])
</pre>
<p>We can then iterate over all of the words to see how many we get correct by simply counting the correct and incorrect predictions:</p>
<pre>num_correct = 0 <br/>num_incorrect = 0 <br/>for word in valid_words:<br/>    shear = random_state.choice(shear_values)<br/>    scale = random_state.choice(scale_values) <br/>    correct, word, prediction = test_prediction(word, clf, shear=shear, scale=scale) <br/>    if correct: <br/>        num_correct += 1 <br/>    else: <br/>        num_incorrect += 1<br/>print("Number correct is {0}".format(num_correct)) <br/>print("Number incorrect is {0}".format(num_incorrect))
</pre>
<p>The results I&#160;get are 3,342 correct and 2,170 incorrect for an accuracy of just over 62 percent. From our original 99 percent per-letter accuracy, this is a big decline. What happened?</p>
<p>The reasons for this decline are listed here:</p>
<ul>
<li>The first factor to impact is our accuracy. All other things being equal, if we have four letters, and 99 percent accuracy per-letter, then we can expect about a 96 percent success rate (all other things being equal) getting four letters in a row (0.99<sup>4</sup>&amp;ap;0.96). A single error in a single letter's prediction results in the wrong word being predicted.</li>
<li>The second impact is the shear value. Our dataset chose randomly between shear values of 0 to 0.5. The previous test used a shear of 0.2. For a value of 0, I get 75 percent accuracy; for a shear of 0.5, the result is much worse at 2.5 percent. The higher the shear, the lower the performance.</li>
<li>The third impact is that often words are incorrectly segmented. Another issue is that some vowels are commonly mistaken, causing more errors than can be expected by the above error rates.</li>
</ul>
<p>Let's examine the second of these issues, and map the relationship between shear and performance. First, we turn our evaluation code into a function that is dependent on a given shear value:</p>
<pre>def evaluation_versus_shear(shear_value):<br/>    num_correct = 0 <br/>    num_incorrect = 0 <br/>    for word in valid_words: <br/>        scale = random_state.choice(scale_values)<br/>        correct, word, prediction = test_prediction(word, clf, shear=shear_value, scale=scale)<br/>        if correct: <br/>            num_correct += 1 <br/>        else: <br/>            num_incorrect += 1<br/>    return num_correct/(num_correct+num_incorrect)
</pre>
<p>Then, we take a list of shear values&#160;and then use this function to evaluate the accuracy for each value. Note that this code will take a while to run, approximately 30 minutes depending on your hardware.</p>
<pre>scores = [evaluation_versus_shear(shear) for shear in shear_values]
</pre>
<p>Finally, plot the result using matplotlib:</p>
<pre>plt.plot(shear_values, scores)
</pre>
<div class="CDPAlignCenter CDPAlign"><img height="333" width="478" class="image-border" src="images/B06162_08_18-1.png"/></div>
<p>You can see that there is a severe drop in performance as the shear value increases past 0.4. Normalizing the input would help, with tasks such as image rotation and unshearing the input.</p>
<div class="packt_tip">Another surprising option to address issues with shear is to increase the amount of training data with high shear values, which can lead to the model learning a more generalized output.</div>
<p>We look into improving the accuracy using post-processing in the next section.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Improving accuracy using a dictionary</h1>
            </header>

            <article>
                
<p>Rather than just returning the given prediction, we can check whether the word actually exists in our dictionary. If it does, then that is our prediction. If it isn't in the dictionary, we can try and find a word that is similar to it and predict that instead. Note that this strategy relies on our assumption that all CAPTCHA words will be valid English words, and therefore this strategy wouldn't work for a random sequence of characters. This is one reason why some CAPTCHAs don't use words.</p>
<p>There is one issue here—how do we determine the closest word? There are many ways to do this. For instance, we can compare the lengths of words. Two words that have a similar length could be considered more similar. However, we commonly consider words to be similar if they have the same letters in the same positions. This is where the <strong>edit distance</strong> comes in.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Ranking mechanisms for word similarity</h1>
            </header>

            <article>
                
<p>The <strong>Levenshtein edit distance</strong> is a commonly used method for comparing two short strings to see how similar they are. It isn't very scalable, so it isn't commonly used for very long strings. The edit distance computes the number of steps it takes to go from one word to another. The steps can be one of the following three actions:</p>
<ul>
<li>Insert a new letter into the word at any position</li>
<li>Delete any letter from the word</li>
<li>Substitute a letter for another one</li>
</ul>
<p>The minimum number of actions needed to transform the first word into the second is given as the distance. Higher values indicate that the words are less similar.</p>
<p>This distance is available in NLTK as <kbd>nltk.metrics.edit_distance</kbd>. We can call it using only two strings and it returns the edit distance:</p>
<pre>from nltk.metrics import edit_distance <br/>steps = edit_distance("STEP", "STOP") <br/>print("The number of steps needed is: {0}".format(steps))
</pre>
<p>When used with different words, the edit distance is quite a good approximation to what many people would intuitively feel are similar words. The edit distance is great for testing spelling mistakes, dictation errors, and name matching (where you can mix up your Marc and Mark spelling quite easily).</p>
<p>However, it isn't very good for our case. We don't really expect letters to be moved around, just individual letter comparisons to be wrong. For this reason, we will create a different distance metric, which is simply the number of letters in the same positions that are incorrect. The code is as follows:</p>
<pre>def compute_distance(prediction, word):<br/>    len_word = min(len(prediction), len(word))<br/>    return len_word - sum([prediction[i] == word[i] for i in range(len_word)])
</pre>
<p>We subtract the value from the length of the prediction word (which is four) to make it a distance metric where lower values indicate more similarity between the words.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Putting it all together</h1>
            </header>

            <article>
                
<p>We can now test our improved prediction function using similar code to before. First, we define a prediction function, which also takes our list of valid words:</p>
<pre>from operator import itemgetter <br/><br/>def improved_prediction(word, net, dictionary, shear=0.2, scale=1.0): <br/>    captcha = create_captcha(word, shear=shear, scale=scale) <br/>    prediction = predict_captcha(captcha, net) <br/><br/>    if prediction not in dictionary:<br/>        distances = sorted([(word, compute_distance(prediction, word)) for word in dictionary],<br/>                           key=itemgetter(1))<br/>        best_word = distances[0] <br/>        prediction = best_word[0]<br/>    return word == prediction, word, prediction
</pre>
<p>We compute the distance between our predicted word and each other word in the&#160;dictionary, and sort it by distance (lowest first). The changes in our testing code are in the following code:</p>
<pre>num_correct = 0 <br/>num_incorrect = 0 <br/>for word in valid_words: <br/>    shear = random_state.choice(shear_values)<br/>    scale = random_state.choice(scale_values)<br/>    correct, word, prediction = improved_prediction(word, clf, valid_words, shear=shear, scale=scale)<br/>    if correct: <br/>        num_correct += 1 <br/>    else: <br/>        num_incorrect += 1<br/>print("Number correct is {0}".format(num_correct)) <br/>print("Number incorrect is {0}".format(num_incorrect))
</pre>
<p>The preceding code will take a while to run (computing all of the distances will take some time) but the net result is 3,037 samples correct and 2,476 samples incorrect. This is an accuracy of 71.5 percent for a boost of nearly 10 percentage points!</p>
<div class="packt_infobox">Looking for a challenge? Update the <kbd>predict_captcha</kbd> function to return the probabilities assigned to each letter. By default, the letter with the highest probability is chosen for each letter in a word. If that doesn't work, choose the next most probable word, by multiplying the per-letter probabilities together.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we worked with images in order to use simple pixel values to predict the letter being portrayed in a CAPTCHA. Our CAPTCHAs were a bit simplified; we only used complete four-letter English words. In practice, the problem is much harder--as it should be! With some improvements, it would be possible to solve much harder CAPTCHAs with neural networks and a methodology similar to what we discussed. The <kbd>scikit-image</kbd> library contains lots of useful functions for extracting shapes from images, functions for improving contrast, and other image tools that will help.</p>
<p>We took our larger problem of predicting words, and created a smaller and simple problem of predicting letters. From here, we were able to create a feed-forward neural network to accurately predict which letter was in the image. At this stage, our results were very good with 97 percent accuracy.</p>
<p>Neural networks are simply connected sets of neurons, which are basic computation devices consisting of a single function. However, when you connect these together, they can solve incredibly complex problems. Neural networks are the basis for deep learning, which is one of the most effective areas of data mining at the moment.</p>
<p>Despite our great per-letter accuracy, the performance when predicting a word drops to just over 60 percent when trying to predict a whole word. We improved our accuracy using a dictionary, searching for the best matching word. To do this, we considered the commonly used edit distance; however, we simplified it&#160;because we were only concerned with individual mistakes on letters, not insertions or deletions. This improvement netted some benefit, but there are still many improvements you could try to further boost the accuracy.</p>
<p>To take the concepts in this chapter further, investigate changing the neural network structure, by adding more hidden layers, or changing the shape of those layers. Investigate the impact this has on the result. Further, try creating a more difficult CAPTCHA--does this drop the accuracy? Can you build a more complicated network to learn it?</p>
<p>Data mining problems such as the CAPTCHA example show that an initial problem statement, such as <em>guess this word</em>, can be broken into individual subtasks that can be performed using data mining. Further, those subtasks can be combined in a few different ways, such as with the use of external information. In this chapter, we combined our letter prediction with a dictionary of valid words to provide a final response, giving better accuracy than letter prediction alone.</p>
<p>In the next chapter, we will continue with string comparisons. We will attempt to determine which author (out of a set of authors) wrote a particular document--using only the content and no other information!</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>
</body>
</html>