# *第 12 章*：用于分类的 K-最近邻

**K-最近邻** (**KNN**) 是当观察或特征不多，且预测类别成员不需要非常高效时，用于分类模型的一个很好的选择。它是一个懒惰学习器，因此比其他分类算法更快地拟合，但在对新观察进行分类时则慢得多。它也可能在极端情况下产生不太准确的预测，但通过适当地调整 *k* 可以改进这一点。我们将在本章开发的模型中仔细考虑这些选择。

KNN 可能是我们能选择的最为直接的非参数算法之一，使其成为一个良好的诊断工具。不需要对特征的分布或特征与目标之间的关系做出任何假设。没有很多超参数需要调整，而且两个关键的超参数——最近邻和距离度量——都很容易理解。

KNN 可以成功用于二元和多类问题，而无需对算法进行任何扩展。

在本章中，我们将涵盖以下主题：

+   KNN 的关键概念

+   二元分类的 KNN

+   多类分类的 KNN

# 技术要求

除了常用的 scikit-learn 库之外，我们还需要 `imblearn`（不平衡学习）库来运行本章中的代码。这个库帮助我们处理显著的类别不平衡。`imblearn` 可以通过 `pip install imbalanced-learn` 安装，或者如果你使用 Anaconda，可以通过 `conda install -c conda-forge imbalanced-learn` 安装。所有代码都已使用 scikit-learn 版本 0.24.2 和 1.0.2 进行测试。

# KNN 的关键概念

KNN 可能是我们将在本书中讨论的最直观的算法。其思想是找到 *k* 个属性最相似的实例，其中这种相似性对目标很重要。最后一个条款是一个重要但可能显然的限定条件。我们关注与目标值相关的属性之间的相似性。

对于每个需要预测目标的观察，KNN 会找到与该观察的特征最相似的 *k* 个训练观察。当目标是分类时，KNN 会选择 *k* 个训练观察中目标的最频繁值。（我们通常选择奇数个 *k* 以避免分类问题中的平局。）

我所说的通过 *训练* 观察到的，是指那些具有已知目标值的观察。KNN 由于是一个懒惰学习器，所以不需要进行真正的训练。我将在本节中更详细地讨论这一点。

以下图表说明了使用 KNN 进行分类，其中 *k* 的值为 1 和 3。当 **k=1** 时，我们会预测新的观察值 **X** 将属于圆形类别。当 **k=3** 时，它将被分配到正方形类别：

![图 12.1 – *k* 的值为 1 和 3 的 KNN](img/B17978_12_001.jpg)

图 12.1 – *k* 的值为 1 和 3 的 KNN

但我们所说的相似或最近的实例是什么意思呢？有几种方法可以衡量相似性，但最常用的度量是欧几里得距离。欧几里得距离是两点之间平方差的和。这可能会让你想起勾股定理。从点 *a* 到点 *b* 的欧几里得距离如下：

![图](img/B17978_12_0011.jpg)

欧几里得距离的一个合理的替代方案是曼哈顿距离。从点 *a* 到点 *b* 的曼哈顿距离如下：

![图](img/B17978_12_002.jpg)

scikit-learn中的默认距离度量是闵可夫斯基距离。从点 *a* 到点 *b* 的闵可夫斯基距离如下：

![图](img/B17978_12_003.jpg)

注意到当 *p* 为1时，它与曼哈顿距离相同。当 *p* 为2时，它与欧几里得距离相同。

曼哈顿距离有时被称为出租车距离。这是因为它反映了两个点在网格路径上的距离。以下图表说明了曼哈顿距离并将其与欧几里得距离进行了比较：

![图12.2 – 欧几里得和曼哈顿距离度量](img/B17978_12_0021.jpg)

图12.2 – 欧几里得和曼哈顿距离度量

使用曼哈顿距离可以在特征类型或尺度差异很大时产生更好的结果。然而，我们可以将距离度量的选择视为一个经验问题；也就是说，我们可以尝试两者（或其他的距离度量）并看看哪个给我们带来性能最好的模型。我们将在下一节通过网格搜索来演示这一点。

如你所怀疑的那样，KNN模型对 *k* 的选择很敏感。较低的 *k* 值会导致一个试图识别观察之间细微差异的模型。在非常低的 *k* 值时，存在过度拟合的实质性风险。但在 *k* 值较高时，我们的模型可能不够灵活。我们再次面临方差-偏差权衡。较低的 *k* 值导致偏差较少而方差较多，而较高的值则相反。

对于 *k* 的选择没有明确的答案。但一个好的经验法则是使用观察数的平方根。然而，就像我们对距离度量所做的那样，我们应该测试模型在不同 *k* 值下的性能。KNN是一种非参数算法。不对底层数据的属性做出假设，例如线性或正态分布的特征。这使得KNN非常灵活。它可以用来模拟特征与目标之间的各种关系。

如前所述，KNN是一种懒惰学习算法。在训练时间不进行任何计算。学习仅在测试时发生。这有其优点和缺点。当数据中有许多实例或维度时，它可能不是一个好的选择，而且预测速度很重要。KNN也往往在稀疏数据上表现不佳，例如包含许多0值的数据集。

在下一节中，我们将使用KNN构建一个二分类模型，然后在下一节构建几个多分类模型。

# KNN用于二分类

KNN算法与决策树算法有一些相同的优点。不需要满足关于特征或残差的分布的先验假设。它是我们试图在前两章中构建的心脏病模型的一个合适的算法。数据集不是很大（30,000个观测值）并且没有太多特征。

注意

心脏病数据集可在[https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease)公开下载。它来源于2020年美国疾病控制与预防中心对超过40万人的调查数据。我已经从这个数据集中随机抽取了30,000个观测值用于本节的分析。数据列包括受访者是否曾经患有心脏病、体重指数、吸烟史、大量饮酒、年龄、糖尿病和肾病。

让我们开始构建我们的模型：

1.  首先，我们必须加载我们在过去几章中使用的一些相同的库。我们还将加载`KneighborsClassifier`：

    [PRE0]

`healthinfo`模块包含了我们在[*第10章*](B17978_10_ePub.xhtml#_idTextAnchor126)，*逻辑回归*中使用的所有代码，用于加载健康信息数据并进行预处理。这里没有必要重复这些步骤。如果你还没有阅读[*第10章*](B17978_10_ePub.xhtml#_idTextAnchor126)，*逻辑回归*，至少浏览一下该章节的第二部分的代码可能会有所帮助。这将让你更好地了解特征。

1.  现在，让我们获取由`healthinfo`模块处理过的数据并显示特征名称：

    [PRE1]

1.  我们可以使用K折交叉验证来评估这个模型。我们已经在[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)，*准备模型评估*中讨论了K折交叉验证。我们将指定我们想要重复10次10个分割。默认值分别是`5`和`10`。

我们模型的精确度，即我们预测心脏病时的正确率，异常低，为`0.17`。灵敏度，即存在心脏病时预测心脏病的比率，也较低，为`0.56`：

[PRE2]

1.  我们可以通过一些超参数调整来提高我们模型的性能。让我们为几个邻居和距离度量创建一个字典。我们还将尝试使用我们的`filter`方法选择不同数量的特征：

    [PRE3]

1.  我们将在网格搜索中的评分将基于**接收者操作特征曲线**（**ROC曲线**）下的面积。我们在[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)，*准备模型评估*中介绍了ROC曲线：

    [PRE4]

1.  我们可以使用随机网格搜索的最佳估计器属性从`selectkbest`获取选定的特征：

    [PRE5]

1.  我们还可以查看最佳参数和最佳得分。11个特征（17个特征中的11个）被选中，正如我们在上一步中看到的。一个*k*（`n_neighbors`）为`254`和曼哈顿距离度量是得分最高的模型的另一个超参数：

    [PRE6]

1.  让我们看看这个模型的更多指标。我们在敏感性方面做得很好，但其他指标并不好：

    [PRE7]

1.  我们还应该绘制混淆矩阵。为此，我们可以查看相对较好的敏感性。在这里，我们正确地将大多数实际阳性识别为阳性。然而，这是以许多假阳性为代价的。我们可以从上一步的精确度得分中看到这一点。大多数时候我们预测阳性，我们都是错误的：

    [PRE8]

这会产生以下图表：

![图12.3 – 超参数调整后的心脏病混淆矩阵](img/B17978_12_0031.jpg)

图12.3 – 超参数调整后的心脏病混淆矩阵

在本节中，你学习了如何使用具有二进制目标的KNN。我们可以遵循非常相似的步骤来使用KNN进行多类分类问题。

# KNN多类分类

构建KNN多类模型相当简单，因为它不需要对算法进行特殊扩展，例如将逻辑回归应用于具有两个以上值的目标所需的扩展。我们可以通过使用我们在[*第10章*](B17978_10_ePub.xhtml#_idTextAnchor126)，*逻辑回归*中的*多项式逻辑回归*部分使用的相同机器故障数据来看到这一点。

注意

这份关于机器故障的数据集可在[https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification)公开使用。有10,000个观测值，12个特征，以及两个可能的靶标。一个是二进制靶标，指定机器是否故障。另一个包含故障类型。该数据集中的实例是合成的，由一个旨在模仿机器故障率和原因的过程生成。

让我们构建我们的机器故障类型模型：

1.  首先，让我们加载现在熟悉的模块：

    [PRE9]

1.  让我们加载机器故障数据并查看其结构。共有10,000个观测值，没有缺失数据。数据包括分类数据和数值数据的组合：

    [PRE10]

1.  让我们也看看一些观测值：

    [PRE11]

1.  我们还应该对分类特征进行一些频率分析。绝大多数观测值，97%，没有出现故障。这种相当明显的类别不平衡可能很难建模。有三种机器类型——高质量、低质量和中等质量：

    [PRE12]

1.  让我们合并一些`failtype`值并检查我们的工作。首先，我们将定义一个函数`setcode`，将故障类型文本映射到故障类型代码。我们将随机分配故障和工具磨损故障到代码`5`，用于其他故障：

    [PRE13]

1.  我们应该查看我们数值特征的描述性统计：

    [PRE14]

1.  现在，我们准备创建训练和测试数据框。我们将使用我们刚刚创建的故障类型代码作为我们的目标：

    [PRE15]

1.  现在，让我们设置列转换。对于数值特征，我们将将异常值设置为中位数，然后缩放数据。我们将使用最小-最大缩放，这将返回从0到1的值（`MinMaxScaler`的默认值）。我们使用这个缩放器，而不是标准缩放器，以避免负值。我们稍后将使用的特征选择方法`selectkbest`不能与负值一起使用：

    [PRE16]

1.  让我们也看看编码后的列。我们需要在过度采样之前做这件事，因为`SMOTENC`模块需要分类特征的列索引。我们进行过度采样是为了处理显著的类别不平衡。我们已在[*第11章*](B17978_11_ePub.xhtml#_idTextAnchor135)中更详细地讨论了这一点，*决策树和随机森林分类*：

    [PRE17]

1.  接下来，我们将为我们的模型设置一个管道。该管道将执行列转换，使用`SMOTENC`进行过度采样，使用`selectkbest`进行特征选择，然后运行KNN模型。记住，我们必须将分类特征的列索引传递给`SMOTENC`，以便它能够正确运行：

    [PRE18]

1.  现在，我们准备拟合我们的模型。我们将进行随机网格搜索，以确定KNN的最佳值和距离度量。我们还将搜索特征选择的最佳*k*值：

    [PRE19]

1.  让我们看看网格搜索发现了什么。除了`processtemperature`之外的所有特征都值得保留在模型中。KNN的最佳值和距离度量分别是`125`和`minkowski`。基于ROC曲线下的面积的最佳分数是`0.9`：

    [PRE20]

1.  让我们看看一个混淆矩阵。查看第一行，我们可以看到当没有发生故障时，发现了相当数量的故障。然而，我们的模型正确地识别了大多数实际的热量、功率和过载故障。这可能不是一个可怕的精确度和敏感度权衡。根据问题，我们可能接受大量假阳性，以在我们的模型中获得可接受的敏感度水平：

    [PRE21]

这产生了以下图表：

![图12.4 – 超参数调整后机器故障类型的混淆矩阵](img/B17978_12_004.jpg)

图12.4 – 超参数调整后机器故障类型的混淆矩阵

1.  我们还应该查看一个分类报告。你可能还记得从[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)，*为模型评估做准备*，宏平均是简单地在类别间取平均。在这里，我们更感兴趣的是加权平均。加权F1分数为`0.81`并不差。记住，F1是精确率和敏感度的调和平均数：

    [PRE22]

机器故障类型的类别不平衡使得建模特别困难。尽管如此，我们的KNN模型表现相对较好，假设大量假阳性不是问题。在这种情况下，一个假阳性可能不像一个假阴性那样成问题。它可能只是需要对看似有故障风险的机器进行更多检查。如果我们将其与实际机器故障的惊讶相比，偏向于敏感度而不是精确度可能是合适的。

让我们在另一个多类问题上尝试KNN。

## 字母识别的KNN

我们可以采取与预测机器故障时使用字母识别相同的策略。只要我们有能够很好地区分字母的特征，KNN就是该模型的合理选择。我们将在本节尝试这种方法。

注意

在本节中，我们将使用字母识别数据。这些数据可在[https://archive-beta.ics.uci.edu/ml/datasets/letter+recognition](https://archive-beta.ics.uci.edu/ml/datasets/letter+recognition)公开使用。有26个字母（全部为大写）和20种不同的字体。16个不同的特征捕捉每个字母的不同属性。

让我们构建模型：

1.  首先，我们将加载我们已经使用过的相同库：

    [PRE23]

1.  现在，我们将加载数据并查看前几个实例。有20,000个观测值和17列。`letter`是我们的目标：

    [PRE24]

1.  现在，让我们创建训练和测试数据框：

    [PRE25]

1.  接下来，让我们实例化一个KNN实例。我们还将设置分层K折交叉验证和超参数的字典。我们将寻找*k*（`n_neighbors`）和距离度量的最佳超参数：

    [PRE26]

1.  现在，我们已经准备好进行彻底的网格搜索。在这里我们进行彻底搜索是因为我们没有很多超参数需要检查。表现最好的距离度量是欧几里得距离。最近邻的*k*值是`3`。这个模型使我们几乎达到95%的准确率：

    [PRE27]

1.  让我们生成预测并绘制一个混淆矩阵：

    [PRE28]

这会产生以下图表：

![图12.5 – 字母预测的混淆矩阵](img/B17978_12_005.jpg)

图12.5 – 字母预测的混淆矩阵

让我们快速总结本章所学的内容。

# 摘要

本章展示了使用KNN进行二分类或多分类分类是多么容易。由于KNN不对正态性或线性做出假设，因此它可以在逻辑回归可能不会产生最佳结果的情况下使用。这种灵活性确实带来了过拟合的真实风险，因此在选择*k*时必须谨慎。我们还在本章探讨了如何调整二分类和多分类模型的超参数。最后，当我们在乎预测速度或处理大型数据集时，KNN并不是一个很好的选择。在上一章中我们探讨的决策树或随机森林分类，在这些情况下通常是一个更好的选择。

另一个非常好的选择是支持向量分类。我们将在下一章探讨支持向量分类。
