- en: Training Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you hear the term **neural networks**, it gives you a sense that its a
    form of biological terminology pertaining to brains. And I have to tell you candidly
    that it's a no brainer to guess that and, in fact, we are treading along the right
    path by doing so. We will see how it is connected to that.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks have brought in a revolution in the data science world. Until
    2011, due to not having enough computation power, the people rooting for neural
    networks were not able to propagate it to the extent that they wanted. But, with
    the advent of cheaper computation solutions and more research in the area of neural
    networks, they have taken the data science and artificial world by storm. Neural
    networks are an algorithm that can be applied in both supervised and unsupervised
    learning. With deeper networks, they are able to provide solutions to unstructured
    data, such as images and text.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network initialization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropouts
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stochastic gradient descent
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent neural networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let me explain first of all what neurons are and how they are structured. The
    following labelled diagram shows a typical neuron:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c407659-7770-4b8f-84ab-cc64be72d799.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: We define neuron as an electrically excitable cell that receives, processes,
    and transmits information through electric and chemical signals. A dendrite is
    a part of it that receives signals from other neurons. One thing that we need
    to pay attention to is that just a single neuron can't do anything and there are
    billions of neurons connected to each other, which enables the electro-chemical
    signal flow and, in turn, the information to flow through it. The information
    passes through an axon and a synapse, before being transmitted.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to a neural network, the structure doesn't change much. Let's
    have a look at it. In the middle, we have a neuron and this neuron gets signals
    from three other neurons, X1, X2, and X3\. All three neurons are connected by
    arrows that act like a synapse. These neurons, X1, X2, and X3, are called **input
    layer neurons**. After passing through the neuron, we get the output value. It's
    interesting to see that the human brain gets an input signal through all the sensors
    such as eyes, ear, touch, and nose and that all the synapses let these electro-chemical
    signals go, and output comes as vision, voice, sense of touch, and smell. A similar
    process is followed in the case of a neural network.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: How a neural network works
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s say we have one set of input and output as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input (X)** | **Output (Y)** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
- en: '| 2 | 4 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
- en: '| 3 | 6 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
- en: '| 4 | 8 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
- en: '| 5 | 10 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
- en: '| 6 | 12 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
- en: In the preceding table, input and output might look to have a linear relationship;
    however, that is not always the case. In addition, every time the model needs
    to initialize. Let's understand the meaning of initialization.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Model initialization
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Going by the preceding table, the network is trying to find a relationship
    between input and output. For example, let''s assume the relationship that comes
    through is the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '*Y = W. X*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding equation, *Y* and *X* are known, and based on that *W* has
    to be found out. But, finding out the value of *W* in one iteration is rare. It
    has to be initialized first. Let''s say *W* is initialized with the value of *3*.
    And the equation turns out to be as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '*Y= 3X*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input (X)** | **Actual Output (Y)** |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| 2 | 6 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| 3 | 9 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| 4 | 12 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| 5 | 15 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| 6 | 18 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: Now we have to assess the output and whether it is close to the desired output.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, the model has been randomly initialized and with this we have been able
    to get an output. In order to assess if the actual output is close to the desired
    output, **loss function** is introduced. It enables the generalization of the
    model, and figures out how well the model is able to reach the desired output.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'We can have a look at the new table, which has got actual output as well as
    desired output:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input (X)** | **Actual Output (Y[a])** | **Desired Output (Y)** |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| 2 | 6 | 4 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| 3 | 9 | 6 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| 4 | 12 | 8 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| 5 | 15 | 10 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| 6 | 18 | 12 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: 'If we have to put the loss function down, it has to be as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss Function = Desired Output-Actual Output*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'However, putting loss function this way would invite both kinds of values:
    negative and positive. In the case of a negative value for the loss function,
    it would mean that the network is overshooting as *Desired Output < Actual Output*
    and in the reverse scenario (*Desired Output > Actual Output*), the network would
    undershoot. In order to get rid of this kind of thing, we will go for having an
    absolute loss:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input(X)** | **Actual Output (Y[a])** | **Desired Output (Y)** | **Loss=Y-Y[a]**
    | **Absolute Loss** |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| 2 | 6 | 4 | -2 | 2 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| 3 | 9 | 6 | -3 | 3 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| 4 | 12 | 8 | -4 | 4 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| 5 | 15 | 10 | -5 | 5 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| 6 | 18 | 12 | -6 | 6 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: Total Absolute Loss = 20
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Having this approach of absolute loss will do no good to the model, as if we
    try to see the preceding table gingerly, the smallest loss is of 2 units and the
    maximum coming through is 6 units. One might get a feeling that the difference
    between maximum and minimum loss is not much (here, 4 units), but it can be huge
    for the model. Hence, a different route is taken altogether. Rather than taking
    absolute loss, we would go for the square of losses:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input(X)** | **Actual output (Y[a])** | **Desired output (Y)** | **Loss=Y-Y[a]**
    | **Square of Loss** |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| 2 | 6 | 4 | -2 | 4 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| 3 | 9 | 6 | -3 | 9 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| 4 | 12 | 8 | -4 | 16 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: '| 5 | 15 | 10 | -5 | 25 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: '| 6 | 18 | 12 | -6 | 36 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: Now, the more the loss, the more the penalization. It can easily make things
    evident where we have more losses.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have to figure out a way to minimize the total loss function and it can be
    achieved by changing the weight. It can be done by using a crude method like modifying
    the parameter *W* over a range of -500 to 500 with a step 0.001\. It will help
    us to find a point where the sum of squares of error becomes 0 or minimum.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: But this approach will work out in this scenario because we don't have too many
    parameters here and computation won't be too challenging. However, when we have
    a number of parameters, the computation would take a hit.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Here, mathematics comes to our rescue in the form of differentiation (maxima
    and minima approach) in order to optimize the weights. The derivative of a function
    at a certain point gives the rate at which this function is changing its values.
    Here, we would take the derivative of loss function. What it will do is to assess
    an impact on total error by making a slight adjustment or change in weight. For
    example, if we try to make a change in weight which is *δW, W= W+ **δW*, we can
    find out how it is influencing loss function. Our end goal is to minimize the
    loss function through this.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the minima will be arrived at *w=2*; hence, we are exploring different
    scenarios here:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '*w<2* implies a positive loss function, negative derivative, meaning that an
    increase of weight will decrease the loss function'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w>2* implies positive loss function, but the derivative is positive, meaning
    that any more increase in the weight will increase the losses'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At *w=2*, loss=0 and the derivative is 0; minima is achieved:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/18242011-6eff-469d-8dde-610e9b8146b6.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: Computation in neural networks
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s look at a simple and shallow network:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74496ea6-7e1a-45ce-9483-1b594efecada.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '**I1**: Input neuron 1'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**I2**: Input neuron 2'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**B1**: Bias 1'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**H1**: Neuron 1 in hidden layer'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**H2**: Neuron 2 in hidden layer'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**B2**: Bias 2'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**O1**: Neuron at output layer'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final value comes at the output neuron **O1**. **O1** gets the input from
    **H1**, **H2**, and **B2**. Since **B2** is a bias neuron, the activation for
    it is always 1\. However, we need to calculate the activation for **H1** and **H2**.
    In order to calculate activation of **H1** and **H2**, activation for **I1**,
    **I2**, and **B1** would be required. It may look like **H1** and **H2** will
    have the same activation, since they have got the same input. But this is not
    the case here as weights of **H1** and **H2** are different. The connectors between
    two neurons represent weights.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Calculation of activation for H1
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s have a look at the part of network involving just **H1**:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a119d44b-03e2-48ea-aea5-a71f3ca56d56.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: 'The hidden layer comes out as in the following formula:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d64d4cf1-b2c9-492f-b3d9-80f867ea7193.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '*A*: Activation function'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x[i]*: Input values'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w[i]*: Weight values'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our scenario, there are three input values, *n=3*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '*x[1]* = *I1* = Input value 1 from first neuron'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x[2 ]*= *I2*= Input value 2 from second neuron'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x[3 ]*= *B1* = *1*'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w1 *= Weight from *I1* to *H1*'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w2 *= Weight from *I2* to *H1*'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w3 *= Weight from *B1* to *H1*'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backward propagation
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this step, we calculate the gradients of the loss function *f(y, y_hat)*
    with respect to *A*, *W*, and *b* called *dA*, *dW,* and *db*. Using these gradients,
    we update the values of the parameters from the last layer to the first.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Activation function
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Activation function is typically introduced in the neural network in order to
    induce non-linearity. Without non-linearity, a neural network will have little
    chance to learn non-linearity. But you might question as to why why we need non-linearity
    in the first place. If we deem every relationship as a linear one, then the model
    won't be able to do justice to the actual relationship because having a linear
    relationship is a rarity. If applied linearity, the model's output won't be a
    generalized one.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Also, the main purpose of an activation function is to convert an input signal
    into an output. Let's say if we try to do away with an activation function, it
    will output a linear result. Linear function is a polynomial of the first degree
    and it's easy to solve but, again, it's not able to capture complex mapping between
    various features, which is very much required in the case of unstructured data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Non-linear functions are those that have a degree more than one. Now we need
    a neural network model to learn and represent almost anything and any arbitrary
    complex function that maps inputs to outputs. Neural networks are also called **universal
    function approximators**. It means that they can compute and learn any function.
    Hence, activation function is an integral part of a neural network to make it
    learn complex functions.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Types of activation functions
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Sigmoid**: This type of activation function comes along as follows:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8bcb8153-74b3-4a24-b927-2967314bbf01.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: 'The value of this function ranges between *0* and *1*. It comes with a lot
    of issues:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing gradient
  id: totrans-110
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Its output is not zero-centered
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It has slow convergence
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperbolic tangent function (tanh)**: The mathematical formula to represent
    it is this:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/76547790-6185-4522-bddc-1e58280a80f8.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'The value of this function ranges between -1 and +1\. However, it still faces
    the vanishing gradient problem:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e924e24-2a35-4a8f-9072-4c49f9d7f9ad.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: '**Rectified Linear Units (ReLU)**: Mathematically, we represent it in the following
    manner:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/141499a5-01e1-4e4a-97d7-21fa7000f8df.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: '![](img/ce1a9375-30b1-4e09-9822-66bf365ed966.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: 'Going by the preceding diagram, ReLU is linear for all positive values, and
    zero for all negative values. This means that the following are true:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: It's cheap to compute as there is no complicated math. The model can therefore
    take less time to train.
  id: totrans-121
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: It converges faster. Linearity means that the slope doesn't hit the plateau
    when *x* gets large. It doesn't have the vanishing gradient problem suffered by
    other activation functions such as sigmoid or tanh.
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Network initialization
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen that there are a number of stages in a neural network model.
    We already know that weight exists between two nodes (of two different layers).
    The weights undergo a linear transformation and, along with values from input
    nodes, it crosses through nonlinear activation function in order to yield the
    value of the next layer. It gets repeated for the next and subsequent layers and
    later on, with the help of backpropagation, optimal values of weights are found
    out.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'For a long time, weights used to get randomly initialized. Later on, it was
    realized that the way we initialize the network has a massive impact on the model.
    Let''s see how we initialize the model:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '**Zero initialization**: In this kind of initialization, all the initial weights
    are set to zero. Due to this, all the neurons of all the layers perform the same
    calculation, which results in producing the same output. It will make the whole
    deep network futile. Predictions coming out of this network would be as good as
    random. Intuitively speaking, it doesn''t perform symmetry breaking. Normally,
    during forward propagation of a neural network, each hidden node gets a signal
    and this signal is nothing but the following:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7ed004a1-bbc9-4521-b36d-17faa6d57a86.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: 'If a network is initialized with zero, then all the hidden nodes will get zero
    signal because all the inputs will be multiplied by zero. Hence, no matter what
    the input value is, if all weights are the same, all units in the hidden layer
    will be the same too. This is called **symmetry**, and it has to be broken in
    order to have more information capturing a good model. Hence, the weights are
    supposed to be randomly initialized or with different values:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0640572-0929-4ab6-8727-a72292178280.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: '**Random initialization**: This kind of initialization helps in symmetry breaking. In
    this method, the weights are randomly initialized very close to zero. Every neuron
    doesn''t perform the same computation as the weight is not equal to zero:'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5d537d8c-7360-4499-a735-08e3745db251.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: '**He-et-al initialization**: This initialization depends on the size of the
    previous layer. It helps in attaining a global minimum of the cost function. The
    weights are random but differ in range depending on the size of the previous layer
    of neurons:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/4ea122bf-7683-472f-b158-b3ffef64ec96.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: Backpropagation
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Backpropagation takes place once feed forward is completed. It stands for **backward
    propagation of errors**. In the case of neural networks, this step begins to compute
    the gradient of error function (loss function) with respect to the weights. One
    can wonder why the term **back** is associated with it. It's due to gradient computation
    that starts backwards through the network. In this, the gradient of the final
    layer of weights gets calculated first and the the weights of the first layer
    are calculated last.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Backpropagation needs three elements:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset**: A dataset that consists of pairs of input-output ![](img/4547391a-da1a-4ad7-b3dd-a8fc87618733.png) where ![](img/413b860f-3d7a-4eb6-b627-8ba6547d1c7a.png) is
    the input and ![](img/516eb17d-0d03-4582-8e5a-b81806b6e626.png) is the output
    that we are expecting. Hence, a set of such input-outputs of size *N* is taken
    and denoted as ![](img/69a7749c-d852-4e99-bff9-f07109e362a8.png).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feed-forward network**: In this, the parameters are denoted as *θ*. The parameters,![](img/ca7375da-5da5-4a0b-a055-11c63e0c870b.png),
    the weight between node *j* in layer *l[k] *and node *i* in layer *l[k-1]*, and the
    bias ![](img/b91ce0ca-b2ff-4a9a-bc69-9b4193a865eb.png) for node *i* in layer *l[k-1]*.
    There are no connections between nodes in the same layer and layers are fully
    connected.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function**: *L(X,θ)*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training a neural network with gradient descent requires the calculation of
    the gradient of the loss/error function *E(X,θ)* with respect to the weights ![](img/dadeb00b-55a2-421c-8bab-946c28b37d4d.png) and
    biases ![](img/1455e031-42d4-4c52-bf4f-5626cbc06bc2.png). Then, according to the
    learning rate *α*, each iteration of gradient descent updates the weights and
    biases collectively, denoted according to the following:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c456d7b-2549-43ba-8427-cdeefb863b33.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: Here ![](img/64229546-bef0-4e41-8c0c-4f7ad5c38557.png) denotes the parameters
    of the neural network at iteration in gradient descent.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already discussed overfitting in detail. However, let's have a recap
    of what we learned and what overfitting is in a neural network scenario.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: By now, we are cognizant of the fact that, when a large number of parameters
    (in deep learning) are available at our disposal to map and explain an event,
    more often than not, the model built using these parameters will tend to have
    a good fit and try to showcase that it has the ability to describe the event properly.
    However, the real test of any model is always on unseen data, and we were able
    to assess how the model fares on such unseen data points. We expect our model
    to have an attribute of generalization and it will enable the model to score on
    test data (unseen) in alignment with the trained one. But, a number of times our
    model fails to generalize when it comes to the unseen data, as the model has not
    learned the insights and causal relationship of the event. In this scenario, one
    might be able to see the huge gulf of variance in training accuracy and test accuracy and,
    needless to say, it is not what we are seeking out of the model. This phenomenon
    is called **overfitting**.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, there are millions of parameters you may encounter and in
    all likelihood, you might fall into the trap of overfitting. As we had defined
    overfitting in the first chapter, it happens when a model learns the detail and
    noise in the training data to the extent that it negatively impacts the performance
    of the model on new data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Prevention of overfitting in NNs
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we already discussed in the earlier chapters, overfitting is a major issue
    that needs to be considered while building models as our work doesn't get over
    only at training phase. The litmus test for any model takes place on unseen data.
    Let's explore the techniques of handling overfitting issues in neural networks.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中讨论的那样，过度拟合是构建模型时需要考虑的主要问题，因为我们的工作并不只限于训练阶段。任何模型的试金石都发生在未见过的数据上。让我们探讨处理神经网络中过度拟合问题的技术。
- en: Vanishing gradient
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消失的梯度
- en: Neural networks have been a revelation in extracting complex features out of
    the data. Be it images or texts, they are able to find the combinations that result
    in better predictions. The deeper the network, the higher the chances of picking
    those complex features. If we keep on adding more hidden layers, the learning
    speed of the added hidden layers get faster.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在从数据中提取复杂特征方面已经是一个启示。无论是图像还是文本，它们都能够找到导致更好预测的组合。网络越深，选择那些复杂特征的机会就越高。如果我们继续添加更多的隐藏层，添加的隐藏层的学习速度会更快。
- en: However, when we get down to backpropagation, which is moving backwards in the
    network to find out gradients of the loss with respect to weights, the gradient
    tends to get smaller and smaller as we head towards the first layer. It that initial
    layers of the deep network become slower learners and later layers tend to learn
    faster. This is called the **vanishing gradient problem**.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们进行反向传播时，这是在网络中向后移动以找到损失相对于权重的梯度，梯度往往会随着我们向第一层前进而越来越小。这意味着深度网络的前几层成为较慢的学习者，而后续层倾向于学习得更快。这被称为**消失的梯度问题**。
- en: '**Initial layers **in the network are important because they are responsible
    to *learn and detect the simple patterns* and are actually the **building blocks** of
    our network. Obviously, if they give improper and **inaccurate** results, then
    how can we expect the next layers and the complete network to perform effectively
    and produce accurate results? The following diagram shows the figure of a ball
    that rolls on a steeper slope:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络初始层**很重要，因为它们负责*学习和检测简单模式*，实际上是我们的网络的**构建块**。显然，如果它们给出不适当和**不准确**的结果，那么我们如何期望下一层和整个网络有效地执行并产生准确的结果？以下图表显示了球在更陡斜率上滚动的图像：'
- en: '![](img/a3ae1b6c-f32c-4cd9-887a-e201c1590813.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a3ae1b6c-f32c-4cd9-887a-e201c1590813.png)'
- en: 'Just to make it a little simpler for us all, let''s say that there are two
    slopes: one being steeper and the other being less steep. Both slopes have got
    balls rolling down them and it is a no brainer that the ball will roll down the
    steeper slope faster than the one that is not as steep. Similar to that, if the
    gradient is large, the learning and training gets faster; otherwise, training
    gets too slow if the gradient is less steep.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们更容易理解，让我们说有两个斜率：一个更陡，另一个较缓。两个斜率上都有球滚下来，很明显球会从更陡的斜率上滚得更快，而不是从较缓的斜率上。类似地，如果梯度大，学习和训练会更快；否则，如果梯度较缓，训练会变得太慢。
- en: 'From backpropagation intuition, we are aware of the fact that the optimization
    algorithms such as gradient descent slowly seeko attain the local optima by regulating
    weights such that the cost function''s output is decreased. The gradient descent
    algorithm updates the weights by the negative of the gradient multiplied by the
    learning rate (*α*) (which is small):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 从反向传播的直觉来看，我们知道优化算法，如梯度下降，通过调节权重以降低成本函数的输出，缓慢地寻求达到局部最优。梯度下降算法通过将梯度的负值乘以学习率（*α*）（它很小）来更新权重：
- en: '![](img/440170ea-fc59-46bc-817e-dec702406d97.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/440170ea-fc59-46bc-817e-dec702406d97.png)'
- en: It says that we have to repeat until it attains convergence. However, there
    are two scenarios here. The first is that, if there are fewer iterations, then
    the accuracy of the result will take a hit; the second is that more iterations
    result in training taking too much time. This happens because weight does not
    change enough at each iteration as the gradient is small (and we know *α* is already
    very small). Hence, weight does not move to the lowest point in the assigned iterations.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 它说我们必须重复进行，直到达到收敛。然而，这里有两种情况。第一种情况是，如果迭代次数较少，那么结果的准确性会受到影响；第二种情况是，迭代次数过多会导致训练时间过长。这是因为每次迭代中权重变化不够，因为梯度小（我们知道*α*已经非常小）。因此，权重不会移动到分配的迭代中的最低点。
- en: 'Let''s talk about that activation function, which might have an impact on the
    vanishing gradient problem. Here, we talk about the sigmoid function, which is
    typically used as an activation function:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7657cf67-19e2-45f3-81b7-4084792ceb64.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: '![](img/2857f1c6-3632-4d5a-acda-04b0ef32219a.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: 'It translates all input values into a range of values between *(0,1)*. If we
    have to find out the derivative of the sigmoid function then:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5d457b0-01e6-4904-baa1-f1e0674a4969.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: 'Let''s plot it now:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df11e32f-2167-467a-bd05-36ce684c8313.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: It is quite evident that the derivative has got the maximum value as 0.25\.
    Hence, the range of values under which it would lie is *(0,1/4)*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical neural network looks like the following diagram:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1acf1c62-c478-4e81-9699-0e7c6f511a05.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: Once the weight parameters are initialized, the input gets multiplied by weights
    and gets passed on through an activation function and, finally, we get a cost
    function (**J**). Subsequently, backpropagation takes place to modify the weights
    through gradient descent in order to minimize **J**.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to calculate the derivative with respect to first weight, we are using
    the chain rule. It will turn out to be like the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a106879-37e1-4614-a153-6ee334fb3b73.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: 'If we just try to study the derivatives in the middle of the preceding expression,
    we get the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ff036d1-3594-41c1-ae0e-7cf946652c0a.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: Part 1—from the output to hidden2.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the output is the activation of the 2nd hidden unit, the expression turns
    out to be like the following:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3941e87a-2dd0-453c-bfd3-e9799e41ebce.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: '![](img/5ecc45d9-810a-4fc3-b0d4-eb19564c6eea.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: 'Similarly for part 2, from hidden 2 to hidden 1, the expression turns out to
    be like the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/adc5ddd6-823d-42ba-b0fe-361a56202f8e.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: '![](img/773e3435-985d-4240-bb66-935d38d4f466.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'On putting everything together, we get the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98f8ca56-7833-4e7e-b0e2-a145aa45817b.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: We know that the maximum value of the derivative of the sigmoid function is
    1/4 and the weights can typically take the values between -1 and 1 if weights
    have been initialized with standard deviation 1 and mean 0\. It will lead to the
    whole expression being smaller. If there is a deep network to be trained, then
    this expression will keep on getting even smaller and, as a result of that, the
    training time will become slow-paced.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming vanishing gradient
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the preceding explanation of vanishing gradient, it comes out that the
    root cause of this problem is the sigmoid function being picked as an activation
    function. The similar problem has been detected when *tanh* is chosen as an activation
    function.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to counter such a scenario, the ReLU function comes to the rescue:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '*ReLU(x)= max(0,x)*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7bd50b5-43f3-4021-96d0-4f8bad02d742.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: If the input is negative or less than zero, the function outputs as zero. In
    the second scenario, if the input is greater than zero, then the output will be
    equal to input.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the derivative of this function and see what happens:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1**: *x<0*:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8afc1d4-1539-4c14-a667-536914439103.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: '**Case 2**: *x>0*:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18ee3326-b6a5-42a1-8b59-34b0438ffd82.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'If we have to plot it, we get the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c495aa3-2edc-4e29-b2c3-469228c57c88.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: So, the derivative of ReLU is either 0 or 1\. The plot comes out to be like
    a step function. Now, we can see that we won't face the vanishing gradient problem
    as the value of the derivative doesn't lie between 0 and 1.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: However, it's still not true. We might still face this problem when the input
    value happens to be negative and we know that derivative turns out to be zero
    in this scenario. Typically, it doesn't happen that the weighted sum ends up negative,
    and we can indeed initialize weights to be only positive and/or normalize input
    between 0 and 1, if we are concerned about the chance of an issue like this occurring.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'There is still a workaround for this kind of scenario. We have got another
    function called **Leaky ReLU**, which appears as the following formula:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '*RELU (x) = max (εx, x)*'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the value of ε is typically 0.2–0.3\. We could plot it, as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bad4eb21-876b-4479-a23a-eece3cf8b89b.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Recurrent neural networks
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our thought process always has a sequence. We always understand things in an
    order. For example, if we watch a movie, we understand the next sequence by connecting
    it with the previous one. We retain the memory of the last sequence and get an
    understanding of the whole movie. We don't always go back to the first sequence
    in order to get it.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Can a neural network act like this? Traditional ones typically cannot operate
    in this manner and that is a major shortcoming. This is where recurrent neural
    networks make a difference. It comes with a loop that allows information to flow:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbf8d6b1-9c18-4882-aa18-6151eafb603f.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: Here, a neural network takes an input as **X[t] **and throws an output in the
    form of **h[t ]**. A recurrent neural network is made up of multiple copies of
    the same network that pass on the message to the successor.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were to go and unroll the preceding network, it would look like the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29ac117d-d864-4bff-a992-4a6ede2b024e.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: This chain-like nature reveals that recurrent neural networks are intimately
    related to sequences and lists. They are the natural architecture of neural networks
    to use for such data. Since the network has got an internal memory, RNNs are able
    to remember the input they received which, in turn, enables them to come up with
    accurate and precise results and predictions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have been talking about sequential data. But we need to have a proper
    understanding of this term, sequential data. This form of data is an order data
    where there exists a relationship between data at time *t* and the data at time
    *t-1*. An example of that kind of data can be financial data, time-series data,
    video, and so on. RNNs allow us to operate over sequences of vectors. For example,
    look at the following image:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5836407-530c-42b7-95b3-db16dfd08cb4.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: 'Each rectangle is represented as a vector, and arrows stand for functions.
    Input vectors are in red, output vectors are in blue, and green vectors hold the
    RNN''s state:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Vanilla mode of processing can be done without including RNN, from a fixed-sized
    input to output
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequencing the output in a proper format
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequencing the input
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sequencing the input and output (for example, machine translation: an RNN which
    reads a sentence in English and then outputs a sentence in some other language,
    like German).'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Syncing the sequenced input and output (for example, video classification where
    label each frame of the video)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations of RNNs
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent neural networks function just right when it comes to short-term dependencies.
    What this means is that, if there is just a single statement to be dealt with,
    a neural network operates fine. For example, if there is a sentence, *India's
    capital is __*, in this scenario we would invariably get the correct result as
    this is a universal statement and there is nothing like a context here. This statement
    has no dependency on the previous sentence and here, there is no previous sentence
    either.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the prediction would be *India's capital is New Delhi*.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterall, the vanilla RNN''s does not understand the context behind an input.
    We will understand with an example:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '*Staying in India meant that I gravitated towards cricket. But, after 10 years,
    I moved to the US for work.*'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '*The popular game in India is ___*.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: One can see that there is a context in the first sentence and then it changes
    in the second one. However, prediction has to be done by the network on the basis
    of the first one. It is highly likely that the popular game in India is cricket,
    but context plays a role here and it has to be understood by the network. Simple
    RNN is a failure here.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: That is where **Long Short-Term Memory** (**LSTM**) comes into the picture.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Use case
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's work on a use case that will help us in understanding the network.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'We will work on a time series problem. We have got the Google stock price dataset.
    One being training and the other being test. We will now look at a use case to
    forecast the stock prices of Google:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the libraries:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, import the training set:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Feature scaling is done in the next step:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s create a data structure with 60 time steps and 1 output:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, reshape the data:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, import the Keras libraries and packages:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will initialize the RNN with the regressor function:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, add the first LSTM layer and some dropout regularization:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, add the second LSTM layer and some dropout regularization:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Add the third LSTM layer and some dropout regularization:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Add a fourth LSTM layer and some dropout regularization:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, add the output layer:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we will compile the RNN:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will fit the RNN to the training set:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We get the real stock price of 2017 as shown:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We get the predicted stock price of 2017 as shown:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we will visualize the results as shown:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Summary
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about neural networks along with their working,
    and were introduced to backward propagation and the activation function. We studied
    network initialization and how can we initialize the different types of models.
    We learned about overfitting and dropouts in the neural network scenario.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: We introduced the concept of RNN, and studied a use case regarding the Google
    stock price dataset. In the next chapter, we will study time series analysis.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
