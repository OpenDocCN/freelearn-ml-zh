- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying Models on a Mobile Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll discuss deploying **machine learning** (**ML**) models
    on mobile devices running on the Android operating system. ML can be used to improve
    the user experience on mobile devices, especially since we can create more autonomous
    features that allow our devices to learn and adapt to user behavior. For example,
    ML can be used for image recognition, allowing devices to identify objects in
    photos and videos. This feature can be useful for applications such as augmented
    reality or photo editing tools. Additionally, ML-powered speech recognition can
    enable voice assistants to better understand and respond to natural language commands.
    Another important benefit of the autonomous features development is that they
    can work without an internet connection. This is particularly useful in situations
    where connectivity is limited or unreliable, such as when traveling in remote
    areas or during natural disasters.
  prefs: []
  type: TYPE_NORMAL
- en: Using C++ on mobile devices allows us to make programs faster and more compact.
    We can utilize as many computational resources as possible because modern compilers
    can optimize the program concerning the target CPU architecture. C++ doesn’t use
    an additional garbage collector for memory management, which can have a significant
    impact on program performance. Program size can be reduced because C++ doesn’t
    use an additional **Virtual Machine** (**VM**) and is compiled directly into machine
    code. Also, the use of C++ can help optimize battery life by more precise resource
    usage and adjusting accordingly. These facts make C++ the right choice for mobile
    devices with a limited amount of resources and can be used to solve heavy computational
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will learn how to implement real-time object
    detection using a camera on an Android mobile platform using PyTorch and YOLOv5\.
    But this chapter is not a comprehensive introduction to Android development; rather,
    it can be used as a starting point for experiments with ML and computer vision
    on an Android platform. It provides a complete minimal example of the project
    that you will be able to extend for your task.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the minimal required project for Android C++ development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the minimal required Kotlin functionality for object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initializing the image-capturing session in the C++ part of the project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using OpenCV to process native camera images and draw results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using PyTorch script to launch the YOLOv5 model on the Android platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Android Studio, **Android Software Development Kit** (**SDK**), and Android
    **Native Development** **Kit** (**NDK**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PyTorch library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A modern C++ compiler with C++20 support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CMake build system version >= 3.22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files for this chapter can be found at the following GitHub repository:
    [https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-edition/tree/main/Chapter14](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-edition/tree/main/Chapter14).'
  prefs: []
  type: TYPE_NORMAL
- en: Developing object detection on Android
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many approaches regarding how to deploy an ML model to a mobile device
    with Android. We can use PyTorch, ExecuTorch, TensorFlow Lite, NCNN, ONNX Runtime,
    or others. We’ll use the PyTorch framework in this chapter since we have discussed
    it in the previous chapters, and because it allows us to use almost any PyTorch
    model with minimal functional restrictions. Unfortunately, we will be able to
    use only the target device CPU for inference. Other frameworks, such as ExecuTorch,
    TensorFlow Lite, NCNN, and ONNX Runtime, allow you to use other inference backends,
    such as onboard GPU or **Neural Processing Unit** (**NPU**). However, this option
    also comes with a notable restriction, which is the lack of certain operators
    or functions, which can limit the types of models that can be deployed on mobile
    devices. Dynamic shape support is usually limited, making it difficult to handle
    data with varying dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge is restricted control flow, which limits the ability to use
    models with dynamic computational graphs and implement advanced algorithms. These
    restrictions can make it more challenging to deploy ML models on mobile platforms
    using the frameworks described earlier. So, there is a trade-off between the model’s
    functionality and the required performance when you deploy ML models on mobile
    devices. To balance functionality and performance, developers must carefully evaluate
    their requirements and choose a framework that meets their specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: The mobile version of the PyTorch framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is an available binary distribution of PyTorch for mobile devices available
    in the Maven repository named `org.pytorch:pytorch_android_lite`. However, this
    distribution is outdated. So, to use the most recent version, we need to build
    it from source code. We can do this in the same way as we compile its regular
    version but with additional CMake parameters to enable mobile mode. You also have
    to install the Android NDK, which includes an appropriate version of the C/C++
    compiler and the Android native libraries that are required to build the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to install Android development tools is to download the Android
    Studio IDE and use the SDK Manager tool from that. You can find the SDK Manager
    under the `cmdline-tools` package. However, you need to have Java in your system;
    for Ubuntu, you can install Java as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command line script shows you how to install all required packages
    for CLI development:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we used the `sdkmanager` manager utility to install all required components
    with appropriate versions. Using this script, the path to NDK will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Having installed build tools and NDK, we can move on to the PyTorch mobile version
    compilation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows you how to use the command line environment
    to check out PyTorch and build it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, we assumed that `/home/[USER]` is the user’s home directory. The main
    requirement when it comes to building the mobile version of PyTorch is to declare
    the `ANDROID_NDK` environmental variable, which should point to the Android NDK
    installation directory. The `ANDROID_ABI` environment variable can be used to
    specify the `arm64-v8a` architecture.
  prefs: []
  type: TYPE_NORMAL
- en: We used the `build_android.sh` script from the PyTorch source code distribution
    to build mobile PyTorch binaries. This script uses the CMake command internally,
    which is why it takes CMake parameter definitions as arguments. Notice that we
    passed the `BUILD_CAFFE2_MOBILE=OFF` parameter to disable building the mobile
    version of `Caffe2`, which is hard to use in the current version because the library
    is deprecated. The second important parameter we used was `BUILD_SHARED_LIBS=ON`,
    which enabled us to build shared libraries. Also, we disabled the Vulkan API support
    by using `DUSE_VULKAN=OFF` because it’s still experimental and has some compilation
    problems. The other parameters that were configured were the Python installation
    paths for intermediate build code generation.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the mobile PyTorch libraries, that is, `libc10.so` and `libtorch.so`,
    we can start developing the application. We are going to build an object detection
    application based on the YOLOv5 neural network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv5 is an object detection model based on the **You Only Look Once** (**YOLO**)
    architecture. It’s a state-of-the-art deep learning model that can detect objects
    in images and videos with high accuracy and speed. The model is relatively small
    and lightweight, making it easy to deploy on resource-constrained devices. Also,
    it’s fast enough, which is important for real-time applications that analyze a
    real-time video stream. It’s open source software, which means that developers
    can freely access the code and modify it to suit their needs.
  prefs: []
  type: TYPE_NORMAL
- en: Using TorchScript for a model snapshot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss how to get the YOLOv5 model TorchScript file
    so that we can use it in our mobile application. In the previous chapters, we
    discussed how to save and load model parameters and how to use the ONNX format
    to share models between frameworks. When we use the PyTorch framework, there is
    another method we can use to share models between the Python API and C++ API called
    **TorchScript**.
  prefs: []
  type: TYPE_NORMAL
- en: This method uses real-time model tracing to get a special type of model definition
    that can be executed by the PyTorch engine, regardless of API. In PyTorch, only
    the Python API can create such definitions, but we can use the C++ API to load
    the model and execute it. Also, the mobile version of the PyTorch framework doesn’t
    allow us to program neural networks with a full-featured C++ API. However, as
    was said earlier, TorchScript allows us to export and run models with complex
    control flow and dynamic shapes, which is not fully possible now for ONNX and
    other formats used in other mobile frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: For now, the YOLOv5 PyTorch model can be directly exported only into TorchScript
    for inference on mobile CPUs. For example, there are YOLOv5 models adapted for
    TensorFlow Lite and NCNN frameworks, but we will not discuss these cases because
    we are using PyTorch mostly. I have to say that using NCNN will allow you to use
    a mobile GPU though the Vulkan API and using TensorFlow Lite or ONNX Runtime for
    Android will allow you to use a mobile NPU for some devices. However, you will
    need to adapt the model into another format by reducing some functionality or
    developing it with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in this example, we are going to use the TorchScript model to perform object
    detection. To get the YOLOv5 model, we have to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the model repository from GitHub and install dependencies; run these
    commands in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: python export.py --weights yolov5s.torchscript --include torchscript --optimize
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The script from the second step automatically traces the model and saves the
    TorchScript file for us. After we made these steps, there will be the `yolo5s.torchscript`
    file, which we will be able to load and use in C++.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have all the prerequisites to move on and make an Android Studio project
    for our application.
  prefs: []
  type: TYPE_NORMAL
- en: The Android Studio project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will use the Android Studio IDE to create our mobile application.
    We can use a default `objectdetection` and select **Kotlin** as the programming
    language, then Android Studio will create a particular project structure; the
    following sample shows the most valuable parts of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `cpp` folder contains the C++ part of the whole project. In this project,
    the Android Studio IDE created the C++ part as a native shared library project
    that had been configured with the CMake build generation system. The `java` folder
    contains the Kotlin part of the project. In our case, it is a single file that
    defines the main activity—the object that’s used as a connection between the UI
    elements and event handlers. The `res` folder contains project resources, such
    as UI elements and string definitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to create the `jniLibs` folder, under the `main` folder, with
    the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Android Studio requires us to place additional native libraries in such folders
    to correctly package them into the final application. It also allows the `arm64-v8a`
    folder because they have only been compiled for this CPU architecture. If you
    have libraries for other architectures, you have to create folders with corresponding
    names.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, in the previous subsection, we learned how to get the YOLOv5 torch script
    model. The model file and corresponding file, along with the class IDs, should
    be placed in the `assets` folder. This folder should be created beside the `JniLibs`
    folder on the same folder level, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The file that maps string class names to numerical IDs, which the model returns,
    can be downloaded from [https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml).
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we simply convert the YAML file into the text one to make its
    parsing simpler.
  prefs: []
  type: TYPE_NORMAL
- en: The IDE uses the Gradle build system for project configuration, so there are
    two files named `build.gradle.kts`, one for the application module and another
    one for the project properties. Look at the `build.gradle` file for the application
    module in our example. There are two variables that define paths to the PyTorch
    source code folder and to the OpenCV Android SDK folder. You need to update their
    values if you change these paths. The prebuilt OpenCV Android SDK can be downloaded
    from the official GitHub repository ([https://github.com/opencv/opencv/releases](https://github.com/opencv/opencv/releases))
    and simply unpacked.
  prefs: []
  type: TYPE_NORMAL
- en: The Kotlin part of the project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this project, we are going to use the native C++ part to draw the captured
    picture with bounding boxes and class labels for detected objects. So, there will
    be no UI code and declarations in the Kotlin part. However, the Kotlin part will
    used to request and check required camera access permissions. Also, it will start
    a camera capture session if permissions are granted. All Kotlin code will be in
    the `MainActivity.kt` file.
  prefs: []
  type: TYPE_NORMAL
- en: Preserving camera orientation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our project, we skip the implementation of device rotation handling to make
    code simpler and show just the most interesting parts of working with the object
    detection model. So, to make our code stable, we have to disable the landscape
    mode, which can be done in the `AndroidManifest.xml` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We added the screen orientation instruction to the activity entity. This is
    not a good solution because there are devices that work only in landscape mode
    and our application will not work with them. In a real production-ready application,
    you should handle different orientation modes; for example, for most smartphones,
    this dirty solution should work.
  prefs: []
  type: TYPE_NORMAL
- en: Handling camera permission requests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are no C++ APIs in Android NDK to request permissions. We can request
    the required permission only from the Java/Kotlin side or with JNI from C++. It’s
    simpler to write the Kotlin code to request the camera permission than to write
    JNI calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is modifying the declaration of the `MainActivity` class to
    be able to process permission request results. It’s done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we inherited the `MainActivity` class from the `OnRequestPermissionsResultCallback`
    interface. It gives us the possibility to override the `onRequestPermissionsResult`
    method where we will be able to check a result. However, to get a result, we have
    to make a request first, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We overrode the `onResume` method of the `Activity` class. This method is called
    every time when our application starts to work or is resumed from the background.
    We initialized the `cameraPermission` variable with the required camera permission
    constant value. Then, we checked whether we already granted this permission using
    the `checkSelfPermission` method. If we don’t have the camera permission, we ask
    for it with the `requestPermissions` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that we used the `CAM_PERMISSION_CODE` code to identify our request
    in the callback method. If we were granted access to a camera, we tried to get
    the back-facing camera ID and initialize the object detection pipeline for this
    camera. If we can’t get access to a camera, we finish the Android activity with
    the `finish` method and the corresponding message. In the `onRequestPermissionsResult`
    method, we check if the required permission was granted, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: At first, we called the parent method to preserve the standard application behavior.
    Then, we checked the permission identification code, `CAM_PERMISSION_CODE`, and
    whether or not the permission was granted. In the failure case, we just show the
    error message and finish the Android activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we said before, in the success case, we looked for the back-facing camera
    ID, which is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We got the instance of the `CameraManager` object and used this object to iterate
    over every camera on a device. For each camera object, we asked for its characteristics,
    supported hardware level, and where this camera faces. If a camera is a regular
    legacy device and faces back, we return its ID. If we didn’t find a suitable device,
    we returned an empty string.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having granted the camera access permission and the camera ID, we called the
    `initObjectDetection` function to start image capturing and object detection.
    This and the `stopObjectDetection` function are functions provided through the
    JNI from the C++ part to the Kotlin part. The `stopObjectDetection` function is
    used to stop the camera capturing session, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the overridden `onPause` activity method, we just stopped the camera capturing
    session. This method is called every time the Android application is closed or
    goes into the background.
  prefs: []
  type: TYPE_NORMAL
- en: Native library loading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are two methods, `initObjectDetection` and `stopObjectDetection`, which
    are JNI calls to the native library functions that are implemented with C++. To
    connect the native library with the Java or Kotlin code, we use JNI. This is a
    standard mechanism that’s used for calling C/C++ functions from Kotlin or Java.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to load the native library with the `System.LoadLibrary` call
    and place it in the companion object for our activity. Then, we have to define
    the methods that are implemented in the native library by declaring them as `external`.
    The following snippet shows how to define these methods in Kotlin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Such declarations allow Kotlin to find the corresponding native library binary,
    load it, and access the functions. JNI works by providing a set of APIs that allow
    Java code to call into native code and vice versa. The JNI API consists of a number
    of functions that can be called from Java or native code. These functions allow
    you to perform tasks, such as creating and accessing Java objects from native
    code, calling Java methods from native code, and accessing native data structures
    from Java.
  prefs: []
  type: TYPE_NORMAL
- en: Internally, JNI works by mapping Java objects and types to their corresponding
    native counterparts. This mapping is done using the `JNIEnv` interface, which
    provides access to the `JNIEnv` is used to find the corresponding native method
    and pass it the necessary arguments. Similarly, when a native method returns a
    value, `JNIEnv` is used to convert the native value to a Java object. The JVM
    manages memory for both Java and native objects. However, native code must explicitly
    allocate and free its own memory. JNI provides functions for allocating and freeing
    memory, as well as for copying data between Java and native memory. JNI code must
    be thread-safe. This means that any data accessed by JNI must be properly synchronized
    to avoid race conditions. Using JNI can have performance implications. Native
    code is typically faster than Java code, but there is overhead associated with
    calling into native code through JNI.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss the C++ part of the project.
  prefs: []
  type: TYPE_NORMAL
- en: The native C++ part of the project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main functionality of this example project is implemented in the native
    C++ part. It’s designed to use the OpenCV library to deal with camera images and
    the PyTorch framework for object detection model inference. Such an approach allows
    you to port this solution to another platform if needed and allows you to use
    standard desktop instruments, such as OpenCV and PyTorch, to develop and debug
    algorithms that will be used on mobile platforms.
  prefs: []
  type: TYPE_NORMAL
- en: There are two main C++ classes in this project. The `Detector` class is the
    application facade that implements a connection with the Android activity image-capturing
    pipeline, and delegates object detection to the second class, `YOLO`. The `YOLO`
    class implements the object detection model loading and its inference.
  prefs: []
  type: TYPE_NORMAL
- en: The following subsections will describe the implementation details of these
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Initialization of object detection with JNI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We finished our discussion of the Kotlin part by talking about the JNI function
    declarations. The corresponding C++ implementation for `initObjectDetection` and
    `stopObjectDetection` are located in the `native-lib.cpp` file. This file is automatically
    created by the Android Studio IDE for the native activity projects. The following
    code snippet shows the `initObjectDetection` function definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We followed JNI rules to make the function declaration correct and visible from
    the Java/Kotlin part. The name of the function includes the full Java package
    name, including namespaces, and our first two required parameters are the `JNIEnv*`
    and `jobject` types. The third parameter is the string and corresponds to the
    camera ID; this is the parameter that exists in the Kotlin declaration of the
    function.
  prefs: []
  type: TYPE_NORMAL
- en: In the function implementation, we checked whether the `ObjectDetector` object
    was already instantiated and, in this case, we called the `allow_camera_session`
    method with the camera ID and then called the `configure_resources` method. These
    calls make the `ObjectDetector` object remember what camera to use and initialize,
    configure the output window, and initialize the image-capturing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second function we used in the Kotlin part is the `stopObjectDetection`,
    and its implementation is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, we just released resources used for the image-capturing pipeline because
    when the application is suspended, access to the camera device is blocked. When
    the application is activated again, the `initObjectDetection` function will be
    called and the image-capturing pipeline will be initialized again.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that we used the `LOGI` and the `LOGE` functions, which are defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We defined these functions to log messages into the Android `logcat` subsystem
    more easily. This series of functions uses the same tag for logging and has fewer
    arguments than the original `__android_log_xxx` functions. Also, the log level
    was encoded in the function name.
  prefs: []
  type: TYPE_NORMAL
- en: Main application loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This project will use the Native App Glue library. This is a library for Android
    developers that helps to create native applications. It provides an abstraction
    layer between the Java code and the native code, making it easier to develop applications
    using both languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of this library allows us to have the standard `main` function with
    a loop that runs continuously, updating the UI, processing user input, and responding
    to system events. The following code snippet shows how we implemented the main
    function in the `native-lib.cpp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This `android_main` function takes the instance of the `android_app` type, instead
    of regular `argc` and `argv` parameters. The `android_app` is a C++ class that
    provides access to the Android framework and allows you to interact with system
    services. Also, you can use it to access the device hardware, such as sensors
    and cameras.
  prefs: []
  type: TYPE_NORMAL
- en: The `android_main` main function is the starting point for our native module.
    So, we initialized the global `object_detector_` object here, and it became available
    for the `initObjectDetection` and `stopObjectDetection` functions. For initialization,
    the `ObjectDetector` instance takes the pointer to the `android_app` object.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we attached the command processing function to the Android application
    object. Finally, we started the main loop, and it worked until the application
    was destroyed (closed). In this loop, we used the `ALooper_pollOnce` Android NDK
    function to get a pointer to the commands (events) poller object.
  prefs: []
  type: TYPE_NORMAL
- en: We called the `process` method of this object to dispatch the current command
    to our `ProcessAndroidCmd` function through the `app` object. At the end of the
    loop, we used our object detector object to grab the current camera picture and
    process it in the `draw_frame` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ProcessAndroidCmd` function is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, we processed only two commands that correspond to the application window
    initialization and termination. We used them to initialize and clear the image-capturing
    pipeline in the object detector. When the window is created, we configure its
    dimensions according to the capturing resolution. The window termination command
    allows us to clear capturing resources to prevent access to the already blocked
    camera device.
  prefs: []
  type: TYPE_NORMAL
- en: That is all the information about the `native-lib.cpp` file. The next subsections
    will look at the `ObjectDetector` class implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: The ObjectDetector class overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is the main facade of the whole object detection pipeline of our application.
    The following list shows the functionality items it implements:'
  prefs: []
  type: TYPE_NORMAL
- en: Camera device access management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application window dimensions configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image-capturing pipeline management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Camera image converting into OpenCV matrix objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawing an object detection result into the application window
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delegating the object detection to the YOLO inference object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we start looking at these item details, let’s see how the constructor,
    the destructor, and some helper methods are implemented. The constructor implementation
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We just saved the pointer to the `android_app` object and created the `YOLO`
    class inference object. Also, we used the `android_app` object to get a pointer
    to the `AssetManager` object, which is used to load files packaged into the **Android
    Application Package** (**APK**). The destructor is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We called the `release_resources` method, which is where we close the opened
    camera device and clear capturing pipeline objects. The following code snippet
    shows the methods that are used from the Kotlin part through the `initObjectDetection`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In `allow_camera_session`, we saved the camera ID string; the device with this
    ID will be opened in the `configure_resources` method. As we already know, the
    camera ID will be passed to `ObjectDetector` only if the required permission is
    granted and there is a back-facing camera on the Android device. So, we defined
    `is_session_allowed` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, we just checked if a camera ID is not empty.
  prefs: []
  type: TYPE_NORMAL
- en: The following subsections will show the main functionality items in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Camera device and application window configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is the `create_camera` method in the `ObjectDetection` class that implements
    the creation of a camera manager object and a camera device opening as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`camera_mgr_` is the `ObjectDetector` member variable and after initialization,
    it is used to open a camera device. The pointer to the opened camera device will
    be stored in the `camera_device_` member variable. Also, notice that we used the
    camera ID string to open the particular device. The `camera_device_callbacks`
    variable is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We defined the `ACameraDevice_stateCallbacks` structure object with references
    to functions that simply report if the camera is opened or closed. These handlers
    can do some more useful work in other applications, but we can’t initialize them
    with nulls due to the API requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `create_camera` method is called in the `configure_resources` method of
    the `ObjectDetection` class. This method is called every time the application
    is activated and it has the following implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In the beginning, we checked that there are all required resources: the camera
    ID, the `android_app` object, and that this object has a pointer to the application
    window. Then, we created a camera manager object and opened a camera device. Using
    the camera manager, we got the camera sensor orientation to configure the appropriate
    width and height for the application window. Also, using values for image capture
    width and height, we configured the window dimensions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here, we used the `ACameraManager_getCameraCharacteristics` function to get
    the camera metadata characteristics object. Then, we read the `ACAMERA_SENSOR_ORIENTATION`
    property with the `ACameraMetadata_getConstEntry` function. After, we chose the
    appropriate width and height order based on the orientation used with the `ANativeWindow_setBuffersGeometry`
    function to set application output window dimensions and rendering buffer format.
  prefs: []
  type: TYPE_NORMAL
- en: The format we set is `32`-bit `800` for height and `600` for width in portrait
    mode. This orientation handling is very simple and is needed only to work with
    output window buffers correctly. Previously, we disabled the landscape mode for
    our application so we will ignore the camera sensor orientation in the camera
    image decoding.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the `configure_resources` method, we created the camera reader
    object and initialized the capturing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Image-capturing pipeline construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Previously, we saw that before the capturing pipeline initialization, we created
    the image reader object. It’s done in the `create_image_reader` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We used `AImageReader_new` to create the `AImageReader` object with a particular
    width and height, the YUV format, and four image buffers. The width and height
    values we used were the same that were used for the output window dimensions configuration.
    The YUV format was used because it’s the native image format for most camera devices.
    Four image buffers were used to make image capturing sightly independent from
    their processing. It means that the image reader will fill one image buffer with
    camera data while we are reading another buffer and processing it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The capture session initialization is a complex process that requires several
    objects’ instantiation and their connection with each other. The `create_session`
    method implements it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We started with getting a native window from the image reader object and acquiring
    it. The window acquisition means that we took the reference to the window and
    the system should not delete it. This image reader window will be used as output
    for the capturing pipeline, so camera images will be drawn into it.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we created the session output object and the container for the session
    output. The capturing session can have several outputs and they should be placed
    into a container. Every session output is a connection object for a concrete surface
    or a window output; in our case, it’s the image reader window.
  prefs: []
  type: TYPE_NORMAL
- en: Having configured session outputs, we created the capture request object and
    made sure that its output target was the image reader window. We configured the
    capture request for our opened camera device and the preview mode. After that,
    we instantiated the capturing session object and pointed it to the opened camera
    device, which had the container with the outputs we created earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we started the capturing by setting the repeated request for the session.
    The connection between the session and capture request is the follows: we created
    the capturing session that was configured with a list of possible outputs, and
    the capture request specifies what surfaces will actually be used. There can be
    several capture requests and several outputs. In our case, we have a single capture
    request with a single output that will be continuously repeated. So, in general,
    we will capture real-time pictures for the camera like a video stream. The following
    picture shows the logical scheme of an image data flow in the capturing session:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – The logical data flow in a capturing session](img/B19849_14_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 – The logical data flow in a capturing session
  prefs: []
  type: TYPE_NORMAL
- en: This is not the actual data flow scheme but the logical one that shows how the
    capture session objects are connected. The dotted line shows the request path
    and the solid line shows the logical image data path.
  prefs: []
  type: TYPE_NORMAL
- en: The capture image and the output window buffer management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When we discussed the main application loop, we mentioned the `draw_frame`
    method, which is called in this loop after command processing. This method is
    used to take a captured image from the image reader object, then detect objects
    on it and draw the detection results in the application window. The following
    code snippet shows the `draw_fame` method implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We acquired the next image received by the image reader object. Remember that
    we initialized it to have four image buffers. So, we acquire images from these
    buffers one by one in the main loop, and while we process one image, the capturing
    session fills another one that has already been processed. It’s done in a circular
    manner. Having the image from a camera, we acquired and locked the application
    window, but if the lock fails, we delete the current image reference, stop processing,
    and go to the next iteration of the main loop. Otherwise, if we successfully lock
    the application window, we process the current image, detect objects on it, and
    draw detection results into an application window—this is done in the `process_image`
    method. This method takes the `AImage` and the `ANativeWindow_Buffer` objects.
  prefs: []
  type: TYPE_NORMAL
- en: When we lock the application window, we get the pointer to the internal buffer
    that will be used for drawing. After we process the image and draw results, we
    unlock the application window to make its buffer available for the system, release
    the reference to the window, and delete the reference to the image object. So,
    this method is mostly about resource management, and the real image processing
    is done in the `process_image` method, which we will discuss in the following
    subsection.
  prefs: []
  type: TYPE_NORMAL
- en: The captured image processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `process_image` method implements the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert Android YUV image data into the OpenCV matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dispatch the image matrix to the YOLO object detector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw detection results into the OpenCV matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the OpenCV results matrix into the RGB (red, blue, green) window buffer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s see implementations for these tasks one by one. The `process_image` method
    signature looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This method takes the application window buffer object for results drawing
    and the image object for actual processing. To be able to process an image, we
    have to convert it into some appropriate data-structure format; in our case, this
    is the OpenCV matrix. We start with image format properties checking as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We checked that the image format is YUV (Luminance (Y), blue luminance (U),
    and red luminance (V)) and the image has three planes, so we can proceed with
    its conversion. Then, we got image dimensions, which will be used later. After
    that, we verified the input data we extracted from the YUV plane data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We got strides, data sizes, and pointers to the actual YUV plane data. In this
    format, the image data is split into three components: luma (`y`), representing
    brightness, and two chroma components (`u` and `v`), which represent color information.
    The `y` component is usually stored at full resolution, while the `u` and `v`
    components may be subsampled. This allows for more efficient storage and transmission
    of video data. The Android YUV image uses the half-sized resolution for `u` and
    `v`. The strides will allow us to correctly access the row data in the plane buffers;
    these strides depend on the image resolution and a data memory layout.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having the YUV plane data and its strides and lengths, we convert them into
    OpenCV matrix objects, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We created the two `cv::Size` objects to store the original image size for
    the Y plane and the half size for the `u` and `v` planes. Then, we used these
    sizes, pointers to data, and strides to create an OpenCV matrix for every plane.
    We didn’t copy actual data into the OpenCV matrix objects; they will use data
    pointers that were passed for initialization. Such a view-creation approach saves
    memory and computational resources. The `y`-plane matrix has the 8-bit single-channel
    type but the `u` and `v` matrices have the 8-bit 2-channel type. We can use these
    matrices with the OpenCV `cvtColorTwoPlane` function to convert them into the
    RGBA format as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We used the address difference to determine the ordering of the u and v planes:
    a positive difference indicates the NV12 format, while a negative difference indicates
    the NV21 format. `NV12` and `NV21` are types of the YUV format that differ in
    the order of the `u` and `v` components in the chroma plane. In `NV12`, the `u`
    component precedes the `v` component, while in `NV21`, it’s the opposite. Such
    plane ordering plays a role in memory consumption and image processing performance,
    so the choice of which to use depends on the actual task and project. Also, the
    format can depend on the actual camera device, which is why we added this detection.'
  prefs: []
  type: TYPE_NORMAL
- en: The `cvtColorTwoPlane` function takes the `y`-plane and `uv`-plane matrices
    as input arguments and outputs the RGBA image matrix into the `rgba_img_` variable.
    The last argument is the flag that tells the function what actual conversion it
    should perform. Now, this function can convert only YUV formats into RGB or RGBA
    formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we said before, our application works only in portrait mode, but to make
    the image look normal, we need to rotate it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Android camera sensors return camera images rotated even if we fixed our orientation,
    so we used the `cv::rotate` function to make it look vertical.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having prepared the RGBA image, we pass it to the `YOLO` object detector and
    get the detection results. For every result item, we draw rectangles and labels
    on the image matrix we have already used for detection. These steps are implemented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We called the `detect` method of the `YOLO` object and got the `results` container.
    This method will be discussed later. Then, for each item in the container, we
    draw a bounding box and a text label for the detected object. We used the OpenCV
    `rectangle` function with the `rgba_img_` destination image argument. Also, the
    text was rendered into the `rgba_img_` object. The detection result is the structure
    defined in the `yolo.h` header file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: So, a detection result has the class index and name properties, the model confidence
    score, and the bounding box in the image coordinates. For our results visualization,
    we used only rectangle and class name properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last task that the `process_image` method does is to render the resulting
    image into the application window buffer. It’s implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We created the OpenCV `buffer_mat` matrix to wrap the given window buffer. Then,
    we simply used the OpenCV `copyTo` method to put the RGBA image with rendered
    rectangles and class labels into the `buffer_mat` object. `buffer_mat` is the
    OpenCV view for the Android window buffer. We created it to follow the window
    buffer format we configured in the `configure_resources` method, the `WINDOW_FORMAT_RGBA_8888`
    format. So, we created the OpenCV matrix with the 8-bit 4-channel type and used
    the buffer stride information to satisfy memory layout access. Such a view allows
    us to write less code and use OpenCV routines for memory management.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the main facade of our object detection application and in the
    following subsections, we will discuss details of how the YOLO model inference
    is implemented and how its results are parsed into the `YOLOResult` structures.
  prefs: []
  type: TYPE_NORMAL
- en: The YOLO wrapper initialization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is only the constructor and the `detect` method in the `YOLO` class public
    API. We already saw that the `YOLO` object is initialized in the `ObjectDetector`
    class constructor, and the `detect` method is used in the `process_image` method.
    The `YOLO` class constructor takes only the asset manager object as a single argument
    and is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Remember that we added the `yolov5s.torchscript` and the `classes.txt` files
    to the `assets` folder of our project. These files can be accessed in the application
    with the `AAssetManager` class object; this object was taken from the `android_app`
    object in the `android_main` function. So, in the constructor, we loaded the model
    binary and classes list file with a call to the `read_asset` function. Then, the
    model binary data was used to load and initialize the PyTorch script module with
    the `torch::jit::_load_for_mobile` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the scripted model should be saved with optimization for mobile
    and loaded with the corresponding function. When PyTorch for mobile was compiled,
    the regular `torch::jit::load` functionality was automatically disabled. Let’s
    look at the `read_asset` function that reads assets from the application bundle
    as `std::vector<char>` objects. The following code shows its implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: There are four Android framework functions that we used to read an asset from
    the application bundle. The `AAssetManager_open` function opened the asset and
    returned the not null pointer to the `AAsset` object. This function assumes that
    the path to the asset is in the file path format and that the root of this path
    is the `assets` folder. After we opened the asset, we used the `AAsset_getLength`
    function to get the file size and allocated the memory for `std::vector<char>`
    with the `std::vector::resize` method. Then, we used the `AAsset_read()` function
    to read the whole file to the `buf` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It takes the pointer to the asset object to read from
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It takes the `void*` pointer to the memory buffer to read in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It measures the size of the bytes to read
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, as you can see, the assets API is pretty much the same as the standard C
    library API for file operations. When we’d finished working with the asset object,
    we used the `AAsset_close` function to notify the system that we didn’t need access
    to this asset anymore. If your assets are in the `.zip` archive format, you should
    check the number of bytes returned by the `AAsset_read` function because the Android
    framework reads archives chunk by chunk.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might see that we didn’t pass the vector of chars directly to the `torch::jit::_load_for_mobile`
    function. This function doesn’t work with standard C++ streams and types; instead,
    it accepts a pointer to an object of the `caffe2::serialize::ReadAdapterInterface`
    class. The following code shows how you to make the concrete implementation of
    the `caffe2::serialize::ReadAdapterInterface` class, which wraps the `std::vector<char>`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ReaderAdapter` class overrides two methods, `size` and `read`, from the
    `caffe2::serialize::ReadAdapterInterface` base class. Their implementations are
    pretty obvious: the `size` method returns the size of the underlying vector object,
    while the `read` method copies the `n` bytes (chars) from the vector to the destination
    buffer with the standard algorithm function, that is, `std::copy_n`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To load class information, we used the `VectorStreamBuf` adapter class to convert
    `std::vector<char>` into the `std::istream` type object. It was done because the
    `YOLO::load_classes` method takes an object of the `std::istream` type. The `VectorStreamBuf`
    implementation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We inherited from the `std::basic_streambuf` class and in the constructor,
    we initialized the `streambuf` internal data with char values from the input vector.
    Then, we used an object of this adapter class as regular C++ input stream. You
    can see it in the `load_classes` method implementation, which is shown in the
    following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The lines in `classes.txt` are in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: So, we read this file line by line and split each line at the position of the
    first space character. The first part of each line is the class identifier, while
    the second one is the class name. To match the model’s evaluation result with
    the correct class name, we created the dictionary (map) object, where the key
    is the `id` value and the value is `label` (e.g., the class name)s.
  prefs: []
  type: TYPE_NORMAL
- en: The YOLO detection inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `detect` method of the `YOLO` class is the place where we do the actual
    object detection. This method takes the OpenCV matrix object that represents the
    RGB image as an argument and its implementation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We defined constants that represent the width and height of the model input;
    it’s `640` x `640` because the YOLO model was trained on images of this size.
    Using these constants, we resized the input image. Also, we removed the alpha
    channel and made the RGB image. We calculated scale factors for image dimensions,
    as they will be used to re-scale detected object boundaries to the original image
    size. Having scaled the image, we converted the OpenCV matrix into a PyTorch Tensor
    object using the `mat2tensor` function, whose implementation we will discuss later.
    The object type cast of the PyTorch Tensor object we added to the container of
    the `torch::jit::IValue` values was done automatically. There is a single element
    in this `inputs` container since the YOLO model takes a single RGB image input.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we used the `forward` function of the YOLO `model_` object to perform
    inference. The PyTorch API script modules return the `torch::jit::Tuple` type.
    So, we explicitly cast the returned `torch::jit::IValue` object to the tuple and
    took the first element. This element was cast to the PyTorch `Tensor` object and
    the batch dimension was removed from it with the `squeeze` method. So, we got
    the `torch::Tensor` type `output` object of size `25200` x `85`. Here, `25200`
    is the number of detected objects and we will apply the non-max suppression algorithm
    to get the final reduced output. The `85` means `80` class scores, `4` bounding
    box locations (x, y, width, height), and `1` confidence score. The resulting tensor
    was parsed into the `YOLOResult` structures in the `output2results` method. As
    we said, we used the `non_max_suppression` method to select the best detection
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the details of all the intermediate functions we used for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Converting OpenCV matrix into torch::Tensor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `mat2tensor` function convents an OpenCV `mat` object into a `torch::Tensor`
    object and is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We used the `torch::from_blob` function to create the torch `Tensor` object
    from the raw data. The data pointer is what we took from the OpenCV object with
    the `data` property. The shape we used, `[HIGHT, WIDTH, CHANNELS]`, follows the
    OpenCV memory layout where the last dimension is the channel number dimension.
    Then, we made tensor float and normalized it to the `[0,1]` interval. PyTorch
    and the YOLO model use different shape layouts to `[CHANNELS, HEIGHT, WIDTH]`.
    So, we transpose the tensor channels appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Processing model output tensor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next function we used is `output2results`, which converts the output Tensor
    object into the vector of the `YOLOResult` structures. It has the following implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'In the beginning, we used the `accessor<float, 2>` method of the torch Tensor
    object to get a very useful accessor for the tensor. This accessor allowed us
    to use the square brackets operator to access the elements in a multidimensional
    tensor. The number `2` means that the tensor is 2D. Then, we made a loop over
    tensor rows because every row corresponds to a single detection result. Inside
    the loop, we did the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We read the confidence score from the element with row index `4`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We continued result row processing if the confidence score was greater than
    the threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We read the `0, 1, 2, 3` elements, which are the [x, y, width, height] coordinates
    of the bounding rectangle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the previously calculated scale factors, we converted these coordinates
    into the [left, top, width, height] format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We read the elements 5-84, which are class probabilities, and selected the class
    with the maximum value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We created the `YOLOResult` structure with calculated values and inserted it
    into the `results_` container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The bounding box calculation was done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The YOLO model returns X and Y coordinates for the center of a rectangle so
    we converted them into the image(screen) coordinate system: to the top-left point.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The class ID selection was implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We used the loop over the last elements that represent 79 class probabilities
    to select the index of the maximum value. This index was used as a class ID.
  prefs: []
  type: TYPE_NORMAL
- en: NMS and IoU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Non-Maximum Suppression** (**NMS**) and **Intersection over Union** (**IoU**)
    are two key algorithms used in YOLO for refining and filtering the output predictions
    to get the best results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'NMS is used to suppress or eliminate duplicate detections that overlap with
    each other. It works by comparing the predicted bounding boxes from the network
    and removing those that have high overlaps with others. For example, if there
    are two bounding boxes predicted for the same object, NMS will keep only the one
    with the highest confidence score and discard the rest. The following picture
    shows how NMS works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – NMS](img/B19849_14_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 – NMS
  prefs: []
  type: TYPE_NORMAL
- en: 'IoU is another algorithm used in conjunction with NMS to measure the overlap
    between bounding boxes. IoU calculates the ratio of an intersection area to a
    union area between two boxes. The `0` to `1`, where `0` means no overlap, and
    `1` indicates perfect overlap. The following picture shows how IoU works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – IoU](img/B19849_14_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.3 – IoU
  prefs: []
  type: TYPE_NORMAL
- en: 'We implemented NMS in the `non_max_suppression` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: At first, we sorted all detection results by confidence score in descending
    order. We marked all results as active. If a detection result is active, then
    we can compare another result with it, otherwise, the result is already suppressed.
    Then, every active detection result was sequentially compared with the following
    active results; remember that the container is sorted. The comparison was done
    by calculating the IoU value for bounding boxes and comparing the IoU value with
    a threshold. If the IoU value is greater than the threshold, we marked the result
    with a lower confidence value as non-active; we suppressed it. So, we defined
    the nested comparison loop. In the outer loop, we ignored suppressed results too.
    Also, this nested loop has the check for the maximum allowed number of results;
    refer to the use of the `nms_limit` value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The IoU algorithm for two bounding boxes is implemented in the `IOU` function
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: At first, we checked bounding boxes for emptiness; if one of the boxes is empty,
    the IoU value is zero. Then, we calculated the intersection area. This was done
    by finding the minimum and maximum values for X and Y, considering both bounding
    boxes, and then taking the product of the difference between these values. The
    union area was calculated by summing the areas of two bounding boxes minus the
    intersection area. You can see this calculation in the return statement where
    we calculated the area’s ratio.
  prefs: []
  type: TYPE_NORMAL
- en: Together, NMS and IoU help improve the accuracy and precision of YOLO by discarding
    false positives and ensuring that only relevant detections are included in the
    final output.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we looked at the implementation of object detection applications
    for the Android system. We learned how to export a pre-trained model from a Python
    program as a PyTorch script file. Then, we delved into developing a mobile application
    with Android Studio IDE and the mobile version of the PyTorch C++ library. In
    the following figure, you can see an example of the application output window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – Object detection application output](img/B19849_14_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.4 – Object detection application output
  prefs: []
  type: TYPE_NORMAL
- en: In this figure, you can see that our application successfully detected a laptop
    and a computer mouse in front of the smartphone camera. Every detection result
    was marked with the bounding box and corresponding label.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to deploy ML models, especially neural networks,
    to mobile platforms. We examined that, on these platforms, we usually need a customized
    build of the ML framework that we used in our project. Mobile platforms use different
    CPUs, and sometimes, they have specialized neural network accelerator devices,
    so you need to compile your application and ML framework in regard to these architectures.
    These architectures differ from development environments, and you often use them
    for two different purposes. The first case is to use powerful machine configuration
    with GPUs to accelerate the ML training process, so you need to build your application
    while taking the use of one or multiple GPUs into account. The other case is using
    a device for inference only. In this case, you typically don’t need a GPU at all
    because a modern CPU can, in many cases, satisfy your performance requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we developed an object detection application for the Android
    platform. We learned how to connect the Kotlin module with the native C++ library
    through JNI. Then, we examined how to build the PyTorch C++ library for Android
    using the NDK and saw what limitations there are to using the mobile version.
  prefs: []
  type: TYPE_NORMAL
- en: This was the last chapter of the book; I hope you have enjoyed this book and
    found it helpful in your journey to mastering the use of C++ for ML. I hope that
    by now, you have gained a solid understanding of how to leverage the power of
    C++ to build robust and efficient ML models. Throughout the book, I have aimed
    to provide clear explanations of complex concepts, practical examples, and step-by-step
    guides to help you get started with C++ for ML. I have also included tips and
    best practices to help you avoid common pitfalls and optimize your models for
    performance. I want to remind you that the possibilities are endless when it comes
    to using C++ for ML.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you are a beginner or an experienced developer, there is always something
    new to learn and explore. With that in mind, I encourage you to continue to push
    your boundaries and experiment with different approaches and techniques. The world
    of ML is constantly evolving, and by staying up to date with the latest trends
    and developments, you can stay ahead of the curve and build cutting-edge models
    that can solve complex problems.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you again for choosing my book and for taking the time to learn about
    using C++ for ML. I hope that you find it to be a valuable resource and that it
    helps you on your journey toward becoming a skilled and successful ML developer.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch C++ API: [https://pytorch.org/cppdocs/](https://pytorch.org/cppdocs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation for app developers: [https://developer.android.com/develop](https://developer.android.com/develop)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Android NDK: [https://developer.android.com/ndk](https://developer.android.com/ndk)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch guides for mobile development: [https://pytorch.org/mobile/android/](https://pytorch.org/mobile/android/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch guide for optimized mobile script exporting: [https://pytorch.org/tutorials/recipes/script_optimized.html](https://pytorch.org/tutorials/recipes/script_optimized.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenCV Android SDK tutorial: [https://docs.opencv.org/4.x/d5/df8/tutorial_dev_with_OCV_on_Android.html](https://docs.opencv.org/4.x/d5/df8/tutorial_dev_with_OCV_on_Android.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ExcuTorch – a new framework for running PyTorch on embedded devices: [https://pytorch.org/executorch/stable/index.html](https://pytorch.org/executorch/stable/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
