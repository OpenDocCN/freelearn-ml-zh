- en: Learning Object Tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about video surveillance, background modeling,
    and morphological image processing. We discussed how we can use different morphological
    operators to apply cool visual effects to input images. In this chapter, we are
    going to learn how to track an object in a live video. We will discuss the different
    characteristics of an object that can be used to track it. We will also learn
    about different methods and techniques for object tracking. Object tracking is
    used extensively in robotics, self-driving cars, vehicle tracking, player tracking
    in sports, and video compression.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will know the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to track objects of a specific color
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build an interactive object tracker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What a corner detector is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to detect good features to track
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build an optical flow-based feature tracker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires familiarity with the basics of the C++ programming language.
    All the code used in this chapter can be downloaded from the following GitHub
    link: [https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_09](https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_09). The
    code can be executed on any operating system, though it is only tested on Ubuntu.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2SidbMc](http://bit.ly/2SidbMc)'
  prefs: []
  type: TYPE_NORMAL
- en: Tracking objects of a specific color
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to build a good object tracker, we need to understand what characteristics
    can be used to make our tracking robust and accurate. So, let's take a baby step
    in that direction and see whether we can use colorspace information to come up
    with a good visual tracker. One thing to keep in mind is that color information
    is sensitive to lighting conditions. In real-world applications, you will have
    to do some preprocessing to take care of that. But for now, let's assume that
    somebody else is doing that and we are getting clean color images.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different colorspaces, and picking a good one will depend on
    the different applications that a user is using. While RGB is the native representation
    on a computer screen, it's not necessarily ideal for humans. When it comes to
    humans, we give names to colors more naturally based on their hue, which is why
    **hue saturation value** (**HSV**) is probably one of the most informative colorspaces.
    It closely aligns with how we perceive colors. Hue refers to the color spectrum,
    saturation refers to the intensity of a particular color, and value refers to
    the brightness of that pixel. This is actually represented in a cylindrical format.
    You can find a simple explanation at [http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html](http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html).
    We can take the pixels of an image to the HSV colorspace and then use this colorspace
    to measure distances in this colorspace and threshold in this space thresholding
    to track a given object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following frame in the video:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1faadcdb-ac4b-42d9-a9e6-912196cc317b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you run it through the colorspace filter and track the object, you will
    see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37d137d4-7ba2-4630-8f26-36b464dc3c5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see here, our tracker recognizes a particular object in the video
    based on the color characteristics. In order to use this tracker, we need to know
    the color distribution of our target object. Here is the code to track a colored
    object, which selects only pixels that have a certain given hue. The code is well-commented,
    so read the explanation about each term to see what''s happening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Building an interactive object tracker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A colorspace-based tracker gives us the freedom to track a colored object, but
    we are also constrained to a predefined color. What if we just want to pick an
    object at random? How do we build an object tracker that can learn the characteristics
    of the selected object and just track it automatically? This is where the **c****ontinuously-adaptive
    meanshift** (**CAMShift**) algorithm comes into picture. It's basically an improved
    version of the meanshift algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of meanshift is actually nice and simple. Let's say we select a
    region of interest and we want our object tracker to track that object. In this
    region, we select a bunch of points based on the color histogram and we compute
    the centroid of spatial points. If the centroid lies at the center of this region,
    we know that the object hasn't moved. But if the centroid is not at the center
    of this region, then we know that the object is moving in some direction. The
    movement of the centroid controls the direction in which the object is moving.
    So, we move the bounding box of the object to a new location so that the new centroid
    becomes the center of this bounding box. Hence, this algorithm is called meanshift,
    because the mean (the centroid) is shifting. This way, we keep ourselves updated
    with the current location of the object.
  prefs: []
  type: TYPE_NORMAL
- en: But the problem with meanshift is that the size of the bounding box is not allowed
    to change. When you move the object away from the camera, the object will appear
    smaller to the human eye, but meanshift will not take that into account. The size
    of the bounding box will remain the same throughout the tracking session. Hence,
    we need to use CAMShift. The advantage of CAMShift is that it can adapt the size
    of the bounding box to the size of the object. Along with that, it can also keep
    track of the orientation of the object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following frame, in which the object is highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a4365f9-5811-42a5-ba22-35ed50858e29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have selected the object, the algorithm computes the histogram
    backprojection and extracts all the information. What is histogram backprojection?
    It''s just a way of identifying how well the image fits into our histogram model.
    We compute the histogram model of a particular thing and then use this model to
    find that thing in an image. Let''s move the object and see how it''s getting
    tracked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d8da352-b1c1-435f-b36a-a9caa1d56c82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It looks like the object is getting tracked fairly well. Let''s change the
    orientation and see whether the tracking is maintained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2d6e845-f927-4510-bd49-5784687e6d03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, the bounding ellipse has changed its location as well as orientation.
    Let''s change the perspective of the object and see whether it''s still able to
    track it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/967cea66-cf87-4a11-bb7c-b2dd25b1d4fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We''re still good! The bounding ellipse has changed the aspect ratio to reflect
    the fact that the object looks skewed now (because of the perspective transformation).
    Let''s look at the user interface functionality in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This function basically captures the coordinates of the rectangle that was selected
    in the window. The user just needs to click and drag with the mouse. There are
    a set of built-in functions in OpenCV that help us to detect these different mouse
    events.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for performing object tracking based on CAMShift:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have the HSV image waiting to be processed. Let''s go ahead and see
    how we can use our thresholds to process this image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see here, we use the HSV image to compute the histogram of the region.
    We use our thresholds to locate the required color in the HSV spectrum and then
    filter out the image based on that. Let''s go ahead and see how we can compute
    the histogram backprojection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to display the results. Using the rotated rectangle, let''s
    draw an ellipse around our region of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Detecting points using the Harris corner detector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Corner detection is a technique used to detect points of interest in an image.
    These interest points are also called feature points, or simply features, in computer
    vision terminology. A corner is basically an intersection of two edges. An interest
    point is basically something that can be uniquely detected in an image. A corner
    is a particular case of an interest point. These interest points help us characterize
    an image. These points are used extensively in applications such as object tracking,
    image classification, and visual search. Since we know that the corners are interesting,
    let's see how can detect them.
  prefs: []
  type: TYPE_NORMAL
- en: In computer vision, there is a popular corner detection technique called the
    Harris corner detector. We basically construct a 2 x 2 matrix based on partial
    derivatives of the grayscale image, and then analyze the eigenvalues. What does
    that even mean? Well, let's dissect it so that we can understand it better. Let's
    consider a small patch in the image. Our goal is to identify whether this patch
    has a corner in it. So, we consider all the neighboring patches and compute the
    intensity difference between our patch and all those neighboring patches. If the
    difference is high in all directions, then we know that our patch has a corner
    in it. This is an oversimplification of the actual algorithm, but it covers the
    gist. If you want to understand the underlying mathematical details, you can check
    out the original paper by *Harris* and *Stephens* at [http://www.bmva.org/bmvc/1988/avc-88-023.pdf](http://www.bmva.org/bmvc/1988/avc-88-023.pdf).
    A corner is a point with strong intensity differences along two directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the Harris corner detector, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8825804c-b5d9-4478-938c-c4e8b1a17e37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, the green circles on the TV remote are the detected corners.
    This will change based on the parameters you choose for the detector. If you modify
    the parameters, you can see that more points might get detected. If you make it
    strict, you might not be able to detect soft corners. Let''s look at the code
    to detect Harris corners:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We converted the image to grayscale and detected corners using our parameters.
    You can find the full code in the `.cpp` files. These parameters play an important
    role in the number of points that will be detected. You can check out the OpenCV
    documentation of `cornerHarris()` at [https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345](https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345).
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have all the information we need. Let''s go ahead and draw circles around
    our corners to display the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, this code takes an input argument: `blockSize`. Depending on
    the size you choose, the performance will vary. Start with a value of four and
    play around with it to see what happens.
  prefs: []
  type: TYPE_NORMAL
- en: Good features to track
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Harris corner detector performs well in many cases, but it can still be improved.
    Around six years after the original paper by *Harris* and *Stephens*, *Shi* and
    *Tomasi* came up with something better and they called it *Good Features to Track*.
    You can read the original paper here: [http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf](http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf).
    They used a different scoring function to improve the overall quality. Using this
    method, we can find the N strongest corners in the given image. This is very useful
    when we don''t want to use every single corner to extract information from the
    image. As we discussed, a good interest point detector is very useful in applications
    such as object tracking, object recognition, and image search.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you apply the Shi-Tomasi corner detector to an image, you will see something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f397c6a1-aa49-41f4-923a-ff22ed465243.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see here, all the important points in the frame are captured. Let''s
    look at the code to track these features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, we extracted the frame and used `goodFeaturesToTrack` to detect
    the corners. It''s important to understand that the number of corners detected
    will depend on our choice of parameters. You can find a detailed explanation at [http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack](http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack).
    Let''s go ahead and draw circles on these points to display the output image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This program takes an input argument: `numCorners`. This value indicates the
    maximum number of corners you want to track. Start with a value of `100` and play
    around with it to see what happens. If you increase this value, you will see more
    feature points getting detected.
  prefs: []
  type: TYPE_NORMAL
- en: Feature-based tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature-based tracking refers to tracking individual feature points across successive
    frames in the video. The advantage here is that we don't have to detect feature
    points in every single frame. We can just detect them once and keep tracking them
    after that. This is more efficient than running the detector on every frame. We
    use a technique called optical flow to track these features. Optical flow is one
    of the most popular techniques in computer vision. We choose a bunch of feature
    points and track them through the video stream. When we detect the feature points,
    we compute the displacement vectors and show the motion of those keypoints between
    consecutive frames. These vectors are called motion vectors. A motion vector for
    a particular point is basically just a directional line indicating where that
    point has moved, as compared to the previous frame. Different methods are used
    to detect these motion vectors. The two most popular algorithms are the **Lucas-Kanade**
    method and the **Farneback** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Lucas-Kanade method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Lucas-Kanade method is used for sparse optical flow tracking. By sparse,
    we mean that the number of feature points is relatively low. You can refer to
    their original paper here: [http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf](http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf).
    We start the process by extracting the feature points. For each feature point,
    we create 3 x 3 patches with the feature point at the center. The assumption here
    is that all the points within each patch will have a similar motion. We can adjust
    the size of this window depending on the problem at hand.'
  prefs: []
  type: TYPE_NORMAL
- en: For each feature point in the current frame, we take the surrounding 3 x 3 patch
    as our reference point. For this patch, we look in its neighborhood in the previous
    frame to get the best match. This neighborhood is usually bigger than 3 x 3 because
    we want to get the patch that's closest to the patch under consideration. Now,
    the path from the center pixel of the matched patch in the previous frame to the
    center pixel of the patch under consideration in the current frame will become
    the motion vector. We do that for all the feature points and extract all the motion
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77fb918e-4022-4492-9af6-58024173479e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to add some points that we want to track. Just go ahead and click on
    a bunch of points on this window with your mouse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81a8a130-e510-4376-b3fe-3c3ce81a862a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If I move into a different position, you will see that the points are still
    being tracked correctly within a small margin of error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0715e6df-ef45-4dea-bf7b-dbda0b2822fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s add a lot of points and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b375668e-563f-46b1-956a-e54925b634ae.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, it will keep tracking those points. But, you will notice that
    some of the points will be dropped because of factors such as prominence or speed
    of movement. If you want to play around with it, you can just keep adding more
    points to it. You can also let the user select a region of interest in the input
    video. You can then extract feature points from this region of interest and track
    the object by drawing a bounding box. It will be a fun exercise!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to do Lucas-Kanade-based tracking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the current image and the previous image to compute the optical flow
    information. Needless to say, the quality of the output will depend on the parameters
    chosen. You can find more details about the parameters at [http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk](http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk).
    To increase quality and robustness, we need to filter out the points that are
    very close to each other because they''re not adding new information. Let''s go
    ahead and do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have the tracking points. The next step is to refine the location of
    those points. What exactly does **refine** mean in this context? To increase the
    speed of computation, there is some level of quantization involved. In layman''s
    terms, you can think of it as rounding off. Now that we have the approximate region,
    we can refine the location of the point within that region to get a more accurate
    outcome. Let''s go ahead and do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Farneback algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gunnar Farneback proposed this optical flow algorithm and it''s used for dense
    tracking. Dense tracking is used extensively in robotics, augmented reality, and
    3D mapping. You can check out the original paper here: [http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf](http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf).
    The Lucas-Kanade method is a sparse technique, which means that we only need to
    process some pixels in the entire image. The Farneback algorithm, on the other
    hand, is a dense technique that requires us to process all the pixels in the given
    image. So, obviously, there is a trade-off here. Dense techniques are more accurate,
    but they are slower. Sparse techniques are less accurate, but they are faster.
    For real-time applications, people tend to prefer sparse techniques. For applications
    where time and complexity are not a factor, people tend to prefer dense techniques
    to extract finer details.'
  prefs: []
  type: TYPE_NORMAL
- en: In his paper, Farneback describes a method for dense optical-flow estimation
    based on polynomial expansion for two frames. Our goal is to estimate the motion
    between these two frames, which is basically a three-step process. In the first
    step, each neighborhood in both frames is approximated by polynomials. In this
    case, we are only interested in quadratic polynomials. The next step is to construct
    a new signal by global displacement. Now that each neighborhood is approximated
    by a polynomial, we need to see what happens if this polynomial undergoes an ideal
    translation. The last step is to compute the global displacement by equating the
    coefficients in the yields of these quadratic polynomials.
  prefs: []
  type: TYPE_NORMAL
- en: Now, how this is feasible? If you think about it, we are assuming that an entire
    signal is a single polynomial and there is a global translation relating the two
    signals. This is not a realistic scenario! So, what are we looking for? Well,
    our goal is to find out whether these errors are small enough so that we can build
    a useful algorithm that can track the features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a static image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/312e61da-2099-4efb-ae7c-e6224558a3fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If I move sideways, we can see that the motion vectors are pointing in a horizontal
    direction. It is simply tracking the movement of my head:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d82a7c3d-8a54-4447-9f85-6d329df90574.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If I move away from the webcam, you can see that the motion vectors are pointing
    in a direction perpendicular to the image plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22a60ecd-e148-4c97-b659-bb2075c9664b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the code to do optical-flow-based tracking using the Farneback algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, we use the Farneback algorithm to compute the optical flow vectors.
    The input parameters to `calcOpticalFlowFarneback` are important when it comes
    to the quality of tracking. You can find details about those parameters at [http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html](http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html).
    Let''s go ahead and draw those vectors on the output image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We used a function called `drawOpticalFlow` to draw those optical flow vectors.
    These vectors indicate the direction of motion. Let''s look at the function to
    see how we draw those vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about object tracking. We learned how to use HSV
    colorspace to track objects of a specific color. We discussed clustering techniques
    for object tracking and how we can build an interactive object tracker using the
    CAMShift algorithm. We looked at corner detectors and how we can track corners
    in a live video. We discussed how to track features in a video using optical flow.
    Finally, we understood the concepts behind the Lucas-Kanade and Farneback algorithms
    and then implemented them.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to discuss segmentation algorithms and how
    we can use them for text recognition.
  prefs: []
  type: TYPE_NORMAL
