- en: Artificial Intelligence and Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能与机器学习
- en: 'In this chapter, we will define what we mean by artificial intelligence, machine
    learning, and cognitive computing. We will study common classes of algorithms
    within the field of machine learning and its broader applications, including the
    following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将定义我们所说的人工智能、机器学习和认知计算。我们将研究机器学习领域内常见的算法类别及其更广泛的应用，包括以下内容：
- en: Supervised learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Reinforced learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Deep learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习
- en: Natural language processing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Cognitive computing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认知计算
- en: Apache Spark's machine learning library, `MLlib`, and how it can be used to
    implement these algorithms within machine learning pipelines
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark的机器学习库`MLlib`以及如何将其用于在机器学习管道中实现这些算法
- en: Artificial intelligence
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能
- en: 'Artificial intelligence is a broad term given to the theory and application
    of machines that exhibit intelligent behavior. Artificial intelligence encompasses
    many applied fields of study, including machine learning and subsequent deep learning,
    as illustrated in *Figure 3.1*:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能是一个广泛的术语，用于描述表现出智能行为的机器的理论和应用。人工智能包括许多应用研究领域，包括机器学习和随后的深度学习，如图*3.1*所示：
- en: '![](img/53453b9b-8180-4467-bc05-f1b64c38f347.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/53453b9b-8180-4467-bc05-f1b64c38f347.png)'
- en: 'Figure 3.1: Artificial intelligence overview'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：人工智能概述
- en: Machine learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: 'Machine learning is an applied field of study within the broader subject of
    artificial intelligence that focuses on learning from data by detecting patterns,
    trends, and relationships in order to make predictions and ultimately deliver
    actionable insights to help decision making. Machine learning models can be split
    into three main types: s*upervised learning*, *unsupervised learning*, and r*einforced
    learning*.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是人工智能更广泛主题下的一个应用研究领域，它通过检测数据中的模式、趋势和关系来学习，以便进行预测，并最终提供可操作的见解以帮助决策。机器学习模型可以分为三种主要类型：*监督学习*、*无监督学习*和*r*强化学习*。
- en: Supervised learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'In supervised learning, the goal is to learn a function that is able to map
    inputs *x* to outputs *y* given a labeled set of input-output pairs *D*, where
    *D* is referred to as the training set and *N* is the number of input-output pairs
    in the training set:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，目标是学习一个函数，该函数能够将输入*x*映射到输出*y*，给定一个标记过的输入-输出对集*D*，其中*D*被称为训练集，*N*是训练集中的输入-输出对的数量：
- en: '![](img/162e8bab-2db6-49dc-916d-96a729263d99.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/162e8bab-2db6-49dc-916d-96a729263d99.png)'
- en: In simple applications of supervised learning models, each training input *x[i]*
    is a numerical vector representing model features such as price, age, and temperature.
    In complex applications, *x[i]* may represent more complex objects, such as a
    time series, images, and text.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习模型的简单应用中，每个训练输入*x[i]*是一个表示模型特征（如价格、年龄和温度）的数值向量。在复杂应用中，*x[i]*可能代表更复杂的对象，如时间序列、图像和文本。
- en: 'When the output *y[i]* (also called the response variable) is categorical in
    nature, then the problem is referred to as a classification problem, where *y[i]*
    belongs to a finite set consisting of *K* elements or possible classifications:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当输出*y[i]*（也称为响应变量）在本质上属于分类性质时，这个问题被称为分类问题，其中*y[i]*属于一个由*K*个元素或可能的分类组成的有限集：
- en: '![](img/ed9d104d-c08b-43df-95de-59b612508414.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ed9d104d-c08b-43df-95de-59b612508414.png)'
- en: When the output *y[i]* is a real number, then the problem is referred to as
    a regression problem.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当输出*y[i]*是一个实数时，这个问题被称为回归问题。
- en: 'So what does this mean in practice? Well, the training set *D* is essentially
    a dataset for which the input features have already been mapped to an output.
    In other words, we already know what the answer is for the training dataset -
    it is *labelled*. For example, if the problem were to predict monthly sales figures
    for an e-commerce website based on the amount of money spent on online advertising
    (that is, a regression problem), the training dataset would already map advertising
    costs (input feature) to known monthly sales figures (output), as illustrated
    in *Figure 3.2*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这在实践中意味着什么呢？嗯，训练集*D*本质上是一个已经将输入特征映射到输出的数据集。换句话说，我们已经知道训练数据集的答案——它是*标记过的*。例如，如果问题是根据在线广告的花费来预测电子商务网站的月销售额（即回归问题），训练数据集就已经将广告成本（输入特征）映射到已知的月销售额（输出），如图*3.2*所示：
- en: '![](img/89aaa636-af32-4b63-9f47-c7aa867064cd.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/89aaa636-af32-4b63-9f47-c7aa867064cd.png)'
- en: 'Figure 3.2: Linear regression training dataset'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：线性回归训练数据集
- en: Supervised learning algorithms will then use this labelled training dataset
    to compute a mathematical function that is the best predictor of the output, given
    the input features. This function can then be applied to a test dataset in order
    to quantify its accuracy, and thereafter to a dataset that it has never seen before
    in order to make predictions!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法将使用这个标记的训练数据集来计算一个数学函数，该函数是给定输入特征的最佳输出预测器。然后可以将该函数应用于测试数据集以量化其准确性，之后应用于它以前从未见过的数据集以进行预测！
- en: Regression problems are where we want to predict a numerical outcome. Examples
    of regression algorithms include **Linear Regression** and **Regression Trees**,
    and examples of real-world use cases include price, weight, and temperature prediction.
    Classification problems are where we want to predict a categorical outcome. Examples
    of classification algorithms include **Logistic Regression**, **Multinomial Logistic
    Regression**, and **Classification Trees**, and examples of real-world use cases
    include image classification and email spam classification. We will study these
    algorithms in more detail in [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml),
    *Supervised Learning Using Apache Spark*, where we will also develop supervised
    learning models that can be applied to real-world use cases whilst providing the
    ability to quantify their accuracy.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题是我们希望预测一个数值结果的地方。回归算法的例子包括**线性回归**和**回归树**，实际应用案例包括价格、重量和温度预测。分类问题是我们希望预测一个分类结果的地方。分类算法的例子包括**逻辑回归**、**多项式逻辑回归**和**分类树**，实际应用案例包括图像分类和电子邮件垃圾邮件分类。我们将在[第4章](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml)《使用Apache
    Spark的监督学习》中更详细地研究这些算法，在那里我们还将开发可以应用于实际应用案例并能够量化其准确性的监督学习模型。
- en: Unsupervised learning
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'In unsupervised learning, the goal is to uncover hidden relationships, trends,
    and patterns, given only the input data *x[i]* with no output *y[i]*. In this
    case, we have the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，目标是揭示隐藏的关系、趋势和模式，给定只有输入数据*x[i]*而没有输出*y[i]*。在这种情况下，我们有以下内容：
- en: '![](img/5cda5569-12fb-4524-953f-e6939182587c.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5cda5569-12fb-4524-953f-e6939182587c.png)'
- en: 'In practice, this means that the emphasis is on uncovering interesting patterns
    and trends within a dataset in the absence of known and correct answers. Subsequently,
    unsupervised learning is commonly referred to as **knowledge discovery** given
    the fact that problems are less well-defined and we are not told what kind of
    patterns are contained within the data. **Clustering** is a well-known example
    of an unsupervised learning algorithm where the goal is to segment data points
    into groups, where all the data points in a specific group share similar features
    or attributes, as illustrated in *Figure 3.3*. Real-world use cases of clustering
    include document classification and clustering customers for marketing purposes:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这意味着在没有已知和正确答案的情况下，重点在于揭示数据集中的有趣模式和趋势。随后，由于问题定义不够明确，我们也没有被告知数据中包含哪些类型的模式，无监督学习通常被称为**知识发现**。**聚类**是一个无监督学习算法的例子，其目标是将数据点分割成组，其中特定组中的所有数据点都共享相似的特征或属性，如图*3.3*所示。聚类的实际应用案例包括文档分类和为营销目的聚类客户：
- en: '![](img/64c7ebf2-ea6e-4e32-a9bc-29b65c097295.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/64c7ebf2-ea6e-4e32-a9bc-29b65c097295.png)'
- en: 'Figure 3.3: Clustering unsupervised learning model'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：聚类无监督学习模型
- en: We will study unsupervised learning algorithms in more detail in [Chapter 5](eed6ec04-8a1b-459e-b225-b2761b26a460.xhtml),
    *Unsupervised Learning Using Apache Spark*, including hands-on development of
    real-world applications.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第5章](eed6ec04-8a1b-459e-b225-b2761b26a460.xhtml)《使用Apache Spark的无监督学习》中更详细地研究无监督学习算法，包括实际应用的动手开发。
- en: Reinforced learning
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: In reinforced learning, a reward (or punishment) system is employed to impact
    behavior over time, based on interactions between an agent and its wider environment.
    An agent will receive state information from the environment and, based on that
    state, will perform an *action*. As a result of that action, the environment will
    transition to a new state that is then provided back to the agent, typically with
    a reward (or punishment). The goal of the agent is to therefore maximize the cumulative
    reward it receives. For example, consider the case of a child learning good behavior
    from bad behavior and being rewarded for good behavior with a treat from its parents.
    In the case of machines, consider the example of computer-based board game players.
    By combining deep learning with reinforced learning, computers can learn how to
    play board games with continually increasing levels of performance such that,
    over time, they become almost unbeatable!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，使用奖励（或惩罚）系统来影响行为，基于代理与其更广泛环境之间的交互。代理将从环境中接收状态信息，并根据该状态执行一个*动作*。由于该动作，环境将过渡到一个新的状态，然后将其提供给代理，通常伴随着奖励（或惩罚）。因此，代理的目标是最大化它收到的累积奖励。例如，考虑一个孩子从不良行为中学习良好行为，并因其良好行为而从父母那里得到奖励的情况。在机器的例子中，考虑基于计算机的棋类游戏玩家的例子。通过将深度学习与强化学习相结合，计算机可以学习如何以不断提高的性能水平玩棋类游戏，以至于随着时间的推移，它们几乎不可战胜！
- en: Reinforced learning is beyond the scope of this book. However, to learn more
    about deep reinforced learning applied to gaming, please visit [https://deepmind.com/blog/deep-reinforcement-learning/](https://deepmind.com/blog/deep-reinforcement-learning/).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习超出了本书的范围。然而，要了解更多关于应用于游戏中的深度强化学习，请访问[https://deepmind.com/blog/deep-reinforcement-learning/](https://deepmind.com/blog/deep-reinforcement-learning/)。
- en: Deep learning
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: In deep learning, a subfield within the broader field of machine learning, the
    goal is still to learn a function but by employing an architecture that mimics
    the neural architecture found in the human brain in order to learn from experience
    using a hierarchy of concepts or representations. This enables us to develop more
    complex and powerful functions in order to predict outcomes better.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习，作为机器学习更广泛领域的一个子领域，目标仍然是学习一个函数，但通过采用模仿人类大脑中发现的神经网络架构的架构，以便使用概念或表示的层次结构从经验中学习。这使得我们能够开发更复杂和强大的函数，以更好地预测结果。
- en: Many machine learning models employ a two-layer architecture, where some sort
    of function maps an input to an output. However, in the human brain, multiple
    layers of processing are found, in other words, a neural network. By mimicking
    natural neural networks, **artificial neural networks (ANN)** offer the ability
    to learn complex non-linear representations with no restrictions on the input
    features and are ideally suited to a wide variety of exciting use cases, including
    speech, image and pattern recognition, **natural language processing** (**NLP**),
    fraud detection, forecasting and price prediction.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习模型采用双层架构，其中某种函数将输入映射到输出。然而，在人类大脑中，存在多层处理，换句话说，是一个神经网络。通过模仿自然神经网络，**人工神经网络（ANN**）能够学习复杂的非线性表示，对输入特征没有限制，非常适合广泛的令人兴奋的应用场景，包括语音、图像和模式识别、**自然语言处理（NLP**）、欺诈检测、预测和价格预测。
- en: Natural neuron
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然神经元
- en: 'Deep learning algorithms mimic the neural architecture found in the human brain.
    If we were to study a single natural neuron in the human brain, we would find
    three primary areas of interest, as illustrated in *Figure 3.4*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法模仿了人类大脑中发现的神经网络架构。如果我们研究人类大脑中的一个单个自然神经元，我们会发现三个主要的研究领域，如图*图3.4*所示：
- en: '![](img/192b5cb4-e0ab-4820-a9b9-ec6c8ebf0d83.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/192b5cb4-e0ab-4820-a9b9-ec6c8ebf0d83.png)'
- en: 'Figure 3.4: A natural neuron'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：一个自然神经元
- en: Dendrites receive chemical signals and electrical impulses from other neurons
    that are collected and aggregated in the cell body. The nucleus, found within
    the cell body, is the control center of the neuron and is responsible for regulating
    cell functions, producing the proteins required to build new dendrites and for
    making the neurotransmitter chemicals used as signals. Signals can be classed
    as either inhibitory or excitatory. If they are inhibitory, this means that they
    are not transmitted to other neurons. If they are excitatory, this means that
    they are transmitted to other neurons via the axon. The axon is responsible for
    communicating signals between neurons, in some cases across distances as long
    as a couple of meters or a short as a few microns. The neuron therefore, as a
    single logical unit, is ultimately responsible for communicating information and
    the average human brain may contain around 100 billion neurons.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 树突接收来自其他神经元的化学信号和电脉冲，这些信号在细胞体中被收集和汇总。位于细胞体内的细胞核是神经元的控制中心，负责调节细胞功能，产生构建新树突所需的蛋白质，以及制造用作信号的神经递质化学物质。信号可以分为抑制性或兴奋性。如果是抑制性的，这意味着它们不会被传递到其他神经元。如果是兴奋性的，这意味着它们将通过轴突传递到其他神经元。轴突负责在神经元之间传递信号，在某些情况下，距离可以长达几米或短至几微米。因此，神经元作为一个单一的逻辑单元，最终负责传递信息，而平均人类大脑可能包含大约1000亿个神经元。
- en: Artificial neuron
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经元
- en: 'The core concepts of the natural neuron can be generalized into components
    of a signal processing system. In this general signal processing system, the signals
    received by the dendrites can be thought of as the inputs. The nucleus can be
    thought of as a central processing unit that collects and aggregates the inputs
    and, depending on the net input magnitude and an activation function, transmits
    outputs along the axon. This general signal processing system, modeled on a natural
    neuron, is called an *artificial neuron* and is illustrated in *Figure 3.5*:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自然神经元的核心理念可以概括为信号处理系统的组成部分。在这个通用信号处理系统中，树突接收到的信号可以被认为是输入。细胞核可以被认为是中央处理单元，它收集和汇总输入，并根据净输入幅度和激活函数，通过轴突传递输出。这个基于自然神经元建模的通用信号处理系统被称为**人工神经元**，并在*图3.5*中展示：
- en: '![](img/46651759-ddc3-4669-8e1b-827bc63b1eca.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/46651759-ddc3-4669-8e1b-827bc63b1eca.png)'
- en: 'Figure 3.5: An artificial neuron'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：人工神经元
- en: Weights
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重
- en: 'In the artificial neuron, weights can amplify or attenuate signals and are
    used to model the established connections to other neurons as found in the natural
    world. By changing the weight vectors, we can effect whether that neuron will
    activate or not based on the aggregation of input values with weights, called
    the weighted or net input *z*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工神经元中，权重可以放大或衰减信号，并用于模拟自然界中发现的与其他神经元建立的连接。通过改变权重向量，我们可以根据输入值与权重的汇总（称为加权或净输入*z*）来影响神经元是否会激活：
- en: '![](img/749ec865-ad53-41cb-aa03-dbaeec6a27c4.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/749ec865-ad53-41cb-aa03-dbaeec6a27c4.png)'
- en: Activation function
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: Once the weighted input plus a bias is calculated, the **activation function**,
    denoted by the Greek letter *phi* (Φ), is used to determine the output of the
    neuron and whether it activates or not. To make this determination, an activation
    function is typically a non-linear function bounded between two values, thereby
    adding non-linearity to ANNs. As most real-world data tends to be non-linear in
    nature when it comes to complex use cases, we require that ANNs have the capability
    to learn these non-linear concepts or representations. This is enabled by non-linear
    activation functions. Examples of activation functions include a Heaviside step
    function, a sigmoid function, and a hyperbolic tangent function.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算了加权输入加上偏差，就使用**激活函数**（用希腊字母*phi*（Φ）表示）来确定神经元的输出以及它是否被激活。为了做出这个决定，激活函数通常是一个介于两个值之间的非线性函数，从而为人工神经网络（ANNs）添加了非线性。由于大多数现实世界数据在复杂用例中往往是非线性的，我们需要人工神经网络具有学习这些非线性概念或表示的能力。这通过非线性激活函数得以实现。激活函数的例子包括Heaviside阶跃函数、Sigmoid函数和对数正切函数。
- en: Heaviside step function
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Heaviside阶跃函数
- en: 'A Heaviside step function is a basic discontinuous function that compares values
    against a simple threshold and is used for classification where the input data
    is *linearly* separable. The neuron is activated if the weighted sum plus a bias
    exceeds a certain threshold, denoted by the Greek letter *theta* (θ) in the equation
    below. If it does not, the neuron is not activated. The following step function
    is an example of a Heaviside step function that is bounded between *1* and *-1*:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Heaviside阶跃函数是一种基本的不连续函数，它将值与一个简单的阈值进行比较，用于输入数据*线性可分*的分类。如果加权总和加上偏差超过某个阈值，则神经元被激活，该阈值用下面的方程中的希腊字母*theta*（θ）表示。如果没有超过，则神经元不被激活。以下阶跃函数是一个介于*1*和*-1*之间的Heaviside阶跃函数的例子：
- en: '![](img/d7e537cb-d821-4892-92d4-d4c27ba3fa4e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7e537cb-d821-4892-92d4-d4c27ba3fa4e.png)'
- en: 'This Heaviside step function is illustrated in *Figure 3.6*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Heaviside阶跃函数如图*3.6*所示：
- en: '![](img/3f2a68ff-30a8-4d82-b842-a730b3e6358f.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f2a68ff-30a8-4d82-b842-a730b3e6358f.png)'
- en: 'Figure 3.6: Heaviside step activation function'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：Heaviside阶跃激活函数
- en: Sigmoid function
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid函数
- en: 'A sigmoid function is a non-linear mathematical function that exhibits a sigmoid
    curve, as illustrated in *Figure 3.7*, and often refers to the sigmoid or logistic
    function:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数是一种非线性数学函数，表现出sigmoid曲线，如图*3.7*所示，通常指的是sigmoid或逻辑函数：
- en: '![](img/620efead-9eb4-4d7f-a693-5323eace3821.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/620efead-9eb4-4d7f-a693-5323eace3821.png)'
- en: 'In this case, the sigmoid activation function is bounded between 0 and 1 and
    is smoothly defined for *all* real input values, making it a better choice of
    activation function than a basic Heaviside step function. This is because, unlike
    the Heaviside step function, non-linear activation functions can distinguish data
    that is *not* linearly separable, such as image and video data. Note that by using
    the sigmoid function as the activation function, the artificial neuron will, in
    fact, correspond to a logistic regression model:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，sigmoid激活函数介于0和1之间，对所有实数输入值都有平滑的定义，这使得它比基本的Heaviside阶跃函数是一个更好的激活函数选择。这是因为，与Heaviside阶跃函数不同，非线性激活函数可以区分那些*非线性可分*的数据，例如图像和视频数据。请注意，使用sigmoid函数作为激活函数时，人工神经元实际上对应于逻辑回归模型：
- en: '![](img/fc5496ba-3a94-4ae7-8e5c-a793f447f8a4.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc5496ba-3a94-4ae7-8e5c-a793f447f8a4.png)'
- en: 'Figure 3.7: Sigmoid activation function'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7：Sigmoid激活函数
- en: Hyperbolic tangent function
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双曲正切函数
- en: 'Finally, a hyperbolic tangent function, as illustrated in *Figure 3.8*, is
    the ratio between the hyperbolic sine and cosine functions:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，双曲正切函数，如图*3.8*所示，是双曲正弦函数和余弦函数的比值：
- en: '![](img/a7027299-b4af-437f-aed3-8bf353128cfc.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7027299-b4af-437f-aed3-8bf353128cfc.png)'
- en: 'In this case, an activation function based on a hyperbolic tangent function
    is bounded between *1* and *-1* and, similar to sigmoid functions, is smoothly
    defined for all real input values:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，基于双曲正切函数的激活函数介于*1*和*-1*之间，类似于sigmoid函数，对所有实数输入值都有平滑的定义：
- en: '![](img/c9014c8e-7d06-4a12-9390-4d17f9379eb9.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9014c8e-7d06-4a12-9390-4d17f9379eb9.png)'
- en: 'Figure 3.8: Hyperbolic tangent function'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8：双曲正切函数
- en: Artificial neural network
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: 'An **artificial neural network** (**ANN**) is a connected group of artificial
    neurons, where the artificial neurons are aggregated into linked **neural** **layers**
    that can be divided into three types:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANN**）是一组相互连接的人工神经元，其中人工神经元被聚合成相互连接的**神经****层**，可以分为三种类型：'
- en: The **input layer** receives input signals from the outside world and passes
    these input signals to the next layer.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**接收来自外部世界的输入信号，并将这些输入信号传递到下一层。'
- en: '**Hidden layers**, if any, perform computations on these signals and pass them
    to the output layer. Therefore, the outputs of the hidden layer(s) act as inputs
    to the final output.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**（如果有），对这些信号进行计算并将它们传递到输出层。因此，隐藏层（的）的输出作为最终输出的输入。'
- en: The **output layer** calculates the final output, which then influences the
    outside world in some manner.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**计算最终输出，然后以某种方式影响外部世界。'
- en: 'Artificial neurons are linked across adjacent neural layers by **edges** that
    have weights associated with them. In general, the addition of more hidden neural
    layers increases the ability of the ANN to learn more complex concepts or representations.
    They are termed *hidden* because of the fact that they do not directly interact
    with the outside world. Note that all ANNs have an input and an output layer,
    with zero or more hidden layers. An ANN where signals are propagated in one direction
    only, in other words, signals are received by the input layer and forwarded to
    the next layer for processing, are called **feedforward** networks. ANNs where
    signals may be propagated back to artificial neurons or neural layers that have
    *already* processed that signal are called **feedback** networks. *Figure 3.9*
    illustrates the logical architecture of a feedforward ANN, where each circle represents
    an artificial neuron sometimes referred to as a **node** or a **unit**, and the
    arrows represent **edges** or **connections** between artificial neurons across
    adjacent neural layers:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元通过具有相关权重的**边**连接到相邻的神经网络层。一般来说，增加更多的隐藏神经网络层可以提高人工神经网络（ANN）学习更复杂概念或表示的能力。它们被称为**隐藏**层，因为它们不直接与外界交互。请注意，所有ANN都有一个输入层和一个输出层，以及零个或多个隐藏层。仅在一个方向上传播信号的人工神经网络，换句话说，信号由输入层接收并转发到下一层进行处理的，被称为**前馈**网络。信号可能被传播回已经处理过该信号的人工神经元或神经层的ANN被称为**反馈**网络。*图3.9*展示了前馈ANN的逻辑架构，其中每个圆圈代表一个人工神经元，有时被称为**节点**或**单元**，箭头代表人工神经元之间相邻神经网络层的**边**或**连接**：
- en: '![](img/1e36af97-c45b-4bd4-9f07-8809de91a4d2.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e36af97-c45b-4bd4-9f07-8809de91a4d2.png)'
- en: 'Figure 3.9: Feedforward artificial neural network'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9：前馈人工神经网络
- en: ANNs can be divided into two classes dependent on their architecture. **Mono-layer**
    or **single-layer** ANNs are characterized by the aggregation of all its constituent
    artificial neurons on the same level with no hidden layers. A single-layer perceptron
    is an example of a mono-layer ANN consisting of just one layer of links between
    input nodes and output nodes. **Multi-layer **ANNs are characterized by the segmentation
    of artificial neurons across multiple linked layers. A multi-layer perceptron
    is an example of a multi-layer ANN consisting of one or more hidden layers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其架构，ANN可以分为两类。**单层**或**单层**ANN的特点是所有组成人工神经元都聚集在同一层，没有隐藏层。一个单层感知器是单层ANN的一个例子，它只包含输入节点和输出节点之间的一层链接。**多层**ANN的特点是人工神经元分布在多个相互连接的层中。多层感知器是多层ANN的一个例子，它包含一个或多个隐藏层。
- en: ANNs learn by optimizing their weights to deliver a desired outcome, and that
    by changing weights, ANNs can deliver different results for the same inputs. The
    goal of optimizing the weights is to minimize a **loss function**—a function that
    calculates the price paid for inaccurate predictions—by finding the best combination
    of weights that best predict the outcome. Recall that the weights represent established
    connections to other neurons; hence, by changing weights, ANNs are, in fact, mimicking
    natural neural networks by changing the connections between neurons. Various processes
    for learning optimal weight coefficients are provided in the following sub-sections
    during our discussions on perceptrons.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ANN通过优化其权重以产生期望的结果来学习，并且通过改变权重，ANN可以为相同的输入产生不同的结果。优化权重的目标是通过找到最佳权重组合来最小化**损失函数**——一个计算不准确预测代价的函数，从而最佳预测结果。回想一下，权重代表与其他神经元建立的连接；因此，通过改变权重，ANN实际上是通过改变神经元之间的连接来模拟自然神经网络的。在讨论感知器时，以下子节中将提供学习最优权重系数的各种过程。
- en: Single-layer perceptron
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单层感知器
- en: '*Figure 3.10* illustrates the architecture of a single-layer perceptron. In
    this single-layer perceptron, an optimal set of weight coefficients are derived
    which, when multiplied by the input features, determines whether to activate the
    neuron or not. Initial weights are set randomly and, if the weighted input results
    in a predicted output that matches the desired output (for example, in a supervised
    learning classification context), then no changes to the weights are made. If
    the predicted output does not match the desired output, then weights are updated
    to reduce the error.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.10*展示了单层感知器的架构。在这个单层感知器中，推导出一个最优的权重系数集，当乘以输入特征时，决定是否激活神经元。初始权重被随机设置，如果加权输入产生的预测输出与期望输出匹配（例如，在监督学习分类的上下文中），则不对权重进行更改。如果预测输出与期望输出不匹配，则更新权重以减少误差。'
- en: 'This makes single-layer perceptrons best suited as classifiers, but only when
    the classes are *linearly separable*:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得单层感知器最适合作为分类器，但仅当类别是*线性可分*时：
- en: '![](img/decfc40a-779e-4dbc-8c46-4b38bd9b20e0.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/decfc40a-779e-4dbc-8c46-4b38bd9b20e0.png)'
- en: 'Figure 3.10: Single-layer perceptron'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10：单层感知器
- en: Multi-layer perceptron
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'Multi-layer perceptrons differ from single-layer perceptrons as a result of
    the introduction of one or more hidden layers, giving them the ability to learn
    non-linear functions. *Figure 3.11* illustrates the architecture of a multi-layer
    perceptron containing one hidden layer:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器与单层感知器不同，这是由于引入了一个或多个隐藏层，使它们能够学习非线性函数。*图3.11*展示了包含一个隐藏层的多层感知器架构：
- en: '![](img/3bcdc382-9e9f-4320-a465-ff853410d8a5.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3bcdc382-9e9f-4320-a465-ff853410d8a5.png)'
- en: 'Figure 3.11: Multi-layer perceptron'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11：多层感知器
- en: '**Backpropagation** is a supervised learning process by which multi-layer perceptrons
    and other ANNs can learn, that is to say, derive an optimal set of weight coefficients.
    The first step in backpropagation is, in fact, **forward propagation**, whereby
    all weights are set randomly initially and the output from the network is calculated
    (similar to single-layer perceptrons, but this time involving one or more hidden
    layers). If the predicted output does not match the desired output, the total
    error at the output nodes is propagated back through the entire network in an
    effort to readjust all weights in the network so that the error is reduced in
    the output layer.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播**是一种监督学习过程，通过这个过程多层感知器和其它人工神经网络可以学习，也就是说，推导出一个最优的权重系数集。反向传播的第一步实际上是**正向传播**，在这个过程中，所有权重最初被随机设置，然后计算网络的输出（类似于单层感知器，但这次涉及一个或多个隐藏层）。如果预测输出与期望输出不匹配，输出节点的总误差将通过整个网络反向传播，以尝试重新调整网络中的所有权重，从而在输出层减少误差。'
- en: 'Multi-layer neural networks, such as multi-layer perceptrons, are generally
    much more compute intensive, since the process to optimize weights involves a
    much greater number of weights and calculations. Therefore, training neural networks,
    which also typically involves a large number of data points in order to learn
    a large number of optimal weight coefficients, requires CPU and memory resources
    that previously were not readily available or cost-effective. However, with the
    advent of distributed systems, like those described in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The
    Big Data Ecosystem *and the availability of cost-effective, high-performance,
    and resilient distributed clusters that support the processing of petabytes of
    data hosted by commodity hardware, research into ANNs and deep learning has exploded,
    as too has their application to exciting real-world artificial intelligence use
    cases, including the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 多层神经网络，如多层感知器，通常计算量更大，因为优化权重的过程涉及更多的权重和计算。因此，训练神经网络，这通常也涉及大量数据点以学习大量最优权重系数，需要CPU和内存资源，这在以前可能并不容易获得或成本效益高。然而，随着分布式系统的出现，如[第1章](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml)《大数据生态系统》中描述的，以及成本效益高、高性能和具有弹性的分布式集群的可用性，这些集群支持由通用硬件托管的数据处理，人工神经网络和深度学习的研究已经爆炸式增长，它们在令人兴奋的现实世界人工智能用例中的应用也是如此，包括以下内容：
- en: Healthcare and combating disease, including predictive diagnosis, drug discovery,
    and gene ontology
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医疗保健和疾病防治，包括预测诊断、药物发现和基因本体
- en: Speech recognition, including language translation
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别，包括语言翻译
- en: Image recognition, including visual search
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像识别，包括视觉搜索
- en: Theoretical physics and astrophysics, including satellite image classification
    and gravitational wave detection
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理论物理学和天体物理学，包括卫星图像分类和引力波探测
- en: In this sub-section, we have discussed two specific types of ANN, single-layer
    perceptrons and multi-layer perceptrons, which we will study in more detail in
    [Chapter 7](337b904f-87f1-4741-bd75-d7fd983185f6.xhtml), *Deep Learning Using
    Apache Spark*, including the hands-on development of real-world applications.
    Other classes of artificial network networks include **convolutional** **neural
    networks** (also described in [Chapter 7](337b904f-87f1-4741-bd75-d7fd983185f6.xhtml),
    *Deep Learning Using Apache Spark*), **recurrent neural networks**, **Kohonen
    self-organizing neural networks**, and **modular** **neural networks**, which
    are beyond the scope of this book. To learn more about ANNs and the exciting field
    of deep learning, please visit [http://deeplearning.net/](http://deeplearning.net/).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了两种特定的ANN类型，单层感知器和多层感知器，我们将在[第7章](337b904f-87f1-4741-bd75-d7fd983185f6.xhtml)《使用Apache
    Spark的深度学习》中更详细地研究，包括实际应用的动手开发。其他类别的神经网络包括**卷积****神经网络**（也在[第7章](337b904f-87f1-4741-bd75-d7fd983185f6.xhtml)《使用Apache
    Spark的深度学习》中描述），**循环神经网络**、**Kohonen自组织神经网络**和**模块化****神经网络**，这些超出了本书的范围。要了解更多关于ANN和令人兴奋的深度学习领域，请访问[http://deeplearning.net/](http://deeplearning.net/)。
- en: NLP
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP
- en: 'NLP refers to a family of computer science disciplines, including machine learning,
    linguistics, information engineering, and data management, used to analyze and
    understand natural languages, including speech and text. NLP can be applied to
    a wide variety of real-world use cases, including the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是指包括机器学习、语言学、信息工程和数据管理在内的一组计算机科学学科，用于分析和理解自然语言，包括语音和文本。NLP可以应用于广泛的现实世界用例，包括以下内容：
- en: '**Named entity recognition** (**NER**): Automatically identifying and parsing
    entities from text, including people, physical addresses, and email addresses'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体识别**（NER）：自动从文本中识别和解析实体，包括人名、物理地址和电子邮件地址'
- en: '**Relationship extraction**: Automatically identifying the types of relationships
    between parsed entities'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关系抽取**：自动识别解析实体之间关系的类型'
- en: '**Machine translation and transcription**: Automatically translating from one
    natural language to another, for example, from English to Chinese'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译和转录**：自动将一种自然语言翻译成另一种语言，例如，从英语翻译成中文'
- en: '**Searching**: Automatically searching across vast collections of structured,
    semi-structured, and unstructured documents and objects in order to fulfill a
    natural language query'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索**：自动在大量结构化、半结构化和非结构化文档和对象中进行搜索，以满足自然语言查询'
- en: '**Speech recognition**: Automatically deriving meaning from human speech'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音识别**：自动从人类语音中提取意义'
- en: '**Sentiment analysis**: Automatically identifying human sentiment toward a
    topic or entity'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：自动识别人类对某个主题或实体的情感'
- en: '**Question answering**: Automatically answering natural, fully-formed questions'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答系统**：自动回答自然、完整的疑问'
- en: A common technique employed in NLP is developing a data engineering pipeline
    that pre-processes text in order to generate features for input into machine learning
    models. Common pre-processing techniques include **tokenization** (splitting text
    into smaller and simpler units called tokens, where tokens are often individual
    words or terms), **stemming** and **lemmatisation** (reducing tokens to a base
    form), and removing **stop-words** (such as *I*, *this*, and *at*). The resultant
    set of terms is converted into features to then feed into machine learning models.
    A very basic algorithm used to convert the set of terms into features is called
    **Bag of Words**, which simply counts the number of occurrences of each unique
    term, thereby converting text into numeric feature vectors.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，常用的一种技术是开发一个数据工程管道，该管道对文本进行预处理，以便为机器学习模型生成特征。常见的预处理技术包括**分词**（将文本分割成更小、更简单的单元，称为标记，标记通常是单个单词或术语）、**词干提取**和**词形还原**（将标记还原到基本形式），以及移除**停用词**（如*I*、*this*和*at*）。生成的术语集被转换成特征，然后输入到机器学习模型中。将术语集转换为特征的一个非常基础的算法称为**词袋模型**，它简单地计算每个唯一术语的出现次数，从而将文本转换为数值特征向量。
- en: NLP is important as it provides a means to achieve true seamless interaction
    between artificially intelligent systems/machines and humans, such as through
    conversation interfaces. We will study NLP in more detail in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural
    Language Processing Using Apache Spark*, including the hands-on development of
    real-world applications.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）很重要，因为它提供了一种在人工智能系统/机器和人类之间实现真正无缝交互的手段，例如通过对话界面。我们将在[第6章](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml)，*使用Apache
    Spark的自然语言处理*中更详细地研究NLP，包括实际应用的动手开发。
- en: Cognitive computing
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 认知计算
- en: Similar to NLP, cognitive computing actually refers to a family of computer
    science disciplines, including machine learning, deep learning, NLP, statistics,
    business intelligence, data engineering, and information retrieval that, together,
    are used to develop systems that simulate human thought processes. Real-world
    implementations of cognitive systems include chatbots and virtual assistants (such
    as Amazon Alexa, Google Assistant, and Microsoft Cortana) that understand natural
    human language and provide contextual conversation interfaces, including question-answering,
    personalized recommendations, and information retrieval systems.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与NLP类似，认知计算实际上是指一系列计算机科学学科，包括机器学习、深度学习、NLP、统计学、商业智能、数据工程和信息检索，这些学科共同用于开发模拟人类思维过程的系统。认知系统的实际应用包括能够理解自然人类语言的聊天机器人和虚拟助手（如亚马逊Alexa、谷歌助手和微软Cortana），它们提供包括问答、个性化推荐和信息检索系统在内的上下文对话界面。
- en: Machine learning pipelines in Apache Spark
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark中的机器学习管道
- en: To end this chapter, we will take a look at how Apache Spark can be used to
    implement the algorithms that we have previously discussed by taking a look at
    how its machine learning library, `MLlib`, works under the hood. `MLlib` provides
    a suite of tools designed to make machine learning accessible, scalable, and easy
    to deploy.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结束本章，我们将探讨如何使用Apache Spark实现我们之前讨论的算法，通过查看其机器学习库`MLlib`的工作原理。`MLlib`提供了一套工具，旨在使机器学习变得易于访问、可扩展且易于部署。
- en: Note that as of Spark 2.0, the `MLlib` RDD-based API is in maintenance mode.
    The examples in this book will use the DataFrame-based API, which is now the primary
    API for `MLlib`. For more information, please visit [https://spark.apache.org/docs/latest/ml-guide.html](https://spark.apache.org/docs/latest/ml-guide.html).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，截至Spark 2.0，基于RDD的`MLlib` API处于维护模式。本书中的示例将使用基于DataFrame的API，这是`MLlib`的当前主要API。有关更多信息，请访问[https://spark.apache.org/docs/latest/ml-guide.html](https://spark.apache.org/docs/latest/ml-guide.html)。
- en: 'At a high level, the typical implementation of machine learning models can
    be thought of as an ordered pipeline of algorithms, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，机器学习模型的典型实现可以被视为一系列按顺序排列的算法，如下所示：
- en: Feature extraction, transformation, and selection
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取、转换和选择
- en: Train a predictive model based on these feature vectors and labels
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于这些特征向量和标签训练预测模型
- en: Make predictions using the trained predictive model
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的预测模型进行预测
- en: Evaluate model performance and accuracy
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型性能和准确性
- en: '`MLlib` exposes two core abstractions that facilitate this high-level pipeline
    and allow machine learning models to be developed in Apache Spark:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`MLlib`公开了两个核心抽象，这些抽象促进了高级管道，并允许在Apache Spark中开发机器学习模型：'
- en: '**Transformers**: Formally, a transformer converts one DataFrame (see [Chapter
    1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The Big Data Ecosystem*) into
    another DataFrame. The new DataFrame will typically contain one or more new columns
    appended to it. In the context of a machine learning model, an input DataFrame
    may consist of a column containing the relevant feature vectors. A transformer
    will then take this input DataFrame and predict a label for each feature vector.
    The transformer will then output a new DataFrame with a new column containing
    the predicted labels.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换器**：正式来说，转换器将一个DataFrame（见[第1章](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml)，*大数据生态系统*）转换为另一个DataFrame。新的DataFrame通常包含一个或多个附加到其上的新列。在机器学习模型的上下文中，输入DataFrame可能包含一个包含相关特征向量的列。然后，转换器将这个输入DataFrame作为输入，并为每个特征向量预测一个标签。转换器将输出一个新的DataFrame，其中包含一个包含预测标签的新列。'
- en: '**Estimators**: Formally, an estimator abstracts a learning algorithm. In practice,
    an estimator is a type of learning algorithm, such as a logistic regression algorithm.
    In this case, the estimator is called *LogisticRegression* in `MLlib`. The estimator
    will take an input DataFrame and call the `fit()` method on it. The output of
    the `fit()` method, and hence the output of the estimator, will be a *trained
    model*. In this example, the *LogisticRegression* estimator will produce a trained
    *LogisticRegressionModel* model object. The model object itself is, in fact, a
    *transformer*, because the trained model can now take a new DataFrame containing
    new feature vectors and make predictions on them.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returning to our definition of a pipeline, this can now be extended. A pipeline
    is, in fact, an ordered sequence of stages where each stage is either a transformer
    or an estimator.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.12* illustrates a pipeline used to train a model. In *Figure 3.12*,
    NLP *transformations* are applied to tokenize raw training text into a set of
    words or terms. The tokenizer is referred to as a feature *transformer*. An algorithm
    called HashingTF is then applied to take the set of terms and convert it into
    fixed-length feature vectors (HashingTF ultimately calculates the term frequencies
    using a hash function). HashingTF is also a *transformer*. The *LogisticRegression*
    estimator is then applied to these feature vectors, via `LogisticRegression.fit()`,
    to generate a trained *LogisticRegressionModel*, which itself is a type of *transformer*:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8665b5eb-e972-45ef-a9eb-8d323d6d4997.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: MLlib training pipeline'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.13* illustrates a pipeline used to test a model. In this diagram,
    similar to the training pipeline, a tokenizer feature *transformer* is used to
    extract terms from the raw test text, and then the HashingTF *transformer* is
    applied to convert the set of terms into fixed-length feature vectors. However,
    since we already have a trained model generated by the training pipeline in *Figure
    3.12*, the feature vectors are passed as input into this trained model *transformer*
    in order to make predictions and output a new DataFrame containing these predictions
    on the test data:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67484bc6-83e0-4a8d-b914-19c943b1c949.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: MLlib test pipeline'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: In addition to providing common machine learning algorithms and methods to extract,
    transform, and select model features and other pipeline abstractions, `MLlib`
    also exposes methods to save trained models and pipelines to an underlying filesystem
    that can then be loaded later on if and when required. `MLlib` also provides utility
    methods covering operations in statistics, linear algebra, and data engineering.
    To learn more about `MLlib`, please visit [http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have defined what is meant by artificial intelligence, machine
    learning, and cognitive computing. We have explored common machine learning algorithms
    at a high level, including deep learning and ANNs, as well as taking a look at
    Apache Spark's machine learning library, `MLlib`, and how it can be used to implement
    these algorithms within machine learning pipelines.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们定义了人工智能、机器学习和认知计算的含义。我们以高层次探讨了常见的机器学习算法，包括深度学习和人工神经网络，同时还简要介绍了Apache
    Spark的机器学习库`MLlib`以及如何将其用于在机器学习管道中实现这些算法。
- en: In the next chapter, we will start developing, deploying, and testing supervised
    machine learning models applied to real-world use cases using `PySpark` and `MLlib`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始使用`PySpark`和`MLlib`开发、部署和测试应用于实际用例的监督式机器学习模型。
