- en: Artificial Intelligence and Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will define what we mean by artificial intelligence, machine
    learning, and cognitive computing. We will study common classes of algorithms
    within the field of machine learning and its broader applications, including the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforced learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cognitive computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark's machine learning library,Â `MLlib`, and how it can be used to
    implement these algorithms within machine learning pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Artificial intelligence is a broad term given to the theory and application
    of machines that exhibit intelligent behavior. Artificial intelligence encompasses
    many applied fields of study, including machine learning and subsequent deep learning,
    as illustrated in *Figure 3.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53453b9b-8180-4467-bc05-f1b64c38f347.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Artificial intelligence overview'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning is an applied field of study within the broader subject of
    artificial intelligence that focuses on learning from data by detecting patterns,
    trends, and relationships in order to make predictions and ultimately deliver
    actionable insights to help decision making. Machine learning models can be split
    into three main types: s*upervised learning*, *unsupervised learning*, and r*einforced
    learning*.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In supervised learning, the goal is to learn a function that is able to map
    inputs *x* to outputs *y* given a labeled set of input-output pairs *D*, where
    *D* is referred to as the training set and *N* is the number of input-output pairs
    in the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/162e8bab-2db6-49dc-916d-96a729263d99.png)'
  prefs: []
  type: TYPE_IMG
- en: In simple applications of supervised learning models, each training input *x[i]*
    is a numerical vector representing model features such as price, age, and temperature.
    In complex applications, *x[i]* may represent more complex objects, such as a
    time series, images, and text.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the output *y[i]* (also called the response variable) is categorical in
    nature, then the problem is referred to as a classification problem, where *y[i]*
    belongs to a finite set consisting of *K* elements or possible classifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed9d104d-c08b-43df-95de-59b612508414.png)'
  prefs: []
  type: TYPE_IMG
- en: When the output *y[i]* is a real number, then the problem is referred to as
    a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'So what does this mean in practice? Well, the training set *D* is essentially
    a dataset for which the input features have already been mapped to an output.
    In other words, we already know what the answer is for the training dataset -
    it is *labelled*. For example, if the problem were to predict monthly sales figures
    for an e-commerce website based on the amount of money spent on online advertising
    (that is, a regression problem), the training dataset would already map advertising
    costs (input feature) to known monthly sales figures (output), as illustrated
    in *Figure 3.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89aaa636-af32-4b63-9f47-c7aa867064cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Linear regression training dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning algorithms will then use this labelled training dataset
    to compute a mathematical function that is the best predictor of the output, given
    the input features. This function can then be applied to a test dataset in order
    to quantify its accuracy, and thereafter to a dataset that it has never seen before
    in order to make predictions!
  prefs: []
  type: TYPE_NORMAL
- en: Regression problems are where we want to predict a numerical outcome. Examples
    of regression algorithms include **Linear Regression** and **Regression Trees**,
    and examples of real-world use cases include price, weight, and temperature prediction.
    Classification problems are where we want to predict a categorical outcome. Examples
    of classification algorithms include **Logistic Regression**, **Multinomial Logistic
    Regression**, and **Classification Trees**, and examples of real-world use cases
    include image classification and email spam classification. We will study these
    algorithms in more detail in [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml),
    *Supervised Learning Using Apache Spark*, where we will also develop supervised
    learning models that can be applied to real-world use cases whilst providing the
    ability to quantify their accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In unsupervised learning, the goal is to uncover hidden relationships, trends,
    and patterns, given only the input data *x[i]* with no output *y[i]*. In this
    case, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5cda5569-12fb-4524-953f-e6939182587c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In practice, this means that the emphasis is on uncovering interesting patterns
    and trends within a dataset in the absence of known and correct answers. Subsequently,
    unsupervised learning is commonly referred to as **knowledge discovery** given
    the fact that problems are less well-defined and we are not told what kind of
    patterns are contained within the data. **Clustering** is a well-known example
    of an unsupervised learning algorithm where the goal is to segment data points
    into groups, where all the data points in a specific group share similar features
    or attributes, as illustrated in *Figure 3.3*. Real-world use cases of clustering
    include document classification and clustering customers for marketing purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64c7ebf2-ea6e-4e32-a9bc-29b65c097295.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Clustering unsupervised learning model'
  prefs: []
  type: TYPE_NORMAL
- en: We will study unsupervised learning algorithms in more detail in [Chapter 5](eed6ec04-8a1b-459e-b225-b2761b26a460.xhtml),
    *Unsupervised Learning Using Apache Spark*, including hands-on development of
    real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforced learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In reinforced learning, a reward (or punishment) system is employed to impact
    behavior over time, based on interactions between an agent and its wider environment.
    An agent will receive state information from the environment and, based on that
    state, will perform an *action*. As a result of that action, the environment will
    transition to a new state that is then provided back to the agent, typically with
    a reward (or punishment). The goal of the agent is to therefore maximize the cumulative
    reward it receives. For example, consider the case of a child learning good behavior
    from bad behavior and being rewarded for good behavior with a treat from its parents.
    In the case of machines, consider the example of computer-based board game players.
    By combining deep learning with reinforced learning, computers can learn how to
    play board games with continually increasing levels of performance such that,
    over time, they become almost unbeatable!
  prefs: []
  type: TYPE_NORMAL
- en: Reinforced learning is beyond the scope of this book. However, to learn more
    about deep reinforced learning applied to gaming, please visit [https://deepmind.com/blog/deep-reinforcement-learning/](https://deepmind.com/blog/deep-reinforcement-learning/).
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In deep learning, a subfield within the broader field of machine learning, the
    goal is still to learn a function but by employing an architecture that mimics
    the neural architecture found in the human brain in order to learn from experience
    using a hierarchy of concepts or representations. This enables us to develop more
    complex and powerful functions in order to predict outcomes better.
  prefs: []
  type: TYPE_NORMAL
- en: Many machine learning models employ a two-layer architecture, where some sort
    of function maps an input to an output. However, in the human brain, multiple
    layers of processing are found, in other words, a neural network. By mimicking
    natural neural networks, **artificial neural networks (ANN)** offer the ability
    to learn complex non-linear representations with no restrictions on the input
    features and are ideally suited to a wide variety of exciting use cases, including
    speech, image and pattern recognition, **natural language processing** (**NLP**),
    fraud detection, forecasting and price prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Natural neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep learning algorithms mimic the neural architecture found in the human brain.
    If we were to study a single natural neuron in the human brain, we would find
    three primary areas of interest, as illustrated in *Figure 3.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/192b5cb4-e0ab-4820-a9b9-ec6c8ebf0d83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: A natural neuron'
  prefs: []
  type: TYPE_NORMAL
- en: Dendrites receive chemical signals and electrical impulses from other neurons
    that are collected and aggregated in the cell body. The nucleus, found within
    the cell body, is the control center of the neuron and is responsible for regulating
    cell functions, producing the proteins required to build new dendrites and for
    making the neurotransmitter chemicals used as signals. Signals can be classed
    as either inhibitory or excitatory. If they are inhibitory, this means that they
    are not transmitted to other neurons. If they are excitatory, this means that
    they are transmitted to other neurons via the axon. The axon is responsible for
    communicating signals between neurons, in some cases across distances as long
    as a couple of meters or a short as a few microns. The neuron therefore, as a
    single logical unit, is ultimately responsible for communicating information and
    the average human brain may contain around 100 billion neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The core concepts of the natural neuron can be generalized into components
    of a signal processing system. In this general signal processing system, the signals
    received by the dendrites can be thought of as the inputs. The nucleus can be
    thought of as a central processing unit that collects and aggregates the inputs
    and, depending on the net input magnitude and an activation function, transmits
    outputs along the axon. This general signal processing system, modeled on a natural
    neuron, is called an *artificial neuron* and is illustrated in *Figure 3.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46651759-ddc3-4669-8e1b-827bc63b1eca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: An artificial neuron'
  prefs: []
  type: TYPE_NORMAL
- en: Weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the artificial neuron, weights can amplify or attenuate signals and are
    used to model the established connections to other neurons as found in the natural
    world. By changing the weight vectors, we can effect whether that neuron will
    activate or not based on the aggregation of input values with weights, called
    the weighted or net input *z*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/749ec865-ad53-41cb-aa03-dbaeec6a27c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Activation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the weighted input plus a bias is calculated, the **activation function**,
    denoted by the Greek letter *phi* (Î¦), is used to determine the output of the
    neuron and whether it activates or not. To make this determination, an activation
    function is typically a non-linear function bounded between two values, thereby
    adding non-linearity to ANNs. As most real-world data tends to be non-linear in
    nature when it comes to complex use cases, we require that ANNs have the capability
    to learn these non-linear concepts or representations. This is enabled by non-linear
    activation functions. Examples of activation functions include a Heaviside step
    function, a sigmoid function, and a hyperbolic tangent function.
  prefs: []
  type: TYPE_NORMAL
- en: Heaviside step function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Heaviside step function is a basic discontinuous function that compares values
    against a simple threshold and is used for classification where the input data
    is *linearly* separable. The neuron is activated if the weighted sum plus a bias
    exceeds a certain threshold, denoted by the Greek letter *theta* (Î¸) in the equation
    below. If it does not, the neuron is not activated. The following step function
    is an example of a Heaviside step function that is bounded between *1* and *-1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7e537cb-d821-4892-92d4-d4c27ba3fa4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This Heaviside step function is illustrated in *Figure 3.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f2a68ff-30a8-4d82-b842-a730b3e6358f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Heaviside step activation function'
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A sigmoid function is a non-linear mathematical function that exhibits a sigmoid
    curve, as illustrated in *Figure 3.7*, and often refers to the sigmoid or logistic
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/620efead-9eb4-4d7f-a693-5323eace3821.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the sigmoid activation function is bounded between 0 and 1 and
    is smoothly defined for *all* real input values, making it a better choice of
    activation function than a basic Heaviside step function. This is because, unlike
    the Heaviside step function, non-linear activation functions can distinguish data
    that is *not* linearly separable, such as image and video data. Note that by using
    the sigmoid function as the activation function, the artificial neuron will, in
    fact, correspond to a logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc5496ba-3a94-4ae7-8e5c-a793f447f8a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Sigmoid activation function'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic tangent function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, a hyperbolic tangent function, as illustrated in *Figure 3.8*, is
    the ratio between the hyperbolic sine and cosine functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7027299-b4af-437f-aed3-8bf353128cfc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, an activation function based on a hyperbolic tangent function
    is bounded between *1* and *-1* and, similar to sigmoid functions, is smoothly
    defined for all real input values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9014c8e-7d06-4a12-9390-4d17f9379eb9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Hyperbolic tangent function'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An **artificial neural network** (**ANN**) is a connected group of artificial
    neurons, where the artificial neurons are aggregated into linked **neural** **layers**
    that can be divided into three types:'
  prefs: []
  type: TYPE_NORMAL
- en: The **input layer** receives input signals from the outside world and passes
    these input signals to the next layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layers**, if any, perform computations on these signals and pass them
    to the output layer. Therefore, the outputs of the hidden layer(s) act as inputs
    to the final output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **output layer** calculates the final output, which then influences the
    outside world in some manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Artificial neurons are linked across adjacent neural layers by **edges** that
    have weights associated with them. In general, the addition of more hidden neural
    layers increases the ability of the ANN to learn more complex concepts or representations.
    They are termed *hidden* because of the fact that they do not directly interact
    with the outside world. Note that all ANNs have an input and an output layer,
    with zero or more hidden layers. An ANN where signals are propagated in one direction
    only, in other words, signals are received by the input layer and forwarded to
    the next layer for processing, are called **feedforward** networks. ANNs where
    signals may be propagated back to artificial neurons or neural layers that have
    *already* processed that signal are called **feedback** networks. *Figure 3.9*
    illustrates the logical architecture of a feedforward ANN, where each circle represents
    an artificial neuron sometimes referred to as a **node** or a **unit**, and the
    arrows represent **edges** or **connections** between artificial neurons across
    adjacent neural layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e36af97-c45b-4bd4-9f07-8809de91a4d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Feedforward artificial neural network'
  prefs: []
  type: TYPE_NORMAL
- en: ANNs can be divided into two classes dependent on their architecture. **Mono-layer**
    or **single-layer** ANNs are characterized by the aggregation of all its constituent
    artificial neurons on the same level with no hidden layers. A single-layer perceptron
    is an example of a mono-layer ANN consisting of just one layer of links between
    input nodes and output nodes. **Multi-layerÂ **ANNs are characterized by the segmentation
    of artificial neurons across multiple linked layers. A multi-layer perceptron
    is an example of a multi-layer ANN consisting of one or more hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: ANNs learn by optimizing their weights to deliver a desired outcome, and that
    by changing weights, ANNs can deliver different results for the same inputs. The
    goal of optimizing the weights is to minimize a **loss function**âa function that
    calculates the price paid for inaccurate predictionsâby finding the best combination
    of weights that best predict the outcome. Recall that the weights represent established
    connections to other neurons; hence, by changing weights, ANNs are, in fact, mimicking
    natural neural networks by changing the connections between neurons. Various processes
    for learning optimal weight coefficients are provided in the following sub-sections
    during our discussions on perceptrons.
  prefs: []
  type: TYPE_NORMAL
- en: Single-layer perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Figure 3.10* illustrates the architecture of a single-layer perceptron. In
    this single-layer perceptron, an optimal set of weight coefficients are derived
    which, when multiplied by the input features, determines whether to activate the
    neuron or not. Initial weights are set randomly and, if the weighted input results
    in a predicted output that matches the desired output (for example, in a supervised
    learning classification context), then no changes to the weights are made. If
    the predicted output does not match the desired output, then weights are updated
    to reduce the error.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes single-layer perceptrons best suited as classifiers, but only when
    the classes are *linearly separable*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/decfc40a-779e-4dbc-8c46-4b38bd9b20e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Single-layer perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Multi-layer perceptrons differ from single-layer perceptrons as a result of
    the introduction of one or more hidden layers, giving them the ability to learn
    non-linear functions. *Figure 3.11* illustrates the architecture of a multi-layer
    perceptron containing one hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bcdc382-9e9f-4320-a465-ff853410d8a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: Multi-layer perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: '**Backpropagation** is a supervised learning process by which multi-layer perceptrons
    and other ANNs can learn, that is to say, derive an optimal set of weight coefficients.
    The first step in backpropagation is, in fact,Â **forward propagation**, whereby
    all weights are set randomly initially and the output from the network is calculated
    (similar to single-layer perceptrons, but this time involving one or more hidden
    layers). If the predicted output does not match the desired output, the total
    error at the output nodes is propagated back through the entire network in an
    effort to readjust all weights in the network so that the error is reduced in
    the output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-layer neural networks, such as multi-layer perceptrons, are generally
    much more compute intensive, since the process to optimize weights involves a
    much greater number of weights and calculations. Therefore, training neural networks,
    which also typically involves a large number of data points in order to learn
    a large number of optimal weight coefficients, requires CPU and memory resources
    that previously were not readily available or cost-effective. However, with the
    advent of distributed systems, like those described in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml),Â *The
    Big Data EcosystemÂ *and the availability of cost-effective, high-performance,
    and resilient distributed clusters that support the processing of petabytes of
    data hosted by commodity hardware, research into ANNs and deep learning has exploded,
    as too has their application to exciting real-world artificial intelligence use
    cases, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Healthcare and combating disease, including predictive diagnosis, drug discovery,
    and gene ontology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech recognition, including language translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image recognition, including visual search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theoretical physics and astrophysics, including satellite image classification
    and gravitational wave detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this sub-section, we have discussed two specific types of ANN,Â single-layer
    perceptrons and multi-layer perceptrons, which we will study in more detail in
    [Chapter 7](337b904f-87f1-4741-bd75-d7fd983185f6.xhtml),Â *Deep Learning Using
    Apache Spark*, including the hands-on development of real-world applications.
    Other classes of artificial network networks include **convolutional** **neural
    networks** (also described in [Chapter 7](337b904f-87f1-4741-bd75-d7fd983185f6.xhtml),
    *Deep Learning Using Apache Spark*), **recurrent neural networks**, **Kohonen
    self-organizing neural networks**, and **modular** **neural networks**, which
    are beyond the scope of this book. To learn more about ANNs and the exciting field
    of deep learning, please visit [http://deeplearning.net/](http://deeplearning.net/).
  prefs: []
  type: TYPE_NORMAL
- en: NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLP refers to a family of computer science disciplines, including machine learning,
    linguistics, information engineering, and data management, used to analyze and
    understand natural languages, including speech and text. NLP can be applied to
    a wide variety of real-world use cases, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Named entity recognition** (**NER**): Automatically identifying and parsing
    entities from text, including people, physical addresses, and email addresses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relationship extraction**: Automatically identifying the types of relationships
    between parsed entities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine translation and transcription**: Automatically translating from one
    natural language to another, for example, from English to Chinese'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Searching**: Automatically searching across vast collections of structured,
    semi-structured, and unstructured documents and objects in order to fulfill a
    natural language query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech recognition**: Automatically deriving meaning from human speech'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: Automatically identifying human sentiment toward a
    topic or entity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question answering**: Automatically answering natural, fully-formed questions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A common technique employed in NLP is developing a data engineering pipeline
    that pre-processes text in order to generate features for input into machine learning
    models. Common pre-processing techniques include **tokenization** (splitting text
    into smaller and simpler units called tokens, where tokens are often individual
    words or terms), **stemming** and **lemmatisation** (reducing tokens to a base
    form), and removing **stop-words** (such as *I*, *this*, andÂ *at*). The resultant
    set of terms is converted into features to then feed into machine learning models.
    A very basic algorithm used to convert the set of terms into features is called
    **Bag of Words**, which simply counts the number of occurrences of each unique
    term, thereby converting text into numeric feature vectors.
  prefs: []
  type: TYPE_NORMAL
- en: NLP is important as it provides a means to achieve true seamless interaction
    between artificially intelligent systems/machines and humans, such as through
    conversation interfaces. We will study NLP in more detail in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml),Â *Natural
    Language Processing Using Apache Spark*, including the hands-on development of
    real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Cognitive computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to NLP, cognitive computing actually refers to a family of computer
    science disciplines, including machine learning, deep learning, NLP, statistics,
    business intelligence, data engineering, and information retrieval that, together,
    are used to develop systems that simulate human thought processes. Real-world
    implementations of cognitive systems include chatbots and virtual assistants (such
    as Amazon Alexa, Google Assistant, and Microsoft Cortana) that understand natural
    human language and provide contextual conversation interfaces, including question-answering,
    personalized recommendations, and information retrieval systems.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning pipelines in Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To end this chapter, we will take a look at how Apache SparkÂ can be used to
    implement the algorithms that we have previously discussed by taking a look at
    how itsÂ machine learning library,Â `MLlib`, works under the hood. `MLlib` provides
    a suite of tools designed to make machine learning accessible, scalable, and easy
    to deploy.
  prefs: []
  type: TYPE_NORMAL
- en: Note that as of Spark 2.0, the `MLlib` RDD-based API is in maintenance mode.
    The examples in this book will use the DataFrame-based API, which is now the primary
    API for `MLlib`. For more information, please visitÂ [https://spark.apache.org/docs/latest/ml-guide.html](https://spark.apache.org/docs/latest/ml-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, the typical implementation of machine learning models can
    be thought of as an ordered pipeline of algorithms, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction, transformation, and selection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a predictive model based on these feature vectors and labels
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make predictions using the trained predictive model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate model performance and accuracy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`MLlib` exposes two core abstractions that facilitate this high-level pipeline
    and allow machine learning models to be developed in Apache Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformers**: Formally, a transformer converts one DataFrame (see [Chapter
    1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The Big Data Ecosystem*) into
    another DataFrame. The new DataFrame will typically contain one or more new columns
    appended to it. In the context of a machine learning model, an input DataFrame
    may consist of a column containing the relevant feature vectors. A transformer
    will then take this input DataFrame and predict a label for each feature vector.
    The transformer will then output a new DataFrame with a new column containing
    the predicted labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Estimators**: Formally, an estimator abstracts a learning algorithm. In practice,
    an estimator is a type of learning algorithm, such as a logistic regression algorithm.
    In this case, the estimator is called *LogisticRegression* in `MLlib`. The estimator
    will take an input DataFrame and call the `fit()` method on it. The output of
    the `fit()` method, and hence the output of the estimator, will be a *trained
    model*. In this example, the *LogisticRegression* estimator will produce a trained
    *LogisticRegressionModel* model object. The model object itself is, in fact, a
    *transformer*, because the trained model can now take a new DataFrame containing
    new feature vectors and make predictions on them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returning to our definition of a pipeline, this can now be extended. A pipeline
    is, in fact, an ordered sequence of stages where each stage is either a transformer
    or an estimator.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.12* illustrates a pipeline used to train a model. In *Figure 3.12*,
    NLPÂ *transformations* are applied to tokenize raw training text into a set of
    words or terms. The tokenizer is referred to as a feature *transformer*. An algorithm
    called HashingTF is then applied to take the set of terms and convert it into
    fixed-length feature vectors (HashingTF ultimately calculates the term frequencies
    using a hash function). HashingTF is also a *transformer*. The *LogisticRegression*
    estimator is then applied to these feature vectors, via `LogisticRegression.fit()`,
    to generate a trained *LogisticRegressionModel*, which itself is a type of *transformer*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8665b5eb-e972-45ef-a9eb-8d323d6d4997.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: MLlib training pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.13* illustrates a pipeline used to test a model. In this diagram,
    similar to the training pipeline, a tokenizer feature *transformer* is used to
    extract terms from the raw test text, and then the HashingTF *transformer* is
    applied to convert the set of terms into fixed-length feature vectors. However,
    since we already have a trained model generated by the training pipeline in *Figure
    3.12*, the feature vectors are passed as input into this trained model *transformer*
    in order to make predictions and output a new DataFrame containing these predictions
    on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67484bc6-83e0-4a8d-b914-19c943b1c949.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: MLlib test pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to providing common machine learning algorithms and methods to extract,
    transform, and select model features and other pipeline abstractions, `MLlib`
    also exposes methods to save trained models and pipelines to an underlying filesystem
    that can then be loaded later on if and when required. `MLlib` also provides utility
    methods covering operations in statistics, linear algebra, and data engineering.
    To learn more about `MLlib`, please visit [http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have defined what is meant by artificial intelligence, machine
    learning, and cognitive computing. We have explored common machine learning algorithms
    at a high level, including deep learning and ANNs, as well as taking a look at
    Apache Spark's machine learning library,Â `MLlib`, and how it can be used to implement
    these algorithms within machine learning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start developing, deploying, and testing supervised
    machine learning models applied to real-world use cases using `PySpark` and `MLlib`.
  prefs: []
  type: TYPE_NORMAL
