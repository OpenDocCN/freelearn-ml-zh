- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multiclassifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding chapters, we saw that multi-label datasets, where a tweet may
    have zero, one, or more labels, are considerably harder to deal with than simple
    multi-class datasets where each tweet has exactly one label, albeit drawn from
    a set of more than one option. In this chapter, we will investigate ways of dealing
    with these cases, looking in particular at the use of **neutral** as a label for
    handling cases where a tweet is allowed to have zero labels; at using varying
    thresholds to enable standard classifiers to return a variable number of labels;
    and at training multiple classifiers, one per label, and allowing them each to
    make a decision about the label they were trained for. The conclusion, as ever,
    will be that there is no single “silver bullet” that provides the best solution
    in every case, but in general, the use of multiple classifiers tends to be better
    than the other approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using confusion matrices to analyze the behavior of classifiers on complex data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **neutral** as a label to deal with tweets that have no label assigned
    to them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varying thresholds to handle multi-label datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training multiple classifiers to handle multi-label datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand how to implement several strategies
    for dealing with muti-label datasets and will have an appreciation of the effectiveness
    of these strategies for different kinds of data.
  prefs: []
  type: TYPE_NORMAL
- en: Multilabel datasets are hard to work with
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start by looking at the performance of a selection of classifiers from
    previous chapters on the main datasets. We have said several times that multi-label
    datasets are particularly challenging, but it is worth bringing together the results
    from the best-performing algorithms to see exactly how challenging they are. *Figure
    10**.1* includes all the major classifiers that we have looked at so far. The
    multi-label datasets are highlighted in gray, and the best-performing classifier
    for each row is marked in bold/asterisks:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **CP** | **NB** | **SVM** | **SNN** | **DNN** | **Transformers**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-EN** | 0.497 | 0.593 | 0.775 | 0.845 | 0.829 | 0.847 | ***** **0.927
    *** |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | 0.348 | 0.353 | 0.227 | 0.224 | 0.242 | 0.246 | ***** **0.418
    *** |'
  prefs: []
  type: TYPE_TB
- en: '| **WASSA-EN** | 0.437 | 0.505 | 0.709 | ***** **0.770 *** | 0.737 | 0.752
    | 0.753 |'
  prefs: []
  type: TYPE_TB
- en: '| **CARER-EN** | 0.350 | 0.395 | 0.776 | 0.770 | ***** **0.820*** | 0.804 |
    0.816 |'
  prefs: []
  type: TYPE_TB
- en: '| **IMDB-EN** | 0.667 | 0.722 | 0.738 | 0.736 | 0.793 | 0.793 | ***** **0.826
    *** |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-AR** | 0.509 | 0.513 | 0.531 | 0.514 | 0.504 | 0.444 | ***** **0.710
    *** |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | ***** **0.386 *** | 0.382 | 0.236 | 0.216 | 0.221 | 0.207
    | 0.359 |'
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | 0.663 | ***** **0.666 *** | 0.494 | 0.631 | 0.028 | 0.026
    | 0.053 |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-ES** | 0.420 | 0.177 | 0.360 | 0.412 | 0.337 | 0.343 | ***** **0.663
    *** |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | 0.271 | 0.278 | 0.230 | 0.226 | 0.221 | 0.222 | ***** **0.340
    *** |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.1 – Selected Jaccard scores for the standard datasets (multi-label
    datasets in gray)
  prefs: []
  type: TYPE_NORMAL
- en: 'Two things stand out from this table:'
  prefs: []
  type: TYPE_NORMAL
- en: For most of the entries in this table, LEX is the worst classifier, with NB
    coming next, and then the others generally scoring fairly similarly. For the multi-label
    cases, however, LEX or CP are always better than anything else except transformers,
    and in a couple of cases, they are better than transformers as well. Given that
    these seem to be the most realistic datasets, since plenty of tweets express no
    emotion and a fair number express more than one, it is worth looking in more detail
    at what is going on in these cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The multi-label cases also score significantly worse overall – while LEX and
    CP score better than most other classifiers on these cases, they do generally
    score worse on them than on the other cases, and for all the other classifiers,
    the gap between these cases and the one emotion/tweet cases is substantial.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These cases seem likely to be the most useful in practice since most tweets
    do not express any sentiment and a fair number express more than one, so algorithms
    that do not deal well with these cases may not be the most suitable for this task.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Confusion matrices* section, we will look at what the various algorithms
    do with the two kinds of datasets. Once we have a clearer idea of why multi-label
    datasets are so much more difficult to handle than single-label ones, and we have
    seen the specific problems that they cause for particular algorithms, we will
    look at ways of dealing with this kind of dataset. We will not carry out these
    experiments with transformer-based models, partly because the time it takes to
    train a transformer makes this infeasible, but more importantly because we need
    to look inside the models to understand what is going on – this is all but impossible
    with transformer-based models.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It can be very difficult to see what kinds of mistakes a classifier makes just
    by looking at the raw output. **Confusion matrices** allow us to visualize a classifier’s
    behavior, making it possible to see when two classes are being systematically
    confused or when a given class is being assigned too few or too many items. Consider
    the following dataset, where each item is classified as A, B, or C by the Gold
    Standard (G) and also has a predicted value (P):'
  prefs: []
  type: TYPE_NORMAL
- en: '| G | C | C | A | B | C | B | C | B | B | B | A | A | B | B | C | C | B | C
    | B | B | C | A | B | A | A | C | C | C | A | A | A | C | B | C | A | A | B |
    A |'
  prefs: []
  type: TYPE_TB
- en: '| P | C | B | B | B | C | A | A | B | B | A | A | A | C | B | A | B | B | C
    | B | C | C | A | B | B | B | C | B | B | B | A | B | C | B | B | A | A | B |
    A |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.2 – Gold Standard and predicted values for the example data
  prefs: []
  type: TYPE_NORMAL
- en: 'It is hard to see any pattern in this table. Simply counting how many cases
    have the same value for G and P gives us 22 out of 38 – that is, an accuracy of
    0.58 – but it is very hard to see what kinds of things it gets right and what
    kinds of things it gets wrong. Converting this into a confusion table can help
    with this. We do this by counting the number of times that an item that ought
    to be assigned C1 as its value is predicted to have C2, producing a table of correct
    versus predicted assignments. The confusion matrix in *Figure 10**.3*, for instance,
    shows that seven things that should have been assigned the label A were indeed
    assigned that label but five were assigned B, and that six things that should
    have been assigned C were assigned C but five were assigned B. This suggests that
    there is something about the properties of Bs that makes it easy to assign things
    to this class when they should be assigned to A or C, which might lead to a line
    of inquiry about which properties of Bs were leading to this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **A** | **B** | **C** |'
  prefs: []
  type: TYPE_TB
- en: '| **A** | 7 | 5 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **B** | 2 | 9 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **C** | 2 | 5 | 6 |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.3 – Confusion matrix for the data in Figure 10.2
  prefs: []
  type: TYPE_NORMAL
- en: 'If `gs` and `p` are the Gold Standard values for a set of points, then `confusion`
    will calculate the confusion matrix: `c` is a table with an entry for each label
    in `gs`, where the value for a label is the set of counts of each time a label
    has been predicted for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Confusion matrices can provide a considerable amount of information about what
    a classifier is doing. We do, however, have a slight problem with constructing
    confusion matrices when the Gold Standard and the prediction can each contain
    a varying number of emotions. Suppose, for instance, that the Gold Standard for
    some tweets is **love+joy** and the prediction is **love+sad+angry**. We want
    to acknowledge that the classifier was right when it predicted **love**, but what
    do we do about the fact that it missed **joy** (that is, there is a false negative)
    and predicted **sad** and **angry** (two false positives)?
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no right answer to this question. We adapt the standard way of constructing
    a confusion matrix as follows, where C[e1][e2] is the score for *e1* in the Gold
    Standard and *e2* in the prediction. We need to add a row and a column for “no
    emotion assigned” (we will use **--** for this class):'
  prefs: []
  type: TYPE_NORMAL
- en: For every case where the Golden Standard and the prediction contain a given
    emotion, *e*, add 1 to *C[e][e]* and remove *e* from both the Golden Standard
    and the prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the Golden Standard is now empty, then every *e* left in the prediction must
    be a false positive, so add 1 to *C[--][e]* for each remaining e.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the prediction is empty, then every *e* left in the Golden Standard must
    be a false negative, so add 1 to *C[e][--]*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If neither of them is empty after removing the shared cases, it is hard to see
    what to do. Consider the preceding example. After removing **love**, we are left
    with **joy** in the Golden Standard and **sad+angry** in the prediction. Is **joy**
    a mistake for **sad**, with **angry** as a false positive? Is **joy** a mistake
    for **angry**, with **sad** as a false positive? Is **joy** a false negative and
    **sad** and **angry** both false positives? This last suggestion does not seem
    right. Suppose we had one case where **joy** was matched with **sad+angry**, another
    where it was matched with **sad+fear**, and another where it was matched with
    just **sad**. If we marked all of these as cases where **joy** was a false negative
    and **sad** was a false positive, we would miss the fact that there appears to
    be a connection between **joy** and **sad**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We deal with this as follows. Suppose there are **G** items left in the Gold
    Standard and **P** items left in the prediction after the labels that appear on
    both have been removed. Here, for each **g** in the Gold Standard and each **p**
    in the prediction, we add *1/P* to *C[p][g]*. Doing this adds a total of **G**
    to the confusion matrix, thus acknowledging that the number of emotions in the
    Gold Standard has not been matched, with each item in the prediction seen being
    as equally likely to be the one that should be substituted for **g**.
  prefs: []
  type: TYPE_NORMAL
- en: The machinery for calculating modified confusion matrices is fairly intricate,
    and including it here would add very little to the preceding explanation. The
    code for this can be found in this book’s GitHub repository – for now, it is probably
    best just to note that when an item can be assigned multiple labels, the confusion
    matrix has to take account of situations where the Gold Standard and the prediction
    both assign multiple labels, with the sets being assigned being of different sizes
    and where some labels are common to both, some only appear in the Gold Standard
    and some only appear in the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The way we do this is not symmetric between the Gold Standard and the prediction,
    but it does provide confusion matrices that tell us something useful about what
    a given classifier is doing. For cases where there is exactly one item in the
    Gold Standard and one in the prediction, it collapses to the standard version,
    and where there are differing numbers in each, it does provide a picture of what
    is going on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by looking at the confusion matrix for CARER-EN using SVM as
    the classifier (the scores for SVMs and DNNs are very similar, and the confusion
    matrices are also very similar, so for convenience, we will use SVMs for the explorations
    here). The following matrix was obtained using a version of SVM which simply picks
    the most likely emotion for each tweet instead of using a threshold to try to
    work out whether there are any that are likely enough for them to be counted and
    if so, whether there are several that could count:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **anger** | **fear** | **joy** | **love** | **sadness** | **surprise**
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 124 | 0 | 1 | 0 | 1 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 1 | 128 | 0 | 0 | 0 | 1 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 0 | 337 | 1 | 0 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 0 | 1 | 73 | 0 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **sadness** | 0 | 1 | 1 | 0 | 293 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **surprise** | 0 | 0 | 0 | 0 | 0 | 37 |  |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.4 – Confusion matrix for CARER-EN, one emotion per tweet, with SVM
    as the classifier
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what you expect a confusion matrix to look like – the largest scores
    down the diagonal with a scattering of other assignments, with the biggest confusion
    being between **love** and **joy**. When we use the same algorithm for SEM11-EN,
    we get a very different picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **anti** | **disg** | **fear** | **joy** | **love** | **opti**
    | **pess** | **sadn** | **surp** | **trus** | **--** |'
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 311 | 2 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **antici** | 8 | 65 | 1 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| **disgus** | 10 | 3 | 36 | 1 | 3 | 0 | 0 | 0 | 0 | 0 | 1 | 182 |'
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 9 | 0 | 0 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 34 |'
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 11 | 2 | 1 | 1 | 186 | 0 | 0 | 0 | 0 | 0 | 1 | 39 |'
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 1 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 7 | 1 | 0 | 2 | 2 | 0 | 20 | 1 | 0 | 0 | 0 | 119 |'
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 19 | 0 | 0 | 0 | 26 |'
  prefs: []
  type: TYPE_TB
- en: '| **sadnes** | 9 | 3 | 1 | 1 | 3 | 0 | 0 | 1 | 16 | 0 | 0 | 119 |'
  prefs: []
  type: TYPE_TB
- en: '| **surpri** | 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 2 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| **--** | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 21 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.5 – Confusion matrix for SEM11-EN, one emotion per tweet, with SVM
    as the classifier
  prefs: []
  type: TYPE_NORMAL
- en: 'We get several false positives (places where nothing was expected but something
    was predicted – that is, the row headed with **--**: two of these have **anger**
    assigned and 21 **trust**). This happens because we have forced the classifier
    to choose something even in cases where the Gold Standard doesn’t expect anything.
    We also have a much larger number of places where there are false negatives (the
    column headed with **--**), where something was expected but nothing was found,
    generally because the Gold Standard had multiple labels and there was only one
    prediction. And there are numerous cases where the assignment is just wrong, with
    an awful lot of things being labeled as **anger** when they should be something
    else.'
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that if the classifier is forced to assign exactly one emotion
    per tweet, then it cannot help producing false positives (if the Gold Standard
    says that nothing should be assigned) and false negatives (if the Gold Standard
    says that more than one emotion should be assigned). If we look at the test set
    in detail, we will see that there are 23 tweets with no emotion assigned, which
    show up as false positives, and 645 tweets with more than one emotion assigned,
    which show up as 1,065 false negatives (because some of them have three or more
    emotions assigned). *There is nothing that can be done about this if our classifier
    assumes that there is one emotion* *per tweet.*
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we have *N* tweets, *X* of which have no emotion assigned to them
    and *Y* have more than one. In this case, there must be at least *X* false positives
    (one for each tweet that should have no labels assigned but the classifier assigns
    one) and at least *Y* false negatives (one for each tweet that should have more
    than one label assigned but the classifier only assigns one), meaning that the
    best possible Jaccard score is *(N-X)/((N-X)+X+Y)*. For the set of 772 tweets
    in SEM11-EN, this comes out as *(772-23)/(772-23+(1065+23)) = 0.41* (the number
    of false negatives is very high because of the preponderance of tweets that should
    be given more than two labels – this equation assumed that tweets were assigned
    zero, one, or two labels). This is a strict upper bound. No classifier that assigns
    exactly one label to each tweet can achieve a higher Jaccard score than 0.41 on
    this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The position is worse than that. Careful inspection of the diagonal shows that
    several emotions have good scores on the diagonal (**anger**, **joy**), while
    others have very poor scores on the diagonal and a lot of false negatives (**disgust**,
    **love**, **optimism**), with several emotions being confused with **anger**.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we look at the KWT.M-AR dataset, we will see an output that is in some
    ways similar and is no more encouraging:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **--** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 0 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 12 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 0 | 0 | 0 | 50 | 0 | 0 | 0 | 44 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 0 | 0 | 0 | 0 | 22 | 0 | 0 | 17 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **--** | 0 | 1 | 0 | 0 | 8 | 3 | 0 | 0 | 751 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.6 – Confusion matrix for KWT.M-AR, one emotion per tweet, with SVM
    as the classifier
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, there are a massive number of false positives, reflecting the fact
    that these datasets have a very high proportion of cases where no emotion is assigned
    in the Gold Standard (763 out of 1,000 in the test set used here, with a maximum
    attainable F1-score of around 0.42). Again, this is inevitable – if a tweet should
    have no emotions assigned to it and the classifier is forced to assign one, then
    we will get a false positive. It is also worth noting that while there are non-trivial
    entries on the diagonal, a surprising number of cases have the correct assignment
    replaced by `angry: 2.38`, `fuming: 2.32`, `annoying: 2.31`, `revenge: 2.26`,
    … for `positivity: 1.82`, 💕`: 1.75`, `rejoice: 1.74`, `gift: 1.72`, `laughing:
    1.70` for `flat: 1.25`, `com: 1.19`, `cup: 1.06`, `need: 1.05`, `major: 1.05`.
    These are not words that are obviously associated with trust, and indeed the links
    between them and this emotion are not strong. So, when the classifier is forced
    to choose an emotion for a tweet that does not contain any words that are linked
    to specific emotions, it is likely to resort to choosing the one for which no
    such words are expected anyway.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If, as suggested previously, large numbers of tweets express either no emotion
    or several emotions, then we have to deal with these issues. There are several
    things we can try:'
  prefs: []
  type: TYPE_NORMAL
- en: We can include an explicit “none-of-the-above” or “neutral” class to represent
    the fact that a tweet does not carry any emotional weight. This is the easiest
    thing to do for “zero emotion” cases, though it will not be ideal in cases where
    more than one emotion is assigned to a tweet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use the fact that some of the classifiers calculate a score for each
    emotion. We will look at this in more detail shortly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can train a set of binary classifiers – **joy** versus **not-joy**, **anger**
    versus **not-anger**, and so on. This will potentially deal with both kinds of
    cases: if each of these classifiers returns the negative version, we will get
    an overall zero assignment, and if more than one returns the positive version,
    we will get multiple assignments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the remainder of this chapter, we will concentrate on the SEM-11 and KWT
    datasets since these are the only ones with variable numbers of labels. If your
    training data assigns exactly one emotion to each tweet, and you want to have
    exactly one emotion assigned when running the classifier on live data, then one
    of the others will generally provide the best solution – LEXCLASSIFIER usually
    provides reasonably accurate results with very little training time, Transformers
    usually provide the best results but take a lot of training, and SVMs and DNNs
    come somewhere in between for both accuracy and training time.
  prefs: []
  type: TYPE_NORMAL
- en: Using “neutral” as a label
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can introduce **neutral** as a label simply by looking at the labels assigned
    by the Gold Standard and assigning **neutral** if no other emotion is assigned.
    This does not affect the CARER-EN set: nothing is assigned **neutral** in the
    training data, so no words are found to be associated with this label, so, in
    turn, nothing is assigned to it by the classifier. The effect on the SEM11-EN
    data is more interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **anti** | **disg** | **fear** | **joy** | **love** | **opti**
    | **pess** | **sadn** | **surp** | **trus** | **neut** | **--** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 311 | 2 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **antici** | 8 | 65 | 1 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| **disgus** | 10 | 3 | 36 | 1 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 182 |'
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 9 | 0 | 0 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 34 |'
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 11 | 2 | 1 | 1 | 186 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 39 |'
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 1 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 7 | 1 | 0 | 2 | 2 | 0 | 20 | 1 | 0 | 0 | 0 | 1 | 118 |'
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 19 | 0 | 0 | 0 | 0 | 26 |'
  prefs: []
  type: TYPE_TB
- en: '| **sadnes** | 9 | 3 | 1 | 1 | 3 | 0 | 0 | 1 | 16 | 0 | 0 | 0 | 119 |'
  prefs: []
  type: TYPE_TB
- en: '| **surpri** | 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 2 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| **neutra** | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 21 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.7 – Confusion matrix for SEM11-EN, one emotion per tweet, with SVM
    as the classifier, and neutral as a label
  prefs: []
  type: TYPE_NORMAL
- en: There is very little change down the diagonal – that is, the classifier gets
    the same actual emotions right with or without **neutral** as a label; most things
    that ought to be classified as neutral are indeed labeled as such, with a couple
    mislabeled as **anger**; a few things are labeled as **neutral** when they should
    not be; and there are still a lot of false negatives because there were a lot
    of tweets that ought to have been given more than one label. These can’t be labeled
    **neutral** by the classifier since it can only assign one label to each tweet,
    so any tweet that ought to have more than one will contribute to the set of false
    negatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'The situation with the KWT examples is intriguing. These have large numbers
    of tweets with no emotion assigned to them, so we expect a lot of false positives
    if the classifier is set to assign one emotion per tweet. The confusion matrices
    for KWT.M-AR without and with **neutral** as a label are given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **--** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 0 | 19 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 23 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 0 | 0 | 0 | 82 | 0 | 0 | 0 | 47 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 0 | 0 | 0 | 0 | 37 | 0 | 0 | 18 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 2 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 2 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 12 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **--** | 0 | 2 | 0 | 3 | 13 | 2 | 0 | 0 | 697 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.8 – Confusion matrix for KWT.M-AR, one emotion per tweet, with SVM
    as the classifier, and neutral not included
  prefs: []
  type: TYPE_NORMAL
- en: As before, most of the scores on the diagonal are quite good – that is, most
    of the time, the classifier assigns the right label where there is a label to
    be assigned. Inevitably, there are a large number of false positives, nearly all
    of which are assigned to **trust**. As before, in almost every case where the
    Gold Standard says there should be no label, the classifier has chosen **trust**,
    rather than distributing the false positives evenly. Again, what seems to be happening
    is that the classifier does not associate any words particularly strongly with
    **trust**, so when it is given a tweet without any very significant words in it,
    it decides it cannot be any of the other classes, for which there are stronger
    clues, so it chooses **trust**.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we allow **neutral** as a label, the situation changes quite dramatically.
    Now, nearly all the false positives are assigned to **neutral**, which is the
    most reasonable outcome. There is a smattering of false negatives because this
    dataset contains tweets with multiple labels, but the diagonal is much clearer
    – most emotions are assigned correctly and most cases with no emotion are assigned
    to **neutral**:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **neut** | **--** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 0 | 19 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 0 | 24 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 0 | 0 | 0 | 81 | 0 | 0 | 0 | 0 | 48 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 0 | 0 | 0 | 0 | 37 | 0 | 0 | 0 | 18 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 2 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 2 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 5 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **neutra** | 0 | 2 | 0 | 2 | 14 | 2 | 0 | 0 | 0 | 697 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.9 – Confusion matrix for KWT.M-AR, one emotion per tweet, with SVM
    as the classifier, and neutral included
  prefs: []
  type: TYPE_NORMAL
- en: So, using **neutral** as a label provides a partial solution to the problems
    of none and multiple labels, but it *cannot* provide a complete one. Even if a
    classifier were 100% accurate when it assigned labels to tweets that ought to
    have exactly one label and when it assigned **neutral** to ones that ought to
    have no labels, it must introduce false negatives for cases where there ought
    to be more than one label.
  prefs: []
  type: TYPE_NORMAL
- en: Now is a good time to introduce a new measure of performance. The fact that
    most of the cases that are neither classified as **neutral** nor given no label
    at all lie on the diagonal suggests that the gross classification assigned to
    a set of tweets might be useful for gauging opinion, even if the assignments to
    individual tweets are unreliable. In particular, false negatives may not matter
    too much when you’re trying to spot general trends, so long as the cases where
    the classifier does assign a label are consistent with the underlying reality.
  prefs: []
  type: TYPE_NORMAL
- en: It is, of course, not possible to spot whether something is a false negative
    or is a genuine instance of a neutral assignment, and then ask what the assignment
    for the false negatives should have been. If we could do that, then we would have
    trained the classifier to do it in the first place, and likewise with false positives.
    The best we can do is assess just how much we would be led astray if we accepted
    all the assignments that the classifier made at face value. So, we define the
    **proportionality** of the classifier as the cosine distance between the proportion
    of concrete tweets assigned to each emotion in the Gold Standard and the predictions
    (that is, ignoring tweets that are assigned to **neutral** or were not given any
    labels at all). The nearer to 1 this is, the more we can expect our classifier
    to give us a reliable overall picture, even if some of the individual assignments
    were wrong.
  prefs: []
  type: TYPE_NORMAL
- en: To take a simple example, suppose that we had a dataset with the 11 emotions
    from the SEM11 data, with the same number of tweets assigned to **pessimism**
    and **sadness**, and that it got everything right except that it labeled exactly
    half the tweets that should be labeled as pessimistic as sad and exactly half
    the tweets that should be labeled as sad as pessimistic. In this case, proportionality
    would be perfect, and you could safely use the classifier to make judgments about
    the overall picture, even though you could not rely on it to tell you whether
    a given tweet was sad or pessimistic. Similarly, if half the tweets in every category
    were assigned no emotion, then proportionality would be perfect, whereas if half
    the tweets in the most common category were assigned neutral but no others were,
    then it would be fairly poor.
  prefs: []
  type: TYPE_NORMAL
- en: 'From now on, we will do this for all the classifiers that are generated by
    the folds that we perform the training on, because the test sets associated with
    the individual folds are comparatively small (which is why we do cross-fold validation
    in the first place) and we lose quite a lot of instances by ignoring neutral and
    unassigned tweets. To calculate the proportionality, we can just count the number
    of tweets whose predicted/Gold Standard includes each emotion, *ignoring neutral
    and unassigned*, and normalize the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can do this for the prediction and the Gold Standard and use cosine
    similarity to calculate the similarity between the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For SEM11-EN, allowing an arbitrary number of emotions per tweet and using
    LEX as a classifier and `neutral` as a label, for instance, the proportions of
    the tweets that have each of the labels assigned in the prediction and the Gold
    Standard are `anger: 0.30`, `anticipation: 0.00`, `disgust: 0.31`, `fear: 0.02`,
    `joy: 0.25`, `love: 0.00`, `optimism: 0.04`, `pessimism: 0.00`, `sadness: 0.06`,
    `surprise: 0.00`, `trust: 0.00` and `anger: 0.18`, `anticipation: 0.06`, `disgust:
    0.17`, `fear: 0.06`, `joy: 0.15`, `love: 0.04`, `optimism: 0.12`, `pessimism:
    0.04`, `sadness: 0.12`, `surprise: 0.02`, `trust: 0.02`, respectively, for a proportionality
    score of 0.89\. If we use the same classifier with `neutral` as a label but allow
    exactly one label per tweet, the proportionality drops to 0.87.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we apply this to the KWT.M-AR dataset, we get `anger: 0.03`, `dissatisfaction:
    0.07`, `fear: 0.00`, `joy: 0.07`, `love: 0.69`, `optimism: 0.10`, `pessimism:
    0.00`, `rejection: 0.02`, `trust: 0.02` for the predictions and `anger: 0.04`,
    `dissatisfaction: 0.16`, `fear: 0.02`, `joy: 0.10`, `love: 0.43`, `optimism: 0.18`,
    `pessimism: 0.01`, `rejection: 0.01`, `trust: 0.05` for the Gold Standard, for
    a proportionality score of 0.94\. If we had not ignored the neutral/unassigned
    cases, the score would have been much higher, at 0.99, because of the huge preponderance
    of cases that are neutral in this dataset. So, we have a useful single number
    that gives us a handle on how reliable a classifier is for providing an overall
    picture, even if it fails to assign a concrete label to every tweet (that is,
    if some are either not assigned anything at all or are assigned **neutral**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This score will typically be quite high since, in most cases, most of the concrete
    scores lie on the diagonal. What matters is whether the distribution of neutral/unassigned
    cases follows the general distribution of the concrete cases – if it does, then
    the classifier will be useful for assessing general trends, even if it does sometimes
    fail to assign concrete labels when it should. So, we will use this measure in
    addition to Jaccard to assess the classifiers in the remainder of this chapter.
    The tables in *Figures 10.10* and *10.12* show what happens to the proportionality
    for various classifiers when we add **neutral** as a label, sticking to assigning
    exactly one label per tweet. As a reference point, we will start by looking at
    what happens if we specify that each classifier returns without using **neutral**.
    As before, the classifier with the best Jaccard score is marked in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **NB** | **SVM** | **DNN** |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | 0.224 (0.813) | 0.229 (0.690) | 0.223 (0.771) | *** 0.242
    (****0.677) *** |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | *** 0.247 (****0.824) *** | 0.216 (0.667) | 0.204 (0.736)
    | 0.207 (0.613) |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | 0.225 (0.799) | *** 0.226 (****0.788) *** | 0.215 (0.888)
    | 0.222 (0.774) |'
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | *** 0.208 (****0.973) *** | 0.108 (0.352) | 0.078 (0.207)
    | 0.026 (0.148) |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.10 – Jaccard and proportionality (in brackets), one label per tweet,
    neutral not included
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.10* shows that if we simply use the original classifiers unchanged
    – that is, with one emotion per tweet and without **neutral** as a label – we
    get fairly poor Jaccard scores, but the proportionality scores for LEX range from
    reasonable to pretty good, with the other classifiers generally doing worse on
    this metric. The proportionality score for LEX on the KWT.M-AR dataset in particular
    is massively better than the same score for any of the other classifiers on this
    dataset. The key here is that NB, SVM, and DNN assign nearly all the cases that
    should have been labeled as **neutral** to **trust** because these cases lack
    any of the distinguishing words that are common in the more clearly marked emotions,
    whereas LEX distributes them more closely in line with the marked cases. It is
    worth noting that the classifier with the best score for a given dataset does
    not always produce the best proportionality for that set:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **--** |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 9 | 19 | 0 | 0 | 11 | 2 | 0 | 0 | 0 | 14 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 4 | 133 | 0 | 2 | 71 | 1 | 0 | 0 | 0 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 3 | 2 | 0 | 12 | 2 | 0 | 0 | 0 | 2 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 1 | 6 | 0 | 53 | 50 | 8 | 0 | 0 | 1 | 18 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 7 | 0 | 8 | 548 | 12 | 0 | 0 | 0 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 5 | 0 | 1 | 44 | 180 | 0 | 0 | 1 | 2 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 4 | 0 | 0 | 7 | 2 | 2 | 0 | 1 | 1 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 2 | 0 | 0 | 3 | 0 | 0 | 3 | 0 | 2 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 1 | 9 | 0 | 2 | 28 | 8 | 0 | 0 | 13 | 3 |  |'
  prefs: []
  type: TYPE_TB
- en: '| **--** | 30 | 880 | 4 | 159 | 2008 | 577 | 2 | 1 | 61 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.11 – Confusion matrix for KWT.M-AR, one label per tweet, with LEX
    as the classifier, and neutral not included
  prefs: []
  type: TYPE_NORMAL
- en: 'When we allow **neutral** as a label, NB and SVM can choose this as the class
    with the least distinctive terms and hence assign cases that should be **neutral**
    to it, leading to a massive improvement in both Jaccard and proportionality for
    these classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **NB** | **SVM** | **DNN** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | **0.222 (0.813)** | **0.227 (0.690)** | **0.222 (0.768)**
    | *** 0.239 (****0.677) *** |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | *** 0.246 (****0.824) *** | **0.216 (0.666)** | **0.204 (0.736)**
    | **0.207 (0.615)** |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | **0.221 (0.800)** | *** 0.222 (****0.787) *** | **0.211 (0.885)**
    | **0.216 (0.774)** |'
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | **0.608 (0.984)** | **0.510 (0.986)** | *** 0.632 (****0.992)
    *** | **0.595 (0.905)** |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.12 – Jaccard and proportionality, one label per tweet, and neutral
    included
  prefs: []
  type: TYPE_NORMAL
- en: So, we can see that using proportionality as a metric allows us to spot general
    trends. Most of our classifiers work better on multi-label datasets if we allow
    **neutral** as a label, particularly when looking at proportionality, but LEX
    performs quite well even without **neutral** as a label.
  prefs: []
  type: TYPE_NORMAL
- en: Thresholds and local thresholds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next option to be explored is the use of thresholds. As we have seen, most
    of our classifiers provide scores for every option for each tweet, with the default
    setting being to choose the option with the highest score. In [*Chapter 6*](B18714_06.xhtml#_idTextAnchor134),
    *Naive Bayes*, we saw that assuming that our classifier will assign exactly one
    label to each tweet puts quite a tight upper bound on how well it can perform
    and that instead of doing that, we can set a threshold and say that everything
    that exceeds that threshold should be accepted as a label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following tweet: “*Hi guys ! I now do lessons via Skype ! Contact
    me for more info . # skype # lesson # basslessons # teacher # free lesson # music
    # groove # rock #* *blues*.”'
  prefs: []
  type: TYPE_NORMAL
- en: The Gold Standard assigns this the scores (‘anger’, 0), (‘anticipation’, 1),
    (‘disgust’, 0), (‘fear’, 0), (‘joy’, 1), (‘love’, 0), (‘optimism’, 0), (‘pessimism’,
    0), (‘sadness’, 0), (‘surprise’, 0), (‘trust’, 0), so it should be labeled as
    **anticipation+joy**.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes assigns this tweet the scores (‘anger’, ‘0.00’), (‘anticipation’,
    ‘0.88’), (‘disgust’, ‘0.00’), (‘fear’, ‘0.00’), (‘joy’, ‘0.11’), (‘love’, ‘0.00’),
    (‘optimism’, ‘0.00’), (‘pessimism’, ‘0.00’), (‘sadness’, ‘0.00’), (‘surprise’,
    ‘0.00’), (‘trust’, ‘0.00’), so if we set the threshold at 0.1, we would get **anticipation+joy**,
    if we set it at 0.2, we would just get **anticipation**, and if we set it at 0.9,
    we would get nothing.
  prefs: []
  type: TYPE_NORMAL
- en: For the same tweet, SVM assigns (‘anger’, ‘-0.77’), (‘anticipation’, ‘0.65’),
    (‘disgust’, ‘-2.64’), (‘fear’, ‘-1.67’), (‘joy’, ‘-0.99’), (‘love’, ‘-1.93’),
    (‘optimism’, ‘-3.52’), (‘pessimism’, ‘-1.61’), (‘sadness’, ‘-2.58’), (‘surprise’,
    ‘-1.47’), (‘trust’, ‘-3.86’). So, this time, if we set the threshold to -1, we
    would get **anger+anticipation+joy**, if we set it to 0, we would just get **anticipation**,
    and if we set it to 1, we would get nothing.
  prefs: []
  type: TYPE_NORMAL
- en: So, using a threshold will let us generate zero or more labels. We have to optimize
    the threshold, but we can do that simply by finding the smallest and greatest
    values that are assigned to any label in any tweet and incrementing evenly between
    these. The `bestThreshold` function, which was provided in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116),
    *Sentiment Lexicons and Vector-Space Models*, will work just as well with the
    raw scores produced by Naive Bayes, SVMs, and DNNs as it did there.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we contrast the scores that were obtained previously by requiring a single
    label and the ones we obtained using a threshold to allow zero or more labels
    on the crucial datasets, we will see that, overall, the latter produces better
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **NB** | **SVM** | **DNN** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | * 0.347 (0.898) * | 0.270 (0.764) | 0.250 (0.828) | 0.273
    (0.729) |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | * 0.377 (0.940) * | 0.257 (0.761) | 0.224 (0.798) | 0.246
    (0.731) |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | * 0.266 (0.890) * | 0.250 (0.837) | 0.228 (0.924) | 0.238
    (0.791) |'
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | * 0.691 (0.990) * | 0.522 (0.988) | 0.631 (0.998) | 0.604
    (0.935) |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.13 – Zero or more emotions per tweet, with optimal global thresholds
  prefs: []
  type: TYPE_NORMAL
- en: 'The scores here are much better than they were with the simple classifiers,
    with the proportionality scores all but perfect in some cases. There is, however,
    still some way to go if we want to get the labels for individual tweets right,
    rather than just getting a good overall picture. The next move is to set a threshold
    for each label, rather than for the dataset as a whole. We will adapt `bestThreshold`
    from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons and
    Vector Space Models* so that we can assign individual thresholds to labels. We
    will make two changes to the original definition:'
  prefs: []
  type: TYPE_NORMAL
- en: We will split it into two cases – one for calculating a global threshold (a
    single threshold for all cases) and another for calculating a local threshold
    for each label.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the original version, we looked at every column in the data, finding the
    minimum and maximum values that occur anywhere, and then looked at the predicted
    values for every column to calculate the Jaccard scores for each potential threshold.
    To calculate local thresholds, we just want to look at one column at a time. We
    can deal with both cases if we specify a range of columns, from `start` to `end`,
    to look at. For the global case, we must set `start=0` and `end=sys.maxsize`;
    for the case where we want to choose the best threshold for the `i` column, we
    must set `start=i` and `end=i+1`. This lets us use the same machinery for calculating
    both types of thresholds. The major changes to the original are highlighted in
    the following updated version:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results of allowing the classifiers to choose different thresholds for
    different labels are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **NB** | **SVM** | **DNN** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | * 0.371 (0.987) * | 0.271 (0.827) | 0.270 (0.809) | 0.277
    (0.811) |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | 0.371 (0.965) | 0.255 (0.854) | 0.236 (0.809) | 0.238 (0.795)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | * 0.267 (0.962) * | 0.192 (0.674) | 0.222 (0.983) | 0.202
    (0.852) |'
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | 0.681 (0.989) | 0.217 (0.163) | 0.615 (0.987) | 0.226 (0.167)
    |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.14 – Zero or more emotions per tweet, with optimal local thresholds
  prefs: []
  type: TYPE_NORMAL
- en: The proportionality scores for LEX have all improved, with LEX now easily giving
    the best proportionality score for SEM11-EN and Naive Bayes now reverting to choosing
    neutral/unassigned for nearly everything for KWT.U-AR and most of the other scores
    decreasing slightly, though the Jaccard scores have only improved for SEM11-EN
    and SEM11-ES. Yet again, different classifiers are better suited to different
    datasets and different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple independent classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using either LEX with optimal local thresholds or Naïve Bayes or SVM with an
    optimal global threshold with `MULTICLASSIFIER` class from [*Chapter 7*](B18714_07.xhtml#_idTextAnchor144),
    *Support Vector Machines*, to allow different kinds of classifiers to be used
    at the lower level. The key change here from the original is that we specify what
    classifier to use in the set of optional arguments, rather than assuming that
    we will be using `SVMCLASSIFIER`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This will make two-way classifiers for **anger** versus **not-anger**, **love**
    versus **not-love**, and so on using the specified kind of sub-classifier. For
    the individual classifiers, there is no point in allowing more than one label
    since while a tweet can satisfy both **love** and **joy**, or both **anger** and
    **fear**, it does not make any sense to allow a tweet to satisfy **anger** and
    **not-anger**. We can still get multiple labels overall if, for instance, both
    **love** versus **not-love** and **joy** versus **not-joy** are satisfied, and
    we can still get zero labels if all the negative labels are chosen, but it makes
    no sense to allow the individual classifiers to assign zero or more than one label.
  prefs: []
  type: TYPE_NORMAL
- en: 'As ever, there is a wide range of settings for the various subclassifiers.
    The main multiclassifier just combines the results of the individual subclassifiers
    and hence has no significant parameters beyond the choice of what to use as the
    subclassifiers, but the individual subclassifiers have the usual range of options.
    The following tables report the scores using just one label per subclassifier
    but varying whether or not neutral is allowed as a label:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **MULTI-LEX** | **MULTI-NB** | **MULTI-SVM** | **MULTI-DNN** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | 0.348 (0.868) | *0.441 (0.996) * | 0.385 (1.000) | 0.422 (0.991)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | 0.363 (0.878) | 0.376 (0.996) | 0.314 (0.997) | 0.333 (0.956)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | 0.260 (0.852) | * 0.296 (0.993) * | 0.256 (0.995) | 0.236
    (0.936) |'
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | 0.304 (0.979) | 0.236 (0.989) | 0.294 (0.996) | 0.182 (0.938)
    |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.15 (a) – 0 or more emotions per tweet, multiple classifiers, -neutral
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **MULTI-LEX** | **MULTI-NB** | **MULTI-SVM** | **MULTI-DNN** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | 0.342 (0.861) | 0.438 (0.996) | 0.381 (1.000) | 0.419 (0.991)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | 0.363 (0.879) | 0.376 (0.996) | 0.313 (0.997) | 0.333 (0.956)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | 0.256 (0.836) | 0.290 (0.993) | 0.250 (0.995) | 0.234 (0.938)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | 0.665 (0.984) | 0.546 (0.989) | 0.617 (0.996) | 0.599 (0.950)
    |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.15 (b) – 0 or more emotions per tweet, multiple classifiers, +neutral
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall picture here is that using multiple independent classifiers to
    decide whether a tweet should have a given label produces the best proportionality
    results yet for zero-to-many datasets. Although the Jaccard scores are only improved
    for SEM11-EN and SEM11-ES, there is considerable variation between the performance
    of the different classifiers under this regime. All four classifiers do marginally
    better on the SEM11 cases when we do not allow **neutral** as a label, but they
    all do substantially better on the KWT.M-AR dataset when we do allow **neutral**.
    This is slightly surprising, given that the individual classifiers are allowed
    to choose not to assign their labels, so it is perfectly possible to get a “no
    label assigned” outcome for a given tweet, even without allowing **neutral**.
    *Figure 10**.16* shows how scores vary as we look at the +/-neutral classifiers
    for KWT.M-AR:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    | **Proportionality** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-LEX, -****NEUTRAL** | 0.400 | 0.559 | 0.467 | 0.319 | 0.304 | 0.979
    |'
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-LEX, +****NEUTRAL** | 0.731 | 0.881 | 0.799 | 0.817 | 0.665 | 0.984
    |'
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-NB, -****NEUTRAL** | 0.338 | 0.441 | 0.383 | 0.247 | 0.236 | 0.989
    |'
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-NB, +****NEUTRAL** | 0.645 | 0.781 | 0.707 | 0.714 | 0.546 | 0.989
    |'
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-SVM,** **-****NEUTRAL** | 0.598 | 0.367 | 0.455 | 0.294 | 0.294 |
    0.996 |'
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-SVM, +****NEUTRAL** | 0.764 | 0.763 | 0.763 | 0.747 | 0.617 | 0.996
    |'
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-DNN, -****NEUTRAL** | 0.255 | 0.389 | 0.308 | 0.194 | 0.182 | 0.938
    |'
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-DNN, +****NEUTRAL** | 0.725 | 0.776 | 0.750 | 0.758 | 0.599 | 0.950
    |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.16 – KWT.M-AR, multiple classifiers, without and with neutral
  prefs: []
  type: TYPE_NORMAL
- en: In every case, both recall and precision go up when we allow neutral as a label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the confusion matrices for Naive Bayes (the others are very similar,
    but Naive Bayes gives the best overall results and hence is the most interesting)
    is revealing:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **--** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 31 | 7 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23 |'
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 0 | 111 | 0 | 0 | 15 | 0 | 0 | 0 | 0 | 94 |'
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 2 | 3 | 0 | 1 | 0 | 0 | 0 | 0 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 6 | 0 | 56 | 12 | 6 | 0 | 0 | 0 | 66 |'
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 2 | 0 | 4 | 327 | 4 | 0 | 0 | 0 | 155 |'
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 2 | 0 | 3 | 0 | 123 | 0 | 0 | 0 | 70 |'
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 1 | 2 | 0 | 0 | 1 | 0 | 2 | 0 | 0 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 0 | 4 | 0 | 1 | 2 | 0 | 0 | 0 | 26 | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| **--** | 65 | 329 | 5 | 182 | 527 | 318 | 6 | 3 | 67 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.17 (a) – Confusion matrix, multiclassifiers with NB as a subclassifier,
    KWT.M-AR, -neutral
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **neut** | **--** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 35 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 0 | 118 | 0 | 0 | 12 | 0 | 0 | 0 | 0 | 92 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 1 | 3 | 0 | 1 | 0 | 0 | 0 | 0 | 13 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 3 | 0 | 58 | 12 | 3 | 0 | 0 | 0 | 67 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 2 | 0 | 2 | 343 | 2 | 0 | 0 | 1 | 138 | 11 |'
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 1 | 0 | 1 | 0 | 126 | 0 | 0 | 0 | 67 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 2 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 18 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 7 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 0 | 2 | 0 | 0 | 1 | 0 | 0 | 0 | 27 | 54 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **neutra** | 2 | 14 | 0 | 15 | 162 | 64 | 0 | 0 | 5 | 3521 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **--** | 60 | 333 | 5 | 165 | 398 | 273 | 6 | 3 | 63 | 378 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.17 (b) – Confusion matrix, multiclassifiers with NB as a subclassifier,
    KWT.M-AR, -neutral
  prefs: []
  type: TYPE_NORMAL
- en: There are small differences between the scores in the main sections of the two
    tables – slightly better scores on the diagonal and slightly lower confusion between
    other labels – but the crucial difference is that, as before, when we do not have
    **neutral** as a label, we get a large number of false positives. Using **neutral**
    as a label reduces the number of false positives, even when we have multiple independent
    classifiers that each make their recommendations without looking at the results
    of the other classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Over the past few chapters, we have looked at a wide range of classifiers and
    compared their performance on a range of datasets. Now, it is time to reflect
    on what we have learned. Our final table of the best classifiers for the datasets
    we have looked at is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **SVM** | **SNN** | **Transformers** | **MULTI-NB** | **LEX, MULTI** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-EN** | 0.845 | 0.829 | * 0.927 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | 0.224 | 0.242 | 0.418 | * 0.438 | 0.347 |'
  prefs: []
  type: TYPE_TB
- en: '| **WASSA-EN** | * 0.770 | 0.737 | 0.753 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **CARER-EN** | 0.770 | * 0.820 | 0.816 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **IMDB-EN** | 0.736 | 0.793 | * 0.826 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-AR** | 0.514 | 0.504 | * 0.710 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | 0.216 | 0.221 | 0.359 | * 0.412 | 0.377 |'
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | 0.631 | 0.028 | 0.053 | 0.537 | * 0.691 |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-ES** | 0.412 | 0.337 | * 0.663 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | 0.226 | 0.221 | * 0.340 | 0.294 | 0.266 |'
  prefs: []
  type: TYPE_TB
- en: Figure 10.18 – Overall best classifiers
  prefs: []
  type: TYPE_NORMAL
- en: 'Given what we have seen, there are several general observations we can make:'
  prefs: []
  type: TYPE_NORMAL
- en: The difference in performance of the various algorithms is not huge. For each
    dataset and each configuration of settings, the performances of the best classifiers
    are very similar, so it may be sensible to take training time as well as scores
    on the various measures into account when choosing a classifier. In particular,
    no classifier is the best in every case, and sometimes, the very simple algorithms
    (LEX and Naive Bayes) are as good as or even better than the more complex ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Datasets where an individual tweet can be assigned zero, one, or more labels
    are considerably more challenging than ones where each tweet is given exactly
    one label. There is, in fact, a clear upper bound to the performance of classifiers
    that do assign exactly one label per tweet to these datasets, and the best results
    are obtained by reconsidering the way that the classifiers are used. Some classifiers
    are more suitable than others for this kind of task, and this must be taken into
    account when you’re choosing a classifier for datasets of this kind. Again, it
    is worth considering training time when choosing this: training a single classifier
    and then setting *N* individual thresholds is considerably quicker than training
    *N* classifiers, and in at least some cases, the difference in performance is
    quite small.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also investigated a variety of preprocessing steps, including the use of
    different tokenizers and stemmers, and we looked at using algorithms that can
    suggest “similar” words to replace words in the target tweets that do not appear
    in the training data. All these tweaks pay off in some situations and not in others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We cannot overstate this: **there is no silver bullet**. Different tasks require
    different classifiers, and you should always investigate a range of classifiers
    before deciding which one you want to use. In particular, if you are working with
    a multi-label dataset, you should consider one of the algorithms from this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: When training a classifier, it is a good idea to look at the confusion matrix
    for the labels that it assigns. Several classifiers, particularly for the datasets
    with large numbers of zero assignments, produce quite good F1 and Jaccard scores
    simply by choosing the most common class (that is, **neutral**!) in every case.
    And when choosing between classifiers, it is a good idea to consider the end task
    that the classifier is needed for. If what you want is to get a feel for an opinion
    on some topic, without worrying too much about what individual tweets say about
    it, then using proportionality as a metric can be a helpful tool. We will use
    this in the next chapter, where we will look at the link between emotions expressed
    in tweets and real-life events over a certain period.
  prefs: []
  type: TYPE_NORMAL
- en: Part 4:Case Study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Part 3* discussed a range of approaches to the task of EA and compared their
    effectiveness on a set of standard datasets. In this final part, we investigate
    how well these approaches work on real-world data that is not connected to the
    standard sets, looking at how changes in the emotions expressed in tweets reflect
    key real-world events. We also examine how robust the various approaches are when
    applied to novel data, showing how approaches that work well when the test data
    is drawn from the same population as the training data can be fragile when applied
    to novel data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B18714_11.xhtml#_idTextAnchor202), *Case Study – The Qatar Blockade*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
