- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Multiclassifiers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šåˆ†ç±»å™¨
- en: In the preceding chapters, we saw that multi-label datasets, where a tweet may
    have zero, one, or more labels, are considerably harder to deal with than simple
    multi-class datasets where each tweet has exactly one label, albeit drawn from
    a set of more than one option. In this chapter, we will investigate ways of dealing
    with these cases, looking in particular at the use of **neutral** as a label for
    handling cases where a tweet is allowed to have zero labels; at using varying
    thresholds to enable standard classifiers to return a variable number of labels;
    and at training multiple classifiers, one per label, and allowing them each to
    make a decision about the label they were trained for. The conclusion, as ever,
    will be that there is no single â€œsilver bulletâ€ that provides the best solution
    in every case, but in general, the use of multiple classifiers tends to be better
    than the other approaches.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¤šæ ‡ç­¾æ•°æ®é›†ï¼Œå…¶ä¸­ä¸€æ¡æ¨æ–‡å¯èƒ½æœ‰é›¶ä¸ªã€ä¸€ä¸ªæˆ–å¤šä¸ªæ ‡ç­¾ï¼Œä¸æ¯ä¸ªæ¨æ–‡æ°å¥½æœ‰ä¸€ä¸ªæ ‡ç­¾çš„ç®€å•å¤šç±»æ•°æ®é›†ç›¸æ¯”ï¼Œå¤„ç†èµ·æ¥è¦å›°éš¾å¾—å¤šï¼Œå°½ç®¡è¿™äº›æ ‡ç­¾æ¥è‡ªä¸€ä¸ªåŒ…å«å¤šä¸ªé€‰é¡¹çš„é›†åˆã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¤„ç†è¿™äº›æƒ…å†µçš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æ¢è®¨ä½¿ç”¨**ä¸­æ€§**æ ‡ç­¾æ¥å¤„ç†å…è®¸æ¨æ–‡æœ‰é›¶ä¸ªæ ‡ç­¾çš„æƒ…å†µï¼›ä½¿ç”¨ä¸åŒçš„é˜ˆå€¼æ¥ä½¿æ ‡å‡†åˆ†ç±»å™¨è¿”å›å¯å˜æ•°é‡çš„æ ‡ç­¾ï¼›ä»¥åŠè®­ç»ƒå¤šä¸ªåˆ†ç±»å™¨ï¼Œæ¯ä¸ªæ ‡ç­¾ä¸€ä¸ªï¼Œå¹¶å…è®¸å®ƒä»¬å„è‡ªå¯¹å…¶è®­ç»ƒçš„æ ‡ç­¾åšå‡ºå†³å®šã€‚ç»“è®ºï¼Œä¸€å¦‚æ—¢å¾€ï¼Œå°†æ˜¯æ²¡æœ‰å•ä¸€çš„â€œé“¶å¼¹â€å¯ä»¥åœ¨æ¯ç§æƒ…å†µä¸‹æä¾›æœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œä½†æ€»çš„æ¥è¯´ï¼Œä½¿ç”¨å¤šä¸ªåˆ†ç±»å™¨å¾€å¾€æ¯”å…¶ä»–æ–¹æ³•æ›´å¥½ã€‚
- en: 'In this chapter, weâ€™ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« æˆ‘ä»¬å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Using confusion matrices to analyze the behavior of classifiers on complex data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ··æ·†çŸ©é˜µæ¥åˆ†æå¤æ‚æ•°æ®ä¸Šåˆ†ç±»å™¨çš„è¡Œä¸º
- en: Using **neutral** as a label to deal with tweets that have no label assigned
    to them
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨**ä¸­æ€§**æ ‡ç­¾æ¥å¤„ç†æœªåˆ†é…æ ‡ç­¾çš„æ¨æ–‡
- en: Varying thresholds to handle multi-label datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸åŒçš„é˜ˆå€¼æ¥å¤„ç†å¤šæ ‡ç­¾æ•°æ®é›†
- en: Training multiple classifiers to handle multi-label datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¤šä¸ªåˆ†ç±»å™¨æ¥å¤„ç†å¤šæ ‡ç­¾æ•°æ®é›†
- en: By the end of this chapter, you will understand how to implement several strategies
    for dealing with muti-label datasets and will have an appreciation of the effectiveness
    of these strategies for different kinds of data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ç»“æŸæ—¶ï¼Œæ‚¨å°†äº†è§£å¦‚ä½•å®æ–½å‡ ç§å¤„ç†å¤šæ ‡ç­¾æ•°æ®é›†çš„ç­–ç•¥ï¼Œå¹¶ä¼šå¯¹è¿™äº›ç­–ç•¥å¯¹ä¸åŒç±»å‹æ•°æ®çš„æœ‰æ•ˆæ€§æœ‰æ‰€è®¤è¯†ã€‚
- en: Multilabel datasets are hard to work with
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šæ ‡ç­¾æ•°æ®é›†éš¾ä»¥å¤„ç†
- en: 'We will start by looking at the performance of a selection of classifiers from
    previous chapters on the main datasets. We have said several times that multi-label
    datasets are particularly challenging, but it is worth bringing together the results
    from the best-performing algorithms to see exactly how challenging they are. *Figure
    10**.1* includes all the major classifiers that we have looked at so far. The
    multi-label datasets are highlighted in gray, and the best-performing classifier
    for each row is marked in bold/asterisks:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»æŸ¥çœ‹å‰å‡ ç« ä¸­é€‰æ‹©çš„å‡ ä¸ªåˆ†ç±»å™¨åœ¨ä¸»è¦æ•°æ®é›†ä¸Šçš„æ€§èƒ½å¼€å§‹ã€‚æˆ‘ä»¬æ›¾å¤šæ¬¡æåˆ°å¤šæ ‡ç­¾æ•°æ®é›†ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½†å°†è¡¨ç°æœ€å¥½çš„ç®—æ³•çš„ç»“æœæ±‡é›†åœ¨ä¸€èµ·ï¼Œå¯ä»¥çœ‹åˆ°å®ƒä»¬ç©¶ç«Ÿæœ‰å¤šå…·æŒ‘æˆ˜æ€§ã€‚*å›¾10.1*åŒ…æ‹¬äº†è¿„ä»Šä¸ºæ­¢æˆ‘ä»¬æŸ¥çœ‹çš„æ‰€æœ‰ä¸»è¦åˆ†ç±»å™¨ã€‚å¤šæ ‡ç­¾æ•°æ®é›†ä»¥ç°è‰²çªå‡ºæ˜¾ç¤ºï¼Œæ¯è¡Œçš„æœ€ä½³æ€§èƒ½åˆ†ç±»å™¨ä»¥ç²—ä½“/æ˜Ÿå·æ ‡è®°ï¼š
- en: '|  | **LEX** | **CP** | **NB** | **SVM** | **SNN** | **DNN** | **Transformers**
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '|  | **LEX** | **CP** | **NB** | **SVM** | **SNN** | **DNN** | **Transformers**
    |'
- en: '| **SEM4-EN** | 0.497 | 0.593 | 0.775 | 0.845 | 0.829 | 0.847 | ***** **0.927
    *** |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| **SEM4-EN** | 0.497 | 0.593 | 0.775 | 0.845 | 0.829 | 0.847 | ***** **0.927
    *** |'
- en: '| **SEM11-EN** | 0.348 | 0.353 | 0.227 | 0.224 | 0.242 | 0.246 | ***** **0.418
    *** |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **SEM11-EN** | 0.348 | 0.353 | 0.227 | 0.224 | 0.242 | 0.246 | ***** **0.418
    *** |'
- en: '| **WASSA-EN** | 0.437 | 0.505 | 0.709 | ***** **0.770 *** | 0.737 | 0.752
    | 0.753 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| **WASSA-EN** | 0.437 | 0.505 | 0.709 | ***** **0.770 *** | 0.737 | 0.752
    | 0.753 |'
- en: '| **CARER-EN** | 0.350 | 0.395 | 0.776 | 0.770 | ***** **0.820*** | 0.804 |
    0.816 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| **CARER-EN** | 0.350 | 0.395 | 0.776 | 0.770 | ***** **0.820*** | 0.804 |
    0.816 |'
- en: '| **IMDB-EN** | 0.667 | 0.722 | 0.738 | 0.736 | 0.793 | 0.793 | ***** **0.826
    *** |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| **IMDB-EN** | 0.667 | 0.722 | 0.738 | 0.736 | 0.793 | 0.793 | ***** **0.826
    *** |'
- en: '| **SEM4-AR** | 0.509 | 0.513 | 0.531 | 0.514 | 0.504 | 0.444 | ***** **0.710
    *** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **SEM4-AR** | 0.509 | 0.513 | 0.531 | 0.514 | 0.504 | 0.444 | ***** **0.710
    *** |'
- en: '| **SEM11-AR** | ***** **0.386 *** | 0.382 | 0.236 | 0.216 | 0.221 | 0.207
    | 0.359 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| **SEM11-AR** | ***** **0.386 *** | 0.382 | 0.236 | 0.216 | 0.221 | 0.207
    | 0.359 |'
- en: '| **KWT.M-AR** | 0.663 | ***** **0.666 *** | 0.494 | 0.631 | 0.028 | 0.026
    | 0.053 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **KWT.M-AR** | 0.663 | ***** **0.666 *** | 0.494 | 0.631 | 0.028 | 0.026
    | 0.053 |'
- en: '| **SEM4-ES** | 0.420 | 0.177 | 0.360 | 0.412 | 0.337 | 0.343 | ***** **0.663
    *** |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **SEM4-ES** | 0.420 | 0.177 | 0.360 | 0.412 | 0.337 | 0.343 | ***** **0.663
    *** |'
- en: '| **SEM11-ES** | 0.271 | 0.278 | 0.230 | 0.226 | 0.221 | 0.222 | ***** **0.340
    *** |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **SEM11-ES** | 0.271 | 0.278 | 0.230 | 0.226 | 0.221 | 0.222 | ***** **0.340
    *** |'
- en: Figure 10.1 â€“ Selected Jaccard scores for the standard datasets (multi-label
    datasets in gray)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.1 â€“ æ ‡å‡†æ•°æ®é›†é€‰æ‹©çš„Jaccardåˆ†æ•°ï¼ˆå¤šæ ‡ç­¾æ•°æ®é›†ä»¥ç°è‰²æ˜¾ç¤ºï¼‰
- en: 'Two things stand out from this table:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™å¼ è¡¨ä¸­ï¼Œæœ‰ä¸¤ç‚¹ç‰¹åˆ«çªå‡ºï¼š
- en: For most of the entries in this table, LEX is the worst classifier, with NB
    coming next, and then the others generally scoring fairly similarly. For the multi-label
    cases, however, LEX or CP are always better than anything else except transformers,
    and in a couple of cases, they are better than transformers as well. Given that
    these seem to be the most realistic datasets, since plenty of tweets express no
    emotion and a fair number express more than one, it is worth looking in more detail
    at what is going on in these cases.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè¡¨æ ¼çš„å¤§éƒ¨åˆ†æ¡ç›®ä¸­ï¼ŒLEX æ˜¯æœ€å·®çš„åˆ†ç±»å™¨ï¼Œå…¶æ¬¡æ˜¯ NBï¼Œç„¶åå…¶ä»–åˆ†ç±»å™¨çš„å¾—åˆ†é€šå¸¸ç›¸å½“ç›¸ä¼¼ã€‚ç„¶è€Œï¼Œå¯¹äºå¤šæ ‡ç­¾æƒ…å†µï¼ŒLEX æˆ– CP æ€»æ˜¯ä¼˜äºé™¤å˜å‹å™¨ä»¥å¤–çš„ä»»ä½•å…¶ä»–åˆ†ç±»å™¨ï¼Œè€Œä¸”åœ¨å‡ ä¸ªæƒ…å†µä¸‹ï¼Œå®ƒä»¬ç”šè‡³ä¼˜äºå˜å‹å™¨ã€‚é‰´äºè¿™äº›æ•°æ®é›†ä¼¼ä¹æ˜¯æœ€ç°å®çš„ï¼Œå› ä¸ºè®¸å¤šæ¨æ–‡æ²¡æœ‰è¡¨è¾¾æƒ…æ„Ÿï¼Œç›¸å½“ä¸€éƒ¨åˆ†æ¨æ–‡è¡¨è¾¾äº†å¤šä¸ªæƒ…æ„Ÿï¼Œå› æ­¤å€¼å¾—æ›´è¯¦ç»†åœ°ç ”ç©¶è¿™äº›æƒ…å†µä¸­å‘ç”Ÿçš„äº‹æƒ…ã€‚
- en: The multi-label cases also score significantly worse overall â€“ while LEX and
    CP score better than most other classifiers on these cases, they do generally
    score worse on them than on the other cases, and for all the other classifiers,
    the gap between these cases and the one emotion/tweet cases is substantial.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šæ ‡ç­¾æƒ…å†µçš„æ•´ä½“å¾—åˆ†ä¹Ÿæ˜¾è‘—æ›´å·® â€“ è™½ç„¶ LEX å’Œ CP åœ¨è¿™äº›æƒ…å†µä¸‹çš„è¡¨ç°ä¼˜äºå¤§å¤šæ•°å…¶ä»–åˆ†ç±»å™¨ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨è¿™äº›æƒ…å†µä¸‹çš„å¾—åˆ†ä½äºå…¶ä»–æƒ…å†µï¼Œè€Œå¯¹äºæ‰€æœ‰å…¶ä»–åˆ†ç±»å™¨ï¼Œè¿™äº›æƒ…å†µä¸å•ä¸€æƒ…æ„Ÿ/æ¨æ–‡æƒ…å†µä¹‹é—´çš„å·®è·æ˜¯æ˜¾è‘—çš„ã€‚
- en: These cases seem likely to be the most useful in practice since most tweets
    do not express any sentiment and a fair number express more than one, so algorithms
    that do not deal well with these cases may not be the most suitable for this task.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æƒ…å†µåœ¨å®è·µä¸­ä¼¼ä¹æœ€æœ‰ç”¨ï¼Œå› ä¸ºå¤§å¤šæ•°æ¨æ–‡éƒ½æ²¡æœ‰è¡¨è¾¾ä»»ä½•æƒ…æ„Ÿï¼Œç›¸å½“ä¸€éƒ¨åˆ†æ¨æ–‡è¡¨è¾¾äº†å¤šä¸ªæƒ…æ„Ÿï¼Œæ‰€ä»¥å¤„ç†è¿™äº›æƒ…å†µä¸ä½³çš„ç®—æ³•å¯èƒ½ä¸é€‚åˆè¿™é¡¹ä»»åŠ¡ã€‚
- en: In the *Confusion matrices* section, we will look at what the various algorithms
    do with the two kinds of datasets. Once we have a clearer idea of why multi-label
    datasets are so much more difficult to handle than single-label ones, and we have
    seen the specific problems that they cause for particular algorithms, we will
    look at ways of dealing with this kind of dataset. We will not carry out these
    experiments with transformer-based models, partly because the time it takes to
    train a transformer makes this infeasible, but more importantly because we need
    to look inside the models to understand what is going on â€“ this is all but impossible
    with transformer-based models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**æ··æ·†çŸ©é˜µ**éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹å„ç§ç®—æ³•å¯¹è¿™ä¸¤ç§æ•°æ®é›†çš„å¤„ç†æ–¹å¼ã€‚ä¸€æ—¦æˆ‘ä»¬æ›´æ¸…æ¥šåœ°äº†è§£ä¸ºä»€ä¹ˆå¤šæ ‡ç­¾æ•°æ®é›†æ¯”å•æ ‡ç­¾æ•°æ®é›†æ›´éš¾å¤„ç†ï¼Œå¹¶ä¸”æˆ‘ä»¬å·²ç»çœ‹åˆ°å®ƒä»¬å¯¹ç‰¹å®šç®—æ³•é€ æˆçš„å…·ä½“é—®é¢˜ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¤„ç†è¿™ç±»æ•°æ®é›†çš„æ–¹æ³•ã€‚æˆ‘ä»¬ä¸ä¼šä½¿ç”¨åŸºäºå˜å‹å™¨çš„æ¨¡å‹è¿›è¡Œè¿™äº›å®éªŒï¼Œéƒ¨åˆ†åŸå› æ˜¯è®­ç»ƒå˜å‹å™¨çš„è€—æ—¶ä½¿å¾—è¿™ä¸å¯è¡Œï¼Œä½†æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦æ·±å…¥äº†è§£æ¨¡å‹ä»¥äº†è§£å…¶å·¥ä½œåŸç†
    â€“ è¿™åœ¨åŸºäºå˜å‹å™¨çš„æ¨¡å‹ä¸­å‡ ä¹æ˜¯ä¸å¯èƒ½çš„ã€‚
- en: Confusion matrices
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ··æ·†çŸ©é˜µ
- en: 'It can be very difficult to see what kinds of mistakes a classifier makes just
    by looking at the raw output. **Confusion matrices** allow us to visualize a classifierâ€™s
    behavior, making it possible to see when two classes are being systematically
    confused or when a given class is being assigned too few or too many items. Consider
    the following dataset, where each item is classified as A, B, or C by the Gold
    Standard (G) and also has a predicted value (P):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…é€šè¿‡æŸ¥çœ‹åŸå§‹è¾“å‡ºï¼Œå¾ˆéš¾çœ‹å‡ºåˆ†ç±»å™¨çŠ¯äº†ä»€ä¹ˆé”™è¯¯ã€‚**æ··æ·†çŸ©é˜µ**ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¯è§†åŒ–åˆ†ç±»å™¨çš„è¡Œä¸ºï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿçœ‹åˆ°ä¸¤ä¸ªç±»åˆ«æ˜¯å¦è¢«ç³»ç»Ÿåœ°æ··æ·†ï¼Œæˆ–è€…æŸä¸ªç±»åˆ«æ˜¯å¦è¢«åˆ†é…äº†å¤ªå¤šæˆ–å¤ªå°‘çš„é¡¹ç›®ã€‚è€ƒè™‘ä»¥ä¸‹æ•°æ®é›†ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®ç”±é»„é‡‘æ ‡å‡†ï¼ˆGï¼‰åˆ†ç±»ä¸º
    Aã€B æˆ– Cï¼Œå¹¶ä¸”è¿˜æœ‰ä¸€ä¸ªé¢„æµ‹å€¼ï¼ˆPï¼‰ï¼š
- en: '| G | C | C | A | B | C | B | C | B | B | B | A | A | B | B | C | C | B | C
    | B | B | C | A | B | A | A | C | C | C | A | A | A | C | B | C | A | A | B |
    A |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| G | C | C | A | B | C | B | C | B | B | B | A | A | B | B | C | C | B | C
    | B | B | C | A | B | A | A | C | C | C | A | A | A | C | B | C | A | A | B |
    A |'
- en: '| P | C | B | B | B | C | A | A | B | B | A | A | A | C | B | A | B | B | C
    | B | C | C | A | B | B | B | C | B | B | B | A | B | C | B | B | A | A | B |
    A |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| P | C | B | B | B | C | A | A | B | B | A | A | A | C | B | A | B | B | C
    | B | C | C | A | B | B | B | C | B | B | B | A | B | C | B | B | A | A | B |
    A |'
- en: Figure 10.2 â€“ Gold Standard and predicted values for the example data
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.2 â€“ ç¤ºä¾‹æ•°æ®çš„é»„é‡‘æ ‡å‡†å’Œé¢„æµ‹å€¼
- en: 'It is hard to see any pattern in this table. Simply counting how many cases
    have the same value for G and P gives us 22 out of 38 â€“ that is, an accuracy of
    0.58 â€“ but it is very hard to see what kinds of things it gets right and what
    kinds of things it gets wrong. Converting this into a confusion table can help
    with this. We do this by counting the number of times that an item that ought
    to be assigned C1 as its value is predicted to have C2, producing a table of correct
    versus predicted assignments. The confusion matrix in *Figure 10**.3*, for instance,
    shows that seven things that should have been assigned the label A were indeed
    assigned that label but five were assigned B, and that six things that should
    have been assigned C were assigned C but five were assigned B. This suggests that
    there is something about the properties of Bs that makes it easy to assign things
    to this class when they should be assigned to A or C, which might lead to a line
    of inquiry about which properties of Bs were leading to this problem:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè¡¨æ ¼ä¸­å¾ˆéš¾çœ‹å‡ºä»»ä½•æ¨¡å¼ã€‚ç®€å•åœ°è®¡ç®— G å’Œ P å€¼ç›¸åŒçš„æ¡ˆä¾‹æ•°é‡ï¼Œæˆ‘ä»¬å¾—åˆ° 22 ä¸ªæ¡ˆä¾‹ä¸­çš„ 38 ä¸ª â€“ å³ï¼Œå‡†ç¡®ç‡ä¸º 0.58 â€“ ä½†å¾ˆéš¾çœ‹å‡ºå®ƒåšå¯¹äº†ä»€ä¹ˆï¼Œåšé”™äº†ä»€ä¹ˆã€‚å°†æ­¤è½¬æ¢ä¸ºæ··æ·†è¡¨å¯ä»¥å¸®åŠ©è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡è®¡ç®—åº”è¯¥åˆ†é…
    C1 ä½œä¸ºå…¶å€¼çš„é¡¹è¢«é¢„æµ‹ä¸ºå…·æœ‰ C2 çš„æ¬¡æ•°æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ªæ­£ç¡®ä¸é¢„æµ‹åˆ†é…çš„è¡¨æ ¼ã€‚ä¾‹å¦‚ï¼Œå›¾ 10.3 ä¸­çš„æ··æ·†çŸ©é˜µæ˜¾ç¤ºï¼Œä¸ƒä¸ªæœ¬åº”è¢«åˆ†é…æ ‡ç­¾ A çš„é¡¹ç¡®å®è¢«åˆ†é…äº†è¯¥æ ‡ç­¾ï¼Œä½†äº”ä¸ªè¢«åˆ†é…äº†
    Bï¼Œè€Œå…­ä¸ªæœ¬åº”è¢«åˆ†é… C çš„é¡¹è¢«åˆ†é…äº† C ä½†äº”ä¸ªè¢«åˆ†é…äº† Bã€‚è¿™è¡¨æ˜ B çš„æŸäº›å±æ€§ä½¿å¾—åœ¨å®ƒä»¬åº”è¯¥è¢«åˆ†é…åˆ° A æˆ– C æ—¶å®¹æ˜“å°†å®ƒä»¬åˆ†é…åˆ°è¿™ä¸ªç±»åˆ«ï¼Œè¿™å¯èƒ½å¯¼è‡´å¯¹å¯¼è‡´æ­¤é—®é¢˜çš„
    B çš„å“ªäº›å±æ€§è¿›è¡Œè°ƒæŸ¥çš„æ¢ç©¶ï¼š
- en: '|  | **A** | **B** | **C** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | **A** | **B** | **C** |'
- en: '| **A** | 7 | 5 | 0 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **A** | 7 | 5 | 0 |'
- en: '| **B** | 2 | 9 | 2 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **B** | 2 | 9 | 2 |'
- en: '| **C** | 2 | 5 | 6 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **C** | 2 | 5 | 6 |'
- en: Figure 10.3 â€“ Confusion matrix for the data in Figure 10.2
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10.3 â€“ å›¾ 10.2 ä¸­æ•°æ®çš„æ··æ·†çŸ©é˜µ
- en: 'If `gs` and `p` are the Gold Standard values for a set of points, then `confusion`
    will calculate the confusion matrix: `c` is a table with an entry for each label
    in `gs`, where the value for a label is the set of counts of each time a label
    has been predicted for it:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ `gs` å’Œ `p` æ˜¯ä¸€ç»„ç‚¹çš„é»„é‡‘æ ‡å‡†å€¼ï¼Œé‚£ä¹ˆ `confusion` å°†è®¡ç®—æ··æ·†çŸ©é˜µï¼š`c` æ˜¯ä¸€ä¸ªè¡¨æ ¼ï¼Œå…¶ä¸­ä¸º `gs` ä¸­çš„æ¯ä¸ªæ ‡ç­¾éƒ½æœ‰ä¸€ä¸ªæ¡ç›®ï¼Œè¯¥æ ‡ç­¾çš„å€¼æ˜¯é¢„æµ‹ä¸ºè¯¥æ ‡ç­¾çš„æ¬¡æ•°é›†åˆï¼š
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Confusion matrices can provide a considerable amount of information about what
    a classifier is doing. We do, however, have a slight problem with constructing
    confusion matrices when the Gold Standard and the prediction can each contain
    a varying number of emotions. Suppose, for instance, that the Gold Standard for
    some tweets is **love+joy** and the prediction is **love+sad+angry**. We want
    to acknowledge that the classifier was right when it predicted **love**, but what
    do we do about the fact that it missed **joy** (that is, there is a false negative)
    and predicted **sad** and **angry** (two false positives)?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ··æ·†çŸ©é˜µå¯ä»¥æä¾›å¤§é‡å…³äºåˆ†ç±»å™¨æ‰€åšäº‹æƒ…çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œå½“é»„é‡‘æ ‡å‡†å’Œé¢„æµ‹å¯ä»¥åŒ…å«ä¸åŒæ•°é‡çš„æƒ…ç»ªæ—¶ï¼Œæ„å»ºæ··æ·†çŸ©é˜µä¼šæœ‰ä¸€äº›å°é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æŸäº›æ¨æ–‡çš„é»„é‡‘æ ‡å‡†æ˜¯**çˆ±+å–œæ‚¦**ï¼Œè€Œé¢„æµ‹æ˜¯**çˆ±+æ‚²ä¼¤+æ„¤æ€’**ã€‚æˆ‘ä»¬æƒ³è¦æ‰¿è®¤å½“åˆ†ç±»å™¨é¢„æµ‹**çˆ±**æ—¶æ˜¯æ­£ç¡®çš„ï¼Œä½†å…³äºå®ƒé—æ¼äº†**å–œæ‚¦**ï¼ˆå³å­˜åœ¨ä¸€ä¸ªå‡é˜´æ€§ï¼‰ä»¥åŠé¢„æµ‹äº†**æ‚²ä¼¤**å’Œ**æ„¤æ€’**ï¼ˆä¸¤ä¸ªå‡é˜³æ€§ï¼‰çš„äº‹å®æˆ‘ä»¬è¯¥å¦‚ä½•å¤„ç†ï¼Ÿ
- en: 'There is no right answer to this question. We adapt the standard way of constructing
    a confusion matrix as follows, where C[e1][e2] is the score for *e1* in the Gold
    Standard and *e2* in the prediction. We need to add a row and a column for â€œno
    emotion assignedâ€ (we will use **--** for this class):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªé—®é¢˜æ²¡æœ‰æ­£ç¡®ç­”æ¡ˆã€‚æˆ‘ä»¬æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è°ƒæ•´æ„å»ºæ··æ·†çŸ©é˜µçš„æ ‡å‡†æ–¹æ³•ï¼Œå…¶ä¸­ C[e1][e2] æ˜¯é»„é‡‘æ ‡å‡†ä¸­ *e1* å’Œé¢„æµ‹ä¸­ *e2* çš„å¾—åˆ†ã€‚æˆ‘ä»¬éœ€è¦ä¸ºâ€œæœªåˆ†é…æƒ…ç»ªâ€æ·»åŠ ä¸€è¡Œå’Œä¸€åˆ—ï¼ˆæˆ‘ä»¬å°†ä½¿ç”¨
    **--** è¡¨ç¤ºæ­¤ç±»ç±»åˆ«ï¼‰ï¼š
- en: For every case where the Golden Standard and the prediction contain a given
    emotion, *e*, add 1 to *C[e][e]* and remove *e* from both the Golden Standard
    and the prediction.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºé»„é‡‘æ ‡å‡†å’Œé¢„æµ‹åŒ…å«ç»™å®šæƒ…ç»ª *e* çš„æ¯ä¸ªæƒ…å†µï¼Œå°† 1 æ·»åŠ åˆ° *C[e][e]*ï¼Œå¹¶ä»é»„é‡‘æ ‡å‡†å’Œé¢„æµ‹ä¸­åˆ é™¤ *e*ã€‚
- en: If the Golden Standard is now empty, then every *e* left in the prediction must
    be a false positive, so add 1 to *C[--][e]* for each remaining e.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœé»„é‡‘æ ‡å‡†ç°åœ¨ä¸ºç©ºï¼Œé‚£ä¹ˆé¢„æµ‹ä¸­å‰©ä½™çš„æ¯ä¸ª *e* å¿…é¡»æ˜¯ä¸€ä¸ªå‡é˜³æ€§ï¼Œå› æ­¤å¯¹äºæ¯ä¸ªå‰©ä½™çš„ *e*ï¼Œå°† 1 æ·»åŠ åˆ° *C[--][e]*ã€‚
- en: If the prediction is empty, then every *e* left in the Golden Standard must
    be a false negative, so add 1 to *C[e][--]*.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœé¢„æµ‹ä¸ºç©ºï¼Œé‚£ä¹ˆé»„é‡‘æ ‡å‡†ä¸­å‰©ä½™çš„æ¯ä¸ª *e* å¿…é¡»æ˜¯ä¸€ä¸ªå‡é˜´æ€§ï¼Œå› æ­¤å°† 1 æ·»åŠ åˆ° *C[e][--]*ã€‚
- en: If neither of them is empty after removing the shared cases, it is hard to see
    what to do. Consider the preceding example. After removing **love**, we are left
    with **joy** in the Golden Standard and **sad+angry** in the prediction. Is **joy**
    a mistake for **sad**, with **angry** as a false positive? Is **joy** a mistake
    for **angry**, with **sad** as a false positive? Is **joy** a false negative and
    **sad** and **angry** both false positives? This last suggestion does not seem
    right. Suppose we had one case where **joy** was matched with **sad+angry**, another
    where it was matched with **sad+fear**, and another where it was matched with
    just **sad**. If we marked all of these as cases where **joy** was a false negative
    and **sad** was a false positive, we would miss the fact that there appears to
    be a connection between **joy** and **sad**.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœåœ¨ç§»é™¤å…±äº«æ¡ˆä¾‹åä¸¤è€…éƒ½ä¸ä¸ºç©ºï¼Œå¾ˆéš¾çœ‹å‡ºè¦åšä»€ä¹ˆã€‚è€ƒè™‘å‰é¢çš„ä¾‹å­ã€‚åœ¨ç§»é™¤**çˆ±**ä¹‹åï¼Œæˆ‘ä»¬åœ¨é‡‘æ ‡å‡†ä¸­å‰©ä¸‹**å¿«ä¹**ï¼Œåœ¨é¢„æµ‹ä¸­å‰©ä¸‹**æ‚²ä¼¤+æ„¤æ€’**ã€‚**å¿«ä¹**æ˜¯**æ‚²ä¼¤**çš„é”™è¯¯ï¼Œè€Œ**æ„¤æ€’**æ˜¯å‡é˜³æ€§å—ï¼Ÿ**å¿«ä¹**æ˜¯**æ„¤æ€’**çš„é”™è¯¯ï¼Œè€Œ**æ‚²ä¼¤**æ˜¯å‡é˜³æ€§å—ï¼Ÿ**å¿«ä¹**æ˜¯å‡é˜´æ€§ï¼Œè€Œ**æ‚²ä¼¤**å’Œ**æ„¤æ€’**éƒ½æ˜¯å‡é˜³æ€§å—ï¼Ÿæœ€åä¸€ä¸ªå»ºè®®ä¼¼ä¹ä¸æ­£ç¡®ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ¡ˆä¾‹ï¼Œå…¶ä¸­**å¿«ä¹**ä¸**æ‚²ä¼¤+æ„¤æ€’**ç›¸åŒ¹é…ï¼Œå¦ä¸€ä¸ªæ¡ˆä¾‹ä¸­å®ƒä¸**æ‚²ä¼¤+ææƒ§**ç›¸åŒ¹é…ï¼Œè¿˜æœ‰ä¸€ä¸ªæ¡ˆä¾‹ä¸­å®ƒä¸**æ‚²ä¼¤**ç›¸åŒ¹é…ã€‚å¦‚æœæˆ‘ä»¬å°†è¿™äº›æƒ…å†µéƒ½æ ‡è®°ä¸º**å¿«ä¹**æ˜¯å‡é˜´æ€§è€Œ**æ‚²ä¼¤**æ˜¯å‡é˜³æ€§çš„æ¡ˆä¾‹ï¼Œæˆ‘ä»¬å°±ä¼šé”™è¿‡**å¿«ä¹**å’Œ**æ‚²ä¼¤**ä¹‹é—´ä¼¼ä¹å­˜åœ¨è”ç³»çš„äº‹å®ã€‚
- en: We deal with this as follows. Suppose there are **G** items left in the Gold
    Standard and **P** items left in the prediction after the labels that appear on
    both have been removed. Here, for each **g** in the Gold Standard and each **p**
    in the prediction, we add *1/P* to *C[p][g]*. Doing this adds a total of **G**
    to the confusion matrix, thus acknowledging that the number of emotions in the
    Gold Standard has not been matched, with each item in the prediction seen being
    as equally likely to be the one that should be substituted for **g**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ˜¯è¿™æ ·å¤„ç†çš„ã€‚å‡è®¾åœ¨ç§»é™¤äº†ä¸¤è€…éƒ½å‡ºç°çš„æ ‡ç­¾ä¹‹åï¼Œé‡‘æ ‡å‡†ä¸­å‰©ä¸‹**G**ä¸ªé¡¹ç›®ï¼Œé¢„æµ‹ä¸­å‰©ä¸‹**P**ä¸ªé¡¹ç›®ã€‚åœ¨è¿™é‡Œï¼Œå¯¹äºé‡‘æ ‡å‡†ä¸­çš„æ¯ä¸ª**g**å’Œé¢„æµ‹ä¸­çš„æ¯ä¸ª**p**ï¼Œæˆ‘ä»¬å‘**C[p][g]**ä¸­æ·»åŠ **1/P**ã€‚è¿™æ ·åšæ€»å…±å‘æ··æ·†çŸ©é˜µä¸­æ·»åŠ äº†**G**ï¼Œä»è€Œæ‰¿è®¤é‡‘æ ‡å‡†ä¸­çš„æƒ…ç»ªæ•°é‡å°šæœªåŒ¹é…ï¼Œé¢„æµ‹ä¸­çš„æ¯ä¸ªé¡¹ç›®è¢«è§†ä¸ºæœ‰åŒç­‰å¯èƒ½æ€§æ˜¯åº”è¯¥æ›¿æ¢**g**çš„é‚£ä¸ªã€‚
- en: The machinery for calculating modified confusion matrices is fairly intricate,
    and including it here would add very little to the preceding explanation. The
    code for this can be found in this bookâ€™s GitHub repository â€“ for now, it is probably
    best just to note that when an item can be assigned multiple labels, the confusion
    matrix has to take account of situations where the Gold Standard and the prediction
    both assign multiple labels, with the sets being assigned being of different sizes
    and where some labels are common to both, some only appear in the Gold Standard
    and some only appear in the prediction.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—ä¿®æ”¹åçš„æ··æ·†çŸ©é˜µçš„æœºåˆ¶ç›¸å½“å¤æ‚ï¼Œå°†å…¶åŒ…å«åœ¨è¿™é‡Œå¯¹å‰é¢çš„è§£é‡Šå¢åŠ çš„å¾ˆå°‘ã€‚è¿™æœ¬ä¹¦çš„GitHubä»“åº“ä¸­æœ‰è¿™ä¸ªä»£ç â€”â€”ç›®å‰ï¼Œæœ€å¥½åªæ˜¯æ³¨æ„ä¸€ä¸‹ï¼Œå½“ä¸€ä¸ªé¡¹ç›®å¯ä»¥åˆ†é…å¤šä¸ªæ ‡ç­¾æ—¶ï¼Œæ··æ·†çŸ©é˜µå¿…é¡»è€ƒè™‘åˆ°é‡‘æ ‡å‡†å’Œé¢„æµ‹éƒ½åˆ†é…äº†å¤šä¸ªæ ‡ç­¾çš„æƒ…å†µï¼Œåˆ†é…çš„é›†åˆå¤§å°ä¸åŒï¼Œå¹¶ä¸”æœ‰äº›æ ‡ç­¾ä¸¤è€…éƒ½æœ‰ï¼Œæœ‰äº›åªå‡ºç°åœ¨é‡‘æ ‡å‡†ä¸­ï¼Œæœ‰äº›åªå‡ºç°åœ¨é¢„æµ‹ä¸­ã€‚
- en: The way we do this is not symmetric between the Gold Standard and the prediction,
    but it does provide confusion matrices that tell us something useful about what
    a given classifier is doing. For cases where there is exactly one item in the
    Gold Standard and one in the prediction, it collapses to the standard version,
    and where there are differing numbers in each, it does provide a picture of what
    is going on.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿™æ ·åšçš„æ–¹å¼åœ¨é‡‘æ ‡å‡†ä¸é¢„æµ‹ä¹‹é—´å¹¶ä¸å¯¹ç§°ï¼Œä½†å®ƒç¡®å®æä¾›äº†æ··æ·†çŸ©é˜µï¼Œè¿™äº›çŸ©é˜µå‘Šè¯‰æˆ‘ä»¬å…³äºç»™å®šåˆ†ç±»å™¨æ­£åœ¨åšä»€ä¹ˆçš„ä¸€äº›æœ‰ç”¨çš„ä¿¡æ¯ã€‚å¯¹äºé‡‘æ ‡å‡†å’Œé¢„æµ‹ä¸­æ°å¥½æœ‰ä¸€ä¸ªé¡¹ç›®çš„æƒ…å†µï¼Œå®ƒå°†é€€åŒ–ä¸ºæ ‡å‡†ç‰ˆæœ¬ï¼Œè€Œå¯¹äºæ¯ä¸ªä¸­éƒ½æœ‰ä¸åŒæ•°é‡é¡¹ç›®çš„æƒ…å†µï¼Œå®ƒç¡®å®æä¾›äº†ä¸€å¹…æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…çš„å›¾æ™¯ã€‚
- en: 'We will start by looking at the confusion matrix for CARER-EN using SVM as
    the classifier (the scores for SVMs and DNNs are very similar, and the confusion
    matrices are also very similar, so for convenience, we will use SVMs for the explorations
    here). The following matrix was obtained using a version of SVM which simply picks
    the most likely emotion for each tweet instead of using a threshold to try to
    work out whether there are any that are likely enough for them to be counted and
    if so, whether there are several that could count:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é¦–å…ˆæŸ¥çœ‹ä½¿ç”¨SVMä½œä¸ºåˆ†ç±»å™¨ï¼ˆSVMå’ŒDNNçš„åˆ†æ•°éå¸¸ç›¸ä¼¼ï¼Œæ··æ·†çŸ©é˜µä¹Ÿéå¸¸ç›¸ä¼¼ï¼Œå› æ­¤ä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘ä»¬å°†åœ¨è¿™é‡Œä½¿ç”¨SVMï¼‰çš„CARER-ENæ··æ·†çŸ©é˜µã€‚ä»¥ä¸‹çŸ©é˜µæ˜¯ä½¿ç”¨ä¸€ä¸ªç®€å•çš„SVMç‰ˆæœ¬è·å¾—çš„ï¼Œè¯¥ç‰ˆæœ¬åªæ˜¯ä¸ºæ¯æ¡æ¨æ–‡é€‰æ‹©æœ€å¯èƒ½çš„æƒ…ç»ªï¼Œè€Œä¸æ˜¯ä½¿ç”¨é˜ˆå€¼æ¥å°è¯•ç¡®å®šæ˜¯å¦æœ‰ä»»ä½•æƒ…ç»ªè¶³å¤Ÿå¯èƒ½è¢«è®¡ç®—ï¼Œå¦‚æœæ˜¯è¿™æ ·ï¼Œæ˜¯å¦æœ‰å‡ ä¸ªå¯ä»¥è®¡ç®—ï¼š
- en: '|  | **anger** | **fear** | **joy** | **love** | **sadness** | **surprise**
    |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 124 | 0 | 1 | 0 | 1 | 0 |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 1 | 128 | 0 | 0 | 0 | 1 |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 0 | 337 | 1 | 0 | 0 |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 0 | 1 | 73 | 0 | 0 |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| **sadness** | 0 | 1 | 1 | 0 | 293 | 0 |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| **surprise** | 0 | 0 | 0 | 0 | 0 | 37 |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: Figure 10.4 â€“ Confusion matrix for CARER-EN, one emotion per tweet, with SVM
    as the classifier
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what you expect a confusion matrix to look like â€“ the largest scores
    down the diagonal with a scattering of other assignments, with the biggest confusion
    being between **love** and **joy**. When we use the same algorithm for SEM11-EN,
    we get a very different picture:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **anti** | **disg** | **fear** | **joy** | **love** | **opti**
    | **pess** | **sadn** | **surp** | **trus** | **--** |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 311 | 2 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: '| **antici** | 8 | 65 | 1 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 12 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
- en: '| **disgus** | 10 | 3 | 36 | 1 | 3 | 0 | 0 | 0 | 0 | 0 | 1 | 182 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 9 | 0 | 0 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 34 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 11 | 2 | 1 | 1 | 186 | 0 | 0 | 0 | 0 | 0 | 1 | 39 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 1 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 40 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 7 | 1 | 0 | 2 | 2 | 0 | 20 | 1 | 0 | 0 | 0 | 119 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 19 | 0 | 0 | 0 | 26 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| **sadnes** | 9 | 3 | 1 | 1 | 3 | 0 | 0 | 1 | 16 | 0 | 0 | 119 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| **surpri** | 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 16 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 2 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 15 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| **--** | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 21 | 0 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: Figure 10.5 â€“ Confusion matrix for SEM11-EN, one emotion per tweet, with SVM
    as the classifier
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'We get several false positives (places where nothing was expected but something
    was predicted â€“ that is, the row headed with **--**: two of these have **anger**
    assigned and 21 **trust**). This happens because we have forced the classifier
    to choose something even in cases where the Gold Standard doesnâ€™t expect anything.
    We also have a much larger number of places where there are false negatives (the
    column headed with **--**), where something was expected but nothing was found,
    generally because the Gold Standard had multiple labels and there was only one
    prediction. And there are numerous cases where the assignment is just wrong, with
    an awful lot of things being labeled as **anger** when they should be something
    else.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that if the classifier is forced to assign exactly one emotion
    per tweet, then it cannot help producing false positives (if the Gold Standard
    says that nothing should be assigned) and false negatives (if the Gold Standard
    says that more than one emotion should be assigned). If we look at the test set
    in detail, we will see that there are 23 tweets with no emotion assigned, which
    show up as false positives, and 645 tweets with more than one emotion assigned,
    which show up as 1,065 false negatives (because some of them have three or more
    emotions assigned). *There is nothing that can be done about this if our classifier
    assumes that there is one emotion* *per tweet.*
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜åœ¨äºï¼Œå¦‚æœåˆ†ç±»å™¨è¢«å¼ºåˆ¶ä¸ºæ¯æ¡æ¨æ–‡åˆ†é…æ°å¥½ä¸€ç§æƒ…ç»ªï¼Œé‚£ä¹ˆå®ƒæ— æ³•é¿å…äº§ç”Ÿè¯¯æŠ¥ï¼ˆå¦‚æœé»„é‡‘æ ‡å‡†è¯´æ²¡æœ‰ä»»ä½•ä¸œè¥¿åº”è¯¥è¢«åˆ†é…ï¼‰å’Œæ¼æŠ¥ï¼ˆå¦‚æœé»„é‡‘æ ‡å‡†è¯´åº”è¯¥åˆ†é…å¤šç§æƒ…ç»ªï¼‰ã€‚å¦‚æœæˆ‘ä»¬ä»”ç»†æŸ¥çœ‹æµ‹è¯•é›†ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°æœ‰23æ¡æ²¡æœ‰åˆ†é…æƒ…ç»ªçš„æ¨æ–‡ï¼Œè¿™äº›æ¨æ–‡æ˜¾ç¤ºä¸ºè¯¯æŠ¥ï¼Œè¿˜æœ‰645æ¡åˆ†é…äº†å¤šç§æƒ…ç»ªçš„æ¨æ–‡ï¼Œè¿™äº›æ¨æ–‡æ˜¾ç¤ºä¸º1,065æ¡æ¼æŠ¥ï¼ˆå› ä¸ºå…¶ä¸­ä¸€äº›è¢«åˆ†é…äº†ä¸‰ç§æˆ–æ›´å¤šæƒ…ç»ªï¼‰ã€‚*å¦‚æœæˆ‘ä»¬çš„åˆ†ç±»å™¨å‡è®¾æ¯æ¡æ¨æ–‡åªæœ‰ä¸€ä¸ªæƒ…ç»ªï¼Œé‚£ä¹ˆå¯¹æ­¤å°±æ— èƒ½ä¸ºåŠ›äº†*ã€‚
- en: Suppose that we have *N* tweets, *X* of which have no emotion assigned to them
    and *Y* have more than one. In this case, there must be at least *X* false positives
    (one for each tweet that should have no labels assigned but the classifier assigns
    one) and at least *Y* false negatives (one for each tweet that should have more
    than one label assigned but the classifier only assigns one), meaning that the
    best possible Jaccard score is *(N-X)/((N-X)+X+Y)*. For the set of 772 tweets
    in SEM11-EN, this comes out as *(772-23)/(772-23+(1065+23)) = 0.41* (the number
    of false negatives is very high because of the preponderance of tweets that should
    be given more than two labels â€“ this equation assumed that tweets were assigned
    zero, one, or two labels). This is a strict upper bound. No classifier that assigns
    exactly one label to each tweet can achieve a higher Jaccard score than 0.41 on
    this dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬å…±æœ‰*N*æ¡æ¨æ–‡ï¼Œå…¶ä¸­*X*æ¡æ²¡æœ‰åˆ†é…æƒ…ç»ªï¼Œ*Y*æ¡åˆ†é…äº†å¤šç§æƒ…ç»ªã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè‡³å°‘ä¼šæœ‰*X*æ¡è¯¯æŠ¥ï¼ˆæ¯æ¡åº”è¯¥æ²¡æœ‰æ ‡ç­¾ä½†åˆ†ç±»å™¨åˆ†é…äº†ä¸€ä¸ªæ ‡ç­¾çš„æ¨æ–‡ï¼‰å’Œè‡³å°‘*Y*æ¡æ¼æŠ¥ï¼ˆæ¯æ¡åº”è¯¥æœ‰å¤šä¸ªæ ‡ç­¾ä½†åˆ†ç±»å™¨åªåˆ†é…äº†ä¸€ä¸ªæ ‡ç­¾çš„æ¨æ–‡ï¼‰ï¼Œè¿™æ„å‘³ç€æœ€ä½³å¯èƒ½çš„Jaccardåˆ†æ•°æ˜¯*(N-X)/((N-X)+X+Y)*ã€‚å¯¹äºSEM11-ENä¸­çš„772æ¡æ¨æ–‡é›†åˆï¼Œè¿™ä¸ªåˆ†æ•°æ˜¯*(772-23)/(772-23+(1065+23))
    = 0.41*ï¼ˆç”±äºåº”è¯¥åˆ†é…ä¸¤ä¸ªä»¥ä¸Šæ ‡ç­¾çš„æ¨æ–‡å å¤šæ•°ï¼Œæ¼æŠ¥çš„æ•°é‡éå¸¸é«˜â€”â€”è¿™ä¸ªæ–¹ç¨‹å‡è®¾æ¨æ–‡è¢«åˆ†é…äº†é›¶ã€ä¸€æˆ–ä¸¤ä¸ªæ ‡ç­¾ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ä¸Šé™ã€‚æ²¡æœ‰ä»»ä½•åˆ†ç±»å™¨èƒ½å¤Ÿåœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šå®ç°é«˜äº0.41çš„Jaccardåˆ†æ•°ã€‚
- en: The position is worse than that. Careful inspection of the diagonal shows that
    several emotions have good scores on the diagonal (**anger**, **joy**), while
    others have very poor scores on the diagonal and a lot of false negatives (**disgust**,
    **love**, **optimism**), with several emotions being confused with **anger**.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ç½®æ¯”è¿™è¿˜è¦ç³Ÿç³•ã€‚ä»”ç»†æ£€æŸ¥å¯¹è§’çº¿æ˜¾ç¤ºï¼Œä¸€äº›æƒ…ç»ªåœ¨å¯¹è§’çº¿ä¸Šæœ‰å¾ˆå¥½çš„åˆ†æ•°ï¼ˆ**æ„¤æ€’**ï¼Œ**å¿«ä¹**ï¼‰ï¼Œè€Œå…¶ä»–æƒ…ç»ªåœ¨å¯¹è§’çº¿ä¸Šåˆ†æ•°éå¸¸ä½ï¼Œå¹¶ä¸”æœ‰å¾ˆå¤šæ¼æŠ¥ï¼ˆ**åŒæ¶**ï¼Œ**çˆ±æƒ…**ï¼Œ**ä¹è§‚**ï¼‰ï¼Œå…¶ä¸­å‡ ç§æƒ…ç»ªä¸**æ„¤æ€’**æ··æ·†ã€‚
- en: 'When we look at the KWT.M-AR dataset, we will see an output that is in some
    ways similar and is no more encouraging:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬æŸ¥çœ‹KWT.M-ARæ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ä¸€äº›æ–¹é¢ç›¸ä¼¼ä½†å¹¶ä¸ä»¤äººé¼“èˆçš„è¾“å‡ºï¼š
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **--** |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | **æ„¤æ€’** | **åŒæ¶** | **ææƒ§** | **å¿«ä¹** | **çˆ±æƒ…** | **ä¹è§‚** | **æ‚²è§‚** | **æ‹’ç»**
    | **ä¿¡ä»»** | **--** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **anger** | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **æ„¤æ€’** | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 0 |'
- en: '| **dissat** | 0 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 1 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| **ä¸æ»¡** | 0 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 1 |'
- en: '| **fear** | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| **ææƒ§** | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |'
- en: '| **joy** | 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 12 | 0 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| **å¿«ä¹** | 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 12 | 0 |'
- en: '| **love** | 0 | 0 | 0 | 0 | 50 | 0 | 0 | 0 | 44 | 3 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| **çˆ±æƒ…** | 0 | 0 | 0 | 0 | 50 | 0 | 0 | 0 | 44 | 3 |'
- en: '| **optimi** | 0 | 0 | 0 | 0 | 0 | 22 | 0 | 0 | 17 | 0 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **ä¹è§‚** | 0 | 0 | 0 | 0 | 0 | 22 | 0 | 0 | 17 | 0 |'
- en: '| **pessim** | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 2 | 2 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| **æ‚²è§‚** | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 2 | 2 |'
- en: '| **reject** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| **æ‹’ç»** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |'
- en: '| **trust** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 0 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **ä¿¡ä»»** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 0 |'
- en: '| **--** | 0 | 1 | 0 | 0 | 8 | 3 | 0 | 0 | 751 | 0 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **--** | 0 | 1 | 0 | 0 | 8 | 3 | 0 | 0 | 751 | 0 |'
- en: Figure 10.6 â€“ Confusion matrix for KWT.M-AR, one emotion per tweet, with SVM
    as the classifier
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.6 â€“ ä½¿ç”¨SVMä½œä¸ºåˆ†ç±»å™¨çš„KWT.M-ARæ¯æ¡æ¨æ–‡ä¸€ä¸ªæƒ…ç»ªçš„æ··æ·†çŸ©é˜µ
- en: 'This time, there are a massive number of false positives, reflecting the fact
    that these datasets have a very high proportion of cases where no emotion is assigned
    in the Gold Standard (763 out of 1,000 in the test set used here, with a maximum
    attainable F1-score of around 0.42). Again, this is inevitable â€“ if a tweet should
    have no emotions assigned to it and the classifier is forced to assign one, then
    we will get a false positive. It is also worth noting that while there are non-trivial
    entries on the diagonal, a surprising number of cases have the correct assignment
    replaced by `angry: 2.38`, `fuming: 2.32`, `annoying: 2.31`, `revenge: 2.26`,
    â€¦ for `positivity: 1.82`, ğŸ’•`: 1.75`, `rejoice: 1.74`, `gift: 1.72`, `laughing:
    1.70` for `flat: 1.25`, `com: 1.19`, `cup: 1.06`, `need: 1.05`, `major: 1.05`.
    These are not words that are obviously associated with trust, and indeed the links
    between them and this emotion are not strong. So, when the classifier is forced
    to choose an emotion for a tweet that does not contain any words that are linked
    to specific emotions, it is likely to resort to choosing the one for which no
    such words are expected anyway.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿™æ¬¡ï¼Œæœ‰å¤§é‡çš„å‡é˜³æ€§ï¼Œè¿™åæ˜ äº†è¿™äº›æ•°æ®é›†åœ¨é»„é‡‘æ ‡å‡†ä¸­æœªåˆ†é…æƒ…æ„Ÿçš„æ¯”ä¾‹éå¸¸é«˜ï¼ˆåœ¨æœ¬æµ‹è¯•é›†ä¸­ï¼Œæœ‰ 763 ä¸ªï¼Œå æ€»æ•°çš„ 76.3%ï¼Œæœ€å¤§å¯è¾¾åˆ°çš„ F1 åˆ†æ•°çº¦ä¸º
    0.42ï¼‰ã€‚åŒæ ·ï¼Œè¿™æ˜¯ä¸å¯é¿å…çš„â€”â€”å¦‚æœä¸€æ¡æ¨æ–‡åº”è¯¥æ²¡æœ‰ä»»ä½•æƒ…æ„Ÿåˆ†é…ï¼Œè€Œåˆ†ç±»å™¨è¢«è¿«åˆ†é…ä¸€ä¸ªï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªå‡é˜³æ€§ã€‚è¿˜å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡å¯¹è§’çº¿ä¸Šçš„éå¹³å‡¡æ¡ç›®å¾ˆå¤šï¼Œä½†ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæœ‰å¤§é‡æƒ…å†µä¸­æ­£ç¡®çš„åˆ†é…è¢«æ›¿æ¢ä¸º
    `angry: 2.38`ï¼Œ`fuming: 2.32`ï¼Œ`annoying: 2.31`ï¼Œ`revenge: 2.26`ï¼Œâ€¦â€¦å¯¹äº `positivity:
    1.82`ï¼ŒğŸ’•`: 1.75`ï¼Œ`rejoice: 1.74`ï¼Œ`gift: 1.72`ï¼Œ`laughing: 1.70` å¯¹äº `flat: 1.25`ï¼Œ`com:
    1.19`ï¼Œ`cup: 1.06`ï¼Œ`need: 1.05`ï¼Œ`major: 1.05`ã€‚è¿™äº›ä¸æ˜¯æ˜¾ç„¶ä¸ä¿¡ä»»ç›¸å…³çš„è¯ï¼Œè€Œä¸”å®ƒä»¬ä¸è¿™ç§æƒ…æ„Ÿä¹‹é—´çš„è”ç³»å¹¶ä¸å¼ºã€‚å› æ­¤ï¼Œå½“åˆ†ç±»å™¨è¢«è¿«ä¸ºä¸åŒ…å«ä»»ä½•ä¸ç‰¹å®šæƒ…æ„Ÿç›¸å…³è”çš„è¯æ±‡çš„æ¨æ–‡é€‰æ‹©æƒ…æ„Ÿæ—¶ï¼Œå®ƒå¾ˆå¯èƒ½ä¼šé€‰æ‹©é‚£äº›æœ¬å°±ä¸æœŸæœ›æœ‰æ­¤ç±»è¯æ±‡çš„æƒ…æ„Ÿã€‚'
- en: 'If, as suggested previously, large numbers of tweets express either no emotion
    or several emotions, then we have to deal with these issues. There are several
    things we can try:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœï¼Œå¦‚å‰æ‰€è¿°ï¼Œå¤§é‡æ¨æ–‡è¡¨è¾¾çš„æ˜¯æ²¡æœ‰æƒ…æ„Ÿæˆ–å¤šç§æƒ…æ„Ÿï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»å¤„ç†è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥å°è¯•ä»¥ä¸‹å‡ ç§æ–¹æ³•ï¼š
- en: We can include an explicit â€œnone-of-the-aboveâ€ or â€œneutralâ€ class to represent
    the fact that a tweet does not carry any emotional weight. This is the easiest
    thing to do for â€œzero emotionâ€ cases, though it will not be ideal in cases where
    more than one emotion is assigned to a tweet.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åŒ…å«ä¸€ä¸ªæ˜ç¡®çš„â€œä»¥ä¸Šçš†éâ€æˆ–â€œä¸­æ€§â€ç±»åˆ«æ¥è¡¨ç¤ºä¸€æ¡æ¨æ–‡æ²¡æœ‰ä»»ä½•æƒ…æ„Ÿæƒé‡ã€‚å¯¹äºâ€œé›¶æƒ…æ„Ÿâ€çš„æƒ…å†µï¼Œè¿™æ˜¯æœ€å®¹æ˜“åšåˆ°çš„ï¼Œå°½ç®¡åœ¨æ¨æ–‡è¢«åˆ†é…äº†å¤šç§æƒ…æ„Ÿçš„æƒ…å†µä¸‹ï¼Œè¿™å¹¶ä¸ç†æƒ³ã€‚
- en: We can use the fact that some of the classifiers calculate a score for each
    emotion. We will look at this in more detail shortly.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åˆ©ç”¨æŸäº›åˆ†ç±»å™¨ä¸ºæ¯ç§æƒ…ç»ªè®¡ç®—åˆ†æ•°çš„äº‹å®ã€‚æˆ‘ä»¬å°†åœ¨ç¨åæ›´è¯¦ç»†åœ°æ¢è®¨è¿™ä¸€ç‚¹ã€‚
- en: 'We can train a set of binary classifiers â€“ **joy** versus **not-joy**, **anger**
    versus **not-anger**, and so on. This will potentially deal with both kinds of
    cases: if each of these classifiers returns the negative version, we will get
    an overall zero assignment, and if more than one returns the positive version,
    we will get multiple assignments.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è®­ç»ƒä¸€ç»„äºŒå…ƒåˆ†ç±»å™¨â€”â€”**å¿«ä¹**ä¸**éå¿«ä¹**ï¼Œ**æ„¤æ€’**ä¸**éæ„¤æ€’**ï¼Œç­‰ç­‰ã€‚è¿™å¯èƒ½ä¼šå¤„ç†ä¸¤ç§æƒ…å†µï¼šå¦‚æœè¿™äº›åˆ†ç±»å™¨ä¸­çš„æ¯ä¸€ä¸ªéƒ½è¿”å›è´Ÿç‰ˆæœ¬ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªæ•´ä½“é›¶åˆ†é…ï¼Œå¦‚æœæœ‰å¤šä¸ªè¿”å›æ­£ç‰ˆæœ¬ï¼Œæˆ‘ä»¬å°†å¾—åˆ°å¤šä¸ªåˆ†é…ã€‚
- en: For the remainder of this chapter, we will concentrate on the SEM-11 and KWT
    datasets since these are the only ones with variable numbers of labels. If your
    training data assigns exactly one emotion to each tweet, and you want to have
    exactly one emotion assigned when running the classifier on live data, then one
    of the others will generally provide the best solution â€“ LEXCLASSIFIER usually
    provides reasonably accurate results with very little training time, Transformers
    usually provide the best results but take a lot of training, and SVMs and DNNs
    come somewhere in between for both accuracy and training time.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« å‰©ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äº SEM-11 å’Œ KWT æ•°æ®é›†ï¼Œå› ä¸ºè¿™äº›æ˜¯å”¯ä¸€å…·æœ‰å¯å˜æ ‡ç­¾æ•°é‡çš„æ•°æ®é›†ã€‚å¦‚æœä½ çš„è®­ç»ƒæ•°æ®ä¸ºæ¯æ¡æ¨æ–‡åˆ†é…äº†æ°å¥½ä¸€ç§æƒ…ç»ªï¼Œå¹¶ä¸”ä½ å¸Œæœ›åœ¨è¿è¡Œåˆ†ç±»å™¨å¯¹å®æ—¶æ•°æ®è¿›è¡Œåˆ†ç±»æ—¶ä¹Ÿæ°å¥½åˆ†é…ä¸€ç§æƒ…ç»ªï¼Œé‚£ä¹ˆå…¶ä»–æ–¹æ³•é€šå¸¸èƒ½æä¾›æœ€ä½³è§£å†³æ–¹æ¡ˆâ€”â€”LEXCLASSIFIER
    é€šå¸¸åœ¨æçŸ­çš„è®­ç»ƒæ—¶é—´å†…æä¾›ç›¸å½“å‡†ç¡®çš„ç»“æœï¼ŒTransformer é€šå¸¸æä¾›æœ€ä½³ç»“æœä½†éœ€è¦å¤§é‡è®­ç»ƒï¼Œè€Œ SVM å’Œ DNN åœ¨å‡†ç¡®æ€§å’Œè®­ç»ƒæ—¶é—´ä¹‹é—´å¤„äºä¸­ç­‰æ°´å¹³ã€‚
- en: Using â€œneutralâ€ as a label
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨â€œä¸­æ€§â€ä½œä¸ºæ ‡ç­¾
- en: 'We can introduce **neutral** as a label simply by looking at the labels assigned
    by the Gold Standard and assigning **neutral** if no other emotion is assigned.
    This does not affect the CARER-EN set: nothing is assigned **neutral** in the
    training data, so no words are found to be associated with this label, so, in
    turn, nothing is assigned to it by the classifier. The effect on the SEM11-EN
    data is more interesting:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹Gold Standardåˆ†é…çš„æ ‡ç­¾æ¥ç®€å•åœ°å¼•å…¥**ä¸­æ€§**ä½œä¸ºæ ‡ç­¾ã€‚è¿™ä¸ä¼šå½±å“CARER-ENé›†ï¼šè®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰åˆ†é…**ä¸­æ€§**ï¼Œå› æ­¤æ²¡æœ‰æ‰¾åˆ°ä¸è¯¥æ ‡ç­¾ç›¸å…³çš„å•è¯ï¼Œå› æ­¤ï¼Œåè¿‡æ¥ï¼Œåˆ†ç±»å™¨æ²¡æœ‰åˆ†é…ä»»ä½•å†…å®¹ã€‚å¯¹SEM11-ENæ•°æ®çš„å½±å“æ›´æœ‰è¶£ï¼š
- en: '|  | **ange** | **anti** | **disg** | **fear** | **joy** | **love** | **opti**
    | **pess** | **sadn** | **surp** | **trus** | **neut** | **--** |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | **æ„¤æ€’** | **åä¹‰** | **åŒæ¶** | **ææƒ§** | **å–œæ‚¦** | **çˆ±æƒ…** | **ä¹è§‚** | **æ‚²è§‚**
    | **æ‚²ä¼¤** | **æƒŠè®¶** | **ä¿¡ä»»** | **ä¸­æ€§** | **--** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **anger** | 311 | 2 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **æ„¤æ€’** | 311 | 2 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
- en: '| **antici** | 8 | 65 | 1 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 12 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| **æœŸå¾…** | 8 | 65 | 1 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 12 |'
- en: '| **disgus** | 10 | 3 | 36 | 1 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 182 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| **åŒæ¶** | 10 | 3 | 36 | 1 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 182 |'
- en: '| **fear** | 9 | 0 | 0 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 34 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| **ææƒ§** | 9 | 0 | 0 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 34 |'
- en: '| **joy** | 11 | 2 | 1 | 1 | 186 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 39 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| **å–œæ‚¦** | 11 | 2 | 1 | 1 | 186 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 39 |'
- en: '| **love** | 0 | 1 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 40 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **çˆ±æƒ…** | 0 | 1 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 40 |'
- en: '| **optimi** | 7 | 1 | 0 | 2 | 2 | 0 | 20 | 1 | 0 | 0 | 0 | 1 | 118 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **ä¹è§‚** | 7 | 1 | 0 | 2 | 2 | 0 | 20 | 1 | 0 | 0 | 0 | 1 | 118 |'
- en: '| **pessim** | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 19 | 0 | 0 | 0 | 0 | 26 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| **æ‚²è§‚** | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 19 | 0 | 0 | 0 | 0 | 26 |'
- en: '| **sadnes** | 9 | 3 | 1 | 1 | 3 | 0 | 0 | 1 | 16 | 0 | 0 | 0 | 119 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| **æ‚²ä¼¤** | 9 | 3 | 1 | 1 | 3 | 0 | 0 | 1 | 16 | 0 | 0 | 0 | 119 |'
- en: '| **surpri** | 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 16 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| **æƒŠè®¶** | 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 16 |'
- en: '| **trust** | 2 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 15 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| **ä¿¡ä»»** | 2 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 15 |'
- en: '| **neutra** | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 21 | 0 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| **ä¸­æ€§** | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 21 | 0 |'
- en: Figure 10.7 â€“ Confusion matrix for SEM11-EN, one emotion per tweet, with SVM
    as the classifier, and neutral as a label
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.7 - SEM11-ENçš„æ··æ·†çŸ©é˜µï¼Œæ¯æ¡æ¨æ–‡ä¸€ä¸ªæƒ…ç»ªï¼Œä½¿ç”¨SVMä½œä¸ºåˆ†ç±»å™¨ï¼Œä¸­æ€§ä½œä¸ºæ ‡ç­¾
- en: There is very little change down the diagonal â€“ that is, the classifier gets
    the same actual emotions right with or without **neutral** as a label; most things
    that ought to be classified as neutral are indeed labeled as such, with a couple
    mislabeled as **anger**; a few things are labeled as **neutral** when they should
    not be; and there are still a lot of false negatives because there were a lot
    of tweets that ought to have been given more than one label. These canâ€™t be labeled
    **neutral** by the classifier since it can only assign one label to each tweet,
    so any tweet that ought to have more than one will contribute to the set of false
    negatives.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¯¹è§’çº¿ä»¥ä¸‹çš„å˜åŒ–éå¸¸å°â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œåˆ†ç±»å™¨åœ¨æœ‰æˆ–æ²¡æœ‰**ä¸­æ€§**æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œå¯¹ç›¸åŒçš„å®é™…æƒ…ç»ªçš„åˆ¤æ–­æ˜¯ç›¸åŒçš„ï¼›å¤§å¤šæ•°æœ¬åº”è¢«å½’ç±»ä¸ºä¸­æ€§çš„äº‹ç‰©ç¡®å®è¢«æ ‡è®°ä¸ºä¸­æ€§ï¼Œæœ‰å°‘æ•°è¢«é”™è¯¯åœ°æ ‡è®°ä¸º**æ„¤æ€’**ï¼›æœ‰å‡ ä»¶äº‹ç‰©è¢«é”™è¯¯åœ°æ ‡è®°ä¸º**ä¸­æ€§**ï¼Œè€Œå®ƒä»¬ä¸åº”è¯¥è¢«è¿™æ ·æ ‡è®°ï¼›ç”±äºæœ‰å¾ˆå¤šæ¨æ–‡æœ¬åº”è¢«èµ‹äºˆå¤šä¸ªæ ‡ç­¾ï¼Œå› æ­¤ä»ç„¶æœ‰å¾ˆå¤šå‡é˜´æ€§ã€‚è¿™äº›æ¨æ–‡ä¸èƒ½è¢«åˆ†ç±»å™¨æ ‡è®°ä¸º**ä¸­æ€§**ï¼Œå› ä¸ºåˆ†ç±»å™¨åªèƒ½ä¸ºæ¯æ¡æ¨æ–‡åˆ†é…ä¸€ä¸ªæ ‡ç­¾ï¼Œæ‰€ä»¥ä»»ä½•æœ¬åº”è¢«èµ‹äºˆå¤šä¸ªæ ‡ç­¾çš„æ¨æ–‡éƒ½å°†å¯¼è‡´å‡é˜´æ€§é›†çš„å¢åŠ ã€‚
- en: 'The situation with the KWT examples is intriguing. These have large numbers
    of tweets with no emotion assigned to them, so we expect a lot of false positives
    if the classifier is set to assign one emotion per tweet. The confusion matrices
    for KWT.M-AR without and with **neutral** as a label are given here:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: KWTç¤ºä¾‹çš„æƒ…å†µå¾ˆå¸å¼•äººã€‚è¿™äº›ç¤ºä¾‹ä¸­æœ‰å¤§é‡æ²¡æœ‰åˆ†é…æƒ…ç»ªçš„æ¨æ–‡ï¼Œå› æ­¤æˆ‘ä»¬é¢„è®¡å¦‚æœåˆ†ç±»å™¨è®¾ç½®ä¸ºæ¯æ¡æ¨æ–‡åˆ†é…ä¸€ä¸ªæƒ…ç»ªï¼Œå°†ä¼šå‡ºç°å¾ˆå¤šå‡é˜³æ€§ã€‚è¿™é‡Œç»™å‡ºäº†KWT.M-ARåœ¨æœ‰å’Œæ²¡æœ‰**ä¸­æ€§**æ ‡ç­¾æ—¶çš„æ··æ·†çŸ©é˜µï¼š
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **--** |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | **æ„¤æ€’** | **åŒæ¶** | **ææƒ§** | **å–œæ‚¦** | **çˆ±æƒ…** | **ä¹è§‚** | **æ‚²è§‚** | **æ‹’ç»**
    | **ä¿¡ä»»** | **--** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **anger** | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 0 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **æ„¤æ€’** | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 0 |'
- en: '| **dissat** | 0 | 19 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 4 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **ä¸æ»¡** | 0 | 19 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 4 |'
- en: '| **fear** | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **ææƒ§** | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
- en: '| **joy** | 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 23 | 2 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| **å–œæ‚¦** | 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 23 | 2 |'
- en: '| **love** | 0 | 0 | 0 | 0 | 82 | 0 | 0 | 0 | 47 | 1 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 0 | 0 | 0 | 0 | 37 | 0 | 0 | 18 | 2 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 2 | 0 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 2 | 0 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 12 | 1 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| **--** | 0 | 2 | 0 | 3 | 13 | 2 | 0 | 0 | 697 | 0 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: Figure 10.8 â€“ Confusion matrix for KWT.M-AR, one emotion per tweet, with SVM
    as the classifier, and neutral not included
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: As before, most of the scores on the diagonal are quite good â€“ that is, most
    of the time, the classifier assigns the right label where there is a label to
    be assigned. Inevitably, there are a large number of false positives, nearly all
    of which are assigned to **trust**. As before, in almost every case where the
    Gold Standard says there should be no label, the classifier has chosen **trust**,
    rather than distributing the false positives evenly. Again, what seems to be happening
    is that the classifier does not associate any words particularly strongly with
    **trust**, so when it is given a tweet without any very significant words in it,
    it decides it cannot be any of the other classes, for which there are stronger
    clues, so it chooses **trust**.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'When we allow **neutral** as a label, the situation changes quite dramatically.
    Now, nearly all the false positives are assigned to **neutral**, which is the
    most reasonable outcome. There is a smattering of false negatives because this
    dataset contains tweets with multiple labels, but the diagonal is much clearer
    â€“ most emotions are assigned correctly and most cases with no emotion are assigned
    to **neutral**:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **neut** | **--** |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 0 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 0 | 19 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 4 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 0 | 24 | 1 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 0 | 0 | 0 | 81 | 0 | 0 | 0 | 0 | 48 | 1 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 0 | 0 | 0 | 0 | 37 | 0 | 0 | 0 | 18 | 2 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 2 | 0 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 2 | 0 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 5 | 1 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| **neutra** | 0 | 2 | 0 | 2 | 14 | 2 | 0 | 0 | 0 | 697 | 0 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: Figure 10.9 â€“ Confusion matrix for KWT.M-AR, one emotion per tweet, with SVM
    as the classifier, and neutral included
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: So, using **neutral** as a label provides a partial solution to the problems
    of none and multiple labels, but it *cannot* provide a complete one. Even if a
    classifier were 100% accurate when it assigned labels to tweets that ought to
    have exactly one label and when it assigned **neutral** to ones that ought to
    have no labels, it must introduce false negatives for cases where there ought
    to be more than one label.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Now is a good time to introduce a new measure of performance. The fact that
    most of the cases that are neither classified as **neutral** nor given no label
    at all lie on the diagonal suggests that the gross classification assigned to
    a set of tweets might be useful for gauging opinion, even if the assignments to
    individual tweets are unreliable. In particular, false negatives may not matter
    too much when youâ€™re trying to spot general trends, so long as the cases where
    the classifier does assign a label are consistent with the underlying reality.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯å¼•å…¥ä¸€ä¸ªæ–°çš„æ€§èƒ½åº¦é‡æŒ‡æ ‡çš„å¥½æ—¶æœºã€‚å¤§å¤šæ•°æ—¢æ²¡æœ‰è¢«åˆ†ç±»ä¸º**ä¸­æ€§**ä¹Ÿæ²¡æœ‰è¢«èµ‹äºˆä»»ä½•æ ‡ç­¾çš„æ¡ˆä¾‹éƒ½ä½äºå¯¹è§’çº¿ä¸Šï¼Œè¿™è¡¨æ˜ä¸€ç»„æ¨æ–‡è¢«åˆ†é…çš„æ€»ä½“åˆ†ç±»å¯èƒ½æœ‰åŠ©äºè¡¡é‡è§‚ç‚¹ï¼Œå³ä½¿å¯¹ä¸ªåˆ«æ¨æ–‡çš„åˆ†é…æ˜¯ä¸å¯é çš„ã€‚ç‰¹åˆ«æ˜¯ï¼Œå½“ä½ è¯•å›¾å‘ç°ä¸€èˆ¬è¶‹åŠ¿æ—¶ï¼Œå‡é˜´æ€§å¯èƒ½ä¸æ˜¯å¾ˆé‡è¦ï¼Œåªè¦åˆ†ç±»å™¨åˆ†é…æ ‡ç­¾çš„æ¡ˆä¾‹ä¸åŸºæœ¬ç°å®ä¸€è‡´ã€‚
- en: It is, of course, not possible to spot whether something is a false negative
    or is a genuine instance of a neutral assignment, and then ask what the assignment
    for the false negatives should have been. If we could do that, then we would have
    trained the classifier to do it in the first place, and likewise with false positives.
    The best we can do is assess just how much we would be led astray if we accepted
    all the assignments that the classifier made at face value. So, we define the
    **proportionality** of the classifier as the cosine distance between the proportion
    of concrete tweets assigned to each emotion in the Gold Standard and the predictions
    (that is, ignoring tweets that are assigned to **neutral** or were not given any
    labels at all). The nearer to 1 this is, the more we can expect our classifier
    to give us a reliable overall picture, even if some of the individual assignments
    were wrong.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä¸å¯èƒ½ç¡®å®šæŸä»¶äº‹æ˜¯å‡é˜´æ€§è¿˜æ˜¯çœŸæ­£çš„ä¸­æ€§åˆ†é…å®ä¾‹ï¼Œç„¶åè¯¢é—®å‡é˜´æ€§çš„åˆ†é…åº”è¯¥æ˜¯ä»€ä¹ˆã€‚å¦‚æœæˆ‘ä»¬èƒ½è¿™æ ·åšï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ€åˆå°±ä¼šè®­ç»ƒåˆ†ç±»å™¨æ¥å®Œæˆè¿™é¡¹ä»»åŠ¡ï¼ŒåŒæ ·å¯¹äºå‡é˜³æ€§ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬èƒ½åšçš„æœ€å¥½çš„äº‹æƒ…å°±æ˜¯è¯„ä¼°å¦‚æœæˆ‘ä»¬æ¥å—åˆ†ç±»å™¨åšå‡ºçš„æ‰€æœ‰åˆ†é…ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šè¢«è¯¯å¯¼åˆ°ä½•ç§ç¨‹åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åˆ†ç±»å™¨çš„**æ¯”ä¾‹æ€§**å®šä¹‰ä¸ºé‡‘æ ‡å‡†ä¸­åˆ†é…ç»™æ¯ç§æƒ…æ„Ÿçš„æ¨æ–‡æ¯”ä¾‹ä¸é¢„æµ‹ï¼ˆå³å¿½ç•¥åˆ†é…ä¸º**ä¸­æ€§**æˆ–å®Œå…¨æ²¡æœ‰æ ‡ç­¾çš„æ¨æ–‡ï¼‰ä¹‹é—´çš„ä½™å¼¦è·ç¦»ã€‚è¿™ä¸ªå€¼è¶Šæ¥è¿‘1ï¼Œæˆ‘ä»¬è¶Šå¯ä»¥æœŸå¾…æˆ‘ä»¬çš„åˆ†ç±»å™¨ç»™å‡ºä¸€ä¸ªå¯é çš„æ€»ä½“æƒ…å†µï¼Œå³ä½¿ä¸€äº›ä¸ªåˆ«åˆ†é…æ˜¯é”™è¯¯çš„ã€‚
- en: To take a simple example, suppose that we had a dataset with the 11 emotions
    from the SEM11 data, with the same number of tweets assigned to **pessimism**
    and **sadness**, and that it got everything right except that it labeled exactly
    half the tweets that should be labeled as pessimistic as sad and exactly half
    the tweets that should be labeled as sad as pessimistic. In this case, proportionality
    would be perfect, and you could safely use the classifier to make judgments about
    the overall picture, even though you could not rely on it to tell you whether
    a given tweet was sad or pessimistic. Similarly, if half the tweets in every category
    were assigned no emotion, then proportionality would be perfect, whereas if half
    the tweets in the most common category were assigned neutral but no others were,
    then it would be fairly poor.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥è¯´æ˜ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«SEM11æ•°æ®ä¸­11ç§æƒ…æ„Ÿçš„è¯­æ–™åº“ï¼Œæ‚²è§‚å’Œæ‚²ä¼¤çš„æ¨æ–‡æ•°é‡ç›¸åŒï¼Œå¹¶ä¸”å®ƒå‡ ä¹å®Œå…¨æ­£ç¡®ï¼Œé™¤äº†æ°å¥½å°†ä¸€åŠæœ¬åº”è¢«æ ‡è®°ä¸ºæ‚²è§‚çš„æ¨æ–‡æ ‡è®°ä¸ºæ‚²ä¼¤ï¼Œä»¥åŠæ°å¥½å°†ä¸€åŠæœ¬åº”è¢«æ ‡è®°ä¸ºæ‚²ä¼¤çš„æ¨æ–‡æ ‡è®°ä¸ºæ‚²è§‚ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯”ä¾‹å°†æ˜¯å®Œç¾çš„ï¼Œä½ å¯ä»¥å®‰å…¨åœ°ä½¿ç”¨è¿™ä¸ªåˆ†ç±»å™¨æ¥å¯¹æ•´ä½“æƒ…å†µåšå‡ºåˆ¤æ–­ï¼Œå³ä½¿ä½ ä¸èƒ½ä¾èµ–å®ƒæ¥å‘Šè¯‰ä½ æŸä¸ªæ¨æ–‡æ˜¯å¦æ‚²ä¼¤æˆ–æ‚²è§‚ã€‚åŒæ ·ï¼Œå¦‚æœæ¯ä¸ªç±»åˆ«ä¸­ä¸€åŠçš„æ¨æ–‡éƒ½æ²¡æœ‰è¢«åˆ†é…æƒ…æ„Ÿï¼Œé‚£ä¹ˆæ¯”ä¾‹å°†æ˜¯å®Œç¾çš„ï¼Œè€Œå¦‚æœæœ€å¸¸è§çš„ç±»åˆ«ä¸­ä¸€åŠçš„æ¨æ–‡è¢«åˆ†é…ä¸ºä¸­æ€§ï¼Œä½†æ²¡æœ‰å…¶ä»–æ¨æ–‡è¢«åˆ†é…ï¼Œé‚£ä¹ˆè¿™å°†æ˜¯ç›¸å½“å·®çš„ã€‚
- en: 'From now on, we will do this for all the classifiers that are generated by
    the folds that we perform the training on, because the test sets associated with
    the individual folds are comparatively small (which is why we do cross-fold validation
    in the first place) and we lose quite a lot of instances by ignoring neutral and
    unassigned tweets. To calculate the proportionality, we can just count the number
    of tweets whose predicted/Gold Standard includes each emotion, *ignoring neutral
    and unassigned*, and normalize the result:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç°åœ¨èµ·ï¼Œæˆ‘ä»¬å°†å¯¹æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ‰€æœ‰åˆ†ç±»å™¨éƒ½è¿™æ ·åšï¼Œå› ä¸ºä¸å•ä¸ªæŠ˜å ç›¸å…³çš„æµ‹è¯•é›†ç›¸å¯¹è¾ƒå°ï¼ˆè¿™å°±æ˜¯æˆ‘ä»¬æœ€åˆè¿›è¡Œäº¤å‰æŠ˜å éªŒè¯çš„åŸå› ï¼‰ï¼Œå¹¶ä¸”é€šè¿‡å¿½ç•¥ä¸­æ€§å’Œæœªåˆ†é…çš„æ¨æ–‡ï¼Œæˆ‘ä»¬å¤±å»äº†å¾ˆå¤šå®ä¾‹ã€‚ä¸ºäº†è®¡ç®—æ¯”ä¾‹æ€§ï¼Œæˆ‘ä»¬åªéœ€è®¡ç®—é¢„æµ‹/é‡‘æ ‡å‡†ä¸­åŒ…å«æ¯ç§æƒ…æ„Ÿçš„æ¨æ–‡æ•°é‡ï¼Œ*å¿½ç•¥ä¸­æ€§å’Œæœªåˆ†é…çš„æ¨æ–‡*ï¼Œå¹¶å°†ç»“æœå½’ä¸€åŒ–ï¼š
- en: '[PRE1]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can do this for the prediction and the Gold Standard and use cosine
    similarity to calculate the similarity between the two:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹é¢„æµ‹å’Œé‡‘æ ‡å‡†è¿›è¡Œè¿™æ ·çš„æ“ä½œï¼Œå¹¶ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ¥è®¡ç®—ä¸¤è€…ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼š
- en: '[PRE2]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For SEM11-EN, allowing an arbitrary number of emotions per tweet and using
    LEX as a classifier and `neutral` as a label, for instance, the proportions of
    the tweets that have each of the labels assigned in the prediction and the Gold
    Standard are `anger: 0.30`, `anticipation: 0.00`, `disgust: 0.31`, `fear: 0.02`,
    `joy: 0.25`, `love: 0.00`, `optimism: 0.04`, `pessimism: 0.00`, `sadness: 0.06`,
    `surprise: 0.00`, `trust: 0.00` and `anger: 0.18`, `anticipation: 0.06`, `disgust:
    0.17`, `fear: 0.06`, `joy: 0.15`, `love: 0.04`, `optimism: 0.12`, `pessimism:
    0.04`, `sadness: 0.12`, `surprise: 0.02`, `trust: 0.02`, respectively, for a proportionality
    score of 0.89\. If we use the same classifier with `neutral` as a label but allow
    exactly one label per tweet, the proportionality drops to 0.87.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'If we apply this to the KWT.M-AR dataset, we get `anger: 0.03`, `dissatisfaction:
    0.07`, `fear: 0.00`, `joy: 0.07`, `love: 0.69`, `optimism: 0.10`, `pessimism:
    0.00`, `rejection: 0.02`, `trust: 0.02` for the predictions and `anger: 0.04`,
    `dissatisfaction: 0.16`, `fear: 0.02`, `joy: 0.10`, `love: 0.43`, `optimism: 0.18`,
    `pessimism: 0.01`, `rejection: 0.01`, `trust: 0.05` for the Gold Standard, for
    a proportionality score of 0.94\. If we had not ignored the neutral/unassigned
    cases, the score would have been much higher, at 0.99, because of the huge preponderance
    of cases that are neutral in this dataset. So, we have a useful single number
    that gives us a handle on how reliable a classifier is for providing an overall
    picture, even if it fails to assign a concrete label to every tweet (that is,
    if some are either not assigned anything at all or are assigned **neutral**).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'This score will typically be quite high since, in most cases, most of the concrete
    scores lie on the diagonal. What matters is whether the distribution of neutral/unassigned
    cases follows the general distribution of the concrete cases â€“ if it does, then
    the classifier will be useful for assessing general trends, even if it does sometimes
    fail to assign concrete labels when it should. So, we will use this measure in
    addition to Jaccard to assess the classifiers in the remainder of this chapter.
    The tables in *Figures 10.10* and *10.12* show what happens to the proportionality
    for various classifiers when we add **neutral** as a label, sticking to assigning
    exactly one label per tweet. As a reference point, we will start by looking at
    what happens if we specify that each classifier returns without using **neutral**.
    As before, the classifier with the best Jaccard score is marked in bold:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **NB** | **SVM** | **DNN** |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | 0.224 (0.813) | 0.229 (0.690) | 0.223 (0.771) | *** 0.242
    (****0.677) *** |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | *** 0.247 (****0.824) *** | 0.216 (0.667) | 0.204 (0.736)
    | 0.207 (0.613) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | 0.225 (0.799) | *** 0.226 (****0.788) *** | 0.215 (0.888)
    | 0.222 (0.774) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | *** 0.208 (****0.973) *** | 0.108 (0.352) | 0.078 (0.207)
    | 0.026 (0.148) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: Figure 10.10 â€“ Jaccard and proportionality (in brackets), one label per tweet,
    neutral not included
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.10* shows that if we simply use the original classifiers unchanged
    â€“ that is, with one emotion per tweet and without **neutral** as a label â€“ we
    get fairly poor Jaccard scores, but the proportionality scores for LEX range from
    reasonable to pretty good, with the other classifiers generally doing worse on
    this metric. The proportionality score for LEX on the KWT.M-AR dataset in particular
    is massively better than the same score for any of the other classifiers on this
    dataset. The key here is that NB, SVM, and DNN assign nearly all the cases that
    should have been labeled as **neutral** to **trust** because these cases lack
    any of the distinguishing words that are common in the more clearly marked emotions,
    whereas LEX distributes them more closely in line with the marked cases. It is
    worth noting that the classifier with the best score for a given dataset does
    not always produce the best proportionality for that set:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **--** |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 9 | 19 | 0 | 0 | 11 | 2 | 0 | 0 | 0 | 14 |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 4 | 133 | 0 | 2 | 71 | 1 | 0 | 0 | 0 | 0 |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 3 | 2 | 0 | 12 | 2 | 0 | 0 | 0 | 2 |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 1 | 6 | 0 | 53 | 50 | 8 | 0 | 0 | 1 | 18 |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 7 | 0 | 8 | 548 | 12 | 0 | 0 | 0 | 0 |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 5 | 0 | 1 | 44 | 180 | 0 | 0 | 1 | 2 |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 4 | 0 | 0 | 7 | 2 | 2 | 0 | 1 | 1 |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 2 | 0 | 0 | 3 | 0 | 0 | 3 | 0 | 2 |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 1 | 9 | 0 | 2 | 28 | 8 | 0 | 0 | 13 | 3 |  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| **--** | 30 | 880 | 4 | 159 | 2008 | 577 | 2 | 1 | 61 | 0 |  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: Figure 10.11 â€“ Confusion matrix for KWT.M-AR, one label per tweet, with LEX
    as the classifier, and neutral not included
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'When we allow **neutral** as a label, NB and SVM can choose this as the class
    with the least distinctive terms and hence assign cases that should be **neutral**
    to it, leading to a massive improvement in both Jaccard and proportionality for
    these classifiers:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **NB** | **SVM** | **DNN** |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | **0.222 (0.813)** | **0.227 (0.690)** | **0.222 (0.768)**
    | *** 0.239 (****0.677) *** |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | *** 0.246 (****0.824) *** | **0.216 (0.666)** | **0.204 (0.736)**
    | **0.207 (0.615)** |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | **0.221 (0.800)** | *** 0.222 (****0.787) *** | **0.211 (0.885)**
    | **0.216 (0.774)** |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | **0.608 (0.984)** | **0.510 (0.986)** | *** 0.632 (****0.992)
    *** | **0.595 (0.905)** |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: Figure 10.12 â€“ Jaccard and proportionality, one label per tweet, and neutral
    included
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: So, we can see that using proportionality as a metric allows us to spot general
    trends. Most of our classifiers work better on multi-label datasets if we allow
    **neutral** as a label, particularly when looking at proportionality, but LEX
    performs quite well even without **neutral** as a label.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Thresholds and local thresholds
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next option to be explored is the use of thresholds. As we have seen, most
    of our classifiers provide scores for every option for each tweet, with the default
    setting being to choose the option with the highest score. In [*Chapter 6*](B18714_06.xhtml#_idTextAnchor134),
    *Naive Bayes*, we saw that assuming that our classifier will assign exactly one
    label to each tweet puts quite a tight upper bound on how well it can perform
    and that instead of doing that, we can set a threshold and say that everything
    that exceeds that threshold should be accepted as a label.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following tweet: â€œ*Hi guys ! I now do lessons via Skype ! Contact
    me for more info . # skype # lesson # basslessons # teacher # free lesson # music
    # groove # rock #* *blues*.â€'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: The Gold Standard assigns this the scores (â€˜angerâ€™, 0), (â€˜anticipationâ€™, 1),
    (â€˜disgustâ€™, 0), (â€˜fearâ€™, 0), (â€˜joyâ€™, 1), (â€˜loveâ€™, 0), (â€˜optimismâ€™, 0), (â€˜pessimismâ€™,
    0), (â€˜sadnessâ€™, 0), (â€˜surpriseâ€™, 0), (â€˜trustâ€™, 0), so it should be labeled as
    **anticipation+joy**.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes assigns this tweet the scores (â€˜angerâ€™, â€˜0.00â€™), (â€˜anticipationâ€™,
    â€˜0.88â€™), (â€˜disgustâ€™, â€˜0.00â€™), (â€˜fearâ€™, â€˜0.00â€™), (â€˜joyâ€™, â€˜0.11â€™), (â€˜loveâ€™, â€˜0.00â€™),
    (â€˜optimismâ€™, â€˜0.00â€™), (â€˜pessimismâ€™, â€˜0.00â€™), (â€˜sadnessâ€™, â€˜0.00â€™), (â€˜surpriseâ€™,
    â€˜0.00â€™), (â€˜trustâ€™, â€˜0.00â€™), so if we set the threshold at 0.1, we would get **anticipation+joy**,
    if we set it at 0.2, we would just get **anticipation**, and if we set it at 0.9,
    we would get nothing.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: For the same tweet, SVM assigns (â€˜angerâ€™, â€˜-0.77â€™), (â€˜anticipationâ€™, â€˜0.65â€™),
    (â€˜disgustâ€™, â€˜-2.64â€™), (â€˜fearâ€™, â€˜-1.67â€™), (â€˜joyâ€™, â€˜-0.99â€™), (â€˜loveâ€™, â€˜-1.93â€™),
    (â€˜optimismâ€™, â€˜-3.52â€™), (â€˜pessimismâ€™, â€˜-1.61â€™), (â€˜sadnessâ€™, â€˜-2.58â€™), (â€˜surpriseâ€™,
    â€˜-1.47â€™), (â€˜trustâ€™, â€˜-3.86â€™). So, this time, if we set the threshold to -1, we
    would get **anger+anticipation+joy**, if we set it to 0, we would just get **anticipation**,
    and if we set it to 1, we would get nothing.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: So, using a threshold will let us generate zero or more labels. We have to optimize
    the threshold, but we can do that simply by finding the smallest and greatest
    values that are assigned to any label in any tweet and incrementing evenly between
    these. The `bestThreshold` function, which was provided in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116),
    *Sentiment Lexicons and Vector-Space Models*, will work just as well with the
    raw scores produced by Naive Bayes, SVMs, and DNNs as it did there.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'If we contrast the scores that were obtained previously by requiring a single
    label and the ones we obtained using a threshold to allow zero or more labels
    on the crucial datasets, we will see that, overall, the latter produces better
    results:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **NB** | **SVM** | **DNN** |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | * 0.347 (0.898) * | 0.270 (0.764) | 0.250 (0.828) | 0.273
    (0.729) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | * 0.377 (0.940) * | 0.257 (0.761) | 0.224 (0.798) | 0.246
    (0.731) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | * 0.266 (0.890) * | 0.250 (0.837) | 0.228 (0.924) | 0.238
    (0.791) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | * 0.691 (0.990) * | 0.522 (0.988) | 0.631 (0.998) | 0.604
    (0.935) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: Figure 10.13 â€“ Zero or more emotions per tweet, with optimal global thresholds
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'The scores here are much better than they were with the simple classifiers,
    with the proportionality scores all but perfect in some cases. There is, however,
    still some way to go if we want to get the labels for individual tweets right,
    rather than just getting a good overall picture. The next move is to set a threshold
    for each label, rather than for the dataset as a whole. We will adapt `bestThreshold`
    from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons and
    Vector Space Models* so that we can assign individual thresholds to labels. We
    will make two changes to the original definition:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: We will split it into two cases â€“ one for calculating a global threshold (a
    single threshold for all cases) and another for calculating a local threshold
    for each label.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the original version, we looked at every column in the data, finding the
    minimum and maximum values that occur anywhere, and then looked at the predicted
    values for every column to calculate the Jaccard scores for each potential threshold.
    To calculate local thresholds, we just want to look at one column at a time. We
    can deal with both cases if we specify a range of columns, from `start` to `end`,
    to look at. For the global case, we must set `start=0` and `end=sys.maxsize`;
    for the case where we want to choose the best threshold for the `i` column, we
    must set `start=i` and `end=i+1`. This lets us use the same machinery for calculating
    both types of thresholds. The major changes to the original are highlighted in
    the following updated version:'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The results of allowing the classifiers to choose different thresholds for
    different labels are shown here:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **NB** | **SVM** | **DNN** |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | * 0.371 (0.987) * | 0.271 (0.827) | 0.270 (0.809) | 0.277
    (0.811) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | 0.371 (0.965) | 0.255 (0.854) | 0.236 (0.809) | 0.238 (0.795)
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | * 0.267 (0.962) * | 0.192 (0.674) | 0.222 (0.983) | 0.202
    (0.852) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | 0.681 (0.989) | 0.217 (0.163) | 0.615 (0.987) | 0.226 (0.167)
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: Figure 10.14 â€“ Zero or more emotions per tweet, with optimal local thresholds
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The proportionality scores for LEX have all improved, with LEX now easily giving
    the best proportionality score for SEM11-EN and Naive Bayes now reverting to choosing
    neutral/unassigned for nearly everything for KWT.U-AR and most of the other scores
    decreasing slightly, though the Jaccard scores have only improved for SEM11-EN
    and SEM11-ES. Yet again, different classifiers are better suited to different
    datasets and different tasks.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Multiple independent classifiers
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using either LEX with optimal local thresholds or NaÃ¯ve Bayes or SVM with an
    optimal global threshold with `MULTICLASSIFIER` class from [*Chapter 7*](B18714_07.xhtml#_idTextAnchor144),
    *Support Vector Machines*, to allow different kinds of classifiers to be used
    at the lower level. The key change here from the original is that we specify what
    classifier to use in the set of optional arguments, rather than assuming that
    we will be using `SVMCLASSIFIER`:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This will make two-way classifiers for **anger** versus **not-anger**, **love**
    versus **not-love**, and so on using the specified kind of sub-classifier. For
    the individual classifiers, there is no point in allowing more than one label
    since while a tweet can satisfy both **love** and **joy**, or both **anger** and
    **fear**, it does not make any sense to allow a tweet to satisfy **anger** and
    **not-anger**. We can still get multiple labels overall if, for instance, both
    **love** versus **not-love** and **joy** versus **not-joy** are satisfied, and
    we can still get zero labels if all the negative labels are chosen, but it makes
    no sense to allow the individual classifiers to assign zero or more than one label.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'As ever, there is a wide range of settings for the various subclassifiers.
    The main multiclassifier just combines the results of the individual subclassifiers
    and hence has no significant parameters beyond the choice of what to use as the
    subclassifiers, but the individual subclassifiers have the usual range of options.
    The following tables report the scores using just one label per subclassifier
    but varying whether or not neutral is allowed as a label:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **MULTI-LEX** | **MULTI-NB** | **MULTI-SVM** | **MULTI-DNN** |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | 0.348 (0.868) | *0.441 (0.996) * | 0.385 (1.000) | 0.422 (0.991)
    |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | 0.363 (0.878) | 0.376 (0.996) | 0.314 (0.997) | 0.333 (0.956)
    |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | 0.260 (0.852) | * 0.296 (0.993) * | 0.256 (0.995) | 0.236
    (0.936) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | 0.304 (0.979) | 0.236 (0.989) | 0.294 (0.996) | 0.182 (0.938)
    |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: Figure 10.15 (a) â€“ 0 or more emotions per tweet, multiple classifiers, -neutral
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **MULTI-LEX** | **MULTI-NB** | **MULTI-SVM** | **MULTI-DNN** |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | 0.342 (0.861) | 0.438 (0.996) | 0.381 (1.000) | 0.419 (0.991)
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | 0.363 (0.879) | 0.376 (0.996) | 0.313 (0.997) | 0.333 (0.956)
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | 0.256 (0.836) | 0.290 (0.993) | 0.250 (0.995) | 0.234 (0.938)
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | 0.665 (0.984) | 0.546 (0.989) | 0.617 (0.996) | 0.599 (0.950)
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: Figure 10.15 (b) â€“ 0 or more emotions per tweet, multiple classifiers, +neutral
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall picture here is that using multiple independent classifiers to
    decide whether a tweet should have a given label produces the best proportionality
    results yet for zero-to-many datasets. Although the Jaccard scores are only improved
    for SEM11-EN and SEM11-ES, there is considerable variation between the performance
    of the different classifiers under this regime. All four classifiers do marginally
    better on the SEM11 cases when we do not allow **neutral** as a label, but they
    all do substantially better on the KWT.M-AR dataset when we do allow **neutral**.
    This is slightly surprising, given that the individual classifiers are allowed
    to choose not to assign their labels, so it is perfectly possible to get a â€œno
    label assignedâ€ outcome for a given tweet, even without allowing **neutral**.
    *Figure 10**.16* shows how scores vary as we look at the +/-neutral classifiers
    for KWT.M-AR:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    | **Proportionality** |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-LEX, -****NEUTRAL** | 0.400 | 0.559 | 0.467 | 0.319 | 0.304 | 0.979
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-LEX, +****NEUTRAL** | 0.731 | 0.881 | 0.799 | 0.817 | 0.665 | 0.984
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-NB, -****NEUTRAL** | 0.338 | 0.441 | 0.383 | 0.247 | 0.236 | 0.989
    |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-NB, +****NEUTRAL** | 0.645 | 0.781 | 0.707 | 0.714 | 0.546 | 0.989
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-SVM,** **-****NEUTRAL** | 0.598 | 0.367 | 0.455 | 0.294 | 0.294 |
    0.996 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-SVM, +****NEUTRAL** | 0.764 | 0.763 | 0.763 | 0.747 | 0.617 | 0.996
    |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-DNN, -****NEUTRAL** | 0.255 | 0.389 | 0.308 | 0.194 | 0.182 | 0.938
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-DNN, +****NEUTRAL** | 0.725 | 0.776 | 0.750 | 0.758 | 0.599 | 0.950
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: Figure 10.16 â€“ KWT.M-AR, multiple classifiers, without and with neutral
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: In every case, both recall and precision go up when we allow neutral as a label.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the confusion matrices for Naive Bayes (the others are very similar,
    but Naive Bayes gives the best overall results and hence is the most interesting)
    is revealing:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **--** |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 31 | 7 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 0 | 111 | 0 | 0 | 15 | 0 | 0 | 0 | 0 | 94 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 2 | 3 | 0 | 1 | 0 | 0 | 0 | 0 | 15 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 6 | 0 | 56 | 12 | 6 | 0 | 0 | 0 | 66 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 2 | 0 | 4 | 327 | 4 | 0 | 0 | 0 | 155 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 2 | 0 | 3 | 0 | 123 | 0 | 0 | 0 | 70 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 1 | 2 | 0 | 0 | 1 | 0 | 2 | 0 | 0 | 15 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 6 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 0 | 4 | 0 | 1 | 2 | 0 | 0 | 0 | 26 | 50 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| **--** | 65 | 329 | 5 | 182 | 527 | 318 | 6 | 3 | 67 | 0 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: Figure 10.17 (a) â€“ Confusion matrix, multiclassifiers with NB as a subclassifier,
    KWT.M-AR, -neutral
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **neut** | **--** |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 35 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23 | 4 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 0 | 118 | 0 | 0 | 12 | 0 | 0 | 0 | 0 | 92 | 4 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 1 | 3 | 0 | 1 | 0 | 0 | 0 | 0 | 13 | 3 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 3 | 0 | 58 | 12 | 3 | 0 | 0 | 0 | 67 | 8 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 2 | 0 | 2 | 343 | 2 | 0 | 0 | 1 | 138 | 11 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 1 | 0 | 1 | 0 | 126 | 0 | 0 | 0 | 67 | 3 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 2 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 18 | 0 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 7 | 0 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 0 | 2 | 0 | 0 | 1 | 0 | 0 | 0 | 27 | 54 | 0 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| **neutra** | 2 | 14 | 0 | 15 | 162 | 64 | 0 | 0 | 5 | 3521 | 0 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| **--** | 60 | 333 | 5 | 165 | 398 | 273 | 6 | 3 | 63 | 378 | 0 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: Figure 10.17 (b) â€“ Confusion matrix, multiclassifiers with NB as a subclassifier,
    KWT.M-AR, -neutral
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: There are small differences between the scores in the main sections of the two
    tables â€“ slightly better scores on the diagonal and slightly lower confusion between
    other labels â€“ but the crucial difference is that, as before, when we do not have
    **neutral** as a label, we get a large number of false positives. Using **neutral**
    as a label reduces the number of false positives, even when we have multiple independent
    classifiers that each make their recommendations without looking at the results
    of the other classifiers.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Over the past few chapters, we have looked at a wide range of classifiers and
    compared their performance on a range of datasets. Now, it is time to reflect
    on what we have learned. Our final table of the best classifiers for the datasets
    we have looked at is as follows:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **SVM** | **SNN** | **Transformers** | **MULTI-NB** | **LEX, MULTI** |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-EN** | 0.845 | 0.829 | * 0.927 |  |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | 0.224 | 0.242 | 0.418 | * 0.438 | 0.347 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| **WASSA-EN** | * 0.770 | 0.737 | 0.753 |  |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| **CARER-EN** | 0.770 | * 0.820 | 0.816 |  |  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| **IMDB-EN** | 0.736 | 0.793 | * 0.826 |  |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-AR** | 0.514 | 0.504 | * 0.710 |  |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | 0.216 | 0.221 | 0.359 | * 0.412 | 0.377 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | 0.631 | 0.028 | 0.053 | 0.537 | * 0.691 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-ES** | 0.412 | 0.337 | * 0.663 |  |  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | 0.226 | 0.221 | * 0.340 | 0.294 | 0.266 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: Figure 10.18 â€“ Overall best classifiers
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Given what we have seen, there are several general observations we can make:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: The difference in performance of the various algorithms is not huge. For each
    dataset and each configuration of settings, the performances of the best classifiers
    are very similar, so it may be sensible to take training time as well as scores
    on the various measures into account when choosing a classifier. In particular,
    no classifier is the best in every case, and sometimes, the very simple algorithms
    (LEX and Naive Bayes) are as good as or even better than the more complex ones.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Datasets where an individual tweet can be assigned zero, one, or more labels
    are considerably more challenging than ones where each tweet is given exactly
    one label. There is, in fact, a clear upper bound to the performance of classifiers
    that do assign exactly one label per tweet to these datasets, and the best results
    are obtained by reconsidering the way that the classifiers are used. Some classifiers
    are more suitable than others for this kind of task, and this must be taken into
    account when youâ€™re choosing a classifier for datasets of this kind. Again, it
    is worth considering training time when choosing this: training a single classifier
    and then setting *N* individual thresholds is considerably quicker than training
    *N* classifiers, and in at least some cases, the difference in performance is
    quite small.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also investigated a variety of preprocessing steps, including the use of
    different tokenizers and stemmers, and we looked at using algorithms that can
    suggest â€œsimilarâ€ words to replace words in the target tweets that do not appear
    in the training data. All these tweaks pay off in some situations and not in others.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We cannot overstate this: **there is no silver bullet**. Different tasks require
    different classifiers, and you should always investigate a range of classifiers
    before deciding which one you want to use. In particular, if you are working with
    a multi-label dataset, you should consider one of the algorithms from this chapter.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: When training a classifier, it is a good idea to look at the confusion matrix
    for the labels that it assigns. Several classifiers, particularly for the datasets
    with large numbers of zero assignments, produce quite good F1 and Jaccard scores
    simply by choosing the most common class (that is, **neutral**!) in every case.
    And when choosing between classifiers, it is a good idea to consider the end task
    that the classifier is needed for. If what you want is to get a feel for an opinion
    on some topic, without worrying too much about what individual tweets say about
    it, then using proportionality as a metric can be a helpful tool. We will use
    this in the next chapter, where we will look at the link between emotions expressed
    in tweets and real-life events over a certain period.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Part 4:Case Study
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Part 3* discussed a range of approaches to the task of EA and compared their
    effectiveness on a set of standard datasets. In this final part, we investigate
    how well these approaches work on real-world data that is not connected to the
    standard sets, looking at how changes in the emotions expressed in tweets reflect
    key real-world events. We also examine how robust the various approaches are when
    applied to novel data, showing how approaches that work well when the test data
    is drawn from the same population as the training data can be fragile when applied
    to novel data.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapter:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B18714_11.xhtml#_idTextAnchor202), *Case Study â€“ The Qatar Blockade*'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
