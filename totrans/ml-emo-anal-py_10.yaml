- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Multiclassifiers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多分类器
- en: In the preceding chapters, we saw that multi-label datasets, where a tweet may
    have zero, one, or more labels, are considerably harder to deal with than simple
    multi-class datasets where each tweet has exactly one label, albeit drawn from
    a set of more than one option. In this chapter, we will investigate ways of dealing
    with these cases, looking in particular at the use of **neutral** as a label for
    handling cases where a tweet is allowed to have zero labels; at using varying
    thresholds to enable standard classifiers to return a variable number of labels;
    and at training multiple classifiers, one per label, and allowing them each to
    make a decision about the label they were trained for. The conclusion, as ever,
    will be that there is no single “silver bullet” that provides the best solution
    in every case, but in general, the use of multiple classifiers tends to be better
    than the other approaches.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了多标签数据集，其中一条推文可能有零个、一个或多个标签，与每个推文恰好有一个标签的简单多类数据集相比，处理起来要困难得多，尽管这些标签来自一个包含多个选项的集合。在本章中，我们将探讨处理这些情况的方法，特别是探讨使用**中性**标签来处理允许推文有零个标签的情况；使用不同的阈值来使标准分类器返回可变数量的标签；以及训练多个分类器，每个标签一个，并允许它们各自对其训练的标签做出决定。结论，一如既往，将是没有单一的“银弹”可以在每种情况下提供最佳解决方案，但总的来说，使用多个分类器往往比其他方法更好。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Using confusion matrices to analyze the behavior of classifiers on complex data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用混淆矩阵来分析复杂数据上分类器的行为
- en: Using **neutral** as a label to deal with tweets that have no label assigned
    to them
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**中性**标签来处理未分配标签的推文
- en: Varying thresholds to handle multi-label datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的阈值来处理多标签数据集
- en: Training multiple classifiers to handle multi-label datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练多个分类器来处理多标签数据集
- en: By the end of this chapter, you will understand how to implement several strategies
    for dealing with muti-label datasets and will have an appreciation of the effectiveness
    of these strategies for different kinds of data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将了解如何实施几种处理多标签数据集的策略，并会对这些策略对不同类型数据的有效性有所认识。
- en: Multilabel datasets are hard to work with
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多标签数据集难以处理
- en: 'We will start by looking at the performance of a selection of classifiers from
    previous chapters on the main datasets. We have said several times that multi-label
    datasets are particularly challenging, but it is worth bringing together the results
    from the best-performing algorithms to see exactly how challenging they are. *Figure
    10**.1* includes all the major classifiers that we have looked at so far. The
    multi-label datasets are highlighted in gray, and the best-performing classifier
    for each row is marked in bold/asterisks:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从查看前几章中选择的几个分类器在主要数据集上的性能开始。我们曾多次提到多标签数据集特别具有挑战性，但将表现最好的算法的结果汇集在一起，可以看到它们究竟有多具挑战性。*图10.1*包括了迄今为止我们查看的所有主要分类器。多标签数据集以灰色突出显示，每行的最佳性能分类器以粗体/星号标记：
- en: '|  | **LEX** | **CP** | **NB** | **SVM** | **SNN** | **DNN** | **Transformers**
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '|  | **LEX** | **CP** | **NB** | **SVM** | **SNN** | **DNN** | **Transformers**
    |'
- en: '| **SEM4-EN** | 0.497 | 0.593 | 0.775 | 0.845 | 0.829 | 0.847 | ***** **0.927
    *** |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| **SEM4-EN** | 0.497 | 0.593 | 0.775 | 0.845 | 0.829 | 0.847 | ***** **0.927
    *** |'
- en: '| **SEM11-EN** | 0.348 | 0.353 | 0.227 | 0.224 | 0.242 | 0.246 | ***** **0.418
    *** |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **SEM11-EN** | 0.348 | 0.353 | 0.227 | 0.224 | 0.242 | 0.246 | ***** **0.418
    *** |'
- en: '| **WASSA-EN** | 0.437 | 0.505 | 0.709 | ***** **0.770 *** | 0.737 | 0.752
    | 0.753 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| **WASSA-EN** | 0.437 | 0.505 | 0.709 | ***** **0.770 *** | 0.737 | 0.752
    | 0.753 |'
- en: '| **CARER-EN** | 0.350 | 0.395 | 0.776 | 0.770 | ***** **0.820*** | 0.804 |
    0.816 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| **CARER-EN** | 0.350 | 0.395 | 0.776 | 0.770 | ***** **0.820*** | 0.804 |
    0.816 |'
- en: '| **IMDB-EN** | 0.667 | 0.722 | 0.738 | 0.736 | 0.793 | 0.793 | ***** **0.826
    *** |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| **IMDB-EN** | 0.667 | 0.722 | 0.738 | 0.736 | 0.793 | 0.793 | ***** **0.826
    *** |'
- en: '| **SEM4-AR** | 0.509 | 0.513 | 0.531 | 0.514 | 0.504 | 0.444 | ***** **0.710
    *** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **SEM4-AR** | 0.509 | 0.513 | 0.531 | 0.514 | 0.504 | 0.444 | ***** **0.710
    *** |'
- en: '| **SEM11-AR** | ***** **0.386 *** | 0.382 | 0.236 | 0.216 | 0.221 | 0.207
    | 0.359 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| **SEM11-AR** | ***** **0.386 *** | 0.382 | 0.236 | 0.216 | 0.221 | 0.207
    | 0.359 |'
- en: '| **KWT.M-AR** | 0.663 | ***** **0.666 *** | 0.494 | 0.631 | 0.028 | 0.026
    | 0.053 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **KWT.M-AR** | 0.663 | ***** **0.666 *** | 0.494 | 0.631 | 0.028 | 0.026
    | 0.053 |'
- en: '| **SEM4-ES** | 0.420 | 0.177 | 0.360 | 0.412 | 0.337 | 0.343 | ***** **0.663
    *** |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **SEM4-ES** | 0.420 | 0.177 | 0.360 | 0.412 | 0.337 | 0.343 | ***** **0.663
    *** |'
- en: '| **SEM11-ES** | 0.271 | 0.278 | 0.230 | 0.226 | 0.221 | 0.222 | ***** **0.340
    *** |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **SEM11-ES** | 0.271 | 0.278 | 0.230 | 0.226 | 0.221 | 0.222 | ***** **0.340
    *** |'
- en: Figure 10.1 – Selected Jaccard scores for the standard datasets (multi-label
    datasets in gray)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 标准数据集选择的Jaccard分数（多标签数据集以灰色显示）
- en: 'Two things stand out from this table:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张表中，有两点特别突出：
- en: For most of the entries in this table, LEX is the worst classifier, with NB
    coming next, and then the others generally scoring fairly similarly. For the multi-label
    cases, however, LEX or CP are always better than anything else except transformers,
    and in a couple of cases, they are better than transformers as well. Given that
    these seem to be the most realistic datasets, since plenty of tweets express no
    emotion and a fair number express more than one, it is worth looking in more detail
    at what is going on in these cases.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个表格的大部分条目中，LEX 是最差的分类器，其次是 NB，然后其他分类器的得分通常相当相似。然而，对于多标签情况，LEX 或 CP 总是优于除变压器以外的任何其他分类器，而且在几个情况下，它们甚至优于变压器。鉴于这些数据集似乎是最现实的，因为许多推文没有表达情感，相当一部分推文表达了多个情感，因此值得更详细地研究这些情况中发生的事情。
- en: The multi-label cases also score significantly worse overall – while LEX and
    CP score better than most other classifiers on these cases, they do generally
    score worse on them than on the other cases, and for all the other classifiers,
    the gap between these cases and the one emotion/tweet cases is substantial.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多标签情况的整体得分也显著更差 – 虽然 LEX 和 CP 在这些情况下的表现优于大多数其他分类器，但它们通常在这些情况下的得分低于其他情况，而对于所有其他分类器，这些情况与单一情感/推文情况之间的差距是显著的。
- en: These cases seem likely to be the most useful in practice since most tweets
    do not express any sentiment and a fair number express more than one, so algorithms
    that do not deal well with these cases may not be the most suitable for this task.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些情况在实践中似乎最有用，因为大多数推文都没有表达任何情感，相当一部分推文表达了多个情感，所以处理这些情况不佳的算法可能不适合这项任务。
- en: In the *Confusion matrices* section, we will look at what the various algorithms
    do with the two kinds of datasets. Once we have a clearer idea of why multi-label
    datasets are so much more difficult to handle than single-label ones, and we have
    seen the specific problems that they cause for particular algorithms, we will
    look at ways of dealing with this kind of dataset. We will not carry out these
    experiments with transformer-based models, partly because the time it takes to
    train a transformer makes this infeasible, but more importantly because we need
    to look inside the models to understand what is going on – this is all but impossible
    with transformer-based models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在**混淆矩阵**部分，我们将查看各种算法对这两种数据集的处理方式。一旦我们更清楚地了解为什么多标签数据集比单标签数据集更难处理，并且我们已经看到它们对特定算法造成的具体问题，我们将探讨处理这类数据集的方法。我们不会使用基于变压器的模型进行这些实验，部分原因是训练变压器的耗时使得这不可行，但更重要的是，我们需要深入了解模型以了解其工作原理
    – 这在基于变压器的模型中几乎是不可能的。
- en: Confusion matrices
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'It can be very difficult to see what kinds of mistakes a classifier makes just
    by looking at the raw output. **Confusion matrices** allow us to visualize a classifier’s
    behavior, making it possible to see when two classes are being systematically
    confused or when a given class is being assigned too few or too many items. Consider
    the following dataset, where each item is classified as A, B, or C by the Gold
    Standard (G) and also has a predicted value (P):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过查看原始输出，很难看出分类器犯了什么错误。**混淆矩阵**使我们能够可视化分类器的行为，使我们能够看到两个类别是否被系统地混淆，或者某个类别是否被分配了太多或太少的项目。考虑以下数据集，其中每个项目由黄金标准（G）分类为
    A、B 或 C，并且还有一个预测值（P）：
- en: '| G | C | C | A | B | C | B | C | B | B | B | A | A | B | B | C | C | B | C
    | B | B | C | A | B | A | A | C | C | C | A | A | A | C | B | C | A | A | B |
    A |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| G | C | C | A | B | C | B | C | B | B | B | A | A | B | B | C | C | B | C
    | B | B | C | A | B | A | A | C | C | C | A | A | A | C | B | C | A | A | B |
    A |'
- en: '| P | C | B | B | B | C | A | A | B | B | A | A | A | C | B | A | B | B | C
    | B | C | C | A | B | B | B | C | B | B | B | A | B | C | B | B | A | A | B |
    A |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| P | C | B | B | B | C | A | A | B | B | A | A | A | C | B | A | B | B | C
    | B | C | C | A | B | B | B | C | B | B | B | A | B | C | B | B | A | A | B |
    A |'
- en: Figure 10.2 – Gold Standard and predicted values for the example data
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 示例数据的黄金标准和预测值
- en: 'It is hard to see any pattern in this table. Simply counting how many cases
    have the same value for G and P gives us 22 out of 38 – that is, an accuracy of
    0.58 – but it is very hard to see what kinds of things it gets right and what
    kinds of things it gets wrong. Converting this into a confusion table can help
    with this. We do this by counting the number of times that an item that ought
    to be assigned C1 as its value is predicted to have C2, producing a table of correct
    versus predicted assignments. The confusion matrix in *Figure 10**.3*, for instance,
    shows that seven things that should have been assigned the label A were indeed
    assigned that label but five were assigned B, and that six things that should
    have been assigned C were assigned C but five were assigned B. This suggests that
    there is something about the properties of Bs that makes it easy to assign things
    to this class when they should be assigned to A or C, which might lead to a line
    of inquiry about which properties of Bs were leading to this problem:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表格中很难看出任何模式。简单地计算 G 和 P 值相同的案例数量，我们得到 22 个案例中的 38 个 – 即，准确率为 0.58 – 但很难看出它做对了什么，做错了什么。将此转换为混淆表可以帮助解决这个问题。我们通过计算应该分配
    C1 作为其值的项被预测为具有 C2 的次数来实现这一点，从而产生一个正确与预测分配的表格。例如，图 10.3 中的混淆矩阵显示，七个本应被分配标签 A 的项确实被分配了该标签，但五个被分配了
    B，而六个本应被分配 C 的项被分配了 C 但五个被分配了 B。这表明 B 的某些属性使得在它们应该被分配到 A 或 C 时容易将它们分配到这个类别，这可能导致对导致此问题的
    B 的哪些属性进行调查的探究：
- en: '|  | **A** | **B** | **C** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | **A** | **B** | **C** |'
- en: '| **A** | 7 | 5 | 0 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **A** | 7 | 5 | 0 |'
- en: '| **B** | 2 | 9 | 2 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **B** | 2 | 9 | 2 |'
- en: '| **C** | 2 | 5 | 6 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **C** | 2 | 5 | 6 |'
- en: Figure 10.3 – Confusion matrix for the data in Figure 10.2
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 图 10.2 中数据的混淆矩阵
- en: 'If `gs` and `p` are the Gold Standard values for a set of points, then `confusion`
    will calculate the confusion matrix: `c` is a table with an entry for each label
    in `gs`, where the value for a label is the set of counts of each time a label
    has been predicted for it:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `gs` 和 `p` 是一组点的黄金标准值，那么 `confusion` 将计算混淆矩阵：`c` 是一个表格，其中为 `gs` 中的每个标签都有一个条目，该标签的值是预测为该标签的次数集合：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Confusion matrices can provide a considerable amount of information about what
    a classifier is doing. We do, however, have a slight problem with constructing
    confusion matrices when the Gold Standard and the prediction can each contain
    a varying number of emotions. Suppose, for instance, that the Gold Standard for
    some tweets is **love+joy** and the prediction is **love+sad+angry**. We want
    to acknowledge that the classifier was right when it predicted **love**, but what
    do we do about the fact that it missed **joy** (that is, there is a false negative)
    and predicted **sad** and **angry** (two false positives)?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵可以提供大量关于分类器所做事情的信息。然而，当黄金标准和预测可以包含不同数量的情绪时，构建混淆矩阵会有一些小问题。例如，假设某些推文的黄金标准是**爱+喜悦**，而预测是**爱+悲伤+愤怒**。我们想要承认当分类器预测**爱**时是正确的，但关于它遗漏了**喜悦**（即存在一个假阴性）以及预测了**悲伤**和**愤怒**（两个假阳性）的事实我们该如何处理？
- en: 'There is no right answer to this question. We adapt the standard way of constructing
    a confusion matrix as follows, where C[e1][e2] is the score for *e1* in the Gold
    Standard and *e2* in the prediction. We need to add a row and a column for “no
    emotion assigned” (we will use **--** for this class):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题没有正确答案。我们按照以下方式调整构建混淆矩阵的标准方法，其中 C[e1][e2] 是黄金标准中 *e1* 和预测中 *e2* 的得分。我们需要为“未分配情绪”添加一行和一列（我们将使用
    **--** 表示此类类别）：
- en: For every case where the Golden Standard and the prediction contain a given
    emotion, *e*, add 1 to *C[e][e]* and remove *e* from both the Golden Standard
    and the prediction.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于黄金标准和预测包含给定情绪 *e* 的每个情况，将 1 添加到 *C[e][e]*，并从黄金标准和预测中删除 *e*。
- en: If the Golden Standard is now empty, then every *e* left in the prediction must
    be a false positive, so add 1 to *C[--][e]* for each remaining e.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果黄金标准现在为空，那么预测中剩余的每个 *e* 必须是一个假阳性，因此对于每个剩余的 *e*，将 1 添加到 *C[--][e]*。
- en: If the prediction is empty, then every *e* left in the Golden Standard must
    be a false negative, so add 1 to *C[e][--]*.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果预测为空，那么黄金标准中剩余的每个 *e* 必须是一个假阴性，因此将 1 添加到 *C[e][--]*。
- en: If neither of them is empty after removing the shared cases, it is hard to see
    what to do. Consider the preceding example. After removing **love**, we are left
    with **joy** in the Golden Standard and **sad+angry** in the prediction. Is **joy**
    a mistake for **sad**, with **angry** as a false positive? Is **joy** a mistake
    for **angry**, with **sad** as a false positive? Is **joy** a false negative and
    **sad** and **angry** both false positives? This last suggestion does not seem
    right. Suppose we had one case where **joy** was matched with **sad+angry**, another
    where it was matched with **sad+fear**, and another where it was matched with
    just **sad**. If we marked all of these as cases where **joy** was a false negative
    and **sad** was a false positive, we would miss the fact that there appears to
    be a connection between **joy** and **sad**.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在移除共享案例后两者都不为空，很难看出要做什么。考虑前面的例子。在移除**爱**之后，我们在金标准中剩下**快乐**，在预测中剩下**悲伤+愤怒**。**快乐**是**悲伤**的错误，而**愤怒**是假阳性吗？**快乐**是**愤怒**的错误，而**悲伤**是假阳性吗？**快乐**是假阴性，而**悲伤**和**愤怒**都是假阳性吗？最后一个建议似乎不正确。假设我们有一个案例，其中**快乐**与**悲伤+愤怒**相匹配，另一个案例中它与**悲伤+恐惧**相匹配，还有一个案例中它与**悲伤**相匹配。如果我们将这些情况都标记为**快乐**是假阴性而**悲伤**是假阳性的案例，我们就会错过**快乐**和**悲伤**之间似乎存在联系的事实。
- en: We deal with this as follows. Suppose there are **G** items left in the Gold
    Standard and **P** items left in the prediction after the labels that appear on
    both have been removed. Here, for each **g** in the Gold Standard and each **p**
    in the prediction, we add *1/P* to *C[p][g]*. Doing this adds a total of **G**
    to the confusion matrix, thus acknowledging that the number of emotions in the
    Gold Standard has not been matched, with each item in the prediction seen being
    as equally likely to be the one that should be substituted for **g**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是这样处理的。假设在移除了两者都出现的标签之后，金标准中剩下**G**个项目，预测中剩下**P**个项目。在这里，对于金标准中的每个**g**和预测中的每个**p**，我们向**C[p][g]**中添加**1/P**。这样做总共向混淆矩阵中添加了**G**，从而承认金标准中的情绪数量尚未匹配，预测中的每个项目被视为有同等可能性是应该替换**g**的那个。
- en: The machinery for calculating modified confusion matrices is fairly intricate,
    and including it here would add very little to the preceding explanation. The
    code for this can be found in this book’s GitHub repository – for now, it is probably
    best just to note that when an item can be assigned multiple labels, the confusion
    matrix has to take account of situations where the Gold Standard and the prediction
    both assign multiple labels, with the sets being assigned being of different sizes
    and where some labels are common to both, some only appear in the Gold Standard
    and some only appear in the prediction.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 计算修改后的混淆矩阵的机制相当复杂，将其包含在这里对前面的解释增加的很少。这本书的GitHub仓库中有这个代码——目前，最好只是注意一下，当一个项目可以分配多个标签时，混淆矩阵必须考虑到金标准和预测都分配了多个标签的情况，分配的集合大小不同，并且有些标签两者都有，有些只出现在金标准中，有些只出现在预测中。
- en: The way we do this is not symmetric between the Gold Standard and the prediction,
    but it does provide confusion matrices that tell us something useful about what
    a given classifier is doing. For cases where there is exactly one item in the
    Gold Standard and one in the prediction, it collapses to the standard version,
    and where there are differing numbers in each, it does provide a picture of what
    is going on.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做的方式在金标准与预测之间并不对称，但它确实提供了混淆矩阵，这些矩阵告诉我们关于给定分类器正在做什么的一些有用的信息。对于金标准和预测中恰好有一个项目的情况，它将退化为标准版本，而对于每个中都有不同数量项目的情况，它确实提供了一幅正在发生的事情的图景。
- en: 'We will start by looking at the confusion matrix for CARER-EN using SVM as
    the classifier (the scores for SVMs and DNNs are very similar, and the confusion
    matrices are also very similar, so for convenience, we will use SVMs for the explorations
    here). The following matrix was obtained using a version of SVM which simply picks
    the most likely emotion for each tweet instead of using a threshold to try to
    work out whether there are any that are likely enough for them to be counted and
    if so, whether there are several that could count:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先查看使用SVM作为分类器（SVM和DNN的分数非常相似，混淆矩阵也非常相似，因此为了方便，我们将在这里使用SVM）的CARER-EN混淆矩阵。以下矩阵是使用一个简单的SVM版本获得的，该版本只是为每条推文选择最可能的情绪，而不是使用阈值来尝试确定是否有任何情绪足够可能被计算，如果是这样，是否有几个可以计算：
- en: '|  | **anger** | **fear** | **joy** | **love** | **sadness** | **surprise**
    |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 124 | 0 | 1 | 0 | 1 | 0 |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 1 | 128 | 0 | 0 | 0 | 1 |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 0 | 337 | 1 | 0 | 0 |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 0 | 1 | 73 | 0 | 0 |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| **sadness** | 0 | 1 | 1 | 0 | 293 | 0 |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| **surprise** | 0 | 0 | 0 | 0 | 0 | 37 |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: Figure 10.4 – Confusion matrix for CARER-EN, one emotion per tweet, with SVM
    as the classifier
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what you expect a confusion matrix to look like – the largest scores
    down the diagonal with a scattering of other assignments, with the biggest confusion
    being between **love** and **joy**. When we use the same algorithm for SEM11-EN,
    we get a very different picture:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **anti** | **disg** | **fear** | **joy** | **love** | **opti**
    | **pess** | **sadn** | **surp** | **trus** | **--** |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 311 | 2 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: '| **antici** | 8 | 65 | 1 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 12 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
- en: '| **disgus** | 10 | 3 | 36 | 1 | 3 | 0 | 0 | 0 | 0 | 0 | 1 | 182 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 9 | 0 | 0 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 34 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 11 | 2 | 1 | 1 | 186 | 0 | 0 | 0 | 0 | 0 | 1 | 39 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 1 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 40 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 7 | 1 | 0 | 2 | 2 | 0 | 20 | 1 | 0 | 0 | 0 | 119 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 19 | 0 | 0 | 0 | 26 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| **sadnes** | 9 | 3 | 1 | 1 | 3 | 0 | 0 | 1 | 16 | 0 | 0 | 119 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| **surpri** | 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 16 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 2 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 15 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| **--** | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 21 | 0 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: Figure 10.5 – Confusion matrix for SEM11-EN, one emotion per tweet, with SVM
    as the classifier
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'We get several false positives (places where nothing was expected but something
    was predicted – that is, the row headed with **--**: two of these have **anger**
    assigned and 21 **trust**). This happens because we have forced the classifier
    to choose something even in cases where the Gold Standard doesn’t expect anything.
    We also have a much larger number of places where there are false negatives (the
    column headed with **--**), where something was expected but nothing was found,
    generally because the Gold Standard had multiple labels and there was only one
    prediction. And there are numerous cases where the assignment is just wrong, with
    an awful lot of things being labeled as **anger** when they should be something
    else.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that if the classifier is forced to assign exactly one emotion
    per tweet, then it cannot help producing false positives (if the Gold Standard
    says that nothing should be assigned) and false negatives (if the Gold Standard
    says that more than one emotion should be assigned). If we look at the test set
    in detail, we will see that there are 23 tweets with no emotion assigned, which
    show up as false positives, and 645 tweets with more than one emotion assigned,
    which show up as 1,065 false negatives (because some of them have three or more
    emotions assigned). *There is nothing that can be done about this if our classifier
    assumes that there is one emotion* *per tweet.*
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，如果分类器被强制为每条推文分配恰好一种情绪，那么它无法避免产生误报（如果黄金标准说没有任何东西应该被分配）和漏报（如果黄金标准说应该分配多种情绪）。如果我们仔细查看测试集，我们会看到有23条没有分配情绪的推文，这些推文显示为误报，还有645条分配了多种情绪的推文，这些推文显示为1,065条漏报（因为其中一些被分配了三种或更多情绪）。*如果我们的分类器假设每条推文只有一个情绪，那么对此就无能为力了*。
- en: Suppose that we have *N* tweets, *X* of which have no emotion assigned to them
    and *Y* have more than one. In this case, there must be at least *X* false positives
    (one for each tweet that should have no labels assigned but the classifier assigns
    one) and at least *Y* false negatives (one for each tweet that should have more
    than one label assigned but the classifier only assigns one), meaning that the
    best possible Jaccard score is *(N-X)/((N-X)+X+Y)*. For the set of 772 tweets
    in SEM11-EN, this comes out as *(772-23)/(772-23+(1065+23)) = 0.41* (the number
    of false negatives is very high because of the preponderance of tweets that should
    be given more than two labels – this equation assumed that tweets were assigned
    zero, one, or two labels). This is a strict upper bound. No classifier that assigns
    exactly one label to each tweet can achieve a higher Jaccard score than 0.41 on
    this dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们共有*N*条推文，其中*X*条没有分配情绪，*Y*条分配了多种情绪。在这种情况下，至少会有*X*条误报（每条应该没有标签但分类器分配了一个标签的推文）和至少*Y*条漏报（每条应该有多个标签但分类器只分配了一个标签的推文），这意味着最佳可能的Jaccard分数是*(N-X)/((N-X)+X+Y)*。对于SEM11-EN中的772条推文集合，这个分数是*(772-23)/(772-23+(1065+23))
    = 0.41*（由于应该分配两个以上标签的推文占多数，漏报的数量非常高——这个方程假设推文被分配了零、一或两个标签）。这是一个严格的上限。没有任何分类器能够在这个数据集上实现高于0.41的Jaccard分数。
- en: The position is worse than that. Careful inspection of the diagonal shows that
    several emotions have good scores on the diagonal (**anger**, **joy**), while
    others have very poor scores on the diagonal and a lot of false negatives (**disgust**,
    **love**, **optimism**), with several emotions being confused with **anger**.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 位置比这还要糟糕。仔细检查对角线显示，一些情绪在对角线上有很好的分数（**愤怒**，**快乐**），而其他情绪在对角线上分数非常低，并且有很多漏报（**厌恶**，**爱情**，**乐观**），其中几种情绪与**愤怒**混淆。
- en: 'When we look at the KWT.M-AR dataset, we will see an output that is in some
    ways similar and is no more encouraging:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看KWT.M-AR数据集时，我们会看到一些方面相似但并不令人鼓舞的输出：
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **--** |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | **愤怒** | **厌恶** | **恐惧** | **快乐** | **爱情** | **乐观** | **悲观** | **拒绝**
    | **信任** | **--** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **anger** | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **愤怒** | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 0 |'
- en: '| **dissat** | 0 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 1 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| **不满** | 0 | 21 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 1 |'
- en: '| **fear** | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| **恐惧** | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |'
- en: '| **joy** | 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 12 | 0 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| **快乐** | 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 12 | 0 |'
- en: '| **love** | 0 | 0 | 0 | 0 | 50 | 0 | 0 | 0 | 44 | 3 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| **爱情** | 0 | 0 | 0 | 0 | 50 | 0 | 0 | 0 | 44 | 3 |'
- en: '| **optimi** | 0 | 0 | 0 | 0 | 0 | 22 | 0 | 0 | 17 | 0 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **乐观** | 0 | 0 | 0 | 0 | 0 | 22 | 0 | 0 | 17 | 0 |'
- en: '| **pessim** | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 2 | 2 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| **悲观** | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 2 | 2 |'
- en: '| **reject** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| **拒绝** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |'
- en: '| **trust** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 0 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **信任** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 0 |'
- en: '| **--** | 0 | 1 | 0 | 0 | 8 | 3 | 0 | 0 | 751 | 0 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **--** | 0 | 1 | 0 | 0 | 8 | 3 | 0 | 0 | 751 | 0 |'
- en: Figure 10.6 – Confusion matrix for KWT.M-AR, one emotion per tweet, with SVM
    as the classifier
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 – 使用SVM作为分类器的KWT.M-AR每条推文一个情绪的混淆矩阵
- en: 'This time, there are a massive number of false positives, reflecting the fact
    that these datasets have a very high proportion of cases where no emotion is assigned
    in the Gold Standard (763 out of 1,000 in the test set used here, with a maximum
    attainable F1-score of around 0.42). Again, this is inevitable – if a tweet should
    have no emotions assigned to it and the classifier is forced to assign one, then
    we will get a false positive. It is also worth noting that while there are non-trivial
    entries on the diagonal, a surprising number of cases have the correct assignment
    replaced by `angry: 2.38`, `fuming: 2.32`, `annoying: 2.31`, `revenge: 2.26`,
    … for `positivity: 1.82`, 💕`: 1.75`, `rejoice: 1.74`, `gift: 1.72`, `laughing:
    1.70` for `flat: 1.25`, `com: 1.19`, `cup: 1.06`, `need: 1.05`, `major: 1.05`.
    These are not words that are obviously associated with trust, and indeed the links
    between them and this emotion are not strong. So, when the classifier is forced
    to choose an emotion for a tweet that does not contain any words that are linked
    to specific emotions, it is likely to resort to choosing the one for which no
    such words are expected anyway.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '这次，有大量的假阳性，这反映了这些数据集在黄金标准中未分配情感的比例非常高（在本测试集中，有 763 个，占总数的 76.3%，最大可达到的 F1 分数约为
    0.42）。同样，这是不可避免的——如果一条推文应该没有任何情感分配，而分类器被迫分配一个，那么我们将得到一个假阳性。还值得注意的是，尽管对角线上的非平凡条目很多，但令人惊讶的是，有大量情况中正确的分配被替换为
    `angry: 2.38`，`fuming: 2.32`，`annoying: 2.31`，`revenge: 2.26`，……对于 `positivity:
    1.82`，💕`: 1.75`，`rejoice: 1.74`，`gift: 1.72`，`laughing: 1.70` 对于 `flat: 1.25`，`com:
    1.19`，`cup: 1.06`，`need: 1.05`，`major: 1.05`。这些不是显然与信任相关的词，而且它们与这种情感之间的联系并不强。因此，当分类器被迫为不包含任何与特定情感相关联的词汇的推文选择情感时，它很可能会选择那些本就不期望有此类词汇的情感。'
- en: 'If, as suggested previously, large numbers of tweets express either no emotion
    or several emotions, then we have to deal with these issues. There are several
    things we can try:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果，如前所述，大量推文表达的是没有情感或多种情感，那么我们必须处理这些问题。我们可以尝试以下几种方法：
- en: We can include an explicit “none-of-the-above” or “neutral” class to represent
    the fact that a tweet does not carry any emotional weight. This is the easiest
    thing to do for “zero emotion” cases, though it will not be ideal in cases where
    more than one emotion is assigned to a tweet.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以包含一个明确的“以上皆非”或“中性”类别来表示一条推文没有任何情感权重。对于“零情感”的情况，这是最容易做到的，尽管在推文被分配了多种情感的情况下，这并不理想。
- en: We can use the fact that some of the classifiers calculate a score for each
    emotion. We will look at this in more detail shortly.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以利用某些分类器为每种情绪计算分数的事实。我们将在稍后更详细地探讨这一点。
- en: 'We can train a set of binary classifiers – **joy** versus **not-joy**, **anger**
    versus **not-anger**, and so on. This will potentially deal with both kinds of
    cases: if each of these classifiers returns the negative version, we will get
    an overall zero assignment, and if more than one returns the positive version,
    we will get multiple assignments.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以训练一组二元分类器——**快乐**与**非快乐**，**愤怒**与**非愤怒**，等等。这可能会处理两种情况：如果这些分类器中的每一个都返回负版本，我们将得到一个整体零分配，如果有多个返回正版本，我们将得到多个分配。
- en: For the remainder of this chapter, we will concentrate on the SEM-11 and KWT
    datasets since these are the only ones with variable numbers of labels. If your
    training data assigns exactly one emotion to each tweet, and you want to have
    exactly one emotion assigned when running the classifier on live data, then one
    of the others will generally provide the best solution – LEXCLASSIFIER usually
    provides reasonably accurate results with very little training time, Transformers
    usually provide the best results but take a lot of training, and SVMs and DNNs
    come somewhere in between for both accuracy and training time.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章剩余部分，我们将专注于 SEM-11 和 KWT 数据集，因为这些是唯一具有可变标签数量的数据集。如果你的训练数据为每条推文分配了恰好一种情绪，并且你希望在运行分类器对实时数据进行分类时也恰好分配一种情绪，那么其他方法通常能提供最佳解决方案——LEXCLASSIFIER
    通常在极短的训练时间内提供相当准确的结果，Transformer 通常提供最佳结果但需要大量训练，而 SVM 和 DNN 在准确性和训练时间之间处于中等水平。
- en: Using “neutral” as a label
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用“中性”作为标签
- en: 'We can introduce **neutral** as a label simply by looking at the labels assigned
    by the Gold Standard and assigning **neutral** if no other emotion is assigned.
    This does not affect the CARER-EN set: nothing is assigned **neutral** in the
    training data, so no words are found to be associated with this label, so, in
    turn, nothing is assigned to it by the classifier. The effect on the SEM11-EN
    data is more interesting:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看Gold Standard分配的标签来简单地引入**中性**作为标签。这不会影响CARER-EN集：训练数据中没有分配**中性**，因此没有找到与该标签相关的单词，因此，反过来，分类器没有分配任何内容。对SEM11-EN数据的影响更有趣：
- en: '|  | **ange** | **anti** | **disg** | **fear** | **joy** | **love** | **opti**
    | **pess** | **sadn** | **surp** | **trus** | **neut** | **--** |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | **愤怒** | **反义** | **厌恶** | **恐惧** | **喜悦** | **爱情** | **乐观** | **悲观**
    | **悲伤** | **惊讶** | **信任** | **中性** | **--** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **anger** | 311 | 2 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **愤怒** | 311 | 2 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
- en: '| **antici** | 8 | 65 | 1 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 12 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| **期待** | 8 | 65 | 1 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 12 |'
- en: '| **disgus** | 10 | 3 | 36 | 1 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 182 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| **厌恶** | 10 | 3 | 36 | 1 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 182 |'
- en: '| **fear** | 9 | 0 | 0 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 34 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| **恐惧** | 9 | 0 | 0 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 34 |'
- en: '| **joy** | 11 | 2 | 1 | 1 | 186 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 39 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| **喜悦** | 11 | 2 | 1 | 1 | 186 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 39 |'
- en: '| **love** | 0 | 1 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 40 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **爱情** | 0 | 1 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 40 |'
- en: '| **optimi** | 7 | 1 | 0 | 2 | 2 | 0 | 20 | 1 | 0 | 0 | 0 | 1 | 118 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **乐观** | 7 | 1 | 0 | 2 | 2 | 0 | 20 | 1 | 0 | 0 | 0 | 1 | 118 |'
- en: '| **pessim** | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 19 | 0 | 0 | 0 | 0 | 26 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| **悲观** | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 19 | 0 | 0 | 0 | 0 | 26 |'
- en: '| **sadnes** | 9 | 3 | 1 | 1 | 3 | 0 | 0 | 1 | 16 | 0 | 0 | 0 | 119 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| **悲伤** | 9 | 3 | 1 | 1 | 3 | 0 | 0 | 1 | 16 | 0 | 0 | 0 | 119 |'
- en: '| **surpri** | 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 16 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| **惊讶** | 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 16 |'
- en: '| **trust** | 2 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 15 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| **信任** | 2 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 15 |'
- en: '| **neutra** | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 21 | 0 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| **中性** | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 21 | 0 |'
- en: Figure 10.7 – Confusion matrix for SEM11-EN, one emotion per tweet, with SVM
    as the classifier, and neutral as a label
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 - SEM11-EN的混淆矩阵，每条推文一个情绪，使用SVM作为分类器，中性作为标签
- en: There is very little change down the diagonal – that is, the classifier gets
    the same actual emotions right with or without **neutral** as a label; most things
    that ought to be classified as neutral are indeed labeled as such, with a couple
    mislabeled as **anger**; a few things are labeled as **neutral** when they should
    not be; and there are still a lot of false negatives because there were a lot
    of tweets that ought to have been given more than one label. These can’t be labeled
    **neutral** by the classifier since it can only assign one label to each tweet,
    so any tweet that ought to have more than one will contribute to the set of false
    negatives.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在对角线以下的变化非常小——也就是说，分类器在有或没有**中性**标签的情况下，对相同的实际情绪的判断是相同的；大多数本应被归类为中性的事物确实被标记为中性，有少数被错误地标记为**愤怒**；有几件事物被错误地标记为**中性**，而它们不应该被这样标记；由于有很多推文本应被赋予多个标签，因此仍然有很多假阴性。这些推文不能被分类器标记为**中性**，因为分类器只能为每条推文分配一个标签，所以任何本应被赋予多个标签的推文都将导致假阴性集的增加。
- en: 'The situation with the KWT examples is intriguing. These have large numbers
    of tweets with no emotion assigned to them, so we expect a lot of false positives
    if the classifier is set to assign one emotion per tweet. The confusion matrices
    for KWT.M-AR without and with **neutral** as a label are given here:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: KWT示例的情况很吸引人。这些示例中有大量没有分配情绪的推文，因此我们预计如果分类器设置为每条推文分配一个情绪，将会出现很多假阳性。这里给出了KWT.M-AR在有和没有**中性**标签时的混淆矩阵：
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **--** |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | **愤怒** | **厌恶** | **恐惧** | **喜悦** | **爱情** | **乐观** | **悲观** | **拒绝**
    | **信任** | **--** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **anger** | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 0 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **愤怒** | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 0 |'
- en: '| **dissat** | 0 | 19 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 4 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **不满** | 0 | 19 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 4 |'
- en: '| **fear** | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **恐惧** | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
- en: '| **joy** | 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 23 | 2 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| **喜悦** | 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 23 | 2 |'
- en: '| **love** | 0 | 0 | 0 | 0 | 82 | 0 | 0 | 0 | 47 | 1 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 0 | 0 | 0 | 0 | 37 | 0 | 0 | 18 | 2 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 2 | 0 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 2 | 0 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 12 | 1 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| **--** | 0 | 2 | 0 | 3 | 13 | 2 | 0 | 0 | 697 | 0 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: Figure 10.8 – Confusion matrix for KWT.M-AR, one emotion per tweet, with SVM
    as the classifier, and neutral not included
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: As before, most of the scores on the diagonal are quite good – that is, most
    of the time, the classifier assigns the right label where there is a label to
    be assigned. Inevitably, there are a large number of false positives, nearly all
    of which are assigned to **trust**. As before, in almost every case where the
    Gold Standard says there should be no label, the classifier has chosen **trust**,
    rather than distributing the false positives evenly. Again, what seems to be happening
    is that the classifier does not associate any words particularly strongly with
    **trust**, so when it is given a tweet without any very significant words in it,
    it decides it cannot be any of the other classes, for which there are stronger
    clues, so it chooses **trust**.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'When we allow **neutral** as a label, the situation changes quite dramatically.
    Now, nearly all the false positives are assigned to **neutral**, which is the
    most reasonable outcome. There is a smattering of false negatives because this
    dataset contains tweets with multiple labels, but the diagonal is much clearer
    – most emotions are assigned correctly and most cases with no emotion are assigned
    to **neutral**:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **neut** | **--** |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 0 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 0 | 19 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 4 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 0 | 24 | 1 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 0 | 0 | 0 | 81 | 0 | 0 | 0 | 0 | 48 | 1 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 0 | 0 | 0 | 0 | 37 | 0 | 0 | 0 | 18 | 2 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 2 | 0 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 2 | 0 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 5 | 1 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| **neutra** | 0 | 2 | 0 | 2 | 14 | 2 | 0 | 0 | 0 | 697 | 0 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: Figure 10.9 – Confusion matrix for KWT.M-AR, one emotion per tweet, with SVM
    as the classifier, and neutral included
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: So, using **neutral** as a label provides a partial solution to the problems
    of none and multiple labels, but it *cannot* provide a complete one. Even if a
    classifier were 100% accurate when it assigned labels to tweets that ought to
    have exactly one label and when it assigned **neutral** to ones that ought to
    have no labels, it must introduce false negatives for cases where there ought
    to be more than one label.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Now is a good time to introduce a new measure of performance. The fact that
    most of the cases that are neither classified as **neutral** nor given no label
    at all lie on the diagonal suggests that the gross classification assigned to
    a set of tweets might be useful for gauging opinion, even if the assignments to
    individual tweets are unreliable. In particular, false negatives may not matter
    too much when you’re trying to spot general trends, so long as the cases where
    the classifier does assign a label are consistent with the underlying reality.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是引入一个新的性能度量指标的好时机。大多数既没有被分类为**中性**也没有被赋予任何标签的案例都位于对角线上，这表明一组推文被分配的总体分类可能有助于衡量观点，即使对个别推文的分配是不可靠的。特别是，当你试图发现一般趋势时，假阴性可能不是很重要，只要分类器分配标签的案例与基本现实一致。
- en: It is, of course, not possible to spot whether something is a false negative
    or is a genuine instance of a neutral assignment, and then ask what the assignment
    for the false negatives should have been. If we could do that, then we would have
    trained the classifier to do it in the first place, and likewise with false positives.
    The best we can do is assess just how much we would be led astray if we accepted
    all the assignments that the classifier made at face value. So, we define the
    **proportionality** of the classifier as the cosine distance between the proportion
    of concrete tweets assigned to each emotion in the Gold Standard and the predictions
    (that is, ignoring tweets that are assigned to **neutral** or were not given any
    labels at all). The nearer to 1 this is, the more we can expect our classifier
    to give us a reliable overall picture, even if some of the individual assignments
    were wrong.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，不可能确定某件事是假阴性还是真正的中性分配实例，然后询问假阴性的分配应该是什么。如果我们能这样做，那么我们最初就会训练分类器来完成这项任务，同样对于假阳性也是如此。我们能做的最好的事情就是评估如果我们接受分类器做出的所有分配，我们可能会被误导到何种程度。因此，我们将分类器的**比例性**定义为金标准中分配给每种情感的推文比例与预测（即忽略分配为**中性**或完全没有标签的推文）之间的余弦距离。这个值越接近1，我们越可以期待我们的分类器给出一个可靠的总体情况，即使一些个别分配是错误的。
- en: To take a simple example, suppose that we had a dataset with the 11 emotions
    from the SEM11 data, with the same number of tweets assigned to **pessimism**
    and **sadness**, and that it got everything right except that it labeled exactly
    half the tweets that should be labeled as pessimistic as sad and exactly half
    the tweets that should be labeled as sad as pessimistic. In this case, proportionality
    would be perfect, and you could safely use the classifier to make judgments about
    the overall picture, even though you could not rely on it to tell you whether
    a given tweet was sad or pessimistic. Similarly, if half the tweets in every category
    were assigned no emotion, then proportionality would be perfect, whereas if half
    the tweets in the most common category were assigned neutral but no others were,
    then it would be fairly poor.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个简单的例子来说明，假设我们有一个包含SEM11数据中11种情感的语料库，悲观和悲伤的推文数量相同，并且它几乎完全正确，除了恰好将一半本应被标记为悲观的推文标记为悲伤，以及恰好将一半本应被标记为悲伤的推文标记为悲观。在这种情况下，比例将是完美的，你可以安全地使用这个分类器来对整体情况做出判断，即使你不能依赖它来告诉你某个推文是否悲伤或悲观。同样，如果每个类别中一半的推文都没有被分配情感，那么比例将是完美的，而如果最常见的类别中一半的推文被分配为中性，但没有其他推文被分配，那么这将是相当差的。
- en: 'From now on, we will do this for all the classifiers that are generated by
    the folds that we perform the training on, because the test sets associated with
    the individual folds are comparatively small (which is why we do cross-fold validation
    in the first place) and we lose quite a lot of instances by ignoring neutral and
    unassigned tweets. To calculate the proportionality, we can just count the number
    of tweets whose predicted/Gold Standard includes each emotion, *ignoring neutral
    and unassigned*, and normalize the result:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在起，我们将对我们在训练过程中生成的所有分类器都这样做，因为与单个折叠相关的测试集相对较小（这就是我们最初进行交叉折叠验证的原因），并且通过忽略中性和未分配的推文，我们失去了很多实例。为了计算比例性，我们只需计算预测/金标准中包含每种情感的推文数量，*忽略中性和未分配的推文*，并将结果归一化：
- en: '[PRE1]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can do this for the prediction and the Gold Standard and use cosine
    similarity to calculate the similarity between the two:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以对预测和金标准进行这样的操作，并使用余弦相似度来计算两者之间的相似度：
- en: '[PRE2]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For SEM11-EN, allowing an arbitrary number of emotions per tweet and using
    LEX as a classifier and `neutral` as a label, for instance, the proportions of
    the tweets that have each of the labels assigned in the prediction and the Gold
    Standard are `anger: 0.30`, `anticipation: 0.00`, `disgust: 0.31`, `fear: 0.02`,
    `joy: 0.25`, `love: 0.00`, `optimism: 0.04`, `pessimism: 0.00`, `sadness: 0.06`,
    `surprise: 0.00`, `trust: 0.00` and `anger: 0.18`, `anticipation: 0.06`, `disgust:
    0.17`, `fear: 0.06`, `joy: 0.15`, `love: 0.04`, `optimism: 0.12`, `pessimism:
    0.04`, `sadness: 0.12`, `surprise: 0.02`, `trust: 0.02`, respectively, for a proportionality
    score of 0.89\. If we use the same classifier with `neutral` as a label but allow
    exactly one label per tweet, the proportionality drops to 0.87.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'If we apply this to the KWT.M-AR dataset, we get `anger: 0.03`, `dissatisfaction:
    0.07`, `fear: 0.00`, `joy: 0.07`, `love: 0.69`, `optimism: 0.10`, `pessimism:
    0.00`, `rejection: 0.02`, `trust: 0.02` for the predictions and `anger: 0.04`,
    `dissatisfaction: 0.16`, `fear: 0.02`, `joy: 0.10`, `love: 0.43`, `optimism: 0.18`,
    `pessimism: 0.01`, `rejection: 0.01`, `trust: 0.05` for the Gold Standard, for
    a proportionality score of 0.94\. If we had not ignored the neutral/unassigned
    cases, the score would have been much higher, at 0.99, because of the huge preponderance
    of cases that are neutral in this dataset. So, we have a useful single number
    that gives us a handle on how reliable a classifier is for providing an overall
    picture, even if it fails to assign a concrete label to every tweet (that is,
    if some are either not assigned anything at all or are assigned **neutral**).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'This score will typically be quite high since, in most cases, most of the concrete
    scores lie on the diagonal. What matters is whether the distribution of neutral/unassigned
    cases follows the general distribution of the concrete cases – if it does, then
    the classifier will be useful for assessing general trends, even if it does sometimes
    fail to assign concrete labels when it should. So, we will use this measure in
    addition to Jaccard to assess the classifiers in the remainder of this chapter.
    The tables in *Figures 10.10* and *10.12* show what happens to the proportionality
    for various classifiers when we add **neutral** as a label, sticking to assigning
    exactly one label per tweet. As a reference point, we will start by looking at
    what happens if we specify that each classifier returns without using **neutral**.
    As before, the classifier with the best Jaccard score is marked in bold:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **NB** | **SVM** | **DNN** |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | 0.224 (0.813) | 0.229 (0.690) | 0.223 (0.771) | *** 0.242
    (****0.677) *** |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | *** 0.247 (****0.824) *** | 0.216 (0.667) | 0.204 (0.736)
    | 0.207 (0.613) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | 0.225 (0.799) | *** 0.226 (****0.788) *** | 0.215 (0.888)
    | 0.222 (0.774) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | *** 0.208 (****0.973) *** | 0.108 (0.352) | 0.078 (0.207)
    | 0.026 (0.148) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: Figure 10.10 – Jaccard and proportionality (in brackets), one label per tweet,
    neutral not included
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.10* shows that if we simply use the original classifiers unchanged
    – that is, with one emotion per tweet and without **neutral** as a label – we
    get fairly poor Jaccard scores, but the proportionality scores for LEX range from
    reasonable to pretty good, with the other classifiers generally doing worse on
    this metric. The proportionality score for LEX on the KWT.M-AR dataset in particular
    is massively better than the same score for any of the other classifiers on this
    dataset. The key here is that NB, SVM, and DNN assign nearly all the cases that
    should have been labeled as **neutral** to **trust** because these cases lack
    any of the distinguishing words that are common in the more clearly marked emotions,
    whereas LEX distributes them more closely in line with the marked cases. It is
    worth noting that the classifier with the best score for a given dataset does
    not always produce the best proportionality for that set:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **--** |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 9 | 19 | 0 | 0 | 11 | 2 | 0 | 0 | 0 | 14 |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 4 | 133 | 0 | 2 | 71 | 1 | 0 | 0 | 0 | 0 |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 3 | 2 | 0 | 12 | 2 | 0 | 0 | 0 | 2 |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 1 | 6 | 0 | 53 | 50 | 8 | 0 | 0 | 1 | 18 |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 7 | 0 | 8 | 548 | 12 | 0 | 0 | 0 | 0 |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 5 | 0 | 1 | 44 | 180 | 0 | 0 | 1 | 2 |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 4 | 0 | 0 | 7 | 2 | 2 | 0 | 1 | 1 |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 2 | 0 | 0 | 3 | 0 | 0 | 3 | 0 | 2 |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 1 | 9 | 0 | 2 | 28 | 8 | 0 | 0 | 13 | 3 |  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| **--** | 30 | 880 | 4 | 159 | 2008 | 577 | 2 | 1 | 61 | 0 |  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: Figure 10.11 – Confusion matrix for KWT.M-AR, one label per tweet, with LEX
    as the classifier, and neutral not included
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'When we allow **neutral** as a label, NB and SVM can choose this as the class
    with the least distinctive terms and hence assign cases that should be **neutral**
    to it, leading to a massive improvement in both Jaccard and proportionality for
    these classifiers:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **NB** | **SVM** | **DNN** |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | **0.222 (0.813)** | **0.227 (0.690)** | **0.222 (0.768)**
    | *** 0.239 (****0.677) *** |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | *** 0.246 (****0.824) *** | **0.216 (0.666)** | **0.204 (0.736)**
    | **0.207 (0.615)** |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | **0.221 (0.800)** | *** 0.222 (****0.787) *** | **0.211 (0.885)**
    | **0.216 (0.774)** |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | **0.608 (0.984)** | **0.510 (0.986)** | *** 0.632 (****0.992)
    *** | **0.595 (0.905)** |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: Figure 10.12 – Jaccard and proportionality, one label per tweet, and neutral
    included
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: So, we can see that using proportionality as a metric allows us to spot general
    trends. Most of our classifiers work better on multi-label datasets if we allow
    **neutral** as a label, particularly when looking at proportionality, but LEX
    performs quite well even without **neutral** as a label.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Thresholds and local thresholds
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next option to be explored is the use of thresholds. As we have seen, most
    of our classifiers provide scores for every option for each tweet, with the default
    setting being to choose the option with the highest score. In [*Chapter 6*](B18714_06.xhtml#_idTextAnchor134),
    *Naive Bayes*, we saw that assuming that our classifier will assign exactly one
    label to each tweet puts quite a tight upper bound on how well it can perform
    and that instead of doing that, we can set a threshold and say that everything
    that exceeds that threshold should be accepted as a label.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following tweet: “*Hi guys ! I now do lessons via Skype ! Contact
    me for more info . # skype # lesson # basslessons # teacher # free lesson # music
    # groove # rock #* *blues*.”'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: The Gold Standard assigns this the scores (‘anger’, 0), (‘anticipation’, 1),
    (‘disgust’, 0), (‘fear’, 0), (‘joy’, 1), (‘love’, 0), (‘optimism’, 0), (‘pessimism’,
    0), (‘sadness’, 0), (‘surprise’, 0), (‘trust’, 0), so it should be labeled as
    **anticipation+joy**.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes assigns this tweet the scores (‘anger’, ‘0.00’), (‘anticipation’,
    ‘0.88’), (‘disgust’, ‘0.00’), (‘fear’, ‘0.00’), (‘joy’, ‘0.11’), (‘love’, ‘0.00’),
    (‘optimism’, ‘0.00’), (‘pessimism’, ‘0.00’), (‘sadness’, ‘0.00’), (‘surprise’,
    ‘0.00’), (‘trust’, ‘0.00’), so if we set the threshold at 0.1, we would get **anticipation+joy**,
    if we set it at 0.2, we would just get **anticipation**, and if we set it at 0.9,
    we would get nothing.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: For the same tweet, SVM assigns (‘anger’, ‘-0.77’), (‘anticipation’, ‘0.65’),
    (‘disgust’, ‘-2.64’), (‘fear’, ‘-1.67’), (‘joy’, ‘-0.99’), (‘love’, ‘-1.93’),
    (‘optimism’, ‘-3.52’), (‘pessimism’, ‘-1.61’), (‘sadness’, ‘-2.58’), (‘surprise’,
    ‘-1.47’), (‘trust’, ‘-3.86’). So, this time, if we set the threshold to -1, we
    would get **anger+anticipation+joy**, if we set it to 0, we would just get **anticipation**,
    and if we set it to 1, we would get nothing.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: So, using a threshold will let us generate zero or more labels. We have to optimize
    the threshold, but we can do that simply by finding the smallest and greatest
    values that are assigned to any label in any tweet and incrementing evenly between
    these. The `bestThreshold` function, which was provided in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116),
    *Sentiment Lexicons and Vector-Space Models*, will work just as well with the
    raw scores produced by Naive Bayes, SVMs, and DNNs as it did there.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'If we contrast the scores that were obtained previously by requiring a single
    label and the ones we obtained using a threshold to allow zero or more labels
    on the crucial datasets, we will see that, overall, the latter produces better
    results:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **NB** | **SVM** | **DNN** |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | * 0.347 (0.898) * | 0.270 (0.764) | 0.250 (0.828) | 0.273
    (0.729) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | * 0.377 (0.940) * | 0.257 (0.761) | 0.224 (0.798) | 0.246
    (0.731) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | * 0.266 (0.890) * | 0.250 (0.837) | 0.228 (0.924) | 0.238
    (0.791) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | * 0.691 (0.990) * | 0.522 (0.988) | 0.631 (0.998) | 0.604
    (0.935) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: Figure 10.13 – Zero or more emotions per tweet, with optimal global thresholds
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'The scores here are much better than they were with the simple classifiers,
    with the proportionality scores all but perfect in some cases. There is, however,
    still some way to go if we want to get the labels for individual tweets right,
    rather than just getting a good overall picture. The next move is to set a threshold
    for each label, rather than for the dataset as a whole. We will adapt `bestThreshold`
    from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons and
    Vector Space Models* so that we can assign individual thresholds to labels. We
    will make two changes to the original definition:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: We will split it into two cases – one for calculating a global threshold (a
    single threshold for all cases) and another for calculating a local threshold
    for each label.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the original version, we looked at every column in the data, finding the
    minimum and maximum values that occur anywhere, and then looked at the predicted
    values for every column to calculate the Jaccard scores for each potential threshold.
    To calculate local thresholds, we just want to look at one column at a time. We
    can deal with both cases if we specify a range of columns, from `start` to `end`,
    to look at. For the global case, we must set `start=0` and `end=sys.maxsize`;
    for the case where we want to choose the best threshold for the `i` column, we
    must set `start=i` and `end=i+1`. This lets us use the same machinery for calculating
    both types of thresholds. The major changes to the original are highlighted in
    the following updated version:'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The results of allowing the classifiers to choose different thresholds for
    different labels are shown here:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **NB** | **SVM** | **DNN** |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | * 0.371 (0.987) * | 0.271 (0.827) | 0.270 (0.809) | 0.277
    (0.811) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | 0.371 (0.965) | 0.255 (0.854) | 0.236 (0.809) | 0.238 (0.795)
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | * 0.267 (0.962) * | 0.192 (0.674) | 0.222 (0.983) | 0.202
    (0.852) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | 0.681 (0.989) | 0.217 (0.163) | 0.615 (0.987) | 0.226 (0.167)
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: Figure 10.14 – Zero or more emotions per tweet, with optimal local thresholds
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The proportionality scores for LEX have all improved, with LEX now easily giving
    the best proportionality score for SEM11-EN and Naive Bayes now reverting to choosing
    neutral/unassigned for nearly everything for KWT.U-AR and most of the other scores
    decreasing slightly, though the Jaccard scores have only improved for SEM11-EN
    and SEM11-ES. Yet again, different classifiers are better suited to different
    datasets and different tasks.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Multiple independent classifiers
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using either LEX with optimal local thresholds or Naïve Bayes or SVM with an
    optimal global threshold with `MULTICLASSIFIER` class from [*Chapter 7*](B18714_07.xhtml#_idTextAnchor144),
    *Support Vector Machines*, to allow different kinds of classifiers to be used
    at the lower level. The key change here from the original is that we specify what
    classifier to use in the set of optional arguments, rather than assuming that
    we will be using `SVMCLASSIFIER`:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This will make two-way classifiers for **anger** versus **not-anger**, **love**
    versus **not-love**, and so on using the specified kind of sub-classifier. For
    the individual classifiers, there is no point in allowing more than one label
    since while a tweet can satisfy both **love** and **joy**, or both **anger** and
    **fear**, it does not make any sense to allow a tweet to satisfy **anger** and
    **not-anger**. We can still get multiple labels overall if, for instance, both
    **love** versus **not-love** and **joy** versus **not-joy** are satisfied, and
    we can still get zero labels if all the negative labels are chosen, but it makes
    no sense to allow the individual classifiers to assign zero or more than one label.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'As ever, there is a wide range of settings for the various subclassifiers.
    The main multiclassifier just combines the results of the individual subclassifiers
    and hence has no significant parameters beyond the choice of what to use as the
    subclassifiers, but the individual subclassifiers have the usual range of options.
    The following tables report the scores using just one label per subclassifier
    but varying whether or not neutral is allowed as a label:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **MULTI-LEX** | **MULTI-NB** | **MULTI-SVM** | **MULTI-DNN** |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | 0.348 (0.868) | *0.441 (0.996) * | 0.385 (1.000) | 0.422 (0.991)
    |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | 0.363 (0.878) | 0.376 (0.996) | 0.314 (0.997) | 0.333 (0.956)
    |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | 0.260 (0.852) | * 0.296 (0.993) * | 0.256 (0.995) | 0.236
    (0.936) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | 0.304 (0.979) | 0.236 (0.989) | 0.294 (0.996) | 0.182 (0.938)
    |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: Figure 10.15 (a) – 0 or more emotions per tweet, multiple classifiers, -neutral
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **MULTI-LEX** | **MULTI-NB** | **MULTI-SVM** | **MULTI-DNN** |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | 0.342 (0.861) | 0.438 (0.996) | 0.381 (1.000) | 0.419 (0.991)
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | 0.363 (0.879) | 0.376 (0.996) | 0.313 (0.997) | 0.333 (0.956)
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | 0.256 (0.836) | 0.290 (0.993) | 0.250 (0.995) | 0.234 (0.938)
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | 0.665 (0.984) | 0.546 (0.989) | 0.617 (0.996) | 0.599 (0.950)
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: Figure 10.15 (b) – 0 or more emotions per tweet, multiple classifiers, +neutral
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall picture here is that using multiple independent classifiers to
    decide whether a tweet should have a given label produces the best proportionality
    results yet for zero-to-many datasets. Although the Jaccard scores are only improved
    for SEM11-EN and SEM11-ES, there is considerable variation between the performance
    of the different classifiers under this regime. All four classifiers do marginally
    better on the SEM11 cases when we do not allow **neutral** as a label, but they
    all do substantially better on the KWT.M-AR dataset when we do allow **neutral**.
    This is slightly surprising, given that the individual classifiers are allowed
    to choose not to assign their labels, so it is perfectly possible to get a “no
    label assigned” outcome for a given tweet, even without allowing **neutral**.
    *Figure 10**.16* shows how scores vary as we look at the +/-neutral classifiers
    for KWT.M-AR:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    | **Proportionality** |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-LEX, -****NEUTRAL** | 0.400 | 0.559 | 0.467 | 0.319 | 0.304 | 0.979
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-LEX, +****NEUTRAL** | 0.731 | 0.881 | 0.799 | 0.817 | 0.665 | 0.984
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-NB, -****NEUTRAL** | 0.338 | 0.441 | 0.383 | 0.247 | 0.236 | 0.989
    |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-NB, +****NEUTRAL** | 0.645 | 0.781 | 0.707 | 0.714 | 0.546 | 0.989
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-SVM,** **-****NEUTRAL** | 0.598 | 0.367 | 0.455 | 0.294 | 0.294 |
    0.996 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-SVM, +****NEUTRAL** | 0.764 | 0.763 | 0.763 | 0.747 | 0.617 | 0.996
    |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-DNN, -****NEUTRAL** | 0.255 | 0.389 | 0.308 | 0.194 | 0.182 | 0.938
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| **MULTI-DNN, +****NEUTRAL** | 0.725 | 0.776 | 0.750 | 0.758 | 0.599 | 0.950
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: Figure 10.16 – KWT.M-AR, multiple classifiers, without and with neutral
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: In every case, both recall and precision go up when we allow neutral as a label.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the confusion matrices for Naive Bayes (the others are very similar,
    but Naive Bayes gives the best overall results and hence is the most interesting)
    is revealing:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **--** |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 31 | 7 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 0 | 111 | 0 | 0 | 15 | 0 | 0 | 0 | 0 | 94 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 2 | 3 | 0 | 1 | 0 | 0 | 0 | 0 | 15 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 6 | 0 | 56 | 12 | 6 | 0 | 0 | 0 | 66 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 2 | 0 | 4 | 327 | 4 | 0 | 0 | 0 | 155 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 2 | 0 | 3 | 0 | 123 | 0 | 0 | 0 | 70 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 1 | 2 | 0 | 0 | 1 | 0 | 2 | 0 | 0 | 15 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 6 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 0 | 4 | 0 | 1 | 2 | 0 | 0 | 0 | 26 | 50 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| **--** | 65 | 329 | 5 | 182 | 527 | 318 | 6 | 3 | 67 | 0 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: Figure 10.17 (a) – Confusion matrix, multiclassifiers with NB as a subclassifier,
    KWT.M-AR, -neutral
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ange** | **diss** | **fear** | **joy** | **love** | **opti** | **pess**
    | **reje** | **trus** | **neut** | **--** |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| **anger** | 35 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23 | 4 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| **dissat** | 0 | 118 | 0 | 0 | 12 | 0 | 0 | 0 | 0 | 92 | 4 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| **fear** | 0 | 1 | 3 | 0 | 1 | 0 | 0 | 0 | 0 | 13 | 3 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| **joy** | 0 | 3 | 0 | 58 | 12 | 3 | 0 | 0 | 0 | 67 | 8 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| **love** | 0 | 2 | 0 | 2 | 343 | 2 | 0 | 0 | 1 | 138 | 11 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| **optimi** | 0 | 1 | 0 | 1 | 0 | 126 | 0 | 0 | 0 | 67 | 3 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| **pessim** | 0 | 2 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 18 | 0 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| **reject** | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 7 | 0 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| **trust** | 0 | 2 | 0 | 0 | 1 | 0 | 0 | 0 | 27 | 54 | 0 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| **neutra** | 2 | 14 | 0 | 15 | 162 | 64 | 0 | 0 | 5 | 3521 | 0 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| **--** | 60 | 333 | 5 | 165 | 398 | 273 | 6 | 3 | 63 | 378 | 0 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: Figure 10.17 (b) – Confusion matrix, multiclassifiers with NB as a subclassifier,
    KWT.M-AR, -neutral
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: There are small differences between the scores in the main sections of the two
    tables – slightly better scores on the diagonal and slightly lower confusion between
    other labels – but the crucial difference is that, as before, when we do not have
    **neutral** as a label, we get a large number of false positives. Using **neutral**
    as a label reduces the number of false positives, even when we have multiple independent
    classifiers that each make their recommendations without looking at the results
    of the other classifiers.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Over the past few chapters, we have looked at a wide range of classifiers and
    compared their performance on a range of datasets. Now, it is time to reflect
    on what we have learned. Our final table of the best classifiers for the datasets
    we have looked at is as follows:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **SVM** | **SNN** | **Transformers** | **MULTI-NB** | **LEX, MULTI** |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-EN** | 0.845 | 0.829 | * 0.927 |  |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | 0.224 | 0.242 | 0.418 | * 0.438 | 0.347 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| **WASSA-EN** | * 0.770 | 0.737 | 0.753 |  |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| **CARER-EN** | 0.770 | * 0.820 | 0.816 |  |  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| **IMDB-EN** | 0.736 | 0.793 | * 0.826 |  |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-AR** | 0.514 | 0.504 | * 0.710 |  |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-AR** | 0.216 | 0.221 | 0.359 | * 0.412 | 0.377 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| **KWT.M-AR** | 0.631 | 0.028 | 0.053 | 0.537 | * 0.691 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-ES** | 0.412 | 0.337 | * 0.663 |  |  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-ES** | 0.226 | 0.221 | * 0.340 | 0.294 | 0.266 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: Figure 10.18 – Overall best classifiers
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Given what we have seen, there are several general observations we can make:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: The difference in performance of the various algorithms is not huge. For each
    dataset and each configuration of settings, the performances of the best classifiers
    are very similar, so it may be sensible to take training time as well as scores
    on the various measures into account when choosing a classifier. In particular,
    no classifier is the best in every case, and sometimes, the very simple algorithms
    (LEX and Naive Bayes) are as good as or even better than the more complex ones.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Datasets where an individual tweet can be assigned zero, one, or more labels
    are considerably more challenging than ones where each tweet is given exactly
    one label. There is, in fact, a clear upper bound to the performance of classifiers
    that do assign exactly one label per tweet to these datasets, and the best results
    are obtained by reconsidering the way that the classifiers are used. Some classifiers
    are more suitable than others for this kind of task, and this must be taken into
    account when you’re choosing a classifier for datasets of this kind. Again, it
    is worth considering training time when choosing this: training a single classifier
    and then setting *N* individual thresholds is considerably quicker than training
    *N* classifiers, and in at least some cases, the difference in performance is
    quite small.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also investigated a variety of preprocessing steps, including the use of
    different tokenizers and stemmers, and we looked at using algorithms that can
    suggest “similar” words to replace words in the target tweets that do not appear
    in the training data. All these tweaks pay off in some situations and not in others.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We cannot overstate this: **there is no silver bullet**. Different tasks require
    different classifiers, and you should always investigate a range of classifiers
    before deciding which one you want to use. In particular, if you are working with
    a multi-label dataset, you should consider one of the algorithms from this chapter.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: When training a classifier, it is a good idea to look at the confusion matrix
    for the labels that it assigns. Several classifiers, particularly for the datasets
    with large numbers of zero assignments, produce quite good F1 and Jaccard scores
    simply by choosing the most common class (that is, **neutral**!) in every case.
    And when choosing between classifiers, it is a good idea to consider the end task
    that the classifier is needed for. If what you want is to get a feel for an opinion
    on some topic, without worrying too much about what individual tweets say about
    it, then using proportionality as a metric can be a helpful tool. We will use
    this in the next chapter, where we will look at the link between emotions expressed
    in tweets and real-life events over a certain period.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Part 4:Case Study
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Part 3* discussed a range of approaches to the task of EA and compared their
    effectiveness on a set of standard datasets. In this final part, we investigate
    how well these approaches work on real-world data that is not connected to the
    standard sets, looking at how changes in the emotions expressed in tweets reflect
    key real-world events. We also examine how robust the various approaches are when
    applied to novel data, showing how approaches that work well when the test data
    is drawn from the same population as the training data can be fragile when applied
    to novel data.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapter:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B18714_11.xhtml#_idTextAnchor202), *Case Study – The Qatar Blockade*'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
