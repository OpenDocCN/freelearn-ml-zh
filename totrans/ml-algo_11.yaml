- en: Introduction to Recommendation Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine an online shop with thousands of articles. If you're not a registered
    user, you'll probably see a homepage with some highlights, but if you've already
    bought some items, it would be interesting if the website showed products that
    you would probably buy, instead of a random selection. This is the purpose of
    a recommender system, and in this chapter, we're going to discuss the most common
    techniques to create such a system.
  prefs: []
  type: TYPE_NORMAL
- en: The basic concepts are users, items, and ratings (or an implicit feedback about
    the products, like the fact of having bought them). Every model must work with
    known data (like in a supervised scenario), to be able to suggest the most suitable
    items or to predict the ratings for all the items not evaluated yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to discuss two different kinds of strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: User or content based
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaborative filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first approach is based on the information we have about users or products
    and its target is to associate a new user with an existing group of peers to suggest
    all the items positively rated by the other members, or to cluster the products
    according to their features and propose a subset of items similar to the one taken
    into account. The second approach, which is a little bit more sophisticated, works
    with explicit ratings and its purpose is to predict this value for every item
    and every user. Even if collaborative filtering needs more computational power
    as, nowadays, the great availability of cheap resources, allows using this algorithm
    with millions of users and products to provide the most accurate recommendations
    in real-time. The model can also be retrained or updated every day.
  prefs: []
  type: TYPE_NORMAL
- en: Naive user-based systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this first scenario, we assume that we have a set of users represented by
    feature vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46d9a100-5870-4a68-b406-0ac4cc3a5d91.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Typical features are age, gender, interests, and so on. All of them must be
    encoded using one of the techniques discussed in the previous chapters (for example,
    they can be binarized). Moreover, we have a set of items:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/538f4289-3d0a-4275-8d41-371d3413d5f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s assume also that there is a relation which associates each user with
    a subset of items (bought or positively reviewed), items for which an explicit
    action or feedback has been performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb0e6274-4416-4300-af08-32961368bafc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In a user-based system, the users are periodically clustered (normally using
    a **k-nearest neighbors** approach), and therefore, considering a generic user
    *u* (also new), we can immediately determine the ball containing all the users
    who are similar (therefore neighbors) to our sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09add924-472f-4bf6-936c-dfb641de04e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we can create the set of suggested items using the relation
    previously introduced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fecf5a0-21aa-464d-bd53-0ddb651235fe.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, the set contains all the unique products positively rated or
    bought by the neighborhood. I've used the adjective naive because there's a similar
    alternative that we're going to discuss in the section dedicated to collaborative
    filtering.
  prefs: []
  type: TYPE_NORMAL
- en: User-based system implementation with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For our purposes, we need to create a dummy dataset of users and products:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We assume that we have 1,000 users with four features represented by integer
    numbers bounded between 0 and 4 or 5\. It doesn't matter what they mean; their
    role is to characterize a user and allow for clustering of the set.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the products, we also need to create the association:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We assume that we have 20 different items (from 1 to 20; 0 means that a user
    didn''t buy anything) and an association matrix where each user is linked to a
    number of products bounded between 0 and 5 (maximum). For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58fec9ca-aca7-45b5-a60a-a1c34802b4ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we need to cluster the users using the `NearestNeighbors` implementation
    provided by scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We have selected to have 20 neighbors and a Euclidean radius equal to 2\. This
    parameter is used when we want to query the model to know which items are contained
    in the ball whose center is a sample and with a fixed radius. In our case, we
    are going to query the model to get all the neighbors of a test user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to build the recommendation list using the association matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For each neighbor, we retrieve the products he/she bought and perform a union,
    avoiding the inclusion of items with zero value (meaning no product) and double
    elements. The result is a list (not sorted) of suggestions that can be obtained
    almost in real time for many different systems. In some cases, when the number
    of users or items is too huge, it's possible to limit the list to a fixed number
    of elements and to reduce the number of neighbors. This approach is also naive
    because it doesn't consider the actual distance (or similarity) between users
    to weigh the suggestions. It's possible to consider the distance as a weighing
    factor, but it's simpler to adopt the collaborative filtering approach which provides
    a more robust solution.
  prefs: []
  type: TYPE_NORMAL
- en: Content-based systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is probably the simplest method and it''s based only on the products,
    modeled as feature vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92064032-622a-45c1-9ef4-63e267b14286.png)'
  prefs: []
  type: TYPE_IMG
- en: Just like the users, the features can also be categorical (indeed, for products
    it's easier), for example, the genre of a book or a movie, and they can be used
    together with numerical values (like price, length, number of positive reviews,
    and so on) after encoding them.
  prefs: []
  type: TYPE_NORMAL
- en: Then a clustering strategy is adopted, even if the most used is **k-nearest
    neighbors** as it allows controlling the size of each neighborhood to determine,
    given a sample product, the quality and the number of suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using scikit-learn, first of all we create a dummy product dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we have 1000 samples with four integer features bounded between
    0 and 100\. Then we proceed, as in the previous example, towards clustering them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, it''s possible to query our model with the method `radius_neighbors()`,which
    allows us to restrict our research only to a limited subset. The default radius
    (set through the parameter `radius`) is 5.0, but we can change it dynamically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Of course, when trying these examples, the number of suggestions can be different,
    as we are using random datasets, so I suggest trying different values for the
    radius (in particular when using different metrics).
  prefs: []
  type: TYPE_NORMAL
- en: 'When clustering with **k-nearest neighbors**, it''s important to consider the
    metric adopted for determining the distance between the samples. The default for
    scikit-learn is the Minkowski distance, which is a generalization of Euclidean
    and Manhattan distance, and is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c39c54b7-284d-4d24-bac8-b7614e489373.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The parameter *p* controls the type of distance and the default value is 2,
    so that the resulting metric is a classical Euclidean distance. Other distances
    are offered by SciPy (in the package `scipy.spatial.distance`) and include, for
    example, the **Hamming** and **Jaccard** distances. The former is defined as the
    disagree proportion between two vectors (if they are binary this is the normalized
    number of different bits). For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: It means there's a disagree proportion of 40 percent, or, considering that both
    vectors are binary, there 4 different bits (out of 10). This measure can be useful
    when it's necessary to emphasize the presence/absence of a particular feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Jaccard distance is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f566e77a-f605-4e6d-abb8-3a3467235362.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s particularly useful to measure the dissimilarity between two different
    sets (*A* and *B*) of items. If our feature vectors are binary, it''s immediate
    to apply this distance using Boolean logic. Using the previous test values, we
    get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This measure is bounded between 0 (equal vectors) and 1 (total dissimilarity).
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the Hamming distance, it can be very useful when it''s necessary to
    compare items where their representation is made up of binary states (like present/absent,
    yes/no, and so forth). If you want to adopt a different metric for **k-nearest
    neighbors**, it''s possible to specify it directly using the `metric` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Model-free (or memory-based) collaborative filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with the user-based approach, let''s consider having two sets of elements:
    users and items. However, in this case, we don''t assume that they have explicit
    features. Instead, we try to model a user-item matrix based on the preferences
    of each user (rows) for each item (columns). For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a0283ec-a54b-4cb6-b25c-9c08e1974a30.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the ratings are bounded between 1 and 5 (0 means no rating), and
    our goal is to cluster the users according to their rating vector (which is, indeed,
    an internal representation based on a particular kind of feature). This allows
    producing recommendations even when there are no explicit pieces of information
    about the user. However, it has a drawback, called **cold-startup**, which means
    that when a new user has no ratings, it's impossible to find the right neighborhood,
    because he/she can belong to virtually any cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Once the clustering is done, it's easy to check which products (not rated yet)
    have the higher rating for a given user and therefore are more likely to be bought.
    It's possible to implement a solution in scikit-learn as we've done before, but
    I'd like to introduce a small framework called **Crab** (see the box at the end
    of this section) that simplifies this process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to build the model, we first need to define the user-item matrix as
    a Python dictionary with the structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'A missing value in a user internal dictionary means no rating. In our example,
    we consider 5 users with 5 items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the user-item matrix has been defined, we need to pick a metric and therefore,
    a distance function *d(u[i], u[j])*, to build a similarity matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29ff8a1a-9a66-41c3-bf82-f12ad7a731c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using Crab, we do this in the following way (using a Euclidean metric):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'There are many metrics, like Pearson or Jaccard, so I suggest visiting the
    website ([http://muricoca.github.io/crab](http://muricoca.github.io/crab)) to
    retrieve further information. At this point, it''s possible to build the recommendation
    system (based on the k-nearest neighbors clustering method) and test it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'So the recommender suggests the following predicted rating for user 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Item 2**: 3.6 (which can be rounded to 4.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Item 5**: 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Item 3**: 2.5 (which can be rounded to 3.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When running the code, it''s possible to see some warnings (Crab is still under
    development); however, they don''t condition the functionality. If you want to
    avoid them, you can use the `catch_warnings()` context manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s possible to suggest all the items, or limit the list to the higher ratings
    (so, for example, avoiding the item 3). This approach is quite similar to the
    user-based model. However, it''s faster (very big matrices can be processed in
    parallel) and it doesn''t take care of details that can produce misleading results.
    Only the ratings are considered as useful features to define a user. Like model-based
    collaborative filtering, the cold-startup problem can be addressed in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Asking the user to rate some items (this approach is often adopted because it's
    easy to show some movie/book covers, asking the user to select what they like
    and what they don't).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Placing the user in an average neighborhood by randomly assigning some mean
    ratings. In this approach, it's possible to start using the recommendation system
    immediately. However, it's necessary to accept a certain degree of error at the
    beginning and to correct the dummy ratings when the real ones are produced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Crab is an open-source framework for building collaborative filtering systems.
    It''s still under development and therefore, doesn''t implement all possible features.
    However, it''s very easy to use and is quite powerful for many tasks. The home
    page with installation instructions and documentation is: [http://muricoca.github.io/crab/index.html](http://muricoca.github.io/crab/index.html).
    Crab depends on scikits.learn, which still has some issues with Python 3\. Therefore,
    I recommend using Python 2.7 for this example. It''s possible to install both
    packages using pip: `pip install -U scikits.learn` and `pip install -U crab`.'
  prefs: []
  type: TYPE_NORMAL
- en: Model-based collaborative filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is currently one of the most advanced approaches and is an extension of
    what was already seen in the previous section. The starting point is always a
    rating-based user-item matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4a3a7f1-4ccd-4132-9a8c-3ff4fd0ca8a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, in this case, we assume the presence of **latent factors** for both
    the users and the items. In other words, we define a generic user as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74322669-b70e-4833-9bbd-ae36981b2a83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A generic item is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92ac9795-5486-4ff0-980f-bba392ddcb95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We don''t know the value of each vector component (for this reason they are
    called latent), but we assume that a ranking is obtained as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e38e388a-30a6-4f06-8a31-b82a19cc5da0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So we can say that a ranking is obtained from a latent space of rank *k*, where
    *k* is the number of latent variables we want to consider in our model. In general,
    there are rules to determine the right value for *k*, so the best approach is
    to check different values and test the model with a subset of known ratings. However,
    there''s still a big problem to solve: finding the latent variables. There are
    several strategies, but before discussing them, it''s important to understand
    the dimensionality of our problem. If we have 1000 users and 500 products, *M*
    has 500,000 elements. If we decide to have rank equal to 10, it means that we
    need to find 5000000 variables constrained by the known ratings. As you can imagine,
    this problem can easily become impossible to solve with standard approaches and
    parallel solutions must be employed.'
  prefs: []
  type: TYPE_NORMAL
- en: Singular Value Decomposition strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first approach is based on the **Singular Value Decomposition** (**SVD**)
    of the user-item matrix. This technique allows transforming a matrix through a
    low-rank factorization and can also be used in an incremental way as described
    in Sarwar B., Karypis G., Konstan J., Riedl J., *Incremental Singular Value Decomposition
    Algorithms for Highly Scalable Recommender Systems*, 2002\. In particular, if
    the user-item matrix has *m* rows and *n* columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1797809a-47b0-41ba-911f-9759b959e05b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have assumed that we have real matrices (which is often true in our case),
    but, in general, they are complex. *U* and *V* are unitary, while sigma is diagonal.
    The columns of *U* contain the left singular vectors, the rows of transposed *V*
    contain the right singular vectors, while the diagonal matrix Sigma contains the
    singular values. Selecting *k* latent factors means taking the first *k* singular
    values and, therefore, the corresponding *k* left and right singular vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db5354fb-d50d-40b2-ae40-bae0ff85b189.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This technique has the advantage of minimizing the Frobenius norm of the difference
    between *M* and *M[k]* for any value of *k*, and therefore, it''s an optimal choice
    to approximate the full decomposition. Before moving to the prediction stage,
    let''s create an example using SciPy. The first thing to do is to create a dummy
    user-item matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re assuming that we have 20 users and 10 products. The ratings are bounded
    between 1 and 5, and 0 means no rating. Now we can decompose *M*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s consider only the first eight singular values, which will have eight
    latent factors for both the users and items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Bear in mind that in SciPy SVD implementation, *V* is already transposed. According
    to Sarwar B., Karypis G., Konstan J., Riedl J., *Incremental Singular Value Decomposition
    Algorithms for Highly Scalable Recommender Systems*, 2002, we can easily get a
    prediction considering the cosine similarity (which is proportional to the dot
    product) between customers and products. The two latent factor matrices are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58bf2712-c778-4fa1-8581-acf00903214f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to take into account the loss of precision, it''s useful also to consider
    the average rating per user (which corresponds to the mean row value of the user-item
    matrix), so that the result rating prediction for the user *i* and the item *j*
    becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/958c5037-4bea-41be-9b57-1eb258b1b4b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here *S[U](i)* and *S[I](j)* are the user and product vectors respectively.
    Continuing with our example, let''s determine the rating prediction for user 5
    and item 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This approach has medium complexity. In particular, the SVD is *O(m³)* and an
    incremental strategy (as described in Sarwar B., Karypis G., Konstan J., Riedl
    J., *Incremental Singular Value Decomposition Algorithms for Highly **Scalable
    Recommender Systems*, 2002) must be employed when new users or items are added;
    however, it can be effective when the number of elements is not too big. In all
    the other cases, the next strategy (together with a parallel architecture) can
    be adopted.
  prefs: []
  type: TYPE_NORMAL
- en: Alternating least squares strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The problem of finding the latent factors can be easily expressed as a least
    square optimization problem by defining the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91166122-a1b9-4316-9d42-a74a00575199.png)'
  prefs: []
  type: TYPE_IMG
- en: 'L is limited only to known samples (user, item). The second term works as a
    regularization factor and the whole problem can easily be solved with any optimization
    method. However, there''s an additional issue: we have two different sets of variables
    to determine (user and item factors). We can solve this problem with an approach
    called **alternating least squares**, described in Koren Y., Bell R., Volinsky
    C., *Matrix Factorization Techniques for Recommender Systems*, IEEE Computer Magazine,
    08/2009\. The algorithm is very easy to describe and can be summarized in two
    main iterating steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p[i]* is fixed and *q[j]* is optimized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*q*[*j* ]is fixed and *p[i]* is optimized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm stops when a predefined precision has been achieved. It can be
    easily implemented with parallel strategies to be able to process huge matrices
    in a short time. Moreover, considering the price of virtual clusters, it's also
    possible to retrain the model periodically, to immediately (with an acceptable
    delay) include new products and users.
  prefs: []
  type: TYPE_NORMAL
- en: Alternating least squares with Apache Spark MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is beyond the scope of this book, so if you want to know more about
    this powerful framework, I suggest you read the online documentation or one the
    many books available. In Pentreath N., *Machine Learning with Spark*, Packt, there's
    an interesting introduction on the library MLlib and how to implement most of
    the algorithms discussed in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Spark is a parallel computational engine that is now part of the Hadoop project
    (even if it doesn't use its code), that can run in local mode or on very large
    clusters (with thousands of nodes), to execute complex tasks using huge amounts
    of data. It's mainly based on Scala, though there are interfaces for Java, Python,
    and R. In this example, we're going to use PySpark, which is the built-in shell
    for running Spark with Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'After launching PySpark in local mode, we get a standard Python prompt and
    we can start working, just like with any other standard Python environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark MLlib implements the ALS algorithm through a very simple mechanism. The
    class `Rating` is a wrapper for the tuple (user, product, rating), so we can easily
    define a dummy dataset (which must be considered only as an example, because it''s
    very limited):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We assumed that we have 200 users and 100 products and we have populated a list
    of ratings by iterating 10 times the main loop which assigns a rating to a random
    product. We're not controlling repetitions or other uncommon situations. The last
    command `sc.parallelize()` is a way to ask Spark to transform our list into a
    structure called **resilient distributed dataset** (**RDD**), which will be used
    for the remaining operations. There are no actual limits to the size of these
    structures, because they are distributed across different executors (if in clustered
    mode) and can work with petabytes datasets just like we work with kilobytes ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can train an `ALS` model (which is formally `MatrixFactorizationModel`)
    and use it to make some predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We want 5 latent factors and 10 optimization iterations. As discussed before,
    it''s not very easy to determine the right rank for each model, so, after a training
    phase, there should always be a validation phase with known data. The mean squared
    error is a good measure to understand how the model is working. We can do it using
    the same training data set. The first thing to do is to remove the ratings (because
    we need only the tuple made up of user and product):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If you''re not familiar with the MapReduce paradigm, you only need to know
    that `map()` applies the same function (in this case, a lambda) to all the elements.
    Now we can massively predict the ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'However, in order to compute the error, we also need to add the user and product,
    to have tuples that can be compared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a sequence of rows with a structure `((user, item), rating)`,
    just like a standard dictionary entry `(key, value)`. This is useful because,
    using Spark, we can join two RDDs by using their keys. We do the same thing for
    the original dataset also, and then we proceed by joining the training values
    with the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now for each key `(user, product)`, we have two values: target and prediction.
    Therefore, we can compute the mean squared error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The first map transforms each row into the squared difference between the target
    and prediction, while the `mean()` function computes the average value. At this
    point, let''s check our error and produce a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'So, our error is quite low but it can be improved by changing the rank or the
    number of iterations. The prediction for the rating of the product 20 by the user
    10 is about 2.8 (that can be rounded to 3). If you run the code, these values
    can be different as we''re using a random user-item matrix. Moreover, if you don''t
    want to use the shell and run the code directly, you need to declare a `SparkContext`
    explicitly at the beginning of your file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We have created a configuration through the `SparkConf` class and specified
    both an application name and a master (in local mode with all cores available).
    This is enough to run our code. However, if you need further information, visit
    the page mentioned in the information box at the end of the chapter. To run the
    application (since Spark 2.0), you must execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: When running a script using `spark-submit`, you will see hundreds of log lines
    that inform you about all the operations that are being performed. Among them,
    at the end of the computation, you'll also see the print function messages (`stdout`).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this is only an introduction to Spark ALS, but I hope it was useful
    to understand how easy this process can be and, at the same time, how the dimensional
    limitations can be effectively addressed.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't know how to set up the environment and launch PySpark, I suggest
    reading the online quick-start guide ([https://spark.apache.org/docs/2.1.0/quick-start.html](https://spark.apache.org/docs/2.1.0/quick-start.html))
    that can be useful even if you don't know all the details and configuration parameters.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sarwar B., Karypis G., Konstan J., Riedl J., *Incremental Singular Value Decomposition
    Algorithms for Highly **Scalable Recommender Systems*, 2002
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koren Y., Bell R., Volinsky C., *Matrix Factorization Techniques For Recommender
    Systems*, IEEE Computer Magazine, 08/2009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pentreath N., *Machine Learning with Spark*, Packt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the main techniques for building a recommender
    system. In a user-based scenario, we assume that we have enough pieces of information
    about the users to be able to cluster them, and moreover, we implicitly assume
    that similar users would like the same products. In this way, it's immediate to
    determine the neighborhood of every new user and to suggest the products positively
    rated by his/her peers. In a similar way, a content-based scenario is based on
    the clustering of products according to their peculiar features. In this case,
    the assumption is weaker, because it's more probable that a user who bought an
    item or rated it positively will do the same with similar products.
  prefs: []
  type: TYPE_NORMAL
- en: Then we introduced collaborative filtering, which is a technique based on explicit
    ratings, used to predict all missing values for all users and products. In the
    memory-based variant, we don't train a model but we try to work directly with
    a user-product matrix, looking for the k-nearest neighbors of a test user, and
    computing the ranking through an average. This approach is very similar to the
    user-based scenario and has the same limitations; in particular, it's very difficult
    to manage large matrices. On the other hand, the model-based approach is more
    complex, but, after training the model, it can predict the ratings in real time.
    Moreover, there are parallel frameworks like Spark, which can be employed to process
    a huge amount of data using a cluster of cheap servers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to introduce some natural language processing
    techniques, which are very important when automatically classifying texts or working
    with machine translation systems.
  prefs: []
  type: TYPE_NORMAL
