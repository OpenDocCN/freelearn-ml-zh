- en: Introduction to Recommendation Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统简介
- en: Imagine an online shop with thousands of articles. If you're not a registered
    user, you'll probably see a homepage with some highlights, but if you've already
    bought some items, it would be interesting if the website showed products that
    you would probably buy, instead of a random selection. This is the purpose of
    a recommender system, and in this chapter, we're going to discuss the most common
    techniques to create such a system.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个拥有数千篇文章的在线商店。如果你不是注册用户，你可能会看到一些突出显示的主页，但如果你已经购买了一些商品，网站显示你可能购买的产品，而不是随机选择的产品，这将是很有趣的。这就是推荐系统的目的，在本章中，我们将讨论创建此类系统最常见的技术。
- en: The basic concepts are users, items, and ratings (or an implicit feedback about
    the products, like the fact of having bought them). Every model must work with
    known data (like in a supervised scenario), to be able to suggest the most suitable
    items or to predict the ratings for all the items not evaluated yet.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 基本概念包括用户、项目和评分（或关于产品的隐式反馈，例如购买的事实）。每个模型都必须与已知数据（如在监督场景中）一起工作，以便能够建议最合适的项目或预测尚未评估的所有项目的评分。
- en: 'We''re going to discuss two different kinds of strategies:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论两种不同的策略：
- en: User or content based
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于用户或内容
- en: Collaborative filtering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协同过滤
- en: The first approach is based on the information we have about users or products
    and its target is to associate a new user with an existing group of peers to suggest
    all the items positively rated by the other members, or to cluster the products
    according to their features and propose a subset of items similar to the one taken
    into account. The second approach, which is a little bit more sophisticated, works
    with explicit ratings and its purpose is to predict this value for every item
    and every user. Even if collaborative filtering needs more computational power
    as, nowadays, the great availability of cheap resources, allows using this algorithm
    with millions of users and products to provide the most accurate recommendations
    in real-time. The model can also be retrained or updated every day.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法基于我们对用户或产品的信息，其目标是将新用户与现有的一组同龄人关联起来，以建议其他成员正面评价的所有项目，或者根据其特征对产品进行聚类，并提出与考虑的项目相似的项目子集。第二种方法稍微复杂一些，使用显式评分，其目的是预测每个项目和每个用户的此值。尽管协同过滤需要更多的计算能力，但如今，廉价资源的广泛可用性允许使用此算法处理数百万用户和产品，以提供最准确的实时推荐。该模型还可以每天重新训练或更新。
- en: Naive user-based systems
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 天真的基于用户的系统
- en: 'In this first scenario, we assume that we have a set of users represented by
    feature vectors:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一个场景中，我们假设我们有一组由特征向量表示的用户：
- en: '![](img/46d9a100-5870-4a68-b406-0ac4cc3a5d91.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/46d9a100-5870-4a68-b406-0ac4cc3a5d91.png)'
- en: 'Typical features are age, gender, interests, and so on. All of them must be
    encoded using one of the techniques discussed in the previous chapters (for example,
    they can be binarized). Moreover, we have a set of items:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的特征包括年龄、性别、兴趣等。所有这些都必须使用前几章中讨论的技术之一进行编码（例如，它们可以被二值化）。此外，我们有一组项目：
- en: '![](img/538f4289-3d0a-4275-8d41-371d3413d5f7.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/538f4289-3d0a-4275-8d41-371d3413d5f7.png)'
- en: 'Let''s assume also that there is a relation which associates each user with
    a subset of items (bought or positively reviewed), items for which an explicit
    action or feedback has been performed:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设也存在一个关系，将每个用户与一组项目（已购买或正面评价）相关联，对于这些项目已执行了明确的行为或反馈：
- en: '![](img/eb0e6274-4416-4300-af08-32961368bafc.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/eb0e6274-4416-4300-af08-32961368bafc.png)'
- en: 'In a user-based system, the users are periodically clustered (normally using
    a **k-nearest neighbors** approach), and therefore, considering a generic user
    *u* (also new), we can immediately determine the ball containing all the users
    who are similar (therefore neighbors) to our sample:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于用户的系统中，用户会定期进行聚类（通常使用**k最近邻**方法），因此，考虑一个通用的用户 *u*（也是新的），我们可以立即确定包含所有与我们样本相似（因此是邻居）的用户球体：
- en: '![](img/09add924-472f-4bf6-936c-dfb641de04e4.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/09add924-472f-4bf6-936c-dfb641de04e4.png)'
- en: 'At this point, we can create the set of suggested items using the relation
    previously introduced:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以使用之前介绍的关系创建建议项目的集合：
- en: '![](img/3fecf5a0-21aa-464d-bd53-0ddb651235fe.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3fecf5a0-21aa-464d-bd53-0ddb651235fe.png)'
- en: In other words, the set contains all the unique products positively rated or
    bought by the neighborhood. I've used the adjective naive because there's a similar
    alternative that we're going to discuss in the section dedicated to collaborative
    filtering.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，这个集合包含所有被邻居积极评价或购买的唯一产品。我之所以使用“天真”这个词，是因为我们将要在专门讨论协同过滤的章节中讨论一个类似的替代方案。
- en: User-based system implementation with scikit-learn
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于用户的系统实现与scikit-learn
- en: 'For our purposes, we need to create a dummy dataset of users and products:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们需要创建一个用户和产品的虚拟数据集：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We assume that we have 1,000 users with four features represented by integer
    numbers bounded between 0 and 4 or 5\. It doesn't matter what they mean; their
    role is to characterize a user and allow for clustering of the set.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设我们有1,000个用户，他们有四个特征，这些特征由介于0和4或5之间的整数表示。它们的具体含义无关紧要；它们的作用是表征用户并允许对集合进行聚类。
- en: 'For the products, we also need to create the association:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于产品，我们还需要创建关联：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We assume that we have 20 different items (from 1 to 20; 0 means that a user
    didn''t buy anything) and an association matrix where each user is linked to a
    number of products bounded between 0 and 5 (maximum). For example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设我们有20个不同的项目（从1到20；0表示用户没有购买任何东西）和一个关联矩阵，其中每个用户都与0到5（最大值）之间的产品数量相关联。例如：
- en: '![](img/58fec9ca-aca7-45b5-a60a-a1c34802b4ad.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/58fec9ca-aca7-45b5-a60a-a1c34802b4ad.png)'
- en: 'At this point, we need to cluster the users using the `NearestNeighbors` implementation
    provided by scikit-learn:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们需要使用scikit-learn提供的`NearestNeighbors`实现来对用户进行聚类：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We have selected to have 20 neighbors and a Euclidean radius equal to 2\. This
    parameter is used when we want to query the model to know which items are contained
    in the ball whose center is a sample and with a fixed radius. In our case, we
    are going to query the model to get all the neighbors of a test user:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了20个邻居和等于2的欧几里得半径。当我们要查询模型以了解包含在以样本为中心且具有固定半径的球体内的项目时，我们会使用这个参数。在我们的案例中，我们将查询模型以获取一个测试用户的全部邻居：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we need to build the recommendation list using the association matrix:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要使用关联矩阵来构建推荐列表：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For each neighbor, we retrieve the products he/she bought and perform a union,
    avoiding the inclusion of items with zero value (meaning no product) and double
    elements. The result is a list (not sorted) of suggestions that can be obtained
    almost in real time for many different systems. In some cases, when the number
    of users or items is too huge, it's possible to limit the list to a fixed number
    of elements and to reduce the number of neighbors. This approach is also naive
    because it doesn't consider the actual distance (or similarity) between users
    to weigh the suggestions. It's possible to consider the distance as a weighing
    factor, but it's simpler to adopt the collaborative filtering approach which provides
    a more robust solution.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个邻居，我们检索他/她购买的产品并执行并集操作，避免包含值为零的项目（表示没有产品）和重复元素。结果是（未排序）建议列表，对于许多不同的系统，几乎可以实时获得。在某些情况下，当用户或项目数量太多时，可以限制列表为固定数量的元素并减少邻居的数量。这种方法也是天真的，因为它没有考虑用户之间的实际距离（或相似性）来权衡建议。可以考虑将距离作为权重因子，但采用提供更稳健解决方案的协同过滤方法更简单。
- en: Content-based systems
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于内容的系统
- en: 'This is probably the simplest method and it''s based only on the products,
    modeled as feature vectors:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是最简单的方法，它仅基于产品，将其建模为特征向量：
- en: '![](img/92064032-622a-45c1-9ef4-63e267b14286.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/92064032-622a-45c1-9ef4-63e267b14286.png)'
- en: Just like the users, the features can also be categorical (indeed, for products
    it's easier), for example, the genre of a book or a movie, and they can be used
    together with numerical values (like price, length, number of positive reviews,
    and so on) after encoding them.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 就像用户一样，特征也可以是分类的（实际上，对于产品来说更容易），例如，书籍或电影的类型，并且它们可以在编码后与数值（如价格、长度、正面评价数量等）一起使用。
- en: Then a clustering strategy is adopted, even if the most used is **k-nearest
    neighbors** as it allows controlling the size of each neighborhood to determine,
    given a sample product, the quality and the number of suggestions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后采用聚类策略，尽管最常用的是**k最近邻**，因为它允许控制每个邻域的大小，从而确定给定一个样本产品，其质量和建议的数量。
- en: 'Using scikit-learn, first of all we create a dummy product dataset:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn，首先我们创建一个虚拟产品数据集：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this case, we have 1000 samples with four integer features bounded between
    0 and 100\. Then we proceed, as in the previous example, towards clustering them:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有 1000 个样本，四个整数特征介于 0 和 100 之间。然后我们继续，就像上一个例子一样，将它们进行聚类：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'At this point, it''s possible to query our model with the method `radius_neighbors()`,which
    allows us to restrict our research only to a limited subset. The default radius
    (set through the parameter `radius`) is 5.0, but we can change it dynamically:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以使用方法 `radius_neighbors()` 来查询我们的模型，这允许我们仅将研究限制在有限的子集。默认半径（通过参数 `radius`
    设置）为 5.0，但我们可以动态地更改它：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Of course, when trying these examples, the number of suggestions can be different,
    as we are using random datasets, so I suggest trying different values for the
    radius (in particular when using different metrics).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，当尝试这些示例时，建议的数量可能不同，因为我们正在使用随机数据集，所以我建议尝试不同的半径值（特别是当使用不同的度量标准时）。
- en: 'When clustering with **k-nearest neighbors**, it''s important to consider the
    metric adopted for determining the distance between the samples. The default for
    scikit-learn is the Minkowski distance, which is a generalization of Euclidean
    and Manhattan distance, and is defined as:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 **k-最近邻** 进行聚类时，考虑用于确定样本之间距离的度量标准非常重要。scikit-learn 的默认值是 Minkowski 距离，它是欧几里得和曼哈顿距离的推广，定义为：
- en: '![](img/c39c54b7-284d-4d24-bac8-b7614e489373.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c39c54b7-284d-4d24-bac8-b7614e489373.png)'
- en: 'The parameter *p* controls the type of distance and the default value is 2,
    so that the resulting metric is a classical Euclidean distance. Other distances
    are offered by SciPy (in the package `scipy.spatial.distance`) and include, for
    example, the **Hamming** and **Jaccard** distances. The former is defined as the
    disagree proportion between two vectors (if they are binary this is the normalized
    number of different bits). For example:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 *p* 控制距离的类型，默认值为 2，因此得到的度量是经典的欧几里得距离。SciPy（在 `scipy.spatial.distance` 包中）提供了其他距离，例如
    **汉明** 和 **杰卡德** 距离。前者定义为两个向量之间的不一致比例（如果它们是二进制的，则这是不同位的标准化数量）。例如：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It means there's a disagree proportion of 40 percent, or, considering that both
    vectors are binary, there 4 different bits (out of 10). This measure can be useful
    when it's necessary to emphasize the presence/absence of a particular feature.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着有 40% 的不一致比例，或者考虑到两个向量都是二进制的，有 4 个不同的位（在 10 位中）。这个度量在需要强调特定特征的呈现/缺失时可能很有用。
- en: 'The Jaccard distance is defined as:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 杰卡德距离定义为：
- en: '![](img/f566e77a-f605-4e6d-abb8-3a3467235362.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f566e77a-f605-4e6d-abb8-3a3467235362.png)'
- en: 'It''s particularly useful to measure the dissimilarity between two different
    sets (*A* and *B*) of items. If our feature vectors are binary, it''s immediate
    to apply this distance using Boolean logic. Using the previous test values, we
    get:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 测量两个不同集合（*A* 和 *B*）的项之间的差异特别有用。如果我们的特征向量是二进制的，则可以使用布尔逻辑立即应用此距离。使用之前的测试值，我们得到：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This measure is bounded between 0 (equal vectors) and 1 (total dissimilarity).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个度量在 0（相等向量）和 1（完全不同）之间有界。
- en: 'As for the Hamming distance, it can be very useful when it''s necessary to
    compare items where their representation is made up of binary states (like present/absent,
    yes/no, and so forth). If you want to adopt a different metric for **k-nearest
    neighbors**, it''s possible to specify it directly using the `metric` parameter:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 至于汉明距离，当需要比较由二进制状态（如存在/不存在、是/否等）组成的项时，它非常有用。如果您想为 **k-最近邻** 采用不同的度量标准，可以直接使用
    `metric` 参数指定它：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Model-free (or memory-based) collaborative filtering
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无模型（或基于记忆）的协同过滤
- en: 'As with the user-based approach, let''s consider having two sets of elements:
    users and items. However, in this case, we don''t assume that they have explicit
    features. Instead, we try to model a user-item matrix based on the preferences
    of each user (rows) for each item (columns). For example:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于用户的方法一样，让我们考虑有两个元素集合：用户和物品。然而，在这种情况下，我们不假设它们有显式特征。相反，我们试图根据每个用户（行）对每个物品（列）的偏好来建模用户-物品矩阵。例如：
- en: '![](img/7a0283ec-a54b-4cb6-b25c-9c08e1974a30.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7a0283ec-a54b-4cb6-b25c-9c08e1974a30.png)'
- en: In this case, the ratings are bounded between 1 and 5 (0 means no rating), and
    our goal is to cluster the users according to their rating vector (which is, indeed,
    an internal representation based on a particular kind of feature). This allows
    producing recommendations even when there are no explicit pieces of information
    about the user. However, it has a drawback, called **cold-startup**, which means
    that when a new user has no ratings, it's impossible to find the right neighborhood,
    because he/she can belong to virtually any cluster.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，评分介于1到5之间（0表示没有评分），我们的目标是根据用户的评分向量（实际上，这是一种基于特定类型特征的内部表示）对用户进行聚类。这允许在没有任何关于用户的明确信息的情况下产生推荐。然而，它有一个缺点，称为**冷启动**，这意味着当一个新用户没有评分时，无法找到正确的邻域，因为他/她可能属于几乎任何聚类。
- en: Once the clustering is done, it's easy to check which products (not rated yet)
    have the higher rating for a given user and therefore are more likely to be bought.
    It's possible to implement a solution in scikit-learn as we've done before, but
    I'd like to introduce a small framework called **Crab** (see the box at the end
    of this section) that simplifies this process.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成聚类，很容易检查哪些产品（尚未评分）对特定用户有更高的评分，因此更有可能被购买。可以像之前那样在scikit-learn中实现解决方案，但我想要介绍一个名为**Crab**的小型框架（见本节末尾的框），它简化了这一过程。
- en: 'In order to build the model, we first need to define the user-item matrix as
    a Python dictionary with the structure:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建模型，我们首先需要将用户-物品矩阵定义为Python字典，其结构如下：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A missing value in a user internal dictionary means no rating. In our example,
    we consider 5 users with 5 items:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 用户内部字典中的缺失值表示没有评分。在我们的例子中，我们考虑了5个用户和5个物品：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once the user-item matrix has been defined, we need to pick a metric and therefore,
    a distance function *d(u[i], u[j])*, to build a similarity matrix:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了用户-物品矩阵，我们需要选择一个度量标准以及因此一个距离函数 *d(u[i], u[j])* 来构建相似度矩阵：
- en: '![](img/29ff8a1a-9a66-41c3-bf82-f12ad7a731c5.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29ff8a1a-9a66-41c3-bf82-f12ad7a731c5.png)'
- en: 'Using Crab, we do this in the following way (using a Euclidean metric):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Crab，我们以以下方式（使用欧几里得距离）进行此操作：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'There are many metrics, like Pearson or Jaccard, so I suggest visiting the
    website ([http://muricoca.github.io/crab](http://muricoca.github.io/crab)) to
    retrieve further information. At this point, it''s possible to build the recommendation
    system (based on the k-nearest neighbors clustering method) and test it:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多度量标准，如皮尔逊或贾卡德，所以我建议访问网站([http://muricoca.github.io/crab](http://muricoca.github.io/crab))以获取更多信息。在此阶段，可以构建基于k最近邻聚类方法的推荐系统并对其进行测试：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'So the recommender suggests the following predicted rating for user 2:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，推荐系统为用户2建议以下预测评分：
- en: '**Item 2**: 3.6 (which can be rounded to 4.0)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物品2**：3.6（可以四舍五入到4.0）'
- en: '**Item 5**: 3'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物品5**：3'
- en: '**Item 3**: 2.5 (which can be rounded to 3.0)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物品3**：2.5（可以四舍五入到3.0）'
- en: 'When running the code, it''s possible to see some warnings (Crab is still under
    development); however, they don''t condition the functionality. If you want to
    avoid them, you can use the `catch_warnings()` context manager:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行代码时，可能会看到一些警告（Crab仍在开发中）；然而，它们并不影响功能。如果您想避免它们，可以使用`catch_warnings()`上下文管理器：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It''s possible to suggest all the items, or limit the list to the higher ratings
    (so, for example, avoiding the item 3). This approach is quite similar to the
    user-based model. However, it''s faster (very big matrices can be processed in
    parallel) and it doesn''t take care of details that can produce misleading results.
    Only the ratings are considered as useful features to define a user. Like model-based
    collaborative filtering, the cold-startup problem can be addressed in two ways:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 可以建议所有物品，或者限制列表只包含高评分（例如，避免物品3）。这种方法与基于用户的模型相当相似。然而，它更快（非常大的矩阵可以并行处理）并且它不关心可能导致误导结果的具体细节。只有评分被视为定义用户的有用特征。像基于模型的协同过滤一样，冷启动问题可以通过两种方式解决：
- en: Asking the user to rate some items (this approach is often adopted because it's
    easy to show some movie/book covers, asking the user to select what they like
    and what they don't).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求用户对一些物品进行评分（这种做法通常被采用，因为它很容易展示一些电影/书籍封面，让用户选择他们喜欢和不喜欢的内容）。
- en: Placing the user in an average neighborhood by randomly assigning some mean
    ratings. In this approach, it's possible to start using the recommendation system
    immediately. However, it's necessary to accept a certain degree of error at the
    beginning and to correct the dummy ratings when the real ones are produced.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过随机分配一些平均评分将用户放置在平均邻域中。在这种方法中，可以立即开始使用推荐系统。然而，在开始时必须接受一定程度的错误，并在产生真实评分时纠正虚拟评分。
- en: 'Crab is an open-source framework for building collaborative filtering systems.
    It''s still under development and therefore, doesn''t implement all possible features.
    However, it''s very easy to use and is quite powerful for many tasks. The home
    page with installation instructions and documentation is: [http://muricoca.github.io/crab/index.html](http://muricoca.github.io/crab/index.html).
    Crab depends on scikits.learn, which still has some issues with Python 3\. Therefore,
    I recommend using Python 2.7 for this example. It''s possible to install both
    packages using pip: `pip install -U scikits.learn` and `pip install -U crab`.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Crab是一个用于构建协同过滤系统的开源框架。它仍在开发中，因此尚未实现所有可能的功能。然而，它非常易于使用，对于许多任务来说非常强大。带有安装说明和文档的主页是：[http://muricoca.github.io/crab/index.html](http://muricoca.github.io/crab/index.html)。Crab依赖于scikits.learn，它仍然与Python
    3有一些问题。因此，我建议在这个例子中使用Python 2.7。可以使用pip安装这两个包：`pip install -U scikits.learn` 和
    `pip install -U crab`。
- en: Model-based collaborative filtering
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于模型的协同过滤
- en: 'This is currently one of the most advanced approaches and is an extension of
    what was already seen in the previous section. The starting point is always a
    rating-based user-item matrix:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是目前最先进的方法之一，是前一个章节中已看到内容的扩展。起点始终是基于评分的用户-项目矩阵：
- en: '![](img/e4a3a7f1-4ccd-4132-9a8c-3ff4fd0ca8a1.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e4a3a7f1-4ccd-4132-9a8c-3ff4fd0ca8a1.png)'
- en: 'However, in this case, we assume the presence of **latent factors** for both
    the users and the items. In other words, we define a generic user as:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这种情况下，我们假设用户和项目都存在潜在因素。换句话说，我们定义一个通用的用户为：
- en: '![](img/74322669-b70e-4833-9bbd-ae36981b2a83.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/74322669-b70e-4833-9bbd-ae36981b2a83.png)'
- en: 'A generic item is defined as:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的项目定义为：
- en: '![](img/92ac9795-5486-4ff0-980f-bba392ddcb95.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/92ac9795-5486-4ff0-980f-bba392ddcb95.png)'
- en: 'We don''t know the value of each vector component (for this reason they are
    called latent), but we assume that a ranking is obtained as:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不知道每个向量分量的值（因此它们被称为潜在值），但我们假设通过以下方式获得排名：
- en: '![](img/e38e388a-30a6-4f06-8a31-b82a19cc5da0.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e38e388a-30a6-4f06-8a31-b82a19cc5da0.png)'
- en: 'So we can say that a ranking is obtained from a latent space of rank *k*, where
    *k* is the number of latent variables we want to consider in our model. In general,
    there are rules to determine the right value for *k*, so the best approach is
    to check different values and test the model with a subset of known ratings. However,
    there''s still a big problem to solve: finding the latent variables. There are
    several strategies, but before discussing them, it''s important to understand
    the dimensionality of our problem. If we have 1000 users and 500 products, *M*
    has 500,000 elements. If we decide to have rank equal to 10, it means that we
    need to find 5000000 variables constrained by the known ratings. As you can imagine,
    this problem can easily become impossible to solve with standard approaches and
    parallel solutions must be employed.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以这样说，排名是从一个包含 *k* 个潜在变量的潜在空间中获得的，其中 *k* 是我们希望在模型中考虑的潜在变量的数量。一般来说，有规则可以确定
    *k* 的正确值，因此最佳方法是检查不同的值，并使用已知评分的子集测试模型。然而，仍然有一个大问题需要解决：找到潜在变量。有几种策略，但在讨论它们之前，了解我们问题的维度很重要。如果我们有1000个用户和500个产品，*M*
    有500,000个元素。如果我们决定排名等于10，这意味着我们需要找到5000000个变量，这些变量受已知评分的限制。正如你可以想象的那样，这个问题很容易变得无法用标准方法解决，并且必须采用并行解决方案。
- en: Singular Value Decomposition strategy
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单值分解策略
- en: 'The first approach is based on the **Singular Value Decomposition** (**SVD**)
    of the user-item matrix. This technique allows transforming a matrix through a
    low-rank factorization and can also be used in an incremental way as described
    in Sarwar B., Karypis G., Konstan J., Riedl J., *Incremental Singular Value Decomposition
    Algorithms for Highly Scalable Recommender Systems*, 2002\. In particular, if
    the user-item matrix has *m* rows and *n* columns:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是基于用户-项目矩阵的**奇异值分解**（**SVD**）。这种技术允许通过低秩分解来转换矩阵，也可以像Sarwar B.，Karypis G.，Konstan
    J.，Riedl J.，*Incremental Singular Value Decomposition Algorithms for Highly Scalable
    Recommender Systems*，2002年描述的那样以增量方式使用。特别是，如果用户-项目矩阵有*m*行和*n*列：
- en: '![](img/1797809a-47b0-41ba-911f-9759b959e05b.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1797809a-47b0-41ba-911f-9759b959e05b.png)'
- en: 'We have assumed that we have real matrices (which is often true in our case),
    but, in general, they are complex. *U* and *V* are unitary, while sigma is diagonal.
    The columns of *U* contain the left singular vectors, the rows of transposed *V*
    contain the right singular vectors, while the diagonal matrix Sigma contains the
    singular values. Selecting *k* latent factors means taking the first *k* singular
    values and, therefore, the corresponding *k* left and right singular vectors:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设我们拥有实矩阵（在我们的情况下通常是真的），但一般来说，它们是复数。*U*和*V*是正交的，而sigma是对角的。*U*的列包含左奇异向量，转置*V*的行包含右奇异向量，而对角矩阵Sigma包含奇异值。选择*k*个潜在因子意味着取前*k*个奇异值，因此，相应的*k*个左和右奇异向量：
- en: '![](img/db5354fb-d50d-40b2-ae40-bae0ff85b189.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/db5354fb-d50d-40b2-ae40-bae0ff85b189.png)'
- en: 'This technique has the advantage of minimizing the Frobenius norm of the difference
    between *M* and *M[k]* for any value of *k*, and therefore, it''s an optimal choice
    to approximate the full decomposition. Before moving to the prediction stage,
    let''s create an example using SciPy. The first thing to do is to create a dummy
    user-item matrix:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术具有最小化*M*和*M[k]*之间Frobenius范数差异的优点，对于任何*k*的值，因此，它是逼近完整分解的最佳选择。在进入预测阶段之前，让我们使用SciPy创建一个示例。首先要做的是创建一个虚拟的用户-项目矩阵：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We''re assuming that we have 20 users and 10 products. The ratings are bounded
    between 1 and 5, and 0 means no rating. Now we can decompose *M*:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设有20个用户和10个产品。评分介于1到5之间，0表示没有评分。现在我们可以分解*M*：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now let''s consider only the first eight singular values, which will have eight
    latent factors for both the users and items:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们只考虑前八个奇异值，这将使用户和项目都有八个潜在因子：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Bear in mind that in SciPy SVD implementation, *V* is already transposed. According
    to Sarwar B., Karypis G., Konstan J., Riedl J., *Incremental Singular Value Decomposition
    Algorithms for Highly Scalable Recommender Systems*, 2002, we can easily get a
    prediction considering the cosine similarity (which is proportional to the dot
    product) between customers and products. The two latent factor matrices are:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在SciPy的SVD实现中，*V*已经转置。根据Sarwar B.，Karypis G.，Konstan J.，Riedl J.，*Incremental
    Singular Value Decomposition Algorithms for Highly Scalable Recommender Systems*，2002年的描述，我们可以很容易地通过考虑客户和产品之间的余弦相似度（与点积成正比）来进行预测。两个潜在因子矩阵是：
- en: '![](img/58bf2712-c778-4fa1-8581-acf00903214f.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/58bf2712-c778-4fa1-8581-acf00903214f.png)'
- en: 'In order to take into account the loss of precision, it''s useful also to consider
    the average rating per user (which corresponds to the mean row value of the user-item
    matrix), so that the result rating prediction for the user *i* and the item *j*
    becomes:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考虑到精度的损失，考虑每个用户的平均评分（这对应于用户-项目矩阵的行平均值）也是很有用的，这样用户*i*和项目*j*的结果评分预测就变为：
- en: '![](img/958c5037-4bea-41be-9b57-1eb258b1b4b8.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/958c5037-4bea-41be-9b57-1eb258b1b4b8.png)'
- en: 'Here *S[U](i)* and *S[I](j)* are the user and product vectors respectively.
    Continuing with our example, let''s determine the rating prediction for user 5
    and item 2:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*S[U](i)*和*S[I](j)*分别是用户和产品向量。继续我们的例子，让我们确定用户5和项目2的评分预测：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This approach has medium complexity. In particular, the SVD is *O(m³)* and an
    incremental strategy (as described in Sarwar B., Karypis G., Konstan J., Riedl
    J., *Incremental Singular Value Decomposition Algorithms for Highly **Scalable
    Recommender Systems*, 2002) must be employed when new users or items are added;
    however, it can be effective when the number of elements is not too big. In all
    the other cases, the next strategy (together with a parallel architecture) can
    be adopted.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法具有中等复杂度。特别是，SVD是*O(m³)*，当添加新用户或项目时，必须采用增量策略（如Sarwar B.、Karypis G.、Konstan
    J.、Riedl J.在2002年发表的*高度可扩展推荐系统增量奇异值分解算法*中所述）；然而，当元素数量不是太多时，它可能非常有效。在所有其他情况下，可以采用下一个策略（与并行架构一起）。
- en: Alternating least squares strategy
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交替最小二乘策略
- en: 'The problem of finding the latent factors can be easily expressed as a least
    square optimization problem by defining the following loss function:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过定义以下损失函数，可以轻松地将寻找潜在因子的难题表达为一个最小二乘优化问题：
- en: '![](img/91166122-a1b9-4316-9d42-a74a00575199.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/91166122-a1b9-4316-9d42-a74a00575199.png)'
- en: 'L is limited only to known samples (user, item). The second term works as a
    regularization factor and the whole problem can easily be solved with any optimization
    method. However, there''s an additional issue: we have two different sets of variables
    to determine (user and item factors). We can solve this problem with an approach
    called **alternating least squares**, described in Koren Y., Bell R., Volinsky
    C., *Matrix Factorization Techniques for Recommender Systems*, IEEE Computer Magazine,
    08/2009\. The algorithm is very easy to describe and can be summarized in two
    main iterating steps:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: L仅限于已知样本（用户、项目）。第二个项作为一个正则化因子，整个问题可以很容易地通过任何优化方法解决。然而，还有一个额外的问题：我们有两组不同的变量需要确定（用户和项目因子）。我们可以通过一种称为**交替最小二乘**的方法来解决此问题，该方法由Koren
    Y.、Bell R.、Volinsky C.在2009年8月的IEEE计算机杂志上发表的*推荐系统矩阵分解技术*中描述。该算法非常容易描述，可以总结为两个主要的迭代步骤：
- en: '*p[i]* is fixed and *q[j]* is optimized'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p[i]*是固定的，*q[j]*是优化的'
- en: '*q*[*j* ]is fixed and *p[i]* is optimized'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*q*[*j*]是固定的，*p[i]*是优化的'
- en: The algorithm stops when a predefined precision has been achieved. It can be
    easily implemented with parallel strategies to be able to process huge matrices
    in a short time. Moreover, considering the price of virtual clusters, it's also
    possible to retrain the model periodically, to immediately (with an acceptable
    delay) include new products and users.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当达到预定义的精度时，算法停止。它可以很容易地通过并行策略实现，以便在短时间内处理大量矩阵。此外，考虑到虚拟集群的成本，还可以定期重新训练模型，以立即（在可接受的延迟内）包含新产品和用户。
- en: Alternating least squares with Apache Spark MLlib
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark MLlib的交替最小二乘
- en: Apache Spark is beyond the scope of this book, so if you want to know more about
    this powerful framework, I suggest you read the online documentation or one the
    many books available. In Pentreath N., *Machine Learning with Spark*, Packt, there's
    an interesting introduction on the library MLlib and how to implement most of
    the algorithms discussed in this book.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark超出了本书的范围，因此如果您想了解更多关于这个强大框架的信息，我建议您阅读在线文档或许多可用的书籍。在Pentreath N.的*Spark机器学习*（Packt）中，有一个关于库MLlib和如何实现本书中讨论的大多数算法的有趣介绍。
- en: Spark is a parallel computational engine that is now part of the Hadoop project
    (even if it doesn't use its code), that can run in local mode or on very large
    clusters (with thousands of nodes), to execute complex tasks using huge amounts
    of data. It's mainly based on Scala, though there are interfaces for Java, Python,
    and R. In this example, we're going to use PySpark, which is the built-in shell
    for running Spark with Python code.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个并行计算引擎，现在是Hadoop项目的一部分（即使它不使用其代码），可以在本地模式或非常大的集群（具有数千个节点）上运行，以使用大量数据执行复杂任务。它主要基于Scala，尽管有Java、Python和R的接口。在这个例子中，我们将使用PySpark，这是运行Spark的Python代码的内置shell。
- en: 'After launching PySpark in local mode, we get a standard Python prompt and
    we can start working, just like with any other standard Python environment:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地模式下启动PySpark后，我们得到一个标准的Python提示符，我们可以开始工作，就像在任何其他标准Python环境中一样：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Spark MLlib implements the ALS algorithm through a very simple mechanism. The
    class `Rating` is a wrapper for the tuple (user, product, rating), so we can easily
    define a dummy dataset (which must be considered only as an example, because it''s
    very limited):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib通过一个非常简单的机制实现了ALS算法。`Rating`类是元组（user, product, rating）的包装器，因此我们可以轻松地定义一个虚拟数据集（这只能被视为一个示例，因为它非常有限）：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We assumed that we have 200 users and 100 products and we have populated a list
    of ratings by iterating 10 times the main loop which assigns a rating to a random
    product. We're not controlling repetitions or other uncommon situations. The last
    command `sc.parallelize()` is a way to ask Spark to transform our list into a
    structure called **resilient distributed dataset** (**RDD**), which will be used
    for the remaining operations. There are no actual limits to the size of these
    structures, because they are distributed across different executors (if in clustered
    mode) and can work with petabytes datasets just like we work with kilobytes ones.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设有200个用户和100个产品，并且通过迭代10次主循环，为随机产品分配评分来填充评分列表。我们没有控制重复或其他不常见的情况。最后的命令`sc.parallelize()`是一种请求Spark将我们的列表转换为称为**弹性分布式数据集**（**RDD**）的结构的方法，它将被用于剩余的操作。这些结构的大小实际上没有限制，因为它们分布在不同的执行器上（如果是在集群模式下），并且可以像处理千字节数据集一样处理PB级的数据集。
- en: 'At this point, we can train an `ALS` model (which is formally `MatrixFactorizationModel`)
    and use it to make some predictions:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以训练一个`ALS`模型（形式上是`MatrixFactorizationModel`），并使用它来进行一些预测：
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We want 5 latent factors and 10 optimization iterations. As discussed before,
    it''s not very easy to determine the right rank for each model, so, after a training
    phase, there should always be a validation phase with known data. The mean squared
    error is a good measure to understand how the model is working. We can do it using
    the same training data set. The first thing to do is to remove the ratings (because
    we need only the tuple made up of user and product):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要5个潜在因素和10次优化迭代。正如之前讨论的那样，确定每个模型的正确秩并不容易，因此，在训练阶段之后，应该始终有一个使用已知数据的验证阶段。均方误差是一个很好的指标，可以用来了解模型的工作情况。我们可以使用相同的训练数据集来完成这项工作。首先要做的是移除评分（因为我们只需要由用户和产品组成的元组）：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If you''re not familiar with the MapReduce paradigm, you only need to know
    that `map()` applies the same function (in this case, a lambda) to all the elements.
    Now we can massively predict the ratings:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉MapReduce范式，您只需要知道`map()`会对所有元素应用相同的函数（在这种情况下，是一个lambda函数）。现在我们可以大量预测评分：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'However, in order to compute the error, we also need to add the user and product,
    to have tuples that can be compared:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了计算误差，我们还需要添加用户和产品，以便可以进行比较：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The result is a sequence of rows with a structure `((user, item), rating)`,
    just like a standard dictionary entry `(key, value)`. This is useful because,
    using Spark, we can join two RDDs by using their keys. We do the same thing for
    the original dataset also, and then we proceed by joining the training values
    with the predictions:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一系列具有结构`((user, item), rating)`的行，就像一个标准的字典条目`(key, value)`。这很有用，因为使用Spark，我们可以通过它们的键来连接两个RDD。我们也对原始数据集做了同样的事情，然后通过连接训练值和预测值来继续操作：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now for each key `(user, product)`, we have two values: target and prediction.
    Therefore, we can compute the mean squared error:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对于每个键 `(user, product)`，我们有两个值：目标和预测。因此，我们可以计算均方误差：
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The first map transforms each row into the squared difference between the target
    and prediction, while the `mean()` function computes the average value. At this
    point, let''s check our error and produce a prediction:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`map`操作将每一行转换为目标和预测之间的平方差，而`mean()`函数计算平均值。在这个时候，让我们检查我们的误差并生成一个预测：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'So, our error is quite low but it can be improved by changing the rank or the
    number of iterations. The prediction for the rating of the product 20 by the user
    10 is about 2.8 (that can be rounded to 3). If you run the code, these values
    can be different as we''re using a random user-item matrix. Moreover, if you don''t
    want to use the shell and run the code directly, you need to declare a `SparkContext`
    explicitly at the beginning of your file:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的误差相当低，但可以通过改变秩或迭代次数来提高。用户10对产品20的评分预测约为2.8（可以四舍五入到3）。如果您运行代码，这些值可能会有所不同，因为我们正在使用随机的用户-项目矩阵。此外，如果您不想使用shell直接运行代码，您需要在文件开头显式声明一个`SparkContext`：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We have created a configuration through the `SparkConf` class and specified
    both an application name and a master (in local mode with all cores available).
    This is enough to run our code. However, if you need further information, visit
    the page mentioned in the information box at the end of the chapter. To run the
    application (since Spark 2.0), you must execute the following command:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过`SparkConf`类创建了一个配置，并指定了应用程序名称和主节点（在本地模式下，使用所有可用核心）。这足以运行我们的代码。然而，如果您需要更多信息，请访问章节末尾信息框中提到的页面。要运行应用程序（自Spark
    2.0起），您必须执行以下命令：
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: When running a script using `spark-submit`, you will see hundreds of log lines
    that inform you about all the operations that are being performed. Among them,
    at the end of the computation, you'll also see the print function messages (`stdout`).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`spark-submit`运行脚本时，您将看到数百行日志，这些日志会通知您正在执行的所有操作。其中，在计算结束时，您还会看到打印函数消息（`stdout`）。
- en: Of course, this is only an introduction to Spark ALS, but I hope it was useful
    to understand how easy this process can be and, at the same time, how the dimensional
    limitations can be effectively addressed.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这只是一个Spark ALS的介绍，但我希望它有助于理解这个过程有多简单，同时如何有效地解决维度限制问题。
- en: If you don't know how to set up the environment and launch PySpark, I suggest
    reading the online quick-start guide ([https://spark.apache.org/docs/2.1.0/quick-start.html](https://spark.apache.org/docs/2.1.0/quick-start.html))
    that can be useful even if you don't know all the details and configuration parameters.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不知道如何设置环境和启动PySpark，我建议阅读在线快速入门指南（[https://spark.apache.org/docs/2.1.0/quick-start.html](https://spark.apache.org/docs/2.1.0/quick-start.html)），即使您不了解所有细节和配置参数，它也可能很有用。
- en: References
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Sarwar B., Karypis G., Konstan J., Riedl J., *Incremental Singular Value Decomposition
    Algorithms for Highly **Scalable Recommender Systems*, 2002
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarwar B.，Karypis G.，Konstan J.，Riedl J.，*高度可扩展推荐系统的增量奇异值分解算法，2002*
- en: Koren Y., Bell R., Volinsky C., *Matrix Factorization Techniques For Recommender
    Systems*, IEEE Computer Magazine, 08/2009
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koren Y.，Bell R.，Volinsky C.，*推荐系统的矩阵分解技术，IEEE计算机杂志，2009年8月*
- en: Pentreath N., *Machine Learning with Spark*, Packt
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pentreath N.，*使用Spark进行机器学习*，Packt
- en: Summary
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the main techniques for building a recommender
    system. In a user-based scenario, we assume that we have enough pieces of information
    about the users to be able to cluster them, and moreover, we implicitly assume
    that similar users would like the same products. In this way, it's immediate to
    determine the neighborhood of every new user and to suggest the products positively
    rated by his/her peers. In a similar way, a content-based scenario is based on
    the clustering of products according to their peculiar features. In this case,
    the assumption is weaker, because it's more probable that a user who bought an
    item or rated it positively will do the same with similar products.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了构建推荐系统的主要技术。在基于用户的场景中，我们假设我们拥有足够关于用户的信息来对他们进行聚类，并且我们隐含地假设相似的用户会喜欢相同的产品。这样，就可以立即确定每个新用户所在的邻域，并建议其同龄人给予正面评价的产品。以类似的方式，基于内容的场景是基于根据产品的独特特征对产品进行聚类。在这种情况下，假设较弱，因为更有可能的是，购买过某个商品或给予正面评价的用户会对类似产品做同样的事情。
- en: Then we introduced collaborative filtering, which is a technique based on explicit
    ratings, used to predict all missing values for all users and products. In the
    memory-based variant, we don't train a model but we try to work directly with
    a user-product matrix, looking for the k-nearest neighbors of a test user, and
    computing the ranking through an average. This approach is very similar to the
    user-based scenario and has the same limitations; in particular, it's very difficult
    to manage large matrices. On the other hand, the model-based approach is more
    complex, but, after training the model, it can predict the ratings in real time.
    Moreover, there are parallel frameworks like Spark, which can be employed to process
    a huge amount of data using a cluster of cheap servers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们介绍了协同过滤，这是一种基于显式评分的技术，用于预测所有用户和产品的所有缺失值。在基于记忆的变体中，我们不训练模型，而是直接与用户-产品矩阵一起工作，寻找测试用户的k个最近邻，并通过平均计算排名。这种方法与基于用户的场景非常相似，并且具有相同的局限性；特别是，管理大型矩阵非常困难。另一方面，基于模型的方法更复杂，但在训练模型后，它可以实时预测评分。此外，还有像Spark这样的并行框架，可以使用廉价的集群服务器处理大量数据。
- en: In the next chapter, we're going to introduce some natural language processing
    techniques, which are very important when automatically classifying texts or working
    with machine translation systems.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一些自然语言处理技术，这些技术在自动分类文本或与机器翻译系统协同工作时非常重要。
