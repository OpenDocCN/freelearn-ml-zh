<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer594">
    <h1 class="chapterNumber">13</h1>
    <h1 class="chapterTitle" id="_idParaDest-289">Challenging Data – Too Much, Too Little, Too Complex</h1>
    <p class="normal">Challenging data takes many forms throughout the course of a machine learning project, and the journey of each new project represents an adventure requiring a pioneer spirit. Beginning with uncharted data that must be explored, the data must then be wrangled before it can be used with the learning algorithm. Even then, there may still be wild aspects of the data that need to be tamed for the project to be successful. Extraneous information must be culled, small-but-important details must be cultivated, and tangled webs of complexity must be cleared from the learner’s path.</p>
    <p class="normal">Conventional wisdom in the big data era suggests that data is treasure, but as the saying goes, one can have “too much of a good thing.” Most machine learning algorithms will happily indulge in as much data as they are fed, which leads to a new set of problems akin to overeating. An abundance of data can overwhelm the learner with unnecessary information, obscure important patterns, and shift the learner’s attention from the details that matter to those that are obvious. Thus, it may be better to avoid the “more is always better” mindset and instead find a balance between quantity and quality.</p>
    <p class="normal">The purpose of this chapter is to consider techniques that can be used to adapt to a dataset’s signal-to-noise ratio. You will learn:</p>
    <ul>
      <li class="bulletList">How to handle datasets with an overwhelming number of features</li>
      <li class="bulletList">Methods for making use of feature values that are missing or appear very infrequently</li>
      <li class="bulletList">Approaches for modeling rare target outcomes</li>
    </ul>
    <p class="normal">You will discover that some learning algorithms are better equipped at performing these techniques independently, while others will require you to intervene more extensively in the process. In either case, due to the prevalence of these types of data issues and their status as some of the most challenging problems in machine learning, it is important to understand the ways that they can be remedied.</p>
    <h1 class="heading-1" id="_idParaDest-290">The challenge of high-dimension data</h1>
    <p class="normal">If someone <a id="_idIndexMarker1433"/>says that they are struggling to handle the size of a dataset, it is easy to assume that they are talking about having too many rows or that the data uses too much memory or storage space. Indeed, these are common issues that cause problems for new machine learning practitioners. In this scenario, the solutions tend to be technical rather than methodological; one generally chooses a more efficient algorithm or uses hardware or a cloud computing platform capable of consuming large datasets. In the worst case, one can take a random sampling and simply discard some of the excessive rows.</p>
    <p class="normal">The challenge of having too much data can also apply to a dataset’s columns, making the dataset overly wide rather than overly long. It may require some creative thinking to imagine why this happens, or why it is a problem, because it is rarely encountered in the tidy confines of teaching examples. Even in real-world practice, it may be quite some time before someone encounters this problem, as useful predictors can be scarce, and datasets are often scrounged piece by piece. For such projects, having too many predictors would be a good problem to have!</p>
    <p class="normal">However, consider a situation in which a data-driven organization, acutely aware of the competitive advantage of big data, has amassed a war chest of information from a variety of sources. Perhaps they collected some of the data directly through the ordinary course of business, purchased supplemental data from vendors, and gathered some via additional sensors or indirect, passive interactions via the internet. All these sources are merged into a single table that provides a rich but highly complex and varied set of features. The resulting table was not carefully constructed piece by piece, but rather through a mishmash of data elements, some of which will be more useful than others. Today, this type of data treasure trove is found predominantly in very large or very data-savvy organizations, but it is likely that an increasing number will have access to similar datasets in the future. Datasets are growing increasingly wide over time, even before considering inherently feature-rich sources such as text, audio, image, or genetic data.</p>
    <p class="normal">The challenge of these types of high-dimension datasets, in short, has much to do with the fact that more data points have been collected than are truly needed to represent the <a id="_idIndexMarker1434"/>underlying pattern. </p>
    <p class="normal">The additional data points add noise or subtle variations across examples and may distract a learning algorithm from the important trends. This describes<a id="_idIndexMarker1435"/> the <strong class="keyWord">curse of dimensionality</strong> in which learners fail as the number of features increases. If we imagine each additional feature as a new dimension of the example—and here, the word “dimension” is used both in a literal and a metaphorical sense—then as the dimensions increase, the richness of our understanding of any given example increases, but so does the example’s relative uniqueness. In a sufficiently high-dimension space, every example is unique, as it is comprised of its own distinct combination of feature values.</p>
    <p class="normal">Consider an analogy. A fingerprint uniquely identifies individual people but it is not necessary to store all of a fingerprint’s details to make an accurate match. In fact, out of the limitless details found in each fingerprint, a forensic investigator may use only 12 to 20 distinct points to confirm a match; even computerized fingerprint scanners use only 60 to 80 points. Any additional detail is likely to be superfluous and would detract from the match quality, and taken to an extreme, might cause failed matches—even if the fingerprints are from the same person! For example, including too much detail might lead to false negatives as the learning algorithm is distracted by the print’s orientation or image quality, but too little detail may lead to false positives as the algorithm has too few features to distinguish among similar candidates. Clearly, it is important to find a balance between too much and too little detail. This is, in essence, the goal of <strong class="keyWord">dimensionality reduction</strong>, which <a id="_idIndexMarker1436"/>seeks to remedy the curse of dimensionality by identifying the important details.</p>
    <figure class="mediaobject"><img alt="A picture containing text  Description automatically generated" src="../Images/B17290_13_01.png"/></figure>
    <p class="packt_figref">Figure 13.1: Dimensionality reduction helps to ignore noise and emphasize the key details that will be helpful to learn the underlying pattern</p>
    <p class="normal">In contrast to the problem of very long datasets, the solutions needed to learn from wide datasets are completely different and are as much conceptual as they are practical. One cannot simply randomly discard columns as is possible with rows because some columns are more useful than others. Instead, a systematic approach is taken, often cooperating with the learning algorithm itself to find the balance between too much and too little detail. As you will learn in the coming sections, some of these methods are integrated into the learning process while others will require a more hands-on approach.</p>
    <h2 class="heading-2" id="_idParaDest-291">Applying feature selection</h2>
    <p class="normal">In the <a id="_idIndexMarker1437"/>context of supervised machine learning, the goal of feature selection is to alleviate the curse of dimensionality by choosing only the most important predictors. Feature selection may also be beneficial even in the case of unsupervised learning due to its ability to simplify datasets by eliminating redundant or useless information. In addition to feature selection’s primary goal of assisting a learning algorithm’s attempts to separate the signal from the noise, additional benefits of the practice include:</p>
    <ul>
      <li class="bulletList">Shrinking the size of the dataset and decreasing storage requirements</li>
      <li class="bulletList">Reducing the time or computational expense for model training</li>
      <li class="bulletList">Enabling data scientists to focus on fewer features for data exploration and visualization</li>
    </ul>
    <p class="normal">Rather than attempting to find the single most optimal complete set of predictors, which can be very computationally expensive, feature selection tends to focus on identifying useful individual features or subsets of features. To do so, feature selection typically relies on heuristics that reduce the number of subsets that are searched. This reduces the computing cost but may lead to missing the best possible solution.</p>
    <p class="normal">To search for subsets of useful features is to assume that some predictors are useless, or at least less useful than others. Yet, despite the validity of this premise, it is not always clear what makes some features useful and others not. Of course, there may be obviously irrelevant features that provide no predictive value, but there may also be useful features that are redundant and therefore unnecessary for the learning algorithm. The trick is recognizing that something that appears redundant in one context may actually be useful in a different context.</p>
    <p class="normal">The following figure illustrates the ability of useful features to disguise themselves as seemingly useless and redundant predictors. The scatterplot depicts a relationship between two hypothetical features, each having values in the approximate range of -50 to 50, and being used to predict a binary outcome, triangles versus circles.</p>
    <figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" src="../Images/B17290_13_02.png"/></figure>
    <p class="packt_figref">Figure 13.2: Features 1 and 2 are seemingly useless and redundant but have predictive value when used together</p>
    <p class="normal">Knowing<a id="_idIndexMarker1438"/> the value of feature 1 or 2 alone provides virtually no value toward predicting the outcome of the target, as the circles and triangles are almost completely evenly split for any value of either feature. In quantitative terms, this is demonstrated by a very weak correlation between the features and the outcome. A simple feature selection algorithm that examines only the relationship between one feature and the outcome may thus determine that neither feature is useful for prediction. Additionally, because the correlation between the two features is about 0.90, a more sophisticated feature selection algorithm that simultaneously considers the pair may inadvertently exclude one of the two due to the seeming redundancy.</p>
    <p class="normal">Despite the seemingly useless and redundant nature of the two features, the scatterplot clearly depicts their predictive ability when used together: if feature 2 is greater than feature 1, then predict triangle; otherwise, predict circle. A useful feature selection method ought to be able to recognize these types of patterns; otherwise, it risks excluding important predictors from the learning algorithm. However, the feature selection technique also needs to consider computational efficiency, as examining every potential combination of features is infeasible except for the smallest of datasets.</p>
    <p class="normal">The need to balance the search for useful, non-redundant features with the possibility that features may only be useful in combination with others is part of the reason there is no one-size-fits-all approach to feature selection. Depending on the use case and the chosen learning <a id="_idIndexMarker1439"/>algorithm, different techniques can be applied that perform a less rigorous or more thorough search of the features.</p>
    <div class="note">
      <p class="normal">For a deeper dive into feature selection, see <em class="italic">An Introduction to Variable and Feature Selection, 2003, Guyon, I. and Elisseeff, A., Journal of Machine Learning Research, Vol. 3, pp. 1157-1182</em>.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-292">Filter methods</h3>
    <p class="normal">Perhaps the <a id="_idIndexMarker1440"/>most accessible form of feature selection is <a id="_idIndexMarker1441"/>the category of <strong class="keyWord">filter methods</strong>, which use a relatively simple scoring function to measure each feature’s importance. The resulting scores can then be used to rank the features and limit the number used in the predictive model. Due to the simplicity of this approach, filter methods are often used as a first step in an iterative process of data exploration, feature engineering, and model building. One might initially apply a crude filter to identify the most interesting candidate features for in-depth exploration and visualization, then apply more vigorous feature selection methods later if further reduction is desired.</p>
    <p class="normal">A single defining characteristic of filter methods is the use of a proxy measure of feature importance. The measure is a proxy because it is a substitute for what we truly care about—the predictive ability of the feature—but we cannot know this without first building the predictive model. Instead, we choose a much simpler metric, which we hope reflects the utility of the feature when it is later added to the model. For instance, in a numeric prediction model, one might compute bivariate correlations between each feature and the target and select only the features that are substantially correlated with the target. For a binary or categorical target, a comparable approach might involve constructing a single-variable classifier, examining contingency tables for strong bivariate relationships between the features and the target, or using a metric like information gain, which was described in <em class="chapterRef">Chapter 5</em>, <em class="italic">Divide and Conquer – Classification Using Decision Trees and Rules</em>. A benefit of these types of simple feature selection metrics is that they are unlikely to contribute to overfitting because the proxy measures use a different approach and make different assumptions about the data than the learning algorithm.</p>
    <p class="normal">The greatest benefit of filter methods may be the fact that they are scalable even for datasets with very large numbers of features. This efficiency stems from the fact the filtering method only computes one importance score for each feature and then sorts the predictors by these <a id="_idIndexMarker1442"/>scores from most to least important. Thus, as the number of features increases, the computational expense grows relatively slowly and in direct proportion <a id="_idIndexMarker1443"/>to the number of predictors. Note that the product of this approach is a rank-ordered list of features rather than a single best set of features; therefore, subjective judgment is required to determine the optimal cutoff between important and not important features.</p>
    <p class="normal">Although filter methods are computationally efficient, they lack the ability to consider groups of features, which means that important predictors may be excluded if they are only useful in combination with others. Additionally, the fact that filter methods are unlikely to contribute to overfitting comes with the potential downside that they also may not result in the set of features that are best suited to work with the desired learning algorithm. The feature selection method described in the next section sacrifices computational efficiency to address each of these concerns.</p>
    <h3 class="heading-3" id="_idParaDest-293">Wrapper methods and embedded methods</h3>
    <p class="normal">In contrast to filter<a id="_idIndexMarker1444"/> methods, which use a proxy measure of variable importance, <strong class="keyWord">wrapper methods</strong> use <a id="_idIndexMarker1445"/>the machine learning algorithm itself to identify the importance of variables or subsets of variables. Wrapper methods <a id="_idIndexMarker1446"/>are based on the simple idea that as more important features are provided to the algorithm, its ability to perform the learning task should improve. In other words, its error rate should be reduced as important predictors are included or the correct combinations are included. </p>
    <p class="normal">Thus, by iteratively building models composed of different combinations of features and examining how the model’s performance changes, it is possible to identify the important predictors and sets of predictors. By systematically testing all possible combinations of features, it is even possible to identify the overall best set of predictors.</p>
    <p class="normal">However, as one might expect, the process of testing all possible combinations of features is extremely computationally inefficient. For a dataset with <em class="italic">p</em> predictors, there are <em class="italic">2</em><sup class="superscript-italic" style="font-style: italic;">p</sup> potential sets of predictors that must be tested, which causes the computational expense of this technique to grow relatively quickly as additional features are added. For example, a dataset with only 10 predictors would require <em class="italic">2</em><sup class="superscript-italic" style="font-style: italic;">10 </sup><em class="italic">= 1,024</em> different models to be evaluated, while a dataset adding just five more predictors would require <em class="italic">2</em><sup class="superscript-italic" style="font-style: italic;">15</sup><em class="italic"> = 32,768</em> models, which is over 30 times the computational cost! Clearly, this approach is not viable except for the smallest of datasets and the simplest of machine learning algorithms. One solution to this problem might be to first reduce the number of features using a filter method, but not only does this risk missing important combinations of features but it would also require such a reduction in dimensionality that it may negate many of the benefits of wrapper methods.</p>
    <p class="normal">Rather than letting <a id="_idIndexMarker1447"/>its inefficiency prevent us from capitalizing on its upsides, we can instead use heuristics to avoid searching every combination of features. In particular, the “greedy” approach described in <em class="chapterRef">Chapter 5</em>, <em class="italic">Divide and Conquer – Classification Using Decision Trees and Rules</em>, which helped grow trees efficiently, can also be used here. You may recall that the idea of a greedy algorithm is to use data on a first-come, first-served basis, with the most predictive features used first. Although this technique is not guaranteed to find the optimal solution, it drastically reduces the number of combinations that must be tested.</p>
    <p class="normal">There are two basic approaches for adapting wrapper methods for greedy feature selection. Both involve probing the learning algorithm by changing one variable at a time. The<a id="_idIndexMarker1448"/> technique of <strong class="keyWord">forward selection</strong> begins by feeding each feature to the model one by one, to determine which of them results in the best one-predictor model. The next iteration of forward selection keeps the first best predictor in the model and tests the remaining features to identify which makes the best two-predictor model. As might be expected, this process can continue selecting the best three-predictor model, four-predictor model, and so on, until all features have been selected. However, as the point of feature selection is specifically not to select the entire set of features, the process of forward selection stops early, when adding additional features no longer improves the model’s performance beyond a specific threshold.</p>
    <p class="normal">The similar <a id="_idIndexMarker1449"/>technique of <strong class="keyWord">backward elimination</strong> works the same way, but in reverse. Beginning with a model containing all features, the model iterates repeatedly, eliminating the least-predictive feature each time, until stopping when eliminating a feature decreases the model’s performance more than a desired threshold.</p>
    <p class="normal">Learning algorithms <a id="_idIndexMarker1450"/>known as <strong class="keyWord">embedded methods</strong> have a form of built-in wrappers much like forward selection. These methods select the best features automatically during the model training process. You are already familiar with one such method, decision trees, which uses greedy forward selection to determine the best feature subset. Most machine learning techniques do not have embedded feature selection; the dimensions must be reduced beforehand. The next section demonstrates how these methods can be applied in R via a variant of the machine learning algorithm introduced in <em class="chapterRef">Chapter 6</em>, <em class="italic">Forecasting Numeric Data – Regression Methods</em>.</p>
    <h3 class="heading-3" id="_idParaDest-294">Example – Using stepwise regression for feature selection</h3>
    <p class="normal">One widely known <a id="_idIndexMarker1451"/>implementation of wrapper <a id="_idIndexMarker1452"/>methods is <strong class="keyWord">stepwise regression</strong>, which uses forward or backward selection to identify a set of features for a regression model. To demonstrate this technique, we’ll revisit the Titanic passenger dataset used in the previous two <a id="_idIndexMarker1453"/>chapters and build a logistic regression model that predicts whether each passenger survived the ill-fated voyage. To begin, we’ll use the tidyverse to read the data and apply some simple data preparation steps. The following sequence of commands creates a missing value indicator for <code class="inlineCode">Age</code>, imputes the average age for the missing <code class="inlineCode">Age</code> values, imputes <code class="inlineCode">X</code> for the missing <code class="inlineCode">Cabin</code> and <code class="inlineCode">Embarked</code> values, and converts <code class="inlineCode">Sex</code> to a factor:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>tidyverse<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> titanic_train <span class="hljs-operator">&lt;-</span> read_csv<span class="hljs-punctuation">(</span><span class="hljs-string">"titanic_train.csv"</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>
      Age_MVI <span class="hljs-operator">=</span> if_else<span class="hljs-punctuation">(</span><span class="hljs-built_in">is.na</span><span class="hljs-punctuation">(</span>Age<span class="hljs-punctuation">),</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">),</span>
      Age <span class="hljs-operator">=</span> if_else<span class="hljs-punctuation">(</span><span class="hljs-built_in">is.na</span><span class="hljs-punctuation">(</span>Age<span class="hljs-punctuation">),</span> mean<span class="hljs-punctuation">(</span>Age<span class="hljs-punctuation">,</span> na.rm <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">),</span> Age<span class="hljs-punctuation">),</span>
      Cabin <span class="hljs-operator">=</span> if_else<span class="hljs-punctuation">(</span><span class="hljs-built_in">is.na</span><span class="hljs-punctuation">(</span>Cabin<span class="hljs-punctuation">),</span> <span class="hljs-string">"X"</span><span class="hljs-punctuation">,</span> Cabin<span class="hljs-punctuation">),</span>
      Embarked <span class="hljs-operator">=</span> factor<span class="hljs-punctuation">(</span>if_else<span class="hljs-punctuation">(</span><span class="hljs-built_in">is.na</span><span class="hljs-punctuation">(</span>Embarked<span class="hljs-punctuation">),</span> <span class="hljs-string">"</span><span class="hljs-string">X"</span><span class="hljs-punctuation">,</span> Embarked<span class="hljs-punctuation">)),</span>
      Sex <span class="hljs-operator">=</span> factor<span class="hljs-punctuation">(</span>Sex<span class="hljs-punctuation">)</span>
    <span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The stepwise process needs to know the starting and ending conditions for feature selection, or the minimum and maximum set of variables that can be included. In our case, we’ll define the simplest possible model as one containing no variables at all—a model with only a constant intercept term. </p>
    <p class="normal">To define this model in R, we’ll use the <code class="inlineCode">glm()</code> function to model survival as a function of a constant intercept using the <code class="inlineCode">Survived ~ 1</code> formula. Setting the <code class="inlineCode">family</code> parameter to <code class="inlineCode">binomial</code> defines a logistic regression model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> simple_model <span class="hljs-operator">&lt;-</span> glm<span class="hljs-punctuation">(</span>Survived <span class="hljs-operator">~</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> family <span class="hljs-operator">=</span> binomial<span class="hljs-punctuation">,</span>
                       data <span class="hljs-operator">=</span> titanic_train<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The full model still uses logistic regression, but includes many more predictors:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> full_model <span class="hljs-operator">&lt;-</span> glm<span class="hljs-punctuation">(</span>Survived <span class="hljs-operator">~</span> Age <span class="hljs-operator">+</span> Age_MVI <span class="hljs-operator">+</span> Embarked <span class="hljs-operator">+</span>
                               Sex <span class="hljs-operator">+</span> Pclass <span class="hljs-operator">+</span> SibSp <span class="hljs-operator">+</span> Fare<span class="hljs-punctuation">,</span>
                    family <span class="hljs-operator">=</span> binomial<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> titanic_train<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Forward selection will begin with the simple model and determine which of the features in the full model are worth including in the final model. The <code class="inlineCode">step()</code> function in the base R <code class="inlineCode">stats</code> package <a id="_idIndexMarker1454"/>provides this functionality; however, because other packages also have <code class="inlineCode">step()</code> functions, specifying <code class="inlineCode">stats::step()</code> ensures the correct one is used. The first function argument <a id="_idIndexMarker1455"/>provides the starting model, the <code class="inlineCode">scope</code> parameter requires the <code class="inlineCode">formula()</code> of the full model, and the direction is set to <code class="inlineCode">forward</code> stepwise regression:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sw_forward <span class="hljs-operator">&lt;-</span> stats<span class="hljs-operator">::</span>step<span class="hljs-punctuation">(</span>simple_model<span class="hljs-punctuation">,</span>
                            scope <span class="hljs-operator">=</span> formula<span class="hljs-punctuation">(</span>full_model<span class="hljs-punctuation">),</span>
                            direction <span class="hljs-operator">=</span> <span class="hljs-string">"forward"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This command generates a set of outputs for each iteration of the stepwise process, but only the first and last iterations are included here for brevity.</p>
    <div class="packt_tip">
      <p class="normal">If you are selecting from a large number of variables, set <code class="inlineCode">trace = 0</code> in the <code class="inlineCode">step()</code> function to turn off the output for each iteration.</p>
    </div>
    <p class="normal">At the start of the stepwise process, it begins with the simple model using the <code class="inlineCode">Survived ~ 1</code> formula, which models survival using only a constant intercept term. The first block of output thus displays the model quality at the start and after evaluating seven other candidate models each with a single additional predictor added. The row labeled <code class="inlineCode">&lt;none&gt;</code> refers to the model’s quality at the start of this iteration and how it ranks compared to the seven other candidates:</p>
    <pre class="programlisting con"><code class="hljs-con">Start:  AIC=1188.66
Survived ~ 1
           Df Deviance    AIC
+ Sex       1    917.8  921.8
+ Pclass    1   1084.4 1088.4
+ Fare      1   1117.6 1121.6
+ Embarked  3   1157.0 1165.0
+ Age_MVI   1   1178.9 1182.9
+ Age       1   1182.3 1186.3
&lt;none&gt;          1186.7 1188.7
+ SibSp     1   1185.5 1189.5
</code></pre>
    <p class="normal">The quality measure used, AIC, is a measure of a model’s relative quality compared to other models. In particular, it refers to the <strong class="keyWord">Akaike information criterion</strong>. While a formal definition of AIC is outside the scope of this chapter, the measure is intended to balance model complexity <a id="_idIndexMarker1456"/>and model fit. Lower AIC values are better. Therefore, the model that includes <code class="inlineCode">Sex</code> is the best out of the six other candidate models as well as the original model. In the final iteration, the base model uses <code class="inlineCode">Sex</code>, <code class="inlineCode">Pclass</code>, <code class="inlineCode">Age</code>, and <code class="inlineCode">SibSp</code>, and no additional <a id="_idIndexMarker1457"/>features reduce the AIC further—the <code class="inlineCode">&lt;none&gt;</code> row is ranked above the candidate models adding <code class="inlineCode">Embarked</code>, <code class="inlineCode">Fare</code>, and <code class="inlineCode">Age_MVI</code> features:</p>
    <pre class="programlisting con"><code class="hljs-con">Step:  AIC=800.84
Survived ~ Sex + Pclass + Age + SibSp
           Df Deviance    AIC
&lt;none&gt;          790.84 800.84
+ Embarked  3   785.27 801.27
+ Fare      1   789.65 801.65
+ Age_MVI   1   790.59 802.59
</code></pre>
    <p class="normal">At this point, the forward selection process stops. We can obtain the formula for the final model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> formula<span class="hljs-punctuation">(</span>sw_forward<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Survived ~ Sex + Pclass + Age + SibSp
</code></pre>
    <p class="normal">We can also obtain the final model’s estimated regression coefficients:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sw_forward<span class="hljs-operator">$</span>coefficients
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">(Intercept)     Sexmale      Pclass         Age       SibSp 
 5.19197585 -2.73980616 -1.17239094 -0.03979317 -0.35778841
</code></pre>
    <p class="normal">Backward elimination is even simpler to execute. By providing a model with the complete set of features to test and setting <code class="inlineCode">direction = "backward"</code>, the model will iterate and systematically eliminate any features that will result in a better AIC. For example, the first step begins with a full set of predictors, but eliminating the <code class="inlineCode">Fare</code>, <code class="inlineCode">Age_MVI</code>, or <code class="inlineCode">Embarked</code> features results in a lower AIC:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sw_backward <span class="hljs-operator">&lt;-</span> stats<span class="hljs-operator">::</span>step<span class="hljs-punctuation">(</span>full_model<span class="hljs-punctuation">,</span> direction <span class="hljs-operator">=</span> <span class="hljs-string">"backward"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Start:  AIC=803.49
Survived ~ Age + Age_MVI + Embarked + Sex + Pclass + SibSp + 
    Fare
           Df Deviance     AIC
- Fare      1   783.88  801.88
- Age_MVI   1   784.81  802.81
- Embarked  3   789.42  803.42
&lt;none&gt;          783.49  803.49
- SibSp     1   796.34  814.34
- Age       1   810.97  828.97
- Pclass    1   844.74  862.74
- Sex       1  1016.36 1034.36
</code></pre>
    <p class="normal">At each<a id="_idIndexMarker1458"/> iteration, the worst feature is eliminated, but by the final step, eliminating any of the remaining features leads to a higher AIC, and <a id="_idIndexMarker1459"/>therefore leads to a lower-quality model than the baseline. Thus, the process stops here: </p>
    <pre class="programlisting con"><code class="hljs-con">Step:  AIC=800.84
Survived ~ Age + Sex + Pclass + SibSp
         Df Deviance     AIC
&lt;none&gt;        790.84  800.84
- SibSp   1   805.33  813.33
- Age     1   819.32  827.32
- Pclass  1   901.80  909.80
- Sex     1  1044.10 1052.10
</code></pre>
    <p class="normal">In this case, forward selection and backward elimination resulted in the same set of predictors, but this is not necessarily always the case. Differences may arise if certain features work better in groups or if they are interrelated in some other way. </p>
    <p class="normal">As noted previously, one of the downsides of the heuristics used by wrapper methods is that they are not guaranteed to find the single most optimal set of predictors; however, this shortcoming is exactly what makes the feature selection process computationally feasible.</p>
    <h3 class="heading-3" id="_idParaDest-295">Example – Using Boruta for feature selection</h3>
    <p class="normal">For a <a id="_idIndexMarker1460"/>more robust yet much more computationally<a id="_idIndexMarker1461"/> intensive feature selection method, the <code class="inlineCode">Boruta</code> package implements a wrapper around the random forest algorithm, which will be introduced in <em class="chapterRef">Chapter 14</em>, <em class="italic">Building Better Learners</em>. For now, it suffices to know that random forests are a variant of decision trees, which provide a measure of variable importance. By systematically testing random subsets of variables repeatedly, it <a id="_idIndexMarker1462"/>is possible to determine whether a feature is significantly more or less important than others using statistical hypothesis testing techniques.</p>
    <div class="note">
      <p class="normal">Because of its heavy reliance on the random forest technique, it is no surprise that the technique shares a name with Boruta, a mythological Slavic creature thought to dwell in swamps and forests. To read more about Boruta’s implementation details, see <em class="italic">Feature Selection with the Boruta Package, Kursa, M. B. and Rudnicki, W. R., 2010, Journal of Statistical Software, Vol. 36, Iss. 11</em>.</p>
    </div>
    <p class="normal">The <code class="inlineCode">Boruta</code> technique <a id="_idIndexMarker1463"/>employs a clever trick using so-called “shadow features” to determine whether a variable is important. These shadow features are copies of the dataset’s original features, but with the values shuffled randomly so that any association between the feature and the target outcome is broken. Thus, these shadow features are, by definition, nonsense and unimportant, and should provide zero predictive benefits to the model except by random chance. They serve as a baseline by which the other features are judged.</p>
    <p class="normal">After running the original features and shadow features through the random forest modeling process, the importance of each original feature is compared to the most important shadow feature. Features that are significantly better than the shadow feature are deemed important; those significantly worse are deemed unimportant and permanently removed. The algorithm iterates repeatedly until all features are deemed important or unimportant, or the process hits a predetermined limit of iterations.</p>
    <p class="normal">To see this in action, let’s apply the <code class="inlineCode">Boruta</code> algorithm to the same Titanic training dataset constructed in the previous section. Just to prove that the algorithm can detect truly useless features, for demonstration purposes we can add one to the dataset. First, we’ll set the random seed to the arbitrary number <code class="inlineCode">12345</code> to ensure your results match those shown here. Then, we’ll assign each of the 891 training examples a random value between 1 and 100. </p>
    <p class="normal">Because the numbers are completely random, this feature should almost certainly be found useless, except in the case of dumb luck:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">12345</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> titanic_train<span class="hljs-operator">$</span>rand_vals <span class="hljs-operator">&lt;-</span> runif<span class="hljs-punctuation">(</span>n <span class="hljs-operator">=</span> <span class="hljs-number">891</span><span class="hljs-punctuation">,</span> <span class="hljs-built_in">min</span> <span class="hljs-operator">=</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-built_in">max</span> <span class="hljs-operator">=</span> <span class="hljs-number">100</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Next, we’ll load the <code class="inlineCode">Boruta</code> package and apply it to the Titanic dataset. The syntax is similar to training a machine learning model; here, we specify the model using the formula interface to list the target and predictors:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>Boruta<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> titanic_boruta <span class="hljs-operator">&lt;-</span> Boruta<span class="hljs-punctuation">(</span>Survived <span class="hljs-operator">~</span> PassengerId <span class="hljs-operator">+</span> Age <span class="hljs-operator">+</span>
                            Sex <span class="hljs-operator">+</span> Pclass <span class="hljs-operator">+</span> SibSp <span class="hljs-operator">+</span> random_vals<span class="hljs-punctuation">,</span>
                           data <span class="hljs-operator">=</span> titanic_train<span class="hljs-punctuation">,</span> doTrace <span class="hljs-operator">=</span> <span class="hljs-number">1</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">doTrace</code> parameter <a id="_idIndexMarker1464"/>is set to <code class="inlineCode">1</code> to request verbose output, which produces a status update as the <a id="_idIndexMarker1465"/>algorithm reaches key points in the iteration process. Here, we see the output after 10 iterations, which shows that the <code class="inlineCode">rand_vals</code> feature has unsurprisingly been rejected as unimportant, while four features were confirmed as important and one feature remains undetermined:</p>
    <pre class="programlisting con"><code class="hljs-con">After 10 iterations, +0.51 secs: 
 confirmed 4 attributes: Age, Pclass, Sex, SibSp;
 rejected 1 attribute: rand_vals;
 still have 1 attribute left.
</code></pre>
    <p class="normal">Once the algorithm has completed, type the name of the object to see the results:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> titanic_boruta
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Boruta performed 99 iterations in 4.555043 secs.
 4 attributes confirmed important: Age, Pclass, Sex, SibSp;
 1 attributes confirmed unimportant: rand_vals;
 1 tentative attributes left: PassengerId;
</code></pre>
    <p class="normal">The <code class="inlineCode">Boruta()</code> function is set to a limit of 100 runs by default, which it hit after iterating 99 times in about 4.5 seconds. Before stopping, four features were found to be important and one was found to be unimportant. The <code class="inlineCode">PassengerId</code> feature, which is listed as tentative, was unable to be confirmed as important or unimportant. Setting the <code class="inlineCode">maxRuns</code> parameter to a higher value than 100 can help come to a conclusion—in this case, setting <code class="inlineCode">maxRuns = 500</code> will confirm <code class="inlineCode">PassengerId</code> to be unimportant after 486 iterations.</p>
    <p class="normal">It is also possible to plot the importance of the features relative to one another:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> plot<span class="hljs-punctuation">(</span>titanic_boruta<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The resulting visualization is shown in <em class="italic">Figure 13.3</em>. For each of the six features, as well as the max, mean (average), and min performing shadow features, a boxplot shows the distribution <a id="_idIndexMarker1466"/>of importance metrics for that feature. Using<a id="_idIndexMarker1467"/> these results, we can confirm that the <code class="inlineCode">PassengerId</code> is slightly less important than the max shadow feature, and <code class="inlineCode">rand_vals</code> is even less important than that:</p>
    <figure class="mediaobject"><img alt="Chart, timeline  Description automatically generated" src="../Images/B17290_13_03.png"/></figure>
    <p class="packt_figref">Figure 13.3: Plotting the Boruta output shows the relative importance of features compared to each other and the shadow features</p>
    <p class="normal">Based on the exploration of the Titanic dataset we performed in <em class="chapterRef">Chapter 11</em>, <em class="italic">Being Successful with Machine Learning</em>, the high importance of the <code class="inlineCode">Sex</code> and <code class="inlineCode">Pclass</code> features is unsurprising. Likewise, we would not expect the <code class="inlineCode">PassengerId</code> to be important, unless the IDs were somehow linked to Titanic survival rather than being assigned at random. This being said, even though the results of this feature selection process did not reveal new insights, the technique would be much more helpful for datasets that are not as easy to explore by hand, or where the real-world meaning of the features is unknown. Of course, this is just one approach for dealing with a large number of features of undetermined importance; the next section describes an alternative that may perform better, especially if many of the features are correlated.</p>
    <div class="packt_tip">
      <p class="normal">The <code class="inlineCode">Boruta</code> technique can be very computationally intensive, and on real-world datasets, it will generally take minutes or even hours to complete rather than seconds as with the Titanic data. The authors of the package estimate that on a modern computer, it needs roughly one hour per million feature-example combinations. For example, a dataset with 10,000 rows and 50 features will take roughly half an hour to complete. Increasing the size of this dataset to 100,000 rows would require about five hours of processing time!</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-296">Performing feature extraction</h2>
    <p class="normal">Feature selection is not the only approach available to reduce the dimensionality of a highly dimensional dataset. Another possibility is to synthesize a smaller number of composite predictors. This is the <a id="_idIndexMarker1468"/>goal of <strong class="keyWord">feature extraction</strong>, a dimensionality reduction technique that creates new features rather than selecting a subset of <a id="_idIndexMarker1469"/>existing features. The extracted features are constructed such that they reduce the amount of redundant information while keeping as much useful information as possible. Of course, finding the ideal balance between too much and too little information is a challenge in itself.</p>
    <h3 class="heading-3" id="_idParaDest-297">Understanding principal component analysis</h3>
    <p class="normal">To begin<a id="_idIndexMarker1470"/> to understand feature extraction, start<a id="_idIndexMarker1471"/> by imagining a dataset with a very large number of features. For instance, to predict applicants likely to default on a loan, a dataset may include hundreds of applicant attributes. Obviously, some of the features are going to be predictive of the target outcome, but it is likely that many of the features are predictive of each other as well. For example, a person’s age, education level, income, zip code, and occupation are all predictive of their likelihood to pay back a loan, but they are also predictive of each other to varying degrees. Their interrelatedness suggests that there is a degree of overlap or joint dependency among them, which is reflected in their covariance and correlation.</p>
    <p class="normal">It may be the case that the reason these five attributes of loan applicants are related is that they are components of a smaller number of attributes that are the true, underlying drivers of loan payment behavior. In particular, we might believe that loan payment likelihood is based on an applicant’s responsibility and affluence, but because these concepts are difficult to measure directly, we instead use multiple readily available proxy measures. The following figure illustrates how each of the five features might capture aspects of the two hidden dimensions of interest. Note that none of the features fully captures either component dimension, but rather, each component dimension is a composite of several features. </p>
    <p class="normal">For instance, a person’s level of responsibility might be captured by their age and education level, while their affluence might be reflected in their income, occupation, and zip code.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_13_04.png"/></figure>
    <p class="packt_figref">Figure 13.4: Five hypothetical attributes of loan applicants might be more simply expressed in two dimensions created from composites of the covariance of each attribute</p>
    <p class="normal">The<a id="_idIndexMarker1472"/> goal of <strong class="keyWord">principal component analysis</strong> (<strong class="keyWord">PCA</strong>) is to <a id="_idIndexMarker1473"/>extract a smaller number of underlying dimensions from a larger number of features by expressing the covariance of multiple correlated attributes as a single vector. Put simply, the covariance refers to the extent to which attributes vary in concert. When one goes up or down, the other tends to go up or down. The resulting vectors are <a id="_idIndexMarker1474"/>known as <strong class="keyWord">principal components</strong> and are constructed as weighted combinations of the original attributes. When applied to a dataset with many correlated features, a much smaller number of principal components may be capable of expressing much of the total variance of the higher-dimension dataset. Although this seems like a lot of technical jargon, and the math required to implement PCA is beyond the scope of the book, we will work toward a conceptual understanding of the process.</p>
    <div class="note">
      <p class="normal">Principal component analysis is closely related to another technique, called <strong class="keyWord">factor analysis</strong>, which<a id="_idIndexMarker1475"/> is a more formal approach for exploring the relationships between observed and unobserved (latent) factors, such as those depicted in the figures here. In practice, both can be applied similarly, but PCA is simpler and avoids building a formal model; it merely reduces the number of dimensions while retaining maximal variation. For a deeper dive into the many subtle distinctions, see the following Stack Exchange thread: <a href="https://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi/"><span class="url">https://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi/</span></a>.</p>
    </div>
    <p class="normal">Revisiting <em class="italic">Figure 13.4</em>, each circle is intended to represent the relationships among each of the five <a id="_idIndexMarker1476"/>features. Circles with greater overlap represent correlated features that may measure a similar underlying concept. Keep in mind that this is a highly simplified representation that does not depict the individual<a id="_idIndexMarker1477"/> data points that would be used to compute the correlations among the features. In reality, these individual data points would represent individual loan applicants and would be positioned in a five-dimensional space with coordinates determined by each applicant’s five feature values. Of course, this is difficult to depict in the two dimensions of this book’s pages, so the circles in this simplified representation should be understood as a cloud-like mass of people with high values of the attribute. In this case, if two features are highly correlated, such as income and education, the two clouds will overlap, because people with high values of one attribute will tend to have high values of the other. <em class="italic">Figure 13.5</em> depicts this relationship:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_13_05.png"/></figure>
    <p class="packt_figref">Figure 13.5: When two features are highly correlated, points with high values of one tend to have high values of the other</p>
    <p class="normal">When examining <em class="italic">Figure 13.5</em>, note that the diagonal arrow that represents the relationship between income and education reflects the covariance between the two features. Knowing whether a point is closer to the start or end of the arrow would provide a good estimate of both income and education. Highly covariant features are thus likely to express similar underlying attributes and therefore may be redundant. In this way, the information expressed by two dimensions, income and education, could be expressed more simply in a single dimension, which would be the principal component of these two features.</p>
    <p class="normal">Applying this relationship to a diagram in three dimensions, we might imagine this principal component as the <em class="italic">z</em> dimension in the following figure:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_13_06.png"/></figure>
    <p class="packt_figref">Figure 13.6: Five attributes with varying degrees of covariance in three dimensions</p>
    <p class="normal">As <a id="_idIndexMarker1478"/>with the two-dimensional case, the positioning<a id="_idIndexMarker1479"/> of the circles is intended to represent the covariance among the features; the circle sizes are meant to represent depth, with larger or smaller circles closer to the front or back of the space. In the three dimensions here, age and education are close on one dimension, occupation and zip code are close on another, and income varies in a third dimension. </p>
    <p class="normal">If we hoped to capture most of the variance while reducing the number of dimensions from three to two, we might project this three-dimensional plot onto a two-dimensional plot as follows:</p>
    <figure class="mediaobject"><img alt="Chart, bubble chart  Description automatically generated" src="../Images/B17290_13_07.png"/></figure>
    <p class="packt_figref">Figure 13.7: Principal component analysis reduces many dimensions into a smaller number of key components</p>
    <p class="normal">With these<a id="_idIndexMarker1480"/> two dimensions, we have constructed the two principal components of the dataset, and in doing so, we have reduced the dimensionality of the dataset from five dimensions with real-world meaning to two dimensions, <em class="italic">x</em> and <em class="italic">y</em>, with no inherent real-world connection. Instead, the two resulting dimensions now reflect linear combinations of the underlying data points; they are <a id="_idIndexMarker1481"/>useful summaries of the underlying data, but are not easily interpretable.</p>
    <p class="normal">We could reduce the dimensionality even further by projecting the dataset onto a line to create a single principal component, as illustrated in <em class="italic">Figure 13.8</em>:</p>
    <figure class="mediaobject"><img alt="Chart, bubble chart  Description automatically generated" src="../Images/B17290_13_08.png"/></figure>
    <p class="packt_figref">Figure 13.8: The first principal component captures the dimension with the greatest variance</p>
    <p class="normal">In this example, the PCA approach extracted one hybrid feature from the dataset’s five original dimensions. Age and education are treated as somewhat redundant, as are occupation and income. </p>
    <p class="normal">Additionally, age and education have an opposite impact on the new feature than zip code—they are pulling the value of <em class="italic">x</em> in opposite directions. If this one-dimensional representation loses too much of the information stored within the original five features, the earlier approaches with two or three components could be used instead. As with many techniques in machine learning, there is a balance between over- and underfitting the data. We’ll see this reflected in a real-world example shortly.</p>
    <p class="normal">Before<a id="_idIndexMarker1482"/> applying PCA, it’s important to know<a id="_idIndexMarker1483"/> that principal components are identified by a deterministic algorithm, which means that the solution is consistent every time the process is completed on a given dataset. Each component vector is also always orthogonal, or perpendicular, to all previous component vectors. The first principal component captures the dimension of highest variance, the next captures the next most, and so on, until a principal component has been constructed for each in the original dataset, or the algorithm stops early when the desired number of components has been reached.</p>
    <h3 class="heading-3" id="_idParaDest-298">Example – Using PCA to reduce highly dimensional social media data</h3>
    <p class="normal">As <a id="_idIndexMarker1484"/>mentioned previously, PCA is a feature extraction technique that reduces the dimensionality of a dataset by synthesizing a smaller set of features from the complete set. We’ll apply this technique to the social media data first described in <em class="chapterRef">Chapter 9</em>, <em class="italic">Finding Groups of Data – Clustering with k-means</em>. You may recall that this dataset includes counts of 36 different words that appeared on the social media pages of 30,000 teens in the United States. The words reflect various interests and activities such as sports, music, religion, and shopping, and although 36 is not an unreasonable number for most machine learning algorithms to handle, if we had more—perhaps hundreds of features—some algorithms might begin to struggle with the curse of dimensionality.</p>
    <p class="normal">We’ll use the tidyverse suite of functions to read and prepare the data. First, we’ll load the package, and use its <code class="inlineCode">read_csv()</code> function to read the social media data as a tibble:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>tidyverse<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> sns_data <span class="hljs-operator">&lt;-</span> read_csv<span class="hljs-punctuation">(</span><span class="hljs-string">"snsdata.csv"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Next, we will <code class="inlineCode">select()</code> only the columns corresponding to the features recording the number of times 36 words were used in each social media profile. The notation here selects from the column named <code class="inlineCode">basketball</code> through the column named <code class="inlineCode">drugs</code> and saves the result in a new tibble called <code class="inlineCode">sns_terms</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sns_terms <span class="hljs-operator">&lt;-</span> sns_data <span class="hljs-operator">|&gt;</span> select<span class="hljs-punctuation">(</span>basketball<span class="hljs-operator">:</span>drugs<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The PCA technique will only work with a matrix of numeric data. However, because each of the resulting 36 columns is a count, no more data preparation is needed. If the dataset included categorical features, it would be necessary to convert these to numeric before proceeding.</p>
    <p class="normal">Base R <a id="_idIndexMarker1485"/>includes a built-in PCA function called <code class="inlineCode">prcomp()</code>, which becomes slow to run as datasets get larger. We’ll use a drop-in substitute from the <code class="inlineCode">irlba</code> package by Bryan W. Lewis, which can be stopped early to return only a subset of the full set of potential principal components. This truncated approach, plus the use of a generally more efficient algorithm, makes the <code class="inlineCode">irlba_prcomp()</code> function much more speedy than <code class="inlineCode">prcomp()</code> on larger datasets, while keeping the syntax and compatibility virtually identical to the base function, in case you are following along with older online tutorials.</p>
    <div class="note">
      <p class="normal">The <code class="inlineCode">irlba</code> package gets its strange-seeming name from the technique it uses: the “implicitly restarted Lanczos bidiagonalization algorithm” developed by Jim Baglama and Lothar Reichel. For more information on this approach, see the package vignette using the <code class="inlineCode">vignette("irlba")</code> command.</p>
    </div>
    <p class="normal">Before beginning, we’ll set the random seed to an arbitrary value of <code class="inlineCode">2023</code> to ensure your results match the book. Then, after loading the required package, we’ll pipe the <code class="inlineCode">sns_terms</code> dataset into the PCA function. The three parameters allow us to limit the result to the first 10 principal components while also standardizing the data by centering each feature around zero and scaling them to have a variance of one. This is usually desirable for much the same reason it is in the k-Nearest Neighbors approach: it prevents features with larger variance from dominating the principal components. The results are saved as an object named <code class="inlineCode">sns_pca</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">2023</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>irlba<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> sns_pca <span class="hljs-operator">&lt;-</span> sns_terms <span class="hljs-operator">|&gt;</span> 
    prcomp_irlba<span class="hljs-punctuation">(</span>n <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-punctuation">,</span> center <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">,</span> scale <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <div class="packt_tip">
      <p class="normal">Although PCA is a deterministic algorithm, the sign—positive or negative—is arbitrary and can vary from run to run, hence the need to set the random seed beforehand to guarantee reproducibility. This Stack Exchange thread has more information on this phenomenon: <a href="https://stats.stackexchange.com/questions/88880/"><span class="url">https://stats.stackexchange.com/questions/88880/</span></a></p>
    </div>
    <p class="normal">Recall <a id="_idIndexMarker1486"/>that each component in the PCA captures a decreasing amount of the dataset’s variance and that we requested 10 of the possible 36 components. A <strong class="keyWord">scree plot</strong>, named <a id="_idIndexMarker1487"/>after the “scree” landslide patterns that form at the bottom of cliffs, helps visualize the amount of variance captured by each component and may thus help to determine the optimal number of components to use. R’s built-in <code class="inlineCode">screeplot()</code> function can be applied to our result to create such a plot. The four parameters supply our PCA result, indicate that we want to plot all 10 components, use a line graph rather than a bar plot, and apply the plot title:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> screeplot<span class="hljs-punctuation">(</span>sns_pca<span class="hljs-punctuation">,</span> npcs <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-punctuation">,</span> type <span class="hljs-operator">=</span> <span class="hljs-string">"</span><span class="hljs-string">lines"</span><span class="hljs-punctuation">,</span>
            main <span class="hljs-operator">=</span> <span class="hljs-string">"Scree Plot of SNS Data Principal Components"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The resulting plot appears as follows:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_13_09.png"/></figure>
    <p class="packt_figref">Figure 13.9: The scree plot depicting the variance of the first 10 principal components of the social media dataset</p>
    <p class="normal">The scree plot shows that there is a substantial drop in the variance captured between the first and second components. The second through fifth components capture approximately the same amount of variance, and then there are additional substantial drops between the fifth and sixth components and between the sixth and seventh components. The seventh through tenth components capture approximately the same amount of <a id="_idIndexMarker1488"/>variance. Based on this result, we might decide to use one, five, or six principal components as our reduced-dimensionality dataset. We can see this numerically by applying the <code class="inlineCode">summary()</code> function to our PCA results object:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>sns_pca<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Importance of components:
                           PC1     PC2     PC3     PC4     PC5
Standard deviation     1.82375 1.30885 1.27008 1.22642 1.20854
Proportion of Variance 0.09239 0.04759 0.04481 0.04178 0.04057
Cumulative Proportion  0.09239 0.13998 0.18478 0.22657 0.26714
                           PC6     PC7     PC8     PC9    PC10
Standard deviation     1.11506 1.04948 1.03828 1.02163 1.01638
Proportion of Variance 0.03454 0.03059 0.02995 0.02899 0.02869
Cumulative Proportion  0.30167 0.33227 0.36221 0.39121 0.41990
</code></pre>
    <p class="normal">The output shows the standard deviation, the proportion of the total variance, and the cumulative proportion of variance for each of the 10 components (labeled <code class="inlineCode">PC1</code> to <code class="inlineCode">PC10</code>). Because standard deviation is the square root of variance, squaring the standard deviations produces the variance values depicted in the scree plot; for example, <em class="italic">1.8237</em><em class="italic">5</em><sup class="superscript-italic" style="font-style: italic;">2</sup><em class="italic"> = 3.326064</em>, which is the value shown for the first component in the scree plot. A component’s proportion of variance is its variance out of the total for all components— not only the 10 shown here, but also the remaining 26 that we could have created. Therefore, the cumulative proportion of variance maxes out at 41.99% rather than the 100% that would be explained by all 36 components.</p>
    <p class="normal">Using PCA as a dimensionality reduction technique requires the user to determine how many components to keep. In this case, if we choose five components, we will capture 26.7% of the variance, or one-fourth of the total information in the original data. Whether or not this is sufficient depends on how much of the remaining 73.3% of variance is signal or noise—something that we can only determine by attempting to build a useful learning algorithm. One thing that makes this process easier is that regardless of how many components we ultimately decide upon, our PCA process is complete; we can simply use as few or as many of the 10 components as desired. For instance, there is no need to re-run the algorithm to obtain the best three versus the best seven components; finding the first seven components will naturally already include the best three and the results will be identical. In a real-world application of PCA, it may be wise to test several different cut points.</p>
    <p class="normal">For simplicity <a id="_idIndexMarker1489"/>here, we’ll reduce the original 36-dimension dataset to five principal components. By default, the <code class="inlineCode">irlba_prcomp()</code> function automatically saves a version of the original dataset that has been transformed into the lower-dimension space. This is found in the resulting <code class="inlineCode">sns_pca</code> list object with the name <code class="inlineCode">x</code>, which we can examine with the <code class="inlineCode">str()</code> command:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> str<span class="hljs-punctuation">(</span>sns_pca<span class="hljs-operator">$</span>x<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"> num [1:30000, 1:10] 1.448 -3.492 0.646 1.041 -4.322 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:10] "PC1" "PC2" "PC3" "PC4" ...
</code></pre>
    <p class="normal">The transformed dataset is a numeric matrix with 30,000 rows like the original dataset but 10 rather than 36 columns with names from <code class="inlineCode">PC1</code> to <code class="inlineCode">PC10</code>. We can see this more clearly by using the <code class="inlineCode">head()</code> command output to see the first few rows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> head<span class="hljs-punctuation">(</span>sns_pca<span class="hljs-operator">$</span>x<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">            PC1         PC2       PC3        PC4         PC5
[1,] -1.4477620  0.07976310 0.3357330 -0.3636082  0.03833596
[2,]  3.4922144  0.36554520 0.7966735 -0.1871626  0.57126163
[3,] -0.6459385 -0.67798166 0.8000251  0.6243070  0.25122261
[4,] -1.0405145  0.08118501 0.4099638 -0.2555128 -0.02620989
[5,]  4.3216304 -1.01754361 3.4112730 -1.9209916 -0.43409869
[6,]  0.2131225 -0.65882053 1.6215828  0.9372545  1.47217369
             PC6          PC7          PC8         PC9        PC10
[1,] -0.01559079  0.007278589 -0.004582346  0.19226144  0.08086065
[2,]  3.02758235 -0.306304037 -1.142422251  0.72992534  0.11203923
[3,] -0.40751994  0.454614417  0.704544996 -0.43734980 -0.07735574
[4,]  0.27837411  0.462898314 -0.175251793 -0.08843005  0.26784326
[5,] -1.11734548 -2.122420077 -2.287638056  2.19992650 -0.26536161
[6,]  0.04614790 -0.654207687  0.285263646  0.69439745 -0.89649127
</code></pre>
    <p class="normal">Recall that in the original dataset, each of the 36 columns indicated the number of times a particular word appeared in the social media profile text. If we standardized the data to have a mean of zero, as we did in <em class="chapterRef">Chapter 9</em>, <em class="italic">Finding Groups of Data – Clustering with k-means</em>, and has been done here for the principal components, then positive and negative values indicate profiles with higher or lower-than-average values, respectively. The trick is that each of the 36 original columns had an obvious interpretation, whereas the PCA results are without apparent meaning.</p>
    <p class="normal">We can<a id="_idIndexMarker1490"/> attempt to understand the components by visualizing the PCA <strong class="keyWord">loadings</strong>, or<a id="_idIndexMarker1491"/> the weights that transform the original data into each of the principal components. Large loadings are more important to a particular component. These loadings are found in the <code class="inlineCode">sns_pca</code> list object with the name <code class="inlineCode">rotation</code>. </p>
    <p class="normal">This is a numeric matrix with 36 rows corresponding to each of the original columns in the dataset and 10 columns that provide the loadings for the principal components. To construct our visualization, we will need to pivot this data such that it has one row per social media term per principal component; that is, we will have <em class="italic">36 * 10 = 360</em> rows in the longer version of the dataset.</p>
    <p class="normal">The following command uses two steps to create the required long dataset. The first step creates a tibble including a <code class="inlineCode">SNS_Term</code> column with one row for each of the 36 terms as well as the <code class="inlineCode">sns_pca$rotation</code> matrix, which is converted into a tibble using <code class="inlineCode">as_tibble()</code>. The combined tibble, with 11 columns and 36 rows, is piped into the <code class="inlineCode">pivot_longer()</code> function, which pivots the table from wide to long format. The three parameters tell the function to pivot the 10 columns from <code class="inlineCode">PC1</code> to <code class="inlineCode">PC10</code>, with the former column names now becoming the rows for a column named <code class="inlineCode">PC</code> and the former column values now becoming row values of a column named <code class="inlineCode">Contribution</code>. The full command creates a tibble with 3 columns and 360 rows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sns_pca_long <span class="hljs-operator">&lt;-</span> tibble<span class="hljs-punctuation">(</span>SNS_Term <span class="hljs-operator">=</span> colnames<span class="hljs-punctuation">(</span>sns_terms<span class="hljs-punctuation">),</span>
                           as_tibble<span class="hljs-punctuation">(</span>sns_pca<span class="hljs-operator">$</span>rotation<span class="hljs-punctuation">))</span> <span class="hljs-operator">|&gt;</span> 
  pivot_longer<span class="hljs-punctuation">(</span>PC1<span class="hljs-operator">:</span>PC10<span class="hljs-punctuation">,</span> names_to <span class="hljs-operator">=</span> <span class="hljs-string">"PC"</span><span class="hljs-punctuation">,</span> values_to <span class="hljs-operator">=</span> <span class="hljs-string">"Contribution"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">ggplot()</code> function can now be used to plot the most important contributing terms for a given principal component. For example, to look at the third principal component, we’ll <code class="inlineCode">filter()</code> the rows to limit to only <code class="inlineCode">PC3</code>, select the top 15 largest contribution values—considering both positive and negative values using the <code class="inlineCode">abs()</code> absolute value function—and mutate the <code class="inlineCode">SNS_Term</code> to reorder by the contribution amount. Ultimately, this is piped into <code class="inlineCode">ggplot()</code> with a number of adjustments to the formatting:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sns_pca_long <span class="hljs-operator">|&gt;</span>
    filter<span class="hljs-punctuation">(</span>PC <span class="hljs-operator">==</span> <span class="hljs-string">"PC3"</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">|&gt;</span>
    top_n<span class="hljs-punctuation">(</span><span class="hljs-number">15</span><span class="hljs-punctuation">,</span> <span class="hljs-built_in">abs</span><span class="hljs-punctuation">(</span>Contribution<span class="hljs-punctuation">))</span> <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>SNS_Term <span class="hljs-operator">=</span> reorder<span class="hljs-punctuation">(</span>SNS_Term<span class="hljs-punctuation">,</span> Contribution<span class="hljs-punctuation">))</span> <span class="hljs-operator">|&gt;</span>
    ggplot<span class="hljs-punctuation">(</span>aes<span class="hljs-punctuation">(</span>SNS_Term<span class="hljs-punctuation">,</span> Contribution<span class="hljs-punctuation">,</span> fill <span class="hljs-operator">=</span> SNS_Term<span class="hljs-punctuation">))</span> <span class="hljs-operator">+</span>
      geom_col<span class="hljs-punctuation">(</span>show.legend <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">,</span> alpha <span class="hljs-operator">=</span> <span class="hljs-number">0.8</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">+</span>
      theme<span class="hljs-punctuation">(</span>axis.text.x <span class="hljs-operator">=</span> element_text<span class="hljs-punctuation">(</span>angle <span class="hljs-operator">=</span> <span class="hljs-number">90</span><span class="hljs-punctuation">,</span> hjust <span class="hljs-operator">=</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span>
           vjust <span class="hljs-operator">=</span> <span class="hljs-number">0.5</span><span class="hljs-punctuation">),</span>  axis.ticks.x <span class="hljs-operator">=</span> element_blank<span class="hljs-punctuation">())</span> <span class="hljs-operator">+</span> 
      labs<span class="hljs-punctuation">(</span>x <span class="hljs-operator">=</span> <span class="hljs-string">"Social Media Term"</span><span class="hljs-punctuation">,</span>
           y <span class="hljs-operator">=</span> <span class="hljs-string">"</span><span class="hljs-string">Relative Importance to Principal Component"</span><span class="hljs-punctuation">,</span>
           title <span class="hljs-operator">=</span> <span class="hljs-string">"Top 15 Contributors to PC3"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The result is <a id="_idIndexMarker1492"/>shown in the plot that follows. Because the terms with positive and negative impacts seem to be split across subjects related to sex, drugs, and rock and roll, one might argue that this principle component has identified a stereotypical dimension of teen identity:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_13_10.png"/></figure>
    <p class="packt_figref">Figure 13.10: The top 15 terms contributing to PC3</p>
    <p class="normal">By repeating the above <code class="inlineCode">ggplot</code> code for the four other principal components among the first five, we observe similar distinctions, as shown in the figure that follows:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_13_11.png"/></figure>
    <p class="packt_figref">Figure 13.11: The top 15 terms contributing to the other four principal components</p>
    <p class="normal"><code class="inlineCode">PC1</code> is particularly<a id="_idIndexMarker1493"/> interesting, as every term has a positive impact; this may be distinguishing people who have anything versus nothing at all on their social media profiles. <code class="inlineCode">PC2</code> seems to favor shopping-related terms, while <code class="inlineCode">PC4</code> seems to be a combination of music and sports, without sex and drugs. Lastly, it seems that <code class="inlineCode">PC5</code> may be distinguishing between sports and non-sports-related terms. Examining the charts in this way will help to understand each component’s impact on the predictive model.</p>
    <div class="note">
      <p class="normal">The previous visualization method was adapted from an outstanding tutorial from Julia Silge, author of <em class="italic">Text Mining with R: A Tidy Approach</em> (2017). For a deeper dive into PCA, see <a href="https://juliasilge.com/blog/stack-overflow-pca/"><span class="url">https://juliasilge.com/blog/stack-overflow-pca/</span></a>.</p>
    </div>
    <p class="normal">An understanding of principal component analysis is of little value if the technique is not useful for building machine learning models. In the previous example, we reduced the dimensionality of a social media dataset from 36 to 10 or fewer components. By merging these components back into the original dataset, we can use them to make predictions about <a id="_idIndexMarker1494"/>a profile’s gender or number of friends. We’ll begin by using the <code class="inlineCode">cbind()</code> function to combine the first four columns of the original data frame with the transformed profile data from the PCA result:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sns_data_pca <span class="hljs-operator">&lt;-</span> cbind<span class="hljs-punctuation">(</span>sns_data<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">4</span><span class="hljs-punctuation">],</span> sns_pca<span class="hljs-operator">$</span>x<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Next, we’ll build a linear regression model predicting the number of social media friends as a function of the first five principal components. This modeling approach was introduced in <em class="chapterRef">Chapter 6</em>, <em class="italic">Forecasting Numeric Data – Regression Methods</em>. The resulting output is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> m <span class="hljs-operator">&lt;-</span> lm<span class="hljs-punctuation">(</span>friends <span class="hljs-operator">~</span> PC1 <span class="hljs-operator">+</span> PC2 <span class="hljs-operator">+</span> PC3 <span class="hljs-operator">+</span> PC4 <span class="hljs-operator">+</span> PC5<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> sns_data_pca<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> m
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Call:
lm(formula = friends ~ PC1 + PC2 + PC3 + PC4 + PC5, data = sns_data_pca)
Coefficients:
(Intercept)          PC1          PC2          PC3          PC4  
    30.1795       1.9857       0.9748      -2.5230       1.1160  
        PC5  
     0.8780
</code></pre>
    <p class="normal">Because the value of the intercept is approximately 30.18, the average person in this dataset has about 30 friends. People with higher values of <code class="inlineCode">PC1</code>, <code class="inlineCode">PC2</code>, <code class="inlineCode">PC4</code>, and <code class="inlineCode">PC5</code> are expected to have more friends, while higher values of <code class="inlineCode">PC3</code> are associated with fewer friends, assuming all else is equal. For example, for each unit increase in <code class="inlineCode">PC2</code>, we would anticipate about one additional friend on average. Given our understanding of the components, these findings make sense; the positive values of <code class="inlineCode">PC2</code>, <code class="inlineCode">PC3</code>, and <code class="inlineCode">PC5</code> were associated with more social activities. In contrast, <code class="inlineCode">PC3</code> was about sex, drugs, and rock and roll, which may be somewhat antisocial.</p>
    <p class="normal">Although this is a very simple example, PCA can be used in the same way with much larger datasets. In addition to mitigating the curse of dimensionality, it also has the benefit of reducing complexity. For instance, a dataset with a very large number of predictors may be too computationally expensive for k-Nearest Neighbors or an artificial neural network to run as is, but by selecting a smaller number of principal components, such techniques <a id="_idIndexMarker1495"/>may be within reach. </p>
    <p class="normal">Feel free to experiment with PCA and contrast this type of feature extraction with other feature selection methods like filters and wrappers; you may find that you have better luck with one approach or the other. Even if you choose not to use dimensionality reduction, there are other problems with highly dimensional data that you will discover in the next section.</p>
    <h1 class="heading-1" id="_idParaDest-299">Making use of sparse data</h1>
    <p class="normal">As datasets increase<a id="_idIndexMarker1496"/> in dimension, some attributes are likely to be <strong class="keyWord">sparse</strong>, which means most observations do not share values of the attribute. This is a natural consequence of the curse of dimensionality in which this ever-increasing detail turns observations into outliers identified by their unique combination of attributes. It is very uncommon for sparse data to have any specific value, or perhaps even any value at all—as was the case in the sparse matrices for text data found in <em class="chapterRef">Chapter 4</em>, <em class="italic">Probabilistic Learning – Classification Using Naive Bayes</em>, and the sparse matrices for shopping cart data in <em class="chapterRef">Chapter 8</em>, <em class="italic">Finding Patterns – Market Basket Analysis Using Association Rules</em>.</p>
    <p class="normal">This is not the same as missing data, where typically a relatively small portion of values are unknown. In sparse data, most values are known, but the number of interesting, meaningful values is dwarfed by an overwhelming number of values that add little value to the learning task. With missing data, machine learning algorithms struggle to learn something from nothing; with sparse data, machine learning algorithms struggle to find the needle in the haystack.</p>
    <h2 class="heading-2" id="_idParaDest-300">Identifying sparse data</h2>
    <p class="normal">Sparse data <a id="_idIndexMarker1497"/>can appear in several interrelated forms. Perhaps the most encountered form is categorical, in which a single feature has a very large number of levels or categories, with some having extremely small counts relative to the others. Features like this are said to have high <strong class="keyWord">cardinality</strong> and will lead to sparse data problems when fed to a learning algorithm. An example of this is zip codes; in the United States, there are over 40,000 postal codes, with some having more than 100,000 residents and others having less than 100. Consequently, if a sparse zip code feature is included in a modeling project, the learning algorithm is likely to struggle to find the balance between ignoring and overemphasizing areas with few residents.</p>
    <p class="normal">Categorical features <a id="_idIndexMarker1498"/>with many levels are often expressed as a series of binary features with one feature per level. We have used such features many times when we manually constructed binary dummy variables, and many learning algorithms do the same automatically for categorical data. This can lead to a situation in which binary features are sparse as the 1 values are overwhelmed by the 0 values. </p>
    <p class="normal">For example, in a zip code dataset for the U.S. population, a tiny fraction of the 330 million residents will fall into each of the 40,000 postal codes, thus making each binary zip code feature highly sparse and difficult for a learning algorithm to use.</p>
    <p class="normal">Many forms of so-called “big” data are inherently highly dimensional and sparse. Sparseness is closely related to the curse of dimensionality. Just like how the ever-expanding universe creates greater voids of empty space between objects, one might argue that every dataset becomes sparse as more dimensions are added. Text data is usually sparse because each word can be treated as a dimension and there are countless words that can appear, each having a low probability of appearing in a specific document. Other big data forms, like DNA data, transactional market basket data, and image data, also often exhibit the problem of sparseness. Unless the dataset’s density is increased, many learning algorithms will struggle to make use of the rich, big dataset.</p>
    <p class="normal">Even a simple numeric range of data can be sparse. This occurs when the distribution of numeric values is wide, which leads to some ranges of the distribution having a very low density. Income is one example of this, because the values generally become increasingly sparse for higher incomes. This is closely related to the problem of outliers, but here we clearly hope to model the outlying values. Sparse numeric data can also be found in cases when the numbers have been stored in an overly specific degree of precision. For example, if age values are stored with decimals rather than integers, such as 24.9167 and 36.4167 rather than simply 24 and 36, this creates an implied void between numbers that some learning algorithms may struggle to ignore. For instance, a decision tree might distinguish between people that are 24.92 and 24.90 years old—probably more likely to be related to overfitting than a meaningful distinction in the real world.</p>
    <p class="normal">Reducing the sparseness of a dataset manually can assist a learning algorithm with identifying the important signals and ignoring the noise. The approach used depends on the type and degree of sparse data as well as the modeling algorithm used. Some algorithms are better than others at handling certain types of sparse data. For example, naive Bayes performs relatively well with sparse categorical data, regression methods do relatively well with sparse numeric data, and decision trees tend to struggle with sparse data in <a id="_idIndexMarker1499"/>general due to their preference for features with a larger number of categories. More sophisticated methods like deep neural networks and boosting can help, but in general, it is better if the dataset can be made denser prior to the learning process.</p>
    <h2 class="heading-2" id="_idParaDest-301">Example – Remapping sparse categorical data</h2>
    <p class="normal">As we <a id="_idIndexMarker1500"/>have seen in prior chapters, when adding a categorical feature to a dataset, it is usually transformed into a set of binary variables equal to the number of levels of the original feature using dummy or one-hot encoding. </p>
    <p class="normal">For example, if there are 40,000 zip codes in the United States, the machine learning algorithm would have 40,000 binary predictors for this feature. This is called a <strong class="keyWord">one-of-n mapping</strong> because<a id="_idIndexMarker1501"/> only one of the 40,000 features would have a value of 1 while the remainder would have values of 0—a case of extreme growth in dimensionality and sparseness.</p>
    <p class="normal">To increase the density of a <a id="_idIndexMarker1502"/>one-of-n mapping, an <strong class="keyWord">m-of-n mapping</strong> may be used instead, which reduces the <em class="italic">n</em> binary variables to a smaller set of <em class="italic">m</em> variables. For example, with zip codes, instead of creating a 40,000-level feature with one level per zip code, one might choose to map into 100 levels by using the first two digits of the zip code from 00 to 99. Similarly, if it would create too much sparseness to include a binary feature for each of the 200 countries in the world, it might be possible to map countries to a smaller set of continents, like Europe, North America, and Asia, instead.</p>
    <p class="normal">When creating an m-of-n mapping, it is best if the groupings represent a shared underlying characteristic, but it is possible to use other approaches as well. Domain knowledge can be helpful for creating a remapping that reflects the shared characteristics of the more granular units. In the absence of domain expertise, the following methods may be appropriate:</p>
    <ul>
      <li class="bulletList">Leave the larger categories as is and group only the categories with small numbers of observations. For example, zip codes for dense urban areas could be included directly, but sparse rural zip codes could be grouped into larger geographic areas.</li>
      <li class="bulletList">Examine the impact of the categories on the target variable by creating a two-way cross table or computing the average outcome by level and group levels that have a similar impact on the response variable. For example, if certain zip codes are more likely to default on a loan, create a new category composed of these zip codes.</li>
      <li class="bulletList">As a more sophisticated variant of the previous method, it may also be possible to build a simple machine learning model that predicts the target using the highly dimensional feature and then group levels of the feature that have a similar relationship with the target or with other predictors. Simple methods like regression and decision trees would be ideal for this approach.</li>
    </ul>
    <p class="normal">Once a <a id="_idIndexMarker1503"/>remapping strategy has been chosen, helpful functions for recoding categorical variables can be found in the <code class="inlineCode">forcats</code> package (<a href="https://forcats.tidyverse.org"><span class="url">https://forcats.tidyverse.org</span></a>), which is part of the base set of packages comprising the tidyverse. The package includes options for automatically recoding categorical variables with sparse levels, or manually recoding if a more guided approach is desired. Detailed information about the package is available in the <em class="italic">R for Data Science</em> chapter at <a href="https://r4ds.hadley.nz/factors.html"><span class="url">https://r4ds.hadley.nz/factors.html</span></a>.</p>
    <p class="normal">We’ll examine a couple of approaches for remapping using the Titanic dataset and the passenger titles created in <em class="chapterRef">Chapter 12</em>, <em class="italic">Advanced Data Preparation</em>. Because the <code class="inlineCode">forcats</code> package is included in the base tidyverse, it can be loaded with the entire suite or on its own using the <code class="inlineCode">library(forcats)</code> command. We’ll begin by loading the tidyverse, reading the Titanic dataset, and then examining the levels of the title feature:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>tidyverse<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> titanic_train <span class="hljs-operator">&lt;-</span> read_csv<span class="hljs-punctuation">(</span><span class="hljs-string">"titanic_train.csv"</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>Title <span class="hljs-operator">=</span> str_extract<span class="hljs-punctuation">(</span>Name<span class="hljs-punctuation">,</span> <span class="hljs-string">", [A-z]+\\."</span><span class="hljs-punctuation">))</span> <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>Title <span class="hljs-operator">=</span> str_replace_all<span class="hljs-punctuation">(</span>Title<span class="hljs-punctuation">,</span> <span class="hljs-string">"[, \\.]"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">""</span><span class="hljs-punctuation">))</span>
<span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>Title<span class="hljs-punctuation">,</span> useNA <span class="hljs-operator">=</span> <span class="hljs-string">"ifany"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    Capt      Col      Don       Dr Jonkheer     Lady    Major 
       1        2        1        7        1        1        2
 
  Master     Miss     Mlle      Mme       Mr      Mrs       Ms 
      40      182        2        1      517      125        1 
     Rev      Sir     &lt;NA&gt;
       6        1        1
</code></pre>
    <p class="normal">In the previous chapter, we used base R’s <code class="inlineCode">recode()</code> function to combine the variants of <em class="italic">Miss</em>, such as <em class="italic">Ms</em>, <em class="italic">Mlle</em>, and <em class="italic">Mme</em>, into a single group. The <code class="inlineCode">forcats</code> package includes an <code class="inlineCode">fct_collapse()</code> function, which is more convenient to use for categorical features with a large number of levels. We’ll use it here to create an m-of-n mapping that creates groups based on knowledge of the titles’ real-world meanings. Note that several of the new categories are one-to-one mappings of the previous categories, but by including a vector of <a id="_idIndexMarker1504"/>labels, we can map several of the old levels to a single new level, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> titanic_train <span class="hljs-operator">&lt;-</span> titanic_train <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>TitleGroup <span class="hljs-operator">=</span> fct_collapse<span class="hljs-punctuation">(</span>Title<span class="hljs-punctuation">,</span> 
      Mr <span class="hljs-operator">=</span> <span class="hljs-string">"Mr"</span><span class="hljs-punctuation">,</span>
      Mrs <span class="hljs-operator">=</span> <span class="hljs-string">"Mrs"</span><span class="hljs-punctuation">,</span>
      Master <span class="hljs-operator">=</span> <span class="hljs-string">"Master"</span><span class="hljs-punctuation">,</span>
      Miss <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">"Miss"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"Mlle"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"Mme"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"Ms"</span><span class="hljs-punctuation">),</span>
      Noble <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">"Don"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"Sir"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"</span><span class="hljs-string">Jonkheer"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"Lady"</span><span class="hljs-punctuation">),</span>
      Military <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">"Capt"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"Col"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"Major"</span><span class="hljs-punctuation">),</span>
      Doctor <span class="hljs-operator">=</span> <span class="hljs-string">"Dr"</span><span class="hljs-punctuation">,</span>
      Clergy <span class="hljs-operator">=</span> <span class="hljs-string">"Rev"</span><span class="hljs-punctuation">,</span>
      other_level <span class="hljs-operator">=</span> <span class="hljs-string">"Other"</span><span class="hljs-punctuation">)</span>
    <span class="hljs-punctuation">)</span> <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>TitleGroup <span class="hljs-operator">=</span> fct_na_value_to_level<span class="hljs-punctuation">(</span>TitleGroup<span class="hljs-punctuation">,</span>
                                              level <span class="hljs-operator">=</span> <span class="hljs-string">"Unknown"</span><span class="hljs-punctuation">))</span> 
</code></pre>
    <p class="normal">Examining the new categorization, we see that the 17 original categories have been reduced to 9:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>TitleGroup<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Military    Noble   Doctor   Master     Miss       Mr      Mrs 
       5        4        7       40      186      517      125 
  Clergy  Unknown 
       6        1 
</code></pre>
    <p class="normal">If we had a much larger set of levels, or in the absence of knowledge of how categories should be grouped, we can leave large categories as is and group the levels with few examples. The <code class="inlineCode">forcats</code> package includes a simple function for examining the levels of our feature. Although this can also be done with base R functions, the <code class="inlineCode">fct_count()</code> function provides a sorted list of the feature levels and their proportions of the overall total:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> fct_count<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>Title<span class="hljs-punctuation">,</span> sort <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">,</span> prop <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"># A tibble: 17 × 3
   f            n       p
   &lt;fct&gt;    &lt;int&gt;   &lt;dbl&gt;
 1 Mr         517 0.580  
 2 Miss       182 0.204  
 3 Mrs        125 0.140  
 4 Master      40 0.0449 
 5 Dr           7 0.00786
 6 Rev          6 0.00673
 7 Col          2 0.00224
 8 Major        2 0.00224
 9 Mlle         2 0.00224
10 Capt         1 0.00112
11 Don          1 0.00112
12 Jonkheer     1 0.00112
13 Lady         1 0.00112
14 Mme          1 0.00112
15 Ms           1 0.00112
16 Sir          1 0.00112
17 NA           1 0.00112
</code></pre>
    <p class="normal">This <a id="_idIndexMarker1505"/>output can inform groupings based on a minimum number or minimum proportion of observations. The <code class="inlineCode">forcats</code> package has a set of <code class="inlineCode">fct_lump()</code> functions to help with this process of “lumping” factor levels into an “other” group. For example, we might take the top three levels and treat everything else as other:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>fct_lump_n<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>Title<span class="hljs-punctuation">,</span> n <span class="hljs-operator">=</span> <span class="hljs-number">3</span><span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"> Miss    Mr   Mrs Other 
  182   517   125    66
</code></pre>
    <p class="normal">Alternatively, we can lump together all levels with less than one percent of the observations:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>fct_lump_prop<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>Title<span class="hljs-punctuation">,</span> prop <span class="hljs-operator">=</span> <span class="hljs-number">0.01</span><span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Master   Miss     Mr    Mrs  Other 
    40    182    517    125     26
</code></pre>
    <p class="normal">Lastly, we might choose to lump together all levels with less than five observations:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>fct_lump_min<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>Title<span class="hljs-punctuation">,</span> <span class="hljs-built_in">min</span> <span class="hljs-operator">=</span> <span class="hljs-number">5</span><span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    Dr Master   Miss     Mr    Mrs    Rev  Other 
     7     40    182    517    125      6     13
</code></pre>
    <p class="normal">The choice of <a id="_idIndexMarker1506"/>which of these three functions to use, as well as the appropriate parameter value, will depend on the dataset used and the desired number of levels for the m-of-n mapping.</p>
    <h2 class="heading-2" id="_idParaDest-302">Example – Binning sparse numeric data</h2>
    <p class="normal">While <a id="_idIndexMarker1507"/>many machine learning methods handle numeric data without trouble, some approaches like decision trees are more likely to struggle with numeric data, especially when it exhibits some of the characteristics of sparseness. A common solution to this problem is <a id="_idIndexMarker1508"/>called <strong class="keyWord">discretization</strong>, which converts a range of numbers into a smaller number of discrete categories called bins. We encountered this method previously in <em class="chapterRef">Chapter 4</em>, <em class="italic">Probabilistic Learning – Classification Using Naive Bayes</em>, as we discretized the numeric data to work with the naive Bayes algorithm. Here, we will apply a similar approach, using modern tidyverse methods, to reduce the dimensionality of the number range to help address the tendency of some methods to over- or underfit to sparse numeric data.</p>
    <p class="normal">As is the case with many machine learning approaches, ideally one would apply subject-matter expertise to determine the cut points for discretizing a numeric range. For example, on a range of age values, perhaps meaningful break points could occur between well-established childhood, adulthood, and elderly age groups, to reflect the impact of these age values in the real world. Similarly, bins may be created for salary levels such as lower, middle, and upper class.</p>
    <p class="normal">In the absence of real-world knowledge of important categories, it is often advisable to use cut points that reflect natural percentiles of data or intuitive increments of values. This may mean dividing a range of numbers using strategies such as:</p>
    <ul>
      <li class="bulletList">Creating groups based on tertiles, quartiles, quintiles, deciles, or percentiles that contain equal proportions of examples (33%, 25%, 20%, 10%, or 1%).</li>
      <li class="bulletList">Using familiar cut points for the underlying range of values, such as grouping time values by hours, half hours, or quarter hours; grouping 0-100 scale values by fives, tens, or twenty-fives; or bucketing large numeric ranges like income by large multiples of 10 or 25.</li>
      <li class="bulletList">Applying the notion of log scaling to skewed data, so that the bins are proportionally wider for the skewed portion of the data where the values are sparser; for example, income might be bucketed into groups of 0-10,000 followed by 10,000-100,000, then 100,000-1,000,000, and 1,000,000 or more.</li>
    </ul>
    <p class="normal">To <a id="_idIndexMarker1509"/>illustrate these approaches, we’ll apply discretization techniques to the fare values in the Titanic dataset used previously. The <code class="inlineCode">head()</code> and <code class="inlineCode">summary()</code> functions illustrate that the values are highly granular and highly sparse on the high end due to their severe right skew:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> head<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>Fare<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1]  7.2500 71.2833  7.9250 53.1000  8.0500  8.4583
&gt; summary(titanic_train$Fare)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   0.00    7.91   14.45   32.20   31.00  512.33
</code></pre>
    <p class="normal">Suppose we are most interested in the difference between first class and other passengers and that we assume that the top 25 percent of fares reflect first-class tickets. We could easily create a binary feature using the tidyverse <code class="inlineCode">if_else()</code> function as follows. If the fare has a value of at least £31, which is the value for the third quartile, then we’ll assume it is a first-class fare and assign a value of <code class="inlineCode">1</code> to the binary-coded <code class="inlineCode">fare_firstclass</code> feature; if not, it receives a <code class="inlineCode">0</code> value. The <code class="inlineCode">missing</code> parameter tells the function to assign the value <code class="inlineCode">0</code> if the fare was missing, under the assumption that first-class fares are very unlikely to be unknown:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> titanic_train <span class="hljs-operator">&lt;-</span> titanic_train <span class="hljs-operator">|&gt;</span> mutate<span class="hljs-punctuation">(</span>
    fare_firstclass <span class="hljs-operator">=</span> if_else<span class="hljs-punctuation">(</span>Fare <span class="hljs-operator">&gt;=</span> <span class="hljs-number">31</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-built_in">missing</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">)</span>
  <span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This reduces a feature with nearly 250 distinct values into a new feature with only two:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>fare_firstclass<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  0   1 
666 225
</code></pre>
    <p class="normal">Although this was a very simple example, it’s a first step toward more complex binning strategies. The <code class="inlineCode">if_else()</code> function, although simple here, would be unwieldy to use for creating a new feature with more than two levels. This would require nesting <code class="inlineCode">if_else()</code> functions within each other, which quickly becomes difficult to maintain. Instead, a tidyverse function called <code class="inlineCode">case_when()</code> allows the construction of a more complex series of checks to determine the result.</p>
    <p class="normal">In the code that follows, the fare data is binned into three levels corresponding roughly to first-, second-, and third-class fare levels. The <code class="inlineCode">case_when()</code> statement is evaluated as a series of ordered if-else statements. The first statement checks whether the fare is at least 31 and <a id="_idIndexMarker1510"/>assigns these examples the first-class category. The second can be read as an else-if statement; that is, if the first statement is not true—the “else”—we check “if” the fare is at least 15 and assign the second-class level if true. The final statement is the ultimate “else” as <code class="inlineCode">TRUE</code> always evaluates to true, and thus all records not categorized by the first and second lines are assigned the third-class level:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> titanic_train <span class="hljs-operator">&lt;-</span> titanic_train <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>
      fare_class <span class="hljs-operator">=</span> case_when<span class="hljs-punctuation">(</span>
        Fare <span class="hljs-operator">&gt;=</span> <span class="hljs-number">31</span> <span class="hljs-operator">~</span> <span class="hljs-string">"1st Class"</span><span class="hljs-punctuation">,</span>
        Fare <span class="hljs-operator">&gt;=</span> <span class="hljs-number">15</span> <span class="hljs-operator">~</span> <span class="hljs-string">"</span><span class="hljs-string">2nd Class"</span><span class="hljs-punctuation">,</span>
        <span class="hljs-literal">TRUE</span> <span class="hljs-operator">~</span> <span class="hljs-string">"3rd Class"</span>
      <span class="hljs-punctuation">)</span>
    <span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The resulting feature has three levels as expected:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>fare_class<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">1st Class 2nd Class 3rd Class 
      225       209       457
</code></pre>
    <p class="normal">In the case that we have zero understanding of the real-world meaning of the fares, such as the knowledge of first, second, and third-class fares, we might instead apply the discretization heuristics described previously, which use natural percentiles or intuitive cut points of values instead of meaningful groups.</p>
    <p class="normal">The <code class="inlineCode">cut()</code> function is included in base R and provides a simple method for creating a factor from a numeric vector. The <code class="inlineCode">breaks</code> parameter specifies the cut points for the numeric range, shown as follows for a three-level factor that matches the previous discretization. The <code class="inlineCode">right = FALSE</code> parameter indicates that the levels should not include the rightmost, or highest, value and the <code class="inlineCode">Inf</code> break point indicates that the final category can span the range of values from 31 to infinity. The resulting categories are identical to the prior result, but use different labels:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>cut<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>Fare<span class="hljs-punctuation">,</span> breaks <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">15</span><span class="hljs-punctuation">,</span> <span class="hljs-number">31</span><span class="hljs-punctuation">,</span> <span class="hljs-literal">Inf</span><span class="hljs-punctuation">),</span>
            right <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  [0,15)  [15,31) [31,Inf) 
     457      209      225
</code></pre>
    <p class="normal">By default, <code class="inlineCode">cut()</code> sets labels for factors that indicate the range of values falling into each level. Square brackets indicate that the bracketed number is included in the level, while parentheses <a id="_idIndexMarker1511"/>indicate a number that is not included. A <code class="inlineCode">labels</code> parameter can be assigned a vector of factor labels for the result, if desired.</p>
    <p class="normal">The <code class="inlineCode">cut()</code> function becomes more interesting when combined with a sequence of values generated by the <code class="inlineCode">seq()</code> function. Here, we create levels for the 11 ranges of values from 0 to 550 in increments of 50:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>cut<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>Fare<span class="hljs-punctuation">,</span> right <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">,</span>
            breaks <span class="hljs-operator">=</span> seq<span class="hljs-punctuation">(</span>from <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> to <span class="hljs-operator">=</span> <span class="hljs-number">550</span><span class="hljs-punctuation">,</span> by <span class="hljs-operator">=</span> <span class="hljs-number">50</span><span class="hljs-punctuation">)))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   [0,50)  [50,100) [100,150) [150,200) [200,250) [250,300) 
      730       108        24         9        11         6 
[300,350) [350,400) [400,450) [450,500) [500,550) 
        0         0         0         0         3 
</code></pre>
    <p class="normal">Using evenly wide intervals here reduces the dimensionality but doesn’t solve the problem of sparseness. The first two levels contain most of the examples, but the remainder have very few, or even zero in some cases.</p>
    <p class="normal">As an alternative to having equally sized intervals, we can construct bins with an equal number of examples. We have used the <code class="inlineCode">quantile()</code> function in previous chapters to identify the cut points for quintiles and percentiles, but we would still need to use these values with a <code class="inlineCode">cut()</code> function to create the factor levels. The following code creates five bins for the quintiles, but could be adapted for quartiles, deciles, or percentiles:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>cut<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>Fare<span class="hljs-punctuation">,</span> right <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">,</span>
            breaks <span class="hljs-operator">=</span> quantile<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>Fare<span class="hljs-punctuation">,</span>
                              probs <span class="hljs-operator">=</span> seq<span class="hljs-punctuation">(</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0.20</span><span class="hljs-punctuation">))))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   [0,7.85) [7.85,10.5) [10.5,21.7) [21.7,39.7)  [39.7,512) 
        166         173         196         174         179
</code></pre>
    <p class="normal">Note that the bins do not contain exactly the same number of examples due to the presence of ties.</p>
    <p class="normal">The tidyverse<a id="_idIndexMarker1512"/> also includes a function for creating quantile-based groups, which may be easier to use in some cases. This <code class="inlineCode">ntile()</code> function divides the data into <code class="inlineCode">n</code> groups of equal size. For example, it can create five groups as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>ntile<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>Fare<span class="hljs-punctuation">,</span> n <span class="hljs-operator">=</span> <span class="hljs-number">5</span><span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  1   2   3   4   5 
179 178 178 178 178
</code></pre>
    <p class="normal">Because the function assigns the groups numeric labels, it is important to convert the resulting vector to a factor. This can be done directly in a <code class="inlineCode">mutate()</code> statement:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> titanic_train <span class="hljs-operator">&lt;-</span> titanic_train <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>fare_level <span class="hljs-operator">=</span> factor<span class="hljs-punctuation">(</span>ntile<span class="hljs-punctuation">(</span>Fare<span class="hljs-punctuation">,</span> n <span class="hljs-operator">=</span> <span class="hljs-number">11</span><span class="hljs-punctuation">)))</span>
</code></pre>
    <p class="normal">The resulting feature has 11 equally proportioned levels:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>titanic_train<span class="hljs-operator">$</span>fare_level<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">1  2  3  4  5  6  7  8  9 10 11 
81 81 81 81 81 81 81 81 81 81 81
</code></pre>
    <p class="normal">Although the level still has numeric labels, because the feature has been coded as a factor, it will still be treated as categorical by most R functions. Of course, it is still important to find the right balance between too few and too many levels.</p>
    <h1 class="heading-1" id="_idParaDest-303">Handling missing data</h1>
    <p class="normal">The <a id="_idIndexMarker1513"/>teaching datasets used for examples in previous chapters rarely had the problem of missing data, where a value that should be present is instead absent. The R language uses the special value <code class="inlineCode">NA</code> to indicate these missing values, which cannot be handled natively by most machine learning functions. In <em class="chapterRef">Chapter 9</em>, <em class="italic">Finding Groups of Data – Clustering with k-means</em>, we were able to replace missing values with a guess of the true value based on other information available in the dataset in a process called imputation. Specifically, the missing age values of high school students were imputed with the average age of students that had the same graduation year. This provided a reasonable estimate of the unknown age value.</p>
    <p class="normal">Missing data<a id="_idIndexMarker1514"/> is a much greater problem in real-world machine learning projects than would be expected given its rarity so far. This is not only because real-world projects are messier and more complex than simple textbook examples. Additionally, as datasets increase in size—as they include more rows or more columns—a relatively small proportion of missingness will cause more problems, as it becomes more likely that any given row or any given column contains at least one missing value. For example, even if the rate of missingness is only one percent, in a dataset with 100 columns, we would expect the average row to have one missing value. In this case, simply excluding all rows with missing values would drastically reduce the size of the dataset to the point of nothingness!</p>
    <p class="normal">In fields like economics, biostatistics, and the social sciences, the gold standard approach to missing <a id="_idIndexMarker1515"/>data is <strong class="keyWord">multiple imputation</strong>, which uses statistical modeling or machine learning techniques to impute all the missing feature values given the non-missing feature values. Because this tends to decrease the variability of the data, and thus inflates the certainty of predictions, modern multiple imputation software tends to add random variation to the imputed values to avoid biasing the inferences made from the dataset. R has many packages for performing multiple imputation, such as:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">mice</code>: Multivariate Imputation by Chained Equations</li>
      <li class="bulletList"><code class="inlineCode">Amelia</code>: A Program for Missing Data (named after the famous pilot Amelia Earhart, who went missing in 1937 during an attempt to become the first female pilot to fly around the globe)</li>
      <li class="bulletList"><code class="inlineCode">Simputation</code>: Simple Imputation, which attempts to simplify missing data handling via the use of tidyverse-compatible functions</li>
      <li class="bulletList"><code class="inlineCode">missForest</code>: Nonparametric Missing Value Imputation using Random Forest, a package that uses state-of-the-art machine learning methods to impute any type of data, even types with complex, nonlinear relationships among the features</li>
    </ul>
    <p class="normal">Despite the wealth of multiple imputation software tools, in comparison to projects in traditional statistics and the social sciences, machine learning projects apply simpler methods for handling missing data. This is because the goals and considerations differ. Machine learning projects tend to focus on methods that work on very large datasets and facilitate prediction on a future, unseen test set, even if certain statistical assumptions are violated. On the other hand, the more formal methods of the social sciences focus on strategies that tend to be more computationally intensive but lead to unbiased estimates for inference and hypothesis testing. Keep this distinction in mind while reading the <a id="_idIndexMarker1516"/>sections that follow, which cover common practical techniques for handling missing data, but are generally not advisable for formal scientific analysis.</p>
    <h2 class="heading-2" id="_idParaDest-304">Understanding types of missing data</h2>
    <p class="normal">Not all missing <a id="_idIndexMarker1517"/>data is created equally, and some types are more problematic than others. For this reason, when preparing data with missing values, it is useful to consider the underlying reasons why a particular value is missing. Try to picture yourself inside the process that generated the dataset and ask yourself why certain values were left blank. Is there a logical reason it is missing? Or, was it left blank purely by mistake or chance alone? Answering these questions helps inform the solution for replacing the missing values in a responsible manner. The answers to these questions also distinguish three different types of missing data, from least problematic to most severe: </p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Data <strong class="keyWord">missing completely at random</strong> (<strong class="keyWord">MCAR</strong>) is independent of the other features and <a id="_idIndexMarker1518"/>its own value; in other words, it would not be possible to predict whether any particular value is missing. The missingness may be caused due to a random data entry error, or some other process that randomly skips the value. Missing completely at random can be imagined as a completely unpredictable process that takes the final matrix of data and randomly selects cells to delete.</li>
      <li class="numberedList">Data <strong class="keyWord">missing at random</strong> (<strong class="keyWord">MAR</strong>) may depend on other features but not on the<a id="_idIndexMarker1519"/> underlying value, which means that certain, predictable rows are more likely than others to contain missing values. For example, households in certain geographic regions may be less willing to report their household income, but assuming they do disclose such information, they do so honestly. Essentially, MAR implies that the missing values are randomly selected after controlling for the underlying factor or factors causing the missingness.</li>
      <li class="numberedList">Data <strong class="keyWord">missing not at random</strong> (<strong class="keyWord">MNAR</strong>) is missing due to a reason related to the missing <a id="_idIndexMarker1520"/>value itself. This data is in essence censored from the dataset for some reason impossible to discern from the other features in the dataset. For instance, poorer individuals may feel less comfortable sharing their income, so they simply <a id="_idIndexMarker1521"/>leave it blank. Another example might be a temperature sensor that reports a missing value for extremely high or low temperatures. It is probable that most real-world missing values are MNAR, as there is usually some unmeasured, hidden mechanism causing the missingness. Very little is truly random in the real world.</li>
    </ol>
    <p class="normal">Imputation methods work well for the first two types of missing data. Although one might be led to believe that MCAR data is the most challenging to impute due to its independence and unpredictability, it is actually the ideal type of missing data to handle. Even though the missingness is completely random, the values that have been randomly hidden may be predictable given the other available features. Stated differently, the <em class="italic">missingness</em> itself is unpredictable, but the underlying missing <em class="italic">values</em> may be quite predictable. Similarly, MAR data is also readily predictable by the given features.</p>
    <p class="normal">Unfortunately, NMAR data, which is perhaps the most common type of missing data, is the least capable of being predicted. Because the missing values were censored by an unknowable process, any model built on this data will have an incomplete picture of the relationship between the missing and non-missing data, and the results are likely to be biased toward the non-missing data. For example, suppose we are trying to build a model of loan default, and poorer people are more likely to leave the income field blank on the loan application. If we impute the missing incomes, the imputed values will tend to be higher than the true values, as our imputation was based only on the available data, which is missing more low values than high values. If lower-income households are more likely to default, a model that uses the biased imputed income values to predict loan outcomes will underestimate the probability of default for households that left income blank.</p>
    <p class="normal">Due to the possibility of such bias, strictly speaking, we should only impute MCAR and MAR data. Yet, imputation may be the lesser of two imperfect options, since excluding rows with missing data from the training dataset will also bias the model if the data is not missing completely at random. Thus, despite violating statistical assumptions, machine learning practitioners often impute missing values rather than removing missing data from the dataset. The following sections demonstrate a few common strategies employed toward this end.</p>
    <h2 class="heading-2" id="_idParaDest-305">Performing missing value imputation</h2>
    <p class="normal">Because <code class="inlineCode">NA</code> values<a id="_idIndexMarker1522"/> cannot be handled directly by many R functions nor most machine learning algorithms, they must be replaced with something else, and ideally, in a way that improves the model’s performance. In machine learning, this type of missing value imputation is a barrier to prediction, which means that simpler approaches that work reasonably well are favored over more complex approaches—even if the complex approaches may be more methodologically and theoretically sound.</p>
    <p class="normal">Missing character-type data may provide the form of missingness with the simplest possible solution, as it is possible to merely treat the missing values like any other value by recoding the <code class="inlineCode">NA</code> values to a literal character string like <code class="inlineCode">'Missing'</code>, <code class="inlineCode">'Unknown'</code>, or another label of your choosing. The string itself is arbitrary; it just needs to be consistent for each missing value within the column. For example, the Titanic dataset includes two categorical features with missing data: <code class="inlineCode">Cabin</code> and <code class="inlineCode">Embarked</code>. We can easily impute <code class="inlineCode">'X'</code> in place of the missing <code class="inlineCode">Cabin</code> values and <code class="inlineCode">'Unknown'</code> in place of missing <code class="inlineCode">Embarked</code> values as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> titanic_train <span class="hljs-operator">&lt;-</span> titanic_train <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>
      Cabin <span class="hljs-operator">=</span> if_else<span class="hljs-punctuation">(</span><span class="hljs-built_in">is.na</span><span class="hljs-punctuation">(</span>Cabin<span class="hljs-punctuation">),</span> <span class="hljs-string">"X"</span><span class="hljs-punctuation">,</span> Cabin<span class="hljs-punctuation">),</span>
      Embarked <span class="hljs-operator">=</span> if_else<span class="hljs-punctuation">(</span><span class="hljs-built_in">is.na</span><span class="hljs-punctuation">(</span>Embarked<span class="hljs-punctuation">),</span> <span class="hljs-string">"Unknown"</span><span class="hljs-punctuation">,</span> Embarked<span class="hljs-punctuation">)</span>
    <span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Although this method has eliminated the <code class="inlineCode">NA</code> values by replacing them with valid character strings, it seems as if more sophisticated approaches ought to be possible. After all, couldn’t we use machine learning to predict missing values using the remaining columns in the dataset? Indeed, this is possible, as we will learn shortly. However, using this type of advanced approach may be overkill if not actually detrimental to the model’s predictive performance.</p>
    <p class="normal">The reasons that one might perform missing value imputation vary across disciplines. In traditional statistics and the social sciences, models are often used for inference and hypothesis testing rather than for predicting and forecasting. When used for inference, it is very important that the relationships among features within the dataset are preserved as carefully as possible, as statisticians seek to carefully estimate and understand each feature’s individual connection to the outcome of interest. Imputing arbitrary values into the missing slots may distort these relationships—particularly in the case that the values are not missing completely at random. </p>
    <p class="normal">Rather, more sophisticated approaches use the other available information to impute a reasonable guess as to the true value, keeping as many rows of data as possible, while also ensuring that the data’s important internal relationships across features are preserved. Ultimately, this increases <a id="_idIndexMarker1523"/>the <strong class="keyWord">statistical power</strong> of the analysis, which relates to the capabilities of detecting patterns and testing hypotheses.</p>
    <p class="normal">In contrast, machine <a id="_idIndexMarker1524"/>learning practitioners are often less concerned with the internal relationships among a dataset’s features, and more focused on the features’ relationship with an external target outcome. From this perspective, there is no strong reason to apply sophisticated imputation strategies. Such methods do not contribute new information that can be used to better predict the target because they merely reinforce internal patterns. On the other hand, by the assumption that the data is not missing at random, it may be less helpful to focus on the specific value to impute in a missing slot, and instead focus effort on trying to use the missingness itself as a predictor of the target.</p>
    <h3 class="heading-3" id="_idParaDest-306">Simple imputation with missing value indicators</h3>
    <p class="normal">The practice <a id="_idIndexMarker1525"/>described in the previous section, in which missing categorical values were replaced with an arbitrary string like <code class="inlineCode">'Missing'</code> or <code class="inlineCode">'Unknown'</code>, is one form of <strong class="keyWord">simple imputation</strong>, in which <code class="inlineCode">NA</code> values are simply replaced by a constant value. For numeric features, we can use an equivalent approach. For each feature with a missing value, choose a value to impute in place of the <code class="inlineCode">NA</code> values. This can be a summary statistic such as mean, median, or mode, or it may be an arbitrary number—the specific value generally doesn’t matter.</p>
    <div class="packt_tip">
      <p class="normal">Although the exact value generally doesn’t matter, the most common approach may be <strong class="keyWord">mean imputation</strong>, perhaps <a id="_idIndexMarker1526"/>due to the common practice of doing this in the field of traditional statistics. An alternative approach is to use a value on the same order of magnitude but outside the range of actual values found in the data. For example, for missing age values in the range of 0 to 100, you may choose to impute the value -1 or 999. Keep in mind that regardless of the value chosen, any summary statistics computed on the feature will be distorted by the imputed values.</p>
    </div>
    <p class="normal">In addition to imputing a value in place of the <code class="inlineCode">NA</code>, it is especially important to create a <strong class="keyWord">missing value indicator</strong> (<strong class="keyWord">MVI</strong>), which<a id="_idIndexMarker1527"/> is a binary variable that indicates whether the feature value was imputed. We’ll do this for the missing Titanic passenger age values using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> titanic_train <span class="hljs-operator">&lt;-</span> titanic_train <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>
      Age_MVI <span class="hljs-operator">=</span> if_else<span class="hljs-punctuation">(</span><span class="hljs-built_in">is.na</span><span class="hljs-punctuation">(</span>Age<span class="hljs-punctuation">),</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">),</span>
      Age <span class="hljs-operator">=</span> if_else<span class="hljs-punctuation">(</span><span class="hljs-built_in">is.na</span><span class="hljs-punctuation">(</span>Age<span class="hljs-punctuation">),</span> mean<span class="hljs-punctuation">(</span>Age<span class="hljs-punctuation">,</span> na.rm <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">),</span> Age<span class="hljs-punctuation">)</span>
    <span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Both the feature <a id="_idIndexMarker1528"/>that has been imputed and the MVI should be included as predictors in the machine learning model. The fact that a value was missing is often an important predictor of the target, and surprisingly often, one of the strongest predictors of the target. This may not actually be unexpected under the simple belief that very little in the real world happens at random; if a value is missing, there is probably an explanation for it. For example, in the Titanic dataset, perhaps the missing age implies something about the passenger’s social status or family background. Similarly, someone who refuses to report their income or occupation on a loan application may be hiding the fact that they make a very small amount of money—which may be a strong predictor of a loan default. This finding that missing values are interesting predictors is also true for larger amounts of missing data; more missingness may lead to even more interesting predictors.</p>
    <h3 class="heading-3" id="_idParaDest-307">Missing value patterns</h3>
    <p class="normal">Expanding upon<a id="_idIndexMarker1529"/> the belief that a missing value may be a highly useful predictor, each additional missing value may contribute to our ability to forecast a specific outcome. In the case of loan application data, a person that has missing data on a single feature may have been intentionally hiding their answer, or they may have just accidentally skipped this question on the loan application form. If a person has missing data on multiple features, the latter excuse no longer applies, or perhaps it implies that they rushed through the application or are generally more irresponsible. We might assume that people with more missing data are more likely to default, but of course, we won’t know until we train the model; it could just as easily be the case that people with more missing data are in fact less likely to default, perhaps because some of the loan application questions don’t apply to their situation.</p>
    <p class="normal">Suppose there actually is a pattern to be found among records with large amounts of missingness, but it is not solely based on the number of missing values, but instead, the specific features that are missing. For example, someone who is afraid to report their income because it is too low may skip related questions on a loan application, whereas a small business owner may skip a different section of questions on employment history because they do not apply to someone who is self-employed. Both cases may have roughly equal numbers of missing values, but their patterns may differ substantially.</p>
    <p class="normal">A <strong class="keyWord">missing value pattern</strong> (<strong class="keyWord">MVP</strong>) can be constructed to capture such behaviors and use them as features <a id="_idIndexMarker1530"/>for machine learning. A missing value pattern is essentially a character string composed of a series of MVIs, with<a id="_idIndexMarker1531"/> each character in the string representing a feature with missing values. <em class="italic">Figure 13.12</em> illustrates how the process works for a simplified loan application dataset. For each of the eight features, we construct a missing value indicator that indicates whether the corresponding cell was missing:</p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated" src="../Images/B17290_13_12.png"/></figure>
    <p class="packt_figref">Figure 13.12: Constructing a missing value pattern begins with creating missing value indicators for each feature with missing data</p>
    <p class="normal">These binary MVIs are then concatenated into a single string. For example, the first row would be represented by the string <code class="inlineCode">'11100000'</code>, which indicates that the first three features for this loan applicant were missing. The second applicant, who had no missing data, would be represented by <code class="inlineCode">'00000000'</code>, while the second and third would be represented by <code class="inlineCode">'00000111'</code> and <code class="inlineCode">'01011101'</code>, respectively. The resulting <code class="inlineCode">mvp</code> R character vector would be converted into a factor to allow a learning algorithm to use it for prediction. Each level of the factor represents a specific pattern of missingness; loan applicants that follow the same pattern may be likely to have similar outcomes.</p>
    <p class="normal">Although missing value patterns can be extremely powerful predictors, they are not without some challenges. First and foremost, in a dataset containing <em class="italic">k</em> features, there are <em class="italic">2</em><sup class="superscript-italic" style="font-style: italic;">k </sup>potential values of a missing value pattern. A dataset with just 10 features may have as many<a id="_idIndexMarker1532"/> as 1,024 levels of the MVP predictor, while a dataset with 25 features would have over 33 million potential levels. A relatively small dataset with 50 features would have almost an uncountable number of potential MVP levels, which would make the predictor useless for modeling.</p>
    <p class="normal">Despite this potential issue, the hope with the MVP approach is that the potentially huge number of levels avoids the curse of dimensionality due to patterns of missingness that are far from uniform or random. In other words, the MVP approach depends strongly on data that is not missing at random; we hope there is a strong underlying pattern driving the missing values, which the missing value patterns will reflect. Overall, the less heterogeneity that is present in the missingness, the more often that certain patterns of missingness appear frequently in the data. Unfortunately, even if one feature is missing completely at random, it can reduce the utility of the MVP approach because even if rows are similar on nearly all of the binary missing value indicators, if a single one differs, it will be treated as a completely different missing value pattern. To address this issue, an alternative is to use the MVI dataset with an unsupervised clustering algorithm like the k-means algorithm covered in <em class="chapterRef">Chapter 9</em>, <em class="italic">Finding Groups of Data – Clustering with k-means</em>, to create clusters of people with similar patterns of missingness.</p>
    <h1 class="heading-1" id="_idParaDest-308">The problem of imbalanced data</h1>
    <p class="normal">One of the most <a id="_idIndexMarker1533"/>challenging <a id="_idIndexMarker1534"/>data issues is <strong class="keyWord">imbalanced data</strong>, which occurs when one or more class levels are much more common than the others. Many, if not most, machine learning algorithms struggle mightily to learn heavily imbalanced datasets, and although there isn’t a specific threshold that determines when a dataset is too off-balance, the problems caused by the lack of balance become increasingly serious as the problem becomes more severe.</p>
    <p class="normal">In the early stages of class imbalance, small problems are found. For instance, simple performance measures like accuracy begin to lose relevance and more sophisticated performance measures like those described in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>, are needed. As the imbalance widens, bigger problems occur. For example, with extremely imbalanced datasets, some machine learning algorithms might struggle to predict the minority group at all. With this in mind, it might be wise to begin worrying about imbalanced data when the split is worse than 80% versus 20%, worry more when it is worse than 90% versus 10%, and assume the worst when the split is more severe than 99% to 1%.</p>
    <div class="packt_tip">
      <p class="normal">A class imbalance can also occur if the real-world misclassification cost of one or more class levels is significantly higher or lower than the others.</p>
    </div>
    <p class="normal">Imbalanced data<a id="_idIndexMarker1535"/> is a prevalent yet important challenge because many of the real-world outcomes we care to predict are significant mainly because they are both rare and costly. This includes prediction tasks to identify outcomes such as:</p>
    <ul>
      <li class="bulletList">Severe illnesses or diseases</li>
      <li class="bulletList">Extreme weather and natural disasters</li>
      <li class="bulletList">Fraudulent activity</li>
      <li class="bulletList">Loan defaults</li>
      <li class="bulletList">Hardware or mechanical failure</li>
      <li class="bulletList">Wealth or so-called “whale” customers</li>
    </ul>
    <p class="normal">As you will soon learn, there is unfortunately no single best way to handle imbalanced classification problems like these and even the more advanced techniques are not without downsides. Perhaps the most important approach is to be aware of the problem of unbalanced data while recognizing that all solutions are imperfect.</p>
    <h2 class="heading-2" id="_idParaDest-309">Simple strategies for rebalancing data</h2>
    <p class="normal">If a dataset <a id="_idIndexMarker1536"/>has a severe imbalance, with some class levels having too many or too few examples, a simple solution to this problem is to subtract examples from the majority classes or add examples of the minority classes. The former strategy<a id="_idIndexMarker1537"/> is called <strong class="keyWord">undersampling</strong>, which in the simplest case involves discarding records at random from the majority classes. The latter approach is<a id="_idIndexMarker1538"/> called <strong class="keyWord">oversampling</strong>. Ideally, one would simply collect more rows of data, but this is usually not possible. Instead, examples of the minority classes are duplicated at random until the desired class balance is achieved.</p>
    <p class="normal">Under- and oversampling each have significant drawbacks but can be effective in certain circumstances. The main danger of undersampling is the risk of dropping examples that express small but important patterns in the data. Therefore, undersampling works best if a dataset is large enough to reduce the risk that removing a substantial portion of the majority classes will completely exclude key training examples. Moreover, it always leads to a feeling of defeat to voluntarily surrender information in the era of big data.</p>
    <p class="normal">Oversampling<a id="_idIndexMarker1539"/> avoids this disappointment by generating additional minority class examples but risks overfitting to unimportant patterns or noise in the minority cases. Both under- and oversampling have been included in more <a id="_idIndexMarker1540"/>advanced <strong class="keyWord">focused sampling</strong> approaches that avoid simple random sampling in favor of favoring records that maximize the decision boundaries between the groups. </p>
    <p class="normal">Such techniques are rarely used in practice due to their computational inefficiency and limited real-world efficacy.</p>
    <div class="note">
      <p class="normal">For a more in-depth review of strategies for handling imbalanced data, refer to <em class="italic">Data Mining for Imbalanced Datasets: An Overview, Chawla, N., 2010, in Data Mining and Knowledge Discovery Handbook, 2nd Edition, Maimon, O. and Rokach. L</em>.</p>
    </div>
    <p class="normal">To illustrate <a id="_idIndexMarker1541"/>resampling techniques, we’ll return to the teenage social media dataset used previously in this chapter, and begin by loading and preparing it with several tidyverse commands. First, using <code class="inlineCode">fct_</code> functions from the <code class="inlineCode">forcats</code> package, the <code class="inlineCode">gender</code> feature is recoded as a factor with <code class="inlineCode">Male</code> and <code class="inlineCode">Female</code> labels and the <code class="inlineCode">NA</code> values are recoded to <code class="inlineCode">Unknown</code>. Then, outlier ages below 13 years or greater than 20 years are replaced by <code class="inlineCode">NA</code> values. Next, using <code class="inlineCode">group_by()</code> in combination with <code class="inlineCode">mutate()</code> allows us to impute the missing ages with the median age by graduation year. Lastly, we <code class="inlineCode">ungroup()</code> the data and reorder the columns with <code class="inlineCode">select()</code> such that our features of interest appear first in the dataset. The full command is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> snsdata <span class="hljs-operator">&lt;-</span> read_csv<span class="hljs-punctuation">(</span><span class="hljs-string">"snsdata.csv"</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>
      gender <span class="hljs-operator">=</span> fct_recode<span class="hljs-punctuation">(</span>gender<span class="hljs-punctuation">,</span> Female <span class="hljs-operator">=</span> <span class="hljs-string">"F"</span><span class="hljs-punctuation">,</span> Male <span class="hljs-operator">=</span> <span class="hljs-string">"M"</span><span class="hljs-punctuation">),</span>
      gender <span class="hljs-operator">=</span> fct_na_value_to_level<span class="hljs-punctuation">(</span>gender<span class="hljs-punctuation">,</span> level <span class="hljs-operator">=</span> <span class="hljs-string">"Unknown"</span><span class="hljs-punctuation">),</span>
      age <span class="hljs-operator">=</span> ifelse<span class="hljs-punctuation">(</span>age <span class="hljs-operator">&lt;</span> <span class="hljs-number">13</span> <span class="hljs-operator">|</span> age <span class="hljs-operator">&gt;</span> <span class="hljs-number">20</span><span class="hljs-punctuation">,</span> <span class="hljs-literal">NA</span><span class="hljs-punctuation">,</span> age<span class="hljs-punctuation">)</span> 
    <span class="hljs-punctuation">)</span> <span class="hljs-operator">|&gt;</span>
    group_by<span class="hljs-punctuation">(</span>gradyear<span class="hljs-punctuation">)</span> <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>age_imp <span class="hljs-operator">=</span> if_else<span class="hljs-punctuation">(</span><span class="hljs-built_in">is.na</span><span class="hljs-punctuation">(</span>age<span class="hljs-punctuation">),</span>
           median<span class="hljs-punctuation">(</span>age<span class="hljs-punctuation">,</span> na.rm <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">),</span> age<span class="hljs-punctuation">))</span> <span class="hljs-operator">|&gt;</span>
    ungroup<span class="hljs-punctuation">()</span> <span class="hljs-operator">|&gt;</span>
    select<span class="hljs-punctuation">(</span>gender<span class="hljs-punctuation">,</span> friends<span class="hljs-punctuation">,</span> gradyear<span class="hljs-punctuation">,</span> age_imp<span class="hljs-punctuation">,</span> basketball<span class="hljs-operator">:</span>drugs<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">In this dataset, males and people of unknown gender are underrepresented, which we can confirm with the <code class="inlineCode">fct_count()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> fct_count<span class="hljs-punctuation">(</span>snsdata<span class="hljs-operator">$</span>gender<span class="hljs-punctuation">,</span> prop <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"># A tibble: 3 × 3
  f           n      p
  &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;
1 Female  22054 0.735 
2 Male     5222 0.174 
3 Unknown  2724 0.0908
</code></pre>
    <p class="normal">One <a id="_idIndexMarker1542"/>approach <a id="_idIndexMarker1543"/>would be to undersample the female and male groups such that all three have the same number of records. The <code class="inlineCode">caret</code> package, which was first introduced in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>, includes a <code class="inlineCode">downSample()</code> function that can perform this technique. The <code class="inlineCode">y</code> parameter is the categorical feature with the levels to be balanced, the <code class="inlineCode">x</code> parameter specifies the remaining columns to include in the resampled data frame, and the <code class="inlineCode">yname</code> parameter is the name of the target column:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>caret<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> sns_undersample <span class="hljs-operator">&lt;-</span> downSample<span class="hljs-punctuation">(</span>x <span class="hljs-operator">=</span> snsdata<span class="hljs-punctuation">[</span><span class="hljs-number">2</span><span class="hljs-operator">:</span><span class="hljs-number">40</span><span class="hljs-punctuation">],</span>
                                y <span class="hljs-operator">=</span> snsdata<span class="hljs-operator">$</span>gender<span class="hljs-punctuation">,</span>
                                yname <span class="hljs-operator">=</span> <span class="hljs-string">"gender"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The resulting dataset includes 2,724 examples of each of the three class levels:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> fct_count<span class="hljs-punctuation">(</span>sns_undersample<span class="hljs-operator">$</span>gender<span class="hljs-punctuation">,</span> prop <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"># A tibble: 3 × 3
  f           n     p
  &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt;
1 Female   2724 0.333
2 Male     2724 0.333
3 Unknown  2724 0.333
</code></pre>
    <p class="normal">The <code class="inlineCode">caret</code> package’s <code class="inlineCode">upSample()</code> function performs oversampling, such that all three levels have the same number of examples as the majority class:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>caret<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> sns_oversample <span class="hljs-operator">&lt;-</span> upSample<span class="hljs-punctuation">(</span>x <span class="hljs-operator">=</span> snsdata<span class="hljs-punctuation">[</span><span class="hljs-number">2</span><span class="hljs-operator">:</span><span class="hljs-number">40</span><span class="hljs-punctuation">],</span>
                             y <span class="hljs-operator">=</span> snsdata<span class="hljs-operator">$</span>gender<span class="hljs-punctuation">,</span>
                             yname <span class="hljs-operator">=</span> <span class="hljs-string">"gender"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The resulting dataset includes 22,054 examples of each of the three gender categories:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> fct_count<span class="hljs-punctuation">(</span>sns_oversample<span class="hljs-operator">$</span>gender<span class="hljs-punctuation">,</span> prop <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"># A tibble: 3 × 3
  f           n     p
  &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt;
1 Female  22054 0.333
2 Male    22054 0.333
3 Unknown 22054 0.333
</code></pre>
    <p class="normal">Whether the<a id="_idIndexMarker1544"/> oversampling or undersampling approach works better depends on the dataset as well as the machine learning algorithm used. It may be wise to build models trained on datasets created by each of these resampling techniques and see which one performs better <a id="_idIndexMarker1545"/>in testing. However, it is very important to keep in mind that the performance measures should be computed on an unbalanced test set; the evaluation should reflect the original class imbalance, as this is how the model will need to perform during real-world deployment.</p>
    <h2 class="heading-2" id="_idParaDest-310">Generating a synthetic balanced dataset with SMOTE</h2>
    <p class="normal">In addition <a id="_idIndexMarker1546"/>to undersampling and oversampling, a third rebalancing approach, called <strong class="keyWord">synthetic generation</strong>, creates <a id="_idIndexMarker1547"/>brand-new examples of the minority <a id="_idIndexMarker1548"/>class with the goal of reducing oversampling’s tendency to overfit the minority class examples. Today, there are many synthetic generation rebalancing methods, but one of the first<a id="_idIndexMarker1549"/> to gain widespread prominence was the <strong class="keyWord">SMOTE</strong> algorithm introduced by <em class="italic">Chawla</em> et al. in 2002, with a name that refers to its use of a synthetic minority oversampling technique. Put simply, the algorithm uses a set of heuristics to construct new records that are similar to but not exactly the same as those previously observed. To construct similar records, SMOTE uses the notion of similarity described in <em class="chapterRef">Chapter 3</em>, <em class="italic">Lazy Learning – Classification Using Nearest Neighbors</em>, and in fact, uses aspects of the k-NN approach directly.</p>
    <div class="note">
      <p class="normal">For more information on the SMOTE algorithm, see <em class="italic">SMOTE: Synthetic Minority Over-Sampling Technique, 2002, Chawla, N., Bowyer, K., Hall, L., and Kegelmeyer, W., Journal of Artificial Intelligence Research, Vol. 16, pp. 321-357</em>.</p>
    </div>
    <p class="normal">To understand <a id="_idIndexMarker1550"/>how SMOTE works, suppose we wanted to oversample a minority class such that the resulting dataset has <a id="_idIndexMarker1551"/>twice as many examples of this class. In the case of standard oversampling, we would simply duplicate each minority record so that it appears twice. In <em class="italic">synthetic</em> oversampling techniques like SMOTE, rather than duplicating each record, we will create a new synthetic record. If more or less oversampling is desired, we simply generate more or less than one new synthetic record per original record.</p>
    <p class="normal">A <a id="_idIndexMarker1552"/>question remains: how exactly are the synthetic records constructed? This is where the k-Nearest Neighbors technique comes in. The algorithm finds the <em class="italic">k</em> nearest neighbors of each of the original observations of the minority class. By convention, <em class="italic">k</em> is often set to five, but it can be set larger or smaller if desired. For each synthetic record to be created, the algorithm randomly selects one of the original observations’ <em class="italic">k</em> nearest neighbors. For example, to double the minority class, it would randomly select one nearest neighbor out of five for each of the original observations; to triple the original data, two out of the five nearest neighbors would be selected for each observation, and so on.</p>
    <p class="normal">Because randomly selecting the nearest neighbors merely copies the original data, one more step is needed to generate synthetic observations. In this step, the algorithm identifies the vector between each of the original observations and its randomly chosen nearest neighbors. A random number between 0 and 1 is chosen to reflect the proportion of distance along this line to place the synthetic data point. This point’s feature values will be somewhere between 100 percent identical to the original observation’s feature values and 100 percent identical to the neighbor’s feature values—or anywhere in between. This is depicted in the following figure, which illustrates how synthetic observations can be randomly placed on the lines connecting the four original data points to their neighbors. Adding the six synthetic observations creates a much better balance of the circle and square classes and thus strengthens the decision boundary and potentially makes the pattern easier for a learning algorithm to discover.</p>
    <figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" src="../Images/B17290_13_13.png"/></figure>
    <p class="packt_figref">Figure 13.13: SMOTE can create six synthetic observations from four original minority observations, which reinforces the decision boundary between the two classes (circles and squares)</p>
    <p class="normal">Of <a id="_idIndexMarker1553"/>course, the<a id="_idIndexMarker1554"/> SMOTE algorithm’s reliance on nearest neighbors and the use of distance functions means that the same <a id="_idIndexMarker1555"/>data preparation caveats apply as would with k-NN. First, the dataset needs to be completely numeric. Second, although it is not strictly necessary, it may be a good idea to transform the numeric feature values to fall on the same scale so that large ranges do not dominate the selection of nearest neighbors. We’ll see this in practice in the section that follows.</p>
    <h3 class="heading-3" id="_idParaDest-311">Example – Applying the SMOTE algorithm in R</h3>
    <p class="normal">There are<a id="_idIndexMarker1556"/> several R packages that include implementations of the SMOTE algorithm. The <code class="inlineCode">DMwR</code> package has a <code class="inlineCode">SMOTE()</code> function that is the subject of <a id="_idIndexMarker1557"/>many tutorials, but at present, is unavailable for recent versions of R. </p>
    <p class="normal">The <code class="inlineCode">smotefamily</code> package includes a variety of SMOTE functions and is well documented, but has not been updated in several years. Thus, we will use the <code class="inlineCode">smote()</code> function in <a id="_idIndexMarker1558"/>the <code class="inlineCode">themis</code> package (<a href="https://themis.tidymodels.org"><span class="url">https://themis.tidymodels.org</span></a>), which is named after Themis, the Greek goddess of justice that is often depicted holding balance scales. This package is both easy to use and well incorporated into the tidyverse.</p>
    <p class="normal">To illustrate the basic syntax of the <code class="inlineCode">smote()</code> function, we’ll begin by piping in the <code class="inlineCode">snsdata</code> dataset and using <code class="inlineCode">gender</code> as the feature to balance:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>themis<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> sns_balanced <span class="hljs-operator">&lt;-</span> snsdata <span class="hljs-operator">|&gt;</span> smote<span class="hljs-punctuation">(</span><span class="hljs-string">"gender"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Checking the<a id="_idIndexMarker1559"/> result, we use the <code class="inlineCode">table()</code> function on the dataset, which grew from 30,000 rows to 66,162 rows but is now balanced across the three gender categories:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>sns_balanced<span class="hljs-operator">$</span>gender<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"> Female    Male Unknown 
  22054   22054   22054
</code></pre>
    <p class="normal">Although this<a id="_idIndexMarker1560"/> created a gender balance, because the SMOTE algorithm relies on nearest neighbors that are determined by distance calculations, it may be better to normalize the data prior to generating the synthetic data. For instance, because the <code class="inlineCode">friends</code> feature ranges from 0 to 830 while the <code class="inlineCode">football</code> feature ranges only from 0 to 15, it is likely that the nearest neighbors will gravitate toward those with similar friend counts rather than similar interests. Applying min-max normalization can help alleviate these concerns by rescaling all features to have a range between 0 and 1.</p>
    <p class="normal">We’ve previously written our own normalization function, which we will implement again here:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> normalize <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
    <span class="hljs-built_in">return</span> <span class="hljs-punctuation">((</span>x <span class="hljs-operator">-</span> <span class="hljs-built_in">min</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">))</span> <span class="hljs-operator">/</span> <span class="hljs-punctuation">(</span><span class="hljs-built_in">max</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span> <span class="hljs-operator">-</span> <span class="hljs-built_in">min</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)))</span>
  <span class="hljs-punctuation">}</span>
</code></pre>
    <p class="normal">To return the data back to its original scale, we’ll also need an <code class="inlineCode">unnormalize()</code> function. As defined here, the function takes two parameters: the first is a vector, <code class="inlineCode">norm_values</code>, which stores the values that have been normalized; the second is a string with the name of the column that has been normalized. We need this column name so that we can obtain the minimum and maximum values for this column from the original, unnormalized data in the <code class="inlineCode">snsdata</code> dataset. The resulting <code class="inlineCode">unnormalized_vals</code> vector uses these min and max values to reverse the normalization, and then the values are rounded to integers as they were in the original data, except for the <code class="inlineCode">age_imp</code> feature, which was originally a decimal. </p>
    <p class="normal">The full <code class="inlineCode">unnormalize()</code> function is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> unnormalize <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>norm_vals<span class="hljs-punctuation">,</span> col_name<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
    old_vals <span class="hljs-operator">&lt;-</span> snsdata<span class="hljs-punctuation">[</span>col_name<span class="hljs-punctuation">]</span>
    unnormalized_vals <span class="hljs-operator">&lt;-</span> norm_vals <span class="hljs-operator">*</span>
      <span class="hljs-punctuation">(</span><span class="hljs-built_in">max</span><span class="hljs-punctuation">(</span>old_vals<span class="hljs-punctuation">)</span> <span class="hljs-operator">-</span> <span class="hljs-built_in">min</span><span class="hljs-punctuation">(</span>old_vals<span class="hljs-punctuation">))</span> <span class="hljs-operator">+</span> <span class="hljs-built_in">min</span><span class="hljs-punctuation">(</span>old_vals<span class="hljs-punctuation">)</span>
    rounded_vals <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">if</span><span class="hljs-punctuation">(</span>col_name <span class="hljs-operator">!=</span> <span class="hljs-string">"age_imp"</span><span class="hljs-punctuation">)</span>
      <span class="hljs-punctuation">{</span> <span class="hljs-built_in">round</span><span class="hljs-punctuation">(</span>unnormalized_vals<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">}</span>
      <span class="hljs-keyword">else</span> <span class="hljs-punctuation">{</span>unnormalized_vals<span class="hljs-punctuation">}</span>  
    <span class="hljs-built_in">return</span> <span class="hljs-punctuation">(</span>rounded_vals<span class="hljs-punctuation">)</span>
  <span class="hljs-punctuation">}</span>
</code></pre>
    <p class="normal">With a sequence <a id="_idIndexMarker1561"/>of pipes, we can apply normalization before using the <code class="inlineCode">smote()</code> function, followed by unnormalization afterward. This uses the dplyr <code class="inlineCode">across()</code> function to normalize and unnormalize the columns where the data type is numeric. In the case of the <code class="inlineCode">unnormalize()</code> function, the syntax is slightly<a id="_idIndexMarker1562"/> more complex due to the use of a lambda, denoted by the tilde (<code class="inlineCode">~</code>) character, which defines a function to be used across the columns where the data type is numeric. The <code class="inlineCode">normalize()</code> function did not require the use of a lambda because it uses only one parameter, whereas <code class="inlineCode">unnormalize()</code> uses two. The <code class="inlineCode">.x</code> refers to the vector of data in the column and is passed as the first parameter, while the <code class="inlineCode">cur_column()</code> function is used to pass the name of the current column as the second parameter. The complete sequence of commands is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> snsdata_balanced <span class="hljs-operator">&lt;-</span> snsdata <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>across<span class="hljs-punctuation">(</span>where<span class="hljs-punctuation">(</span><span class="hljs-built_in">is.numeric</span><span class="hljs-punctuation">),</span> normalize<span class="hljs-punctuation">))</span> <span class="hljs-operator">|&gt;</span>
    smote<span class="hljs-punctuation">(</span><span class="hljs-string">"gender"</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">|&gt;</span>
    mutate<span class="hljs-punctuation">(</span>across<span class="hljs-punctuation">(</span>where<span class="hljs-punctuation">(</span><span class="hljs-built_in">is.numeric</span><span class="hljs-punctuation">),</span> <span class="hljs-operator">~</span>unnormalize<span class="hljs-punctuation">(</span>.x<span class="hljs-punctuation">,</span> cur_column<span class="hljs-punctuation">())))</span>
</code></pre>
    <p class="normal">As before, comparing the gender balance before and after SMOTE, we see that the categories are now equivalent:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>snsdata<span class="hljs-operator">$</span>gender<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Female    Male Unknown 
  22054    5222    2724
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>snsdata_balanced<span class="hljs-operator">$</span>gender<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Female    Male Unknown 
  22054   22054   22054
</code></pre>
    <p class="normal">Note that we now have over four times as many males and over eight times as many records with unknown gender—or, that roughly three synthetic male records and seven synthetic unknown gender records have been added for each original record with male or unknown gender, respectively. The number of female examples has stayed the same. This balanced <a id="_idIndexMarker1563"/>dataset can now be used with machine learning <a id="_idIndexMarker1564"/>algorithms, keeping in mind that the model will be based mostly on synthetic cases rather than “real” examples of the minority classes. Whether or not this results in improved performance may vary from project to project, for reasons that are discussed in the following section.</p>
    <h2 class="heading-2" id="_idParaDest-312">Considering whether balanced is always better</h2>
    <p class="normal">Although it is<a id="_idIndexMarker1565"/> undeniable that severely imbalanced datasets cause challenges for learning algorithms, the best approach for handling the imbalance is quite unclear. Some even argue that the best approach is to do nothing at all! The issue is whether artificially balancing the dataset can improve the <em class="italic">overall</em> performance of a learning algorithm, or if it is just trading a reduction in specificity for an improvement in sensitivity. Because a learning algorithm that has been trained on an artificially balanced dataset will someday be deployed on the original, imbalanced dataset, it seems that the practice of balancing is simply adjusting the learner’s sense of the cost of one type of error versus the other. It is therefore counterintuitive to understand how throwing away data could result in a smarter model—that is, one that is better able to <em class="italic">truly</em> distinguish among the outcomes.</p>
    <p class="normal">Among those skeptical of artificially balancing the training data, prolific biostatistician Frank Harrell has written much on this subject. In a thoughtful blog post, he wrote that:</p>
    <blockquote class="packt_quote">
      <p class="quote">”Users of machine classifiers know that a highly imbalanced sample with regard to a binary outcome variable Y results in a strange classifier… For this reason the odd practice of subsampling the controls is used in an attempt to balance the frequencies and get some variation that will lead to sensible looking classifiers (users of regression models would never exclude good data to get an answer). Then they have to, in some ill-defined way, construct the classifier to make up for biasing the sample.”</p>
    </blockquote>
    <p class="normal">Clearly, Harrell does not think balancing the sample is generally a wise approach!</p>
    <div class="note">
      <p class="normal">For more of Harrell’s writings on this subject, see <a href="http://www.fharrell.com/post/classification/"><span class="url">http://www.fharrell.com/post/classification/</span></a> as well as <a href="http://www.fharrell.com/post/class-damage/"><span class="url">http://www.fharrell.com/post/class-damage/</span></a>.</p>
    </div>
    <p class="normal">Nina Zumel, author of <em class="italic">Practical Data Science with R</em>, performed experiments to determine whether artificially balancing the dataset improved classification performance. After conducting an experiment, she concluded that:</p>
    <blockquote class="packt_quote">
      <p class="quote">”Classification tends to be easier when the classes are nearly balanced… But I have always been skeptical of the claim that artificially balancing the classes always helps, when the model is to be run on a population with the native class prevalences… balancing the classes, or enrichment in general, is of limited value if your goal is to apply class labels… [it] is not a good idea for logistic regression models.”</p>
    </blockquote>
    <p class="normal">Much like <a id="_idIndexMarker1566"/>Frank Harrell, Nina Zumel is also suspicious of the need to artificially balance datasets for classification models. Yet, both perspectives are in opposition to a body of empirical and anecdotal evidence suggesting that artificially balancing a dataset, in fact, does improve the performance of a model.</p>
    <div class="note">
      <p class="normal">For a full description of Zumel’s experiment on imbalanced data classification, see <a href="https://win-vector.com/2015/02/27/does-balancing-classes-improve-classifier-performance/"><span class="url">https://win-vector.com/2015/02/27/does-balancing-classes-improve-classifier-performance/</span></a>.</p>
    </div>
    <p class="normal">What explains this contradictory result? It may have something to do with the choice of tool. Statistical learning algorithms, such as regression, may be well <strong class="keyWord">calibrated</strong>, meaning that they do a good job estimating the true underlying probabilities of an outcome—even for rare outcomes. Many machine learning algorithms, such as decision trees and naive Bayes, are decidedly not well calibrated, and thus may need a bit of help via artificial balancing in order to produce reasonable probabilities.</p>
    <p class="normal">Whether or not a balancing strategy is employed, it is important to use a model evaluation approach that reflects the natural imbalance that the model will be expected to perform on during deployment. This means favoring cost-aware measures like kappa, sensitivity and specificity, or precision and recall, as well as an examination of the <strong class="keyWord">receiver operating characteristic</strong> (<strong class="keyWord">ROC</strong>) curve, as discussed in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>. </p>
    <p class="normal">While it is a good idea to be skeptical of artificially balancing the dataset, it may also be worth a shot for the most challenging data problems.</p>
    <h1 class="heading-1" id="_idParaDest-313">Summary</h1>
    <p class="normal">This chapter was intended to expose you to several new types of challenging data that, although infrequently found in simple teaching examples, are regularly encountered in practice. Despite popular adages that tell us that “one can’t have too much of a good thing” or that “more is always better,” this is not always the case for machine learning algorithms, which may be distracted by irrelevant data or have trouble finding the needle in the haystack if overwhelmed by less important details. One of the seeming paradoxes of the so-called big data era is the fact that more data is simultaneously what makes machine learning possible and what makes it challenging; indeed, too much data can even lead to a so-called “curse of dimensionality.”</p>
    <p class="normal">As disappointing as it is to throw away some of the treasure of big data, this is sometimes necessary to help the learning algorithm perform as desired. Perhaps it is better to think of this as data curation in which the most relevant details are brought to the forefront. Dimensionality reduction techniques like feature selection and feature extraction are important for algorithms that don’t have built-in selection methods, but also provide benefits such as improved computational efficiency, which can be a key bottleneck for large datasets. Sparse data also requires a helping hand to bring the important details to the attention of the learning algorithm, much like the problems of outliers and missing data.</p>
    <p class="normal">Missing data, which has only been a minor problem in the book so far, presents a significant challenge in many real-world datasets. Machine learning practitioners often choose the simplest approach to solving the problem—that is, the least work necessary to get the model to perform reasonably well—yet machine learning-based approaches like multiple imputation are being used to create complete datasets in the fields of traditional statistics, biostatistics, and the social sciences.</p>
    <p class="normal">The problem of imbalanced data may be the most difficult form of challenging data to address. Many of the most important machine learning applications involve predictions on imbalanced datasets, but there are no easy solutions, only compromises. Techniques like over- and undersampling are simple but have significant downsides, while more complex techniques like SMOTE are promising but may introduce new problems of their own, and the community is divided on the best approach. The most important lesson, regardless, is to ensure that the evaluation strategy reflects the conditions the model will encounter during deployment. </p>
    <p class="normal">For example, even if the model is trained on an artificially balanced dataset, it should be tested and evaluated using the natural balance of outcomes.</p>
    <p class="normal">With these data challenges now behind us, the next chapter once again focuses on model building, and although data preparation is a substantial component of building better learners—after all, garbage in leads to garbage out—there is much more we can do to enhance the learning process itself. However, such techniques will require more than an off-the-shelf algorithm; they will require creativity and determination to maximize the learners’ potential.</p>
    <h1 class="heading-1" id="_idParaDest-314">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 4000 people at:</p>
    <p class="normal"><a href="https://packt.link/r"><span class="url">https://packt.link/r</span></a></p>
    <p class="normal"><img alt="" src="../Images/r.jpg"/></p>
  </div>
</body></html>