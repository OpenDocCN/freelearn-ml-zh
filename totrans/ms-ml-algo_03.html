<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Graph-Based Semi-Supervised Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we continue our discussion about semi-supervised learning, considering a family of algorithms that is based on the graph obtained from the dataset and the existing relationships among samples. The problems that we are going to discuss belong to two main categories: the propagation of class labels to unlabeled samples and the use of non-linear techniques based on the manifold assumption to reduce the dimensionality of the original dataset. In particular, this chapter covers the following propagation algorithms:</p>
<ul>
<li>Label propagation based on the weight matrix</li>
<li>Label propagation in Scikit-Learn (based on transition probabilities)</li>
<li>Label spreading</li>
<li>Propagation based on Markov random walks</li>
</ul>
<p>For the manifold learning section, we're discussing:</p>
<ul>
<li>Isomap algorithm and multidimensional scaling approach</li>
<li>Locally linear embedding</li>
<li>Laplacian Spectral Embedding</li>
<li>t-SNE</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Label propagation</h1>
                </header>
            
            <article>
                
<p><strong>Label propagation</strong> is a family of semi-supervised algorithms based on a graph representation of the dataset. In particular, if we have <em>N</em> labeled points (with bipolar labels +1 and -1) and <em>M</em> unlabeled points (denoted by <em>y=0</em>), it's possible to build an undirected graph based on a measure of geometric affinity among samples. If <em>G = {V, E}</em> is the formal definition of the graph, the set of vertices is made up of sample labels <em>V = { -1, +1, 0 }</em>, while the edge set is based on an <strong>affinity matrix</strong> <em>W</em> (often called <strong>adjacency matrix</strong> when the graph is unweighted), which depends only on the <em>X</em> values, not on the labels.</p>
<p>In the following graph, there's an example of such a structure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a857d71f-f48b-4a65-92f3-112d69183aa5.png" style="width:17.42em;height:16.83em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Example of binary graph</div>
<p>In the preceding example graph, there are four labeled points (two with <em>y=+1</em> and two with <em>y=-1</em>), and two unlabeled points (<em>y=0</em>). The affinity matrix is normally symmetric and square with dimensions equal to <em>(N+M) x (N+M)</em>. It can be obtained with different approaches. The most common ones, also adopted by Scikit-Learn, are:</p>
<ul>
<li><strong><em>k</em>-Nearest Neighbors</strong> (we are going to discuss this algorithm with further details in <a href="59f765c2-2ad0-4605-826e-349080f85f1f.xhtml" target="_blank">Chapter 8</a>, <em>Clustering Algorithms</em>):</li>
</ul>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3c0b235e-449d-4c59-8046-6130be025841.png" style="width:16.25em;height:3.42em;"/></p>
<ul>
<li><strong>Radial basis function kernel</strong>:</li>
</ul>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/34ccb44e-6279-4bcd-8b1a-49853b3cfdcf.png" style="width:10.17em;height:2.08em;"/></p>
<p>Sometimes, in the radial basis function kernel, the parameter <em>γ</em> is represented as the reciprocal of <em>2σ²</em>; however, small <em>γ</em> values corresponding to a large variance increase the radius, including farther points and <em>smoothing</em> the class over a number of samples, while large <span><em>γ</em> values restrict the boundaries to a subset that tends to a single sample. Instead, in the k-nearest neighbors kernel, the parameter <em>k</em> controls the number of samples to consider as neighbors.</span></p>
<p>To describe the basic algorithm, we also need to introduce the <strong>degree matrix</strong> (<em>D</em>):</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7084e81a-bdeb-4eca-8e57-a56a7302bbb7.png" style="width:39.50em;height:5.83em;"/></p>
<p>It is a diagonal matrix where each non-null element represents the <em>degree</em> of the corresponding vertex. This can be the number of incoming edges, or a measure proportional to it (as in the case of <em>W</em> based on the radial basis function). The general idea of label propagation is to let each node propagate its label to its neighbors and iterate the procedure until convergence.</p>
<p>Formally, if we have a dataset containing both labeled and unlabeled samples:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d61fa3a2-6d63-46ed-a068-f89324c21af7.png" style="width:35.08em;height:3.75em;"/></p>
<p>The complete steps of the <strong>label propagation</strong> algorithm (as proposed by <span>Zhu and Ghahramani in <em>Learning from Labeled and Unlabeled Data with Label Propagation</em></span>, <span><em>Zhu X.</em></span>, <span><em>Ghahramani Z.</em></span>, <span><em>CMU-CALD-02-107</em></span>) are:</p>
<ol>
<li>Select an affinity matrix type (KNN or RBF) and compute <em>W</em></li>
<li>Compute the degree matrix <em>D</em></li>
<li>Define <em>Y<sup>(0)</sup> = Y</em></li>
<li>Define <em>Y<sub>L</sub> = {y<sub>0</sub>, y<sub>1</sub>, ..., y<sub>N</sub>}</em></li>
<li>Iterate until convergence of the following steps:</li>
</ol>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/892bd875-805e-429d-b1ab-f9a0b86176ee.png" style="width:11.75em;height:4.00em;"/></p>
<p>The first update performs a propagation step with both labeled and unlabeled points. Each label is spread from a node through its outgoing edges, and the corresponding weight, normalized with the degree, increases or decreases the <em>effect</em> of each contribution. The second command instead resets all <em>y</em> values for the labeled samples. The final labels can be obtained as:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/68bc4da1-a9a8-4ed4-b39e-bbd3bceb56eb.png" style="width:13.42em;height:2.33em;"/></p>
<p>The proof of convergence is very easy. If we partition the matrix <em>D<sup>-1</sup>W</em> according to the relationship among labeled and unlabeled samples, we get:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a787530c-3ecc-4854-9855-63b2401ae8bf.png" style="width:13.67em;height:3.25em;"/></p>
<p>If we consider that only the first <em>N</em> components of <em>Y</em> are non-null and they are clamped at the end of each iteration, the matrix can be rewritten as:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e0f9ae2a-763c-4924-b8a5-99531e422ca7.png" style="width:24.17em;height:3.33em;"/></p>
<p>We are interested in proving the convergence for the part regarding the unlabeled samples (the labeled ones are fixed), so we can write the update rule as:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0b64dca0-af08-46d3-968f-bae377f18c79.png" style="width:16.33em;height:2.17em;"/></p>
<p>Transforming the recursion into an iterative process, the previous formula becomes:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/834692a0-7f1e-4423-aaa6-c62595755203.png" style="width:26.08em;height:4.00em;"/></p>
<p>In the previous expression, the second term is null, so we need to prove that the first term converges; however, it's easy to recognize a truncated matrix geometrical series (Neumann series), and <em>A<sub>UU</sub></em> is constructed to have all eigenvalues <em>|λ<sub>i</sub>| &lt; 1</em>, therefore the series converges to:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7abc27fc-c438-4f50-bba7-8bba0c25d66d.png" style="width:30.83em;height:4.00em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of label propagation</h1>
                </header>
            
            <article>
                
<p>We can implement the algorithm in Python, using a test bidimensional dataset:</p>
<pre>from sklearn.datasets import make_classification<br/><br/>nb_samples = 100<br/>nb_unlabeled = 75<br/><br/>X, Y = make_classification(n_samples=nb_samples, n_features=2, n_informative=2, n_redundant=0, random_state=1000)<br/>Y[Y==0] = -1<br/>Y[nb_samples - nb_unlabeled:nb_samples] = 0</pre>
<p>As in the other examples, we set <em>y = 0</em> for all unlabeled samples (75 out of 100). The corresponding plot is shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6daecfc9-002c-40ce-86d5-d33559e5fb02.png" style="width:46.92em;height:35.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Partially labeled dataset</div>
<p>The dots marked with a cross are unlabeled. At this point, we can define the affinity matrix. In this case, we compute it using both methods:</p>
<pre>from sklearn.neighbors import kneighbors_graph<br/><br/>nb_neighbors = 2<br/><br/>W_knn_sparse = kneighbors_graph(X, n_neighbors=nb_neighbors, mode='connectivity', include_self=True)<br/>W_knn = W_knn_sparse.toarray()</pre>
<p>The KNN matrix is obtained using the Scikit-Learn function <kbd>kneighbors_graph()</kbd> with the parameters <kbd>n_neighbors=2</kbd> and <kbd>mode='connectivity'</kbd>; the alternative is <kbd>'distance'</kbd>, which returns the distances instead of 0 and 1 to indicate the absence/presence of an edge. The <kbd>include_self=True</kbd> parameter is useful, as we want to have <em>W<sub>ii</sub> = 1</em>.</p>
<p>For the RBF matrix, we need to define it manually:</p>
<pre>import numpy as np<br/><br/>def rbf(x1, x2, gamma=10.0):<br/>    n = np.linalg.norm(x1 - x2, ord=1)<br/>    return np.exp(-gamma * np.power(n, 2))<br/><br/>W_rbf = np.zeros((nb_samples, nb_samples))<br/><br/>for i in range(nb_samples):<br/>    for j in range(nb_samples):<br/>        W_rbf[i, j] = rbf(X[i], X[j])</pre>
<p>The default value for <em>γ</em> is <em>10</em>, corresponding to a standard deviation <em>σ</em> equal to <em>0.22</em>. When using this method, it's important to set a correct value for <em>γ</em>; otherwise, the propagation can degenerate in the predominance of a class (<em>γ</em> too small). Now, we can compute the degree matrices and its inverse. As the procedure is identical, from this point on we continue using the RBF affinity matrix:</p>
<pre>D_rbf = np.diag(np.sum(W_rbf, axis=1))<br/>D_rbf_inv = np.linalg.inv(D_rbf)<strong><br/></strong></pre>
<p>The algorithm is implemented using a variable threshold. The value adopted here is <kbd>0.01</kbd>:</p>
<pre>tolerance = 0.01<br/><br/>Yt = Y.copy()<br/>Y_prev = np.zeros((nb_samples,))<br/>iterations = 0<br/><br/>while np.linalg.norm(Yt - Y_prev, ord=1) &gt; tolerance:<br/>    P = np.dot(D_rbf_inv, W_rbf)<br/>    Yt = np.dot(P, Yt)<br/>    Yt[0:nb_samples - nb_unlabeled] = Y[0:nb_samples - nb_unlabeled]<br/>    Y_prev = Yt.copy()<br/><br/>Y_final = np.sign(Yt)</pre>
<p>The final result is shown in the following double plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6d34f804-4d77-4999-bc98-8a0f5ca99ae5.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Original dataset (left); dataset after a complete label propagation (right)</div>
<p>As it's possible to see, in the original dataset there's a round dot surrounded by square ones (-0.9, -1). As this algorithm keeps the original labels, we find the same situation after the propagation of labels. This condition could be acceptable, even if both the smoothness and clustering assumptions are contradicted. Assuming that it's reasonable, it's possible to force a <em>correction</em> by relaxing the algorithm:</p>
<pre>tolerance = 0.01<br/><br/>Yt = Y.copy()<br/>Y_prev = np.zeros((nb_samples,))<br/>iterations = 0<br/><br/>while np.linalg.norm(Yt - Y_prev, ord=1) &gt; tolerance:<br/>    P = np.dot(D_rbf_inv, W_rbf)<br/>    Yt = np.dot(P, Yt)<br/>    Y_prev = Yt.copy()<br/><br/>Y_final = np.sign(Yt)</pre>
<p>In this way, we don't reset the original labels, letting the propagation change all those values that disagree with the neighborhood. The result is shown in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/38d6bb5b-425c-41d3-9bfa-94ccb560e004.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Original dataset (left); dataset after a complete label propagation with overwrite (right)</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Label propagation in Scikit-Learn</h1>
                </header>
            
            <article>
                
<p>Scikit-Learn implements a slightly different algorithm p<span>roposed by </span><span>Zhu and Ghahramani (in the aforementioned paper) where the affinity matrix <em>W</em> can be computed using both methods (KNN and RBF), but it is normalized to become a probability transition matrix:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/14a975e7-8317-4b6b-9476-6aab77891f64.png" style="width:12.67em;height:3.25em;"/></p>
<p>The algorithm operates like a Markov random walk, with the following sequence (assuming that there are <em>Q</em> different labels):</p>
<ol>
<li>Define a matrix <em>Y<sup>M</sup><sub>i</sub> = [P(label=y<sub>0</sub>), P(label=y<sub>1</sub>), ..., and P(label=y<sub>Q</sub>)]</em>, where <em>P(label=y<span>i</span>)</em> is the probability of the label <em><span>y</span><span>i</span></em><span>,</span> and each row is normalized so that all the elements sum up to <em>1</em></li>
<li>Define <em>Y<sup>(0)</sup><span> </span>= <span>Y</span><sup>M</sup></em></li>
</ol>
<ol start="3">
<li>Iterate until convergence of the following steps:</li>
</ol>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9595ccab-77ee-404e-8a4d-d631644d7591.png" style="width:16.50em;height:8.42em;"/></p>
<p>The first update performs a label propagation step. As we're working with probabilities, it's necessary (second step) to renormalize the rows so that their element sums up to <em>1</em>. The last update resets the original labels for all labeled samples. In this case, it means imposing a <em>P(label=y<sub>i</sub>) = 1</em> to the corresponding label, and setting all the others to zero. T<span>he proof of convergence is very similar to the one for label propagation algorithms, and can be found in <em>Learning from Labeled and Unlabeled Data with Label Propagation</em></span>,<span><em> </em></span><em>Zhu X.</em>, <em>Ghahramani Z.</em>, <em>CMU-CALD-02-107. </em>The most important result is that the solution can be obtained in closed form (without any iteration) through this formula:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fc8a69f4-f2cd-4e5c-b878-219bc03ef988.png" style="width:13.00em;height:1.67em;"/></div>
<p>The first term is the sum of a generalized geometric series, where <em>P<sub>uu</sub></em> is the unlabeled-unlabeled part of the transition matrix <em>P</em>. <em>P<sub>ul</sub></em>, instead, is the unlabeled-labeled part of the same matrix.</p>
<p>For our Python example, we need to build the dataset differently, because Scikit-Learn considers a sample unlabeled if <em>y=-1</em>:</p>
<pre>from sklearn.datasets import make_classification<br/><br/>nb_samples = 1000<br/>nb_unlabeled = 750<br/><br/>X, Y = make_classification(n_samples=nb_samples, n_features=2, n_informative=2, n_redundant=0, random_state=100)<br/>Y[nb_samples - nb_unlabeled:nb_samples] = -1</pre>
<p>We can now train a <kbd>LabelPropagation</kbd> instance with an RBF kernel and <kbd>gamma=10.0</kbd>:</p>
<pre>from sklearn.semi_supervised import LabelPropagation<br/><br/>lp = LabelPropagation(kernel='rbf', gamma=10.0)<br/>lp.fit(X, Y)<br/><br/>Y_final = lp.predict(X)</pre>
<p>The result is shown in the following double plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3871338c-0515-4545-b288-00b1cf2319e9.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Original dataset (left). Dataset after a Scikit-Learn label propagation (right)</span></div>
<p>As expected, the propagation converged to a solution that respects both the smoothness and the clustering assumption.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Label spreading</h1>
                </header>
            
            <article>
                
<p>The last algorithm (proposed by Zhou et al.) that we need to analyze is called <strong>label spreading</strong>, and it's based on the normalized graph Laplacian:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/86147c4f-38c0-4a51-af9d-81278e3fe790.png" style="width:9.08em;height:1.83em;"/></p>
<p>This matrix has each a diagonal element <em>l<sub>ii</sub></em> equal to <em>1</em>, if the degree <em>deg(l<sub>ii</sub>) &gt; 0</em> (0 otherwise) and all the other elements equal to:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/30c5dc27-5c4b-4b06-92bc-3c4c24322f69.png" style="width:25.33em;height:3.67em;"/></p>
<p>The behavior of this matrix is analogous to a discrete Laplacian operator, whose real-value version is the fundamental element of all diffusion equations. To better understand this concept, let's consider the generic heat equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/25203c70-650d-4ac9-8d41-f9b06964de5d.png" style="width:7.08em;height:2.83em;"/></p>
<p>This equation describes the behavior of the temperature of a room when a point is suddenly heated. From basic physics concepts, we know that heat will spread until the temperature reaches an equilibrium point and the speed of variation is proportional to the Laplacian of the distribution. If we consider a bidimensional grid at the equilibrium (the derivative with respect to when time becomes null) and we discretize the Laplacian operator (<em>∇<sup>2</sup> = ∇ · ∇</em>) considering the incremental ratios, we obtain:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d6c5dfc2-8f95-4cf8-883f-a5035e5aa388.png" style="width:61.42em;height:5.75em;"/></p>
<p>Therefore, at the equilibrium, each point has a value that is the mean of the direct neighbors. It's possible to prove the finite-difference equation has a single fixed point that can be found iteratively, starting from every initial condition. In addition to this idea, label spreading adopts a clamping factor <em>α</em> for the labeled samples. If <span><em>α=0</em>, the algorithm will always reset the labels to the original values (like for label propagation), while with a value in the interval <em>(0, 1]</em>, the percentage of clamped labels decreases progressively until <em>α=1</em>, when all the labels are overwritten.</span></p>
<p>The complete steps of the <strong>label spreading</strong> algorithm are:</p>
<ol>
<li>Select an affinity matrix type (KNN or RBF) and compute <em>W</em></li>
<li>Compute the degree matrix <em>D</em></li>
<li>Compute the normalized graph Laplacian <em>L</em></li>
<li>Define <em>Y<sup>(0)</sup><span> </span>= Y</em></li>
</ol>
<ol start="5">
<li>Define <span><em>α</em> in the interval <em>[0, 1]</em></span></li>
<li><span>Iterate until convergence of the following step:</span></li>
</ol>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f1abf280-e4cf-4d61-9f5a-75043a721e23.png" style="width:17.67em;height:2.08em;"/></p>
<p>It's possible to show (as demonstrated in <em><span>Semi-Supervised Learning</span></em>, <span><em><span>Chapelle O.</span></em></span>,<span><em><span> Schölkopf B</span></em></span>., <span><em><span>Zien A.</span></em></span>, (edited by), <span><em><span>The MIT Press</span></em></span>) that this algorithm is equivalent to the minimization of a quadratic cost function with the following structure:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b4b79495-24d9-4ce3-a660-bb9ba37f65c5.png" style="width:34.00em;height:3.50em;"/></p>
<p>The first term imposes consistency between original labels and estimated ones (for the labeled samples). The second term acts as a normalization factor, forcing the unlabeled terms to become zero, while the third term, which is probably the least intuitive, is needed to guarantee geometrical coherence in terms of smoothness. As we have seen in the previous paragraph, when a hard-clamping is adopted, the smoothness assumption could be violated. By minimizing this term (<em>μ</em> is proportional to <em>α</em>), it's possible to penalize the rapid changes inside the high-density regions. Also in this case, the proof of convergence is very similar to the one for label propagation algorithms, and will be omitted. The interested reader can find it in <em><span>Semi-Supervised Learning</span></em>,<em><span> </span></em><em><span>Chapelle O.</span></em>,<em><span> Schölkopf B.</span></em>, <em><span>Zien A.</span></em>, (edited by), <em><span>The MIT Press.</span></em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of label spreading</h1>
                </header>
            
            <article>
                
<p>We can test this algorithm using the Scikit-Learn implementation. Let's start by creating a very dense dataset:</p>
<pre>from sklearn.datasets import make_classification<br/><br/>nb_samples = 5000<br/>nb_unlabeled = 1000<br/><br/>X, Y = make_classification(n_samples=nb_samples, n_features=2, n_informative=2, n_redundant=0, random_state=100)<br/>Y[nb_samples - nb_unlabeled:nb_samples] = -1</pre>
<p>We can train a <kbd>LabelSpreading</kbd> instance with a clamping factor <kbd>alpha=0.2</kbd>. We want to preserve 80% of the original labels but, at the same time, we need a smooth solution:</p>
<pre>from sklearn.semi_supervised import LabelSpreading<br/><br/>ls = LabelSpreading(kernel='rbf', gamma=10.0, alpha=0.2)<br/>ls.fit(X, Y)<br/><br/>Y_final = ls.predict(X)</pre>
<p>The result is shown, as usual, together with the original dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/40b3c130-fc58-47bd-b456-230f36cbd7e9.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Original dataset (left). Dataset after a complete label spreading (right)</span></div>
<p>As it's possible to see in the first figure (left), in the central part of the cluster <em>(x [-1, 0])</em>, there's an area of circle dots. Using a hard-clamping, this <em>aisle </em>would remain unchanged, violating both the smoothness and clustering assumptions. Setting <em>α &gt; 0</em>, it's possible to avoid this problem. Of course, the choice of <span><em>α</em> is strictly correlated with each single problem. If we know that the original labels are absolutely correct, allowing the algorithm to change them can be counterproductive. In this case, for example, it would be better to preprocess the dataset, filtering out all those samples that violate the semi-supervised assumptions. If, instead, we are not sure that all samples are drawn from the same <em>p<sub>data</sub></em>, and it's possible to be in the presence of spurious elements, using a higher <em>α</em> value can smooth the dataset without any other operation.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Label propagation based on Markov random walks</h1>
                </header>
            
            <article>
                
<p>The goal of this algorithm proposed by <span>Zhu and Ghahramani</span> is to find the probability distribution of target labels for unlabeled samples given a mixed dataset. This objective is achieved through the simulation of a stochastic process, where each unlabeled sample walks through the graph until it reaches a stationary absorbing state, a labeled sample where it stops acquiring the corresponding label. The main difference with other similar approaches is that in this case, we consider the probability of reaching a labeled sample. In this way, the problem acquires a closed form and can be easily solved.</p>
<p>The first step is to always build a k-nearest neighbors graph with all <em>N</em> samples, and define a weight matrix <em>W</em> based on an RBF kernel:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/19f63010-f444-401f-bcd3-b9e9172a32e9.png" style="width:11.33em;height:2.33em;"/></p>
<p><em>W<sub>ij</sub> = 0</em> is <em>x</em><sub><em>i</em>, </sub>and <em>x<sub>j</sub></em> are not neighbors and <em>W<sub>ii</sub> = 1</em>. The transition probability matrix, similarly to the Scikit-Learn label propagation algorithm, is built as:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4f1890c2-e927-4afb-9fa3-27ef6492dbe3.png" style="width:13.33em;height:3.42em;"/></p>
<p>In a more compact way, it can be rewritten as <em>P = D<sup>-1</sup>W</em>. If we now consider a <em>test sample</em>, starting from the state <em>x<sub>i</sub></em> and randomly walking until an absorbing labeled state is found (we call this label <em>y<sup>∞</sup></em>), the probability (referred to as <strong>binary classification</strong>) can be expressed as:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8b36e8bb-1b5c-4ec3-bebb-8512091ca2f7.png" style="width:37.00em;height:4.17em;"/></p>
<p>When <em>x<sub>i</sub></em> is labeled, the state is final, and it is represented by the indicator function based on the condition <em>y<sub>i</sub>=1</em>. When the sample is unlabeled, we need to consider the sum of all possible transitions starting from <em>x<sub>i</sub></em> and ending in the closest absorbing state, with label <em>y=1</em> weighted by the relative transition probabilities. </p>
<p>We can rewrite this expression in matrix form. If we create a vector <em>P<sup>∞</sup> = [ P<sub>L</sub>(y<sup>∞</sup>=1|X<sub>L</sub>)</em>, <em>P<sub>U</sub><span>(y</span><sup>∞</sup>=1|X<sub>U</sub>) ]</em>, where the first component is based on labeled samples and the second on the unlabeled ones, we can write:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/693fff6a-7cf3-47ca-b449-706521663575.png" style="width:9.08em;height:1.25em;"/></p>
<p>If we now expand the matrices, we get:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/81ec3959-f4ac-4ee5-ba07-e2b65dded4ff.png" style="width:38.50em;height:3.75em;"/></p>
<p>As we are interested only in the unlabeled samples, we can consider only the second equation:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/24164fb4-ff31-4d99-af72-d5d67c3fc335.png" style="width:37.25em;height:1.58em;"/></p>
<p>Simplifying the expression, we get the following linear system:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dee84fb5-6771-46b1-a953-51816cfec912.png" style="width:27.92em;height:1.42em;"/></p>
<p>The term <em>(D<sub>uu</sub> - W<sub>uu</sub>)</em> is the unlabeled-unlabeled part of the unnormalized graph Laplacian <em>L = D - W</em>. By solving this system, we can get the probabilities for the class <em>y=1</em> for all unlabeled samples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of label propagation based on Markov random walks</h1>
                </header>
            
            <article>
                
<p>For this Python example of label propagation based on Markov random walks, we are going to use a bidimensional dataset containing 50 labeled samples belonging to two different classes, and 1,950 unlabeled samples:</p>
<pre>from sklearn.datasets import make_blobs<br/><br/>nb_samples = 2000<br/>nb_unlabeled = 1950<br/>nb_classes = 2<br/><br/>X, Y = make_blobs(n_samples=nb_samples, <br/>                  n_features=2, <br/>                  centers=nb_classes,<br/>                  cluster_std=2.5,<br/>                  random_state=500)<br/><br/>Y[nb_samples - nb_unlabeled:] = -1</pre>
<p>The plot of the dataset is shown in the following diagram (the crosses represent the unlabeled samples):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4eb4f91d-25b3-4345-be04-e504f66fe33f.png" style="width:59.25em;height:35.08em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Partially labeled dataset</div>
<p>We can now create the graph (using <kbd>n_neighbors=15</kbd>) and the weight matrix:</p>
<pre>import numpy as np<br/><br/>from sklearn.neighbors import kneighbors_graph<br/><br/>def rbf(x1, x2, sigma=1.0):<br/>    d = np.linalg.norm(x1 - x2, ord=1)<br/>    return np.exp(-np.power(d, 2.0) / (2 * np.power(sigma, 2)))<br/><br/>W = kneighbors_graph(X, n_neighbors=15, mode='connectivity', include_self=True).toarray()<br/><br/>for i in range(nb_samples):<br/>    for j in range(nb_samples):<br/>        if W[i, j] != 0.0:<br/>            W[i, j] = rbf(X[i], X[j])</pre>
<p>Now, we need to compute the unlabeled part of the unnormalized graph Laplacian and the unlabeled-labeled part of the matrix <em>W</em>:</p>
<pre>D = np.diag(np.sum(W, axis=1))<br/>L = D - W<br/>Luu = L[nb_samples - nb_unlabeled:, nb_samples - nb_unlabeled:]<br/>Wul = W[nb_samples - nb_unlabeled:, 0:nb_samples - nb_unlabeled,]<br/>Yl = Y[0:nb_samples - nb_unlabeled]</pre>
<p>At this point, it's possible to solve the linear system using the NumPy function <kbd>np.linalg.solve()</kbd>, which accepts as parameters the matrix <em>A</em> and the vector <em>b</em> of a generic system in the form <em>Ax=b</em>. Once we have the solution, we can merge the new labels with the original ones (where the unlabeled samples have been marked with <em>-1</em>). In this case, we don't need to convert the probabilities, because we are using <em>0</em> and <em>1</em> as labels. In general, it's necessary to use a threshold (0.5) to select the right label:</p>
<pre>Yu = np.round(np.linalg.solve(Luu, np.dot(Wul, Yl)))<br/>Y[nb_samples - nb_unlabeled:] = Yu.copy()</pre>
<p>Replotting the dataset, we get:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f7f28e6c-b0ab-4ec9-9f9c-ffbcf24992f5.png" style="width:56.08em;height:36.67em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Dataset after a complete Markov random walk label propagation</div>
<p>As expected, without any iteration, the labels have been successfully propagated to all samples in perfect compliance with the clustering assumption. Both this algorithm and label propagation can work using a closed-form solution, so they are very fast even when the number of samples is high; however, there's a fundamental problem regarding the choice of <em>σ/γ</em> for the RBF kernel. As the same authors <span>Zhu and Ghahramani remark, there is no standard solution, but it's possible to consider when <em>σ → 0</em> and when <em>σ → ∞</em>. In the first case, only the nearest point has an influence, while in the second case, the influence is extended to the whole sample space, and the unlabeled points tend to acquire the same label. The authors suggest considering the entropy of all samples, trying to find the best σ value that minimizes it. This solution can be very effective, but sometimes the minimum entropy corresponds to a label configuration that isn't impossible to achieve using these algorithms. The best approach is to try different values (at different scales) and select the one corresponding to a valid configuration with the lowest entropy. In our case, it's possible to compute the entropy of the unlabeled samples as:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/14d06359-bf07-4221-8044-e51d200428de.png" style="width:19.67em;height:4.50em;"/></p>
<p>The Python code to perform this computation is:</p>
<pre>Pu = np.linalg.solve(Luu, np.dot(Wul, Yl))<br/>H = -np.sum(Pu * np.log(Pu + 1e-6))</pre>
<p>The term <kbd>1e-6</kbd> has been added to avoid numerical problems when the probability is null. Repeating this process for different values allows us to find a set of candidates that can be restricted to a single value with a direct evaluation of the labeling accuracy (for example, when there is no precise information about the real distribution, it's possible to consider the coherence of each cluster and the separation between them). Another approach is called <strong>class rebalancing</strong>, and it's based on the idea of reweighting the probabilities of unlabeled samples to rebalance the number of points belonging to each class when the new unlabeled samples are added to the set. If we have <em>N</em> labeled points and <em>M</em> unlabeled ones, with <em>K</em> classes, the weight factor <em>w<sub>j</sub></em> for the class <em>j</em> can be obtained as:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5ae97f0c-83f4-434d-bcf1-c0b4abf09c59.png" style="width:13.42em;height:5.00em;"/></p>
<p>The numerator is the average computed over the labeled samples belonging to class <em>k</em>, while the denominator is the average over the unlabeled ones whose estimated class is <em>k</em>. The final decision about a class is no longer based only on the highest probability, but on: </p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d0c00d0b-2fd0-4c13-93d8-67f3c04f34fd.png" style="width:17.67em;height:2.00em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Manifold learning</h1>
                </header>
            
            <article>
                
<p>In <a href="f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml">Chapter 02</a>, <em>Introduction to Semi-Supervised Learning</em>, we discussed the manifold assumption, saying that high-dimensional data normally lies on low-dimensional manifolds. Of course, this is not a theorem, but in many real cases, the assumption is proven to be correct, and it allows us to work with non-linear dimensionality reduction algorithms that would be otherwise unacceptable. In this section, we're going to analyze some of these algorithms. They are all implemented in Scikit-Learn, therefore it's easy to try them with complex datasets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Isomap</h1>
                </header>
            
            <article>
                
<p><strong>Isomap</strong> is one of the simplest algorithms, and it's based on the idea of reducing the dimensionality while trying to preserve the geodesic distances measured on the original manifold where the input data lies. The algorithm works in three steps. The first operation is a k-nearest neighbors clustering and the construction of the following graph. The vertices will be the samples, while the edges represent the connections among nearest neighbors, and their weight is proportional to the distance to the corresponding neighbor. </p>
<p>The second step adopts the <strong>Dijkstra algorithm</strong> to compute the shortest pairwise distances on the graph of all couples of samples. In the following graph, there's a portion of a graph, where some shortest distances are marked:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/030d05e4-9518-4067-8ca2-8431a703a413.png" style="width:18.58em;height:18.17em;"/> </div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Example of a graph with marked shortest distances</div>
<p>For example, as <em>x<sub>3</sub></em> is a neighbor of <em>x<sub>5</sub></em> and <em>x<sub>7</sub></em>, applying the <span>Dijkstra algorithm, we could get the shortest paths <em>d(x<sub>3</sub>, x<sub>5</sub>) = w<sub>53</sub></em> and <em>d(x<sub>3</sub>, x<sub>7</sub>) = w<sub>73</sub></em>. The computational complexity of this step is about <em>O(n²log n + n²k)</em>, which is lower than <em>O(n³)</em> when <em>k &lt;&lt; n</em> (a condition normally met); however, for large graphs (with <em>n &gt;&gt; 1</em>), this is often the most expensive part of the whole algorithm.</span></p>
<p>The third step is called <strong>metric multidimensional scaling</strong>, which is a technique for finding a low-dimensional representation while trying to preserve the inner product among samples. If we have a <em>P</em>-dimensional dataset <em>X</em>, the algorithm must find a <em>Q</em>-dimensional set <em>Φ</em> with <em>Q &lt; P</em> minimizing the function:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a316c204-dade-4948-8693-15f2d01c96bf.png" style="width:17.58em;height:3.25em;"/></p>
<p>As proven in <em><span>Semi-Supervised Learning</span></em>  <em><span>Chapelle O.</span></em>,<em><span> Schölkopf B.</span></em>, <em><span>Zien A.</span></em>, (edited by), <em><span>The MIT Press</span></em>,<em><span> </span></em><span>the optimization is achieved by taking the top <em>Q</em> eigenvectors of the Gram matrix <em>G<sub>ij</sub> = x<sub>i</sub> · x</em><sub><em>j</em> </sub>(or in matrix form, <em>G=XX<sup>T</sup></em></span> if <span><em>X ∈ ℜ<sup>n × M</sup></em>);</span> h<span>owever, as the <strong>Isomap</strong> algorithm works with pairwise distances, we need to compute the matrix <em>D</em> of squared distances:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4a0e1b64-7619-4716-b7c1-a03f465db905.png" style="width:10.08em;height:1.92em;"/></p>
<p>If the <em>X</em> dataset is zero-centered, it's possible to derive a simplified Gram matrix from <em>D</em>, as described by M. A. A. Cox and T. F. Cox:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/10897a36-bcd6-411b-8743-f727a6a29cc8.png" style="width:37.17em;height:3.17em;"/></p>
<p><strong>Isomap</strong> computes the top <em>Q</em> eigenvalues <em>λ<sub>1</sub>, λ<span>2</span>, ..., <span>λ</span><sub>Q</sub></em> <span>of <em>G</em></span><sub><em>D</em> </sub>and the corresponding eigenvectors <em>ν<sub>1</sub>, <span>ν</span><sub>2</sub></em><span><em>, ..., ν<sub>Q</sub></em> and determines the <em>Q</em>-dimensional vectors as:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b2983ed5-32f1-4d17-af07-2367b64f4cdb.png" style="width:19.33em;height:4.42em;"/></p>
<p>As we're going to discuss in <a href="8d541a43-8790-4a91-a79b-e48496f75d90.xhtml" target="_blank">Chapter 5</a>, <em>EM Algorithm and Applications</em> (and also as pointed out by Saul, Weinberger, Sha, Ham, and Lee in <em><span>Spectral Methods for Dimensionality Reduction</span></em>,<em><span> </span></em><em><span>Saul L. K., Weinberger K. Q.</span></em>, <em><span>Sha F.</span></em>, <em><span>Ham J.</span></em>, <em><span>and Lee D. D.</span></em>), this kind of projection is also exploited by <strong>Principal Component Analysis</strong> (<strong><span>PCA</span></strong>), which finds out the direction with the highest variance, corresponding to the top k eigenvectors of the covariance matrix. In fact, when applying the SVD to the dataset <em>X</em>, we get:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ae41243f-3035-4652-93bf-969ff0301b9a.png" style="width:33.67em;height:1.58em;"/></p>
<p>The diagonal matrix <em>Λ</em> contains the eigenvalues of both <em>XX<sup>T</sup></em> and <em>X<sup>T</sup>X</em>; therefore, the eigenvalues <em>λ<sub>Gi</sub></em> of <em>G</em> are equal to <em>M<span>λ<sub><sup>Σ</sup></sub></span><sub>i</sub></em> where <em><span>λ<sub><sup>Σ</sup></sub></span></em><sub><em>i</em> </sub>are the eigenvalues of the covariance matrix <em><span>Σ = M</span><sup>-1</sup><span>X</span><sup>T</sup><span>X</span></em>. Hence, Isomap achieves the dimensionality reduction, trying to preserve the pairwise distances, while projecting the dataset in the subspace determined by a group of eigenvectors, where the maximum explained variance is achieved. In terms of information theory, this condition guarantees the minimum loss with an effective reduction of dimensionality.</p>
<div class="packt_infobox"><span>S</span>cikit-Learn also implements the Floyd-Warshall algorithm, which is slightly slower. For further information, please refer to<span> <em>Introduction to Algorithms</em></span>, <span><em>Cormen T. H.</em></span>, <span><em>Leiserson C. E.</em></span>, <span><em>Rivest R. L.</em></span>, <span><em>The MIT Press</em>.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of Isomap</h1>
                </header>
            
            <article>
                
<p>We can now test the Scikit-Learn <strong>Isomap</strong> implementation using the Olivetti faces dataset (provided by AT&amp;T Laboratories, Cambridge), which is made up of 400 64 × 64 grayscale portraits belonging to 40 different people. Examples of these images are shown here:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bc559a7c-5399-4618-a868-bc6956689dc2.png" style="width:63.67em;height:8.17em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Subset of the Olivetti faces dataset</div>
<p>The original dimensionality is 4096, but we want to visualize the dataset in two dimensions. It's important to understand that using the Euclidean distance for measuring the similarity of images might not the best choice, and it's surprising to see how well the samples are clustered by such a simple algorithm.</p>
<p>The first step is loading the dataset:</p>
<pre>from sklearn.datasets import fetch_olivetti_faces<br/><br/>faces = fetch_olivetti_faces()</pre>
<p>The <kbd>faces</kbd> dictionary contains three main elements:</p>
<ul>
<li><kbd>images</kbd>: Image array with shape 400 <span>× 64 × 64</span></li>
<li><kbd>data</kbd>: Flattened array with shape 400 <span>× </span>4096</li>
<li><kbd>target</kbd>: Array with shape 400 <span>× 1 containing the labels (0, 39)</span></li>
</ul>
<p>At this point, we can instantiate the <kbd>Isomap</kbd> class provided by Scikit-Learn, setting <kbd>n_components=2</kbd> and <kbd>n_neighbors=5</kbd> (the reader can try different configurations), and then fitting the model:</p>
<pre>from sklearn.manifold import Isomap<br/><br/>isomap = Isomap(n_neighbors=5, n_components=2)<br/>X_isomap = isomap.fit_transform(faces['data'])</pre>
<p>As the resulting plot with 400 elements is very dense, I preferred to show in the following plot only the first 100 samples:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fb25fa4e-0e12-4f89-b6d3-8fd8fdc74b16.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Isomap applied to 100 samples drawn from the Olivetti faces dataset</div>
<p>As it's possible to see, samples belonging to the same class are grouped in rather dense agglomerates. The classes that seem better separated are 7 and 1. Checking the corresponding faces, for class 7, we get:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/26a4a525-f57f-4fd0-9a2e-b2da1046cd20.png" style="width:65.58em;height:8.42em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Samples belonging to class 7</div>
<p>The set contains portraits of a young woman with a fair complexion, quite different from the majority of other people. Instead, for class 1, we get:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0aedec99-9ac9-4df5-816a-2a68bdb52129.png" style="width:66.17em;height:8.42em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Samples belonging to class 1</div>
<p>In this case, it's a man with big glasses and a particular mouth expression. In the dataset, there are only a few people with glasses, and one of them has a dark beard. We can conclude that <strong>Isomap</strong> created a low-dimensional representation that is really coherent with the original geodesic distances. In some cases, there's a partial clustering overlap that can be mitigated by increasing the dimensionality or adopting a more complex strategy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Locally linear embedding</h1>
                </header>
            
            <article>
                
<p>Contrary to Isomap, which works with the pairwise distances, this algorithm is based on the assumption that a high-dimensional dataset lying on a smooth manifold can have local linear structures that it tries to preserve during the dimensionality reduction process. <strong>Locally Linear Embedding</strong> (<strong>LLE</strong>), like Isomap, is based on three steps. The first one is applying the <em>k</em>-nearest neighbor algorithm to create a directed graph (in Isomap, it was undirected), where the vertices are the input samples and the edges represent a neighborhood relationship. As the graph is direct, a point <em>x<sub>i</sub></em> can be a neighbor of <em>x<sub>j</sub></em>, but the opposite could be false. It means that the weight matrix can be asymmetric.</p>
<p>The second step is based on the main assumption of local linearity. For example, consider the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/17a66576-7e7d-4da4-a1bc-1bdf45b9dda7.png" style="width:18.17em;height:18.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Graph where a neighborhood is marked with a shaded rectangle</div>
<p>The rectangle delimits a small neighboorhood. If we consider the point <em>x<sub>5</sub></em>, the local linearity assumption allows us to think that <em>x<sub>5</sub> = w<sub>56</sub>x<sub>6</sub> + w<sub>53</sub>x</em><sub><em>3</em>, </sub>without considering the cyclic relationship. This concept can be formalized for all <em>N</em> <em>P</em>-dimensional points through the minimization of the following function:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ec133037-488c-4333-a614-fab4cc445bfe.png" style="width:36.33em;height:5.17em;"/></p>
<p>In order to address the problem of low-rank neighborhood matrices (think about the previous example, with a number of neighbors equal to 20), Scikit-Learn also implements a regularizer that<span> is based on a small arbitrary additive constant that is added to the local weights</span> (according to a variant called <strong>Modified LLE</strong> or <strong>MLLE</strong><em>)</em>. At the end of this step, the matrix W that better matches the linear relationships among neighbors will be selected for the next phase.</p>
<p>In the third step, locally linear embedding tries to determine the low-dimensional (<em>Q &lt; P</em>) representation that best reproduces the original relationship among nearest neighbors. This is achieved by minimizing the following function:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/298f8db2-e6a9-419a-a88f-16eede731038.png" style="width:49.67em;height:5.42em;"/></p>
<p>The solution for this problem is obtained through the adoption of the <strong>Rayleigh-Ritz method</strong>, an algorithm to extract a subset of eigenvectors and eigenvalues from a very large sparse matrix. For further details, read <em><span>A spectrum slicing method for the Kohn–Sham problem, </span>Schofield G. Chelikowsky J. R.; Saad Y., Computer Physics Communications. 183</em>. The initial part of the final procedure consists of determining the matrix <em>D</em>:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/451a151a-82cd-493e-b5b7-151bd5fbcbd7.png" style="width:11.58em;height:1.58em;"/></p>
<p>It's possible to prove the last eigenvector (if the eigenvalues are sorted in descending order, it's the bottom one) has all components <em>v<sub>1</sub><sup>(N)</sup>, <span>v</span><sub>2</sub><sup>(N)</sup></em><span><em>, ..., v<sub>N</sub><sup>(N) </sup>= v</em>, and the corresponding eigenvalue is null. As Saul and Roweis (<em>An introduction to locally linear embedding</em></span>,<span><em> </em><em>Saul L. K.</em></span>, <span><em>Roweis S. T.</em>) pointed out, all the other <em>Q</em> eigenvectors (from the bottom) are orthogonal, and this allows them to have zero-centered embedding. Hence, t</span>he last eigenvector is discarded, while the remaining Q eigenvectors determine the embedding vectors <em>φ<sub>i</sub></em>.</p>
<div class="packt_infobox"><span>For further details about MLLE, please refer to <em>MLLE: Modified Locally Linear Embedding Using Multiple Weights</em></span>,<span><em> </em></span><em>Zhang Z., Wang J.</em>,<em> </em><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382" target="_blank">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of locally linear embedding</h1>
                </header>
            
            <article>
                
<p> We can now apply this algorithm to the Olivetti faces dataset, instantiating the Scikit-Learn class <kbd>LocallyLinearEmbedding</kbd> with <kbd>n_components=2</kbd> and <kbd>n_neighbors=15</kbd>:</p>
<pre>from sklearn.manifold import LocallyLinearEmbedding<br/><br/>lle = LocallyLinearEmbedding(n_neighbors=15, n_components=2)<br/>X_lle = lle.fit_transform(faces['data'])</pre>
<p>The result (limited to the first 100 samples) is shown in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/dc32a388-61c7-4671-8b2b-3d5c2c123179.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Locally linear embedding applied to 100 samples drawn from the Olivetti faces dataset</div>
<p>Even if the strategy is different from Isomap, we can determine some coherent clusters. In this case, the similarity is obtained through the conjunction of small linear blocks; for the faces, they can represent particular micro-features, like the shape of the nose or the presence of glasses, that remain invariant in the different portraits of the same person. LLE is, in general, preferable when the original dataset is intrinsically locally linear, possibly lying on a smooth manifold. In other words, LLE is a reasonable choice when small parts of a sample are structured in a way that allows the reconstruction of a point given the neighbors and the weights. This is often true for images, but it can be difficult to determine for a generic dataset. When the result doesn't reproduce the original clustering, it's possible to employ the next algorithm or <strong>t-SNE</strong>, which is one the most advanced.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Laplacian Spectral Embedding</h1>
                </header>
            
            <article>
                
<p>This algorithm, based on the spectral decomposition of a graph Laplacian, has been proposed in order to perform a non-linear dimensionality reduction to try to preserve the nearness of points in the <em>P</em>-dimensional manifold when remapping on a <em>Q</em>-dimensional (with <em>Q &lt; P</em>) subspace.</p>
<p>The procedure is very similar to the other algorithms. The first step is a <em>k</em>-nearest neighbor clustering to generate a graph where the vertices (we can assume to have <em>N</em> elements) are the samples, and the edges are weighted using an RBF kernel:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4b995de2-674b-45f0-8e21-137a7cedd0fb.png" style="width:10.50em;height:2.17em;"/></p>
<p>The resulting graph is undirected and symmetric. We can now define a pseudo-degree matrix <em>D</em>:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/924b2158-8338-476f-9d88-854b753d8a0f.png" style="width:23.58em;height:4.08em;"/></p>
<p>The low-dimensional representation <em>Φ</em> is obtained by minimizing the function:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1e635be3-4000-4606-853c-3d53eb27a48c.png" style="width:41.58em;height:4.42em;"/></p>
<p>If the two points <em>x<sub>i</sub></em> and <em>x<sub>j</sub></em> are near, the corresponding <em>W<sub>ij</sub></em> is close to <em>1</em>, while it tends to 0 when the distance tends to <em>∞</em>. <em>D<sub>ii</sub></em> is the sum of all weights originating from <em>x<sub>i</sub></em> (and the same for <em>D<sub>jj</sub></em>). Now, let's suppose that <em><span>x</span><sub>i</sub></em><span> is very close only to </span><em><span>x</span></em><sub><em>j</em> </sub>so, to approximate <em>D<sub>ii</sub> = D<sub>jj</sub> ≈ W<sub>ij</sub></em>. The resulting formula is a square loss based on the difference between the vectors <em>φ<sub>i</sub></em> and <em><span>φ</span><sub>j</sub></em>. When instead there are multiple <em>closeness</em> relationships to consider, the factor <em>W<sub>ij</sub></em> divided by the square root of <em>D<sub>ii</sub>D<sub>jj</sub></em> allows reweighting the new distances to find the best trade-off for the whole dataset. In practice, <em>L<sub>Φ</sub></em> is not minimized directly. In fact, it's possible to prove that the minimum can be obtained through the spectral decomposition of the symmetric normalized graph Laplacian (the name derives from this procedure):</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d93c1574-7601-48d6-9ecc-9b79d84be4d6.png" style="width:11.75em;height:2.00em;"/></p>
<p>Just like for the LLE algorithm, Laplacian Spectral Embedding also works with the bottom <em>Q + 1</em> eigenvectors. The mathematical theory behind the last step is always based on the application of the Rayleigh-Ritz method. The last one is discarded, and the remaining <em>Q</em> determines the low-dimensional representation <em><span>φ</span></em><sub><em>i</em>.</sub></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of Laplacian Spectral Embedding</h1>
                </header>
            
            <article>
                
<p>Let's apply this algorithm to the same dataset using the Scikit-Learn class <kbd>SpectralEmbedding</kbd>, with <kbd>n_components=2</kbd> and <kbd>n_neighbors=15</kbd>:</p>
<pre>from sklearn.manifold import SpectralEmbedding<br/><br/>se = SpectralEmbedding(n_components=2, n_neighbors=15)<br/>X_se = se.fit_transform(faces['data'])</pre>
<p>The resulting plot (zoomed in due to the presence of a high-density region) is shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b627d15a-85ad-4aa7-a113-4f9468463063.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Laplacian Spectral Embedding applied to the Olivetti faces dataset</div>
<p>Even in this case, we can see that some classes are grouped into small clusters, but at the same time, we observe many agglomerates where there are mixed samples. Both this and the previous method work with local pieces of information, trying to find low-dimensional representations that could preserve the geometrical structure of micro-features. This condition drives to a mapping where close points <em>share</em> local features (this is almost always true for images, but it's very difficult to prove for generic samples). Therefore, we can observe small clusters containing elements belonging to the same class, but also some <em>apparent</em> outliers, which, on the original manifold, can be globally different even if they share local <em>patches</em>. Instead, methods like Isomap or t-SNE work with the whole distribution, and try to determine a representation that is almost isometric with the original dataset considering its global properties.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">t-SNE</h1>
                </header>
            
            <article>
                
<p>This algorithm, proposed by Van der Mateen and Hinton and formally known as <strong>t-Distributed Stochastic Neighbor Embedding</strong> (<strong>t-SNE</strong>), is one of the most powerful manifold dimensionality reduction techniques. Contrary to the other methods, this algorithm starts with a fundamental assumption: the similarity between two <em>N</em>-dimensional points <em>x<sub>i</sub></em> and <em>x<sub>j</sub></em> can be represented as the conditional probability <em>p(<span>x</span><sub>j</sub>|<span>x</span><sub>i</sub>)</em> where each point is represented by a Gaussian distribution centered in <em>x<sub>i</sub></em> and with variance <em>σ<sub>i</sub></em>. The variances are selected starting from the desired perplexity, defined as:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e19f867d-dc23-4af6-aaeb-85b043b4b402.png" style="width:12.42em;height:1.75em;"/></p>
<p>Low-perplexity values indicate a low uncertainty, and are normally preferable. In common t-SNE tasks, values in the range <em>10÷50</em> are normally acceptable.</p>
<p>The assumption on the conditional probabilities can be interpreted thinking that if two samples are very similar, the probability associated with the first sample conditioned to the second one is high, while dissimilar points yield low conditional probabilities. For example, thinking about images, a point centered in the pupil can have as neighbors some points belonging to an eyelash. In terms of probabilities, we can think that <em>p(eyelash|pupil)</em> is quite high, while <em>p(nose|pupil)</em> is obviously lower. t-SNE models these conditional probabilities as:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c6b5d716-61c2-49a9-ab85-8b46e8267481.png" style="width:17.75em;height:8.17em;"/></p>
<p>The probabilities <em><span>p(</span><span>x</span><sub>i</sub><span>|</span><span>x</span><sub>i</sub></em><span><em>)</em> are set to zero, so the previous formula can be extended to the whole graph. In order to solve the problem in an easier way, the conditional probabilities are also symmetrized:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d1ab9e9c-a17a-49f4-8013-cc6fe8f9aefa.png" style="width:18.08em;height:3.17em;"/></p>
<p><span>The probability distribution so obtained represents the high-dimensional input relationship. As our goal is to reduce the dimensionality to a value <em>M &lt; N</em>, we can think about a similar probabilistic representation for the target points <em>φ<sub>i</sub></em>, using a student-t distribution with one degree of freedom:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3a1f8af3-c855-4f07-9696-b95dc7c9d696.png" style="width:21.83em;height:6.00em;"/></p>
<p>We want the low-dimensional distribution <em>Q</em> to be as close as possible to the high-dimensional distribution <em>P</em>; therefore, the aim of the <strong>t-SNE</strong> algorithm is to minimize the Kullback-Leibler divergence between <em>P</em> and <em>Q</em>:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5f8a6945-e821-4be0-a118-8b6efc69d58d.png" style="width:42.92em;height:4.00em;"/></p>
<p>The first term is the entropy of the original distribution <em>P</em>, while the second one is the cross-entropy <em>H(P, Q)</em>, which has to be minimized to solve the problem. The best approach is based on a gradient-descent algorithm, but there are also some useful variations that can improve the performance discussed in <em>Visualizing High-Dimensional Data Using t-SNE</em>, <em>Van der Maaten L.J.P., Hinton G.E., Journal of Machine Learning Research 9 (Nov), 2008.</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of t-distributed stochastic neighbor embedding </h1>
                </header>
            
            <article>
                
<p>We can apply this powerful algorithm to the same Olivetti faces dataset, using the Scikit-Learn class <kbd>TSNE</kbd> with <kbd>n_components=2</kbd> and <kbd>perplexity=20</kbd>:</p>
<pre>from sklearn.manifold import TSNE<br/><br/>tsne = TSNE(n_components=2, perplexity=20)<br/>X_tsne = tsne.fit_transform(faces['data'])</pre>
<p>The result for all 400 samples is shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2b23c4bd-e002-43c1-be89-8fd83ab1a5e1.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">t-SNE applied to the Olivetti faces dataset</div>
<p>A visual inspection of the label distribution can confirm that t-SNE recreated the optimal clustering starting from the original <span>high-dimensional distribution. This algorithm can be employed in several non-linear dimensionality reduction tasks, such as images, word embeddings, or complex feature vectors. Its main strength is hidden in the assumption to consider the similarities as probabilities, without the need to impose any constraint on the pairwise distances, either global or local. Under a certain viewpoint, it's possible</span> to consider t-SNE as <span>a reverse multiclass classification problem based on a cross-entropy cost function. Our goal is to find the labels (low-dimensional representation) given the original distribution and an assumption about the output distribution.</span></p>
<p>At this point, we could try to answer a natural question: which algorithm must be employed? The obvious answer is it depends on the single problem. When it's useful to reduce the dimensionality, preserving the global similarity among vectors (this is the case when the samples are long feature vectors without local properties, such as word embeddings or data encodings), t-SNE or Isomap are good choices. When instead it's necessary to keep the local distances (for example, the structure of a visual patch that can be shared by different samples also belonging to different classes) as close as possible to the original representation, locally linear embedding or spectral embedding algori<span>thms are preferable.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have introduced the most important label propagation techniques. In particular, we have seen how to build a dataset graph based on a weighting kernel, and how to use the geometric information provided by unlabeled samples to determine the most likely class. The basic approach works by iterating the multiplication of the label vector times the weight matrix until a stable point is reached and we have proven that, under simple assumptions, it is always possible.</p>
<p>Another approach, implemented by Scikit-Learn, is based on the transition probability from a state (represented by a sample) to another one, until the convergence to a labeled point. The probability matrix is obtained using a normalized weight matrix to encourage transitions associated to close points and discourage all the <em>long jumps</em>. The main drawback of these two methods is the hard-clamping of labeled samples; this constraint can be useful if we <em>trust</em> our dataset, but it can be a limitation in the presence of outliers whose label has been wrongly assigned.</p>
<p>Label spreading solves this problem by introducing a clamping factor that determines the percentage of clamped labels. The algorithm is very similar to label propagation, but it's based on graph Laplacian and can be employed in all those problems where the data-generating distribution is not well-determined and the probability of noise is high.</p>
<p>The propagation based on Markov random walks is a very simple algorithm that can estimate the class distribution of unlabeled samples through a stochastic process. It's possible to imagine it as a <em>test sample</em> that walks through the graph until it reaches a final labeled state (acquiring the corresponding label). The algorithm is very fast and it has a closed-form solution that can be found by solving a linear system.</p>
<p>The next topic was the introduction of manifold learning with the Isomap algorithm, which is a simple but powerful solution based on a graph built using a <em>k</em>-nearest neighbors algorithm (this is a common step in most of these algorithms). The original pairwise distances are processed using the multidimensional scaling technique, which allows obtaining a low-dimensional representation where the distances between samples are preserved.</p>
<p>Two different approaches, based on local pieces of information, are locally linear embedding and Laplacian Spectral Embedding. The former tries to preserve the local linearity present in the original manifold, while the latter, which is based on the spectral decomposition of the normalized graph Laplacian, tries to preserve the nearness of original samples. Both methods are suitable for all those tasks where it's important not to consider the whole original distribution, but the similarity induced by small data <em>patches</em>.</p>
<p>We closed this chapter by discussing t-SNE, which is a very powerful algorithm that tries to model a low-dimensional distribution that is as similar as possible to the original high-dimensional one. This task is achieved by minimizing the Kullback-Leibler divergence between the two distributions. t-SNE is a state-of-the-art algorithm, useful whenever it's important to consider the whole original distribution and the similarity between entire samples.</p>
<p>In the next chapter, <a href="0870cfd8-26b1-4e7a-9bf6-c81c84ac46b3.xhtml" target="_blank">Chapter 4</a>, <em>Bayesian Networks and Hidden Markov Models </em>we're going to introduce Bayesian networks in both a static and dynamic context, and hidden Markov models, with practical prediction examples. These algorithms allow modeling complex probabilistic scenarios made up of observed and latent variables, and infer future states using optimized sampling methods based only on the observations.</p>


            </article>

            
        </section>
    </body></html>