<html><head></head><body>
<div id="_idContainer032">
<h1 class="chapter-number" id="_idParaDest-61"><a id="_idTextAnchor063"/><span class="koboSpan" id="kobo.1.1">3</span></h1>
<h1 id="_idParaDest-62"><a id="_idTextAnchor064"/><span class="koboSpan" id="kobo.2.1">Prediction Using Classification and Regression</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Classification algorithms return accurate predictions based on our observations. </span><span class="koboSpan" id="kobo.3.2">Starting from a set of predefined class labels, the classifier assigns each piece of input data a class label according to the training model. </span><span class="koboSpan" id="kobo.3.3">Classification algorithms learn linear or non-linear associations between independent and categorical dependent variables. </span><span class="koboSpan" id="kobo.3.4">For example, a classification algorithm may learn to predict the weather as clear sky, gentle showers or heavy rain, and so on. </span><span class="koboSpan" id="kobo.3.5">Regression relates a set of independent variables to a dependent variable, numeric or continuous, for example, predicting rainfall in units of millimeters. </span><span class="koboSpan" id="kobo.3.6">Through this technique, it is possible to understand how the value of the dependent variable changes as the independent variable varies. </span><span class="koboSpan" id="kobo.3.7">This chapter shows us how to classify an object using nearest neighbors and how to perform an accurate regression analysis in a MATLAB environment. </span><span class="koboSpan" id="kobo.3.8">The aim of this chapter is to provide you with an introduction, background information, and a basic knowledge of classification and regression techniques and how to perform them </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">in MATLAB.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">Introducing classification methods </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">using MATLAB</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Building an effective and </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">accurate classifier</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Exploring different types </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">of regression</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Making predictions with regression analysis </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">in MATLAB</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Evaluating </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">model performance</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Using advanced techniques for model evaluation and selection </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">in MATLAB</span></span></li>
</ul>
<h1 id="_idParaDest-63"><a id="_idTextAnchor065"/><span class="koboSpan" id="kobo.19.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.20.1">In this chapter, we will introduce basic concepts relating to machine learning. </span><span class="koboSpan" id="kobo.20.2">To understand these topics, a basic knowledge of algebra and mathematical modeling is needed. </span><span class="koboSpan" id="kobo.20.3">A working knowledge of the MATLAB environment is </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">also required.</span></span></p>
<p><span class="koboSpan" id="kobo.22.1">To work with the MATLAB code in this chapter, you need the following files (available on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">at </span></span><a href="https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition"><span class="No-Break"><span class="koboSpan" id="kobo.24.1">https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.25.1">):</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.26.1">datatraining.txt</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.27.1">VehiclesItaly.xlsx</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.28.1">Employees.xlsx</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.29.1">AirfoilSelfNoise.xlsx</span></strong></span></li>
</ul>
<h1 id="_idParaDest-64"><a id="_idTextAnchor066"/><span class="koboSpan" id="kobo.30.1">Introducing classification methods using MATLAB</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.31.1">Classification</span></strong><span class="koboSpan" id="kobo.32.1"> methods </span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.33.1">are an essential component of machine</span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.34.1"> learning and data analysis. </span><span class="koboSpan" id="kobo.34.2">These methods allow us to categorize data into predefined classes or groups based on specific characteristics or attributes. </span><span class="koboSpan" id="kobo.34.3">By utilizing classification algorithms, we can train models to make predictions or assign labels to new, unseen data points. </span><span class="koboSpan" id="kobo.34.4">Classification plays a vital role in various domains, including image recognition, spam filtering, sentiment analysis, fraud detection, and medical diagnosis. </span><span class="koboSpan" id="kobo.34.5">It enables us to make informed decisions, identify patterns, and gain insights </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">from data.</span></span></p>
<p><span class="koboSpan" id="kobo.36.1">There are numerous classification algorithms available, each with its own strengths, assumptions, and applications. </span><span class="koboSpan" id="kobo.36.2">Some common classification methods include decision trees, </span><strong class="bold"><span class="koboSpan" id="kobo.37.1">support vector machines</span></strong><span class="koboSpan" id="kobo.38.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.39.1">SVMs</span></strong><span class="koboSpan" id="kobo.40.1">), random forests, logistic regression, and naive Bayes </span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.41.1">classifiers. </span><span class="koboSpan" id="kobo.41.2">SVM has two variations: SVC for classification and SVR for regression. </span><span class="koboSpan" id="kobo.41.3">To effectively employ classification methods, we need to understand the underlying concepts, techniques, and evaluation metrics associated with them. </span><span class="koboSpan" id="kobo.41.4">Additionally, data preparation and feature engineering play crucial roles in ensuring accurate and reliable </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">classification results.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">Throughout our learning journey, we will explore various classification methods, learn how to implement them in the MATLAB environment, and understand the necessary steps for data preparation and model evaluation. </span><span class="koboSpan" id="kobo.43.2">By the end, you will be equipped with the knowledge and skills to apply classification techniques to your own datasets and extract</span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.44.1"> valuable</span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.45.1"> insights. </span><span class="koboSpan" id="kobo.45.2">Let’s start by exploring </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">decision trees.</span></span></p>
<h2 id="_idParaDest-65"><a id="_idTextAnchor067"/><span class="koboSpan" id="kobo.47.1">Decision trees for decision-making</span></h2>
<p><span class="koboSpan" id="kobo.48.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.49.1">decision tree</span></strong><span class="koboSpan" id="kobo.50.1"> serves as a</span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.51.1"> visual representation of a choice or decision being made. </span><span class="koboSpan" id="kobo.51.2">It captures the complexity of decision-making processes where the most interesting option may not always be the most useful, and situations may not always offer clear-cut choices. </span><span class="koboSpan" id="kobo.51.3">Often, decisions are determined by a series of conditional factors that need to be evaluated. </span><span class="koboSpan" id="kobo.51.4">Representing this concept solely through tables and numbers can be challenging, as the justification behind the decision is not </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">immediately apparent.</span></span></p>
<p><span class="koboSpan" id="kobo.53.1">A decision tree structure helps us convey the same information with enhanced readability by emphasizing the specific branches that lead to the decision or evaluation. </span><span class="koboSpan" id="kobo.53.2">The technology of decision trees is valuable in identifying strategies or achieving goals by creating models with probable outcomes. </span><span class="koboSpan" id="kobo.53.3">The graphical representation of a decision tree immediately guides readers to comprehend the result. </span><span class="koboSpan" id="kobo.53.4">A visual representation is much more effective than a table filled with numbers. </span><span class="koboSpan" id="kobo.53.5">The human mind prefers to see the solution first and then trace back to understand the reasoning behind it, rather than being presented with a series of algebraic descriptions, percentages, and data to describe </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">a result.</span></span></p>
<p><span class="koboSpan" id="kobo.55.1">A decision tree is composed of the </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">following elements:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.57.1">Nodes</span></strong><span class="koboSpan" id="kobo.58.1">: These contain</span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.59.1"> the names of independent variables or factors involved in the </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">decision-making process.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.61.1">Branches</span></strong><span class="koboSpan" id="kobo.62.1">: These are labeled with the possible values of the independent variables, representing different pathways </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">or choices.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.64.1">Leaf nodes</span></strong><span class="koboSpan" id="kobo.65.1">: These represent the classes or outcomes, where observations are grouped based on the values of an independent variable. </span><span class="koboSpan" id="kobo.65.2">Leaf nodes are connected to the nodes through </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">the branches.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.67.1">Using these tools, we assign labels to data and classes to represent the confidence level of the classification itself. </span><span class="koboSpan" id="kobo.67.2">The decision tree provides the probability or likelihood of belonging to a specific class, reflecting the level of confidence in </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">the classification.</span></span></p>
<p><span class="koboSpan" id="kobo.69.1">The process of classification begins with a set of pre-classified data known as the training set. </span><span class="koboSpan" id="kobo.69.2">The goal is to define rules that characterize different classes based on this data. </span><span class="koboSpan" id="kobo.69.3">Once the model is built, it is tested using a separate test set. </span><span class="koboSpan" id="kobo.69.4">The resulting descriptions or classes are then generalized through inference or induction. </span><span class="koboSpan" id="kobo.69.5">These generalized descriptions are then used to classify records whose membership class </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">is unknown.</span></span></p>
<p><span class="koboSpan" id="kobo.71.1">Decision trees offer a straightforward approach to classifying objects into a finite number of classes. </span><span class="koboSpan" id="kobo.71.2">They are constructed by recursively dividing the records into homogeneous subsets based on a target attribute, which must </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">be categorical.</span></span></p>
<p><span class="koboSpan" id="kobo.73.1">There are two types of classification rules: univariate and multivariate. </span><strong class="bold"><span class="koboSpan" id="kobo.74.1">Univariate rules</span></strong><span class="koboSpan" id="kobo.75.1"> consider a</span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.76.1"> single predictor or target attribute at a time, while </span><strong class="bold"><span class="koboSpan" id="kobo.77.1">multivariate algorithms</span></strong><span class="koboSpan" id="kobo.78.1"> represent </span><a id="_idIndexMarker211"/><span class="koboSpan" id="kobo.79.1">the predictor as a linear combination of variables. </span><span class="koboSpan" id="kobo.79.2">The process of subdivision results in a hierarchical tree structure, where subsets are referred to as nodes, and the final nodes are called leaf nodes. </span><span class="koboSpan" id="kobo.79.3">Nodes are labeled with the attribute name, branches are labeled with possible attribute values, and leaf nodes are labeled with different values</span><a id="_idIndexMarker212"/><span class="koboSpan" id="kobo.80.1"> representing the </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">membership classes.</span></span></p>
<p><span class="koboSpan" id="kobo.82.1">To classify an object using a decision tree, we use the </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.84.1">Begin at the root of </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">the tree.</span></span></li>
<li><span class="koboSpan" id="kobo.86.1">Select the instance attribute associated with the </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">current node.</span></span></li>
<li><span class="koboSpan" id="kobo.88.1">Follow the branch corresponding to the attribute value assigned to </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">the instance.</span></span></li>
<li><span class="koboSpan" id="kobo.90.1">If a leaf node is reached, return the label associated with that leaf. </span><span class="koboSpan" id="kobo.90.2">Otherwise, repeat from </span><em class="italic"><span class="koboSpan" id="kobo.91.1">step 2</span></em><span class="koboSpan" id="kobo.92.1">, starting from the </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">current node.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.94.1">By traversing the decision tree, the path taken represents the classification rules or production rules. </span><span class="koboSpan" id="kobo.94.2">The branches represent the values assumed by different attributes, and the leaves represent the classification outcomes. </span><span class="koboSpan" id="kobo.94.3">Each rule is written along the tree, from the node to the corresponding leaf. </span><span class="koboSpan" id="kobo.94.4">All possible paths in the tree represent the different </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">classification rules.</span></span></p>
<p><span class="koboSpan" id="kobo.96.1">In summary, when classifying an instance using a decision tree, the process involves starting from the root, selecting the appropriate attribute at each node, following the corresponding branch based on the attribute value, and repeating these steps until a leaf node is reached, and</span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.97.1"> then returning the </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">associated label.</span></span></p>
<h2 id="_idParaDest-66"><a id="_idTextAnchor068"/><span class="koboSpan" id="kobo.99.1">Exploring decision trees in MATLAB</span></h2>
<p><span class="koboSpan" id="kobo.100.1">In the MATLAB</span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.101.1"> environment, the Statistics and Machine </span><a id="_idIndexMarker215"/><span class="koboSpan" id="kobo.102.1">Learning Toolbox provides the necessary tools for constructing a classification tree from raw data. </span><span class="koboSpan" id="kobo.102.2">In this section, we will delve into a well-known example featured in many machine learning books—the iris flower dataset. </span><span class="koboSpan" id="kobo.102.3">Let’s explore how to handle this within the MATLAB environment. </span><span class="koboSpan" id="kobo.102.4">We already used this dataset in </span><a href="B21156_02.xhtml#_idTextAnchor040"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.103.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.104.1">, </span><em class="italic"><span class="koboSpan" id="kobo.105.1">Working with Data </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.106.1">in MATLAB.</span></em></span></p>
<p><span class="koboSpan" id="kobo.107.1">Within this dataset, there are 50 samples belonging to each of three iris species: Iris setosa, Iris virginica, and Iris versicolor. </span><span class="koboSpan" id="kobo.107.2">Each sample consists of four measured features—sepal length, sepal width, petal length, and petal width—all measured in centimeters. </span><span class="koboSpan" id="kobo.107.3">The dataset includes the </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">following variables:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.109.1">Sepal length </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">in cm</span></span></li>
<li><span class="koboSpan" id="kobo.111.1">Sepal width </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">in cm</span></span></li>
<li><span class="koboSpan" id="kobo.113.1">Petal length </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">in cm</span></span></li>
<li><span class="koboSpan" id="kobo.115.1">Petal width </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">in cm</span></span></li>
<li><span class="koboSpan" id="kobo.117.1">Class: </span><strong class="source-inline"><span class="koboSpan" id="kobo.118.1">setosa</span></strong><span class="koboSpan" id="kobo.119.1">, </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.120.1">versicolour</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">, </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.122.1">virginica</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.123.1">Our goal is to create a classification tree that can accurately classify flower species based on the size of their sepals and petals. </span><span class="koboSpan" id="kobo.123.2">Fortunately, there is no need to connect to the previously mentioned external archive to upload data to MATLAB’s workspace. </span><span class="koboSpan" id="kobo.123.3">MATLAB already includes a file containing the necessary data within its </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">software distribution:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.125.1">To import this data, simply execute the </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.127.1">
load fisheriris</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.128.1">In MATLAB, two variables, namely </span><strong class="source-inline"><span class="koboSpan" id="kobo.129.1">meas</span></strong><span class="koboSpan" id="kobo.130.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.131.1">species</span></strong><span class="koboSpan" id="kobo.132.1">, have been successfully imported. </span><strong class="source-inline"><span class="koboSpan" id="kobo.133.1">meas</span></strong><span class="koboSpan" id="kobo.134.1"> contains the measurements for sepal and petal length and width, forming a 150x4 double matrix. </span><span class="koboSpan" id="kobo.134.2">On the other hand, </span><strong class="source-inline"><span class="koboSpan" id="kobo.135.1">species</span></strong><span class="koboSpan" id="kobo.136.1"> represents the classification information and is structured as a 150x1 cell array. </span><span class="koboSpan" id="kobo.136.2">Now, let’s examine the distribution of the three species within the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.137.1">species</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.138.1"> variable:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.139.1">tabulate(species)
       Value    Count   Percent
      setosa       50     33.33%
  versicolor       50     33.33%
   virginica       50     33.33%</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.140.1">After careful analysis, it has been determined that the sample is evenly distributed among the three species. </span><span class="koboSpan" id="kobo.140.2">To gain an overview of the listed floral species’ features, we can utilize a scatter plot matrix. </span><span class="koboSpan" id="kobo.140.3">This showcases the scatter plots of the species’ features in a convenient </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">matrix format.</span></span></p></li> <li><span class="koboSpan" id="kobo.142.1">To effectively</span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.143.1"> display</span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.144.1"> the characteristics of the species in pairs and distinguish observations within their respective groups, we can leverage the </span><strong class="source-inline"><span class="koboSpan" id="kobo.145.1">gplotmatrix()</span></strong><span class="koboSpan" id="kobo.146.1"> function. </span><span class="koboSpan" id="kobo.146.2">This function is specifically designed to create a matrix of </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">scatter plots:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.148.1">
gplotmatrix(meas, meas, species);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.149.1">The variables we want to compare in pairs are contained in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.150.1">meas</span></strong><span class="koboSpan" id="kobo.151.1"> variable, while the groups are contained in the species variable. </span><span class="koboSpan" id="kobo.151.2">Each set of axes in the resulting figure contains a scatter plot of a column of </span><strong class="source-inline"><span class="koboSpan" id="kobo.152.1">meas</span></strong><span class="koboSpan" id="kobo.153.1"> against a column of </span><strong class="source-inline"><span class="koboSpan" id="kobo.154.1">meas</span></strong><span class="koboSpan" id="kobo.155.1">. </span><span class="koboSpan" id="kobo.155.2">All graphs are grouped to describe bivariate relationships between combinations </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">of variables</span></span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">.</span></span></p><p class="list-inset"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.158.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.159.1">.1</span></em><span class="koboSpan" id="kobo.160.1"> depicts the scatter plot of the floral features for the three </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">iris species.</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer025">
<span class="koboSpan" id="kobo.162.1"><img alt="Figure 3.1 – Matrix of scatter plots grouped by species" src="image/B21156_03_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.163.1">Figure 3.1 – Matrix of scatter plots grouped by species</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.164.1">Upon a preliminary analysis of </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.165.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.166.1">.1</span></em><span class="koboSpan" id="kobo.167.1">, it becomes evident that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.168.1">setosa</span></strong><span class="koboSpan" id="kobo.169.1"> species' values are distinctly separated from the other two species, because the markers are located at various distances from the others. </span><span class="koboSpan" id="kobo.169.2">Conversely, the values of the other two</span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.170.1"> species </span><a id="_idIndexMarker219"/><span class="koboSpan" id="kobo.171.1">exhibit overlapping patterns across </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">all plots.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.173.1">Now, let’s focus on investigating the variations in petal measurements across different species. </span><span class="koboSpan" id="kobo.173.2">To achieve this, we can utilize the two columns containing petal measurements, specifically the third and fourth columns. </span><span class="koboSpan" id="kobo.173.3">To visually represent the distribution of these measurements by species, we can employ a modified version of the scatter plot that we have used previously. </span><span class="koboSpan" id="kobo.173.4">This can be achieved using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.174.1">gscatter()</span></strong><span class="koboSpan" id="kobo.175.1"> function, which generates a scatter plot grouped by a specified grouping variable. </span><span class="koboSpan" id="kobo.175.2">The function requires two vectors of the same size as arguments. </span><span class="koboSpan" id="kobo.175.3">The grouping variable must be provided in the form of a categorical variable, vector, character array, or cell array of </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">character vectors:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.177.1">
gscatter(meas(:,3), meas(:,4), species,'rgb','osd');
xlabel('Petal length');
ylabel('Petal width');</span></pre> <p class="list-inset"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.178.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.179.1">.2</span></em><span class="koboSpan" id="kobo.180.1"> distinctly illustrates the distribution of the three floral species, each occupying distinct regions within </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">the plot.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer026">
<span class="koboSpan" id="kobo.182.1"><img alt="Figure 3.2 – Scatter plot grouped by species" src="image/B21156_03_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.183.1">Figure 3.2 – Scatter plot grouped by species</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.184.1">These </span><a id="_idIndexMarker220"/><span class="koboSpan" id="kobo.185.1">observations</span><a id="_idIndexMarker221"/><span class="koboSpan" id="kobo.186.1"> indicate that it is feasible to perform classification based on </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">petal characteristics.</span></span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.188.1">To create a classification tree, we can utilize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.189.1">fitctree()</span></strong><span class="koboSpan" id="kobo.190.1"> function. </span><span class="koboSpan" id="kobo.190.2">This function generates a fitted binary decision tree by considering the input and output variables provided </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">as inputs:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.192.1">
ClassTree= fitctree(meas,species);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.193.1">The resulting binary tree is constructed by splitting branching nodes according to the values of a specific column within the </span><strong class="source-inline"><span class="koboSpan" id="kobo.194.1">meas</span></strong><span class="koboSpan" id="kobo.195.1"> matrix. </span><span class="koboSpan" id="kobo.195.2">We can visualize the created tree using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.196.1">view()</span></strong><span class="koboSpan" id="kobo.197.1"> function. </span><span class="koboSpan" id="kobo.197.2">There are two available ways to utilize </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">this function:</span></span></p><ul><li><span class="koboSpan" id="kobo.199.1">Using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.200.1">view(ClassTree)</span></strong><span class="koboSpan" id="kobo.201.1"> command will provide a text description of </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">the tree</span></span></li><li><span class="koboSpan" id="kobo.203.1">Using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.204.1">view(ClassTree,'mode','graph')</span></strong><span class="koboSpan" id="kobo.205.1"> command will generate a graphical representation of </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">the tree</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.207.1">Let’s explore both </span><a id="_idIndexMarker222"/><span class="koboSpan" id="kobo.208.1">options</span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.209.1"> to gain a comprehensive understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">the tree:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.211.1">view(ClassTree)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.212.1">The following results </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">are printed:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.214.1">Decision tree for classification
1  if x3&lt;2.45 then node 2 elseif x3&gt;=2.45 then node 3 else setosa
2  class = setosa
3  if x4&lt;1.75 then node 4 elseif x4&gt;=1.75 then node 5 else versicolor
4  if x3&lt;4.95 then node 6 elseif x3&gt;=4.95 then node 7 else versicolor
5  class = virginica
6  if x4&lt;1.65 then node 8 elseif x4&gt;=1.65 then node 9 else versicolor
7  class = virginica
8  class = versicolor
9  class = virginica</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.215.1">As observed in the provided code snippet, the classification utilizes only two input variables, </span><strong class="source-inline"><span class="koboSpan" id="kobo.216.1">x3</span></strong><span class="koboSpan" id="kobo.217.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.218.1">x4</span></strong><span class="koboSpan" id="kobo.219.1">, which represent the length and width of the petals, respectively. </span><span class="koboSpan" id="kobo.219.2">Now, let’s proceed to visualize a graphical description of the tree for </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">better comprehension:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.221.1">view(ClassTree,'mode','graph')</span></pre><p class="list-inset"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.222.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.223.1">.3</span></em><span class="koboSpan" id="kobo.224.1"> presents a graphical representation of the tree, depicting its branches and leaf nodes. </span><span class="koboSpan" id="kobo.224.2">Each</span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.225.1"> node includes the conditions </span><a id="_idIndexMarker225"/><span class="koboSpan" id="kobo.226.1">that must be met to traverse a </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">particular branch.</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer027">
<span class="koboSpan" id="kobo.228.1"><img alt="Figure 3.3 – Graphical description of the tree" src="image/B21156_03_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.229.1">Figure 3.3 – Graphical description of the tree</span></p>
<p class="list-inset"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.230.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.231.1">.3</span></em><span class="koboSpan" id="kobo.232.1"> offers valuable insights into the classification of the three floral species, providing immediate information. </span><span class="koboSpan" id="kobo.232.2">In many cases, decision tree construction is primarily focused on predicting class labels or responses. </span><span class="koboSpan" id="kobo.232.3">Once a tree is constructed, it becomes straightforward to predict responses for </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">new data.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.234.1">Let’s consider a scenario where a new combination of four data points, representing the length and width of the sepal and petal for a specific class of floral species, has been identified. </span><span class="koboSpan" id="kobo.234.2">We can use this data to predict the </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">iris species.</span></span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.236.1">To predict the classification for new data based on the previously created and trained decision tree named </span><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">ClassTree</span></strong><span class="koboSpan" id="kobo.238.1">, use the </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.240.1">
predict(ClassTree,meas)
ans =
  1×1 cell array
    {'setosa'}</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.241.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">predict()</span></strong><span class="koboSpan" id="kobo.243.1"> function</span><a id="_idIndexMarker226"/><span class="koboSpan" id="kobo.244.1"> returns a vector comprising the</span><a id="_idIndexMarker227"/><span class="koboSpan" id="kobo.245.1"> predicted class labels for the predictor data, which can be in the form of a table or matrix. </span><span class="koboSpan" id="kobo.245.2">These predictions are based on the trained classification tree. </span><span class="koboSpan" id="kobo.245.3">In the given case, only one prediction is made because the input variable contains a single record. </span><span class="koboSpan" id="kobo.245.4">If a data matrix with multiple observations were used, the function would produce a series of results equivalent to the number of rows in the </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">data matrix.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.247.1">Having built a classification tree from our data, the next step is to assess the model’s performance in predicting new observations. </span><span class="koboSpan" id="kobo.247.2">Various tools are available to measure the quality of the tree. </span><span class="koboSpan" id="kobo.247.3">A commonly used method is </span><strong class="bold"><span class="koboSpan" id="kobo.248.1">resubstitution error</span></strong><span class="koboSpan" id="kobo.249.1">, which </span><a id="_idIndexMarker228"/><span class="koboSpan" id="kobo.250.1">calculates the difference between the predicted responses and the actual responses in the training data. </span><span class="koboSpan" id="kobo.250.2">It serves as an initial estimate of the model’s performance but only provides insight in one direction. </span><span class="koboSpan" id="kobo.250.3">A high resubstitution error suggests that the tree’s predictions may not </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">be accurate.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.252.1">To calculate the resubstitution error, you can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">following command:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.254.1">resuberror = resubLoss(ClassTree)
resuberror =
    0.0200</span></pre></li> <li><span class="koboSpan" id="kobo.255.1">The low value obtained indicates that the classification tree accurately classifies a significant portion of the data. </span><span class="koboSpan" id="kobo.255.2">However, to enhance the assessment of the tree’s predictive accuracy, we can</span><a id="_idIndexMarker229"/><span class="koboSpan" id="kobo.256.1"> perform </span><strong class="bold"><span class="koboSpan" id="kobo.257.1">cross-validation</span></strong><span class="koboSpan" id="kobo.258.1">. </span><span class="koboSpan" id="kobo.258.2">This technique assesses the model’s performance by splitting the data into training and testing subsets. </span><span class="koboSpan" id="kobo.258.3">The tree is trained on the training data and evaluated on the testing data. </span><strong class="bold"><span class="koboSpan" id="kobo.259.1">Cross-validation</span></strong><span class="koboSpan" id="kobo.260.1"> helps estimate how well the tree generalizes to unseen data and avoids overfitting. </span><span class="koboSpan" id="kobo.260.2">MATLAB provides functions such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.261.1">crossval()</span></strong><span class="koboSpan" id="kobo.262.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.263.1">kfoldLoss()</span></strong><span class="koboSpan" id="kobo.264.1"> for </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">performing cross-validation.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.266.1">During cross-validation, the training data is divided randomly by default into 10 parts. </span><span class="koboSpan" id="kobo.266.2">Subsequently, 10 new trees are trained, with each tree utilizing 9 parts of the data for training. </span><span class="koboSpan" id="kobo.266.3">The</span><a id="_idIndexMarker230"/><span class="koboSpan" id="kobo.267.1"> predictive accuracy of each new</span><a id="_idIndexMarker231"/><span class="koboSpan" id="kobo.268.1"> tree is then evaluated on the remaining data that was not used for training that particular tree. </span><span class="koboSpan" id="kobo.268.2">Unlike the resubstitution error, this approach provides a reliable estimate of the predictive accuracy of the resulting tree since it tests the new trees on fresh, </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">unseen data.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.270.1">Performing cross-validation helps gauge the tree’s generalizability and offers a more robust measure of its </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">predictive performance:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.272.1">
cvrtree = crossval(ClassTree)
cvrtree =
  ClassificationPartitionedModel
    CrossValidatedModel: 'Tree'
         PredictorNames: {'x1'  'x2'  'x3'  'x4'}
           ResponseName: 'Y'
        NumObservations: 150
                  KFold: 10
              Partition: [1×1 cvpartition]
             ClassNames: {'setosa'  'versicolor'  'virginica'}
         ScoreTransform: 'none'</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.273.1">We used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.274.1">crossval()</span></strong><span class="koboSpan" id="kobo.275.1"> function, which performs a loss estimate using cross-validation. </span><span class="koboSpan" id="kobo.275.2">A cross-validated classification model is returned. </span><span class="koboSpan" id="kobo.275.3">A number of properties are now available in </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">MATLAB’s workspace.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.277.1">To calculate the cross-validation loss, you can utilize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.278.1">kfoldLoss()</span></strong><span class="koboSpan" id="kobo.279.1"> function in the </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">following manner:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.281.1">cvloss = kfoldLoss(cvrtree)
cvloss =
    0.0533</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.282.1">Here, we calculated the classification loss for observations not used for training by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">kfoldLoss()</span></strong><span class="koboSpan" id="kobo.284.1"> function. </span><span class="koboSpan" id="kobo.284.2">The low calculated value confirms that the model </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">works well.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.286.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.287.1">kfoldLoss()</span></strong><span class="koboSpan" id="kobo.288.1"> function calculates the average loss across all folds during cross-validation. </span><span class="koboSpan" id="kobo.288.2">It provides a measure of the predictive accuracy of the classification tree on unseen data. </span><span class="koboSpan" id="kobo.288.3">By default, </span><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">kfoldLoss()</span></strong><span class="koboSpan" id="kobo.290.1"> employs 10-fold cross-validation, where the data is randomly divided into 10 parts. </span><span class="koboSpan" id="kobo.290.2">Each part is then used as a testing set while the remaining data is used for training </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">the tree.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.292.1">Using </span><strong class="source-inline"><span class="koboSpan" id="kobo.293.1">kfoldLoss()</span></strong><span class="koboSpan" id="kobo.294.1"> enables us to obtain a more reliable estimate of the tree’s predictive accuracy as it assesses the model’s performance on new and unseen data, rather than the training </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">data itself.</span></span></p></li> </ol>
<p><span class="koboSpan" id="kobo.296.1">At the start of this </span><a id="_idIndexMarker232"/><span class="koboSpan" id="kobo.297.1">section, we</span><a id="_idIndexMarker233"/><span class="koboSpan" id="kobo.298.1"> listed several classification methods. </span><span class="koboSpan" id="kobo.298.2">In the following section, we will address SVM techniques and discriminant analysis </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">for classification.</span></span></p>
<h1 id="_idParaDest-67"><a id="_idTextAnchor069"/><span class="koboSpan" id="kobo.300.1">Building an effective and accurate classifier</span></h1>
<p><span class="koboSpan" id="kobo.301.1">Classification in machine learning is a supervised learning task that involves categorizing or classifying data into predefined classes or categories. </span><span class="koboSpan" id="kobo.301.2">It is one of the fundamental and widely used techniques in machine learning and data mining. </span><span class="koboSpan" id="kobo.301.3">The goal of classification is to develop a model or classifier that can accurately assign new, unseen instances to the correct class based on their features or attributes. </span><span class="koboSpan" id="kobo.301.4">The classifier learns patterns and relationships from a labeled training dataset, where each instance is associated with a known </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">class label.</span></span></p>
<p><span class="koboSpan" id="kobo.303.1">We will first </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">discuss </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.305.1">SVMs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">.</span></span></p>
<h2 id="_idParaDest-68"><a id="_idTextAnchor070"/><span class="koboSpan" id="kobo.307.1">SVMs explained</span></h2>
<p><span class="koboSpan" id="kobo.308.1">SVMs are powerful</span><a id="_idIndexMarker234"/><span class="koboSpan" id="kobo.309.1"> supervised machine learning algorithms used for classification and regression tasks. </span><span class="koboSpan" id="kobo.309.2">They are particularly effective in solving complex problems with a clear margin of separation between classes. </span><span class="koboSpan" id="kobo.309.3">SVMs can handle both linearly separable and non-linearly separable data by transforming the input space into a higher-dimensional feature space. </span><span class="koboSpan" id="kobo.309.4">The main idea behind SVMs is to find the best possible decision boundary, known as</span><a id="_idIndexMarker235"/><span class="koboSpan" id="kobo.310.1"> the </span><em class="italic"><span class="koboSpan" id="kobo.311.1">hyperplane</span></em><span class="koboSpan" id="kobo.312.1">, that maximally separates the classes while minimizing classification errors. </span><span class="koboSpan" id="kobo.312.2">The hyperplane is determined by a subset of training examples called support vectors, which are the closest points to the decision boundary. </span><span class="koboSpan" id="kobo.312.3">SVMs employ a kernel function to map the input data into a higher-dimensional space, where linear separation is more feasible. </span><span class="koboSpan" id="kobo.312.4">This allows SVMs to solve non-linear problems by finding non-linear decision boundaries in the transformed </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">feature space.</span></span></p>
<p><span class="koboSpan" id="kobo.314.1">SVMs excel in tackling intricate problems where there exists a discernible gap between different categories. </span><span class="koboSpan" id="kobo.314.2">They possess the capacity to handle datasets that can be separated both linearly and non-linearly through a process involving the transformation of the initial input space into a higher-dimensional feature space. </span><span class="koboSpan" id="kobo.314.3">The fundamental concept underpinning SVMs revolves around the quest for the optimal decision boundary, known as the hyperplane. </span><span class="koboSpan" id="kobo.314.4">SVMs leverage a kernel function to project the input data into this higher-dimensional space, where achieving linear separation becomes more attainable. </span><span class="koboSpan" id="kobo.314.5">This distinctive capability empowers SVMs to address non-linear problems by identifying non-linear decision boundaries within the transformed </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">feature space.</span></span></p>
<p><span class="koboSpan" id="kobo.316.1">The training of an SVM involves optimizing a cost function that penalizes misclassified examples and maximizes the margin between the support vectors and the decision boundary. </span><span class="koboSpan" id="kobo.316.2">There are different variations of SVMs, such as </span><strong class="bold"><span class="koboSpan" id="kobo.317.1">C-support vector machine</span></strong><em class="italic"> </em><span class="koboSpan" id="kobo.318.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.319.1">C-SVM</span></strong><span class="koboSpan" id="kobo.320.1">) for </span><a id="_idIndexMarker236"/><span class="koboSpan" id="kobo.321.1">classification tasks and </span><strong class="bold"><span class="koboSpan" id="kobo.322.1">epsilon-SVM</span></strong><span class="koboSpan" id="kobo.323.1"> for </span><a id="_idIndexMarker237"/><span class="koboSpan" id="kobo.324.1">regression tasks. </span><span class="koboSpan" id="kobo.324.2">SVMs are widely used in various domains, including image classification, text categorization, bioinformatics, and finance. </span><span class="koboSpan" id="kobo.324.3">They are known for their ability to handle high-dimensional data, handle outliers well, and generalize effectively to </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">unseen data.</span></span></p>
<p><span class="koboSpan" id="kobo.326.1">An SVM-based classification technique enables the classification of both linear and non-linear datasets. </span><span class="koboSpan" id="kobo.326.2">In SVMs, the training data instances are represented on a plane with dimensions equal to the number of attributes in each instance. </span><span class="koboSpan" id="kobo.326.3">For instance, a three-dimensional plane is used</span><a id="_idIndexMarker238"/><span class="koboSpan" id="kobo.327.1"> to represent instances with three attributes. </span><span class="koboSpan" id="kobo.327.2">The three main components of an SVM classifier are </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.329.1">Lines or hyperplanes, which act as boundaries to classify instances into </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">different classes</span></span></li>
<li><span class="koboSpan" id="kobo.331.1">Margins, which are the distances between the closest instances of </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">different classes</span></span></li>
<li><span class="koboSpan" id="kobo.333.1">Support vectors, which are the instances within the hyperplane boundaries that are challenging </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">to classify</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.335.1">SVMs separate data by finding an optimal hyperplane in a higher-dimensional feature space that maximizes the margin between </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">different classes.</span></span></p>
<p><span class="koboSpan" id="kobo.337.1">SVMs can handle linearly separable datasets, where instances can be separated by straight lines on the plane, as well as non-linearly separable datasets. </span><span class="koboSpan" id="kobo.337.2">In the case of linearly separable instances, the goal is to find the lines or hyperplanes that maximize the margin value. </span><span class="koboSpan" id="kobo.337.3">This selection minimizes the </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">classification error.</span></span></p>
<p><span class="koboSpan" id="kobo.339.1">For non-linearly separable datasets, the classification process is more complex. </span><span class="koboSpan" id="kobo.339.2">It involves two phases that build upon the </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">previous approach.</span></span></p>
<p><span class="koboSpan" id="kobo.341.1">In the first phase, instances are mapped to a higher-dimensional space to achieve linear separability. </span><span class="koboSpan" id="kobo.341.2">In the second phase, the previous approach is used to find a line or hyperplane that maximizes the margin, taking advantage of the now linearly </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">separable instances.</span></span></p>
<p><span class="koboSpan" id="kobo.343.1">To handle datasets that require non-linear functions for </span><a id="_idIndexMarker239"/><span class="koboSpan" id="kobo.344.1">separation, </span><strong class="bold"><span class="koboSpan" id="kobo.345.1">feature spaces</span></strong><span class="koboSpan" id="kobo.346.1"> are used in SVMs. </span><span class="koboSpan" id="kobo.346.2">This technique involves mapping the initial data to a higher-dimensional space. </span><span class="koboSpan" id="kobo.346.3">If the initial data has </span><em class="italic"><span class="koboSpan" id="kobo.347.1">m</span></em><span class="koboSpan" id="kobo.348.1"> dimensions and the feature space has </span><em class="italic"><span class="koboSpan" id="kobo.349.1">n</span></em><span class="koboSpan" id="kobo.350.1"> dimensions, with </span><em class="italic"><span class="koboSpan" id="kobo.351.1">m &gt; n</span></em><span class="koboSpan" id="kobo.352.1">, a mapping function is applied. </span><span class="koboSpan" id="kobo.352.2">Suppose we have an input denoted with </span><em class="italic"><span class="koboSpan" id="kobo.353.1">x</span></em><span class="koboSpan" id="kobo.354.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.355.1">y</span></em><span class="koboSpan" id="kobo.356.1">. </span><span class="koboSpan" id="kobo.356.2">The concept of kernel spaces is particularly useful for algorithms that rely on training data solely through scalar products. </span><span class="koboSpan" id="kobo.356.3">In this case, instead of explicitly finding </span><em class="italic"><span class="koboSpan" id="kobo.357.1">f(x)</span></em><span class="koboSpan" id="kobo.358.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.359.1">f(y)</span></em><span class="koboSpan" id="kobo.360.1"> in the </span><em class="italic"><span class="koboSpan" id="kobo.361.1">m</span></em><span class="koboSpan" id="kobo.362.1">-dimensional space, we only need to calculate their scalar product, denoted as </span><em class="italic"><span class="koboSpan" id="kobo.363.1">f(x) · </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.364.1">f(y)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.366.1">The function </span><em class="italic"><span class="koboSpan" id="kobo.367.1">f</span></em><span class="koboSpan" id="kobo.368.1"> is employed to map the input from the original </span><em class="italic"><span class="koboSpan" id="kobo.369.1">n</span></em><span class="koboSpan" id="kobo.370.1">-dimensional space to a higher-dimensional </span><em class="italic"><span class="koboSpan" id="kobo.371.1">m</span></em><span class="koboSpan" id="kobo.372.1">-space. </span><span class="koboSpan" id="kobo.372.2">To simplify this computation, especially in large spaces, a kernel function is utilized. </span><span class="koboSpan" id="kobo.372.3">The kernel function directly provides the scalar product of the images, eliminating the</span><a id="_idIndexMarker240"/><span class="koboSpan" id="kobo.373.1"> need for explicit mapping and making the calculation </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">more efficient.</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.375.1">K</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.376.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.377.1">x</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.378.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.379.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.380.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.381.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.382.1">f</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.383.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.384.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.385.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.386.1">·</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.387.1">f</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.388.1">(</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.389.1">y</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.390.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.391.1">Here, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">the following:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.393.1">x</span></em><span class="koboSpan" id="kobo.394.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.395.1">y</span></em><span class="koboSpan" id="kobo.396.1"> are </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">input vectors</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.398.1">f</span></em><span class="koboSpan" id="kobo.399.1"> is a </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">transformation function</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.401.1">The dot product operation, denoted by </span><em class="italic"><span class="koboSpan" id="kobo.402.1">f(x) · f(y)</span></em><span class="koboSpan" id="kobo.403.1">, calculates the dot product of the transformed input vectors. </span><span class="koboSpan" id="kobo.403.2">It allows us to efficiently compute the scalar product of the mapped input vectors without explicitly calculating the mapping function, </span><em class="italic"><span class="koboSpan" id="kobo.404.1">f(x)</span></em><span class="koboSpan" id="kobo.405.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.406.1">f(y)</span></em><span class="koboSpan" id="kobo.407.1">. </span><span class="koboSpan" id="kobo.407.2">This dot product operation is an essential component in various algorithms that utilize kernel methods for classification and </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">regression tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.409.1">The purpose</span><a id="_idIndexMarker241"/><span class="koboSpan" id="kobo.410.1"> of the </span><strong class="bold"><span class="koboSpan" id="kobo.411.1">kernel function</span></strong><span class="koboSpan" id="kobo.412.1"> is to transform the input data into a suitable form, particularly when it is not feasible to determine a linearly separable hyperplane, which</span><a id="_idIndexMarker242"/><span class="koboSpan" id="kobo.413.1"> is </span><a id="_idIndexMarker243"/><span class="koboSpan" id="kobo.414.1">often</span><a id="_idIndexMarker244"/><span class="koboSpan" id="kobo.415.1"> the </span><a id="_idIndexMarker245"/><span class="koboSpan" id="kobo.416.1">case. </span><span class="koboSpan" id="kobo.416.2">There are several commonly used kernels, including </span><strong class="bold"><span class="koboSpan" id="kobo.417.1">linear</span></strong><span class="koboSpan" id="kobo.418.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.419.1">polynomial</span></strong><span class="koboSpan" id="kobo.420.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.421.1">radial basis function</span></strong><span class="koboSpan" id="kobo.422.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.423.1">RBF</span></strong><span class="koboSpan" id="kobo.424.1">), </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.426.1">sigmoid</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.428.1">In this methodology, the training phase and the subsequent error evaluation activity play a crucial role. </span><span class="koboSpan" id="kobo.428.2">To accomplish this, the data is split into two subsets, named training and testing. </span><span class="koboSpan" id="kobo.428.3">The </span><strong class="bold"><span class="koboSpan" id="kobo.429.1">training set</span></strong><span class="koboSpan" id="kobo.430.1"> is</span><a id="_idIndexMarker246"/><span class="koboSpan" id="kobo.431.1"> utilized for algorithm training and comprises labeled inputs and outputs for supervised learning. </span><span class="koboSpan" id="kobo.431.2">Typically, it consists of approximately 80% of the total data. </span><span class="koboSpan" id="kobo.431.3">The </span><strong class="bold"><span class="koboSpan" id="kobo.432.1">testing set</span></strong><span class="koboSpan" id="kobo.433.1">, on</span><a id="_idIndexMarker247"/><span class="koboSpan" id="kobo.434.1"> the other hand, is used to evaluate the accuracy of the SVM. </span><span class="koboSpan" id="kobo.434.2">It contains 20% of the remaining data that was not used in training, and it helps measure the prediction error. </span><span class="koboSpan" id="kobo.434.3">This phase also serves to test the algorithm in real-world scenarios, simulating the actual usage of the </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">trained model.</span></span></p>
<p><span class="koboSpan" id="kobo.436.1">One notable advantage of SVM-based classifiers is their capability to handle complex and non-linear classification problems while ensuring a high level of accuracy. </span><span class="koboSpan" id="kobo.436.2">However, for simpler problems, the accuracy may be comparable to that of decision tree-based classification techniques. </span><span class="koboSpan" id="kobo.436.3">Some drawbacks of SVMs include the relatively longer time required for model</span><a id="_idIndexMarker248"/><span class="koboSpan" id="kobo.437.1"> creation, although it is still faster compared to neural networks. </span><span class="koboSpan" id="kobo.437.2">The other drawback is the lack of interpretability of the model’s </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">inner workings.</span></span></p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor071"/><span class="koboSpan" id="kobo.439.1">Supervised classification using SVM</span></h2>
<p><span class="koboSpan" id="kobo.440.1">Supervised</span><a id="_idIndexMarker249"/><span class="koboSpan" id="kobo.441.1"> classification is a </span><a id="_idIndexMarker250"/><span class="koboSpan" id="kobo.442.1">machine learning technique used to classify or categorize data into predefined classes or categories. </span><span class="koboSpan" id="kobo.442.2">Supervised classification relies on labeled training data, in which data points are linked to a specific class. </span><span class="koboSpan" id="kobo.442.3">The objective of supervised classification is to construct a model capable of making precise predictions regarding the class of previously unseen or fresh data points, utilizing their inherent features </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">as guidance.</span></span></p>
<p><span class="koboSpan" id="kobo.444.1">To understand how to implement the SVM algorithm in MATLAB, we will use a dataset for binary classification. </span><strong class="bold"><span class="koboSpan" id="kobo.445.1">Binary classification</span></strong><span class="koboSpan" id="kobo.446.1"> is a</span><a id="_idIndexMarker251"/><span class="koboSpan" id="kobo.447.1"> specific type of supervised classification where the goal is to classify data into one of two mutually exclusive classes or categories. </span><span class="koboSpan" id="kobo.447.2">The two classes are often referred to as the positive class and the negative class, or class 1 and class 0. </span><span class="koboSpan" id="kobo.447.3">In binary classification, the labeled training data consists of examples where each data point is associated with one of the two classes. </span><span class="koboSpan" id="kobo.447.4">The machine learning algorithm is then trained on this labeled data to learn a model that can accurately predict the class of unseen </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">data points.</span></span></p>
<p><span class="koboSpan" id="kobo.449.1">The dataset consists of features such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.450.1">Temperature</span></strong><span class="koboSpan" id="kobo.451.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.452.1">Humidity</span></strong><span class="koboSpan" id="kobo.453.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.454.1">Light</span></strong><span class="koboSpan" id="kobo.455.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.456.1">CO2</span></strong><span class="koboSpan" id="kobo.457.1">, and the target variable is room occupancy. </span><span class="koboSpan" id="kobo.457.2">The ground-truth occupancy information was obtained by capturing time-stamped pictures </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">every minute.</span></span></p>
<p><span class="koboSpan" id="kobo.459.1">The goal of this experiment is to train a binary classification model that can predict whether the room is occupied or unoccupied based on the given sensor data. </span><span class="koboSpan" id="kobo.459.2">The features (</span><strong class="source-inline"><span class="koboSpan" id="kobo.460.1">Temperature</span></strong><span class="koboSpan" id="kobo.461.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.462.1">Humidity</span></strong><span class="koboSpan" id="kobo.463.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.464.1">Light</span></strong><span class="koboSpan" id="kobo.465.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.466.1">CO2</span></strong><span class="koboSpan" id="kobo.467.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.468.1">Humidity Ratio</span></strong><span class="koboSpan" id="kobo.469.1">) are used as input to the model, and the corresponding occupancy status (</span><strong class="source-inline"><span class="koboSpan" id="kobo.470.1">occupied</span></strong><span class="koboSpan" id="kobo.471.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.472.1">unoccupied</span></strong><span class="koboSpan" id="kobo.473.1">) serves as the target variable for training </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">and evaluation.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.475.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.476.1">To download the dataset and a short summary of the variables contained within it, please refer to UCI Machine Learning Repository at the following </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">link: </span></span><a href="https://doi.org/10.24432/C5X01N"><span class="No-Break"><span class="koboSpan" id="kobo.478.1">https://doi.org/10.24432/C5X01N</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.479.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.480.1">By analyzing the relationship between the sensor data and the ground-truth occupancy, the model aims to learn patterns and make accurate predictions on unseen data. </span><span class="koboSpan" id="kobo.480.2">The training process involves feeding the labeled data into a binary classification algorithm, using SVM. </span><span class="koboSpan" id="kobo.480.3">The algorithm learns from the features and their corresponding occupancy labels to create a </span><a id="_idIndexMarker252"/><span class="koboSpan" id="kobo.481.1">model </span><a id="_idIndexMarker253"/><span class="koboSpan" id="kobo.482.1">that can classify </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">new instances.</span></span></p>
<p><span class="koboSpan" id="kobo.484.1">Take the </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.486.1">We start by importing the data into the MATLAB environment. </span><span class="koboSpan" id="kobo.486.2">To do that, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.487.1">readtable()</span></strong><span class="koboSpan" id="kobo.488.1"> function, as we learned in </span><a href="B21156_02.xhtml#_idTextAnchor040"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.489.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.490.1">, </span><em class="italic"><span class="koboSpan" id="kobo.491.1">Working with Data in MATLAB</span></em><span class="koboSpan" id="kobo.492.1">, in the </span><em class="italic"><span class="koboSpan" id="kobo.493.1">Importing data into MATLAB</span></em><span class="koboSpan" id="kobo.494.1"> section. </span><span class="koboSpan" id="kobo.494.2">We need to use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.495.1">datatraining.txt</span></strong><span class="koboSpan" id="kobo.496.1"> file, containing eight variables (</span><strong class="source-inline"><span class="koboSpan" id="kobo.497.1">Num</span></strong><span class="koboSpan" id="kobo.498.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.499.1">date</span></strong><span class="koboSpan" id="kobo.500.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.501.1">Temperature</span></strong><span class="koboSpan" id="kobo.502.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.503.1">Humidity</span></strong><span class="koboSpan" id="kobo.504.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.505.1">Light</span></strong><span class="koboSpan" id="kobo.506.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.507.1">CO2</span></strong><span class="koboSpan" id="kobo.508.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.509.1">HumidityRatio</span></strong><span class="koboSpan" id="kobo.510.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.511.1">Occupancy</span></strong><span class="koboSpan" id="kobo.512.1">). </span><span class="koboSpan" id="kobo.512.2">But for our purpose, only six variables are needed, that is, the last six. </span><span class="koboSpan" id="kobo.512.3">So, we have to set some options to use in the function call, </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.514.1">
opts = detectImportOptions('datatraining.txt');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.515.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.516.1">detectImportOptions()</span></strong><span class="koboSpan" id="kobo.517.1"> function in MATLAB is used to automatically determine the import options for a specific file. </span><span class="koboSpan" id="kobo.517.2">It analyzes the file’s format and content to provide a suggested set of options that can be used with other file input functions, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.518.1">readtable()</span></strong><span class="koboSpan" id="kobo.519.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.520.1">readmatrix()</span></strong><span class="koboSpan" id="kobo.521.1">, to import the data from the file. </span><span class="koboSpan" id="kobo.521.2">The function returns an </span><strong class="source-inline"><span class="koboSpan" id="kobo.522.1">opts</span></strong><span class="koboSpan" id="kobo.523.1"> object, which is an instance of the class. </span><span class="koboSpan" id="kobo.523.2">This object contains various properties and methods that allow you to inspect and modify the import options. </span><span class="koboSpan" id="kobo.523.3">We can explore the data detected by simply calling the name of the </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">object (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.525.1">opts</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">).</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.527.1">The first two columns are not useful for our purpose, so we can modify this property using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.528.1">opts.SelectedVariableNames</span></strong><span class="koboSpan" id="kobo.529.1"> option, </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">as follows:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.531.1">opts.SelectedVariableNames = [3:8];</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.532.1">Only the last six variables have </span><span class="No-Break"><span class="koboSpan" id="kobo.533.1">been selected.</span></span></p></li> <li><span class="koboSpan" id="kobo.534.1">Now we are ready to import </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">the dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.536.1">
DataMatrix = readtable('datatraining.txt',opts);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.537.1">We can see, in the MATLAB workspace, a new variable named </span><strong class="source-inline"><span class="koboSpan" id="kobo.538.1">DataMatrix</span></strong><span class="koboSpan" id="kobo.539.1"> as a table of size 814x6. </span><span class="koboSpan" id="kobo.539.2">This table contains the five predictors (</span><strong class="source-inline"><span class="koboSpan" id="kobo.540.1">Temperature</span></strong><span class="koboSpan" id="kobo.541.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.542.1">Humidity</span></strong><span class="koboSpan" id="kobo.543.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.544.1">Light</span></strong><span class="koboSpan" id="kobo.545.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.546.1">CO2</span></strong><span class="koboSpan" id="kobo.547.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.548.1">HumidityRatio</span></strong><span class="koboSpan" id="kobo.549.1">) and one dichotomic </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">response (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.551.1">Occupancy</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">).</span></span></p></li> <li><span class="koboSpan" id="kobo.553.1">To train an</span><a id="_idIndexMarker254"/><span class="koboSpan" id="kobo.554.1"> SVM </span><a id="_idIndexMarker255"/><span class="koboSpan" id="kobo.555.1">classifier for binary classification problems, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.556.1">fitcsvm()</span></strong><span class="koboSpan" id="kobo.557.1"> function </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.559.1">
SVMClassifier =  fitcsvm(DataMatrix(:,1:5),DataMatrix(:,6));</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.560.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.561.1">DataMatrix(:,1:5)</span></strong><span class="koboSpan" id="kobo.562.1"> parameter represents the predictor data, which is an 8,143-by-5 matrix, where 8,143 is the matrix’s dimension consisting of the number of observations, denoting data points, and 5 signifies the count of predictor variables, or features. </span><span class="koboSpan" id="kobo.562.2">Each row within this matrix represents a distinct observation, while each column pertains to an individual predictor variable. </span><span class="koboSpan" id="kobo.562.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.563.1">DataMatrix(:,6)</span></strong><span class="koboSpan" id="kobo.564.1"> parameter represents the response data, which is an 8,143-by-1 vector. </span><span class="koboSpan" id="kobo.564.2">For binary classification, this vector should be a binary vector of 0s and 1s, where 0 represents the negative class and 1 represents the </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">positive class.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.566.1">The function returns an </span><strong class="source-inline"><span class="koboSpan" id="kobo.567.1">SVMClassifier</span></strong><span class="koboSpan" id="kobo.568.1"> object, which represents the trained SVM classifier. </span><span class="koboSpan" id="kobo.568.2">This object contains information about the trained model, such as support vectors, coefficients, and kernel parameters. </span><span class="koboSpan" id="kobo.568.3">We can print some of this information simply by typing the name of the model into the MATLAB </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">command prompt:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.570.1">SVMClassifier
SVMClassifier =
  ClassificationSVM
           PredictorNames: {'Temperature'  'Humidity'  'Light'  'CO2'  'HumidityRatio'}
             ResponseName: 'Occupancy'
    CategoricalPredictors: []
               ClassNames: [0 1]
           ScoreTransform: 'none'
          NumObservations: 8143
                    Alpha: [256×1 double]
                     Bias: -7.1986
         KernelParameters: [1×1 struct]
           BoxConstraints: [8143×1 double]
          ConvergenceInfo: [1×1 struct]
          IsSupportVector: [8143×1 logical]
                   Solver: 'SMO'</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.571.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.572.1">fitcsvm()</span></strong><span class="koboSpan" id="kobo.573.1"> function </span><a id="_idIndexMarker256"/><span class="koboSpan" id="kobo.574.1">applies </span><a id="_idIndexMarker257"/><span class="koboSpan" id="kobo.575.1">the linear kernel by default. </span><span class="koboSpan" id="kobo.575.2">It’s possible to set a different kernel using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.576.1">KernelFunction</span></strong><span class="koboSpan" id="kobo.577.1"> parameter. </span><span class="koboSpan" id="kobo.577.2">The supported kernels are </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">the following:</span></span></p><ul><li><strong class="bold"><span class="koboSpan" id="kobo.579.1">Linear</span></strong><span class="koboSpan" id="kobo.580.1">: This is a commonly used kernel function. </span><span class="koboSpan" id="kobo.580.2">It assumes that the data can be separated by a</span><a id="_idIndexMarker258"/><span class="koboSpan" id="kobo.581.1"> linear decision boundary or hyperplane. </span><span class="koboSpan" id="kobo.581.2">The linear kernel computes the dot product between the input feature vectors, which effectively measures the similarity between the samples in the original </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">feature space.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.583.1">Polynomial</span></strong><span class="koboSpan" id="kobo.584.1">: This can be used to handle non-linearly separable data. </span><span class="koboSpan" id="kobo.584.2">It maps the input</span><a id="_idIndexMarker259"/><span class="koboSpan" id="kobo.585.1"> feature vectors into a higher-dimensional space using polynomial functions, which allows for the learning of non-linear decision boundaries. </span><span class="koboSpan" id="kobo.585.2">This kernel requires setting the degree of the polynomial and the </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">scale factor.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.587.1">Gaussian or RBF</span></strong><span class="koboSpan" id="kobo.588.1">: This can </span><a id="_idIndexMarker260"/><span class="koboSpan" id="kobo.589.1">handle non-linearly separable data by mapping the input feature vectors into an infinite-dimensional feature space. </span><span class="koboSpan" id="kobo.589.2">The RBF kernel is particularly effective when the decision boundary is complex or not well defined. </span><span class="koboSpan" id="kobo.589.3">This kernel requires setting the kernel scale or gamma parameter, which determines the influence of each </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">training example.</span></span></li></ul></li> <li><span class="koboSpan" id="kobo.591.1">Calculating the resubstitution error helps us gauge the performance of the method employed. </span><span class="koboSpan" id="kobo.591.2">As mentioned earlier, this error is determined by measuring the disparity between the predicted and actual responses in the training data. </span><span class="koboSpan" id="kobo.591.3">While it offers an</span><a id="_idIndexMarker261"/><span class="koboSpan" id="kobo.592.1"> initial</span><a id="_idIndexMarker262"/><span class="koboSpan" id="kobo.593.1"> estimate of the model’s performance, it only provides insights in one direction. </span><span class="koboSpan" id="kobo.593.2">A high resubstitution error implies that the predictions made by the tree model may not </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">be accurate:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.595.1">
ResubError = resubLoss(SVMClassifier);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.596.1">The resubstitution error is typically expressed as a percentage or a decimal value between 0 and 1, where 0 represents a perfect fit (no errors) and 1 represents a complete mismatch (all predictions are incorrect). </span><span class="koboSpan" id="kobo.596.2">Let’s see the </span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">results obtained:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.598.1">ResubError = 0.1459</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.599.1">Based on our analysis, we have encountered an error of at least 15%. </span><span class="koboSpan" id="kobo.599.2">This suggests that there is room for improvement in the performance of the classifier. </span><span class="koboSpan" id="kobo.599.3">It is likely that the data cannot be effectively separated using a linear plane, indicating the need to modify the </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">kernel function.</span></span></p></li> <li><span class="koboSpan" id="kobo.601.1">To improve the performance of the model, we can choose an SVM classifier based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">RBF kernel:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.603.1">
RbfSVMClassifier = fitcsvm(DataMatrix(:,1:5),DataMatrix(:,6),'Standardize',true,'KernelFunction','RBF','KernelScale','auto');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.604.1">Now, we can repeat the resubstitution error calculation to compare the performance of the </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">two models:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.606.1">ResubError = resubLoss(RbfSVMClassifier);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.607.1">The following result </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">is returned:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.609.1">ResubError = 0.0061</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.610.1">We initially had an error rate of 15%, but we have successfully reduced it to 0.61%. </span><span class="koboSpan" id="kobo.610.2">This significant improvement indicates that the model is robust in handling </span><a id="_idIndexMarker263"/><span class="koboSpan" id="kobo.611.1">non-linearity</span><a id="_idIndexMarker264"/><span class="koboSpan" id="kobo.612.1"> in </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">the data.</span></span></p></li> </ol>
<p><span class="koboSpan" id="kobo.614.1">Now that we have introduced classification, it is time to explore the world of regression, which allows us to work with continuous </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">numerical values.</span></span></p>
<h1 id="_idParaDest-70"><a id="_idTextAnchor072"/><span class="koboSpan" id="kobo.616.1">Exploring different types of regression</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.617.1">Regression analysis</span></strong><span class="koboSpan" id="kobo.618.1"> is a statistical </span><a id="_idIndexMarker265"/><span class="koboSpan" id="kobo.619.1">method used to examine the connection between a group of independent variables (also known as explanatory variables) and a dependent variable (referred to as the response variable). </span><span class="koboSpan" id="kobo.619.2">By employing this technique, it becomes possible to comprehend how the value of the response variable fluctuates when the explanatory variable </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">is altered.</span></span></p>
<p><span class="koboSpan" id="kobo.621.1">Regression analysis serves a dual purpose: explanatory and predictive. </span><span class="koboSpan" id="kobo.621.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.622.1">explanatory</span></strong><span class="koboSpan" id="kobo.623.1"> role helps us </span><a id="_idIndexMarker266"/><span class="koboSpan" id="kobo.624.1">understand and assess the impact of independent variables on the dependent variable based on a specific theoretical model. </span><span class="koboSpan" id="kobo.624.2">It allows us to quantify the relationship and determine the magnitude and significance of the effects. </span><span class="koboSpan" id="kobo.624.3">In the </span><strong class="bold"><span class="koboSpan" id="kobo.625.1">predictive</span></strong><span class="koboSpan" id="kobo.626.1"> role, regression</span><a id="_idIndexMarker267"/><span class="koboSpan" id="kobo.627.1"> analysis aims to identify the optimal linear combination of independent variables</span><a id="_idIndexMarker268"/><span class="koboSpan" id="kobo.628.1"> to predict the value of the dependent variable accurately. </span><span class="koboSpan" id="kobo.628.2">By utilizing this technique, we can make predictions based on the observed relationships </span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">between variables.</span></span></p>
<h2 id="_idParaDest-71"><a id="_idTextAnchor073"/><span class="koboSpan" id="kobo.630.1">Introducing linear regression</span></h2>
<p><span class="koboSpan" id="kobo.631.1">To characterize the </span><a id="_idIndexMarker269"/><span class="koboSpan" id="kobo.632.1">relationship between variables, we can employ a mathematical function that captures the observed behavior, interpolates the data, and represents its underlying trend while retaining its key information. </span><span class="koboSpan" id="kobo.632.2">Linear regression is a method that specifically aims to identify a line that can effectively represent the distribution of points in a two-dimensional plane. </span><span class="koboSpan" id="kobo.632.3">When the observed points closely align with the line, it indicates that the chosen model accurately describes the relationship between </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">the variables.</span></span></p>
<p><span class="koboSpan" id="kobo.634.1">While there are theoretically infinite lines that can interpolate the observations, in practice, only one mathematical model can optimize the data representation. </span><span class="koboSpan" id="kobo.634.2">In the case of a linear mathematical relationship, the observations of the variable </span><em class="italic"><span class="koboSpan" id="kobo.635.1">y</span></em><span class="koboSpan" id="kobo.636.1"> can be derived from a linear function of the observations of the variable </span><em class="italic"><span class="koboSpan" id="kobo.637.1">x</span></em><span class="koboSpan" id="kobo.638.1">. </span><span class="koboSpan" id="kobo.638.2">For each observation, this relationship can be expressed </span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.640.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.641.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.642.1">α</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.643.1">*</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.644.1">x</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.645.1">+</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.646.1">β</span></span></span></p>
<p><span class="koboSpan" id="kobo.647.1">In the given formula, </span><em class="italic"><span class="koboSpan" id="kobo.648.1">x</span></em><span class="koboSpan" id="kobo.649.1"> represents the explanatory variable, and </span><em class="italic"><span class="koboSpan" id="kobo.650.1">y</span></em><span class="koboSpan" id="kobo.651.1"> represents the response variable. </span><span class="koboSpan" id="kobo.651.2">The parameters </span><em class="italic"><span class="koboSpan" id="kobo.652.1">α</span></em><span class="koboSpan" id="kobo.653.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.654.1">β</span></em><span class="koboSpan" id="kobo.655.1"> correspond to the slope of the line and the intercept with the </span><em class="italic"><span class="koboSpan" id="kobo.656.1">y</span></em><span class="koboSpan" id="kobo.657.1"> axis, respectively. </span><span class="koboSpan" id="kobo.657.2">These parameters need to be estimated using the collected observations for both variables included in </span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.659.1">Of particular interest is the slope </span><em class="italic"><span class="koboSpan" id="kobo.660.1">α</span></em><span class="koboSpan" id="kobo.661.1">, which signifies the change in the average response for each unit increase in the explanatory variable. </span><span class="koboSpan" id="kobo.661.2">What happens when this coefficient changes? </span><span class="koboSpan" id="kobo.661.3">If the slope is positive, the regression line ascends from left to right, indicating that the response increases as the explanatory variable increases. </span><span class="koboSpan" id="kobo.661.4">Conversely, if the slope is negative, the line descends from left to right, suggesting that the response decreases with an increase in the explanatory variable. </span><span class="koboSpan" id="kobo.661.5">When the slope is 0, it implies that the explanatory variable has no effect on the </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">response value.</span></span></p>
<p><span class="koboSpan" id="kobo.663.1">However, the significance of the relationship between the variables is not solely determined by the sign of </span><em class="italic"><span class="koboSpan" id="kobo.664.1">α</span></em><span class="koboSpan" id="kobo.665.1">; its value is also crucial. </span><span class="koboSpan" id="kobo.665.2">In the case of a positive slope, the average response is higher when the explanatory variable is higher. </span><span class="koboSpan" id="kobo.665.3">Conversely, for a negative slope, the average response is lower when the explanatory variable is higher. </span><span class="koboSpan" id="kobo.665.4">The magnitude of the slope plays a vital role in understanding the strength and nature of the relationship between </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">the variables.</span></span></p>
<p><span class="koboSpan" id="kobo.667.1">In MATLAB, you can perform simple linear regression using the built-in functions and tools available in the Statistics and Machine Learning Toolbox. </span><span class="koboSpan" id="kobo.667.2">Another approach to perform simple linear regression in MATLAB is by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.668.1">polyfit()</span></strong><span class="koboSpan" id="kobo.669.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.670.1">polyval()</span></strong><span class="koboSpan" id="kobo.671.1"> functions. </span><span class="koboSpan" id="kobo.671.2">These </span><a id="_idIndexMarker270"/><span class="koboSpan" id="kobo.672.1">functions allow you to fit data to a pattern that is linear in </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">the coefficients.</span></span></p>
<h2 id="_idParaDest-72"><a id="_idTextAnchor074"/><span class="koboSpan" id="kobo.674.1">Linear regression model in MATLAB</span></h2>
<p><span class="koboSpan" id="kobo.675.1">A convenient</span><a id="_idIndexMarker271"/><span class="koboSpan" id="kobo.676.1"> method to construct a linear regression model in </span><a id="_idIndexMarker272"/><span class="koboSpan" id="kobo.677.1">MATLAB is by employing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.678.1">fitlm()</span></strong><span class="koboSpan" id="kobo.679.1"> function, which is a part of the Statistics and Machine Learning Toolbox. </span><span class="koboSpan" id="kobo.679.2">This function offers a straightforward approach to creating and analyzing linear </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">regression models.</span></span></p>
<p><span class="koboSpan" id="kobo.681.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.682.1">fitlm()</span></strong><span class="koboSpan" id="kobo.683.1"> function generates a </span><strong class="source-inline"><span class="koboSpan" id="kobo.684.1">LinearModel</span></strong><span class="koboSpan" id="kobo.685.1"> object in MATLAB. </span><span class="koboSpan" id="kobo.685.2">This object contains various properties that can be easily examined by simply selecting it. </span><span class="koboSpan" id="kobo.685.3">Additionally, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.686.1">LinearModel</span></strong><span class="koboSpan" id="kobo.687.1"> object offers several methods, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.688.1">plot</span></strong><span class="koboSpan" id="kobo.689.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.690.1">plotResiduals</span></strong><span class="koboSpan" id="kobo.691.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.692.1">plotDiagnostics</span></strong><span class="koboSpan" id="kobo.693.1">, which facilitate creating plots and conducting </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">diagnostic analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.695.1">A </span><strong class="source-inline"><span class="koboSpan" id="kobo.696.1">LinearModel</span></strong><span class="koboSpan" id="kobo.697.1"> object encapsulates training data, model description, diagnostic information, and fitted coefficients for a linear regression. </span><span class="koboSpan" id="kobo.697.2">By default, when using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.698.1">fitlm()</span></strong><span class="koboSpan" id="kobo.699.1"> function, the last variable in a table or dataset array is considered the response variable. </span><span class="koboSpan" id="kobo.699.2">Alternatively, you can specify the predictors and response variables explicitly using a formula. </span><span class="koboSpan" id="kobo.699.3">Moreover, you have the flexibility to designate a specific column as the response variable by utilizing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.700.1">ResponseVar</span></strong><span class="koboSpan" id="kobo.701.1"> name-value pair argument. </span><span class="koboSpan" id="kobo.701.2">To define a set of columns as predictors, you can employ the </span><strong class="source-inline"><span class="koboSpan" id="kobo.702.1">PredictorVars</span></strong><span class="koboSpan" id="kobo.703.1"> name-value pair argument. </span><span class="koboSpan" id="kobo.703.2">The predictor variables can be numeric or of any grouping variable type, such as logical or categorical. </span><span class="koboSpan" id="kobo.703.3">However, the response variable must be numeric or logical </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">in nature.</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.705.1">To gain insight into the functionality of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.706.1">fitlm()</span></strong><span class="koboSpan" id="kobo.707.1"> function, let’s consider a dataset that includes information on the number of vehicles registered in Italy and the population of various regions. </span><span class="koboSpan" id="kobo.707.2">The dataset includes the </span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">following fields:</span></span><ul><li><span class="koboSpan" id="kobo.709.1">Names of Italian </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">regions (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.711.1">Region</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.712.1">)</span></span></li><li><span class="koboSpan" id="kobo.713.1">Vehicle registrations for each </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">region (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.715.1">Registrations</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.716.1">)</span></span></li><li><span class="koboSpan" id="kobo.717.1">Resident population in each </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">region (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.719.1">Population</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.720.1">)</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.721.1">Let’s start by importing the data into </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">a table:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.723.1">
VehicleData = readtable('VehiclesItaly.xlsx');</span></pre></li> <li><span class="koboSpan" id="kobo.724.1">To fit a linear regression model with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.725.1">Registrations</span></strong><span class="koboSpan" id="kobo.726.1"> variable as the response and </span><strong class="source-inline"><span class="koboSpan" id="kobo.727.1">Population</span></strong><span class="koboSpan" id="kobo.728.1"> as the explanatory variable (predictor), we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.729.1">fitlm()</span></strong><span class="koboSpan" id="kobo.730.1"> function </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.732.1">
LRModel = fitlm(VehicleData,'Registrations~Population');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.733.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.734.1">disp()</span></strong><span class="koboSpan" id="kobo.735.1"> command</span><a id="_idIndexMarker273"/><span class="koboSpan" id="kobo.736.1"> displays the model summary, providing </span><a id="_idIndexMarker274"/><span class="koboSpan" id="kobo.737.1">information about the coefficients, p-values, R-squared value, and other </span><span class="No-Break"><span class="koboSpan" id="kobo.738.1">statistical metrics:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.739.1">disp(LRModel)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.740.1">The following information </span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">is printed:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.742.1">Linear regression model:
    Registrations ~ 1 + Population
Estimated Coefficients:
           Estimate       SE       tStat       pValue
          ________    ________    ______    __________
(Intercept) 70549        41016      1.72       0.10258
Population 0.59212    0.010488    56.458    1.0323e-21
Number of observations: 20, Error degrees of freedom: 18
Root Mean Squared Error: 1.16e+05
R-squared: 0.994, Adjusted R-Squared: 0.994
F-statistic vs. </span><span class="koboSpan" id="kobo.742.2">constant model: 3.19e+03, p-value = 1.03e-21</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.743.1">The results of the linear regression model include the model formula and estimated coefficients. </span><span class="koboSpan" id="kobo.743.2">Each term in the model is represented by a row, and the following columns provide </span><span class="No-Break"><span class="koboSpan" id="kobo.744.1">additional information:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.745.1">Estimate</span></strong><span class="koboSpan" id="kobo.746.1">: The estimated coefficient value for each corresponding term in </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">the model.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.748.1">SE</span></strong><span class="koboSpan" id="kobo.749.1">: The</span><a id="_idIndexMarker275"/><span class="koboSpan" id="kobo.750.1"> standard </span><a id="_idIndexMarker276"/><span class="koboSpan" id="kobo.751.1">error of </span><span class="No-Break"><span class="koboSpan" id="kobo.752.1">the estimate.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.753.1">tStat</span></strong><span class="koboSpan" id="kobo.754.1">: The t-statistic for each coefficient, which tests the null hypothesis that the coefficient is 0 against the alternative that it is different from 0, given the other predictors in </span><span class="No-Break"><span class="koboSpan" id="kobo.755.1">the model.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.756.1">pValue</span></strong><span class="koboSpan" id="kobo.757.1">: The p-value for the F-statistic of the hypothesis test that the coefficient is equal to 0. </span><span class="koboSpan" id="kobo.757.2">In our example, a p-value lower than 0.05 indicates that the term is significant at the 5% significance level, given the other terms in </span><span class="No-Break"><span class="koboSpan" id="kobo.758.1">the model.</span></span></li><li><span class="koboSpan" id="kobo.759.1">Interpretation of the intercept adds more meaning to the problem statement, as an average 70,549 registrations are happening in </span><span class="No-Break"><span class="koboSpan" id="kobo.760.1">the city.</span></span></li><li><span class="koboSpan" id="kobo.761.1">We can also explain the slope (0.59212) as with every 1,000-person increase in population, registrations increase by 592, with approximately 6 out of every 10 people </span><span class="No-Break"><span class="koboSpan" id="kobo.762.1">having vehicles.</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.763.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.764.1">t-test</span></strong><span class="koboSpan" id="kobo.765.1"> is a parametric</span><a id="_idIndexMarker277"/><span class="koboSpan" id="kobo.766.1"> test used to determine whether the average value of a distribution significantly differs from a reference value. </span><span class="koboSpan" id="kobo.766.2">In addition, the following information is provided about the </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">created model:</span></span></p><ul><li><strong class="bold"><span class="koboSpan" id="kobo.768.1">Number of observations</span></strong><span class="koboSpan" id="kobo.769.1">: The number of records in the data without any NaN values. </span><span class="koboSpan" id="kobo.769.2">In our example, there are </span><span class="No-Break"><span class="koboSpan" id="kobo.770.1">20 observations.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.771.1">Error degrees of freedom</span></strong><span class="koboSpan" id="kobo.772.1">: The number of observations minus the number of estimated coefficients, stored as a positive integer value. </span><span class="koboSpan" id="kobo.772.2">In our example, with two predictors, the error degrees of freedom </span><span class="No-Break"><span class="koboSpan" id="kobo.773.1">are 18.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.774.1">Root mean squared error (RMSE)</span></strong><span class="koboSpan" id="kobo.775.1">: The</span><a id="_idIndexMarker278"/><span class="koboSpan" id="kobo.776.1"> root mean squared error of the residuals, representing the square root of the mean </span><span class="No-Break"><span class="koboSpan" id="kobo.777.1">squared error.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.778.1">R-squared</span></strong><span class="koboSpan" id="kobo.779.1">: The proportion of the total sum of squares explained by the model, also known </span><a id="_idIndexMarker279"/><span class="koboSpan" id="kobo.780.1">as the coefficient of determination. </span><span class="koboSpan" id="kobo.780.2">In our example, the high R-squared value suggests that the model explains approximately 99% of the variability in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.781.1">Registrations</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.782.1">response variable.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.783.1">Adjusted R-squared</span></strong><span class="koboSpan" id="kobo.784.1">: A modified</span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.785.1"> version of R-squared that takes into account the number of predictors in </span><span class="No-Break"><span class="koboSpan" id="kobo.786.1">the model.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.787.1">F-statistic versus constant model</span></strong><span class="koboSpan" id="kobo.788.1">: The </span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.789.1">test statistic for the F-test on the regression model, which tests for a significant linear regression relationship between the response variable and the </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">predictor variables.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.791.1">p-value</span></strong><span class="koboSpan" id="kobo.792.1">: The p-value for </span><a id="_idIndexMarker282"/><span class="koboSpan" id="kobo.793.1">the F-test on the model. </span><span class="koboSpan" id="kobo.793.2">In our example, the model is significant with a very low p-value </span><span class="No-Break"><span class="koboSpan" id="kobo.794.1">of 1.03e-21.</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.795.1">Let’s examine the</span><a id="_idIndexMarker283"/><span class="koboSpan" id="kobo.796.1"> results</span><a id="_idIndexMarker284"/><span class="koboSpan" id="kobo.797.1"> of the last MATLAB command. </span><span class="koboSpan" id="kobo.797.2">Two values stand out among the others: R-squared and p-value. </span><span class="koboSpan" id="kobo.797.3">The calculated R-squared value is very high, equal to 0.994. </span><span class="koboSpan" id="kobo.797.4">This indicates that a substantial variation in the response variable can be explained by the predictor variable. </span><span class="koboSpan" id="kobo.797.5">On the other hand, the p-value is very small, but understanding its significance requires further exploration. </span><span class="koboSpan" id="kobo.797.6">During a statistical significance test, we start by assuming the </span><strong class="bold"><span class="koboSpan" id="kobo.798.1">null hypothesis</span></strong><span class="koboSpan" id="kobo.799.1">, which</span><a id="_idIndexMarker285"/><span class="koboSpan" id="kobo.800.1"> states that there is no difference between the groups with respect to the parameters being considered. </span><span class="koboSpan" id="kobo.800.2">Under the null hypothesis, any observed differences are attributed </span><span class="No-Break"><span class="koboSpan" id="kobo.801.1">to chance.</span></span></p></li> </ol>
<p><span class="koboSpan" id="kobo.802.1">Now we must make a decision, whether to accept or reject the null hypothesis. </span><span class="koboSpan" id="kobo.802.2">To make this determination, we analyze our data using a significance test. </span><span class="koboSpan" id="kobo.802.3">If the test </span><em class="italic"><span class="koboSpan" id="kobo.803.1">suggests</span></em><span class="koboSpan" id="kobo.804.1"> rejecting the null hypothesis, we declare the observed difference as statistically significant. </span><span class="koboSpan" id="kobo.804.2">Conversely, if the test </span><em class="italic"><span class="koboSpan" id="kobo.805.1">advises</span></em><span class="koboSpan" id="kobo.806.1"> accepting the null hypothesis, the difference is deemed statistically not significant. </span><span class="koboSpan" id="kobo.806.2">The results of a statistical test always come with a certain level of uncertainty and probability. </span><span class="koboSpan" id="kobo.806.3">Thus, a decision to reject the null hypothesis is likely correct but may also be incorrect. </span><span class="koboSpan" id="kobo.806.4">Assessing the risk of making an error is known as the </span><strong class="bold"><span class="koboSpan" id="kobo.807.1">significance level</span></strong><span class="koboSpan" id="kobo.808.1"> of</span><a id="_idIndexMarker286"/> <span class="No-Break"><span class="koboSpan" id="kobo.809.1">the test.</span></span></p>
<p><span class="koboSpan" id="kobo.810.1">This significance level, also known as the p-value, provides a quantitative estimate of the probability that the observed differences are due to chance. </span><span class="koboSpan" id="kobo.810.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.811.1">p-value</span></strong><span class="koboSpan" id="kobo.812.1"> is a probability</span><a id="_idIndexMarker287"/><span class="koboSpan" id="kobo.813.1"> and can only assume values between 0 and 1. </span><span class="koboSpan" id="kobo.813.2">A p-value approaching 0 indicates a low probability that the observed difference is due to chance. </span><span class="koboSpan" id="kobo.813.3">Researchers typically choose a significance level of 0.05 (5%) or 0.01 (1%). </span><span class="koboSpan" id="kobo.813.4">In our case, we calculated a p-value of 1.03e-21, which is far below the chosen significance level. </span><span class="koboSpan" id="kobo.813.5">This suggests that the observed difference is statistically significant and not likely due </span><span class="No-Break"><span class="koboSpan" id="kobo.814.1">to chance.</span></span></p>
<p><span class="koboSpan" id="kobo.815.1">After having seen the </span><a id="_idIndexMarker288"/><span class="koboSpan" id="kobo.816.1">first </span><a id="_idIndexMarker289"/><span class="koboSpan" id="kobo.817.1">example of linear regression that deals with continuous predictors, it is necessary to see how to deal with the case in which the type of predictors </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">is different.</span></span></p>
<h1 id="_idParaDest-73"><a id="_idTextAnchor075"/><span class="koboSpan" id="kobo.819.1">Making predictions with regression analysis in MATLAB</span></h1>
<p><span class="koboSpan" id="kobo.820.1">Having explored</span><a id="_idIndexMarker290"/><span class="koboSpan" id="kobo.821.1"> numerous instances of </span><a id="_idIndexMarker291"/><span class="koboSpan" id="kobo.822.1">linear regression, we can confidently assert that we comprehend the underlying mechanisms of this statistical method. </span><span class="koboSpan" id="kobo.822.2">Non-linear regression is used to model the relationship between a dependent variable and one or more independent variables when the relationship is not linear. </span><span class="koboSpan" id="kobo.822.3">In contrast to linear regression, where the relationship is assumed to be a straight line, non-linear regression allows for more complex and flexible relationships </span><span class="No-Break"><span class="koboSpan" id="kobo.823.1">between variables.</span></span></p>
<p><span class="koboSpan" id="kobo.824.1">Up until now, we have exclusively employed continuous variables as predictors. </span><span class="koboSpan" id="kobo.824.2">However, what transpires when the predictors are categorical variables? </span><span class="koboSpan" id="kobo.824.3">No need to fret, as the fundamental </span><a id="_idIndexMarker292"/><span class="koboSpan" id="kobo.825.1">principles </span><a id="_idIndexMarker293"/><span class="koboSpan" id="kobo.826.1">of regression techniques </span><span class="No-Break"><span class="koboSpan" id="kobo.827.1">remain unchanged.</span></span></p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.828.1">Multiple linear regression with categorical predictor</span></h2>
<p><span class="koboSpan" id="kobo.829.1">Categorical variables </span><a id="_idIndexMarker294"/><span class="koboSpan" id="kobo.830.1">differ from numerical</span><a id="_idIndexMarker295"/><span class="koboSpan" id="kobo.831.1"> ones as they do not stem from measurement operations but rather from classification and comparison operations. </span><span class="koboSpan" id="kobo.831.2">Categorical variables can be categorized into nominal, dichotomous, or </span><span class="No-Break"><span class="koboSpan" id="kobo.832.1">ordinal groups.</span></span></p>
<p><span class="koboSpan" id="kobo.833.1">Now, let’s delve into a real-life scenario. </span><span class="koboSpan" id="kobo.833.2">Within a company, we have diligently gathered information on employee salaries, which are determined by their years of experience. </span><span class="koboSpan" id="kobo.833.3">Our objective is to construct a model that enables us to track the progression of an employee’s salary over time. </span><span class="koboSpan" id="kobo.833.4">We have categorized the employees into three types: Management, Technical Staff, and </span><span class="No-Break"><span class="koboSpan" id="kobo.834.1">General Staff.</span></span></p>
<p><span class="koboSpan" id="kobo.835.1">Let's now see practically how to process the </span><span class="No-Break"><span class="koboSpan" id="kobo.836.1">MATLAB code:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.837.1">To begin our analysis, we will import the relevant data from an Excel worksheet named </span><strong class="source-inline"><span class="koboSpan" id="kobo.838.1">employees.xlsx</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.839.1">into MATLAB:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.840.1">
EmployeesSalary = readtable('employees.xlsx');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.841.1">We now have a table containing three variables: </span><strong class="source-inline"><span class="koboSpan" id="kobo.842.1">YearsExperience</span></strong><span class="koboSpan" id="kobo.843.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.844.1">Salary</span></strong><span class="koboSpan" id="kobo.845.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.846.1">LevelOfEmployee</span></strong><span class="koboSpan" id="kobo.847.1">. </span><span class="koboSpan" id="kobo.847.2">The nature and meaning of these variables are </span><span class="No-Break"><span class="koboSpan" id="kobo.848.1">readily apparent.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.849.1">Let’s examine the characteristics of </span><span class="No-Break"><span class="koboSpan" id="kobo.850.1">these variables:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.851.1">summary(EmployeesSalary)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.852.1">The following results </span><span class="No-Break"><span class="koboSpan" id="kobo.853.1">are printed:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.854.1">Variables:
    YearsExperience: 120×1 double
        Values:
            Min            1
            Median      20.5
            Max           40
    Salary: 120×1 double
        Values:
            Min          20
            Median       41
            Max          82
    LevelOfEmployee: 120×1 cell array of character vectors</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.855.1">The first two variables, </span><strong class="source-inline"><span class="koboSpan" id="kobo.856.1">YearsExperience</span></strong><span class="koboSpan" id="kobo.857.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.858.1">Salary</span></strong><span class="koboSpan" id="kobo.859.1">, are of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.860.1">double</span></strong><span class="koboSpan" id="kobo.861.1"> data type, representing numerical values. </span><span class="koboSpan" id="kobo.861.2">However, the third variable, </span><strong class="source-inline"><span class="koboSpan" id="kobo.862.1">LevelOfEmployee</span></strong><span class="koboSpan" id="kobo.863.1">, consists of character vectors. </span><span class="koboSpan" id="kobo.863.2">It is evident that this variable represents a categorical variable since it contains the three types of employees mentioned earlier. </span><span class="koboSpan" id="kobo.863.3">Nevertheless, MATLAB is unaware of this categorization, and it</span><a id="_idIndexMarker296"/><span class="koboSpan" id="kobo.864.1"> is</span><a id="_idIndexMarker297"/><span class="koboSpan" id="kobo.865.1"> our responsibility to inform it. </span><span class="koboSpan" id="kobo.865.2">In practice, we need to transform this variable into a </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">categorical type:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.867.1">EmployeesSalary.LevelOfEmployee=categorical(EmployeesSalary.LevelOfEmployee);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.868.1">Now we can check the type of </span><span class="No-Break"><span class="koboSpan" id="kobo.869.1">the variable:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.870.1">class(EmployeesSalary.LevelOfEmployee)
ans =
    'categorical'</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.871.1">So, the variable is corrected and recognized </span><span class="No-Break"><span class="koboSpan" id="kobo.872.1">by MATLAB.</span></span></p></li> <li><span class="koboSpan" id="kobo.873.1">Now, it is time to create the regression model by applying the </span><strong class="source-inline"><span class="koboSpan" id="kobo.874.1">fitlm()</span></strong><span class="koboSpan" id="kobo.875.1"> function using </span><strong class="source-inline"><span class="koboSpan" id="kobo.876.1">Salary</span></strong><span class="koboSpan" id="kobo.877.1"> as the dependent variable, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.878.1">YearsExperience</span></strong><span class="koboSpan" id="kobo.879.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.880.1">LevelOfEmployee</span></strong><span class="koboSpan" id="kobo.881.1"> as the independent variables. </span><span class="koboSpan" id="kobo.881.2">Given that </span><strong class="source-inline"><span class="koboSpan" id="kobo.882.1">LevelOfEmployee</span></strong><span class="koboSpan" id="kobo.883.1"> is a categorical variable with three levels (</span><strong class="source-inline"><span class="koboSpan" id="kobo.884.1">Management</span></strong><span class="koboSpan" id="kobo.885.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.886.1">TechnicalStaff</span></strong><span class="koboSpan" id="kobo.887.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.888.1">GeneralStaff</span></strong><span class="koboSpan" id="kobo.889.1">), it is represented in the model as two indicator variables. </span><span class="koboSpan" id="kobo.889.2">In MATLAB, categorical predictors are typically included as dummy indicator variables. </span><span class="koboSpan" id="kobo.889.3">An indicator variable takes on values of 0 or 1. </span><span class="koboSpan" id="kobo.889.4">For a categorical variable with </span><em class="italic"><span class="koboSpan" id="kobo.890.1">n</span></em><span class="koboSpan" id="kobo.891.1"> categories, it can be represented by </span><em class="italic"><span class="koboSpan" id="kobo.892.1">n – 1</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.893.1">indicator variables.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.894.1">To account for the distinctions between the different employee types, we can incorporate interaction terms between </span><strong class="source-inline"><span class="koboSpan" id="kobo.895.1">YearsExperience</span></strong><span class="koboSpan" id="kobo.896.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.897.1">LevelOfEmployee</span></strong><span class="koboSpan" id="kobo.898.1">. </span><span class="koboSpan" id="kobo.898.2">This allows us to capture the interaction effects between years of experience and</span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.899.1"> the </span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.900.1">specific </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">employee category:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.902.1">
LRModelCat = fitlm(EmployeesSalary,'Salary~YearsExperience*LevelOfEmployee');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.903.1">Let’s see the characteristics of </span><span class="No-Break"><span class="koboSpan" id="kobo.904.1">the model:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer028">
<span class="koboSpan" id="kobo.905.1"><img alt="Figure 3.4 – Regression model summary" src="image/B21156_03_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.906.1">Figure 3.4 – Regression model summary</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.907.1">Based on the results, the model equation is </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.909.1">
Salary =
  20.2
+ 0.25 * YearsExperience
+ 30.2 * LevelOfEmployee(Management)
+ 10.4 * LevelOfEmployee(TechnicalStaff)
+ 0.49 * YearsExperience * LevelOfEmployee(Management)
+ 0.24 * YearsExperience * LevelOfEmployee(TechnicalStaff)</span></pre> <p class="list-inset"><span class="koboSpan" id="kobo.910.1">In this equation, the term </span><strong class="source-inline"><span class="koboSpan" id="kobo.911.1">LevelOfEmployee(GeneralStaff)</span></strong><span class="koboSpan" id="kobo.912.1"> is not included because the first level, by default, serves as the reference group. </span><span class="koboSpan" id="kobo.912.2">However, the first-order terms for </span><strong class="source-inline"><span class="koboSpan" id="kobo.913.1">YearsExperience</span></strong><span class="koboSpan" id="kobo.914.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.915.1">LevelOfEmployee</span></strong><span class="koboSpan" id="kobo.916.1">, along with all the interactions, </span><span class="No-Break"><span class="koboSpan" id="kobo.917.1">are present.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.918.1">It is apparent that a </span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.919.1">single </span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.920.1">equation for the entire system is insufficient to obtain accurate wage estimates. </span><span class="koboSpan" id="kobo.920.2">To address this, we need to differentiate between the three categories of employees and create separate models for each. </span><span class="koboSpan" id="kobo.920.3">Consequently, we obtain the following three equations to capture the wage dynamics for each </span><span class="No-Break"><span class="koboSpan" id="kobo.921.1">employee category:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.922.1">
LevelOfEmployee(GeneralStaff):
Salary = 20.2 + 0.25 * YearsExperience
LevelOfEmployee(TechnicalStaff):
Salary = (20.2 + 10.4) + (0.25 + 0.24) * YearsExperience
LevelOfEmployee(Management):
Salary = (20.2 + 30.2) + (0.25 + 0.49) * YearsExperience</span></pre> <p class="list-inset"><span class="koboSpan" id="kobo.923.1">From a simple comparison between the three equations we have just seen, we can see that the intercept progressively increases with the category of employees (</span><strong class="source-inline"><span class="koboSpan" id="kobo.924.1">20.2;20.2 + 10.4; 20.2 + 30.2</span></strong><span class="koboSpan" id="kobo.925.1">); this shifts the regression line upward. </span><span class="koboSpan" id="kobo.925.2">Similarly, the slope also progressively increases with the employee category (</span><strong class="source-inline"><span class="koboSpan" id="kobo.926.1">0.25 ; 0.25 + 0.24; 0.25 + 0.49</span></strong><span class="koboSpan" id="kobo.927.1">), leading to a greater increase in salary as the number of years of </span><span class="No-Break"><span class="koboSpan" id="kobo.928.1">experience increases.</span></span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.929.1">To enhance our understanding of the progress made thus far, let’s incorporate the following lines into the scatter plot of </span><span class="No-Break"><span class="koboSpan" id="kobo.930.1">the data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.931.1">
Xvalues = linspace(min(EmployeesSalary.YearsExperience),max(EmployeesSalary.YearsExperience));
figure()
gscatter(EmployeesSalary.YearsExperience, EmployeesSalary.Salary, EmployeesSalary.LevelOfEmployee,'bgr','x.o')
title('Salary of  Employees versus Years of the Experience, Grouped by Level of Employee')
line(Xvalues,feval(LRModelCat,Xvalues,'GeneralStaff'),'Color','b','LineWidth',2)
line(Xvalues,feval(LRModelCat,Xvalues,'TechnicalStaff'),'Color','r','LineWidth',2)
line(Xvalues,feval(LRModelCat,Xvalues,'Management'),'Color','g','LineWidth',2)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.932.1">The results</span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.933.1"> are </span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.934.1">shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.935.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.936.1">.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.938.1">In the previously suggested code, we first created a linearly spaced vector between the minimum and maximum values of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.939.1">YearsExperience</span></strong><span class="koboSpan" id="kobo.940.1"> variable using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.941.1">linspace()</span></strong><span class="koboSpan" id="kobo.942.1"> function. </span><span class="koboSpan" id="kobo.942.2">Subsequently, we generated a scatter plot depicting the relationship between employee salaries (</span><strong class="source-inline"><span class="koboSpan" id="kobo.943.1">Salary</span></strong><span class="koboSpan" id="kobo.944.1">) and years of experience (</span><strong class="source-inline"><span class="koboSpan" id="kobo.945.1">YearsExperience</span></strong><span class="koboSpan" id="kobo.946.1">), with the data points grouped by the level of </span><span class="No-Break"><span class="koboSpan" id="kobo.947.1">employee (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.948.1">LevelOfEmployee</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.949.1">).</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer029">
<span class="koboSpan" id="kobo.950.1"><img alt="Figure 3.5 – Scatter plot with the three straight lines that fit three data groups" src="image/B21156_03_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.951.1">Figure 3.5 – Scatter plot with the three straight lines that fit three data groups</span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.952.1">Finally, we added the three lines representing the respective trends for each employee category. </span><span class="koboSpan" id="kobo.952.2">This was accomplished by utilizing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.953.1">feval()</span></strong><span class="koboSpan" id="kobo.954.1"> function to evaluate the model at the specified points in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.955.1">Xvalues</span></strong><span class="koboSpan" id="kobo.956.1"> variable. </span><span class="koboSpan" id="kobo.956.2">It is now evident that the three straight lines, corresponding to the three equations, are distinguished by</span><a id="_idIndexMarker304"/><span class="koboSpan" id="kobo.957.1"> both</span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.958.1"> their intercepts </span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">and slopes.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.960.1">After having analyzed two regression examples in detail, we can see how to evaluate the performance of the models created </span><span class="No-Break"><span class="koboSpan" id="kobo.961.1">so far.</span></span></p>
<h1 id="_idParaDest-75"><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.962.1">Evaluating model performance</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.963.1">Model performance</span></strong><span class="koboSpan" id="kobo.964.1"> refers to </span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.965.1">how well a model fits the given data and accurately predicts outcomes. </span><span class="koboSpan" id="kobo.965.2">It is important to evaluate model performance to assess its reliability and effectiveness in making predictions or in capturing the underlying patterns in the data. </span><span class="koboSpan" id="kobo.965.3">One commonly used metric to evaluate model performance is the R-squared value, also known as the coefficient of determination. </span><span class="koboSpan" id="kobo.965.4">R-squared measures the proportion of the variance in the dependent variable that can be explained by the independent variables in the model. </span><span class="koboSpan" id="kobo.965.5">A higher R-squared value indicates a better fit, as it means a larger proportion of the variability in the data is accounted for by </span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.967.1">However, R-squared alone may not provide a complete picture of model performance. </span><span class="koboSpan" id="kobo.967.2">Other metrics, such</span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.968.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.969.1">mean squared error</span></strong><span class="koboSpan" id="kobo.970.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.971.1">MSE</span></strong><span class="koboSpan" id="kobo.972.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.973.1">mean absolute error</span></strong><span class="koboSpan" id="kobo.974.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.975.1">MAE</span></strong><span class="koboSpan" id="kobo.976.1">), can be </span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.977.1">used to assess the average prediction error of the model. </span><span class="koboSpan" id="kobo.977.2">Lower values of MSE or MAE indicate better predictive performance. </span><span class="koboSpan" id="kobo.977.3">Furthermore, it is important to consider the context and specific requirements of the problem at hand. </span><span class="koboSpan" id="kobo.977.4">For example, if the residuals (the differences between the predicted and actual values) exhibit certain patterns or non-random behavior, it may indicate that the model is missing important factors or </span><span class="No-Break"><span class="koboSpan" id="kobo.978.1">violating assumptions.</span></span></p>
<p><span class="koboSpan" id="kobo.979.1">Additionally, cross-validation techniques can be used to further evaluate the model’s performance by assessing its ability to generalize to new, unseen data. </span><span class="koboSpan" id="kobo.979.2">This helps to ensure that the model is not overfitting the training data and can make accurate predictions on new observations. </span><span class="koboSpan" id="kobo.979.3">In summary, model performance is assessed through various metrics, including R-squared, MSE, MAE, and consideration of the residuals and cross-validation results. </span><span class="koboSpan" id="kobo.979.4">Evaluating these aspects provides a comprehensive understanding of how well the model performs and its suitability for the </span><span class="No-Break"><span class="koboSpan" id="kobo.980.1">given task.</span></span></p>
<p><span class="koboSpan" id="kobo.981.1">In the next example, we will exploit the evaluation metrics to improve the performance of the model by identifying </span><span class="No-Break"><span class="koboSpan" id="kobo.982.1">the outliers.</span></span></p>
<h2 id="_idParaDest-76"><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.983.1">Reducing outlier effects</span></h2>
<p><span class="koboSpan" id="kobo.984.1">As mentioned </span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.985.1">earlier, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.986.1">fitlm()</span></strong><span class="koboSpan" id="kobo.987.1"> function generates a </span><strong class="source-inline"><span class="koboSpan" id="kobo.988.1">LinearModel</span></strong><span class="koboSpan" id="kobo.989.1"> object that contains valuable information about the linear regression, including training data, model description, diagnostic details, and estimated coefficients. </span><span class="koboSpan" id="kobo.989.2">Now, we can utilize some of these properties to gain additional insights from the model. </span><span class="koboSpan" id="kobo.989.3">The least-squares method is commonly employed when we have sufficient knowledge about the model’s shape and aim to determine its parameters. </span><span class="koboSpan" id="kobo.989.4">It is also useful for </span><span class="No-Break"><span class="koboSpan" id="kobo.990.1">model exploration.</span></span></p>
<p><span class="koboSpan" id="kobo.991.1">However, this method requires manual inspection of the data to identify and handle outliers. </span><span class="koboSpan" id="kobo.991.2">Therefore, we will now examine whether there are any outliers in the data that should be excluded from the fitting process. </span><span class="koboSpan" id="kobo.991.3">Residual plots can aid us in this analysis. </span><span class="koboSpan" id="kobo.991.4">The most used plots include the default histogram plot, which displays the range and frequencies of the residuals, and the probability plot, which compares the distribution of the residuals to a normal distribution with a </span><span class="No-Break"><span class="koboSpan" id="kobo.992.1">similar variance:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.993.1">We start as usual by importing </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">the data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.995.1">
VehicleData = readtable('VehiclesItaly.xlsx');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.996.1">Then we build</span><a id="_idIndexMarker310"/> <span class="No-Break"><span class="koboSpan" id="kobo.997.1">the model:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.998.1">LRModel = fitlm(VehicleData,'Registrations~Population');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.999.1">Now, we can work on the evaluation of </span><span class="No-Break"><span class="koboSpan" id="kobo.1000.1">the model.</span></span></p></li> <li><span class="koboSpan" id="kobo.1001.1">To start, we extract some different evaluation metrics of </span><span class="No-Break"><span class="koboSpan" id="kobo.1002.1">the model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1003.1">
LRModel.Rsquared</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1004.1">The following results </span><span class="No-Break"><span class="koboSpan" id="kobo.1005.1">are returned:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1006.1">Ordinary: 0.9944
Adjusted: 0.9941</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1007.1">Let’s extract </span><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">another metric:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1009.1">LRModel.MSE</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1010.1">Let’s see the </span><span class="No-Break"><span class="koboSpan" id="kobo.1011.1">value obtained:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1012.1">1.3407e+10</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1013.1">MSE quantifies the mean squared disparity between the model’s predicted values and the actual observed values. </span><span class="koboSpan" id="kobo.1013.2">The MSE is calculated by taking the average of the squared differences between the predicted values and the true values over the </span><span class="No-Break"><span class="koboSpan" id="kobo.1014.1">entire dataset.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1015.1">The R-squared and MSE values obtained will be used to compare the performance of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1016.1">different models.</span></span></p></li> <li><span class="koboSpan" id="kobo.1017.1">To show how residual plots are useful for identifying outliers, we will refer to the example used in the </span><em class="italic"><span class="koboSpan" id="kobo.1018.1">Linear regression model in MATLAB</span></em><span class="koboSpan" id="kobo.1019.1"> section. </span><span class="koboSpan" id="kobo.1019.2">We will use the same model (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1020.1">LRModel</span></strong><span class="koboSpan" id="kobo.1021.1">). </span><span class="koboSpan" id="kobo.1021.2">To visualize the residuals, we will utilize a specific property of the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1022.1">LinearModel</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1023.1"> object:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1024.1">
plotResiduals(LRModel)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1025.1">Upon analyzing the histogram obtained, a distinct asymmetry is evident in the negative values. </span><span class="koboSpan" id="kobo.1025.2">Specifically, the observations near </span><strong class="source-inline"><span class="koboSpan" id="kobo.1026.1">-2 *105</span></strong><span class="koboSpan" id="kobo.1027.1"> appear to be potential outliers. </span><span class="koboSpan" id="kobo.1027.2">To achieve a more accurate fit, we will construct a probability plot. </span><span class="koboSpan" id="kobo.1027.3">As previously </span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.1028.1">mentioned, the plot in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1029.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1030.1">.6</span></em><span class="koboSpan" id="kobo.1031.1"> demonstrates the comparison between the distribution of residuals and a normal distribution with </span><span class="No-Break"><span class="koboSpan" id="kobo.1032.1">similar variance.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1033.1">The probability plot of residuals also reveals potential outliers, particularly in the lower left region. </span><span class="koboSpan" id="kobo.1033.2">We can observe three values that significantly deviate from the dotted line. </span><span class="koboSpan" id="kobo.1033.3">On the other hand, the probability plot appears reasonably linear for the remaining residual values, indicating a reasonable fit to normally </span><span class="No-Break"><span class="koboSpan" id="kobo.1034.1">distributed residuals.</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer030">
<span class="koboSpan" id="kobo.1035.1"><img alt="Figure 3.6 – Histogram of residuals and probability plot of residuals for the linear regression model" src="image/B21156_03_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1036.1">Figure 3.6 – Histogram of residuals and probability plot of residuals for the linear regression model</span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.1037.1">We can identify these</span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.1038.1"> outliers and proceed to remove them from the dataset. </span><span class="koboSpan" id="kobo.1038.2">To locate them, we can utilize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1039.1">find()</span></strong><span class="koboSpan" id="kobo.1040.1"> function, examining values to the left of the abscissa equal </span><span class="No-Break"><span class="koboSpan" id="kobo.1041.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1042.1">-1.5*105</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1043.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1044.1">
outliers = find(LRModel.Residuals.Raw &lt; -1.5*10^5)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1045.1">The following results </span><span class="No-Break"><span class="koboSpan" id="kobo.1046.1">are returned:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1047.1">outliers =
     9
    13
    18</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1048.1">Currently, we have the outliers detected in </span><span class="No-Break"><span class="koboSpan" id="kobo.1049.1">the data.</span></span></p></li> <li><span class="koboSpan" id="kobo.1050.1">Now, at this stage, we can proceed to create the model once again, this time excluding the aforementioned </span><span class="No-Break"><span class="koboSpan" id="kobo.1051.1">outlier values:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1052.1">
LRModel2 = fitlm(VehicleData,'Registrations~Population','Exclude',outliers)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1053.1">Now we can extract the evaluation metrics to compare </span><span class="No-Break"><span class="koboSpan" id="kobo.1054.1">the models:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1055.1">LRModel2.Rsquared</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1056.1">The following values </span><span class="No-Break"><span class="koboSpan" id="kobo.1057.1">are printed:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1058.1">Ordinary: 0.9974
Adjusted: 0.9972</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1059.1">The model’s performance has noticeably improved, as indicated by the increased R-squared value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1060.1">0.997</span></strong><span class="koboSpan" id="kobo.1061.1"> compared to the previous model’s R-squared value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1062.1">0.994</span></strong><span class="koboSpan" id="kobo.1063.1">. </span><span class="koboSpan" id="kobo.1063.2">This improvement suggests that the new model provides a better fit to the data. </span><span class="koboSpan" id="kobo.1063.3">Now we will extract </span><span class="No-Break"><span class="koboSpan" id="kobo.1064.1">the MSE:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1065.1">LRModel2.MSE
ans =
   6.8047e+09</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1066.1">From a comparison, we can see an improvement; the MSE decreased from </span><strong class="source-inline"><span class="koboSpan" id="kobo.1067.1">1.3407e+10</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1068.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1069.1">6.8047e+09</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1070.1">.</span></span></p></li> </ol>
<p><span class="koboSpan" id="kobo.1071.1">To further evaluate the model’s performance, we can assess other metrics, such as the MSE or MAE. </span><span class="koboSpan" id="kobo.1071.2">Additionally, conducting cross-validation or assessing the residuals’ distribution and patterns can provide further insights into the </span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">model’s effectiveness.</span></span></p>
<p><span class="koboSpan" id="kobo.1073.1">Having discussed the </span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.1074.1">utilization of evaluation metrics to enhance model performance, it is now opportune to delve into these methodologies and strive toward constructing increasingly effective models. </span><span class="koboSpan" id="kobo.1074.2">By employing these techniques, we aim to improve the overall performance and predictive capabilities of </span><span class="No-Break"><span class="koboSpan" id="kobo.1075.1">our models.</span></span></p>
<h1 id="_idParaDest-77"><a id="_idTextAnchor079"/><span class="koboSpan" id="kobo.1076.1">Using advanced techniques for model evaluation and selection in MATLAB</span></h1>
<p><span class="koboSpan" id="kobo.1077.1">Model evaluation</span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.1078.1"> and</span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.1079.1"> selection are crucial steps in machine learning to ensure the chosen model performs well on unseen data and generalizes effectively. </span><span class="koboSpan" id="kobo.1079.2">When it comes to advanced techniques for model evaluation and selection in MATLAB, there are several approaches you </span><span class="No-Break"><span class="koboSpan" id="kobo.1080.1">can consider.</span></span></p>
<p><span class="koboSpan" id="kobo.1081.1">In the subsequent sub-section, we will take a look at the most important techniques for model evaluation </span><span class="No-Break"><span class="koboSpan" id="kobo.1082.1">and selection.</span></span></p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor080"/><span class="koboSpan" id="kobo.1083.1">Understanding k-fold cross-validation</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.1084.1">K-fold cross-validation</span></strong><span class="koboSpan" id="kobo.1085.1"> is a widely </span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.1086.1">used technique for model evaluation and selection. </span><span class="koboSpan" id="kobo.1086.2">It involves partitioning the dataset into </span><em class="italic"><span class="koboSpan" id="kobo.1087.1">k</span></em><span class="koboSpan" id="kobo.1088.1"> equally sized subsets or folds. </span><span class="koboSpan" id="kobo.1088.2">The model undergoes training and assessment in </span><em class="italic"><span class="koboSpan" id="kobo.1089.1">k</span></em><span class="koboSpan" id="kobo.1090.1"> iterations, with each iteration employing a distinct fold as the validation set while using the remaining folds as the training set. </span><span class="koboSpan" id="kobo.1090.2">The outcomes of each iteration are then averaged to derive a comprehensive performance estimation. </span><span class="koboSpan" id="kobo.1090.3">This is the essence of how k-fold </span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1">cross-validation operates:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.1092.1">Splitting the dataset</span></strong><span class="koboSpan" id="kobo.1093.1">: Partition the dataset into </span><em class="italic"><span class="koboSpan" id="kobo.1094.1">k</span></em><span class="koboSpan" id="kobo.1095.1"> non-overlapping folds. </span><span class="koboSpan" id="kobo.1095.2">Generally, the value of </span><em class="italic"><span class="koboSpan" id="kobo.1096.1">k</span></em><span class="koboSpan" id="kobo.1097.1"> falls within the range of 5 to 10, although it can be adjusted based on factors such as dataset size </span><span class="No-Break"><span class="koboSpan" id="kobo.1098.1">and complexity.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1099.1">Training and evaluating the model</span></strong><span class="koboSpan" id="kobo.1100.1">: This entails a repetitive process repeated </span><em class="italic"><span class="koboSpan" id="kobo.1101.1">k</span></em><span class="koboSpan" id="kobo.1102.1"> times. </span><span class="koboSpan" id="kobo.1102.2">Within each iteration, the model is trained using </span><em class="italic"><span class="koboSpan" id="kobo.1103.1">k – 1</span></em><span class="koboSpan" id="kobo.1104.1"> folds and subsequently assessed for its performance on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1105.1">remaining fold.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1106.1">Performance metric aggregation</span></strong><span class="koboSpan" id="kobo.1107.1">: Calculate the performance metric of interest for each iteration. </span><span class="koboSpan" id="kobo.1107.2">The metrics from all iterations are then averaged to obtain a robust estimation of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1108.1">model’s performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1109.1">Model selection</span></strong><span class="koboSpan" id="kobo.1110.1">: Use the average performance metric to compare and select the best model among different algorithms or </span><span class="No-Break"><span class="koboSpan" id="kobo.1111.1">hyperparameter configurations.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1112.1">K-fold cross-validation helps address the limitations of single train-test splits by providing a more reliable estimate of a model’s performance. </span><span class="koboSpan" id="kobo.1112.2">It helps assess how well the model generalizes to unseen data and reduces the risk of overfitting or underfitting. </span><span class="koboSpan" id="kobo.1112.3">MATLAB provides built-in functions, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1113.1">crossval()</span></strong><span class="koboSpan" id="kobo.1114.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1115.1">cvpartition()</span></strong><span class="koboSpan" id="kobo.1116.1">, to facilitate k-fold cross-validation. </span><span class="koboSpan" id="kobo.1116.2">These functions automate the process and allow for easy implementation and </span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.1117.1">evaluation of models using </span><span class="No-Break"><span class="koboSpan" id="kobo.1118.1">k-fold cross-validation.</span></span></p>
<p><span class="koboSpan" id="kobo.1119.1">Let's see how we can practically approach a </span><span class="No-Break"><span class="koboSpan" id="kobo.1120.1">k-fold cross-validation:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1121.1">To understand the use of cross-validation, we employ a NASA dataset derived from a sequence of aerodynamic and acoustic experiments conducted in an anechoic wind tunnel. </span><span class="koboSpan" id="kobo.1121.2">The dataset comprises several fields, including </span><span class="No-Break"><span class="koboSpan" id="kobo.1122.1">the following:</span></span><ul><li><span class="koboSpan" id="kobo.1123.1">Frequency, in Hertz (</span><span class="No-Break"><span class="koboSpan" id="kobo.1124.1">named </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1125.1">FreqH</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1126.1">).</span></span></li><li><span class="koboSpan" id="kobo.1127.1">Angle of attack, in degrees (</span><span class="No-Break"><span class="koboSpan" id="kobo.1128.1">named </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1129.1">AngleD</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1130.1">).</span></span></li><li><span class="koboSpan" id="kobo.1131.1">Chord length, in meters (</span><span class="No-Break"><span class="koboSpan" id="kobo.1132.1">named </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1133.1">ChLenM</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1134.1">).</span></span></li><li><span class="koboSpan" id="kobo.1135.1">Free-stream velocity, in meters per second (</span><span class="No-Break"><span class="koboSpan" id="kobo.1136.1">named </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1137.1">FStVelMs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1138.1">).</span></span></li><li><span class="koboSpan" id="kobo.1139.1">Suction side displacement thickness, in meters (</span><span class="No-Break"><span class="koboSpan" id="kobo.1140.1">named </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1141.1">SucSDTM</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1142.1">).</span></span></li><li><span class="koboSpan" id="kobo.1143.1">Scaled sound pressure level, in decibels (</span><span class="No-Break"><span class="koboSpan" id="kobo.1144.1">named </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1145.1">SPLdB</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1146.1">).</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.1147.1">As usual, we start importing the dataset into the </span><span class="No-Break"><span class="koboSpan" id="kobo.1148.1">MATLAB environment:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1149.1">
AirfoilSelfNoise = readtable('AirfoilSelfNoise.xlsx');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1150.1">So, data is now available in the MATLAB workspace. </span><span class="koboSpan" id="kobo.1150.2">To apply cross-validation in our model, we can use the apps available in the MATLAB environment, for example, the Regression Learner app. </span><span class="koboSpan" id="kobo.1150.3">This app facilitates a streamlined and efficient process for conducting step-by-step regression analysis. </span><span class="koboSpan" id="kobo.1150.4">With this app, importing and exploring data, selecting features, defining validation schemes, training models, and evaluating results becomes remarkably straightforward and swift. </span><span class="koboSpan" id="kobo.1150.5">The app offers automated training functionality, enabling the search for the optimal regression model type. </span><span class="koboSpan" id="kobo.1150.6">It includes options such as linear regression models, regression trees, Gaussian process regression models, support vector regression, and an ensemble of regression trees. </span><span class="koboSpan" id="kobo.1150.7">Additionally, the trained model can be exported to the workspace for reuse with new data or to generate MATLAB code for programmatic regression. </span><span class="koboSpan" id="kobo.1150.8">By leveraging the Regression Learner app, users can save time and effort in performing regression analysis, benefiting from its intuitive interface and </span><span class="No-Break"><span class="koboSpan" id="kobo.1151.1">powerful features.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1152.1">To open the app, select the </span><strong class="bold"><span class="koboSpan" id="kobo.1153.1">APPS</span></strong><span class="koboSpan" id="kobo.1154.1"> tab on the MATLAB toolstrip, and click on the </span><strong class="bold"><span class="koboSpan" id="kobo.1155.1">Regression Learner</span></strong><span class="koboSpan" id="kobo.1156.1"> icon. </span><span class="koboSpan" id="kobo.1156.2">The Regression Learner app window will open, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1157.1">following </span></span><span class="No-Break"><a id="_idIndexMarker318"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1158.1">figure:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer031">
<span class="koboSpan" id="kobo.1159.1"><img alt="Figure 3.7 – Regression Learner app window" src="image/B21156_03_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1160.1">Figure 3.7 – Regression Learner app window</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1161.1">To import data from the MATLAB workspace into the Regression Learner app, follow </span><span class="No-Break"><span class="koboSpan" id="kobo.1162.1">these steps:</span></span></p>
<ol>
<li class="upper-roman"><span class="koboSpan" id="kobo.1163.1">Open the Regression Learner app by clicking on the </span><strong class="bold"><span class="koboSpan" id="kobo.1164.1">New Session</span></strong><span class="koboSpan" id="kobo.1165.1"> button located in the </span><strong class="bold"><span class="koboSpan" id="kobo.1166.1">File</span></strong><span class="koboSpan" id="kobo.1167.1"> section of the </span><strong class="bold"><span class="koboSpan" id="kobo.1168.1">Regression </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1169.1">Learner</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1170.1"> tab.</span></span></li>
<li class="upper-roman"><span class="koboSpan" id="kobo.1171.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.1172.1">New Session</span></strong><span class="koboSpan" id="kobo.1173.1"> dialog box will appear, containing </span><span class="No-Break"><span class="koboSpan" id="kobo.1174.1">three sections:</span></span><ol><li class="lower-roman"><em class="italic"><span class="koboSpan" id="kobo.1175.1">Step 1</span></em><span class="koboSpan" id="kobo.1176.1">: Select a table or matrix. </span><span class="koboSpan" id="kobo.1176.2">In this section, choose the source of your data. </span><span class="koboSpan" id="kobo.1176.3">You can select a table or a matrix containing the data you want </span><span class="No-Break"><span class="koboSpan" id="kobo.1177.1">to analyze.</span></span></li><li class="lower-roman"><em class="italic"><span class="koboSpan" id="kobo.1178.1">Step 2</span></em><span class="koboSpan" id="kobo.1179.1">: Select predictors and a response. </span><span class="koboSpan" id="kobo.1179.2">In this section, specify the variables that will serve as predictors and the variable that will be the response variable. </span><span class="koboSpan" id="kobo.1179.3">This allows you to define the type of variables involved in the analysis. </span><span class="koboSpan" id="kobo.1179.4">In this case, we can select </span><strong class="bold"><span class="koboSpan" id="kobo.1180.1">SPLdB</span></strong><span class="koboSpan" id="kobo.1181.1"> as the response and the other variables </span><span class="No-Break"><span class="koboSpan" id="kobo.1182.1">as predictors.</span></span></li><li class="lower-roman"><em class="italic"><span class="koboSpan" id="kobo.1183.1">Step 3</span></em><span class="koboSpan" id="kobo.1184.1">: Define a validation method. </span><span class="koboSpan" id="kobo.1184.2">In this section, you can choose the type of validation method to evaluate the predictive accuracy of the fitted models. </span><span class="koboSpan" id="kobo.1184.3">Different validation methods, such as cross-validation or holdout validation, can be selected to estimate model performance on </span><span class="No-Break"><span class="koboSpan" id="kobo.1185.1">new data.</span></span></li></ol></li>
</ol>
<p class="list-inset"><span class="koboSpan" id="kobo.1186.1">Validation methods </span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.1187.1">play a crucial role in assessing how well the fitted models can make accurate predictions on unseen data. </span><span class="koboSpan" id="kobo.1187.2">The Regression Learner app provides tools to evaluate and compare models based on their estimated performance, enabling the selection of the best model for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1188.1">given data.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1189.1">We will choose </span><strong class="bold"><span class="koboSpan" id="kobo.1190.1">Cross-Validation</span></strong><span class="koboSpan" id="kobo.1191.1"> as the validation scheme that performs a k-fold cross-validation. </span><span class="koboSpan" id="kobo.1191.2">We can set the number of folds (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1192.1">5</span></strong><span class="koboSpan" id="kobo.1193.1"> by default), then we can press the </span><strong class="bold"><span class="koboSpan" id="kobo.1194.1">Start </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1195.1">Session</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1196.1"> button.</span></span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.1197.1">To select the desired model type in the Regression Learner app, follow </span><span class="No-Break"><span class="koboSpan" id="kobo.1198.1">these steps:</span></span><ol><li class="upper-roman"><span class="koboSpan" id="kobo.1199.1">Expand the </span><strong class="bold"><span class="koboSpan" id="kobo.1200.1">Model Type</span></strong><span class="koboSpan" id="kobo.1201.1"> section by clicking on the arrow icon present in </span><span class="No-Break"><span class="koboSpan" id="kobo.1202.1">the app.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.1203.1">A list of available regression models will be displayed, including </span><span class="No-Break"><span class="koboSpan" id="kobo.1204.1">the following:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1205.1">Linear </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1206.1">regression models</span></strong></span></li><li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1207.1">Regression trees</span></strong></span></li><li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1208.1">SVMs</span></strong></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1209.1">Gaussian process </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1210.1">regression models</span></strong></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1211.1">Ensembles </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1212.1">of trees</span></strong></span></li></ul></li><li class="upper-roman"><span class="koboSpan" id="kobo.1213.1">To get started, select the </span><strong class="bold"><span class="koboSpan" id="kobo.1214.1">All Quick-To-Train</span></strong><span class="koboSpan" id="kobo.1215.1"> option. </span><span class="koboSpan" id="kobo.1215.2">This option allows you to select all the models available for this type of problem, so it will be possible to train the models using default </span><span class="No-Break"><span class="koboSpan" id="kobo.1216.1">settings quickly.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.1217.1">Click on the </span><strong class="bold"><span class="koboSpan" id="kobo.1218.1">Train</span></strong><span class="koboSpan" id="kobo.1219.1"> icon to initiate the training process. </span><span class="koboSpan" id="kobo.1219.2">The app will start training the selected models, and a selection of model types will appear in the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1220.1">History</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1221.1"> section.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.1222.1">Once the models finish training, the best RMSE score will be highlighted in a box. </span><span class="koboSpan" id="kobo.1222.2">This score indicates the </span><span class="No-Break"><span class="koboSpan" id="kobo.1223.1">model’s performance.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.1224.1">To further improve the model’s performance, you can try training with all available </span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.1225.1">algorithms. </span><span class="koboSpan" id="kobo.1225.2">Click on </span><strong class="bold"><span class="koboSpan" id="kobo.1226.1">All</span></strong><span class="koboSpan" id="kobo.1227.1"> and then </span><span class="No-Break"><span class="koboSpan" id="kobo.1228.1">click </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1229.1">Train</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1230.1">.</span></span></li></ol></li>
<li><span class="koboSpan" id="kobo.1231.1">By comparing the results in the </span><strong class="bold"><span class="koboSpan" id="kobo.1232.1">History</span></strong><span class="koboSpan" id="kobo.1233.1"> section, you can observe the RMSE scores for each model. </span><span class="koboSpan" id="kobo.1233.2">The lower RMSE score represents better performance. </span><span class="koboSpan" id="kobo.1233.3">In the example provided, the Gaussian process regression model achieved the </span><em class="italic"><span class="koboSpan" id="kobo.1234.1">lowest RMSE score (RMSE = 1.49)</span></em><span class="koboSpan" id="kobo.1235.1">, while the boosted trees model obtained the </span><em class="italic"><span class="koboSpan" id="kobo.1236.1">highest RMSE score (RMSE = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1237.1">6.18)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1238.1">.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.1239.1">To assess the improvements obtained, you can compare the predicted versus actual plots for the extreme models. </span><span class="koboSpan" id="kobo.1239.2">In the model with the lowest RMSE, the data points should closely align with the reference line, indicating a close match between the predicted and </span><span class="No-Break"><span class="koboSpan" id="kobo.1240.1">actual data.</span></span></p></li>
</ol>
<p><span class="koboSpan" id="kobo.1241.1">Overall, the Regression Learner app provides a visual representation of the results, allowing you to</span><a id="_idIndexMarker321"/><span class="koboSpan" id="kobo.1242.1"> easily compare and analyze the performance of different models. </span><span class="koboSpan" id="kobo.1242.2">Let’s see another way to select </span><span class="No-Break"><span class="koboSpan" id="kobo.1243.1">the data.</span></span></p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor081"/><span class="koboSpan" id="kobo.1244.1">Exploring leave-one-out cross-validation</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.1245.1">Leave-one-out cross-validation</span></strong><span class="koboSpan" id="kobo.1246.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1247.1">LOOCV</span></strong><span class="koboSpan" id="kobo.1248.1">) is a specific type of cross-validation technique used for</span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.1249.1"> model evaluation. </span><span class="koboSpan" id="kobo.1249.2">In LOOCV, each data point is sequentially held out as the validation set while the rest of the data is used for training the model. </span><span class="koboSpan" id="kobo.1249.3">This process is repeated for all data points, and the performance of the model is evaluated by averaging the results across all iterations. </span><span class="koboSpan" id="kobo.1249.4">The LOOCV technique works in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1250.1">following way:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1251.1">For each data point in the dataset, do </span><span class="No-Break"><span class="koboSpan" id="kobo.1252.1">the following:</span></span><ul><li><span class="koboSpan" id="kobo.1253.1">Remove the data point from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1254.1">training set</span></span></li><li><span class="koboSpan" id="kobo.1255.1">Train the model using the remaining </span><span class="No-Break"><span class="koboSpan" id="kobo.1256.1">data points</span></span></li><li><span class="koboSpan" id="kobo.1257.1">Use the trained model to predict the removed </span><span class="No-Break"><span class="koboSpan" id="kobo.1258.1">data point</span></span></li><li><span class="koboSpan" id="kobo.1259.1">Evaluate the performance of the model by comparing the predicted value with the actual value of the removed </span><span class="No-Break"><span class="koboSpan" id="kobo.1260.1">data point</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.1261.1">Calculate the performance metric (such as MSE or accuracy) for </span><span class="No-Break"><span class="koboSpan" id="kobo.1262.1">each iteration.</span></span></li>
<li><span class="koboSpan" id="kobo.1263.1">Compute the average performance metric across all iterations to obtain an overall estimation of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1264.1">model’s performance.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1265.1">LOOCV is particularly useful when the dataset is small or when there is a limited amount of data available. </span><span class="koboSpan" id="kobo.1265.2">Since each data point serves as a validation set once, LOOCV provides a more reliable estimate of the model’s performance compared to other cross-validation techniques. </span><span class="koboSpan" id="kobo.1265.3">It is worth mentioning that this technique is good for small datasets, as it is difficult to judge its variance error as the data points are almost the same for each fold; it just means one new record in </span><span class="No-Break"><span class="koboSpan" id="kobo.1266.1">each fold.</span></span></p>
<p><span class="koboSpan" id="kobo.1267.1">In MATLAB, you can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1268.1">cvpartition()</span></strong><span class="koboSpan" id="kobo.1269.1"> function with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1270.1">LeaveOut</span></strong><span class="koboSpan" id="kobo.1271.1"> option to generate indices for performing LOOCV. </span><span class="koboSpan" id="kobo.1271.2">These indices can then be used to partition the data for training and validation purposes in </span><span class="No-Break"><span class="koboSpan" id="kobo.1272.1">a loop.</span></span></p>
<p><span class="koboSpan" id="kobo.1273.1">Now, we will introduce the </span><span class="No-Break"><span class="koboSpan" id="kobo.1274.1">bootstrap method.</span></span></p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.1275.1">Introducing the bootstrap method</span></h2>
<p><span class="koboSpan" id="kobo.1276.1">The bootstrap method is a</span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.1277.1"> resampling technique used for estimating the sampling distribution of a statistic. </span><span class="koboSpan" id="kobo.1277.2">This process entails generating multiple bootstrap samples by randomly selecting data points from the original dataset with replacement, from which estimates of the statistic can be obtained. </span><span class="koboSpan" id="kobo.1277.3">This method allows us to assess the variability and uncertainty associated with the statistic </span><span class="No-Break"><span class="koboSpan" id="kobo.1278.1">of interest.</span></span></p>
<p><span class="koboSpan" id="kobo.1279.1">The bootstrap algorithm provides the </span><span class="No-Break"><span class="koboSpan" id="kobo.1280.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1281.1">Start with an original dataset of </span><span class="No-Break"><span class="koboSpan" id="kobo.1282.1">size </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1283.1">N</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1284.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.1285.1">Randomly sample </span><em class="italic"><span class="koboSpan" id="kobo.1286.1">N</span></em><span class="koboSpan" id="kobo.1287.1"> data points from the original dataset with replacement to create a bootstrap sample. </span><span class="koboSpan" id="kobo.1287.2">This means that each data point in the bootstrap sample is selected independently, and it is possible to select the same data point </span><span class="No-Break"><span class="koboSpan" id="kobo.1288.1">multiple times.</span></span></li>
<li><span class="koboSpan" id="kobo.1289.1">Compute the desired statistic on the bootstrap sample. </span><span class="koboSpan" id="kobo.1289.2">This could be the mean, median, standard deviation, or any other statistic </span><span class="No-Break"><span class="koboSpan" id="kobo.1290.1">of interest.</span></span></li>
<li><span class="koboSpan" id="kobo.1291.1">Repeat </span><em class="italic"><span class="koboSpan" id="kobo.1292.1">steps 2</span></em><span class="koboSpan" id="kobo.1293.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1294.1">3</span></em><span class="koboSpan" id="kobo.1295.1"> a number of times equal to </span><em class="italic"><span class="koboSpan" id="kobo.1296.1">B</span></em><span class="koboSpan" id="kobo.1297.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.1298.1">B</span></em><span class="koboSpan" id="kobo.1299.1"> is the number of bootstrap </span><span class="No-Break"><span class="koboSpan" id="kobo.1300.1">iterations desired.</span></span></li>
<li><span class="koboSpan" id="kobo.1301.1">Collect the computed statistics from each bootstrap sample to create the </span><span class="No-Break"><span class="koboSpan" id="kobo.1302.1">bootstrap distribution.</span></span></li>
<li><span class="koboSpan" id="kobo.1303.1">Calculate the desired confidence intervals or standard errors based on the bootstrap distribution to quantify the uncertainty around </span><span class="No-Break"><span class="koboSpan" id="kobo.1304.1">the statistic.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1305.1">The bootstrap method provides a robust approach for estimating the sampling distribution when the underlying distribution of the data is unknown or when analytical methods are not readily available. </span><span class="koboSpan" id="kobo.1305.2">It can be used for various purposes, such as hypothesis testing, constructing confidence intervals, and assessing </span><span class="No-Break"><span class="koboSpan" id="kobo.1306.1">model stability.</span></span></p>
<p><span class="koboSpan" id="kobo.1307.1">The bootstrap method holds significance as it enables the estimation of a model’s performance attributes, encompassing measures such as central tendency and uncertainty, all without requiring stringent assumptions about the underlying data distribution. </span><span class="koboSpan" id="kobo.1307.2">This method enhances the reliability and robustness of assessing a model’s performance, particularly in scenarios where data is scarce or the data distribution remains </span><span class="No-Break"><span class="koboSpan" id="kobo.1308.1">poorly</span></span><span class="No-Break"><a id="_idIndexMarker324"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1309.1"> defined.</span></span></p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor083"/><span class="koboSpan" id="kobo.1310.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1311.1">In this chapter, we have gained valuable insights into performing accurate classification tasks within the MATLAB environment. </span><span class="koboSpan" id="kobo.1311.2">We began by delving into the realm of decision tree methods, where we familiarized ourselves with key concepts such as nodes, branches, and leaf nodes. </span><span class="koboSpan" id="kobo.1311.3">By repeatedly dividing records into homogeneous subsets based on the target attribute, we learned how to classify objects into distinct classes effectively. </span><span class="koboSpan" id="kobo.1311.4">Moreover, we explored the prediction aspect of SVMs, which are particularly effective in solving complex problems with a clear margin of separation between classes. </span><span class="koboSpan" id="kobo.1311.5">SVMs can handle both linearly separable and non-linearly separable data by transforming the input space into a higher-dimensional </span><span class="No-Break"><span class="koboSpan" id="kobo.1312.1">feature space.</span></span></p>
<p><span class="koboSpan" id="kobo.1313.1">In the subsequent section, our focus shifted toward conducting precise regression analysis within the MATLAB environment. </span><span class="koboSpan" id="kobo.1313.2">We commenced by delving into simple linear regression, gaining an understanding of its definition and the process of obtaining ordinary least squares estimation. </span><span class="koboSpan" id="kobo.1313.3">Additionally, we explored multiple techniques for quantifying the intercept and slope of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1314.1">linear relationship.</span></span></p>
<p><span class="koboSpan" id="kobo.1315.1">Subsequently, we unearthed the linear regression model builder, a valuable tool for constructing an object encompassing training data, model description, diagnostic information, and fitted coefficients necessary for linear regression. </span><span class="koboSpan" id="kobo.1315.2">Furthermore, we familiarized ourselves with the correct interpretation of simulation results and grasped the techniques to mitigate the influence of outliers through </span><span class="No-Break"><span class="koboSpan" id="kobo.1316.1">robust regression.</span></span></p>
<p><span class="koboSpan" id="kobo.1317.1">Then, we understood how to use the tools available in MATLAB to perform an accurate evaluation of the model trained. </span><span class="koboSpan" id="kobo.1317.2">Finally, we discovered the cross-validation methods to increase the performance of the model, finding the </span><span class="No-Break"><span class="koboSpan" id="kobo.1318.1">best-performing model.</span></span></p>
<p><span class="koboSpan" id="kobo.1319.1">In the next chapter, we will explore the clustering methodology to find hidden patterns or groupings in </span><span class="No-Break"><span class="koboSpan" id="kobo.1320.1">a dataset.</span></span></p>
</div>
</body></html>