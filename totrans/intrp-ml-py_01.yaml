- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interpretation, Interpretability, and Explainability; and Why Does It All Matter?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We live in a world whose rules and procedures are ever-increasingly governed
    by data and algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, there are rules about who gets approved for credit or released
    on bail, and which social media posts might get censored. There are also procedures
    to determine which marketing tactics are most effective and which chest x-ray
    features might diagnose a positive case of pneumonia.
  prefs: []
  type: TYPE_NORMAL
- en: We expect this because it is nothing new!
  prefs: []
  type: TYPE_NORMAL
- en: But not so long ago, rules and procedures such as these used to be hardcoded
    into software, textbooks, and paper forms, and humans were the ultimate decision-makers.
    Often, it was entirely up to human discretion. Decisions depended on human discretion
    because rules and procedures were rigid and, therefore, not always applicable.
    There were *always* exceptions, so a human was needed to make them.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you apply for a mortgage, your approval depended on an acceptable
    and reasonably lengthy credit history. This data, in turn, would produce a credit
    score using a scoring algorithm. Then, the bank had rules that determined what
    score was good enough for the mortgage you wanted. Your loan officer could follow
    it or not.
  prefs: []
  type: TYPE_NORMAL
- en: These days, financial institutions train models on thousands of mortgage outcomes,
    with dozens of variables. These models can be used to determine the likelihood
    that you would default on a mortgage with a presumed high accuracy. If there is
    a loan officer to stamp the approval or denial, it’s no longer merely a guideline
    but an algorithmic decision. How could it be wrong? How could it be right? How
    and why was the decision made?
  prefs: []
  type: TYPE_NORMAL
- en: Hold on to that thought because, throughout this book, we will be learning the
    answers to these questions and many more!
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine learning model interpretation enables you to understand the logic behind
    a decision and trace back the detailed steps of the process behind the logic.
    This chapter introduces machine learning interpretation and related concepts,
    such as interpretability, explainability, black-box models, and transparency.
    This chapter provides definitions for these terms to avoid ambiguity and underpins
    the value of machine learning interpretability. These are the main topics we are
    going to cover:'
  prefs: []
  type: TYPE_NORMAL
- en: What is machine learning interpretation?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the difference between interpretation and explainability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A business case for interpretability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow the example in this chapter, you will need Python 3, either running
    in a Jupyter environment or in your favorite **integrated development environment**
    (**IDE**) such as PyCharm, Atom, VSCode, PyDev, or Idle. The example also requires
    the `pandas`, `sklearn`, `matplotlib`, and `scipy` Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter is located here: [https://packt.link/Lzryo](https://packt.link/Lzryo).'
  prefs: []
  type: TYPE_NORMAL
- en: What is machine learning interpretation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To interpret something is to *explain the meaning of it*. In the context of
    machine learning, that something is an algorithm. More specifically, that algorithm
    is a mathematical one that takes input data and produces an output, much like
    with any formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the most basic of models, simple linear regression, illustrated
    in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18406_01_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Once fitted to the data, the meaning of this model is that ![](img/B18406_01_002.png)
    predictions are a weighted sum of the *x* features with the *β* coefficients.
    In this case, there’s only one *x* **feature** or **predictor** variable, and
    the *y* variable is typically called the **response** or **target** variable.
    A simple linear regression formula single-handedly explains the transformation,
    which is performed on the input data x[1] to produce the output ![](img/B18406_01_002.png).
    The following example can illustrate this concept in further detail.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding a simple weight prediction model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you go to this web page maintained by the University of California, [http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights),
    you can find a link to download a dataset of 25,000 synthetic records of the weights
    and heights of 18-year-olds. We won’t use the entire dataset but only the sample
    table on the web page itself with 200 records. We scrape the table from the web
    page and fit a linear regression model to the data. The model uses the height
    to predict the weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, *x*[1]= *height* and *y*= *weight*, so the formula for the
    linear regression model would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18406_01_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can find the code for this example here: [https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/01/WeightPrediction.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/01/WeightPrediction.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this example, you need to install the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas` to load the table into a DataFrame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn` (`scikit-learn`) to fit the linear regression model and calculate
    its error'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` to visualize the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scipy` to test the correlation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You should load all of them first, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the libraries are all loaded, you use `pandas` to fetch the contents of
    the web page, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`pandas` can turn the raw **HyperText Markup Language** **(HTML**) contents
    of the table into a DataFrame, which we subset to only include only two columns.
    And voilà! We now have a DataFrame with `Heights(Inches)` in one column and `Weights(Pounds)`
    in another.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the data, we must transform it so that it conforms to the
    model’s specifications. `sklearn` needs it as `numpy` arrays with (200,1) dimensions,
    so we must first extract the `Height(Inches)` and `Weight(Pounds)` `pandas` series.
    Then, we turn them into (200,) `numpy` arrays, and, finally, reshape them into
    (200,1) dimensions. The following commands perform all the necessary transformation
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we initialize the scikit-learn `LinearRegression` model and `fit` it
    with the training data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To output the fitted linear regression model formula in scikit-learn, you must
    extract the intercept and coefficients. This is the **formula** that explains
    how it makes predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This tells us that, on average, for every additional pound, there are 3.4 inches
    of height.
  prefs: []
  type: TYPE_NORMAL
- en: However, *explaining how the model works* is only one way to explain this linear
    regression model, and this is only one side of the story. The model isn’t perfect
    because the actual outcomes and the predicted outcomes are not the same for the
    training data. The difference between them is the **error** or **residuals**.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways of understanding an error in a model. You can use an error
    function such as `mean_absolute_error` to measure the deviation between the predicted
    values and the actual values, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: A 7.8 mean absolute error means that, on average, the prediction is 7.8 pounds
    from the actual value, but this might not be intuitive or informative. Visualizing
    the linear regression model can shed some light on how accurate these predictions
    truly are.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be done by using a `matplotlib` scatterplot and overlaying the linear
    model (in blue) and the *mean absolute error* (as two parallel bands in gray),
    as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 1.1 – Linear regression model to predict weight based on height ](img/B18406_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Linear regression model to predict weight based on height'
  prefs: []
  type: TYPE_NORMAL
- en: As you can appreciate from the plot in *Figure 1.1*, there are many times in
    which the actuals are 20–25 pounds away from the prediction. Yet the mean absolute
    error can fool you into thinking that the error is always closer to 8\. This is
    why it is essential to visualize the error of the model to understand its distribution.
    Judging from this graph, we can tell that there are no red flags that stand out
    about this distribution, such as residuals being more spread out for one range
    of heights than for others. Since it is more or less equally spread out, we say
    it’s **homoscedastic**. In the case of linear regression, this is one of many
    model assumptions you should test for, along with linearity, normality, independence,
    and lack of multicollinearity (if there’s more than one feature). These assumptions
    ensure that you are using the right model for the job. In other words, the height
    and weight can be explained with a linear relationship, and it is a good idea
    to do so, statistically speaking.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this model, we are trying to establish a linear relationship between *x*
    height and *y* weight. This association is called a **linear correlation**. One
    way to measure this relationship’s strength is with **Pearson’s correlation coefficient**.
    This statistical method measures the association between two variables using their
    covariance divided by their standard deviations. It is a number between -1 and
    1 whereby the closer the number is to 0, the weaker the association is. If the
    number is positive, there is a positive association, and if it’s negative, there
    is a negative one. In Python, you can compute Pearson’s correlation coefficient
    with the `pearsonr` function from `scipy`, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The number is positive, which is no surprise because as height increases, weight
    also tends to increase, but it is also closer to 1 than to 0, denoting that it
    is strongly correlated. The second number produced by the `pearsonr` function
    is the *p*-value for testing non-correlation. If we test that it’s less than a
    threshold of 5%, we can say there’s sufficient evidence of this correlation, as
    illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It confirms with a `True` that it is statistically significant.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how a model performs under different circumstances can help us
    *explain why it makes certain predictions*, and when it cannot. Let’s imagine
    we are asked to explain why someone who is 71 inches tall was predicted to have
    a weight of 134 pounds but instead weighed 18 pounds more. Judging from what we
    know about the model, this margin of error is not unusual even though it’s not
    ideal. However, there are many circumstances in which we cannot expect this model
    to be reliable. What if we were asked to predict the weight of a person who is
    56 inches tall with the help of this model? Could we assure the same level of
    accuracy? Definitely not, because we fit the model on the data of subjects no
    shorter than 63 inches. The same is true if we were asked to predict the weight
    of a 9-year-old, because the training data was for 18-year-olds.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the acceptable results, this weight prediction model was not a realistic
    example. If you wanted to be more accurate but—more importantly—faithful to what
    can really impact the weight of an individual, you would need to add more variables.
    You can add—say—gender at birth, age, diet, and activity levels. This is where
    it gets interesting because you have to make sure *it is fair to include them,
    or to exclude them*. For instance, if gender were included and most of our dataset
    was composed of males, how could you ensure accuracy for females? This is what
    is called **selection bias**. And what if weight had more to do with lifestyle
    choices and circumstances such as poverty and pregnancy than gender? If these
    variables aren’t included, this is called **omitted variable bias**. And then,
    does it make sense to include the sensitive gender variable at the risk of adding
    bias to the model?
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have multiple features that you have vetted for bias, you can find
    out and *explain which features impact model performance*. We call this **feature
    importance**. However, as we add more variables, we increase the complexity of
    the model. Paradoxically, this is a problem for interpretation, and we will explore
    this in further detail in the following chapters. For now, the key takeaway should
    be that model interpretation has a lot to do with explaining the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Can we explain how predictions were made, and how the model works?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we ensure that they are reliable and safe?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we explain that predictions were made without bias?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And ultimately, the question we are trying to answer is this:'
  prefs: []
  type: TYPE_NORMAL
- en: Can we trust the model?
  prefs: []
  type: TYPE_NORMAL
- en: The three main concepts of interpretable machine learning directly relate to
    the three preceding questions and have the acronym of **FAT**, which stands for
    **fairness**, **accountability**, and **transparency**. If you can explain that
    predictions were made without discernible bias, then there is **fairness**. If
    you can explain why it makes certain predictions, then there’s **accountability**.
    And if you can explain how predictions were made and how the model works, then
    there’s **transparency**. There are many ethical concerns associated with these
    concepts, as shown here in *Figure 1.2*. It’s portrayed as a triangle because
    each layer depends on the previous one.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Three main concept of Interpretable Machine Learning ](img/B18406_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Three main concepts of interpretable machine learning'
  prefs: []
  type: TYPE_NORMAL
- en: Some researchers and companies have expanded FAT under a larger umbrella of
    ethical **AI**, thus turning FAT into FATE. However, both concepts very much overlap
    since interpretable machine learning is how FAT principles and ethical concerns
    get implemented in machine learning. In this book, we will discuss ethics in this
    context. For instance, *Chapter 13*, *Adversarial Robustness*, discusses reliability,
    safety, and security. *Chapter 11*, *Bias Mitigating and Causal Inference Methods*,
    discusses fairness. That being said, interpretable machine learning can be leveraged
    with no ethical aim in mind, and also for unethical reasons too.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the difference between interpretability and explainability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Something you’ve probably noticed when reading the first few pages of this book
    is that the verbs *interpret* and *explain*, as well as the nouns *interpretation*
    and *explanation*, have been used interchangeably. This is not surprising, considering
    that to interpret is to explain the meaning of something. Despite that, the related
    terms *interpretability* and *explainability* should not be used interchangeably,
    even though they are often mistaken for synonyms. Most practitioners don’t make
    any distinction and many academics reverse the definitions provided in this book.
  prefs: []
  type: TYPE_NORMAL
- en: What is interpretability?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Interpretability is the extent to which humans, including non-subject-matter
    experts, can understand the cause and effect, and input and output, of a machine
    learning model. To say a model has a high level of interpretability means you
    can describe in a human-interpretable way its inference. In other words, why does
    an input to a model produce a specific output? What are the requirements and constraints
    of the input data? What are the confidence bounds of the predictions? Or, why
    does one variable have a more substantial impact on the prediction than another?
    For interpretability, detailing how a model works is only relevant to the extent
    that it can explain its predictions and justify that it’s the right model for
    the use case.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter’s example, we could explain that there’s a linear relationship
    between human height and weight, so using linear regression rather than a non-linear
    model makes sense. We can prove this statistically because the variables involved
    don’t violate the assumptions of linear regression. Even when statistics support
    your explanation, we still should consult with the domain knowledge area involved
    in the use case. In this scenario, we rest assured, biologically speaking, because
    our knowledge of human physiology doesn’t contradict the relationship between
    height and weight.
  prefs: []
  type: TYPE_NORMAL
- en: Beware of complexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many machine learning models are inherently harder to understand simply because
    of the math involved in the inner workings of the model or the specific model
    architecture. In addition to this, many choices are made that can increase complexity
    and make the models less interpretable, from dataset selection to feature selection
    and engineering to model training and tuning choices. This complexity makes explaining
    how machine learning models work a challenge. Machine learning interpretability
    is a very active area of research, so there’s still much debate on its precise
    definition. The debate includes whether total transparency is needed to qualify
    a machine learning model as sufficiently interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: This book favors the understanding that the definition of interpretability shouldn’t
    necessarily exclude opaque models, which, for the most part, are complex, as long
    as the choices made don’t compromise their trustworthiness. This compromise is
    what is generally called **post-hoc interpretability**. After all, much like a
    complex machine learning model, we can’t explain exactly how a human brain makes
    a choice, yet we often trust its decision because we can ask a human for their
    reasoning. Post-hoc machine learning interpretation is similar, except it’s a
    human explaining the reasoning on behalf of the model. Using this particular concept
    of interpretability is advantageous because we can interpret opaque models and
    not sacrifice the accuracy of our predictions. We will discuss this in further
    detail in *Chapter 3*, *Interpretation Challenges*.
  prefs: []
  type: TYPE_NORMAL
- en: When does interpretability matter?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Decision-making systems don’t always require interpretability. There are two
    cases that are offered as exceptions, outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: When incorrect results have no significant consequences. For instance, what
    if a machine learning model is trained to find and read the postal code in a package,
    occasionally misreads it, and sends it elsewhere? There’s little chance of discriminatory
    bias, and the cost of misclassification is relatively low. It doesn’t occur often
    enough to magnify the cost beyond acceptable thresholds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When there are consequences, but these have been studied sufficiently and validated
    enough in the real world to make decisions without human involvement. This is
    the case with a **Traffic-Alert And Collision-Avoidance System** (**TCAS**), which
    alerts the pilot of another aircraft that poses a threat of a mid-air collision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, interpretability is needed for these systems to have the
    following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Minable for scientific knowledge**: For example, meteorologists have much
    to learn from a climate model, but only if it’s easy to interpret.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reliable and safe**: The decisions made by a self-driving vehicle must be
    debuggable so that its developers can understand and correct points of failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethical**: A translation model might use gender-biased word embeddings that
    result in discriminatory translations, such as a doctor being paired with male
    pronouns, but you must be able to find these instances easily to correct them.
    However, the system must be designed in such a way that you can be made aware
    of a problem before it is released to the public.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conclusive and consistent**: Sometimes, machine learning models may have
    incomplete and mutually exclusive objectives—for instance, a cholesterol-control
    system may not consider how likely a patient is to adhere to the diet or drug
    regimen, or there might be a trade-off between one objective and another, such
    as safety and non-discrimination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By explaining the decisions of a model, we can cover gaps in our understanding
    of the problem—*its incompleteness*. One of the most significant issues is that
    given the high accuracy of our machine learning solutions, we tend to increase
    our confidence level to a point where we think we fully understand the problem.
    Then, we are misled into thinking our solution covers *EVERYTHING*!
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of this book, we discussed how leveraging data to produce algorithmic
    rules is nothing new. However, we used to second-guess these rules, and now we
    don’t. Therefore, a human used to be accountable, and now it’s the algorithm.
    In this case, the algorithm is a machine learning model that is accountable for
    all of the ethical ramifications this entails. This switch has a lot to do with
    accuracy. The problem is that although a model may surpass human accuracy in aggregate,
    machine learning models have yet to interpret their results as a human would.
    Therefore, it doesn’t second-guess its decisions, so as a solution, it lacks a
    desirable level of completeness. That’s why we need to interpret models so that
    we can cover at least some of that gap. So, why is machine learning interpretation
    not already a standard part of the data science pipeline? In addition to our bias
    toward focusing on accuracy alone, one of the biggest impediments is the daunting
    concept of black-box models.
  prefs: []
  type: TYPE_NORMAL
- en: What are black-box models?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is just another term for opaque models. A black box refers to a system
    in which only the input and outputs are observable, and you cannot understand
    what is transforming the inputs into the outputs. In the case of machine learning,
    a black-box model can be opened, but its mechanisms are not easily understood.
  prefs: []
  type: TYPE_NORMAL
- en: What are white-box models?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These are the opposite of black-box models (see *Figure 1.3*). They are also
    known as transparent because they achieve total or near-total interpretation transparency.
    We call them **intrinsically interpretable** in this book, and we cover them in
    more detail in *Chapter 3*, *Interpretation Challenges*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at a comparison between the models here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Visual comparison between white- and black-box models ](img/B18406_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Visual comparison between white- and black-box models'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will examine the property that separates white- and black-box models:
    explainability.'
  prefs: []
  type: TYPE_NORMAL
- en: What is explainability?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Explainability encompasses everything interpretability is. The difference is
    that it goes deeper on the transparency requirement than interpretability because
    it demands human-friendly explanations for a model’s inner workings and the model
    training process, and not just model inference. Depending on the application,
    this requirement might extend to various degrees of model, design, and algorithmic
    transparency. There are three types of transparency, outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model transparency**: Being able to explain how a model is trained step by
    step. In the case of our simple weight prediction model, we can explain how the
    optimization method called **ordinary least squares** finds the *β* coefficient
    that minimizes errors in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Design transparency**: Being able to explain choices made, such as model
    architecture and hyperparameters. For instance, we could justify these choices
    based on the size or nature of the training data. If we were performing a sales
    forecast and we knew that our sales were seasonal over the year, this could be
    a sound parameter choice. If we had doubts, we could always use some well-established
    statistical method to find seasonality patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithmic transparency**: Being able to explain automated optimizations
    such as grid search for hyperparameters, but note that the ones that can’t be
    reproduced because of their random nature—such as random search for hyperparameter
    optimization, early stopping, and stochastic gradient descent—make the algorithm
    non-transparent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Black-box models are called *opaque* simply because they lack *model transparency*,
    but for many models this is unavoidable, however justified the model choice might
    be. In many scenarios, even if you outputted the math involved in, for example,
    training a neural network or a random forest, it would raise more doubts than
    generate trust. There are at least a few reasons for this, outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Not “statistically grounded”**: An opaque model training process maps an
    input to an optimal output, leaving behind what appears to be an arbitrary trail
    of parameters. These parameters are optimized to a cost function but are not grounded
    in statistical theory, which makes predictions hard to justify and explain in
    statistical terms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uncertainty and non-reproducibility**: Many opaque models are equally reproducible
    because they use random numbers to initialize their weights, regularize or optimize
    their hyperparameters, or make use of stochastic discrimination (such is the case
    for random forest algorithms).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting and the curse of dimensionality**: Many of these models operate
    in a high-dimensional space. This doesn’t elicit trust because it’s harder to
    generalize on a larger number of dimensions. After all, there’s more opportunity
    to overfit a model the more dimensions you add.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The limitations of human cognition**: Transparent models are often used for
    smaller datasets with fewer dimensions. They also tend to not complicate the interactions
    between these dimensions more than necessary. This lack of complexity makes it
    easier to visualize what the model is doing and its outcomes. Humans are not very
    good at understanding many dimensions, so using transparent models tends to make
    this much easier to understand. That being said, even these models can get so
    complex they might become opaque. For instance, if a decision tree model is 100
    levels deep or a linear regression model has 100 features, it’s no longer easy
    for us to understand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Occam’s razor**: This is what is called the principle of simplicity or parsimony.
    It states that the simplest solution is usually the right one. Whether true or
    not, humans also have a bias for simplicity, and transparent models are known
    for—if anything—their simplicity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why and when does explainability matter?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Trustworthy and ethical decision-making is the main motivation for interpretability.
    Explainability has additional motivations such as causality, transferability,
    and informativeness. Therefore, there are many use cases in which total or nearly
    total transparency is valued, and rightly so. Some of these are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scientific research**: Reproducibility is essential for scientific research.
    Also, using statistically grounded optimization methods is especially desirable
    when causality needs to be established.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clinical trials**: These must also produce reproducible findings and be statistically
    grounded. In addition to this, given the potential of overfitting, they must use
    the fewest dimensions possible and models that don’t complicate them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumer product safety testing**: Much as with clinical trials, when life-and-death
    safety is a concern, simplicity is preferred whenever possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Public policy and law**: This is a more nuanced discussion, as part of what
    legal scholars call **algorithmic governance**, and they have distinguished between
    **fishbowl transparency** and **reasoned transparency**. Fishbowl transparency
    seeks total explainability and it’s closer to the rigor required for consumer
    product safety testing, whereas reasoned transparency is one where post-hoc interpretability,
    as defined earlier, would suffice. One day, the government could be entirely run
    by algorithms. When that happens, it’s hard to tell which policies will align
    with which form of transparency, but there are many areas of public policy, such
    as criminal justice, where absolute transparency is necessary. However, whenever
    total transparency contradicts privacy or security objectives, a less rigorous
    form of transparency may be preferred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Criminal investigation and regulatory compliance audits**: If something goes
    wrong, such as an accident at a chemical factory caused by a robot malfunction
    or a crash by an autonomous vehicle, an investigator would need a **decision trail**.
    This is to facilitate the assignment of accountability and legal liability. Even
    when no accident has happened, this kind of auditing can be performed when mandated
    by authorities. Compliance auditing applies to industries that are regulated,
    such as financial services, utilities, transportation, and healthcare. In many
    cases, fishbowl transparency is preferred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A business case for interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section describes several practical business benefits of machine learning
    interpretability, such as better decisions, as well as being more trusted, ethical,
    and profitable.
  prefs: []
  type: TYPE_NORMAL
- en: Better decisions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Typically, machine learning models are trained and then evaluated against the
    desired metrics. If they pass quality control against a hold-out dataset, they
    are deployed. However, once tested in the real world, things can get wild, as
    in the following hypothetical scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: A high-frequency trading algorithm could single-handedly crash the stock market.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hundreds of smart home devices might inexplicably burst into unprompted laughter,
    terrifying their users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: License-plate recognition systems could incorrectly read a new kind of license
    plate and fine the wrong drivers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A racially biased surveillance system could incorrectly detect an intruder,
    and because of this guards shoot an innocent office worker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A self-driving car could mistake snow for a pavement, crash into a cliff, and
    injure passengers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any system is prone to error, so this is not to say that interpretability is
    a cure-all. However, focusing on just optimizing metrics can be a recipe for disaster.
    In the lab, the model might generalize well, but if you don’t know why the model
    is making the decisions, then you can miss an opportunity for improvement. For
    instance, knowing *what* a self-driving car identifies as a road is not enough,
    but knowing *why* could help improve the model. If, say, one of the reasons was
    that road is light-colored like the snow, this could be dangerous. Checking the
    model’s assumptions and conclusions can lead to an improvement in the model by
    introducing winter road images into the dataset or feeding real-time weather data
    into the model. Also, if this doesn’t work, maybe an algorithmic fail-safe can
    stop it from acting on a decision that it’s not entirely confident about.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main reasons why a focus on machine learning interpretability leads
    to better decision-making was mentioned earlier when we discussed completeness.
    If we think a model is complete, what is the point of making it better? Furthermore,
    if we don’t question the model’s reasoning, then our understanding of the problem
    must be complete. If this is the case, perhaps we shouldn’t be using machine learning
    to solve the problem in the first place! Machine learning creates an algorithm
    that would otherwise be too complicated to program in *if-else* statements, precisely
    to be used for cases where our understanding of the problem is incomplete!
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that when we predict or estimate something, especially with a high
    level of accuracy, we think we control it. This is what is called the **illusion
    of control bias**. We can’t underestimate the complexity of a problem just because,
    in aggregate, the model gets it right almost all the time. Even for a human, the
    difference between snow and concrete pavement can be blurry and difficult to explain.
    How would you even begin to describe this difference in such a way that it is
    always accurate? A model can learn these differences, but it doesn’t make it any
    less complex. Examining a model for points of failure and continuously being vigilant
    for outliers requires a different outlook, whereby we admit that we can’t control
    the model, but we can try to understand it through interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some additional decision biases that can adversely impact
    a model, and serve as reasons why interpretability can lead to better decision-making:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conservatism bias**: When we get new information, we don’t change our prior
    beliefs. With this bias, entrenched pre-existing information trumps new information,
    but models ought to evolve. Hence, an attitude that values questioning prior assumptions
    is a healthy one to have.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Salience bias**: Some prominent or more visible things may stand out more
    than others, but statistically speaking, they should get equal attention to others.
    This bias could inform our choice of features, so an interpretability mindset
    can expand our understanding of a problem to include other less perceived features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fundamental attribution error**: This bias causes us to attribute outcomes
    to behavior rather than circumstances, character rather than situations, and nature
    rather than nurture. Interpretability asks us to explore deeper and look for the
    less obvious relationships between our variables or those that could be missing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One crucial benefit of model interpretation is locating *outliers*. These outliers
    could be a potential new source of revenue or a liability waiting to happen. Knowing
    this can help us to prepare and strategize accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: More trusted brands
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Trust is defined as a belief in the reliability, ability, or credibility of
    something or someone. In the context of organizations, trust is their reputation;
    and in the unforgiving court of public opinion, all it takes is one accident,
    controversy, or fiasco to lose public confidence. This, in turn, can cause investor
    confidence to wane.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider what happened to Boeing after the 737 MAX debacle or Facebook
    after the Cambridge Analytica elections scandal. In both cases, the technological
    failures were shrouded in mystery, leading to massive public distrust.
  prefs: []
  type: TYPE_NORMAL
- en: And these were examples of, for the most part, decisions made by people. With
    decisions made exclusively by machine learning models, this could get worse because
    it is easy to drop the ball and keep the accountability in the model’s corner.
    For instance, if you started to see offensive material in your Facebook feed,
    Facebook could say it’s because its model was trained with *your data* such as
    your comments and likes, so it’s really a reflection of *what you want to see*.
    Not their fault—your fault. If the police targeted our neighborhood for aggressive
    policing because it uses PredPol, an algorithm that predicts where and when crimes
    will occur, it could blame the algorithm. On the other hand, the makers of this
    algorithm could blame the police because the software is trained on their police
    reports. This generates a potentially troubling feedback loop, not to mention
    an accountability gap. And if some pranksters or hackers physically place strange
    textured meshes onto a highway (see [https://arxiv.org/pdf/2101.06784.pdf](https://arxiv.org/pdf/2101.06784.pdf)),
    this could cause a Tesla self-driving car to veer into the wrong lane. Is this
    Tesla’s fault for not anticipating this possibility, or the hackers’ for throwing
    a monkey wrench into their model? This is called an **adversarial attack**, and
    we discuss this in *Chapter 13*, *Adversarial Robustness*.
  prefs: []
  type: TYPE_NORMAL
- en: It is undoubtedly one of the goals of machine learning interpretability to make
    models better at making decisions. But even when they fail, you can show that
    you tried. Trust is not lost entirely because of the failure itself but because
    of the lack of accountability, and even in cases where it is not fair to accept
    all the blame, some accountability is better than none. For instance, in the previous
    set of examples, Facebook could look for clues as to why offensive material is
    shown more often and then commit to finding ways to make it happen less, even
    if this means making less money. PredPol could find other sources of crime-rate
    datasets that are potentially less biased, even if they are smaller. They could
    also use techniques to mitigate bias in existing datasets (these are covered in
    *Chapter 11*, *Bias Mitigation and Causal Inference Methods*). And Tesla could
    audit its systems for adversarial attacks, even if this delays the shipment of
    its cars. All of these are interpretability solutions. Once they become common
    practice, they can lead to an increase in not only public trust—be it from users
    and customers—but also internal stakeholders such as employees and investors.
  prefs: []
  type: TYPE_NORMAL
- en: Many public relation AI blunders have occurred over the past couple of years.
    Due to trust issues, many AI-driven technologies are losing public support, to
    the detriment of both companies that monetize AI and users that could benefit
    from them. This, in part, requires a legal framework at a national or global level
    and, at the organizational end for those that deploy these technologies, more
    accountability.
  prefs: []
  type: TYPE_NORMAL
- en: More ethical
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three schools of thought for ethics: utilitarians focus on consequences,
    deontologists are concerned with duty, and teleologicalists are more interested
    in overall moral character. So, this means that there are different ways to examine
    ethical problems. For instance, there are useful lessons to draw from all of them.
    There are cases in which you want to produce the greatest amount of “good,” despite
    some harm being produced in the process. Other times, ethical boundaries must
    be treated as lines in the sand you mustn’t cross. And at other times, it’s about
    developing a righteous disposition, much like many religions aspire to do. Regardless
    of the school of ethics that we align with, our notion of what it is evolves with
    time because it mirrors our current values. At this moment, in Western cultures,
    these values include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Human welfare
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ownership and property
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Freedom from bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Universal usability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autonomy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Informed consent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accountability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Courtesy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environmental sustainability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical transgressions are cases whereby you cross the moral boundaries that
    these values seek to uphold, be it by discriminating against someone or polluting
    their environment, whether it’s against the law or not. Ethical dilemmas occur
    when you have a choice between options that lead to transgressions, so you have
    to choose between one and another.
  prefs: []
  type: TYPE_NORMAL
- en: The first reason machine learning is related to ethics is that technologies
    and ethical dilemmas have an intrinsically linked history.
  prefs: []
  type: TYPE_NORMAL
- en: Even the first widely adopted tools made by humans brought progress but also
    caused harm, such as accidents, war, and job losses. This is not to say that technology
    is always bad but that we lack the foresight to measure and control its consequences
    over time. In AI’s case, it is not clear what the harmful long-term effects are.
    What we can anticipate is that there will be a major loss of jobs and an immense
    demand for energy to power our data centers, which could put stress on the environment.
    There’s speculation that AI could create an “algocratic” surveillance state run
    by algorithms, infringing on values such as privacy, autonomy, and ownership.
    Some readers might point to examples of this already happening.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second reason is even more consequential than the first. It’s that predictive
    modeling is a technological first for humanity: machine learning is a technology
    that can make decisions for us, and these decisions can produce individual ethical
    transgressions that are hard to trace. The problem with this is that accountability
    is essential to morality because you have to know who to blame for human dignity,
    atonement, closure, or criminal prosecution. However, many technologies have accountability
    issues to begin with, because moral responsibility is often shared in any case.
    For instance, maybe the reason for a car crash was partly due to the driver, mechanic,
    and car manufacturer. The same can happen with a machine learning model, except
    it gets trickier. After all, a model’s programming has no programmer because the
    “programming” was learned from data, and there are things a model can learn from
    data that can result in ethical transgressions. Top among them are biases such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample bias**: When your data, the sample, doesn’t represent the environment
    accurately, also known as the population'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exclusion bias**: When you omit features or groups that could otherwise explain
    a critical phenomenon with the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prejudice bias**: When stereotypes influence your data, either directly or
    indirectly'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measurement bias**: When faulty measurements distort your data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretability comes in handy to mitigate bias, as seen in *Chapter 11*, *Bias
    Mitigation and Causal Inference Methods*, or even placing guardrails on the right
    features, which may be a source of bias. This is covered in *Chapter 12*, *Monotonic
    Constraints and Model Tuning for Interpretability*. As explained in this chapter,
    explanations go a long way in establishing accountability, which is a moral imperative.
    Also, by explaining the reasoning behind models, you can find ethical issues before
    they cause any harm. But there are even more ways in which models’ potentially
    worrisome ethical ramifications can be controlled, and this has less to do with
    interpretability and more to do with design. There are frameworks such as **human-centered
    design, value-sensitive design**, and **techno-moral virtue ethics** that can
    be used to incorporate ethical considerations into every technological design
    choice. An article by Kirsten Martin ([https://doi.org/10.1007/s10551-018-3921-3](https://doi.org/10.1007/s10551-018-3921-3))
    also proposes a specific framework for algorithms. This book won’t delve into
    algorithm design aspects too much, but for those readers interested in the larger
    umbrella of ethical AI, this article is an excellent place to start.
  prefs: []
  type: TYPE_NORMAL
- en: Organizations should take the ethics of algorithmic decision-making seriously
    because ethical transgressions have monetary and reputation costs. But also, AI
    left to its own devices could undermine the very values that sustain democracy
    and the economy that allows businesses to thrive.
  prefs: []
  type: TYPE_NORMAL
- en: More profitable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As seen already in this section, interpretability improves algorithmic decisions,
    boosting trust and mitigating ethical transgressions.
  prefs: []
  type: TYPE_NORMAL
- en: When we leverage previously unknown opportunities and mitigate threats such
    as accidental failures through better decision-making, we are likely to improve
    the bottom line; and if we increase trust in an AI-powered technology, we are
    likely to increase its use and enhance overall brand reputation, which also has
    a beneficial impact on profits. On the other hand, ethical transgressions can
    occur by design or by accident, and when they are discovered, they adversely impact
    both profits and reputation.
  prefs: []
  type: TYPE_NORMAL
- en: When businesses incorporate interpretability into their machine learning workflows,
    it’s a virtuous cycle, and it results in higher profitability. In the case of
    a non-profit or government, profits might not be a motive. Still, finances are
    undoubtedly involved because lawsuits, lousy decision-making, and tarnished reputations
    are expensive. Ultimately, technological progress is contingent not only on the
    engineering and scientific skills and materials that make it possible but its
    voluntary adoption by the general public.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has shown us what machine learning interpretation is and what it
    is not, and the importance of interpretability. In the next chapter, we will learn
    what can make machine learning models so challenging to interpret, and how you
    would classify interpretation methods in both category and scope.
  prefs: []
  type: TYPE_NORMAL
- en: Image sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Martin, K. (2019). *Ethical Implications and Accountability of Algorithms*.
    Journal of Business Ethics 160\. 835–850\. [https://doi.org/10.1007/s10551-018-3921-3](https://doi.org/10.1007/s10551-018-3921-3
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statistics Online Computational Resource, University of Southern California.
    (1993). Growth Survey of 25,000 children from birth to 18 years of age recruited
    from Maternal and Child Health Centers. Originally retrieved from [http://www.socr.ucla.edu/](http://www.socr.ucla.edu/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lipton, Zachary (2017). *The Mythos of Model Interpretability*. ICML 2016 Human
    Interpretability in Machine Learning Workshop: [https://doi.org/10.1145/3236386.3241340](https://doi.org/10.1145/3236386.3241340)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roscher, R., Bohn, B., Duarte, M.F. & Garcke, J. (2020). *Explainable Machine
    Learning for Scientific Insights and Discoveries*. IEEE Access, 8, 42200-42216:
    [https://dx.doi.org/10.1109/ACCESS.2020.2976199](https://dx.doi.org/10.1109/ACCESS.2020.2976199)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Doshi-Velez, F. & Kim, B. (2017). *Towards A Rigorous Science of Interpretable
    Machine Learning*: [http://arxiv.org/abs/1702.08608](http://arxiv.org/abs/1702.08608)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arrieta, A.B., Diaz-Rodriguez, N., Ser, J.D., Bennetot, A., Tabik, S., Barbado,
    A., Garc’ia, S., Gil-L’opez, S., Molina, D., Benjamins, R., Chatila, R., & Herrera,
    F. (2020). *Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities
    and Challenges toward Responsible AI*: [https://arxiv.org/abs/1910.10045](https://arxiv.org/abs/1910.10045)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coglianese, C. & Lehr, D. (2019). *Transparency and algorithmic governance*.
    Administrative Law Review, 71, 1-4: [https://ssrn.com/abstract=3293008](https://ssrn.com/abstract=3293008)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weller, Adrian. (2019) *Transparency: Motivations and Challenges*. arXiv:1708.01870
    [Cs]: [http://arxiv.org/abs/1708.01870](http://arxiv.org/abs/1708.01870)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/inml](Chapter_1.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code107161072033138125.png)'
  prefs: []
  type: TYPE_IMG
