- en: Introduction - Machine Learning and Statistical Science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning has definitely been one of the most talked about fields in
    recent years, and for good reason. Every day new applications and models are discovered,
    and researchers around the world announce impressive advances in the quality of
    results on a daily basis.
  prefs: []
  type: TYPE_NORMAL
- en: Each day, many new practitioners decide to take courses and search for introductory
    materials so they can employ these newly available techniques that will improve
    their applications. But in many cases, the whole corpus of machine learning, as
    normally explained in the literature, requires a good understanding of mathematical
    concepts as a prerequisite, thus imposing a high bar for programmers who typically
    have good algorithmic skills but are less familiar with higher mathematical concepts.
  prefs: []
  type: TYPE_NORMAL
- en: This first chapter will be a general introduction to the field, covering the
    main study areas of machine learning, and will offer an overview of the basic
    statistics, probability, and calculus, accompanied by source code examples in
    a way that allows you to experiment with the provided formulas and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this first chapter, you will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is machine learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning areas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elements of statistics and probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elements of calculus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The world around us provides huge amounts of data. At a basic level, we are
    continually acquiring and learning from text, image, sound, and other types of
    information surrounding us. The availability of data, then, is the first step
    in the process of acquiring the skills to perform a task.
  prefs: []
  type: TYPE_NORMAL
- en: A myriad of computing devices around the world collect and store an overwhelming
    amount of information that is image-, video-, and text-based. So, the raw material
    for learning is clearly abundant, and it's available in a format that a computer
    can deal with.
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s the starting point for the rise of the discipline discussed in this
    book: the study of techniques and methods allowing computers to learn from data
    without being explicitly programmed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A more formal definition of machine learning, from *Tom Mitchell*, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"A computer program is said to learn from experience E with respect to some
    class of tasks T and performance measure P, if its performance at tasks in T,
    as measured by P, improves with experience E."'
  prefs: []
  type: TYPE_NORMAL
- en: 'This definition is complete, and reinstates the elements that play a role in
    every machine learning project: the task to perform, the successive experiments,
    and a clear and appropriate performance measure. In simpler words, we have a program
    that improves how it performs a task based on experience and guided by a certain
    criterion.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning in the bigger picture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning as a discipline is not an isolated field—it is framed inside
    a wider domain, **Artificial Intelligence** (**AI**). But as you can guess, machine
    learning didn''t appear from the void. As a discipline it has its predecessors,
    and it has been evolving in stages of increasing complexity in the following four
    clearly differentiated steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The first model of machine learning involved rule-based decisions and a simple
    level of data-based algorithms that includes in itself, and as a prerequisite,
    all the possible ramifications and decision rules, implying that all the possible
    options will be hardcoded into the model beforehand by an expert in the field.
    This structure was implemented in the majority of applications developed since
    the first programming languages appeared in 1950\. The main data type and function
    being handled by this kind of algorithm is the Boolean, as it exclusively dealt
    with yes or no decisions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During the second developmental stage of statistical reasoning, we started to
    let the probabilistic characteristics of the data have a say, in addition to the
    previous choices set up in advance. This better reflects the fuzzy nature of real-world
    problems, where outliers are common and where it is more important to take into
    account the nondeterministic tendencies of the data than the rigid approach of
    fixed questions. This discipline adds to the mix of mathematical tools elements
    of **Bayesian probability theory**. Methods pertaining to this category include
    curve fitting (usually of linear or polynomial), which has the common property
    of working with numerical data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The machine learning stage is the realm in which we are going to be working
    throughout this book, and it involves more complex tasks than the simplest Bayesian
    elements of the previous stage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The most outstanding feature of machine learning algorithms is that they can
    generalize models from data but the models are capable of generating their own
    feature selectors, which aren't limited by a rigid target function, as they are
    generated and defined as the training process evolves. Another differentiator
    of this kind of model is that they can take a large variety of data types as input,
    such as speech, images, video, text, and other data susceptible to being represented
    as vectors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'AI is the last step in the scale of abstraction capabilities that, in a way,
    include all previous algorithm types, but with one key difference: AI algorithms
    are able to apply the learned knowledge to solve tasks that had never been considered
    during training. The types of data with which this algorithm works are even more
    generic than the types of data supported by machine learning, and they should
    be able, by definition, to transfer problem-solving capabilities from one data
    type to another, without a complete retraining of the model. In this way, we could
    develop an algorithm for object detection in black and white images and the model
    could abstract the knowledge to apply the model to color images.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following diagram, we represent these four stages of development towards
    real AI applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b800807-4849-42df-8074-74bb08da83a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Types of machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s try to dissect the different types of machine learning project, starting
    from the grade of previous knowledge from the point of view of the implementer.
    The project can be of the following types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: In this type of learning, we are given a sample set
    of real data, accompanied by the result the model should give us after applying
    it. In statistical terms, we have the outcome of all the training set experiments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning:** This type of learning provides only the sample data
    from the problem domain, but the task of grouping similar data and applying a
    category has no previous information from which it can be inferred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning:** This type of learning doesn''t have a labeled sample
    set and has a different number of participating elements, which include an agent,
    an environment, and learning an optimum policy or set of steps, maximizing a goal-oriented
    approach by using rewards or penalties (the result of each attempt).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4e5dfbd-6f8a-470b-a6aa-753885eb15bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Main areas of Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: Grades of supervision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The learning process supports gradual steps in the realm of supervision:'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning doesn't have previous knowledge of the class or value
    of any sample, it should infer it automatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semi-Supervised Learning, needs a seed of known samples, and the model infers
    the remaining samples class or value from that seed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Supervised Learning: This approach normally includes a set of known samples,
    called training set, another set used to validate the model''s generalization,
    and a third one, called test set, which is used after the training process to
    have an independent number of samples outside of the training set, and warranty 
    independence of testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following diagram, depicts the mentioned approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3605a99e-96d0-45ba-b54e-94b060b405c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphical depiction of the training techniques for Unsupervised, Semi-Supervised
    and Supervised Learning
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning strategies - regression versus classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This type of learning has the following two main types of problem to solve:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression problem**: This type of problem accepts samples from the problem
    domain and, after training the model, minimizes the error by comparing the output
    with the real answers, which allows the prediction of the right answer when given
    a new unknown sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification problem**: This type of problem uses samples from the domain
    to assign a label or group to new unknown samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised problem solving–clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The vast majority of unsupervised problem solving consist of grouping items
    by looking at similarities or the value of shared features of the observed items,
    because there is no certain information about the *a* *priori* classes. This type
    of technique is called clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Outside of these main problem types, there is a mix of both, which is called
    semi-supervised problem solving, in which we can train a labeled set of elements
    and also use inference to assign information to unlabeled data during training
    time. To assign data to unknown entities, three main criteria are used—smoothness
    (points close to each other are of the same class), cluster (data tends to form
    clusters, a special case of smoothness), and manifold (data pertains to a manifold
    of much lower dimensionality than the original domain).
  prefs: []
  type: TYPE_NORMAL
- en: Tools of the trade–programming language and libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As this book is aimed at developers, we think that the approach of explaining
    the mathematical concepts using real code comes naturally.
  prefs: []
  type: TYPE_NORMAL
- en: When choosing the programming language for the code examples, the first approach
    was to use multiple technologies, including some cutting-edge libraries. After
    consulting the community, it was clear that a simple language would be preferable
    when explaining the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Among the options, the ideal candidate would be a language that is simple to
    understand, with real-world machine learning adoption, and that is also relevant.
  prefs: []
  type: TYPE_NORMAL
- en: The clearest candidate for this task was Python, which fulfils all these conditions,
    and especially in the last few  years has become the go-to language for machine
    learning, both for newcomers and professional practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following graph, we compare the previous star in the machine learning
    programming language field, R, and we can clearly conclude the huge, favorable
    tendency towards using Python. This means that the skills you acquire in this
    book will be relevant now and in the foreseeable future:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/508009c9-5f18-4375-9acb-d3773b6406be.png)'
  prefs: []
  type: TYPE_IMG
- en: Interest graph for R and Python in the Machine Learning realm.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to Python code, we will have the help of a number of the most well-known
    numerical, statistical, and graphical libraries in the Python ecosystem, namely
    pandas, NumPy, and matplotlib. For the **deep neural network** examples, we will
    use the Keras library, with TensorFlow as the backend.
  prefs: []
  type: TYPE_NORMAL
- en: The Python language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is a general-purpose scripting language, created by the Dutch programmer
    Guido Van Rossum in 1989\. It possesses a very simple syntax with great extensibility,
    thanks to its numerous extension libraries, making it a very suitable language
    for prototyping and general coding. Because of its native C bindings, it can also
    be a candidate for production deployment.
  prefs: []
  type: TYPE_NORMAL
- en: The language is actually used in a variety of areas, ranging from web development
    to scientific computing, in addition to its use as a general scripting tool.
  prefs: []
  type: TYPE_NORMAL
- en: The NumPy library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we had to choose a definitive must-use library for use in this book, and
    a non-trivial mathematical application written in Python, it would have to be
    NumPy. This library will help us implement applications using statistics and linear
    algebra routines with the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: A versatile and performant N-dimensional array object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many mathematical functions that can be applied to these arrays in a seamless
    manner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear algebra primitives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random number distributions and a powerful statistics package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compatibility with all the major machine learning packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NumPy library will be used extensively throughout this book, using many
    of its primitives to simplify the concept explanations with code.
  prefs: []
  type: TYPE_NORMAL
- en: The matplotlib library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data plotting is an integral part of data science and is normally the first
    step an analyst performs to get a sense of what's going on in the provided set
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, we need a very powerful library to be able to graph the input
    data, and also to represent the resulting output. In this book, we will use Python's matplotlib
    library to describe concepts and the results from our models.
  prefs: []
  type: TYPE_NORMAL
- en: What's matplotlib?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Matplotlib is an extensively used plotting library, especially designed for
    2D graphs. From this library, we will focus on using the `pyplot` module, which
    is a part of the API of matplotlib and has MATLAB-like methods, with direct NumPy
    support. For those of you not familiar with MATLAB, it has been the default mathematical
    notebook environment for the scientific and engineering fields for decades.
  prefs: []
  type: TYPE_NORMAL
- en: The method described will be used to illustrate a large proportion of the concepts
    involved, and in fact, the reader will be able to generate many of the examples
    in this book with just these two libraries, and using the provided code.
  prefs: []
  type: TYPE_NORMAL
- en: Pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pandas** complements the previously mentioned libraries with a special structure,
    called `DataFrame`, and also adds many statistical and data mangling methods,
    such as I/O, for many different formats, such as slicing, subsetting, handling
    missing data, merging, and reshaping, among others.'
  prefs: []
  type: TYPE_NORMAL
- en: The `DataFrame` object is one of the most useful features of the whole library,
    providing a special 2D data structure with columns that can be of different data
    types. Its structure is very similar to a database table, but immersed in a flexible
    programming runtime and ecosystem, such as SciPy. These data structures are also
    compatible with NumPy matrices, so we can also apply high-performance operations
    to the data with minimal effort.
  prefs: []
  type: TYPE_NORMAL
- en: SciPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**SciPy** is a stack of very useful scientific Python libraries, including
    NumPy, pandas, matplotlib, and others, but it also the core library of the ecosystem,
    with which we can also perform many additional fundamental mathematical operations,
    such as integration, optimization, interpolation, signal processing, linear algebra,
    statistics, and file I/O.'
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Jupyter** is a clear example of a successful Python-based project, and it''s
    also one of the most powerful devices we will employ to explore and understand
    data through code.'
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter notebooks are documents consisting of intertwined cells of code, graphics,
    or formatted text, resulting in a very versatile and powerful research environment.
    All these elements are wrapped in a convenient web interface that interacts with
    the **IPython** interactive interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a Jupyter notebook is loaded, the whole environment and all the variables
    are in memory and can be changed and redefined, allowing research and experimentation,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/000a5a89-87d2-4089-858f-1acec71d359a.png)'
  prefs: []
  type: TYPE_IMG
- en: Jupyter notebook
  prefs: []
  type: TYPE_NORMAL
- en: This tool will be an important part of this book's teaching process, because
    most of the Python examples will be provided in this format. In the last chapter
    of the book, you will find the full installation instructions.
  prefs: []
  type: TYPE_NORMAL
- en: After installing, you can cd into the directory where your notebooks reside,
    and then call Jupyter by typing `jupyter notebook`
  prefs: []
  type: TYPE_NORMAL
- en: Basic mathematical concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the previous sections, this main target audience of the book is
    developers who want to understand machine learning algorithms. But in order to
    really grasp the motivations and reason behind them, it's necessary to review
    and build all the fundamental reasoning, which includes statistics, probability,
    and calculus.
  prefs: []
  type: TYPE_NORMAL
- en: We will first start with some of the fundamentals of statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics - the basic pillar of modeling uncertainty
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statistics can be defined as a discipline that uses data samples to extract
    and support conclusions about larger samples of data. Given that machine learning
    comprises a big part of the study of the properties of data and the assignment
    of values to data, we will use many statistical concepts to define and justify
    the different methods.
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive statistics - main operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following sections, we will start defining the fundamental operations
    and measures of the discipline of statistics in order to be able to advance from
    the fundamental concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Mean
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is one of the most intuitive and most frequently used concepts in statistics.
    Given a set of numbers, the mean of that set is the sum of all the elements divided
    by the number of elements in the set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula that represents the mean is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2acf8cde-aeac-4554-b571-448bd799d28d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Although this is a very simple concept, we will write a Python code sample
    in which we will create a sample set, represent it as a line plot, and mark the
    mean of the whole set as a line, which should be at the weighted center of the
    samples. It will serve as an introduction to Python syntax, and also as a way
    of experimenting with Jupyter notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This program will output a time series of the dataset elements, and will then
    draw a line at the mean height.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the following graph shows, the mean is a succinct (one value) way of describing
    the tendency of a sample set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d908460-8057-473d-a6b5-a311ad66e608.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this first example, we worked with a very homogeneous sample set, so the
    mean is very informative regarding its values. But let''s try the same sample
    with a very dispersed sample set (you are encouraged to play with the values too):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03c8e0c7-6e83-4148-91a1-950e3a2cf8ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the first example, the mean isn't sufficient to describe non-homogeneous
    or very dispersed samples.
  prefs: []
  type: TYPE_NORMAL
- en: In order to add a unique value describing how dispersed the sample set's values
    are, we need to look at the concept of variance, which needs the mean of the sample
    set as a starting point, and then averages the distances of the samples from the
    provided mean. The greater the variance, the more scattered the sample set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The canonical definition of variance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1539e143-9244-4fbc-8b57-d4cd167b3d52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s write the following sample code snippet to illustrate this concept,
    adopting the previously used libraries. For the sake of clarity, we are repeating
    the declaration of the `mean` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the variance of the second set was much higher, given the really
    dispersed values. The fact that we are computing the mean of the squared distance
    helps to really outline the differences, as it is a quadratic operation.
  prefs: []
  type: TYPE_NORMAL
- en: Standard deviation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Standard deviation is simply a means of regularizing the square nature of the
    mean square used in the variance, effectively linearizing this term. This measure
    can be useful for other, more complex operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the official form of standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c008034-4bb8-42ac-b591-761cca2cc4b6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Probability and random variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now about to study the single most important discipline required for
    understanding all the concepts of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '**Probability** is a mathematical discipline, and its main occupation is the
    study of random events. In a more practical definition, probability normally tries
    to quantify the level of certainty (or conversely, uncertainty) associated with
    an event, from a universe of possible occurrences.'
  prefs: []
  type: TYPE_NORMAL
- en: Events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to understand probabilities, we first need to define events. An event
    is, given an experiment in which we perform a determined action with different
    possible results, a subset of all the possible outcomes for that experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of events are a particular dice number appearing, and a product defect
    of particular type appearing on an assembly line.
  prefs: []
  type: TYPE_NORMAL
- en: Probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following the previous definitions, probability is the likelihood of the occurrence
    of an event. Probability is quantified as a real number between *0* and *1*, and
    the assigned probability *P* increases towards *1* when the likelihood of the
    event occurring increases.
  prefs: []
  type: TYPE_NORMAL
- en: The mathematical expression for the probability of the occurrence of an event
    is `P(E)`.
  prefs: []
  type: TYPE_NORMAL
- en: Random variables and distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When assigning event probabilities, we could also try to cover the entire sample
    and assign one probability value to each of the possible outcomes for the sample
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: This process does indeed have all the characteristics of a function, and thus
    we will have a random variable that will have a value for each one of the possible
    event outcomes. We will call this function a random function.
  prefs: []
  type: TYPE_NORMAL
- en: 'These variables can be of the following two types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discrete**: If the number of outcomes is finite, or countably infinite'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous**: If the outcome set belongs to a continuous interval'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This probability function is also called **probability distribution**.
  prefs: []
  type: TYPE_NORMAL
- en: Useful probability distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Between the multiple possible probability distributions, there are a number
    of functions that have been studied and analyzed for their special properties,
    or the popular problems they represent.
  prefs: []
  type: TYPE_NORMAL
- en: We will describe the most common ones that have a special effect on the development
    of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Bernoulli distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin with a simple distribution: one that has a binary outcome, and
    is very much like tossing a (fair) coin.'
  prefs: []
  type: TYPE_NORMAL
- en: This distribution represents a single event that takes the value *1* (let's
    call this *heads*) with a probability of *p*, and *0* (lets call this *tails*),
    with probability *1-p*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to visualize this, let''s generate a large number of events of a Bernoulli
    distribution using `np` and graph the tendency of this distribution, with the
    following only two possible outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the binomial distribution, through an histogram,
    showing the complementary nature of the outcomes'' probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90ea2658-e894-4b1f-9016-ad72cc77407b.png)'
  prefs: []
  type: TYPE_IMG
- en: Binomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here we see the very clear tendency of the complementing probabilities
    of the possible outcomes. Now let''s complement the model with a larger number
    of possible outcomes. When their number is greater than 2, we are talking about
    a **multinomial distribution**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7cc00694-f4b0-45f0-a2d8-6554080d226e.png)'
  prefs: []
  type: TYPE_IMG
- en: Multinomial distribution with 100 possible outcomes
  prefs: []
  type: TYPE_NORMAL
- en: Uniform distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This very common distribution is the first continuous distribution that we will
    see. As the name implies, it has a constant probability value for any interval
    of the domain.
  prefs: []
  type: TYPE_NORMAL
- en: In order to integrate to 1, *a* and *b* being the extreme of the function, this
    probability has the value of *1/(b-a)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s generate a plot with a sample uniform distribution using a very regular
    histogram, as generated by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Take look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/755f4b86-a37b-4c05-a2e5-93f5ea2342ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Uniform distribution
  prefs: []
  type: TYPE_NORMAL
- en: Normal distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This very common continuous random function, also called a **Gaussian** **function**,
    can be defined with the simple metrics of the mean and the variance, although
    in a somewhat complex form.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the canonical form of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42c42ced-bec7-4dea-a3dc-c6e1b45db226.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the generated distribution''s histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bf6938b-884e-466b-8614-8a4e8e50e1f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Normal distribution
  prefs: []
  type: TYPE_NORMAL
- en: Logistic distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This distribution is similar to the normal distribution, but with the morphological
    difference of having a more elongated tail. The main importance of this distribution
    lies in its **cumulative distribution function** (**CDF**), which we will be using
    in the following chapters, and will certainly look familiar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first represent the base distribution by using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2c191e4-ddfa-4cb1-97f0-0998f23f549b.png)'
  prefs: []
  type: TYPE_IMG
- en: Logistic (red) vs Normal (blue) distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, as mentioned before, let''s compute the CDF of the logistic distribution
    so that you will see a very familiar figure, the **s****igmoid** curve, which
    we will see again when we review neural network activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ec70b7c-685f-415f-8b2f-5c9d93534b79.png)'
  prefs: []
  type: TYPE_IMG
- en: Inverse of the logistic distribution
  prefs: []
  type: TYPE_NORMAL
- en: Statistical measures for probability functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see the most common statistical measures that can be
    applied to probabilities. The first measures are the mean and variance, which
    do not differ from the definitions we saw in the introduction to statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Skewness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This measure represents the lateral deviation, or in general terms, the deviation
    from the center, or the symmetry (or lack thereof) of a probability distribution.
    In general, if skewness is negative, it implies a deviation to the right, and
    if it is positive, it implies a deviation to the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e8d2dbb-28e5-451c-988a-5b53640d8eb0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following diagram, which depicts the skewness statistical
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d684c1d-41fe-4475-a68e-60369d3965b7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Depiction of the how the distribution shape influences Skewness.
  prefs: []
  type: TYPE_NORMAL
- en: Kurtosis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Kurtosis** gives us an idea of the central concentration of a distribution,
    defining how acute the central area is, or the reverse—how distributed the function''s
    tail is.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for kurtosis is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f87d984-ff69-4dea-8389-8830f0c69fee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following diagram, we can clearly see how the new metrics that we are
    learning can be intuitively understood:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c2ec98-3b71-4319-907f-03a7a2880f8a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Depiction of the how the distribution shape influences Kurtosis
  prefs: []
  type: TYPE_NORMAL
- en: Differential calculus elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To cover the minimum basic knowledge of machine learning, especially the learning
    algorithms such as gradient descent, we will introduce you to the concepts involved
    in differential calculus.
  prefs: []
  type: TYPE_NORMAL
- en: Preliminary knowledge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Covering the calculus terminology necessary to get to gradient descent theory
    would take many chapters, so we will assume you have an understanding of the concepts
    of the properties of the most well-known continuous functions, such as **linear**,
    **quadratic**, **logarithmic**, and **exponential**, and the concept of **limit**.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of clarity, we will develop the concept of the functions of one
    variable, and then expand briefly to cover multivariate functions.
  prefs: []
  type: TYPE_NORMAL
- en: In search of changes–derivatives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We established the concept of functions in the previous section. With the exception
    of constant functions defined in the entire domain, all functions have some sort
    of value dynamics. That means that *f(x1)* is different than *f(x2)* for some
    determined values of *x*.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of differential calculus is to measure change. For this specific
    task, many mathematicians of the 17th century (Leibniz and Newton were the most
    prominent exponents) worked hard to find a simple model to measure and predict
    how a symbolically defined function changed over time.
  prefs: []
  type: TYPE_NORMAL
- en: This research guided the field to one wonderful concept—a symbolic result that,
    under certain conditions, tells you how much and in which direction a function
    changes at a certain point. This is the concept of a derivative.
  prefs: []
  type: TYPE_NORMAL
- en: Sliding on the slope
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we want to measure how a function changes over time, the first intuitive
    step would be to take the value of a function and then measure it at the subsequent
    point. Subtracting the second value from the first would give us an idea of how
    much the function changes over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code example, we first defined a sample quadratic equation
    (`2*x²`*)* and then defined the part of the domain in which we will work with
    the `arange` function (from `0` to `0.5`, in `0.1` steps).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we define an interval for which we measure the change of *y* over *x*,
    and draw lines indicating this measurement, as shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7432239b-c655-45a9-815a-2c294b102e5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial depiction of a starting setup for implementing differentiation
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we measure the function at *x=1* and *x=4*, and define the rate
    of change for this interval as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61be1ece-dea2-4591-b83d-236daf453cb7.png)'
  prefs: []
  type: TYPE_IMG
- en: Applying the formula, the result for the sample is *(36-0)/3= 12*.
  prefs: []
  type: TYPE_NORMAL
- en: This initial approach can serve as a way of approximately measuring this dynamic,
    but it's too dependent on the points at which we take the measurement, and it
    has to be taken at every interval we need.
  prefs: []
  type: TYPE_NORMAL
- en: To have a better idea of the dynamics of a function, we need to be able to define
    and measure the instantaneous change rate at every point in the function's domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'This idea of instantaneous change brings to us the need to reduce the distance
    between the domain''s *x* values, taken at a point where there are very short
    distances between them. We will formulate this approach with an initial value
    *x,* and the subsequent value, *x + Δx*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d6ebb35-bc64-486f-a2b1-4db589e91d53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following code, we approximate the difference, reducing *Δx* progressively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we first defined an initial delta, which brought an
    initial approximation. Then, we apply the difference function, with diminishing
    values of delta, thanks us to powering `0.1` with incremental powers. The results
    we get are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As the separation diminishes, it becomes clear that the change rate will hover
    around `4`. But when does this process stop? In fact, we could say that this process
    can be followed ad infinitum, at least in a numeric sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is when the concept of limit intuitively appears. We will then define
    this process, of making Δ indefinitely smaller, and will call it the derivative
    of *f(x)* or *f''(x)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0827170-c744-4e24-b8ee-17285eaa90e8.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the formal definition of the derivative.
  prefs: []
  type: TYPE_NORMAL
- en: But mathematicians didn't stop with these tedious calculations, making a large
    number of numerical operations (which were mostly done manually of the 17th century),
    and wanted to further simplify these operations.
  prefs: []
  type: TYPE_NORMAL
- en: '*What if we perform another step that can symbolically define the derivative
    of a function?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'That would require building a function that gives us the derivative of the
    corresponding function, just by replacing the `x` variable value. That huge step
    was also reached in the 17th century, for different function families, starting
    with the parabolas *(y=x²+b)*, and following with more complex functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93726066-60a0-42c5-b785-d619a8c0c91e.png)'
  prefs: []
  type: TYPE_IMG
- en: Chain rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One very important result of the symbolic determination of a function's derivative
    is the chain rule. This formula, first mentioned in a paper by Leibniz in 1676,
    made it possible to solve the derivatives of composite functions in a very simple
    and elegant manner, simplifying the solution for very complex functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to define the chain rule, if we suppose a function *f*, which is defined
    as a function of another function *g*, *f(g(x))* of *F*, the derivative can be
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd8487a6-80c2-4a93-a643-584234bbea8f.png)'
  prefs: []
  type: TYPE_IMG
- en: The formula of the chain rule allows us to differentiate formulas whose input
    values depend on another function. This is the same as searching the rate of change
    of a function that is linked to a previous one. The chain rule is one of the main
    theoretical concepts employed in the training phase of neural networks, because
    in those layered structures, the output of the first neuron layers will be the
    inputs of the following, giving, as a result, a composite function that, most
    of the time, is of more than one nesting level.
  prefs: []
  type: TYPE_NORMAL
- en: Partial derivatives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now we've been working with univariate functions, but the type of function
    we will mostly work with from now on will be multivariate, as the dataset will
    contain much more than one column and each one of them will represent a different
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, we will need to know how the function changes in a relationship
    with only one dimension, which will involve looking at how one column of the dataset
    contributes to the total number of function changes.
  prefs: []
  type: TYPE_NORMAL
- en: The calculation of partial derivatives consists of applying the already known
    derivation rules to the multivariate function, considering the variables are not
    being derived as constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following power rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f(x,y) = 2x³y*'
  prefs: []
  type: TYPE_NORMAL
- en: 'When differentiating this function with respect to *x*, considering *y* a constant,
    we can rewrite it as *3 . 2 y x²*, and applying the derivative to the variable
    *x* allows us to obtain the following derivative:'
  prefs: []
  type: TYPE_NORMAL
- en: '*d/dx (f(x,y)) = 6y*x²*'
  prefs: []
  type: TYPE_NORMAL
- en: Using these techniques, we can proceed with the more complex multivariate functions,
    which will be part of our feature set, normally consisting of much more than two
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we worked through many different conceptual elements, including
    an overview of some basic mathematical concepts, which serve as a base for the
    machine learning concepts.
  prefs: []
  type: TYPE_NORMAL
- en: These concepts will be useful when we formally explain the mechanisms of the
    different modeling methods, and we encourage you to improve your understanding
    of them as much as possible, before and while reading the chapters, to better
    grasp how the algorithm works.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will have a quick overview of the the the complete workflow
    of a machine learning project, which will help us to understand the various elements
    involved, from data gathering to result evaluation.
  prefs: []
  type: TYPE_NORMAL
