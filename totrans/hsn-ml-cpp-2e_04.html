<html><head></head><body>
		<div id="_idContainer167">
			<h1 class="chapter-number" id="_idParaDest-83"><a id="_idTextAnchor228"/>4</h1>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor229"/>Clustering</h1>
			<p><strong class="bold">Clustering</strong> is an unsupervised machine<a id="_idIndexMarker428"/> learning method that’s used for splitting the original dataset of objects into groups classified by properties. In <strong class="bold">machine learning</strong>, an object is typically represented <a id="_idIndexMarker429"/>as a point in a multidimensional metric space. Every space dimension corresponds to an object property (feature), and the metric is a function of the values of these properties. Depending on the types of dimensions in this space, which can be both numerical and categorical, we choose a type of clustering algorithm and specific metric function. This choice depends on the nature of different object <span class="No-Break">properties’ types.</span></p>
			<p>At the present stage, clustering is often used as the first step in data analysis. The task of clustering was formulated in scientific areas such as statistics, pattern recognition, optimization, and machine learning. At the time of writing, the number of methods for partitioning groups of objects into clusters is quite large—several dozen algorithms, and even more when you take into account their <span class="No-Break">various modifications.</span></p>
			<p>The following topics will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Measuring distance <span class="No-Break">in clustering</span></li>
				<li>Types of <span class="No-Break">clustering algorithms</span></li>
				<li>Examples of using the <strong class="source-inline">mlpack</strong> library for dealing with the clustering <span class="No-Break">task samples</span></li>
				<li>Examples of using the <strong class="source-inline">Dlib</strong> library for dealing with the clustering <span class="No-Break">task samples</span></li>
				<li>Plotting data <span class="No-Break">with C++</span></li>
			</ul>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor230"/>Technical requirements</h1>
			<p>You’ll require the following technologies and installations to complete <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>A modern C++ compiler with <span class="No-Break">C++17 support</span></li>
				<li>CMake build system version &gt;= <span class="No-Break">3.8</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">Dlib</strong></span><span class="No-Break"> library</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">mlpack</strong></span><span class="No-Break"> library</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">plotcpp</strong></span><span class="No-Break"> library</span></li>
			</ul>
			<p>The code files for this chapter can be found in this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter04"><span class="No-Break">https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter04</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor231"/>Measuring distance in clustering</h1>
			<p>A metric or distance measure is essential in clustering because it determines the similarity between objects. However, before applying a distance measure to objects, we must make a vector of object characteristics; usually, this is a set of numerical values, such as human height or weight. Also, some <a id="_idIndexMarker430"/>algorithms can work with categorical object features (or characteristics). The standard practice is to normalize feature values. Normalization ensures that each feature has the same impact in a distance measure calculation. Many distance measure functions can be used in the scope of the clustering task. The most popular ones that are used for numerical properties are <strong class="bold">Euclidean distance</strong>, <strong class="bold">squared Euclidean distance</strong>, <strong class="bold">Manhattan distance</strong>, and <strong class="bold">Chebyshev distance</strong>. The following subsections describe them <span class="No-Break">in detail.</span></p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor232"/>Euclidean distance</h2>
			<p>Euclidean distance is the most <a id="_idIndexMarker431"/>widely used distance measure. In general, this is a geometric distance in the multidimensional space. The formula for Euclidean distance is <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer123">
					<img alt="" role="presentation" src="image/B19849_Formula_012.jpg"/>
				</div>
			</div>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor233"/>Squared Euclidean distance</h2>
			<p>Squared Euclidean distance has the same properties as Euclidean distance but assigns greater significance (weight) to <a id="_idIndexMarker432"/>the distant values than to closer ones. Here’s the formula for squared <span class="No-Break">Euclidean distance:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer124">
					<img alt="" role="presentation" src="image/B19849_Formula_021.jpg"/>
				</div>
			</div>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor234"/>Manhattan distance</h2>
			<p>Manhattan distance is an average difference by coordinates. In most cases, its value gives the same clustering results <a id="_idIndexMarker433"/>as Euclidean distance. However, it reduces the significance (weight) of the distant values (outliers). Here’s the formula for <span class="No-Break">Manhattan distance:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer125">
					<img alt="" role="presentation" src="image/B19849_Formula_031.jpg"/>
				</div>
			</div>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor235"/>Chebyshev distance</h2>
			<p>Chebyshev distance can be useful <a id="_idIndexMarker434"/>when we need to classify two objects as different when they differ only by one of the coordinates. Here’s the formula for <span class="No-Break">Chebyshev distance:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer126">
					<img alt="" role="presentation" src="image/B19849_Formula_041.jpg"/>
				</div>
			</div>
			<p>The following diagram shows the differences between the <span class="No-Break">various distances:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer127">
					<img alt="Figure 4.1 – The difference between different distance measures" src="image/B19849_04_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – The difference between different distance measures</p>
			<p>Here, we can see that <em class="italic">Manhattan</em> distance is the sum of the distances in both dimensions, like walking along city blocks. <em class="italic">Euclidean</em> distance is just the length of a straight line. <em class="italic">Chebyshev</em> distance<a id="_idIndexMarker435"/> is a more flexible alternative to <em class="italic">Manhattan</em> distance because diagonal moves are also taken <span class="No-Break">into account.</span></p>
			<p>In this section, we became familiar with the main clustering concept, which is a distance measure. In the following section, we’ll discuss various types of <span class="No-Break">clustering algorithms.</span></p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor236"/>Types of clustering algorithms</h1>
			<p>There are different types of clustering that we can classify into the following groups: <strong class="bold">partition-based</strong>, <strong class="bold">spectral</strong>, <strong class="bold">hierarchical</strong>, <strong class="bold">density-based</strong>, and <strong class="bold">model-based</strong>. The partition-based group of clustering algorithms can be logically divided into distance-based methods and ones<a id="_idIndexMarker436"/> based on <span class="No-Break">graph theory.</span></p>
			<p>Before we cover different types of clustering algorithms, let’s understand the main difference between clustering and classification. The main difference between the two is an undefined set of target groups, which is determined by the clustering algorithm. The set of target groups (clusters) is the <span class="No-Break">algorithm’s result.</span></p>
			<p>We can split cluster analysis into the <span class="No-Break">following phases:</span></p>
			<ul>
				<li>Selecting objects <span class="No-Break">for clustering</span></li>
				<li>Determining the set of object properties that we’ll use for <span class="No-Break">the metric</span></li>
				<li>Normalizing <span class="No-Break">property values</span></li>
				<li>Calculating <span class="No-Break">the metric</span></li>
				<li>Identifying distinct groups of objects based on <span class="No-Break">metric values</span></li>
			</ul>
			<p>After analyzing clustering<a id="_idIndexMarker437"/> results, some correction may be required for the selected metric of the <span class="No-Break">chosen algorithm.</span></p>
			<p>We can use clustering for various <a id="_idIndexMarker438"/>real-world tasks, including <span class="No-Break">the following:</span></p>
			<ul>
				<li>Splitting news into several categories <span class="No-Break">for advertisers</span></li>
				<li>Identifying customer groups by their preferences for <span class="No-Break">market analysis</span></li>
				<li>Identifying plant and animal groups for <span class="No-Break">biological studies</span></li>
				<li>Identifying and categorizing properties for city planning <span class="No-Break">and management</span></li>
				<li>Detecting earthquake epicenter clusters to identify <span class="No-Break">danger zones</span></li>
				<li>Categorizing groups of insurance policyholders for <span class="No-Break">risk management</span></li>
				<li>Categorizing books <span class="No-Break">in libraries</span></li>
				<li>Searching for hidden structural similarities in <span class="No-Break">the data</span></li>
			</ul>
			<p>With that, let’s dive into the different types of <span class="No-Break">clustering algorithms.</span></p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor237"/>Partition-based clustering algorithms</h2>
			<p>The partition-based methods use a similarity <a id="_idIndexMarker439"/>measure to combine objects into groups. A practitioner usually<a id="_idIndexMarker440"/> selects the similarity measure for such kinds of algorithms, using prior knowledge about a problem or heuristics to select the measure properly. Sometimes, several measures need to be tried with the same algorithm so that the best one can be chosen. Also, partition-based methods usually require either the number of desired clusters or a threshold that regulates the number of output clusters to be specified explicitly. The choice of a similarity measure can significantly affect the quality and <a id="_idIndexMarker441"/>accuracy of the clusters produced, potentially<a id="_idIndexMarker442"/> leading to misinterpretations of data patterns <span class="No-Break">and insights.</span></p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor238"/>Distance-based clustering algorithms</h2>
			<p>The most known representatives of this<a id="_idIndexMarker443"/> family of methods are the k-means and k-medoids algorithms. They take the <em class="italic">k</em> input parameter and divide the data space into <em class="italic">k</em> clusters so that the similarity between objects in one cluster is<a id="_idIndexMarker444"/> maximal. Also, they minimize the similarity between objects of different clusters. The similarity value is calculated as the distance from the object to the cluster center. The main difference between these methods lies in the way the cluster center <span class="No-Break">is defined.</span></p>
			<p>With the k-means algorithm, the similarity is proportional to the distance to the cluster center of mass. The cluster center of mass is the average value of cluster objects’ coordinates in the data space. The k-means algorithm can be briefly described with a few steps. First, we select <em class="italic">k</em> random objects and define each of them as a cluster prototype that represents the cluster’s center of mass. Then, the remaining objects are attached to the cluster with greater similarity. After that, the center of mass of each cluster is recalculated. For each obtained partition, a particular evaluation function is calculated, the values of which at each step form a converging series. This process continues until the specified series converges to its <span class="No-Break">limit value.</span></p>
			<p>In other words, moving objects from one cluster to another ends when the clusters remain unchanged. Minimizing the evaluation function allows the resulting clusters to be as compact and separate as possible. The k-means method works well when clusters are compact <em class="italic">clouds</em> that are significantly separated from each other. It’s useful for processing large amounts of data but isn’t applicable for detecting clusters of non-convex shapes or clusters with very different sizes. Moreover, the method is susceptible to noise and isolated points since even a small number of such points can significantly affect how the center mass of the cluster <span class="No-Break">is calculated.</span></p>
			<p>To reduce the influence of noise and isolated points on the clustering result, the k-medoids algorithm, in contrast to the k-means algorithm, uses one of the cluster objects (known as the representative object) as the center of the cluster. As in the k-means method, <em class="italic">k</em> representative objects are selected at random. Each of the remaining objects is combined into a cluster with the nearest representative object. Then, each representative object is replaced iteratively with an arbitrary unrepresentative object from the data space. The replacement process continues until the quality of the resulting clusters improves. The<a id="_idIndexMarker445"/> clustering quality is determined by the sum of deviations between objects <a id="_idIndexMarker446"/>and the representative object of the corresponding cluster, which the method tries to minimize. Thus, the iterations continue until the representative object in each of the clusters becomes <span class="No-Break">the medoid.</span></p>
			<p>The <strong class="bold">medoid</strong> is the object closest to the<a id="_idIndexMarker447"/> center of the cluster. The algorithm is poorly scalable for processing large amounts of data, but this problem is solved by the <strong class="bold">Clustering Large Applications based on RANdomized Search</strong> (<strong class="bold">CLARANS</strong>) algorithm, which<a id="_idIndexMarker448"/> complements the k-medoids method. CLARANS attempts to address scalability issues by using a randomized search technique to find good solutions more efficiently. Such an approach makes it possible to quickly converge on a good solution without exhaustively <a id="_idIndexMarker449"/>searching all possible combinations of medoids. For multidimensional clustering, the <strong class="bold">Projected Clustering</strong> (<strong class="bold">PROCLUS</strong>) algorithm can <span class="No-Break">be used.</span></p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor239"/>Graph theory-based clustering algorithms</h2>
			<p>The essence of algorithms <a id="_idIndexMarker450"/>based on graph theory is to represent target objects in graph form. Graph vertices correspond to objects, and the edge weights are equal to the distance between vertices. The advantages of graph <a id="_idIndexMarker451"/>clustering algorithms are their excellent visibility, relative ease of implementation, and their ability to make various improvements based on geometrical considerations. The main graph theory concepts used for clustering are selecting connected components, constructing a minimum spanning tree, and multilayer <span class="No-Break">graph clustering.</span></p>
			<p>The algorithm for selecting connected components is based on the <em class="italic">R </em>input parameter, and the algorithm removes all edges in the graph with distances greater than <em class="italic">R</em>. Only the closest pairs of objects remain connected. The algorithm’s goal is to find the <em class="italic">R</em> value at which the graph collapses into several connected components. The resulting components are clusters. To select the <em class="italic">R</em> parameter, a histogram of the distribution of pairwise distances is usually constructed. For problems with a well-defined cluster data structure, there will be two peaks in the histogram—one corresponds to in-cluster distances and the second to inter-cluster distances. The <em class="italic">R</em> parameter is selected from the minimum zone between these peaks. Managing the number of clusters using the distance threshold can <span class="No-Break">be difficult.</span></p>
			<p>The minimum spanning tree <a id="_idIndexMarker452"/>algorithm builds a minimal spanning tree on the graph, and then successively removes <a id="_idIndexMarker453"/>the edges with the highest weight. The following diagram shows the minimum spanning tree that’s been obtained for <span class="No-Break">nine objects:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer128">
					<img alt="Figure 4.2 – Spanning tree example" src="image/B19849_04_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Spanning tree example</p>
			<p>By removing the link between <em class="italic">C</em> and <em class="italic">D</em>, with a length of 6 units (the edge with the maximum distance), we obtain two clusters: <em class="italic">{A, B, C}</em> and <em class="italic">{D, E, F, G, H, I}</em>. We can divide the second cluster into two more clusters by removing the <em class="italic">EF</em> edge, which has a length of <span class="No-Break">4 units.</span></p>
			<p>The multilayer clustering algorithm is based on identifying connected components of a graph at some level of distance between objects (vertices). The threshold, <em class="italic">C, </em>defines the distance level—for example, if the distance between objects is <img alt="" role="presentation" src="image/B19849_Formula_051.png"/>, <span class="No-Break">then <img alt="" role="presentation" src="image/B19849_Formula_061.png"/>.</span></p>
			<p>The layer clustering algorithm generates a sequence of sub-graphs of the graph, <em class="italic">G</em>, that reflect the hierarchical <a id="_idIndexMarker454"/>relationships between clusters, <span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_071.png"/></span>, where the <span class="No-Break">following applies:</span></p>
			<ul>
				<li><span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_081.png"/></span>: A sub-graph on <span class="No-Break">the <img alt="" role="presentation" src="image/B19849_Formula_09.png"/>level</span></li>
				<li><img alt="" role="presentation" src="image/B19849_Formula_101.png"/></li>
				<li><span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_112.png"/></span>: The <em class="italic">t</em><span class="superscript">th</span> threshold <span class="No-Break">of distance</span></li>
				<li><img alt="" role="presentation" src="image/B19849_Formula_123.png"/>: The number of <span class="No-Break">hierarchy levels</span></li>
				<li><span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_131.png"/></span>, o: An empty set of graph edges, <span class="No-Break">when </span><span class="No-Break"><span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_142.png"/></span></span></li>
				<li><img alt="" role="presentation" src="image/B19849_Formula_151.png"/>: A graph of objects without thresholds on distance, <span class="No-Break">when </span><span class="No-Break"><span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_162.png"/></span></span></li>
			</ul>
			<p>By changing the <img alt="" role="presentation" src="image/B19849_Formula_171.png"/> distance<a id="_idIndexMarker455"/> thresholds, where <img alt="" role="presentation" src="image/B19849_Formula_181.png"/>, it’s possible to control the hierarchy depth of the resulting clusters. Thus, a multilayer clustering algorithm can create both flat and hierarchical <span class="No-Break">data partitioning.</span></p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor240"/>Spectral clustering algorithms</h2>
			<p>Spectral clustering refers to all methods that divide a set of data into clusters using the eigenvectors of the adjacency matrix of <a id="_idIndexMarker456"/>a graph or other matrices derived from it. An adjacency matrix describes a complete graph with vertices in objects and edges <a id="_idIndexMarker457"/>between each pair of objects with a weight corresponding to the degree of similarity between these vertices. Spectral clustering involves transforming the initial set of objects into a set of<a id="_idIndexMarker458"/> points in space whose coordinates are elements of eigenvectors. The formal name for such a task is the <strong class="bold">normalized </strong><span class="No-Break"><strong class="bold">cuts problem</strong></span><span class="No-Break">.</span></p>
			<p>The resulting set of points is then clustered using standard methods—for example, with the k-means algorithm. Changing the representation created by eigenvectors allows us to set the properties of the original set of clusters more clearly. Thus, spectral clustering can separate points that can’t be separated by applying k-means—for example, when the k-means method gets a convex set of points. The main disadvantage of spectral clustering is its cubic computational complexity and quadratic <span class="No-Break">memory requirements.</span></p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor241"/>Hierarchical clustering algorithms</h2>
			<p>Among the algorithms of hierarchical clustering, there are two main types: <strong class="bold">bottom-up</strong> and <strong class="bold">top-down-based algorithms</strong>. Top-down<a id="_idIndexMarker459"/> algorithms work on the principle that at the beginning, all objects are placed in one cluster, which is then divided into smaller and smaller clusters. Bottom-up algorithms <a id="_idIndexMarker460"/>are more common than top-down ones. They place each object in a separate cluster<a id="_idIndexMarker461"/> at the beginning of the work and then merge clusters into larger ones until all the objects in the dataset are contained in one cluster, building a system of nested partitions. The results of such algorithms are <a id="_idIndexMarker462"/>usually presented in tree form, called <a id="_idIndexMarker463"/>a <strong class="bold">dendrogram</strong>. A classic example of such a tree is the <em class="italic">Tree of Life</em>, which describes the classification of animals <span class="No-Break">and plants.</span></p>
			<p>The main problem with hierarchical methods is the difficulty of determining the stop condition in such a way as to isolate natural clusters and, at the same time, prevent their excessive splitting. Another problem with hierarchical clustering methods is choosing the point of separation or merging of clusters. This choice is critical because after splitting or merging clusters at each subsequent step, the method will operate only on newly formed clusters. Therefore, the wrong choice of a merge or split point at any step can lead to poor-quality clustering. Also, hierarchical methods can’t be applied to large datasets because deciding whether to divide or merge clusters requires a large number of objects and clusters to be analyzed, which leads to a significant computational complexity of <span class="No-Break">the method.</span></p>
			<p>There are several metrics or linkage<a id="_idIndexMarker464"/> criteria for cluster union that are used in hierarchical <span class="No-Break">clustering methods:</span></p>
			<ul>
				<li><strong class="bold">Single linkage (nearest neighbor distance)</strong>: In this method, the distance between the two clusters is determined by the distance between the two closest objects (nearest neighbors) in different clusters. The resulting clusters tend to <span class="No-Break">chain together.</span></li>
				<li><strong class="bold">Complete linkage (distance between the most distant neighbors)</strong>: In this method, the distances between clusters are determined by the largest distance between any two objects in different clusters (that is, the most distant neighbors). This method<a id="_idIndexMarker465"/> usually works very well when objects come from separate groups. If the clusters are elongated or their natural type is <em class="italic">chained</em>, then this method <span class="No-Break">is unsuitable.</span></li>
				<li><strong class="bold">Unweighted pairwise mean linkage</strong>: In this method, the distance between two different clusters is calculated as the average distance between all pairs of objects in them. This method is useful when objects form different groups, but it works equally well in the case of elongated (<span class="No-Break">chained-type) clusters.</span></li>
				<li><strong class="bold">Weighted pairwise mean linkage</strong>: This method is identical to the unweighted pairwise<a id="_idIndexMarker466"/> mean method, except that the size of the corresponding clusters (the number of <a id="_idIndexMarker467"/>objects contained in them) is used as a weighting factor in the calculations. Therefore, this method should be used when we assume unequal <span class="No-Break">cluster sizes.</span></li>
				<li><strong class="bold">Weighted centroid linkage</strong>: In this method, the distance between two clusters is defined as the distance between their centers <span class="No-Break">of mass.</span></li>
				<li><strong class="bold">Weighted centroid linkage</strong> (<strong class="bold">median</strong>): This method is identical to the previous one, except that the calculations use weights for the distance measured between cluster sizes. Therefore, if there are significant differences in-cluster sizes, this method is preferable to the <span class="No-Break">previous one.</span></li>
			</ul>
			<p>The following diagram displays a hierarchical <span class="No-Break">clustering dendrogram:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer143">
					<img alt="Figure 4.3 – Hierarchical clustering example" src="image/B19849_04_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Hierarchical clustering example</p>
			<p>The preceding diagram shows an example of a dendrogram for hierarchical clustering, where you can see how the <a id="_idIndexMarker468"/>number of clusters depends on the distance<a id="_idIndexMarker469"/> between objects. Larger distances lead to a smaller number <span class="No-Break">of clusters.</span></p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor242"/>Density-based clustering algorithms</h2>
			<p>In density-based methods, clusters <a id="_idIndexMarker470"/>are considered as regions where the multiple <a id="_idIndexMarker471"/>objects’ density is high. This is separated by regions with a low density <span class="No-Break">of objects.</span></p>
			<p>The <strong class="bold">Density-Based Spatial Clustering of Applications with Noise</strong> (<strong class="bold">DBSCAN</strong>) algorithm is one of the first <a id="_idIndexMarker472"/>density clustering algorithms to be created. The basis of this algorithm is several statements, detailed <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The <img alt="" role="presentation" src="image/B19849_Formula_191.png"/> property of an object is the <img alt="" role="presentation" src="image/B19849_Formula_20.png"/> radius neighborhood area around <span class="No-Break">the object.</span></li>
				<li>The root object is an object whose <span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_211.png"/></span> contains a minimum non-zero number of objects. Assume <a id="_idIndexMarker473"/>that this minimum number equals a predefined value <span class="No-Break">named </span><span class="No-Break"><em class="italic">MinPts</em></span><span class="No-Break">.</span></li>
				<li>The <em class="italic">p</em> object is directly<a id="_idIndexMarker474"/> densely accessible from the <em class="italic">q</em> object if <em class="italic">p</em> is in the <span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_221.png"/></span> property of <em class="italic">q</em> and <em class="italic">q</em> is the <span class="No-Break">root object.</span></li>
				<li>The <em class="italic">p</em> object is densely accessible from the <em class="italic">q</em> object for the given <img alt="" role="presentation" src="image/B19849_Formula_23.png"/> and <em class="italic">MinPts</em> if there’s a sequence of <span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_24.png"/></span> objects, where <span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_25.png"/></span> and <span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_261.png"/></span>, such that <span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_271.png"/></span> is directly densely accessible from <span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_282.png"/></span>, <span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_29.png"/></span>.</li>
				<li>The <em class="italic">p</em> object is densely connected to the <em class="italic">q</em> object for the given <img alt="" role="presentation" src="image/B19849_Formula_301.png"/> and <em class="italic">MinPts</em> if there’s an <em class="italic">o</em> object such that <em class="italic">p</em> and <em class="italic">q</em> are densely accessible <span class="No-Break">from </span><span class="No-Break"><em class="italic">o</em></span><span class="No-Break">.</span></li>
			</ul>
			<p>The DBSCAN algorithm checks the neighborhood of each object to search for clusters. If the <span class="subscript"><img alt="" role="presentation" src="image/B19849_Formula_311.png"/></span> property of the <em class="italic">p</em> object contains more points than <em class="italic">MinPts</em>, then a new cluster is created with the <em class="italic">p</em> object as a root object. DBSCAN then iteratively collects objects directly densely accessible from root objects, which can lead to the union of several densely accessible clusters. The process ends when no new objects can be added to <span class="No-Break">any cluster.</span></p>
			<p>Unlike the partition-based methods, DBSCAN doesn’t require the number of clusters to be specified in advance; it only requires the <img alt="" role="presentation" src="image/B19849_Formula_321.png"/> and <em class="italic">MinPts</em> values as these parameters directly affect the result of clustering. The optimal values of these parameters are difficult to determine, especially for multidimensional data spaces. Also, the distributed data in such spaces is often asymmetrical, which makes it impossible to use global density parameters for their clustering. For clustering <a id="_idIndexMarker475"/>multidimensional data spaces, there’s the <strong class="bold">Subspace Clustering</strong> (<strong class="bold">SUBCLU</strong>) algorithm, which is based on the <span class="No-Break">DBSCAN algorithm.</span></p>
			<p>The <strong class="bold">MeanShift</strong> approach also falls<a id="_idIndexMarker476"/> into the category of density-based clustering algorithms. It’s a non-parametric algorithm that shifts dataset points toward the center of the highest-density region within a certain radius. The algorithm makes such shifts iteratively until points converge to a local maximum of the density function. Such local maxima are also called<a id="_idIndexMarker477"/> the mode, so the algorithm is sometimes called mode-seeking. These local maximums represent<a id="_idIndexMarker478"/> the cluster centroids in <span class="No-Break">the dataset.</span></p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor243"/>Model-based clustering algorithms</h2>
			<p>Model-based algorithms<a id="_idIndexMarker479"/> assume that there’s a particular mathematical model of the cluster in the data space and try to maximize the likelihood of this model and<a id="_idIndexMarker480"/> the data available. Often, this uses the apparatus of <span class="No-Break">mathematical statistics.</span></p>
			<p>The <strong class="bold">Expectation-Maximization</strong> (<strong class="bold">EM</strong>) algorithm assumes that the dataset can be modeled using a linear combination of <a id="_idIndexMarker481"/>multidimensional normal distributions. Its purpose is to estimate distribution parameters that maximize the likelihood function that’s used as a measure of model quality. In other words, it assumes that the data in each cluster obeys a particular distribution law—namely, the normal distribution. With this assumption, it’s possible to determine the optimal parameters of the distribution law—the mean and variance at which the likelihood function is maximal. Thus, we can assume that any object belongs to all clusters, but with a different probability. In this instance, the task will be to fit the set of distributions to the data and determine the probabilities of the object belonging to each cluster. The object should be assigned to the cluster for which this probability is higher than <span class="No-Break">the others.</span></p>
			<p>The EM algorithm is simple and easy to implement. It isn’t sensitive to isolated objects and quickly converges in the case of successful initialization. However, it requires us to specify <em class="italic">k</em> number of clusters, which implies a <em class="italic">priori</em> knowledge about the data. Also, if the initialization fails, the algorithm may be slow to converge, or we might obtain a poor-quality result. Such algorithms don’t apply to high-dimensionality spaces since, in this case, it’s complicated to assume a mathematical model for distributing data in <span class="No-Break">this space.</span></p>
			<p>Now that we understand the various types of clustering algorithms, let’s look at their uses in many industries to group similar data points into clusters. Here are some examples of how clustering algorithms can <span class="No-Break">be applied:</span></p>
			<ul>
				<li><strong class="bold">Customer segmentation</strong>: Clustering algorithms can be used to segment customers based on their purchase history, demographics, and other attributes. This information can then be used for<a id="_idIndexMarker482"/> targeted marketing campaigns, personalized product recommendations, and <span class="No-Break">customer service.</span></li>
				<li><strong class="bold">Image recognition</strong>: In the field <a id="_idIndexMarker483"/>of computer vision, clustering algorithms are used to group images based on visual features such as color, texture, and shape. This can be useful for image classification, object detection, and <span class="No-Break">scene understanding.</span></li>
				<li><strong class="bold">Fraud detection</strong>: In finance, clustering algorithms can detect suspicious transactions by grouping them based on similarities in transaction patterns. This helps to identify potential fraud and prevent <span class="No-Break">financial losses.</span></li>
				<li><strong class="bold">Recommender systems</strong>: In e-commerce, clustering algorithms group products based on customer preferences. This allows recommender systems to suggest relevant products to customers, increasing sales and <span class="No-Break">customer satisfaction.</span></li>
				<li><strong class="bold">Social network analysis</strong>: In social media, clustering algorithms identify groups of users with similar interests <a id="_idIndexMarker484"/>or behaviors. This enables targeted advertising, content creation, and <span class="No-Break">community building.</span></li>
				<li><strong class="bold">Genomics</strong>: In biology, clustering algorithms analyze gene expression data to identify groups of genes that are co-expressed under specific conditions. This aids in understanding gene function and <span class="No-Break">disease mechanisms.</span></li>
				<li><strong class="bold">Text mining</strong>: In natural language processing, clustering algorithms categorize documents based on their content. This is useful for topic modeling, document classification, and <span class="No-Break">information retrieval.</span></li>
			</ul>
			<p>These are just a few examples of the wide range of applications of clustering algorithms. The specific use case will depend on the industry, dataset, and <span class="No-Break">business objectives.</span></p>
			<p>In this section, we discussed various clustering algorithms and their uses. In the following sections, we’ll learn how to use them in real-world examples with various <span class="No-Break">C++ libraries.</span></p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor244"/>Examples of using the mlpack library for dealing with the clustering task samples</h1>
			<p>The <strong class="source-inline">mlpack</strong> library contains implementations of the model-based, density-based, and partition-based clustering approaches. The <a id="_idIndexMarker485"/>model-based algorithm is called <strong class="bold">Gaussian Mixture Models</strong> (<strong class="bold">GMM</strong>) and is based on EM, while<a id="_idIndexMarker486"/> the partition-based algorithm is the k-means algorithm. There are two density-based algorithms we can use: DBSCAN and <span class="No-Break">MeanShift clustering.</span></p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor245"/>GMM and EM with mlpack</h2>
			<p>The GMM algorithm assumes that clusters can <a id="_idIndexMarker487"/>be fit to some Gaussian (normal) distributions; it uses the EM approach for training. There are the <strong class="source-inline">GMM</strong> and <strong class="bold">EMFit</strong> classes in the <strong class="source-inline">mlpack</strong> library <a id="_idIndexMarker488"/>that implement this <a id="_idIndexMarker489"/>approach, as illustrated in <a id="_idIndexMarker490"/>the following <span class="No-Break">code</span><span class="No-Break"><a id="_idIndexMarker491"/></span><span class="No-Break"> snippet:</span></p>
			<pre class="source-code">
GMM gmm(num_clusters, /*dimensionality*/ 2);
KMeans&lt;&gt; kmeans;
size_t max_iterations = 250;
double tolerance = 1e-10;
EMFit&lt;KMeans&lt;&gt;, NoConstraint&gt; em(max_iterations, tolerance, kmeans);
gmm.Train(inputs, /*trials*/ 3, /*use_existing_model*/ false, em);</pre>			<p>Notice that the constructor of the <strong class="source-inline">GMM</strong> class takes the desired number of clusters and feature dimensionality as an argument. After <strong class="source-inline">GMM</strong> object initialization, the object of the <strong class="source-inline">EMFit</strong> class was initialized with maximum iterations, tolerance, and a clustering object. The tolerance parameter in <strong class="source-inline">EMFit</strong> controls how similar two points must be to be considered as part of the same cluster. A higher tolerance value means that the algorithm will group more points, resulting in fewer clusters. Conversely, a lower tolerance value leads to more clusters with <a id="_idIndexMarker492"/>fewer points in each one. The clustering object—in our case, <strong class="source-inline">kmeans</strong>—will be<a id="_idIndexMarker493"/> used by the algorithm to find initial centroids for Gaussian fitting. Then, we passed the training features and the <strong class="source-inline">EM</strong> object into the training method. Now, we have the <a id="_idIndexMarker494"/>trained <strong class="source-inline">GMM</strong> model. In the <strong class="source-inline">mlpack</strong> library, the trained <strong class="source-inline">gmm</strong> object should be used to classify new feature points, but<a id="_idIndexMarker495"/> we can use it to show cluster assignments for the original data that we used for training. The following piece of code shows these steps and also plots the results <span class="No-Break">of clustering:</span></p>
			<pre class="source-code">
arma::Row&lt;size_t&gt; assignments;
gmm.Classify(inputs, assignments);
Clusters plot_clusters;
for (size_t i = 0; i != inputs.n_cols; ++i) {
  auto cluser_idx = assignments[i];
  plot_clusters[cluser_idx].first.push_back(inputs.at(0, i));
  plot_clusters[cluser_idx].second.push_back(inputs.at(1, i));
  }
PlotClusters(plot_clusters, "GMM", name + "-gmm.png");</pre>			<p>Here, we used the <strong class="source-inline">GMM::Classify()</strong> method to identify which cluster our objects belong to. This method filled a row vector of cluster identifiers per element that corresponds to the input data. The resulting cluster indices were used for filling the <strong class="source-inline">plot_clusters</strong> container. This<a id="_idIndexMarker496"/> container maps cluster indices with input data coordinates for plotting. It was<a id="_idIndexMarker497"/> used as an argument for the <strong class="source-inline">PlotClusters()</strong> function, which visualized the clustering result, as illustrated in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer158">
					<img alt="Figure 4.4 – MLPack GMM clustering visualization" src="image/B19849_04_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – mlpack GMM clustering visualization</p>
			<p>In the preceding picture, we can<a id="_idIndexMarker498"/> see how the GMM and EM<a id="_idIndexMarker499"/> algorithms work on different <span class="No-Break">artificial datasets.</span></p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor246"/>K-means clustering with mlpack</h2>
			<p>The k-means algorithm in the <strong class="source-inline">mlpack</strong> library is implemented in the <strong class="source-inline">KMeans</strong> class. The constructor of this class<a id="_idIndexMarker500"/> takes several parameters, with the most important ones being the number of iterations and the object for distance<a id="_idIndexMarker501"/> metric calculation. In the following example, we’ll use the default values so that the constructor will be called without parameters. Once we’ve constructed an object of the <strong class="source-inline">KMeans</strong> type, we’ll use the <strong class="source-inline">KMeans::Cluster()</strong> method to run the algorithm and assign a cluster label to each of the input elements, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
arma::Row&lt;size_t&gt; assignments;
KMeans&lt;&gt; kmeans;
kmeans.Cluster(inputs, num_clusters, assignments);</pre>			<p>The result of clusterization is the <strong class="source-inline">assignments</strong> container object with labels. Notice that the desired number of clusters <a id="_idIndexMarker502"/>was passed as an argument for the <strong class="source-inline">Cluster</strong> method. The following <a id="_idIndexMarker503"/>code sample shows how to plot the results <span class="No-Break">of clustering:</span></p>
			<pre class="source-code">
Clusters plot_clusters;
for (size_t i = 0; i != inputs.n_cols; ++i) {
  auto cluser_idx = assignments[i];
  plot_clusters[cluser_idx].first.push_back(inputs.at(0, i));
  plot_clusters[cluser_idx].second.push_back(inputs.at(1, i));
}
PlotClusters(plot_clusters, "K-Means", name + "-kmeans.png");</pre>			<p>As we can see, the code for visualization is the same as it was for the previous example—it’s the result of the uniform clustering API in the <strong class="source-inline">mlpack</strong> library. We received the same <strong class="source-inline">assignments</strong> container that we converted in the data structure, which is suitable for the visualization library we’re using, and called the <strong class="source-inline">PlotClusters</strong> function. The visualization result is illustrated in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer159">
					<img alt="Figure 4.5 – MLPack K-means clustering visualization" src="image/B19849_04_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – mlpack K-means clustering visualization</p>
			<p>In the preceding<a id="_idIndexMarker504"/> figure, we can see how the k-means algorithm works<a id="_idIndexMarker505"/> on different <span class="No-Break">artificial datasets.</span></p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor247"/>DBSCAN with mlpack</h2>
			<p>The <strong class="source-inline">DBSCAN</strong> class implements the<a id="_idIndexMarker506"/> corresponding algorithm in the <strong class="source-inline">mlpack</strong> library. The constructor of this class takes several parameters, but<a id="_idIndexMarker507"/> the two most important are the epsilon and the minimum points number. In the following code snippet, we’re creating the object of <span class="No-Break">this class:</span></p>
			<pre class="source-code">
DBSCAN&lt;&gt; dbscan(/*epsilon*/ 0.1, /*min_points*/ 15);</pre>			<p>Here, <strong class="source-inline">epsilon</strong> is the radius of a range search, while <strong class="source-inline">min_points</strong> is the minimum number of points required to form a cluster. After constructing an object of the <strong class="source-inline">BSSCAN</strong> type, we can use the <strong class="source-inline">Cluster()</strong> method to run the algorithm and assign a cluster label to each of the input elements, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
dbscan.Cluster(inputs, assignments);</pre>			<p>The result of clusterization is an <strong class="source-inline">assignments</strong> container object with labels. Notice that for this algorithm, we didn’t specify the desired number of clusters because the algorithm determined them by itself. The code for the visualization is the same as for the previous examples—we convert the <strong class="source-inline">assignments</strong> container into a data structure that’s suitable for the visualization library we’re using and call the <strong class="source-inline">PlotClusters</strong> function. The following figure shows the DBSCAN clustering <span class="No-Break">visualization result:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer160">
					<img alt="Figure 4.6 – MLPack DBSCAN clustering visualization" src="image/B19849_04_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – mlpack DBSCAN clustering visualization</p>
			<p>In the preceding figure, we can<a id="_idIndexMarker508"/> see how the DBSCAN algorithm works on different artificial datasets. The <a id="_idIndexMarker509"/>main difference from previous algorithms is the bigger number of clusters that the algorithm found. From this, we can see that their centroids are near some local <span class="No-Break">density maximums.</span></p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor248"/>MeanShift clustering with mlpack</h2>
			<p>The <strong class="source-inline">MeanShift</strong> class implements the corresponding algorithm in the <strong class="source-inline">mlpack</strong> library. The constructor of this class takes <a id="_idIndexMarker510"/>several parameters, with the most important one being the density region search radius. It’s quite tricky to <a id="_idIndexMarker511"/>manually find the appropriate value for this parameter. However, the library gives us a very useful method to determine it automatically. In the following code snippet, we’re creating an object of the <strong class="source-inline">MeanShift</strong> class without specifying the radius <span class="No-Break">parameter explicitly:</span></p>
			<pre class="source-code">
MeanShift&lt;&gt; mean_shift;
auto radius = mean_shift.EstimateRadius(inputs);
mean_shift.Radius(radius);</pre>			<p>Here, we used the <strong class="source-inline">EstimateRadius</strong> method to get the automatic radius estimation, which is based on the <strong class="bold">k-nearest neighbor</strong> (<strong class="bold">KNN</strong>) search. Once we’ve initialized the <strong class="source-inline">MeanShift</strong> object with the<a id="_idIndexMarker512"/> appropriate<a id="_idIndexMarker513"/> search radius value, we can use the <strong class="source-inline">Cluster()</strong> method to run<a id="_idIndexMarker514"/> the algorithm and assign a cluster label to each of the input elements, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
arma::Row&lt;size_t&gt; assignments;
arma::mat centroids;
mean_shift.Cluster(inputs, assignments, centroids);</pre>			<p>The result of clusterization is the <strong class="source-inline">assignments</strong> container object with labels and an additional matrix that contains the cluster centroids coordinates. For this algorithm, we also didn’t specify the number of clusters because the algorithm determined them by itself. The code for visualization is the same as for the previous examples—we convert the <strong class="source-inline">assignments</strong> container into a data structure suitable for the visualization library we’re using and call the <strong class="source-inline">PlotClusters</strong> function. The following figure shows the <strong class="source-inline">MeanShift</strong> clustering <span class="No-Break">visualization result:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer161">
					<img alt="Figure 4.7 – MLPack MeanShift clustering visualization" src="image/B19849_04_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – mlpack MeanShift clustering visualization</p>
			<p>The preceding figure shows how the MeanShift algorithm works on different artificial datasets. We can see that the results <a id="_idIndexMarker515"/>are somehow similar to those for K-means clustering but the number of<a id="_idIndexMarker516"/> clusters was determined automatically. We can also see that in the one of datasets, the algorithm failed to get the correct number of clusters, so we must perform experiments with search radius values to get more precise <span class="No-Break">clustering results.</span></p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor249"/>Examples of using the Dlib library for dealing with the clustering task samples</h1>
			<p>The <strong class="source-inline">Dlib</strong> library provides k-means, spectral, hierarchical, and<a id="_idIndexMarker517"/> two more graph clustering algorithms—<strong class="bold">Newman</strong> and <strong class="bold">Chinese Whispers</strong>—as clustering methods. Let’s take <span class="No-Break">a look.</span></p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor250"/>K-means clustering with Dlib</h2>
			<p>The <strong class="source-inline">Dlib</strong> library uses kernel functions as the distance functions for the k-means algorithm. An example of such a function<a id="_idIndexMarker518"/> is the radial basis<a id="_idIndexMarker519"/> function. As an initial step, we define the required types, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
 typedef matrix&lt;double, 2, 1&gt; sample_type;
 typedef radial_basis_kernel&lt;sample_type&gt; kernel_type;</pre>			<p>Then, we initialize an object of the <strong class="source-inline">kkmeans</strong> type. Its constructor takes an object that will define cluster centroids as input parameters. We can use an object of the <strong class="source-inline">kcentroid</strong> type for this purpose. Its constructor takes three parameters: the first one is the object that defines the kernel (distance function), the second is the numerical accuracy for the centroid <a id="_idIndexMarker520"/>estimation, and the third is the upper limit on the runtime complexity (actually, the<a id="_idIndexMarker521"/> maximum number of dictionary vectors the <strong class="source-inline">kcentroid</strong> object is allowed to use), as illustrated in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
kcentroid&lt;kernel_type&gt; kc(kernel_type(0.1), 0.01, 8);
kkmeans&lt;kernel_type&gt; kmeans(kc);</pre>			<p>As a next step, we initialize cluster centers with the <strong class="source-inline">pick_initial_centers()</strong> function. This function takes the number of clusters, the output container for center objects, the training data, and the distance function object as parameters, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
std::vector&lt;sample_type&gt; samples; //training dataset
...
size_t num_clusters = 2;
std::vector&lt;sample_type&gt; initial_centers;
pick_initial_centers(num_clusters,
                     initial_centers,
                     samples,
                     kmeans.get_kernel());</pre>			<p>When initial centers are selected, we can use them for the <strong class="source-inline">kkmeans::train()</strong> method to determine exact clusters, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
kmeans.set_number_of_centers(num_clusters);
kmeans.train(samples, initial_centers);
for (size_t i = 0; i != samples.size(); i++) {
  auto cluster_idx = kmeans(samples[i]);
  ...
}</pre>			<p>We used the <strong class="source-inline">kmeans</strong> object as a functor to perform clustering on a single data item. The clustering result will be<a id="_idIndexMarker522"/> the cluster’s index for the item. Then, we used cluster indices to visualize the<a id="_idIndexMarker523"/> final clustering result, as illustrated in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer162">
					<img alt="Figure 4.8 – D﻿lib K-means clustering visualization" src="image/B19849_04_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – Dlib K-means clustering visualization</p>
			<p>In the preceding figure, we can see how the k-means clustering algorithm that’s implemented in the <strong class="source-inline">Dlib</strong> library <a id="_idIndexMarker524"/>works on different <a id="_idIndexMarker525"/><span class="No-Break">artificial datasets.</span></p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor251"/>Spectral clustering with Dlib</h2>
			<p>The spectral clustering algorithm<a id="_idIndexMarker526"/> in the <strong class="source-inline">Dlib</strong> library is implemented in the <strong class="source-inline">spectral_cluster</strong> function. It takes the distance function object, the training dataset, and the number of clusters as parameters. As a result, it returns a container with cluster indices, which have the same ordering as the input data. In the<a id="_idIndexMarker527"/> following sample, an object of the <strong class="source-inline">knn_kernel</strong> type is used as a distance function. You’ll find its implementation in the samples provided in this book. This <strong class="source-inline">knn_kernel</strong> distance function object estimates the first KNN objects to the given one. These objects are determined with the KNN algorithm, which uses the Euclidean distance for the distance measure, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
typedef matrix&lt;double, 2, 1&gt; sample_type;
typedef knn_kernel&lt;sample_type&gt; kernel_type;
...
std::vector&lt;sample_type&gt; samples;
...
std::vector&lt;unsigned long&gt; clusters =
spectral_cluster(kernel_type(samples, 15),
                 samples,
                 num_clusters);</pre>			<p>The <strong class="source-inline">spectral_cluster()</strong> function call filled the <strong class="source-inline">clusters</strong> object with cluster index values, which we can use to visualize the clustering result, as illustrated in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer163">
					<img alt="Figure 4.9 – D﻿lib spectral clustering visualization" src="image/B19849_04_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – Dlib spectral clustering visualization</p>
			<p>In the preceding figure, we can see how the <a id="_idIndexMarker528"/>spectral clustering algorithm<a id="_idIndexMarker529"/> that’s implemented in the <strong class="source-inline">Dlib</strong> library works on different <span class="No-Break">artificial datasets.</span></p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor252"/>Hierarchical clustering with Dlib</h2>
			<p>The <strong class="source-inline">Dlib</strong> library implements the <a id="_idIndexMarker530"/>agglomerative<a id="_idIndexMarker531"/> hierarchical (bottom-up) clustering algorithm. The <strong class="source-inline">bottom_up_cluster()</strong> function implements this algorithm. This function takes the matrix of distances between dataset objects, the cluster indices container (as the output parameter), and the number of clusters as input parameters. Note that it <a id="_idIndexMarker532"/>returns the container with<a id="_idIndexMarker533"/> cluster indices in the order of distances provided in <span class="No-Break">the matrix.</span></p>
			<p>In the following code sample, we’ve filled the distance matrix with pairwise Euclidean distances between each pair of elements in the <span class="No-Break">input dataset:</span></p>
			<pre class="source-code">
matrix&lt;double&gt; dists(inputs.nr(), inputs.nr());
for (long r = 0; r &lt; dists.nr(); ++r) {
  for (long c = 0; c &lt; dists.nc(); ++c) {
    dists(r, c) = length(subm(inputs, r, 0, 1, 2) -
                         subm(inputs, c, 0, 1, 2));
  }
}
std::vector&lt;unsigned long&gt; clusters;
bottom_up_cluster(dists, clusters, num_clusters);</pre>			<p>The <strong class="source-inline">bottom_up_cluster()</strong> function call filled the <strong class="source-inline">clusters</strong> object with cluster index values, which we can use to visualize the clustering result, as illustrated in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer164">
					<img alt="Figure 4.10 – D﻿lib hierarchical clustering visualization" src="image/B19849_04_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – Dlib hierarchical clustering visualization</p>
			<p>In the preceding <a id="_idIndexMarker534"/>figure, we can see how the hierarchical clustering<a id="_idIndexMarker535"/> algorithm that’s implemented in the <strong class="source-inline">Dlib</strong> library works on different <span class="No-Break">artificial datasets.</span></p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor253"/>Newman modularity-based graph clustering algorithm with Dlib</h2>
			<p>The implementation of this algorithm is based on the work <em class="italic">Modularity and community structure in networks</em>, by M. E. J. Newman. This algorithm is based on the modularity matrix for a network or a graph and it isn’t based on particular graph theory. However, it does have some similarities with spectral clustering because it also <span class="No-Break">uses eigenvectors.</span></p>
			<p>The <strong class="source-inline">Dlib</strong> library implements this <a id="_idIndexMarker536"/>algorithm in the <strong class="source-inline">newman_cluster()</strong> function, which takes a vector of weighted graph edges and outputs the container with cluster indices for each vertex. The vector of weighted graph edges<a id="_idIndexMarker537"/> represents the connections between nodes in the network, with each edge having a weight that indicates its strength. The weights are used to determine the similarity between nodes and thus influence the clustering process. The initial step for using this algorithm is to define graph edges. In the following code sample, we’re making edges between almost every pair of dataset objects. Notice that we only use pairs with a distance greater than a threshold (this was done for performance considerations). The threshold distance can be adjusted to achieve different levels of granularity in the <span class="No-Break">clustering results.</span></p>
			<p>Also, this algorithm doesn’t require prior knowledge of the number of clusters as it can determine the number of clusters by itself. Here’s <span class="No-Break">the code:</span></p>
			<pre class="source-code">
for (long i = 0; i &lt; inputs.nr(); ++i) {
  for (long j = 0; j &lt; inputs.nr(); ++j) {
    auto dist = length(subm(inputs, i, 0, 1, 2) -
                       subm(inputs, j, 0, 1, 2));
    if (dist &lt; 0.5)
      edges.push_back(sample_pair(i, j, dist));
  }
}
remove_duplicate_edges(edges);
std::vector&lt;unsigned long&gt; clusters;
const auto num_clusters = newman_cluster(edges, clusters);</pre>			<p>The <strong class="source-inline">newman_cluster()</strong> function call filled the <strong class="source-inline">clusters</strong> object with cluster index values, which we can use to visualize the clustering result. Notice that another approach for edge weight calculation can<a id="_idIndexMarker538"/> lead to another clustering result. Also, edge weight values should be initialized <a id="_idIndexMarker539"/>according to a certain task. The edge length was chosen only for <span class="No-Break">demonstration purposes.</span></p>
			<p>The result can be seen in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer165">
					<img alt="Figure 4.11 – D﻿lib Newman clustering visualization" src="image/B19849_04_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – Dlib Newman clustering visualization</p>
			<p>In the preceding figure, we can see how the Newman clustering algorithm that’s implemented in the <strong class="source-inline">Dlib</strong> library works on different <span class="No-Break">artificial datasets.</span></p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor254"/>Chinese Whispers – graph clustering algorithm with Dlib</h2>
			<p>The Chinese Whispers algorithm is an<a id="_idIndexMarker540"/> algorithm that’s used to partition the nodes of weighted, undirected graphs. It was described in the paper <em class="italic">Chinese Whispers – an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems</em>, by Chris Biemann. This <a id="_idIndexMarker541"/>algorithm also doesn’t use any unique graph theory methods; instead, it uses the idea of using local contexts for clustering, so it can be classified as a <span class="No-Break">density-based method.</span></p>
			<p>In the <strong class="source-inline">Dlib</strong> library, this algorithm is implemented in the <strong class="source-inline">chinese_whispers()</strong> function, which takes the vector of weighted graph edges and outputs the container with cluster indices for each of the vertices. For performance considerations, we limit the number of edges between dataset objects with a threshold on distance. The meaning of weighted graph edges and threshold parameters are the same as for the Newman algorithm. Moreover, as with the Newman algorithm, this one also determines the number of resulting clusters by itself. The code can be seen in the <span class="No-Break">following snippet:</span></p>
			<pre class="source-code">
std::vector&lt;sample_pair&gt; edges;
for (long i = 0; i &lt; inputs.nr(); ++i) {
  for (long j = 0; j &lt; inputs.nr(); ++j) {
    auto dist = length(subm(inputs, i, 0, 1, 2) -
                       subm(inputs, j, 0, 1, 2));
    if (dist &lt; 1)
      edges.push_back(sample_pair(i, j, dist));
  }
}
std::vector&lt;unsigned long&gt; clusters;
const auto num_clusters = chinese_whispers(edges,clusters);</pre>			<p>The <strong class="source-inline">chinese_whispers()</strong> function call filled the <strong class="source-inline">clusters</strong> object with cluster index values, which we can use to visualize the clustering result. Notice that we used <strong class="source-inline">1</strong> as the threshold for edge weights; another threshold value can lead to another clustering result. Also, edge <a id="_idIndexMarker542"/>weight values should be<a id="_idIndexMarker543"/> initialized according to a certain task. The edge length was chosen only for <span class="No-Break">demonstration purposes.</span></p>
			<p>The result can be seen in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer166">
					<img alt="Figure 4.12 – D﻿lib Chinese Whispers clustering visualization" src="image/B19849_04_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – Dlib Chinese Whispers clustering visualization</p>
			<p>In the preceding figure, we can see how the Chinese Whispers clustering algorithm that’s implemented<a id="_idIndexMarker544"/> in the <strong class="source-inline">Dlib</strong> library works on different <span class="No-Break">artificial datasets.</span></p>
			<p>In this and previous sections, we saw<a id="_idIndexMarker545"/> a lot of examples of images that show clustering results. The following section will explain how to use the <strong class="source-inline">plotcpp</strong> library, which we used to plot <span class="No-Break">these images.</span></p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor255"/>Plotting data with C++</h1>
			<p>After clustering, we plot the<a id="_idIndexMarker546"/> results with the <strong class="source-inline">plotcpp</strong> library, which is a thin wrapper around the <strong class="source-inline">gnuplot</strong> command-line utility. With this library, we can draw points on a scatter plot or draw lines. The initial step to start plotting with this library is creating an object of the <strong class="source-inline">Plot</strong> class. Then, we must specify the output destination of the drawing. We can set the destination with the <strong class="source-inline">Plot::SetTerminal()</strong> method, which takes a string with a destination point abbreviation. For example, we can use the <strong class="source-inline">qt</strong> string <a id="_idIndexMarker547"/>value to show the <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>) window with our drawing or we can use a string with a picture file extension to save a drawing to a file, as in the code sample that follows. We can also configure the title of the drawing, the axis labels, and some other parameters with the <strong class="source-inline">Plot</strong> class methods. However, it does not cover all possible configurations available for <strong class="source-inline">gnuplot</strong>. In cases where we need some unique options, we can use the <strong class="source-inline">Plot::gnuplotCommand()</strong> method to make a direct <span class="No-Break"><strong class="source-inline">gnuplot</strong></span><span class="No-Break"> configuration.</span></p>
			<p>There are two drawing approaches we can follow to draw a set of different graphics on <span class="No-Break">one plot:</span></p>
			<ol>
				<li>We can use the <strong class="source-inline">Draw2D()</strong> method with objects of the <strong class="source-inline">Points</strong> or <strong class="source-inline">Lines</strong> classes, but in this case, we should specify all graphics configurations <span class="No-Break">before compilation.</span></li>
				<li>We can use the <strong class="source-inline">Plot::StartDraw2D()</strong> method to get an intermediate drawing state object. Then, we can use the <strong class="source-inline">Plot::AddDrawing()</strong> method to add different drawings to one plot. The <strong class="source-inline">Plot::EndDraw2D()</strong> method should be called after we’ve drawn the <span class="No-Break">last graphics.</span></li>
			</ol>
			<p>We can use the <strong class="source-inline">Points</strong> type to draw points. An object of this type should be initialized with start and end forward iterators for the integral numeric data types, which represent coordinates. We should specify three iterators as points coordinates, two iterators for the <em class="italic">x</em> coordinates, which is where they start and end, and one iterator for the <em class="italic">y</em> coordinates’ start. The number of coordinates in the containers should be the same. The last parameter is the <strong class="source-inline">gnuplot</strong> visual style configuration. Objects of the <strong class="source-inline">Lines</strong> class can be configured in the <span class="No-Break">same way.</span></p>
			<p>Once we’ve completed all drawing operations, we should call the <strong class="source-inline">Plot::Flush()</strong> method to render all <a id="_idIndexMarker548"/>commands to the window or the file, as shown in the following <span class="No-Break">code block:</span></p>
			<pre class="source-code">
// Define helper data types for point clusters coordinates
// Container type for single coordinate values
using Coords = std::vector&lt;DataType&gt;;
// Paired x, y coordinate containers
using PointCoords = std::pair&lt;Coords, Coords&gt;;
// Clusters mapping container
using Clusters = std::unordered_map&lt;index_t, PointCoords&gt;;
// define color values container
const std::vector&lt;std::string&gt; colors{
    "black", "red",    "blue",  "green",
    "cyan",  "yellow", "brown", "magenta"};
...
    // Function for clusters visualization
    void
    PlotClusters(const Clusters&amp; clusters,
                 const std::string&amp; name,
                 const std::string&amp; file_name) {
  // Instantiate plotting object
  plotcpp::Plot plt;
  // Configure plotting object
  plt.SetTerminal("png");
  plt.SetOutput(file_name);
  plt.SetTitle(name);
  plt.SetXLabel("x");
  plt.SetYLabel("y");
  plt.SetAutoscale();
  plt.gnuplotCommand("set grid");
  // Start 2D scatter plot drawing
  auto draw_state =
      plt.StartDraw2D&lt;Coords::const_iterator&gt;();
  for (auto&amp; cluster : clusters) {
    std::stringstream params;
    // Configure cluster visualization color string
    params &lt;&lt; "lc rgb '" &lt;&lt; colors[cluster.first]
           &lt;&lt; "' pt 7";
    // Create cluster name string
    auto cluster_name =
        std::to_string(cluster.first) + " cls";
    // Create points visualization object using "cluster"
    // points
    plotcpp::Points points(cluster.second.first.begin(),
                           cluster.second.first.end(),
                           cluster.second.second.begin(),
                           cluster_name, params.str());
       // Add current cluster visualization to the 2D scatter
    // plot
    plt.AddDrawing(draw_state, points);
  }
  // Finalize 2D scatter plot
  plt.EndDraw2D(draw_state);
  // Render the plot
  plt.Flush();
}</pre>			<p>In this example, we learned how to plot our clustering results with the <strong class="source-inline">plotcpp</strong> library. We must be able to configure different visualization parameters, such as the type of the plot, point colors, and axes names as these parameters make our plot more informative. We also<a id="_idIndexMarker549"/> learned how to save this plot in a file so that we can use it later or insert it into another document. This library will be used throughout this book for <span class="No-Break">visualizing results.</span></p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor256"/>Summary</h1>
			<p>In this chapter, we considered what clustering is and how it differs from classification. We looked at different types of clustering methods, such as partition-based, spectral, hierarchical, density-based, and model-based methods. We also observed that partition-based methods can be divided into more categories, such as distance-based methods and graph <span class="No-Break">theory-based methods.</span></p>
			<p>Then, we used implementations of these algorithms, including the k-means algorithm (the distance-based method), the GMM algorithm (the model-based method), the Newman modularity-based algorithm, and the Chinese Whispers algorithm, for graph clustering. We also learned how to use the hierarchical and spectral clustering algorithm implementations in programs. We saw that the crucial issues for successful clustering include the choice of the distance measure function, the initialization step, the splitting or merging strategy, and prior knowledge of the number <span class="No-Break">of clusters.</span></p>
			<p>A combination of these issues is unique for each specific algorithm. We also saw that a clustering algorithm’s results depend a lot on dataset characteristics and that we should choose the algorithm according <span class="No-Break">to these.</span></p>
			<p>At the end of this chapter, we studied how we can visualize clustering results with the <span class="No-Break"><strong class="source-inline">plotcpp</strong></span><span class="No-Break"> library.</span></p>
			<p>In the next chapter, we’ll learn what a data anomaly is and what machine learning algorithms exist for anomaly detection. We’ll also see how anomaly detection algorithms can be used to solve real-life problems, and which properties of such algorithms play a more significant role in <span class="No-Break">different tasks.</span></p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor257"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
			<ul>
				<li><em class="italic">The 5 Clustering Algorithms Data Scientists Need to </em><span class="No-Break"><em class="italic">Know</em></span><span class="No-Break">: </span><a href="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68"><span class="No-Break">https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68</span></a></li>
				<li><span class="No-Break"><em class="italic">Clustering</em></span><span class="No-Break">: </span><a href="https://scikit-learn.org/stable/modules/clustering.html"><span class="No-Break">https://scikit-learn.org/stable/modules/clustering.html</span></a></li>
				<li><em class="italic">Different Types of Clustering </em><span class="No-Break"><em class="italic">Algorithm</em></span><span class="No-Break">: </span><a href="https://www.geeksforgeeks.org/different-types-clustering-algorithm/"><span class="No-Break">https://www.geeksforgeeks.org/different-types-clustering-algorithm/</span></a></li>
				<li><em class="italic">An introduction to clustering and different methods of </em><span class="No-Break"><em class="italic">clustering</em></span><span class="No-Break">: </span><a href="https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/"><span class="No-Break">https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/</span></a></li>
				<li>Graph theory introductory book: <em class="italic">Graph Theory</em> (<em class="italic">Graduate Texts in Mathematics</em>), by Adrian Bondy and <span class="No-Break">U.S.R. Murty</span></li>
				<li><em class="italic">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>, by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, covers a lot of aspects of machine learning theory <span class="No-Break">and algorithms</span></li>
			</ul>
		</div>
	</body></html>