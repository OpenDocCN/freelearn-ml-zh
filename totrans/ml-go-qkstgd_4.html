<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Unsupervised Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">While the majority of machine learning problems involve labeled data, as we saw in the previous chapter, there is another important branch called <strong>unsupervised learning</strong>. This applies in situations where you may not have labels for the input data, and so the algorithm cannot work by trying to predict output labels from each input. Instead, unsupervised algorithms work by trying to spot patterns or structure in the input. It can be a useful technique when carrying out exploratory analysis on a large dataset with many different input variables. In this situation, it would be incredibly time-consuming to plot charts of all the different variables to try to spot patterns, so instead, unsupervised learning can be used to do this automatically.</p>
<p>As humans, we are very familiar with this concept: much of what we do is never explicitly taught to us by someone else. Instead, we explore the world around us, looking for, and discovering, patterns. For this reason, unsupervised learning is of particular interest to researchers who are trying to develop systems for <strong>general intelligence</strong>: computers that can learn what they need independently<sup>[1]</sup>.</p>
<p>In this chapter, we are going to introduce two popular unsupervised algorithms and implement them in Go. First, we will use a <strong>clustering algorithm</strong> to separate a dataset into distinct groups without any guidance about what to look for. Then, we will use a technique called <strong>principal component analysis</strong> to compress a dataset by first finding hidden structures within it.</p>
<p>This will just scratch the surface of what unsupervised learning is able to achieve. Some cutting-edge algorithms are able to allow computers to carry out tasks that normally require human creativity. One example worth looking at is NVIDIA's system for creating realistic pictures from sketches<sup>[2]</sup>. <span>You can also find code examples online for networks that can make realistic changes to how an image appears, for instance, turning horses into zebras, or oranges into apples<sup>[3]</sup>.</span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Clustering </li>
<li>Principal component analysis</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering</h1>
                </header>
            
            <article>
                
<p>Clustering algorithms are designed to split a dataset up into groups. Once trained, any new data can be assigned to a group when it arrives. Suppose you are working with a dataset of customer information for an e-commerce store. You might use clustering to identify groups of customers, for example, business/private customers. This information can then be used to make decisions about how to best serve those customer types.</p>
<p>You might also use clustering as a preparatory step before applying supervised learning. For example, a dataset of images may require manual labeling, which is often time-consuming and costly. If you can segment the dataset into groups with a clustering algorithm, then you may be able to save time by only labeling a fraction of the images, and then assuming that each cluster contains images with the same label.</p>
<p>Clustering has also been applied to computer vision applications in autonomous vehicles, where it can be used to help a vehicle navigate on an unknown stretch of road. By clustering the data from the vehicles cameras, it is possible to identify which area of each incoming image contains the road on which the vehicle must drive<sup><span>[4]</span></sup>.</p>
<p>For our example, we are going to use a dataset containing measurements of different types of iris flower, which you can download using the <kbd>./download-iris.sh</kbd> script in the code repository. This data is often used to demonstrate supervised learning: you can use machine learning to classify the data according to the species of iris flower. In this case, however, we will not provide labels to the clustering algorithm, meaning that it has to identify clusters purely from the measurement data:</p>
<ol>
<li>First, load the data into Go, as we have done in previous examples:</li>
</ol>
<pre style="padding-left: 60px">import (<br/> "fmt"<br/> "github.com/kniren/gota/dataframe"<br/> "github.com/kniren/gota/series"<br/> "io/ioutil"<br/> "bytes"<br/> "math/rand"<br/>)<br/><br/>const path = "../datasets/iris/iris.csv"<br/><br/>b, err := ioutil.ReadFile(path)<br/>if err != nil {<br/>    fmt.Println("Error!", err)<br/>}<br/>df := dataframe.ReadCSV(bytes.NewReader(b))<br/>df.SetNames("petal length", "petal width", "sepal length", "sepal width", "species")</pre>
<ol start="2">
<li>Next, we need to prepare the data by splitting the species column from the rest of the data: this will only be used for the final assessment of the groups after clustering. To do this, use the <kbd>DataFrameToXYs</kbd> func from previous examples:</li>
</ol>
<pre style="padding-left: 60px">features, classification := DataFrameToXYs(df, "species")</pre>
<ol start="3">
<li>Now, we can train an algorithm called <strong>k-means</strong> to try to split the dataset into three clusters. k-means works by initially choosing the middle (known as the <strong>centroid</strong>) of each cluster at random, and assigning each data point in the training set to its nearest centroid. It then iteratively updates the positions of each cluster, reassigning the data points at each step until it reaches convergence.</li>
</ol>
<div class="packt_tip"><strong>k-means</strong> is a simple algorithm and is fast to train, so it is a good starting point when clustering data. However, it does require you to specify how many clusters to find, which is not always obvious. Other clustering algorithms, such as DBSCAN, do not have this limitation.</div>
<p class="mce-root">Using the k-means implementation in goml, we can try to find three clusters within the data. Often, you may need to use trial and error to find out how many clusters to use—K. If you have lots of very small clusters after running k-means, then you probably need to reduce K:</p>
<pre>import (<br/>    "gonum.org/v1/plot"<br/>    "gonum.org/v1/plot/plotter"<br/>    "gonum.org/v1/plot/plotutil"<br/>    "gonum.org/v1/plot/vg"<br/>    "github.com/cdipaolo/goml/cluster"<br/>    "github.com/cdipaolo/goml/base"<br/>    "bufio"<br/>    "strconv"<br/>)<br/><br/>model := cluster.NewKMeans(3, 30, features)<br/><br/>if err := model.Learn(); err != nil {</pre>
<pre>    panic(err)<br/>}</pre>
<p>Once we have fitted the model to the data, we can generate a prediction from it; that is, find out which cluster each data point belongs to:</p>
<pre>func PredictionsToScatterData(features [][]float64, model base.Model, featureForXAxis, featureForYAxis int) (map[int]plotter.XYs) {<br/>    ret := make(map[int]plotter.XYs)<br/>    if features == nil {<br/>        panic("No features to plot")<br/>    }<br/>    <br/>    for i := range features {<br/>        var pt struct{X, Y float64}<br/>        pt.X = features[i][featureForXAxis]<br/>        pt.Y = features[i][featureForYAxis]<br/>        p, _ := model.Predict(features[i])<br/>        ret[int(p[0])] = append(ret[int(p[0])], pt)<br/>    }<br/>    return ret<br/>}<br/><br/>scatterData := PredictionsToScatterData(features, model, 2, 3)</pre>
<p class="mce-root"><span>Now, we are able to plot the clusters using the following code:</span></p>
<pre>func PredictionsToScatterData(features [][]float64, model base.Model, featureForXAxis, featureForYAxis int) (map[int]plotter.XYs) {<br/>    ret := make(map[int]plotter.XYs)<br/>    if features == nil {<br/>        panic("No features to plot")<br/>    }<br/>    <br/>    for i := range features {<br/>        var pt struct{X, Y float64}<br/>        pt.X = features[i][featureForXAxis]<br/>        pt.Y = features[i][featureForYAxis]<br/>        p, _ := model.Predict(features[i])<br/>        ret[int(p[0])] = append(ret[int(p[0])], pt)<br/>    }<br/>    return ret<br/>}<br/><br/>scatterData := PredictionsToScatterData(features, model, 2, 3)</pre>
<p>What this does is display the data using two of the input features, <kbd>Sepal width</kbd> and <kbd>Sepal length</kbd>, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-440 image-border" src="assets/f936c844-bdf7-4f9f-87a7-0531cad7c1e5.png" style="width:29.08em;height:23.00em;"/></p>
<p><span>The shape of each point is set according to the iris species, while the color is set by the output of k-means, that is, which cluster the algorithm has assigned each data point to. What we can now see is that the clusters match the species of each iris almost exactly: k-means has been able to subdivide the data into three distinct groups that correspond to the different species.</span></p>
<p class="mce-root"><span>While k-means works very well in this case, you might find that you need to use a different algorithm on your own datasets. The scikit-learn library for Python comes with a useful demonstration of which algorithms work best on different types of dataset<sup>[5]</sup></span><span>. You might also find that it is helpful to prepare your data in some way; for example, normalize it or apply a non-linear transformation to it. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Principal component analysis</h1>
                </header>
            
            <article>
                
<p><strong>Principal component analysis</strong> (<strong>PCA</strong>) is a way to reduce the number of dimensions in a dataset. We can think of it as a way of compressing a dataset. Suppose you have 100 different variables in your dataset. It may be the case that many of these variables are correlated with each other. If this is the case, then it is possible to explain most of the variation in the data by combining variables to build a smaller set of data. PCA performs this task: it tries to find linear combinations of your input variables, and reports how much variation is explained by each combination.</p>
<div class="packt_infobox">PCA is a method for reducing the dimensions in a dataset: in effect, summarizing it so that you can focus on the most important features, which explain most of the variation in the dataset.</div>
<p>PCA can be useful for machine learning <span>in two ways</span>:</p>
<ul>
<li>It can be a useful preprocessing step before applying a supervised learning method. After running PCA on your data, you may discover, for instance, that 95% of the variation is explained by only a handful of variables. You can use this knowledge to reduce the number of variables in your input data, which means that your subsequent model will train much faster.</li>
<li>It can also be helpful when visualizing a dataset prior to building a model. If your data has more than three variables, it can be very hard to visualize it on a graph and understand what patterns it contains. PCA lets you transform the data so that you can plot just the most important aspects of it.</li>
</ul>
<p>For our example, we are going to use PCA to visualize the iris dataset. Currently, this has four input features: petal width, petal length, sepal width, and sepal length. Using PCA, we can reduce this down to two variables, which we can then visualize easily on a scatter plot.</p>
<ol>
<li>Start by loading the sepal data as before, and normalizing it as follows:</li>
</ol>
<pre style="padding-left: 60px">df = Standardise(df, "petal length")<br/>df = Standardise(df, "petal width")<br/>df = Standardise(df, "sepal length")<br/>df = Standardise(df, "sepal width")<br/>labels := df.Col("species").Float()<br/>df = DropColumn(df, "species")</pre>
<ol start="2">
<li>Next, we need to convert the data into a matrix format. The <kbd>gonum</kbd> library has a <kbd>mat64</kbd> type that we can use for this purpose:</li>
</ol>
<pre style="padding-left: 60px">import (<br/>    "github.com/gonum/matrix/mat64"<br/>)<br/><br/>// DataFrameToMatrix converts the given dataframe to a gonum matrix<br/>func DataFrameToMatrix(df dataframe.DataFrame) mat64.Matrix {<br/>    var x []float64 //slice to hold matrix entries in row-major order<br/>    <br/>    for i := 0; i &lt; df.Nrow(); i++ {<br/>        for j := 0; j &lt; df.Ncol(); j ++ {<br/>            x = append(x, df.Elem(i,j).Float())<br/>        } <br/>    }<br/>    return mat64.NewDense(df.Nrow(), df.Ncol(), x)<br/>}<br/><br/>features := DataFrameToMatrix(df)</pre>
<div class="packt_infobox">PCA works by finding the <strong>eigenvectors</strong> and <strong>eigenvalues</strong> of the dataset. For this reason, most software libraries need the data to be in a matrix structure, so that standard linear algebra routines such as <strong>blas</strong> and <strong>lapack</strong> can be used to do the calculations.</div>
<ol start="3">
<li>Now, we can make use of gonum's <kbd>stat</kbd> package for the PCA implementation:</li>
</ol>
<pre style="padding-left: 60px">model := stat.PC{}<br/>if ok := model.PrincipalComponents(features, nil); !ok {<br/>  fmt.Println("Error!")<br/>}<br/>variances := model.Vars(nil)<br/>components := model.Vectors(nil)</pre>
<p style="padding-left: 60px">This gives us two variables: <kbd>components</kbd>, which is a matrix telling us how to map the original variables to the new components; and <kbd>variances</kbd>, which tells us how much variance is explained by each component. If we print out the variance in each component, we can see that the first two explain 96% of the entire dataset (component 1 to 73%, and component 2 to 23%):</p>
<pre style="padding-left: 60px">total_variance := 0.0<br/>for i := range variances {<br/>  total_variance += variances[i]<br/>}<br/><br/>for i := range variances {<br/>  fmt.Printf("Component %d: %5.3f\n", i+1, variances[i]/total_variance)<br/>}</pre>
<ol start="4">
<li>Finally, we can transform the data into the new components, and keep the first two so that we can use them for visualization:</li>
</ol>
<pre style="padding-left: 60px">transform := mat64.NewDense(df.Nrow(), 4, nil)<br/>transform.Mul(features, components)<br/><br/>func PCAToScatterData(m mat64.Matrix, labels []float64) map[int]plotter.XYs {<br/>    ret := make(map[int]plotter.XYs)<br/>    nrows, _ := m.Dims()<br/>    for i := 0; i &lt; nrows; i++ {<br/>        var pt struct{X, Y float64}<br/>        pt.X = m.At(i, 0)<br/>        pt.Y = m.At(i, 1)<br/>        ret[int(labels[i])] = append(ret[int(labels[i])], pt)<br/>    }<br/>    return ret<br/>} <br/><br/>scatterData := PCAToScatterData(transform, labels)</pre>
<p>The following diagram shows each data point according to the first two principle components, while the colors indicate which iris species each one belongs to. We can now see that the three groups form distinct bands along the first component, which we could not have easily seen when plotting the four original input features against one another:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-441 image-border" src="assets/8e4e724c-c63c-4871-95ce-1d20bbf7ac30.png" style="width:23.58em;height:18.75em;"/></p>
<p>You could now try training a supervised learning model to use the first two PCA features to predict the iris species: compare its performance against a model trained on all four input features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have covered two common techniques in unsupervised machine learning. Both are often used by data scientists for exploratory analysis, but can also form part of a data processing pipeline in a production system. You have learned how to <span>train a clustering algorithm to divide data automatically into groups. This technique might be used to categorize newly registered customers on an e-commerce website, so that they can be served with personalized information. We also introduced p</span>rincipal component analysis as a means of compressing data, in other words, reducing its dimensionality. This may be used as a preprocessing step before running a supervised learning technique in order to reduce the size of the dataset.</p>
<p>In both cases, it is possible to make use of the <kbd>gonum</kbd> and <kbd>goml</kbd> libraries to build efficient implementations in Go with minimal code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further readings</h1>
                </header>
            
            <article>
                
<ol>
<li><a href="https://deepmind.com/blog/unsupervised-learning/">https://deepmind.com/blog/unsupervised-learning/</a>. Retrieved April 12, 2019.</li>
<li><a href="https://blogs.nvidia.com/blog/2019/03/18/gaugan-photorealistic-landscapes-nvidia-research/">https://blogs.nvidia.com/blog/2019/03/18/gaugan-photorealistic-landscapes-nvidia-research/</a>. <span>Retrieved April 12, 2019.</span></li>
<li><a href="https://github.com/junyanz/CycleGAN">https://github.com/junyanz/CycleGAN</a>. <span>Retrieved April 12, 2019</span>.</li>
<li><a href="http://robots.stanford.edu/papers/dahlkamp.adaptvision06.pdf">http://robots.stanford.edu/papers/dahlkamp.adaptvision06.pdf</a>. <span>Retrieved April 13, 2019</span>.</li>
<li><a href="https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods">https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods</a>. <span>Retrieved April 12, 2019</span>.</li>
</ol>


            </article>

            
        </section>
    </body></html>