- en: Neural Networks with TensorFlow and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural network is a supervised learning algorithm that is loosely inspired by
    the way the brain functions. Similarly to the way neurons are connected to each
    other in the brain, a neural network takes an input and passes it through a function,
    based on which certain subsequent neurons get excited, and the output is produced.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus on the practical implementation of neural networks
    with TensorFlow and Keras. TensorFlow provides a low-level framework to create
    neural network models. Keras is a high-level neural network API that significantly
    simplifies the task of defining neural network models. We''ll show how to use
    Keras on top of TensorFlow to define and train models on GCP. We''ll present the
    Keras API in Python and work with a simple feedforward network applied on the
    classic MNIST dataset. Also, we will go through the different components of a
    neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics and loss functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depth of the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The origin of neural networks comes from the fact that every function cannot
    be approximated by a linear/logistic regression—there can be potentially complex
    shapes within data that can only be approximated by complex functions.
  prefs: []
  type: TYPE_NORMAL
- en: The more complex the function (with some way to take care of overfitting), the
    better the prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The following image explains the way in which neural networks work towards fitting
    data into a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical structure of a neural network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/865559ee-6429-4f87-814d-e3cf61f67be0.png)'
  prefs: []
  type: TYPE_IMG
- en: The input level/layer in this diagram is typically made up of the independent
    variables that are used to predict the output (dependent variable) level or layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden level/layer is used to transform the input variables into a higher-order
    function. The way in which a hidden layer transforms the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4a500a3-be3d-4cb7-ae44-32b5749d9bce.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, *x[1]*, *x[2]*, ..., *x[n]* are the independent variables
    and *x[0]* is the bias term (similar to the way we have a bias in linear/logistic
    regression).
  prefs: []
  type: TYPE_NORMAL
- en: '*w[1]*, *w[2]*, ..., *w[n]* are the weights given to each of the input variables.
    If *a* is one of the neurons in the hidden layer, it would be equal to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64ad64ea-3c02-4406-8d03-e44531e9d5d5.png)'
  prefs: []
  type: TYPE_IMG
- en: The function that we see in this equation is the activation function that we
    are applying on top of the summation so that we attain nonlinearity. We need nonlinearity
    so that our model can learn complex patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, having more than one hidden layer helps in achieving a high amount
    of nonlinearity.
  prefs: []
  type: TYPE_NORMAL
- en: A detail of the various parameters that can be tweaked in a neural network will
    be provided in the subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Google Cloud Datalab
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to set up Google Cloud Datalab, we click on the Cloud Shell icon:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/736905a6-f3b1-47a2-aaeb-53b4119139a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Within the Cloud Shell, set the project that needs to be worked on, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the project is set, configure the zone as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, create a Datalab instance by specifying:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a CPU version:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For a GPU version:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, you need to request a GPU version through the quotas page, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe6460f0-3652-461f-9321-19d69eaddf09.png)'
  prefs: []
  type: TYPE_IMG
- en: Submit the quota request and you should receive the permission to use GPU in
    the given region soon.
  prefs: []
  type: TYPE_NORMAL
- en: Note that a GPU version is better while building neural network models as the
    multiple processors in the GPU can then work on updating multiple weights of a
    neural network in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Change the port to `8081` to open Datalab and thereby the notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and importing the required packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow, as a package, is built to perform neural network computations. It
    works with the lazy evaluation concept, where the various elements of a neural
    network connection are to be specified, before executing the code.
  prefs: []
  type: TYPE_NORMAL
- en: Another API named Keras makes building neural networks a lot easier. In this
    chapter, we will be first leveraging the Keras package with TensorFlow running
    in its backend, and then we'll show how to build a neural network using the premade
    estimator and a custom estimator in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we understood how to set up Datalab notebooks. In
    this chapter, we will see how to install and import the required packages into
    Datalab notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Datalab comes with a preinstalled TensorFlow package. However,
    it does not contain Keras by default. Let''s look at installing the `keras` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once Keras is installed, let''s import both the required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Working details of a simple neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to understand how neural networks work, we will build a very simple
    network. The input and the expected output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that `x` is the input dataset with two variables for each of the two rows.
    `y` is the expected output for the two inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, we have the input and output layers in place.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, for one of the preceding data points, the input and the output
    values of the network will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1de07bf-73a8-44e7-8719-8c01f8e8122b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In traditional machine learning, you would find the relation directly between
    the input and output values. However, the neural network architecture works with
    the following intuition:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"The input values can be represented in a richer (higher) dimensional space.
    The more the dimensions in which the input values are represented, the more is
    the complexity in the input dataset captured."*'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the preceding intuition, let''s build a hidden layer with three units
    in a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc46a2d8-8de2-4b5b-9bc2-dbee9603b1a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that the layer is built, let''s make connections between each unit, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3984e44-7444-4192-870e-4d9b77380dd1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that a connection between each unit is made, there will be a certain amount
    of weightage that is associated with each connection. In the following diagram,
    we will initialize the weight that each connection represents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9917cca2-b2e9-47f1-8765-7f6423fec1c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the weights **W** represent the strength of connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have built a simple neural network. Let''s randomly initialize the weight
    values between the input and hidden layers to understand how the hidden layer
    values are computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f25dab5f-fd2b-4b22-a21f-b9744f0c2437.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hidden layer values are computed as the multiplications of the input values
    and weights associated with them, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*h1 = 1*1 + 2*(2) = 5*'
  prefs: []
  type: TYPE_NORMAL
- en: '*h2 = 1*0.5 + 2*(-1) = -1.5*'
  prefs: []
  type: TYPE_NORMAL
- en: '*h3 = 1*(-0.2) + 2*0.1 = 0*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed023d95-dd5e-460a-9cc8-e39ffa0ae792.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that the hidden values are calculated, we pass them through an activation
    function. The intuition for an activation function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"The neural network in the state that we presented previously (without an
    activation function) is a big linear combination of input variables. Nonlinearity
    can only be obtained by performing an activation on top of the hidden layer values."*'
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, as of now, we will assume that the nonlinearity that we are
    going to apply is the sigmoid function.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sigmoid function works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It takes an input value, *x*, and transforms into a new value, *1/(1+exp(-x))*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The nonlinearity of a sigmoid curve looks like this for various values of *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6bd9cc7f-a00b-4fee-9499-db4cd4263c5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the hidden layer values, which were 5, -1.5, and 0, are transformed to
    **0.99**, **0.18**, and **0.5**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17f2320a-f801-4a04-bad1-839c73ddf6f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that the hidden layer values are computed, let's initialize the weights
    connecting the hidden layer to the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that again the weights are initialized randomly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b165d73c-fe58-45c8-bddb-ab9ec7da4d28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that the weights are initialized, let''s calculate the value associated
    with the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*0.99*1 + 0.18*(-1) + 0.5*0.2 = 0.91*'
  prefs: []
  type: TYPE_NORMAL
- en: The expected value at the output layer is *0.91*, while the actual value is
    0.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the loss associated in this scenario is *(0.91 - 0)^2 = 0.83*.
  prefs: []
  type: TYPE_NORMAL
- en: The process until now, where we calculate the loss corresponding to the weight
    values, is called the **feedforward process**.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, in this section, we have understood:'
  prefs: []
  type: TYPE_NORMAL
- en: Weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preceding scenario, while the loss function remains constant for a given
    objective that we try to solve, the weight initialization and activation functions
    can vary for different network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: The objective for the problem laid out just now would be to minimize the loss
    corresponding to a network architecture by iteratively varying the weights.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the preceding architecture, the loss can be reduced by changing
    the final weight from the hidden layer to the output layer connection from *0.2*
    to *0.1*. Once the weight is changed, the loss reduces from *0.83* to *0.74*.
  prefs: []
  type: TYPE_NORMAL
- en: The process by which weights are changed iteratively to minimize the loss value
    is called **backpropagation**.
  prefs: []
  type: TYPE_NORMAL
- en: The number of times a weight change happens per given dataset is called the
    **epoch**. Essentially, an epoch constitutes feedforward and backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: One of the techniques to intelligently arrive at the optimal weight values is
    called **gradient descent**—more on various weight optimizers in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we have seen the intuition of how weights are updated
    in backpropagation. In this section, we will see the details of how the weight
    update process works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1388d54b-4868-4ad2-a2d0-084e37258c47.png)'
  prefs: []
  type: TYPE_IMG
- en: In the backpropagation process, we start with the weights at the end of the
    neural network and work backwards.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram (1), we iteratively change the values of weights by
    a small amount (0.01) for each of the weights connecting the hidden layer to the
    output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Original weight** | **Changed weight** | **Error** | **Reduction in error**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.01 | 0.84261 | -1.811 |'
  prefs: []
  type: TYPE_TB
- en: '| -1 | -0.99 | 0.849 | -0.32 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.2 | 0.21 | 0.837 | -0.91 |'
  prefs: []
  type: TYPE_TB
- en: 'From the preceding table, we notice that instead of increasing the weight values,
    one should reduce them to improve the error:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Original weight** | **Changed weight** | **Error** | **Reduction in error**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.99 | 0.8108 | 1.792 |'
  prefs: []
  type: TYPE_TB
- en: '| -1 | -1.01 | 0.8248 | 0.327 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.2 | 0.19 | 0.819 | 0.9075 |'
  prefs: []
  type: TYPE_TB
- en: Now we note that, for some weight updates, the improvement in error is high,
    while for some other weight updates, the improvement in error is low.
  prefs: []
  type: TYPE_NORMAL
- en: This indicates that, for some weights for which error is improving by a lot,
    the weight update could be faster; while for some weights for which the error
    improvement is relatively low, the weight update could be slower.
  prefs: []
  type: TYPE_NORMAL
- en: 'The changed weight for the weight with a value of 1 could then be:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Changed weight = original weight + learning rate X reduction in error*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let''s assume that the learning rate is *0.05*; then:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Changed weight = 1 + 0.05*(1.792) = 1.089*'
  prefs: []
  type: TYPE_NORMAL
- en: The other weights will be changed using the same formula.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, the learning rate helps us in building trust in the algorithm.
    For example, when deciding on the magnitude of a weight update, we would potentially
    not change everything in one go but take a more careful approach in updating the
    weights more slowly.
  prefs: []
  type: TYPE_NORMAL
- en: Once all the weights are updated using the process laid out, the backpropagation
    process is done and we proceed with forward propagation again.
  prefs: []
  type: TYPE_NORMAL
- en: A feedforward and a backpropagation step are together called an **epoch**.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we calculated the error value in predicting a data point at a time,
    thus forming a batch size of 1\. In practice, we calculate the error values for
    a group of data points and then keep updating the weights using a batch of data
    rather than a single data point.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a simple neural network in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the preceding discussion, we have seen that the key components in a neural
    network are:'
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation in a hidden layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Along with these, there are a few other key components in a neural network.
    However, we will learn about them in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we will build a neural network model with the given toy dataset in
    Keras, with the knowledge we''ve gained in the sections so far:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51b9d334-61a2-4fb3-812a-231a893e4a0a.png)'
  prefs: []
  type: TYPE_IMG
- en: The sequential model is a linear stack of layers (input, hidden, and output).
  prefs: []
  type: TYPE_NORMAL
- en: Within each layer, `dense` helps in implementing the operations specified in
    the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us go ahead and build the network as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9edb3c7f-67a4-441c-af99-5eef74270293.png)'
  prefs: []
  type: TYPE_IMG
- en: In our data, we take the input dataset, which is two-dimensional, and convert
    it into a three-dimensional hidden layer unit in the first step.
  prefs: []
  type: TYPE_NORMAL
- en: Once the hidden layer values are calculated, we pass them through a sigmoid
    activation in the second step.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding two steps are captured in the second line of the model specification.
  prefs: []
  type: TYPE_NORMAL
- en: From the hidden layer, we connect it to an output layer that is one-dimensional,
    and hence the third line of code has `Dense(1)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the summary of the model that we specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7356041-3d62-45a9-86da-c8dae010d184.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s understand the output shape column for the preceding summary: `(None,
    3)`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`None` means that the output is agnostic of the number of inputs (not to be
    confused with the dimension of inputs). `3` represents the number of units in
    the hidden layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly `(None,1)` in the second layer represents the dimension of the output
    layer (which has only one unit in the output layer).
  prefs: []
  type: TYPE_NORMAL
- en: '`Param #` represents the number of parameters associated with the network.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the connections between the input and hidden layers have a total of
    nine parameters, as there are six weight values (as shown in the diagram in the
    previous section) and three bias terms associated with each unit in the hidden
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, there are four parameters in the connection between the hidden and
    output layers, as there are three weight values between the hidden and output
    layers and one bias term associated with the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the network architecture is specified, let''s compile the model, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b05fbe6d-bd08-49cc-b2ed-a6c07c011ce3.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding line of code, we are specifying that the loss is calculated
    based on mean squared error, which is the average of the squared difference between
    actual and predicted values across all data points in the input.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we specify that the optimization technique is based on stochastic
    gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the model structure, the loss function that we are computing, and optimization
    technique that we are using are specified, let's fit the model on the input and
    output values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The additional metrics that we need to specify while fitting the model are:'
  prefs: []
  type: TYPE_NORMAL
- en: Input and output values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of epochs to be run on the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/26661b76-cd2a-4e69-93c2-2e950eaf36d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the input and output variables that we specified are `x`, `y`.
  prefs: []
  type: TYPE_NORMAL
- en: Also, you should notice that the loss values decrease over different epochs,
    as the weight values are adjusted to minimize the loss as much as possible over
    the 10 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the model is built, let''s look at obtaining the weight values at
    each layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c51afcf7-0d82-486b-98e5-0d9b59acd7dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The values corresponding to a new input value can now be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1ebc879-11d9-4654-9f39-a8859a799e9e.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code snippet, we have initialized a new input and predicted
    the output corresponding to this new input using the optimal weights that were
    obtained by running the model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand how the output is obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtain the values corresponding to the three units in the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*h1 = 2*(-0.985) + 5*(-0.3587) + 0.00195 = -3.76*'
  prefs: []
  type: TYPE_NORMAL
- en: '*h2 = 2*0.537 + 5*(-0.8225) + 0.0011 = -3.025*'
  prefs: []
  type: TYPE_NORMAL
- en: '*h3 = 2*(-0.24) + 5*0.98 - 0.0027 = 4.421*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the hidden layer values are calculated, we pass them through the sigmoid
    activation function, as specified in the model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '*final h1 = sigmoid(h1) = 0.0226*'
  prefs: []
  type: TYPE_NORMAL
- en: '*final h2 = sigmoid(h2) = 0.0462*'
  prefs: []
  type: TYPE_NORMAL
- en: '*final h3 = sigmoid(h3) = 0.988*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the final hidden layer unit values are obtained, we multiply them with
    the weights connecting hidden layer to output layer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Output = 0.0226 * 0.834+ 0.0462*0.6618 + (-0.401)*0.988 + 0.14615 = -0.20051*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the value we obtained is the same value that was obtained in the `model.predict`
    function. This proves the architecture functionality that we have learnt so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have built the model, let''s re-execute our code and see whether
    the results remain the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e49c79d-154a-451b-8f57-58517b12a09a.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the loss values are different from what we obtained in the previous
    iteration. This is because weights are randomly initialized in the first epoch
    of a neural network run. One way to fix this is by setting a seed. A seed helps
    in initializing the same set of random values every time a neural network runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the seed should be run every time a model is rebuilt. The code snippet
    for setting a seed looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0076a96-ceac-44f4-b3e4-f824fc296f81.png)'
  prefs: []
  type: TYPE_IMG
- en: Understanding the various loss functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, there are two types of dependent variables—continuous
    and categorical variables. In the case of continuous variable prediction, the
    loss (error) function can be calculated by using the sum of squared error values
    across all predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases where the dependent variable is a categorical variable with only two
    distinct values associated with it, loss is calculated as the binary cross-entropy
    error using this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y*logp + (1-y)*log(1-p)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases where the dependent variable is a categorical variable with multiple
    distinct values, the loss is calculated using the categorical cross-entropy error
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*∑ y*logp*'
  prefs: []
  type: TYPE_NORMAL
- en: Where *p* is the probability of the event being a 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Categorical variables are typically one-hot encoded in practice as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's say the output across three different rows is `[1,2,3]`; then the output
    values are represented as `[[1,0,0], [0,1,0], [0,0,1]]`. where each index value
    represents whether a distinct value is present or not. In the above example, the
    zeroth index corresponds to 1 and hence only the first row has a value of 1 for
    zeroth while the rest have a value of 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other loss functions that are available in Keras are:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean absolute percentage error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean squared logarithmic error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Squared hinge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical hinge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logcosh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softmax activation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the preceding section, we should notice that in the case of categorical
    variable prediction, the number of units in the output layer would be the same
    as the number of distinct values in the dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that the predicted value cannot be greater than 1 or less than 0
    for any of the units in the output layer. At the same time, the sum of the values
    across all nodes in the output should be equal to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say the output across two nodes of output is -1 and 5\.
    Given that the expected value of outputs should be between 0 and 1 (the probability
    of an event happening), we pass the output values through softmax activation,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pass the values through an exponential function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*exp(-1) = 0.367*'
  prefs: []
  type: TYPE_NORMAL
- en: '*exp(5) = 148*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Normalize the output values to obtain a probability between 0 to 1 and also
    to ensure that the sum of probabilities between the two output nodes is 1:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*0.367/(0.367+148) =0.001*'
  prefs: []
  type: TYPE_NORMAL
- en: '*148/(0.367+148) = 0.999*'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the softmax activation helps us in converting the output values into probability
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Building a more complex network in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have built a neural network that is fairly simple. A traditional
    neural network would have a few more parameters that can be varied to achieve
    a better predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand them by using the classic MNIST dataset. MNIST is a handwritten
    digit dataset that contains images of size 28 x 28 pixels that are represented
    as NumPy arrays of 28 x 28 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Each image is of a digit and the challenge in hand is to predict the digit the
    image corresponds to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s download and explore some of the images present in the MNIST dataset,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f34a52e8-4f0d-44fa-9495-131e838eedb9.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code snippet, we are importing the MNIST object and downloading
    the MNIST dataset using the `load_data` function.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that the `load_data` function helps in automatically splitting the
    MNIST dataset into train and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize one of the images within the train dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0241eec9-501f-4751-b3d3-92f08e45db49.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the preceding digit is 5 and the grid that we are seeing is 28 x 28
    in size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the shapes of input and output to further understand the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3f4fa73-b959-48af-b3f1-914a0f973d38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Given that each input image is 28 x 28 in size, let''s flatten it to get the
    scores of the `784` pixel values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04f96bfc-b9bc-4773-8ce4-878cf33925e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output layer needs to predict whether the image corresponds to one of the
    digits from 0 to 9\. Thus, the output layer consists of 10 units corresponding
    to each of the 10 different digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13a72a8e-6d12-4524-a411-bab582b3cd45.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code, `to_categorical` provides a one hot-encoded version of
    the label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the train and test datasets in place, let''s go ahead and
    build the architecture of neural network in the following section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c02cda27-9b0a-4fb8-bcda-67e5a506bb3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that `batch_size` in the preceding screenshot refers to the number of
    data points that are considered to update weights. The intuition for batch size
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"If, in a dataset of 1,000 data points, the batch size is 100, then there
    are 10 weight updates while sweeping through the whole data"*.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the accuracy in predicting the labels on the test dataset is ∼91%.
  prefs: []
  type: TYPE_NORMAL
- en: This accuracy increases to 94.9% once the number of epochs reaches 300\. Note
    that for an accuracy of 94.9% on the test dataset, the accuracy on the train dataset
    is ∼99%.
  prefs: []
  type: TYPE_NORMAL
- en: This is a classic case of overfitting, and the ways to deal with it will be
    discussed in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have considered only the sigmoid activation function in a hidden
    layer. However, there are quite a few other activation functions that are useful
    in building a neural network. This chart gives the details of various activation
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fa34ec3-ebff-4b57-9437-f0da6dc3ccc6.png)'
  prefs: []
  type: TYPE_IMG
- en: The more commonly used activation functions are ReLU, TanH, and logistic or
    sigmoid activations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore the accuracy on the test dataset for various activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe94c2b5-ed6b-439e-b860-635d1068f7c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the accuracy on the test dataset is a mere 29.75% when using ReLU
    activation.
  prefs: []
  type: TYPE_NORMAL
- en: However, while performing ReLU activation, it is always a good idea to scale
    the data before fitting the model. Scaling is a way of reducing the magnitude
    of all the values in the input dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s scale the inputs first, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be4ffadd-9399-4638-b695-f58eba88976a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s rerun the model and see the accuracy on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e95b47e-5803-4f46-9e0b-dce2c9c006b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that after running 10 iterations, the accuracy on the test dataset is 88.1%.
    Now, let's run the model for 300 epochs so that we can compare the outputs of
    sigmoid activation and ReLU activation.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that the accuracy on the test dataset is 95.76%, which is slightly
    better than the sigmoid activation accuracy. However, the accuracy on the train
    dataset is 96%, which indicates that it is not likely to overfit on the dataset;
    hence, more epochs might further increase the accuracy on the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let's rerun the model using TanH activation without scaling first and with scaling
    later.
  prefs: []
  type: TYPE_NORMAL
- en: When the model is run on unscaled data, the accuracy after 10 epochs is 92.89%,
    and after 300 epochs, it is 94.6%.
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy on the test data once we scale the input dataset is 88% after 10
    epochs and 93% after 300 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the issue of overfitting does not arise (train dataset accuracy is
    much higher than test dataset accuracy) when the datasets are scaled, irrespective
    of the activation function used.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we explored various activation functions and noticed
    that the ReLU activation function gives a better result when run over a high number
    of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at the impact of varying the optimizer while the
    activation function remains ReLU on the scaled dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The various loss functions and their corresponding accuracies on the test dataset
    when run for 10 epochs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Optimizer** | **Test dataset accuracy** |'
  prefs: []
  type: TYPE_TB
- en: '| SGD | 88% |'
  prefs: []
  type: TYPE_TB
- en: '| RMSprop | 98.44% |'
  prefs: []
  type: TYPE_TB
- en: '| Adam | 98.4% |'
  prefs: []
  type: TYPE_TB
- en: Now we have seen that RMSprop and Adam optimizers perform better than the stochastic
    gradient descent optimizer; let's look at the other parameter within an optimizer
    that can be modified to improve the accuracy of the model—learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning rate of an optimizer can be varied by specifying it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bce8d1f4-5631-4b31-9079-3de0738f108a.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code snippet, `lr` represents learning rate. The typical values
    of learning rate vary between 0.001 and 0.1.
  prefs: []
  type: TYPE_NORMAL
- en: On the MNIST dataset, the accuracy did not improve further when we changed the
    learning rate; however, typically for a lower learning rate, more epochs are required
    to reach the same amount of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the depth of network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An increase in the depth of a hidden layer is the same as increasing the number
    of hidden layers in a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, for a higher number of hidden units in a hidden layer and/or higher
    number of hidden layers, the predictions are more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that the Adam optimizer or RMSprop has a saturated accuracy after certain
    number of epochs, let''s switch back to stochastic gradient descent to understand
    the accuracy when the model is run for 300 epochs; but we are using more number
    of units in the hidden layer this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed5b6e76-c5a8-4d5e-b74a-64472e449166.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that by using 2,000 units in the hidden layer, our accuracy increases to
    95.76% by the end of 300 epochs. This is potentially because the input can now
    be expressed in a higher dimensional space, and hence a better representation
    can be learned when compared to the 1,000-dimensional space scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will increase the number of hidden layers to understand the impact
    of building deep neural networks on accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4230117b-a90e-4e2c-b2c8-ba6c95dc5557.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that when the network is deep, with two hidden layers instead of one, the
    accuracy after 300 epochs is 97.24%, which is a clear improvement when compared
    to the single hidden layer network.
  prefs: []
  type: TYPE_NORMAL
- en: Similar, to the way in which the network learned more complex representations
    of data when the number of hidden units in a layer increased, the network also
    learned complex representations of data when the number of hidden layers increased.
  prefs: []
  type: TYPE_NORMAL
- en: Impact on change in batch size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed earlier, the lesser the batch size, the more often the weights
    get updated in a given neural network. This results in a lesser number of epochs
    required to achieve a certain accuracy on the network. At the same time, if the
    batch size is too low, the network structure might result in instability in the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compare the previously built network with a lower batch size in one
    scenario and a bigger batch size in the next scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3e45aaa-8509-4236-a5dc-119658fddd62.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that in the preceding scenario, where the batch size is very high, the
    test dataset accuracy at the end of 300 epochs is only 89.91%.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is that the network with batch size 1,024 would have learned
    the weights much faster than the network with batch size 30,000, as the number
    of weight updates is much higher when the batch size is lower.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next scenario, we will reduce the batch size to a very small number
    to see the impact on network accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1f1e527-2398-405b-8b81-f4fb18872520.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that while accuracy improves considerably very quickly to 97.77% within
    10 epochs itself, it takes significant time to produce results, as the number
    of weight updates is high per epoch. This results in more calculations and thus
    more time to execute.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing neural networks in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In previous sections, we have understood how a neural network works and also
    how to build a neural network model in Keras. In this section, we will be working
    toward building a neural network model in TensorFlow. There are two ways in which
    we can build models in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: Using premade estimators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining custom estimators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using premade estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Premade estimators are similar to the methods available in packages such as
    scikit-learn, where the input features and output labels are specified, along
    with the various hyperparameters. A method can then optimize for solving a loss
    function that is predefined to a default value but can be varied by passing a
    different function in a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore building the training and test datasets in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b7a409f4-1dd3-4f2f-9290-0dad3ff2b104.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Import the dataset. We will work on the `MNIST` dataset for this exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b940c4c3-e1c2-445f-8467-90e4b6790406.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The shapes of images and labels are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/809adaf0-0328-4a47-a4cf-3114e34e6635.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The premade function works on label value instead of the one-hot encoded version.
    Let''s convert the one-hot encoded label into a value, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5baad61b-cf52-426e-a337-d9630167f0d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s understand how the data points look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99112603-b35c-4f9c-9b17-bdb3a1749bbc.png)![](img/6a89b41a-b076-4a78-9155-e777a6988c16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Input the dataset into a function that consumes the independent (`x`) and dependent
    (`y`) variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3f55e04a-fbe8-4980-b108-93febe1843be.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we named the independent variables as `x2` and dependent variable
    as `y`.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that we have passed the arrays that form the independent and dependent
    variable values.
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size` indicates the number of training examples that are consumed to
    calculate the loss function, and `num_epochs = None` indicates that the number
    of epochs to be run will be provided later.'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_input_fn` returns features and labels, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bad45146-8465-4949-aab3-ef433ae3013a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we pass the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0dcc8b8-d5b3-4b53-ae3a-4854fb1982f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in the case of the test dataset, `num_epochs = 1` as we pass it through
    only the feedforward for the test dataset once the model weights are derived from
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 'A dataset could potentially contain multiple columns, so let''s specify the
    feature column and its type, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f6f4e63-208b-43ff-a80b-8fe617bd421b.png)![](img/62dca6f6-d5ac-4671-ab7d-c764a436580d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If there are multiple columns, we would specify all the columns in a list,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_columns = [feature_x1, feature_x2]`'
  prefs: []
  type: TYPE_NORMAL
- en: Where `feature_x1` is one feature and `feature_x2` is another feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we shall specify the number of hidden layers and also the hidden units
    in each layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d9de532-1d77-4152-9fff-da1e0a314d25.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that by specifying the number of hidden units in the preceding way, we
    have specified that there are three hidden layers, where the first hidden layer
    has **512** units, the second hidden layer has **256** units, and the final hidden
    layer has **128** units.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have specified the features and hidden layers, let''s specify the
    architecture of neural network, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab29fc30-66ea-45c6-b222-70d96bd8e8c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have specified the model architecture, we can go ahead and train
    the model. If you would like to further change the hyperparameters that are available
    in the function, you can check out the hyperparameter levers that are available
    by using the `help` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49440387-51ad-428c-a503-4944dba2af7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code runs the neural network model for 2,000 epochs in total:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/742bca8a-62a9-4168-acbf-1320b8a2860d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that our model is run, let''s evaluate the accuracy on the test dataset,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dab4748b-8130-4ece-93a3-805dbb7ed300.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the accuracy of the model on the test dataset is 97.2%.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have been implementing a model using premade estimators; in the next
    sections, we will look into defining the model without premade estimators.
  prefs: []
  type: TYPE_NORMAL
- en: Creating custom estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A premade estimator limits the full potential to which TensorFlow can be used;
    for example, we would not be able to have different dropout values after different
    layers. In this regard, let''s go ahead and create a function of our own, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/138f6adc-4399-44c0-9025-4b9af8a89d6e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s explore each part of the preceding snippet of code in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a300918-5bfc-4b29-947b-b3e4d3fa44ac.png)'
  prefs: []
  type: TYPE_IMG
- en: The function takes features (independent variables) and labels (dependent variable)
    as input. `mode` indicates whether we want to train, predict, or evaluate the
    given data.
  prefs: []
  type: TYPE_NORMAL
- en: '`params` provides us with the functionality to supply information about parameters;
    for example, learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d56831e5-22b5-45f0-94f3-0a6092653fe8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding snippet of code is similar to the way in which we defined model
    architecture in Keras, where we specified the inputs, the hidden layer activation,
    and the number of units in the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6829427b-481e-4d0f-bf9e-d79ee3d174eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If our mode is to predict the class, we would not have to train the model,
    but just pass the predicted class, thus estimator spec in such scenario would
    just need to calculate the `y_pred_cls` values, thus the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66dcb13b-ddfe-43af-ba70-6b1870d8331d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the mode is to train or test the model, we would have to calculate the loss
    and hence the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97320ae6-c050-4b42-b029-5c70ea927109.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code, the first line is used to define the cross-entropy calculation.
    The second line takes the average of cross entropy across all the rows.
  prefs: []
  type: TYPE_NORMAL
- en: '`optimizer` specifies the optimizer we are interested in and the learning rate.
    `train_op` specifies that we are interested in minimizing loss, and the `global_step`
    parameter keeps a count of the step (epoch) that the model is currently in. `metrics`
    specifies the metrics that we are interested in calculating, and the final `spec`
    that would be calculated would be a combination of all the preceding parameters
    that we have defined.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model architecture and the estimator spec that needs to be returned
    are defined, we define the parameters and mode as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e13a8c9-38ce-44c2-b097-44a682bf8753.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding code, the function learns the parameter that needs to be
    changed and also the model architecture that needs to be worked on (`model_fn`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f622206b-09cd-4071-8c50-26772df523dc.png)'
  prefs: []
  type: TYPE_IMG
- en: We run the model by specifying the mode (in this case, `train`) by a certain
    number of epochs (`2000` in this case).
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the model, we evaluate the model''s accuracy on the test dataset,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3d42e5a-24e4-4ba3-b678-6c10cb6c08f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned to set up Datalab to execute neural networks on
    Google Cloud. We also learned the structure of a neural network and how various
    parameters, such as depth, number of hidden units, activation function, optimizer,
    batch size, and number of epochs, impact the accuracy of the model. We also saw
    how to implement a neural network in both Keras and TensorFlow. Topics such as
    using premade estimators and creating custom estimators in TensorFlow were covered.
  prefs: []
  type: TYPE_NORMAL
