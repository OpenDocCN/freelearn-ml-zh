- en: Neural Networks with TensorFlow and Keras
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 和 Keras 的神经网络
- en: Neural network is a supervised learning algorithm that is loosely inspired by
    the way the brain functions. Similarly to the way neurons are connected to each
    other in the brain, a neural network takes an input and passes it through a function,
    based on which certain subsequent neurons get excited, and the output is produced.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一种受大脑功能启发的不严格监督学习算法。类似于大脑中神经元相互连接的方式，神经网络接收输入并通过一个函数传递它，基于此，某些后续神经元被激活，从而产生输出。
- en: 'In this chapter, we will focus on the practical implementation of neural networks
    with TensorFlow and Keras. TensorFlow provides a low-level framework to create
    neural network models. Keras is a high-level neural network API that significantly
    simplifies the task of defining neural network models. We''ll show how to use
    Keras on top of TensorFlow to define and train models on GCP. We''ll present the
    Keras API in Python and work with a simple feedforward network applied on the
    classic MNIST dataset. Also, we will go through the different components of a
    neural network:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将关注使用 TensorFlow 和 Keras 的神经网络的实际实现。TensorFlow 提供了一个低级框架来创建神经网络模型。Keras
    是一个高级神经网络 API，它显著简化了定义神经网络模型的任务。我们将展示如何在 TensorFlow 上使用 Keras 来定义和训练 GCP 上的模型。我们将以
    Python 中的 Keras API 展示，并使用经典 MNIST 数据集上的简单前馈网络进行操作。此外，我们还将介绍神经网络的各个组成部分：
- en: Initialization
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化
- en: Metrics and loss functions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标和损失函数
- en: Activation functions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Depth of the network
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络深度
- en: Overview of a neural network
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络概述
- en: The origin of neural networks comes from the fact that every function cannot
    be approximated by a linear/logistic regression—there can be potentially complex
    shapes within data that can only be approximated by complex functions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的起源在于每个函数都不能用线性/逻辑回归来近似——数据中可能存在只能由复杂函数近似的潜在复杂形状。
- en: The more complex the function (with some way to take care of overfitting), the
    better the prediction accuracy.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 函数越复杂（有某种处理过拟合的方法），预测精度就越好。
- en: The following image explains the way in which neural networks work towards fitting
    data into a model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像解释了神经网络如何将数据拟合到模型中的方式。
- en: 'The typical structure of a neural network is as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的典型结构如下：
- en: '![](img/865559ee-6429-4f87-814d-e3cf61f67be0.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/865559ee-6429-4f87-814d-e3cf61f67be0.png)'
- en: The input level/layer in this diagram is typically made up of the independent
    variables that are used to predict the output (dependent variable) level or layer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在此图中，输入层通常由用于预测输出（因变量）层或层的独立变量组成。
- en: 'The hidden level/layer is used to transform the input variables into a higher-order
    function. The way in which a hidden layer transforms the output is as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层用于将输入变量转换成高阶函数。隐藏层转换输出的方式如下：
- en: '![](img/a4a500a3-be3d-4cb7-ae44-32b5749d9bce.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4a500a3-be3d-4cb7-ae44-32b5749d9bce.png)'
- en: In the preceding diagram, *x[1]*, *x[2]*, ..., *x[n]* are the independent variables
    and *x[0]* is the bias term (similar to the way we have a bias in linear/logistic
    regression).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，*x[1]*、*x[2]*、...、*x[n]* 是独立变量，而 *x[0]* 是偏置项（类似于我们在线性/逻辑回归中有一个偏置的方式）。
- en: '*w[1]*, *w[2]*, ..., *w[n]* are the weights given to each of the input variables.
    If *a* is one of the neurons in the hidden layer, it would be equal to:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*w[1]*、*w[2]*、...、*w[n]* 是分配给每个输入变量的权重。如果 *a* 是隐藏层中的一个神经元，它将等于：'
- en: '![](img/64ad64ea-3c02-4406-8d03-e44531e9d5d5.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64ad64ea-3c02-4406-8d03-e44531e9d5d5.png)'
- en: The function that we see in this equation is the activation function that we
    are applying on top of the summation so that we attain nonlinearity. We need nonlinearity
    so that our model can learn complex patterns.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在此方程中我们看到的是我们在求和之上应用的激活函数，以便我们获得非线性。我们需要非线性，以便我们的模型能够学习复杂的模式。
- en: Moreover, having more than one hidden layer helps in achieving a high amount
    of nonlinearity.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，拥有多个隐藏层有助于实现高非线性。
- en: A detail of the various parameters that can be tweaked in a neural network will
    be provided in the subsequent sections.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的章节将提供可以调整的神经网络各种参数的详细信息。
- en: Setting up Google Cloud Datalab
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Google Cloud Datalab
- en: 'In order to set up Google Cloud Datalab, we click on the Cloud Shell icon:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置 Google Cloud Datalab，我们点击 Cloud Shell 图标：
- en: '![](img/736905a6-f3b1-47a2-aaeb-53b4119139a8.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/736905a6-f3b1-47a2-aaeb-53b4119139a8.png)'
- en: 'Within the Cloud Shell, set the project that needs to be worked on, as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the project is set, configure the zone as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, create a Datalab instance by specifying:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'For a CPU version:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For a GPU version:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, you need to request a GPU version through the quotas page, as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe6460f0-3652-461f-9321-19d69eaddf09.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Submit the quota request and you should receive the permission to use GPU in
    the given region soon.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Note that a GPU version is better while building neural network models as the
    multiple processors in the GPU can then work on updating multiple weights of a
    neural network in parallel.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Change the port to `8081` to open Datalab and thereby the notebooks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Installing and importing the required packages
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow, as a package, is built to perform neural network computations. It
    works with the lazy evaluation concept, where the various elements of a neural
    network connection are to be specified, before executing the code.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Another API named Keras makes building neural networks a lot easier. In this
    chapter, we will be first leveraging the Keras package with TensorFlow running
    in its backend, and then we'll show how to build a neural network using the premade
    estimator and a custom estimator in TensorFlow.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we understood how to set up Datalab notebooks. In
    this chapter, we will see how to install and import the required packages into
    Datalab notebooks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Datalab comes with a preinstalled TensorFlow package. However,
    it does not contain Keras by default. Let''s look at installing the `keras` package:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once Keras is installed, let''s import both the required packages:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Working details of a simple neural network
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to understand how neural networks work, we will build a very simple
    network. The input and the expected output are as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that `x` is the input dataset with two variables for each of the two rows.
    `y` is the expected output for the two inputs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, we have the input and output layers in place.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, for one of the preceding data points, the input and the output
    values of the network will look like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1de07bf-73a8-44e7-8719-8c01f8e8122b.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: 'In traditional machine learning, you would find the relation directly between
    the input and output values. However, the neural network architecture works with
    the following intuition:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '*"The input values can be represented in a richer (higher) dimensional space.
    The more the dimensions in which the input values are represented, the more is
    the complexity in the input dataset captured."*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'With the preceding intuition, let''s build a hidden layer with three units
    in a neural network:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc46a2d8-8de2-4b5b-9bc2-dbee9603b1a0.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: 'Now that the layer is built, let''s make connections between each unit, as
    follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3984e44-7444-4192-870e-4d9b77380dd1.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'Now that a connection between each unit is made, there will be a certain amount
    of weightage that is associated with each connection. In the following diagram,
    we will initialize the weight that each connection represents:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9917cca2-b2e9-47f1-8765-7f6423fec1c2.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: Note that the weights **W** represent the strength of connection.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have built a simple neural network. Let''s randomly initialize the weight
    values between the input and hidden layers to understand how the hidden layer
    values are computed:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f25dab5f-fd2b-4b22-a21f-b9744f0c2437.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'Hidden layer values are computed as the multiplications of the input values
    and weights associated with them, as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '*h1 = 1*1 + 2*(2) = 5*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '*h2 = 1*0.5 + 2*(-1) = -1.5*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '*h3 = 1*(-0.2) + 2*0.1 = 0*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed023d95-dd5e-460a-9cc8-e39ffa0ae792.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: 'Now that the hidden values are calculated, we pass them through an activation
    function. The intuition for an activation function is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '*"The neural network in the state that we presented previously (without an
    activation function) is a big linear combination of input variables. Nonlinearity
    can only be obtained by performing an activation on top of the hidden layer values."*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, as of now, we will assume that the nonlinearity that we are
    going to apply is the sigmoid function.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'A sigmoid function works as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: It takes an input value, *x*, and transforms into a new value, *1/(1+exp(-x))*
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The nonlinearity of a sigmoid curve looks like this for various values of *x*:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6bd9cc7f-a00b-4fee-9499-db4cd4263c5c.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the hidden layer values, which were 5, -1.5, and 0, are transformed to
    **0.99**, **0.18**, and **0.5**:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17f2320a-f801-4a04-bad1-839c73ddf6f7.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: Now that the hidden layer values are computed, let's initialize the weights
    connecting the hidden layer to the output layer.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that again the weights are initialized randomly:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b165d73c-fe58-45c8-bddb-ab9ec7da4d28.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: 'Now that the weights are initialized, let''s calculate the value associated
    with the output layer:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '*0.99*1 + 0.18*(-1) + 0.5*0.2 = 0.91*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: The expected value at the output layer is *0.91*, while the actual value is
    0.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the loss associated in this scenario is *(0.91 - 0)^2 = 0.83*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: The process until now, where we calculate the loss corresponding to the weight
    values, is called the **feedforward process**.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, in this section, we have understood:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Weights
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation function
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss calculation
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preceding scenario, while the loss function remains constant for a given
    objective that we try to solve, the weight initialization and activation functions
    can vary for different network architectures.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: The objective for the problem laid out just now would be to minimize the loss
    corresponding to a network architecture by iteratively varying the weights.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the preceding architecture, the loss can be reduced by changing
    the final weight from the hidden layer to the output layer connection from *0.2*
    to *0.1*. Once the weight is changed, the loss reduces from *0.83* to *0.74*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: The process by which weights are changed iteratively to minimize the loss value
    is called **backpropagation**.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: The number of times a weight change happens per given dataset is called the
    **epoch**. Essentially, an epoch constitutes feedforward and backpropagation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: One of the techniques to intelligently arrive at the optimal weight values is
    called **gradient descent**—more on various weight optimizers in a later section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we have seen the intuition of how weights are updated
    in backpropagation. In this section, we will see the details of how the weight
    update process works:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1388d54b-4868-4ad2-a2d0-084e37258c47.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: In the backpropagation process, we start with the weights at the end of the
    neural network and work backwards.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram (1), we iteratively change the values of weights by
    a small amount (0.01) for each of the weights connecting the hidden layer to the
    output layer:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '| **Original weight** | **Changed weight** | **Error** | **Reduction in error**
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.01 | 0.84261 | -1.811 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
- en: '| -1 | -0.99 | 0.849 | -0.32 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
- en: '| 0.2 | 0.21 | 0.837 | -0.91 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
- en: 'From the preceding table, we notice that instead of increasing the weight values,
    one should reduce them to improve the error:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '| **Original weight** | **Changed weight** | **Error** | **Reduction in error**
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.99 | 0.8108 | 1.792 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
- en: '| -1 | -1.01 | 0.8248 | 0.327 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
- en: '| 0.2 | 0.19 | 0.819 | 0.9075 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
- en: Now we note that, for some weight updates, the improvement in error is high,
    while for some other weight updates, the improvement in error is low.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: This indicates that, for some weights for which error is improving by a lot,
    the weight update could be faster; while for some weights for which the error
    improvement is relatively low, the weight update could be slower.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'The changed weight for the weight with a value of 1 could then be:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '*Changed weight = original weight + learning rate X reduction in error*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let''s assume that the learning rate is *0.05*; then:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '*Changed weight = 1 + 0.05*(1.792) = 1.089*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: The other weights will be changed using the same formula.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, the learning rate helps us in building trust in the algorithm.
    For example, when deciding on the magnitude of a weight update, we would potentially
    not change everything in one go but take a more careful approach in updating the
    weights more slowly.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Once all the weights are updated using the process laid out, the backpropagation
    process is done and we proceed with forward propagation again.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: A feedforward and a backpropagation step are together called an **epoch**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Note that we calculated the error value in predicting a data point at a time,
    thus forming a batch size of 1\. In practice, we calculate the error values for
    a group of data points and then keep updating the weights using a batch of data
    rather than a single data point.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a simple neural network in Keras
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the preceding discussion, we have seen that the key components in a neural
    network are:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layers
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation in a hidden layer
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss function
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Along with these, there are a few other key components in a neural network.
    However, we will learn about them in a later section.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we will build a neural network model with the given toy dataset in
    Keras, with the knowledge we''ve gained in the sections so far:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant functions:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51b9d334-61a2-4fb3-812a-231a893e4a0a.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: The sequential model is a linear stack of layers (input, hidden, and output).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Within each layer, `dense` helps in implementing the operations specified in
    the network.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us go ahead and build the network as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9edb3c7f-67a4-441c-af99-5eef74270293.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: In our data, we take the input dataset, which is two-dimensional, and convert
    it into a three-dimensional hidden layer unit in the first step.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Once the hidden layer values are calculated, we pass them through a sigmoid
    activation in the second step.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: The preceding two steps are captured in the second line of the model specification.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: From the hidden layer, we connect it to an output layer that is one-dimensional,
    and hence the third line of code has `Dense(1)`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the summary of the model that we specified:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7356041-3d62-45a9-86da-c8dae010d184.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'Let''s understand the output shape column for the preceding summary: `(None,
    3)`.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '`None` means that the output is agnostic of the number of inputs (not to be
    confused with the dimension of inputs). `3` represents the number of units in
    the hidden layer.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Similarly `(None,1)` in the second layer represents the dimension of the output
    layer (which has only one unit in the output layer).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '`Param #` represents the number of parameters associated with the network.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Note that the connections between the input and hidden layers have a total of
    nine parameters, as there are six weight values (as shown in the diagram in the
    previous section) and three bias terms associated with each unit in the hidden
    layer.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, there are four parameters in the connection between the hidden and
    output layers, as there are three weight values between the hidden and output
    layers and one bias term associated with the output layer.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the network architecture is specified, let''s compile the model, as
    follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b05fbe6d-bd08-49cc-b2ed-a6c07c011ce3.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: In the preceding line of code, we are specifying that the loss is calculated
    based on mean squared error, which is the average of the squared difference between
    actual and predicted values across all data points in the input.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we specify that the optimization technique is based on stochastic
    gradient descent.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Now that the model structure, the loss function that we are computing, and optimization
    technique that we are using are specified, let's fit the model on the input and
    output values.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'The additional metrics that we need to specify while fitting the model are:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Input and output values
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of epochs to be run on the model:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/26661b76-cd2a-4e69-93c2-2e950eaf36d3.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: Note that the input and output variables that we specified are `x`, `y`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Also, you should notice that the loss values decrease over different epochs,
    as the weight values are adjusted to minimize the loss as much as possible over
    the 10 epochs.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the model is built, let''s look at obtaining the weight values at
    each layer:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c51afcf7-0d82-486b-98e5-0d9b59acd7dc.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: 'The values corresponding to a new input value can now be calculated as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1ebc879-11d9-4654-9f39-a8859a799e9e.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: In the preceding code snippet, we have initialized a new input and predicted
    the output corresponding to this new input using the optimal weights that were
    obtained by running the model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand how the output is obtained.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtain the values corresponding to the three units in the hidden layer:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '*h1 = 2*(-0.985) + 5*(-0.3587) + 0.00195 = -3.76*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '*h2 = 2*0.537 + 5*(-0.8225) + 0.0011 = -3.025*'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '*h3 = 2*(-0.24) + 5*0.98 - 0.0027 = 4.421*'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the hidden layer values are calculated, we pass them through the sigmoid
    activation function, as specified in the model architecture:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '*final h1 = sigmoid(h1) = 0.0226*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '*final h2 = sigmoid(h2) = 0.0462*'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '*final h3 = sigmoid(h3) = 0.988*'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the final hidden layer unit values are obtained, we multiply them with
    the weights connecting hidden layer to output layer, as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '*Output = 0.0226 * 0.834+ 0.0462*0.6618 + (-0.401)*0.988 + 0.14615 = -0.20051*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Note that the value we obtained is the same value that was obtained in the `model.predict`
    function. This proves the architecture functionality that we have learnt so far.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have built the model, let''s re-execute our code and see whether
    the results remain the same:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e49c79d-154a-451b-8f57-58517b12a09a.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: Note that the loss values are different from what we obtained in the previous
    iteration. This is because weights are randomly initialized in the first epoch
    of a neural network run. One way to fix this is by setting a seed. A seed helps
    in initializing the same set of random values every time a neural network runs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the seed should be run every time a model is rebuilt. The code snippet
    for setting a seed looks like this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0076a96-ceac-44f4-b3e4-f824fc296f81.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: Understanding the various loss functions
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, there are two types of dependent variables—continuous
    and categorical variables. In the case of continuous variable prediction, the
    loss (error) function can be calculated by using the sum of squared error values
    across all predictions.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，存在两种类型的因变量——连续变量和分类变量。在连续变量预测的情况下，损失（误差）函数可以通过计算所有预测的平方误差值之和来计算。
- en: 'In cases where the dependent variable is a categorical variable with only two
    distinct values associated with it, loss is calculated as the binary cross-entropy
    error using this formula:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在因变量是与其相关联的唯一两个不同值的分类变量的情况下，损失是通过使用此公式的二进制交叉熵误差来计算的：
- en: '*y*logp + (1-y)*log(1-p)*'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*y*logp + (1-y)*log(1-p)*'
- en: 'In cases where the dependent variable is a categorical variable with multiple
    distinct values, the loss is calculated using the categorical cross-entropy error
    as:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在因变量是具有多个不同值的分类变量的情况下，损失是通过使用分类交叉熵误差来计算的：
- en: '*∑ y*logp*'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*∑ y*logp*'
- en: Where *p* is the probability of the event being a 1.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p* 是事件为1的概率。
- en: 'Categorical variables are typically one-hot encoded in practice as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，分类变量通常按以下方式进行one-hot编码：
- en: Let's say the output across three different rows is `[1,2,3]`; then the output
    values are represented as `[[1,0,0], [0,1,0], [0,0,1]]`. where each index value
    represents whether a distinct value is present or not. In the above example, the
    zeroth index corresponds to 1 and hence only the first row has a value of 1 for
    zeroth while the rest have a value of 0.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 假设三个不同行的输出为`[1,2,3]`；则输出值表示为`[[1,0,0], [0,1,0], [0,0,1]]`。其中每个索引值表示是否存在一个不同的值。在上面的例子中，零索引对应于1，因此只有第一行在零索引处有值为1，其余的值为0。
- en: 'The other loss functions that are available in Keras are:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中可用的其他损失函数包括：
- en: Mean absolute error
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方绝对误差
- en: Mean absolute percentage error
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方绝对百分比误差
- en: Mean squared logarithmic error
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方对数误差
- en: Squared hinge
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平方铰链
- en: Hinge
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 铰链
- en: Categorical hinge
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类铰链
- en: Logcosh
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logcosh
- en: Softmax activation
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax激活
- en: From the preceding section, we should notice that in the case of categorical
    variable prediction, the number of units in the output layer would be the same
    as the number of distinct values in the dependent variable.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的章节中，我们应该注意到，在分类变量预测的情况下，输出层的单元数将与因变量中不同值的数量相同。
- en: Also, note that the predicted value cannot be greater than 1 or less than 0
    for any of the units in the output layer. At the same time, the sum of the values
    across all nodes in the output should be equal to 1.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，输出层的任何单元的预测值都不能大于1或小于0。同时，输出层中所有节点值的总和应等于1。
- en: 'For example, let''s say the output across two nodes of output is -1 and 5\.
    Given that the expected value of outputs should be between 0 and 1 (the probability
    of an event happening), we pass the output values through softmax activation,
    as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设两个输出节点的输出值分别为-1和5。鉴于输出值的期望值应在0到1之间（即事件发生的概率），我们将输出值通过softmax激活函数传递，如下所示：
- en: 'Pass the values through an exponential function:'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过指数函数传递值：
- en: '*exp(-1) = 0.367*'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*exp(-1) = 0.367*'
- en: '*exp(5) = 148*'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*exp(5) = 148*'
- en: 'Normalize the output values to obtain a probability between 0 to 1 and also
    to ensure that the sum of probabilities between the two output nodes is 1:'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输出值归一化以获得介于0到1之间的概率，并确保两个输出节点之间的概率总和为1：
- en: '*0.367/(0.367+148) =0.001*'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*0.367/(0.367+148) =0.001*'
- en: '*148/(0.367+148) = 0.999*'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*148/(0.367+148) = 0.999*'
- en: Thus, the softmax activation helps us in converting the output values into probability
    numbers.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，softmax激活帮助我们将输出值转换为概率数。
- en: Building a more complex network in Keras
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Keras中构建更复杂的网络
- en: So far, we have built a neural network that is fairly simple. A traditional
    neural network would have a few more parameters that can be varied to achieve
    a better predictive power.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经构建了一个相当简单的神经网络。传统的神经网络会有更多可变的参数，以实现更好的预测能力。
- en: Let's understand them by using the classic MNIST dataset. MNIST is a handwritten
    digit dataset that contains images of size 28 x 28 pixels that are represented
    as NumPy arrays of 28 x 28 dimensions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用经典的MNIST数据集来理解它们。MNIST是一个包含28 x 28像素大小的手写数字数据集，这些图像表示为28 x 28维度的NumPy数组。
- en: Each image is of a digit and the challenge in hand is to predict the digit the
    image corresponds to.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像都是一个数字，当前的挑战是预测图像对应的数字。
- en: 'Let''s download and explore some of the images present in the MNIST dataset,
    as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f34a52e8-4f0d-44fa-9495-131e838eedb9.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: In the preceding code snippet, we are importing the MNIST object and downloading
    the MNIST dataset using the `load_data` function.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Also note that the `load_data` function helps in automatically splitting the
    MNIST dataset into train and test datasets.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize one of the images within the train dataset:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0241eec9-501f-4751-b3d3-92f08e45db49.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: Note that the preceding digit is 5 and the grid that we are seeing is 28 x 28
    in size.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the shapes of input and output to further understand the datasets:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3f4fa73-b959-48af-b3f1-914a0f973d38.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: 'Given that each input image is 28 x 28 in size, let''s flatten it to get the
    scores of the `784` pixel values:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04f96bfc-b9bc-4773-8ce4-878cf33925e4.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: 'The output layer needs to predict whether the image corresponds to one of the
    digits from 0 to 9\. Thus, the output layer consists of 10 units corresponding
    to each of the 10 different digits:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13a72a8e-6d12-4524-a411-bab582b3cd45.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: In the preceding code, `to_categorical` provides a one hot-encoded version of
    the label.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the train and test datasets in place, let''s go ahead and
    build the architecture of neural network in the following section:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c02cda27-9b0a-4fb8-bcda-67e5a506bb3e.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: 'Note that `batch_size` in the preceding screenshot refers to the number of
    data points that are considered to update weights. The intuition for batch size
    is:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '*"If, in a dataset of 1,000 data points, the batch size is 100, then there
    are 10 weight updates while sweeping through the whole data"*.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Note that the accuracy in predicting the labels on the test dataset is ∼91%.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: This accuracy increases to 94.9% once the number of epochs reaches 300\. Note
    that for an accuracy of 94.9% on the test dataset, the accuracy on the train dataset
    is ∼99%.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: This is a classic case of overfitting, and the ways to deal with it will be
    discussed in subsequent chapters.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have considered only the sigmoid activation function in a hidden
    layer. However, there are quite a few other activation functions that are useful
    in building a neural network. This chart gives the details of various activation
    functions:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fa34ec3-ebff-4b57-9437-f0da6dc3ccc6.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: The more commonly used activation functions are ReLU, TanH, and logistic or
    sigmoid activations.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore the accuracy on the test dataset for various activation functions:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe94c2b5-ed6b-439e-b860-635d1068f7c4.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: Note that the accuracy on the test dataset is a mere 29.75% when using ReLU
    activation.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: However, while performing ReLU activation, it is always a good idea to scale
    the data before fitting the model. Scaling is a way of reducing the magnitude
    of all the values in the input dataset.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s scale the inputs first, as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be4ffadd-9399-4638-b695-f58eba88976a.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s rerun the model and see the accuracy on the test dataset:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e95b47e-5803-4f46-9e0b-dce2c9c006b3.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: Note that after running 10 iterations, the accuracy on the test dataset is 88.1%.
    Now, let's run the model for 300 epochs so that we can compare the outputs of
    sigmoid activation and ReLU activation.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that the accuracy on the test dataset is 95.76%, which is slightly
    better than the sigmoid activation accuracy. However, the accuracy on the train
    dataset is 96%, which indicates that it is not likely to overfit on the dataset;
    hence, more epochs might further increase the accuracy on the test dataset.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Let's rerun the model using TanH activation without scaling first and with scaling
    later.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: When the model is run on unscaled data, the accuracy after 10 epochs is 92.89%,
    and after 300 epochs, it is 94.6%.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy on the test data once we scale the input dataset is 88% after 10
    epochs and 93% after 300 epochs.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Note that the issue of overfitting does not arise (train dataset accuracy is
    much higher than test dataset accuracy) when the datasets are scaled, irrespective
    of the activation function used.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we explored various activation functions and noticed
    that the ReLU activation function gives a better result when run over a high number
    of epochs.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at the impact of varying the optimizer while the
    activation function remains ReLU on the scaled dataset.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'The various loss functions and their corresponding accuracies on the test dataset
    when run for 10 epochs are as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '| **Optimizer** | **Test dataset accuracy** |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '| SGD | 88% |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| RMSprop | 98.44% |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: '| Adam | 98.4% |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: Now we have seen that RMSprop and Adam optimizers perform better than the stochastic
    gradient descent optimizer; let's look at the other parameter within an optimizer
    that can be modified to improve the accuracy of the model—learning rate.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning rate of an optimizer can be varied by specifying it as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bce8d1f4-5631-4b31-9079-3de0738f108a.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: In the preceding code snippet, `lr` represents learning rate. The typical values
    of learning rate vary between 0.001 and 0.1.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: On the MNIST dataset, the accuracy did not improve further when we changed the
    learning rate; however, typically for a lower learning rate, more epochs are required
    to reach the same amount of accuracy.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the depth of network
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An increase in the depth of a hidden layer is the same as increasing the number
    of hidden layers in a neural network.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Typically, for a higher number of hidden units in a hidden layer and/or higher
    number of hidden layers, the predictions are more accurate.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that the Adam optimizer or RMSprop has a saturated accuracy after certain
    number of epochs, let''s switch back to stochastic gradient descent to understand
    the accuracy when the model is run for 300 epochs; but we are using more number
    of units in the hidden layer this time:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed5b6e76-c5a8-4d5e-b74a-64472e449166.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: Note that by using 2,000 units in the hidden layer, our accuracy increases to
    95.76% by the end of 300 epochs. This is potentially because the input can now
    be expressed in a higher dimensional space, and hence a better representation
    can be learned when compared to the 1,000-dimensional space scenario.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will increase the number of hidden layers to understand the impact
    of building deep neural networks on accuracy:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4230117b-a90e-4e2c-b2c8-ba6c95dc5557.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: Note that when the network is deep, with two hidden layers instead of one, the
    accuracy after 300 epochs is 97.24%, which is a clear improvement when compared
    to the single hidden layer network.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Similar, to the way in which the network learned more complex representations
    of data when the number of hidden units in a layer increased, the network also
    learned complex representations of data when the number of hidden layers increased.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Impact on change in batch size
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed earlier, the lesser the batch size, the more often the weights
    get updated in a given neural network. This results in a lesser number of epochs
    required to achieve a certain accuracy on the network. At the same time, if the
    batch size is too low, the network structure might result in instability in the
    model.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compare the previously built network with a lower batch size in one
    scenario and a bigger batch size in the next scenario:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3e45aaa-8509-4236-a5dc-119658fddd62.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: Note that in the preceding scenario, where the batch size is very high, the
    test dataset accuracy at the end of 300 epochs is only 89.91%.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is that the network with batch size 1,024 would have learned
    the weights much faster than the network with batch size 30,000, as the number
    of weight updates is much higher when the batch size is lower.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next scenario, we will reduce the batch size to a very small number
    to see the impact on network accuracy:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1f1e527-2398-405b-8b81-f4fb18872520.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: Note that while accuracy improves considerably very quickly to 97.77% within
    10 epochs itself, it takes significant time to produce results, as the number
    of weight updates is high per epoch. This results in more calculations and thus
    more time to execute.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Implementing neural networks in TensorFlow
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In previous sections, we have understood how a neural network works and also
    how to build a neural network model in Keras. In this section, we will be working
    toward building a neural network model in TensorFlow. There are two ways in which
    we can build models in TensorFlow:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Using premade estimators
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining custom estimators
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using premade estimators
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Premade estimators are similar to the methods available in packages such as
    scikit-learn, where the input features and output labels are specified, along
    with the various hyperparameters. A method can then optimize for solving a loss
    function that is predefined to a default value but can be varied by passing a
    different function in a parameter.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore building the training and test datasets in the code:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b7a409f4-1dd3-4f2f-9290-0dad3ff2b104.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: 'Import the dataset. We will work on the `MNIST` dataset for this exercise:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b940c4c3-e1c2-445f-8467-90e4b6790406.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: 'The shapes of images and labels are as follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/809adaf0-0328-4a47-a4cf-3114e34e6635.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: 'The premade function works on label value instead of the one-hot encoded version.
    Let''s convert the one-hot encoded label into a value, as follows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5baad61b-cf52-426e-a337-d9630167f0d8.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
- en: 'Let''s understand how the data points look:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99112603-b35c-4f9c-9b17-bdb3a1749bbc.png)![](img/6a89b41a-b076-4a78-9155-e777a6988c16.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: 'Input the dataset into a function that consumes the independent (`x`) and dependent
    (`y`) variables:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3f55e04a-fbe8-4980-b108-93febe1843be.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: Note that we named the independent variables as `x2` and dependent variable
    as `y`.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that we have passed the arrays that form the independent and dependent
    variable values.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size` indicates the number of training examples that are consumed to
    calculate the loss function, and `num_epochs = None` indicates that the number
    of epochs to be run will be provided later.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '`train_input_fn` returns features and labels, as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bad45146-8465-4949-aab3-ef433ae3013a.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we pass the test dataset:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0dcc8b8-d5b3-4b53-ae3a-4854fb1982f8.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
- en: Note that, in the case of the test dataset, `num_epochs = 1` as we pass it through
    only the feedforward for the test dataset once the model weights are derived from
    training.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'A dataset could potentially contain multiple columns, so let''s specify the
    feature column and its type, as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f6f4e63-208b-43ff-a80b-8fe617bd421b.png)![](img/62dca6f6-d5ac-4671-ab7d-c764a436580d.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
- en: 'If there are multiple columns, we would specify all the columns in a list,
    as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_columns = [feature_x1, feature_x2]`'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Where `feature_x1` is one feature and `feature_x2` is another feature.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we shall specify the number of hidden layers and also the hidden units
    in each layer:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d9de532-1d77-4152-9fff-da1e0a314d25.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
- en: Note that by specifying the number of hidden units in the preceding way, we
    have specified that there are three hidden layers, where the first hidden layer
    has **512** units, the second hidden layer has **256** units, and the final hidden
    layer has **128** units.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have specified the features and hidden layers, let''s specify the
    architecture of neural network, as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab29fc30-66ea-45c6-b222-70d96bd8e8c4.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have specified the model architecture, we can go ahead and train
    the model. If you would like to further change the hyperparameters that are available
    in the function, you can check out the hyperparameter levers that are available
    by using the `help` function, as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49440387-51ad-428c-a503-4944dba2af7e.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: 'The following code runs the neural network model for 2,000 epochs in total:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/742bca8a-62a9-4168-acbf-1320b8a2860d.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: 'Now that our model is run, let''s evaluate the accuracy on the test dataset,
    as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dab4748b-8130-4ece-93a3-805dbb7ed300.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: We can see that the accuracy of the model on the test dataset is 97.2%.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have been implementing a model using premade estimators; in the next
    sections, we will look into defining the model without premade estimators.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Creating custom estimators
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A premade estimator limits the full potential to which TensorFlow can be used;
    for example, we would not be able to have different dropout values after different
    layers. In this regard, let''s go ahead and create a function of our own, as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/138f6adc-4399-44c0-9025-4b9af8a89d6e.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
- en: 'Let''s explore each part of the preceding snippet of code in detail:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a300918-5bfc-4b29-947b-b3e4d3fa44ac.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
- en: The function takes features (independent variables) and labels (dependent variable)
    as input. `mode` indicates whether we want to train, predict, or evaluate the
    given data.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '`params` provides us with the functionality to supply information about parameters;
    for example, learning rate:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d56831e5-22b5-45f0-94f3-0a6092653fe8.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
- en: 'The preceding snippet of code is similar to the way in which we defined model
    architecture in Keras, where we specified the inputs, the hidden layer activation,
    and the number of units in the hidden layer:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6829427b-481e-4d0f-bf9e-d79ee3d174eb.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
- en: 'If our mode is to predict the class, we would not have to train the model,
    but just pass the predicted class, thus estimator spec in such scenario would
    just need to calculate the `y_pred_cls` values, thus the following code:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66dcb13b-ddfe-43af-ba70-6b1870d8331d.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
- en: 'If the mode is to train or test the model, we would have to calculate the loss
    and hence the following code:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97320ae6-c050-4b42-b029-5c70ea927109.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
- en: In the preceding code, the first line is used to define the cross-entropy calculation.
    The second line takes the average of cross entropy across all the rows.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '`optimizer` specifies the optimizer we are interested in and the learning rate.
    `train_op` specifies that we are interested in minimizing loss, and the `global_step`
    parameter keeps a count of the step (epoch) that the model is currently in. `metrics`
    specifies the metrics that we are interested in calculating, and the final `spec`
    that would be calculated would be a combination of all the preceding parameters
    that we have defined.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model architecture and the estimator spec that needs to be returned
    are defined, we define the parameters and mode as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e13a8c9-38ce-44c2-b097-44a682bf8753.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding code, the function learns the parameter that needs to be
    changed and also the model architecture that needs to be worked on (`model_fn`):'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f622206b-09cd-4071-8c50-26772df523dc.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
- en: We run the model by specifying the mode (in this case, `train`) by a certain
    number of epochs (`2000` in this case).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the model, we evaluate the model''s accuracy on the test dataset,
    as follows:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3d42e5a-24e4-4ba3-b678-6c10cb6c08f4.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
- en: Summary
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned to set up Datalab to execute neural networks on
    Google Cloud. We also learned the structure of a neural network and how various
    parameters, such as depth, number of hidden units, activation function, optimizer,
    batch size, and number of epochs, impact the accuracy of the model. We also saw
    how to implement a neural network in both Keras and TensorFlow. Topics such as
    using premade estimators and creating custom estimators in TensorFlow were covered.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
