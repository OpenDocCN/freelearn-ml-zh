<html><head></head><body>
		<div id="_idContainer138">
			<h1 id="_idParaDest-34"><a id="_idTextAnchor035"/>Chapter 2: Graph Machine Learning</h1>
			<p><strong class="bold">Machine learning</strong> is a<a id="_idIndexMarker180"/> subset of artificial intelligence that aims to provide systems with the ability to <em class="italic">learn</em> and improve from data. It has achieved impressive results in many different applications, especially where it is difficult or unfeasible to explicitly define rules to solve a specific task. For instance, we can train algorithms to recognize spam emails, translate sentences into other languages, recognize objects in an image, and so on.</p>
			<p>In recent years, there has been an increasing interest in applying machine learning to <em class="italic">graph-structured data</em>. Here, the primary objective is to automatically learn suitable representations to make predictions, discover new patterns, and understand complex dynamics in a better manner with respect to "traditional" machine learning approaches.</p>
			<p>This chapter will first review some of the basic machine learning concepts. Then, an introduction to graph machine learning will be provided, with a particular focus on <strong class="bold">representation learning</strong>. We will then analyze a practical example to guide you through the comprehension of the theoretical concepts.<a id="_idTextAnchor036"/></p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>A refresher on machine learning</li>
				<li>What is machine learning on graphs and why is it important?</li>
				<li>A general taxonomy to navigate among graph machine learning algorithms</li>
			</ul>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor037"/>Technical requirements</h1>
			<p>We will be using Jupyter notebooks with <em class="italic">Python 3.8</em> for all of our exercises. The following is a list of the Python libraries that need to be installed for this chapter using <strong class="source-inline">pip</strong>. For example, run <strong class="source-inline">pip install networkx==2.5</strong> on the command line, and so on:</p>
			<p class="source-code">Jupyter==1.0.0</p>
			<p class="source-code">networkx==2.5</p>
			<p class="source-code">matplotlib==3.2.2</p>
			<p class="source-code">node2vec==0.3.3</p>
			<p class="source-code">karateclub==1.0.19</p>
			<p class="source-code">scipy==1.6.2</p>
			<p>All the code files relevant to this chapter are available at <a href="https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter02">https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter02</a>.</p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor038"/>Understanding machine learning on graphs</h1>
			<p>Of the branches of <a id="_idIndexMarker181"/>artificial intelligence, <strong class="bold">machine learning</strong> is one that has attracted the most attention in recent years. It refers to a class of computer algorithms that automatically learn and improve their skills through experience <em class="italic">without being explicitly programmed</em>. Such an approach takes inspiration from nature. Imagine an athlete who faces a novel movement for the first time: they start slowly, carefully imitating the gesture of a coach, trying, making mistakes, and trying again. Eventually, they will improve, becoming more and more confident.</p>
			<p>Now, how does this concept translate to machines? It is essentially an optimization problem. The goal is to find a mathematical model that is able to achieve the best possible performance on a particular task. Performance can be measured using a specific performance metric (also known<a id="_idIndexMarker182"/> as a <strong class="bold">loss function</strong> or <strong class="bold">cost function</strong>). In a common <a id="_idIndexMarker183"/>learning task, the algorithm is provided with data, possibly lots of it. The algorithm uses this data to iteratively make decisions or predictions for the specific task. At each iteration, decisions are evaluated using the loss function. The resulting <em class="italic">error</em> is used to update the model parameters in a way that, hopefully, means the model <a id="_idIndexMarker184"/>will perform better. This process is commonly called <strong class="bold">training</strong>. </p>
			<p>More formally, let's consider a particular task, <em class="italic">T</em>, and a performance metric, <em class="italic">P</em>, which allows us to quantify how good an algorithm is performing on <em class="italic">T</em>. According to Mitchell (Mitchell et al., 1997), an algorithm is said to learn from experience, <em class="italic">E</em>, if its performance at task <em class="italic">T</em>, measured by <em class="italic">P</em>, improves with experience <em class="italic">E</em>.</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor039"/>Basic principles of machine learning</h2>
			<p>Machine learning algorithms<a id="_idIndexMarker185"/> fall into three main categories, known as <em class="italic">supervised</em>, <em class="italic">unsupervised</em>, and <em class="italic">semi-supervised</em> learning. These learning paradigms depend on the way data is provided to the algorithm and how performance is evaluated.</p>
			<p><strong class="bold">Supervised learning</strong> is the <a id="_idIndexMarker186"/>learning paradigm used when we know the answer to the problem. In this scenario, the dataset is composed of samples of pairs of the form <em class="italic">&lt;x,y&gt;</em>, where <em class="italic">x</em> is the input (for example, an image or a voice signal) and <em class="italic">y</em> is the corresponding desired output (for example, what the image represents or what the voice is saying). The input variables are also known as <em class="italic">features</em>, while the output is usually referred to as <em class="italic">labels</em>, <em class="italic">targets</em>, and <em class="italic">annotations</em>. In supervised settings, performance is often evaluated using a <em class="italic">distance function</em>. This function measures the differences between the prediction and the expected output. According to the type of labels, supervised learning can be further divided into the following:</p>
			<ul>
				<li><strong class="bold">Classification</strong>: Here, the labels<a id="_idIndexMarker187"/> are discrete and refer to the "class" the input belongs to. Examples of classification are determining the<a id="_idIndexMarker188"/> object in a photo or predicting whether an email is spam or not.</li>
				<li><strong class="bold">Regression</strong>: The target is<a id="_idIndexMarker189"/> continuous. Examples of regression problems are predicting the <a id="_idIndexMarker190"/>temperature in a building or predicting the selling price of any particular product.</li>
			</ul>
			<p><strong class="bold">Unsupervised learning</strong> differs <a id="_idIndexMarker191"/>from supervised learning since the answer to the problem is not known. In this context, we do not have any labels and only the inputs, <em class="italic">&lt;x&gt;</em>, are provided. The goal is thus deducing structures and patterns, attempting to find similarities. </p>
			<p>Discovering groups of similar examples (clustering) is one of these problems, as well as giving new representations of the data in a high-dimensional space. </p>
			<p>In <strong class="bold">semi-supervised learning</strong>, the<a id="_idIndexMarker192"/> algorithm is trained using a combination of labeled and unlabeled data. Usually, to direct the research of structures present in the unlabeled input data, a limited amount of labeled data is used.</p>
			<p>It is also worth mentioning that <strong class="bold">reinforcement learning</strong> is used for training machine learning models to make a <a id="_idIndexMarker193"/>sequence of decisions. The artificial intelligence algorithm faces aÂ game-like situation, getting <em class="italic">penalties</em> or <em class="italic">rewards</em> based on the actions performed. The role of the algorithm is to understand how to act in order to maximize rewards and minimize penalties.</p>
			<p>Minimizing the error on <a id="_idIndexMarker194"/>the training data is not enough. The keyword in machine learning is <em class="italic">learning</em>. It means that algorithms must be able to achieve the same level of performance even on unseen data. The most common way of evaluating the generalization capabilities of machine learning algorithms is to divide the dataset into two parts: the <strong class="bold">training set</strong> and the <strong class="bold">test set</strong>. The <a id="_idIndexMarker195"/>model is trained on the training set, where the loss function<a id="_idIndexMarker196"/> is computed and used to update the parameters. After training, the model's performance is evaluated on the test set. Moreover, when more data is available, the test set can be further divided<a id="_idIndexMarker197"/> into <strong class="bold">validation</strong> and <strong class="bold">test</strong> sets. The validation set is commonly used for assessing the model's performance during training.</p>
			<p>When training a machine learning algorithm, three situations can be observed:</p>
			<ul>
				<li>In the first situation, the model reaches a low level of performance over the training set. This situation is commonly<a id="_idIndexMarker198"/> known as <strong class="bold">underfitting</strong>, meaning that the model is not powerful enough to address the task. </li>
				<li>In the second situation, the model achieves a high level of performance over the training set but struggles at generalizing over testing data. This situation is known as <strong class="bold">overfitting</strong>. In this<a id="_idIndexMarker199"/> case, the model is simply memorizing the training data, without actually understanding the true relations among them. </li>
				<li>Finally, the ideal situation is when the model is able to achieve (possibly) the highest level of performance over both training and testing data. </li>
			</ul>
			<p>An example of overfitting and underfitting<a id="_idIndexMarker200"/> is given by the risk curve shown in <em class="italic">Figure 2.1</em>. From the figure, it is possible to see how the performances on the training and test sets change according to the complexity of the model (the number of parameters to be fitted):</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B16069_02_01.jpg" alt="Figure 2.1 â Risk curve describing the prediction error on training and test set error in the function of the model complexity (number of parameters of the model)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 â Risk curve describing the prediction error on training and test set error in the function of the model complexity (number of parameters of the model)</p>
			<p>Overfitting<a id="_idIndexMarker201"/> is one of the<a id="_idIndexMarker202"/> main problems that affect machine learning practitioners. It can occur due to several reasons. Some of the reasons can be as follows:</p>
			<ul>
				<li>The dataset can be ill-defined or not sufficiently representative of the task. In this case, adding more data could help to mitigate the problem. </li>
				<li>The mathematical model used for addressing the problem is too powerful for the task. In this case, proper constraints can be added to the loss function in order to reduce the model's "power." Such <a id="_idIndexMarker203"/>constraints are called <strong class="bold">regularization</strong> terms.</li>
			</ul>
			<p>Machine learning has achieved impressive results in many fields, becoming one of the most diffused and effective approaches in computer vision, pattern recognition, and natural language processing, among others. </p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor040"/>The benefit of machine learning on graphs</h2>
			<p>Several machine<a id="_idIndexMarker204"/> learning algorithms have been developed, each with its own advantages and limitations. Among those, it is worth mentioning regression algorithms (for example, linear and logistic regression), instance-based algorithms (for example, k-nearest neighbor or support vector machines), decision tree algorithms, Bayesian algorithms (for example, naÃ¯ve Bayes), clustering algorithms (for example, k-means), and artificial neural networks.</p>
			<p>But what is the key to all of this success? </p>
			<p>Essentially, one thing: machine learning can automatically address tasks that are easy for humans to do. These tasks can be too complex to describe using traditional computer algorithms and, in some cases, they have shown even better capabilities than human beings. This is especially true when dealing with graphsâthey can differ in several more ways than an image or audio signal because of their complex structure. By using graph machine learning, we can create algorithms to automatically detect and interpret recurring latent patterns.</p>
			<p>For these reasons, there has been an increasing interest in <em class="italic">learning representations</em> for graph-structured data and many machine learning algorithms have been developed for handling graphs. For example, we might be interested in determining the role of a protein in a biological interaction graph, predicting the evolution of a collaboration network, recommending new products to a user in a social network, and many more (we will discuss these and more applications in <a href="B16069_10_Final_JM_ePub.xhtml#_idTextAnchor150"><em class="italic">Chapter 10</em></a>, <em class="italic">The Future of Graphs</em>). </p>
			<p>Due to their nature, graphs can be analyzed at different levels of granularity: at the node, edge, and graph level (the whole graph), as depicted in <em class="italic">Figure 2.2</em>. For each of those levels, different problems could be faced and, as a consequence, specific algorithms should be used:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B16069_02_02.jpg" alt="Figure 2.2 â Visual representation of the three different levels of granularity in graphs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 â Visual representation of the three different levels of granularity in graphs</p>
			<p>In the following bullet points, we will give some examples of machine learning problems that could be faced for each of those levels:</p>
			<ul>
				<li><strong class="bold">Node level</strong>: Given<a id="_idIndexMarker205"/> a (possibly large) graph, <img src="image/Formula_02_001.png" alt=""/>, the goal is to classify each vertex, <img src="image/Formula_02_002.png" alt=""/>, into the right class. In this setting, the dataset includes <em class="italic">G</em> and a list of pairs, <em class="italic">&lt; vi,yi &gt;</em>, where <em class="italic">vi</em> is a node of graph <em class="italic">G</em> and <em class="italic">y</em><em class="italic">i</em> is the class to which the node belongs.</li>
				<li><strong class="bold">Edge level</strong>: Given<a id="_idIndexMarker206"/> a (possibly large) graph, <img src="image/Formula_02_003.png" alt=""/>, the goal is to classify each edge, <img src="image/Formula_02_004.png" alt=""/>, into the right class. In this setting, the dataset includes <em class="italic">G</em> and a list of pairs, <em class="italic">&lt; ei,yi &gt;</em>, where <em class="italic">ei</em> is an edge of graph <em class="italic">G</em> and <em class="italic">yi</em> is the class to which the edge belongs. Another typical task for this level of<a id="_idIndexMarker207"/> granularity is <strong class="bold">link prediction</strong>, the problem of predicting the existence of a link between two existing nodes in a graph.</li>
				<li><strong class="bold">Graph level</strong>: Given a dataset with <em class="italic">m</em> different graphs, the task is to build a machine learning algorithm capable of classifying a graph<a id="_idIndexMarker208"/> into the right class. We can then see this problem as a classification problem, where the dataset is defined by a list of pairs, <em class="italic">&lt;Gi,yi</em><em class="italic">&gt;</em>, where <em class="italic">Gi</em> is a graph and <em class="italic">yi</em> is the class the graph belongs to.</li>
			</ul>
			<p>In this section, we discussed some basic concepts of machine learning. Moreover, we have enriched our description by introducing some of the common machine learning problems when dealing with graphs. Having those theoretical principles as a basis, we will now introduce some more complex concepts relating to graph machine learning.</p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor041"/>The generalized graph embedding problem</h1>
			<p>In classical <a id="_idIndexMarker209"/>machine learning applications, a common way to process the input data is to build from a set of features, in a process called <strong class="bold">feature engineering</strong>, which is <a id="_idIndexMarker210"/>capable of giving a compact and meaningful representation of each instance present in the dataset. </p>
			<p>The dataset obtained from the feature engineering step will be then used as input for the machine learning algorithm. If this process usually works well for a large range of problems, it may not be the optimal solution when we are dealing with graphs. Indeed, due to their well-defined structure, finding a suitable representation capable of incorporating all the useful information might not be an easy task. </p>
			<p>The first, and most straightforward, way of creating features capable of representing structural information from graphs is the <em class="italic">extraction of certain statistics</em>. For instance, a graph could be represented by its degree distribution, efficiency, and all the metrics we described in the previous chapter. </p>
			<p>A more complex procedure consists of applying specific kernel functions or, in other cases, engineering-specific features that are capable of incorporating the desired properties into the final machine learning model. However, as you can imagine, this process could be really time-consuming and, in certain cases, the features used in the model could represent just a subset of the information that is really needed to get the best performance for the final model. </p>
			<p>In the last decade, a lot of work has been done in order to define new approaches for creating meaningful and compact representations of graphs. The general idea behind all these approaches is to create algorithms capable of <em class="italic">learning</em> a good representation of the original dataset such that geometric relationships in the new space reflect the structure of the original graph. We usually call the process of learning a good representation of <a id="_idIndexMarker211"/>a given graph <strong class="bold">representation learning</strong> or <strong class="bold">network embedding</strong>. We will provide a more formal definition as follows.</p>
			<p><strong class="bold">Representation learning</strong> (<strong class="bold">network embedding</strong>) is the task that aims to learn a mapping <a id="_idIndexMarker212"/>function, <img src="image/Formula_02_005.png" alt=""/>, from a discrete graph to a continuous domain. Function <img src="image/Formula_02_006.png" alt=""/> will be capable of <a id="_idIndexMarker213"/>performing a low-dimensional vector representation such that the properties (local and global) of graph <img src="image/Formula_02_007.png" alt=""/> are preserved.</p>
			<p>Once mapping <img src="image/Formula_02_008.png" alt=""/> is learned, it could be applied to the graph and the resulting mapping could be used as a feature set for a machine learning algorithm. A graphical example of this process is visible in <em class="italic">Figure 2.3</em>:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B16069_02_03.jpg" alt="Figure 2.3 â Example of a workflow for a network embedding algorithm&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 â Example of a workflow for a network embedding algorithm</p>
			<p>Mapping <a id="_idIndexMarker214"/>function <img src="image/Formula_02_009.png" alt=""/> can also be applied in order to learn the vector representation for nodes and edges. As we already mentioned, machine learning problems on graphs could occur at different levels of granularity. As a consequence, different embedding algorithms have been developed in order to learn functions to generate the vectorial representation of nodes (<img src="image/Formula_02_010.png" alt=""/> (also known<a id="_idIndexMarker215"/> as <strong class="bold">node embedding</strong>) or edges (<img src="image/Formula_02_011.png" alt=""/>) (also known as <strong class="bold">edge embedding</strong>). Those mapping functions try to build a vector space such<a id="_idIndexMarker216"/> that the geometric relationships in the new space reflect the structure of the original graph, node, or edges. As a result, we will see that graphs, nodes, or <a id="_idIndexMarker217"/>edges that are similar in the original space will also be similar in the new space.</p>
			<p>In other words, in the space generated by the embedding function, similar structures will have <em class="italic">a small Euclidean distance</em>, while dissimilar structures will have <em class="italic">a large Euclidean distance</em>. It is important to highlight that while most embedding algorithms generate a mapping in Euclidean vector spaces, there has recently been an interest in non-Euclidean mapping functions.</p>
			<p>Let's now see a <a id="_idIndexMarker218"/>practical example of what an embedding space<a id="_idIndexMarker219"/> looks like, and how similarity can be seen in the new space. In the following code block, we show an example using a particular embedding algorithm known as <strong class="bold">Node to Vector</strong> (<strong class="bold">Node2Vec</strong>). We will describe<a id="_idIndexMarker220"/> how it works in the next chapter. At the moment, we will just say that the algorithm will map each node of graph <em class="italic">G</em> in a vector:</p>
			<p class="source-code">import networkx as nx</p>
			<p class="source-code">from node2vec import Node2Vec</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">G = nx.barbell_graph(m1=7, m2=4)</p>
			<p class="source-code">node2vec = Node2Vec(G, dimensions=2)</p>
			<p class="source-code">model = node2vec.fit(window=10)</p>
			<p class="source-code">fig, ax = plt.subplots()</p>
			<p class="source-code">for x in G.nodes():</p>
			<p class="source-code">Â Â Â Â v = model.wv.get_vector(str(x))</p>
			<p class="source-code">Â Â Â Â ax.scatter(v[0],v[1], s=1000)</p>
			<p class="source-code">Â Â Â Â ax.annotate(str(x), (v[0],v[1]), fontsize=12)</p>
			<p>In the preceding code, we have done the following:</p>
			<ol>
				<li>We generated a barbell graph (described in the previous chapter).</li>
				<li>The Node2Vec embedding algorithm is then used in order to map each node of the graph in a vector of two dimensions. </li>
				<li>Finally, the two-dimensional vectors generated by the embedding algorithm, representing the nodes of the original graph, are plotted. </li>
			</ol>
			<p>The result is shown in <em class="italic">Figure 2.4</em>:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B16069_02_04.jpg" alt="Figure 2.4 â Application of the Node2Vec algorithm to a graph (left) to generate the embedding vector of its nodes (right)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 â Application of the Node2Vec algorithm to a graph (left) to generate the embedding vector of its nodes (right)</p>
			<p>From <em class="italic">Figure 2.4</em>, it is<a id="_idIndexMarker221"/> easy to see that nodes that have a similar structure are close to each other and are distant from nodes that have dissimilar structures. It is also interesting to observe how good Node2Vec is at discriminating group 1 from group 3. Since the algorithm uses neighboring information of each node to generate the representation, the clear discrimination of those two groups is possible. </p>
			<p>Another example on the same graph can be performed using the <strong class="bold">Edge to Vector</strong> (<strong class="bold">Edge2Vec</strong>) algorithm in <a id="_idIndexMarker222"/>order to generate a mapping for the edges for the same graph, <em class="italic">G</em>:</p>
			<p class="source-code">from node2vec.edges import HadamardEmbedder</p>
			<p class="source-code">edges_embs = HadamardEmbedder(keyed_vectors=model.wv)</p>
			<p class="source-code">fig, ax = plt.subplots()</p>
			<p class="source-code">for x in G.edges():</p>
			<p class="source-code">Â Â Â Â v = edges_embs[(str(x[0]), str(x[1]))]</p>
			<p class="source-code">Â Â Â Â ax.scatter(v[0],v[1], s=1000)</p>
			<p class="source-code">Â Â Â Â ax.annotate(str(x), (v[0],v[1]), fontsize=12)</p>
			<p>In the preceding code, we have done the following:</p>
			<ol>
				<li value="1">We generated a barbell graph (described in the previous chapter).</li>
				<li>The <strong class="source-inline">HadamardEmbedder</strong> embedding algorithm is applied to the result of the Node2Vec algorithm (<strong class="source-inline">keyed_vectors=model.wv</strong>) used in order to map each edge of the graph in a vector of two dimensions. </li>
				<li>Finally, the <a id="_idIndexMarker223"/>two-dimensional vectors generated by the embedding algorithm, representing the nodes of the original graph, are plotted. </li>
			</ol>
			<p>The results are shown in <em class="italic">Figure 2.5</em>:</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B16069_02_05.jpg" alt="Figure 2.5 â Application of the Hadamard algorithm to a graph (left) to generate the embedding vector of its edges (right)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 â Application of the Hadamard algorithm to a graph (left) to generate the embedding vector of its edges (right)</p>
			<p>As for node embedding, in <em class="italic">Figure 2.5</em>, we reported the results of the edge embedding algorithm. From the figure, it is easy to see that the edge embedding algorithm clearly identifies similar edges. As expected, edges belonging to groups 1, 2, and 3 are clustered in well-defined and well-grouped regions. Moreover, the (6,7) and (10,11) edges, belonging to groups 4 and 5, respectively, are well clustered in specific groups.</p>
			<p>Finally, we will provide an example of <a id="_idIndexMarker224"/>a <strong class="bold">Graph to Vector</strong> (<strong class="bold">Grap2Vec</strong>) embedding algorithm. This algorithm maps a single graph in a vector. As for another example, we will discuss this algorithm in more detail in the next chapter. In the following code block, we provide a Python example showing how to use the Graph2Vec algorithm <a id="_idIndexMarker225"/>in order to generate the embedding representation on a set of graphs:</p>
			<p class="source-code">import random</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">from karateclub import Graph2Vec</p>
			<p class="source-code">n_graphs = 20</p>
			<p class="source-code">def generate_random():</p>
			<p class="source-code">Â Â Â Â n = random.randint(5, 20)</p>
			<p class="source-code">Â Â Â Â k = random.randint(5, n)</p>
			<p class="source-code">Â Â Â Â p = random.uniform(0, 1)</p>
			<p class="source-code">Â Â Â Â return nx.watts_strogatz_graph(n,k,p)</p>
			<p class="source-code">Gs = [generate_random() for x in range(n_graphs)]</p>
			<p class="source-code">model = Graph2Vec(dimensions=2)</p>
			<p class="source-code">model.fit(Gs)</p>
			<p class="source-code">embeddings = model.get_embedding()</p>
			<p class="source-code">fig, ax = plt.subplots(figsize=(10,10))</p>
			<p class="source-code">for i,vec in enumerate(embeddings):</p>
			<p class="source-code">Â Â Â Â ax.scatter(vec[0],vec[1], s=1000)</p>
			<p class="source-code">Â Â Â Â ax.annotate(str(i), (vec[0],vec[1]), fontsize=16)</p>
			<p>In this example, the following has been done:</p>
			<ol>
				<li value="1">20 Watts-Strogatz graphs (described in the previous chapter) have been generated with random parameters. </li>
				<li>We have then executed the graph embedding algorithm in order to generate a two-dimensional vector representation of each graph. </li>
				<li>Finally, the generated vectors are plotted in their Euclidean space. </li>
			</ol>
			<p>The results of this <a id="_idIndexMarker226"/>example are shown in <em class="italic">Figure 2.6</em>:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B16069_02_06.jpg" alt="Figure 2.6 â Plot of two embedding vectors generated by the Graph2Vec algorithm applied to 20 randomly generated Watts-Strogatz graphs (left). Extraction of two graphs with a large Euclidean distance (Graph 12 and Graph 8 on the top right) and two graphs with a low Euclidean distance (Graph 14 and Graph 4 on the bottom right) is shown"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 â Plot of two embedding vectors generated by the Graph2Vec algorithm applied to 20 randomly generated Watts-Strogatz graphs (left). Extraction of two graphs with a large Euclidean distance (Graph 12 and Graph 8 at the top right) and two graphs with a low Euclidean distance (Graph 14 and Graph 4 at the bottom right) is shown</p>
			<p>As we can see from <em class="italic">Figure 2.6</em>, graphs with a large Euclidean distance, such as graph 12 and graph 8, have a different structure. The former is generated with the <strong class="source-inline">nx.watts_strogatz_graph(20,20,0.2857)</strong> parameter and the latter with the <strong class="source-inline">nx.watts_strogatz_graph(13,6,0.8621)</strong> parameter. In contrast, a graph with a low Euclidean distance, such as graph 14 and graph 8, has a similar structure. Graph 14 is generated with the <strong class="source-inline">nx.watts_strogatz_graph(9,9,0.5091)</strong> command, while graph 4 is generated with <strong class="source-inline">nx.watts_strogatz_graph(10,5,0.5659)</strong>.</p>
			<p>In the scientific <a id="_idIndexMarker227"/>literature, a plethora of embedding methods has been developed. We will describe in detail and use some of them in the next section of this book. These methods are usually classified into two main types: <em class="italic">transductive</em> and <em class="italic">inductive</em>, depending on the update procedure of the function when new samples are added. If new nodes are provided, transductive methods update the model (for example, re-train) to infer information about the nodes, while in inductive methods, models are expected to generalize to new nodes, edges, or graphs that were not observed during training.</p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor042"/>The taxonomy of graph embedding machine learning algorithms</h1>
			<p>A wide variety of<a id="_idIndexMarker228"/> methods to generate a compact space for graph representation have been developed. In recent years, a trend has been observed of researchers and machine learning practitioners converging toward a unified notation to provide a common definition to describe such algorithms. In this section, we will be introduced to a simplified version of the taxonomy defined in the paper <em class="italic">Machine Learning on Graphs: A Model and Comprehensive Taxonomy</em> (<a href="https://arxiv.org/abs/2005.03675">https://arxiv.org/abs/2005.03675</a>).</p>
			<p>In this formal representation, every graph, node, or edge embedding method can be described by two fundamental components, named the encoder and the decoder. The <strong class="bold">encoder</strong> (<strong class="bold">ENC</strong>) maps the<a id="_idIndexMarker229"/> input into the embedding space, while<a id="_idIndexMarker230"/> the <strong class="bold">decoder</strong> (<strong class="bold">DEC</strong>) decodes structural information about the graph from the learned embedding (<em class="italic">Figure 2.7</em>). </p>
			<p>The framework described in the paper follows an intuitive idea: if we are able to encode a graph such that the decoder is able to retrieve all the necessary information, then the embedding must contain a compressed version of all this information and can be used to downstream machine learning tasks:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B16069_02_07.jpg" alt="Figure 2.7 â Generalized encoder (ENC) and decoder (DEC) architecture for embedding algorithms&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 â Generalized encoder (ENC) and decoder (DEC) architecture for embedding algorithms</p>
			<p>In many <a id="_idIndexMarker231"/>graph-based machine learning algorithms for representation learning, the decoder is usually designed to map pairs of node embeddings to a real value, usually representing the proximity (distance) of the nodes in the original graphs. For example, it is possible to implement the decoder such that, given the embedding representation of two nodes, <img src="image/Formula_02_012.png" alt=""/> and <img src="image/Formula_02_013.png" alt=""/>, <img src="image/Formula_02_014.png" alt=""/> if in the input graph an edge connecting the two nodes, <img src="image/Formula_02_015.png" alt=""/>, exists. In practice, more effective <em class="italic">proximity functions</em> can be used to measure the similarity between nodes.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor043"/>The categorization of embedding algorithms</h2>
			<p>Inspired by the general framework depicted in <em class="italic">Figure 2.7</em>, we will now provide a categorization of the various<a id="_idIndexMarker232"/> embedding algorithms into four main groups. Moreover, in order to help you to better understand this categorization, we shall provide simple code snapshots in pseudo-code. In our pseudo-code formalism, we denote <strong class="source-inline">G</strong> as a generic <strong class="source-inline">networkx</strong> graph, with <strong class="source-inline">graphs_list</strong> as a list of <strong class="source-inline">networkx</strong> graphs and <strong class="source-inline">model</strong> as a generic embedding algorithm:</p>
			<ul>
				<li><strong class="bold">Shallow embedding methods</strong>: These <a id="_idIndexMarker233"/>methods are able to learn and return only the embedding values for the learned input data. <em class="italic">Node2Vec</em>, <em class="italic">Edge2Vec</em>, and <em class="italic">Graph2Vec</em>, which we previously discussed, are <a id="_idIndexMarker234"/>examples of shallow embedding methods. Indeed, they can only return a vectorial representation of the data they learned during the <em class="italic">fit</em> procedure. It is not possible to obtain the embedding vector for unseen data. A typical way to use these methods is as follows:<p class="source-code">model.fit(graphs_list)</p><p class="source-code">embedding = model.get_embedding()[i]</p><p>In the code, a generic shallow embedding method is trained on a list of graphs (line 1). Once the model is fitted, we can only get the embedding vector of the <em class="italic">i</em>th graph belonging to <strong class="source-inline">graphs_list</strong> (line 2). Unsupervised and supervised shallow embedding methods will be described, respectively, in <a href="B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 3</em></a>, <em class="italic">Unsupervised Graph Learning</em>, and <a href="B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064"><em class="italic">Chapter 4</em></a>, <em class="italic">Supervised Graph Learning</em>.</p></li>
				<li><strong class="bold">Graph autoencoding methods</strong>: These <a id="_idIndexMarker235"/>methods do not <a id="_idIndexMarker236"/>simply learn how to map the input graphs in vectors; they learn a more general mapping function, <img src="image/Formula_02_016.png" alt=""/>, capable of also generating the embedding vector for unseen instances. A typical way to use them is as follows:<p class="source-code">model.fit(graphs_list)</p><p class="source-code">embedding = model.get_embedding(G)</p><p>The model is trained on <strong class="source-inline">graphs_list</strong> (line 1). Once the model is fitted on the input training set, it is <a id="_idIndexMarker237"/>possible to use it to generate the embedding <a id="_idIndexMarker238"/>vector of a new unseen graph, <strong class="source-inline">G</strong>. Graph autoencoding methods will be described in <a href="B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 3</em></a>, <em class="italic">Unsupervised Graph Learning</em>.</p></li>
				<li><strong class="bold">Neighborhood aggregation methods</strong>: These algorithms can be used to extract embeddings at the <a id="_idIndexMarker239"/>graph level, where nodes are labeled <a id="_idIndexMarker240"/>with some properties. Moreover, as for the graph autoencoding methods, the algorithms belonging to this class are able to learn a general mapping function, <img src="image/Formula_02_017.png" alt=""/>, also capable of generating the embedding vector for unseen instances.<p>A nice property of those algorithms is the possibility to build an embedding space where not only the internal structure of the graph is taken into account but also some external information, defined as properties of its nodes. For instance, with this method, we can have an embedding space capable of identifying, at the same time, graphs with similar structures and different properties on nodes. Unsupervised and supervised neighborhood aggregation methods will be described in <a href="B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 3</em></a>, <em class="italic">Unsupervised Graph Learning</em>, and <a href="B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064"><em class="italic">Chapter 4</em></a>, <em class="italic">Supervised Graph Learning</em>, respectively.</p></li>
				<li><strong class="bold">Graph regularization methods</strong>: Methods based on graph regularization are slightly different from the <a id="_idIndexMarker241"/>ones listed in the preceding points. Here, we do not have a graph as input. Instead, the objective is to learn from a set <a id="_idIndexMarker242"/>of features by exploiting their "interaction" to regularize the process. In more detail, a graph can be constructed from the features by considering feature similarities. The main idea is based on the assumption that nearby nodes in a graph are likely to have the same labels. Therefore, the loss function is designed to constrain the labels to be consistent with the graph structure. For example, regularization might constrain neighboring nodes to share similar embeddings, in terms of their distance in the L2 norm. For this reason, the encoder only <a id="_idIndexMarker243"/>uses <em class="italic">X</em> node features as input.<p>The algorithms<a id="_idIndexMarker244"/> belonging to this family learn a function, <img src="image/Formula_02_018.png" alt=""/>, that maps a specific set of features (<img src="image/Formula_02_019.png" alt=""/>) to an embedding vector. As for the graph autoencoding and neighborhood aggregation methods, this algorithm is also able to apply the learned function to new, unseen features. Graph regularization methods will be described in <a href="B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064"><em class="italic">Chapter 4</em></a>, <em class="italic">Supervised Graph Learning</em>.</p></li>
			</ul>
			<p>For algorithms belonging to the group of shallow embedding methods and neighborhood aggregation methods, it is possible to define an <em class="italic">unsupervised</em> and <em class="italic">supervised</em> version. The ones belonging to graph autoencoding methods are suitable in unsupervised tasks, while the algorithms belonging to graph regularization methods are used in semi-supervised/supervised settings. </p>
			<p>For unsupervised algorithms, the embedding of a specific dataset is performed only using the information contained in the input dataset, such as nodes, edges, or graphs. For the supervised setting, external information is used to guide the embedding process. That information is usually classed as a label, such as a pair, <em class="italic">&lt;Gi,yi&gt;</em>, that assigns to each graph a specific class. This process is more complex than the unsupervised one since the model tries to find the best vectorial representation in order to find the best assignment of a label to an instance. In order to clarify this concept, we can think, for instance, of the <em class="italic">convolutional neural networks</em> for image classification. During their training process, neural networks try to classify each image into the right class by performing the fitting of various convolutional filters at the same time. The goal of those convolutional filters is to find a compact representation of the input data in order to maximize the prediction performances. The same concept is also valid for supervised graph embedding, where the algorithm tries to find the best graph representation in order to maximize the performance of a class assignment task.</p>
			<p>From a more <a id="_idIndexMarker245"/>mathematical perspective, all these <a id="_idIndexMarker246"/>models are trained with a proper loss function. This function can be generalized using two terms: </p>
			<ul>
				<li>The first is used in supervised settings to minimize the difference between the prediction and the target.</li>
				<li>The second is used to evaluate the similarity between the input graph and the one reconstructed after the ENC + DEC steps (which is the structure reconstruction error).</li>
			</ul>
			<p> Formally, it can be defined as follows:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/Formula_02_020.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_02_021.png" alt=""/> is the loss function in the supervised settings. The model is optimized to minimize, for each instance, the error between the right (<img src="image/Formula_02_022.png" alt=""/>) and the predicted class (<img src="image/Formula_02_023.png" alt=""/>). <img src="image/Formula_02_024.png" alt=""/> is the loss function representing the reconstruction error between the input graph (<img src="image/Formula_02_025.png" alt=""/>) and the one obtained after the ENC + DEC process (<img src="image/Formula_02_026.png" alt=""/>). For unsupervised settings, we have the same loss but <img src="image/Formula_02_027.png" alt=""/>, since we do not have a target variable to use.</p>
			<p>It is important to <a id="_idIndexMarker247"/>highlight the main role that these<a id="_idIndexMarker248"/> algorithms play when we try to solve a machine learning problem on a graph. They can be used <em class="italic">passively</em> in order to transform a graph into a feature vector suitable for a classical machine learning algorithm or for data visualization tasks. But they can also be used <em class="italic">actively</em> during the learning process, where the machine learning algorithm finds a compact and meaningful solution to a specific problem.</p>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor044"/>SummaryÂ </h1>
			<p>In this chapter, we refreshed some basic <em class="italic">machine learning</em> concepts and discovered how they can be applied to graphs. We defined basic <em class="italic">graph machine learning</em> terminology with a particular focus on <em class="italic">graph representation learning</em>. A taxonomy of the main graph machine learning algorithms was presented in order to clarify what differentiates the various ranges of solutions developed over the years. Finally, practical examples were provided to begin understanding how the theory can be applied to practical problems.</p>
			<p>In the next chapter, we will revise the main graph-based machine learning algorithms. We will analyze their behavior and see how they can be used in practice.</p>
		</div>
	</body></html>