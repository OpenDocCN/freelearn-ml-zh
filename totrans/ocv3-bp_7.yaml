- en: Chapter 6. Efficient Person Identification Using Biometric Properties
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The rise of digital media is greater than ever. People are placing more and
    more of their personal information on digital carriers like laptops, smartphones,
    and tablets. However, many of these systems do not provide efficient identification
    and authentication systems to ensure that strangers cannot access your personal
    data. This is where biometrics-based identification systems come into play and
    try to make your data more secure and less vulnerable to malicious people.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: These identification systems can be used to lock down your computer, avoid people
    getting into a secure room, and so on, but, with technology improving each day,
    we are only one step away from further digitalizing our personal lives. How about
    using your facial expressions to unlock your door? How about opening your car
    with your fingerprint? The possibilities are endless.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Many techniques and algorithms are already available in open source computer
    vision and machine learning packages like OpenCV to efficiently use these personal
    identification properties. Of course, this opens up the possibility for enthusiastic
    computer vision programmers to create many different applications based on these
    techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on techniques that use individual biometrics
    in order to create personal authentication systems that outperform standard available
    login systems based on passwords. We will take a deeper look at iris and fingerprint
    recognition, face detection, and face recognition.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: We will first discuss the main principles behind each biometric technique, and
    then we'll show an implementation based on the OpenCV 3 library. For some of the
    biometrics, we will make use of the available open source frameworks out there.
    All datasets used to demonstrate the techniques are available for free online
    for research purposes. However, if you want to apply them to a commercial application,
    be sure to check their licenses!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will illustrate how you can combine several biometric classifications
    to increase the chance of successfully identifying a specific person based on
    the probability of the individual biometrics.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, you will be able to create a fully functional identification
    system that will help you to avoid your personal details being stolen by any malicious
    party out there.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Biometrics, a general approach
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The general idea behind identifying a person using a biometric property is the
    same for all biometrics out there. There are several steps that we should follow
    in the correct order if we want to achieve decent results. Moreover, we will point
    out some major points inside these general steps that will help you improve your
    recognition rate with extreme measures.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – getting a good training dataset and applying application-specific normalization
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key to most biometric identification systems is to collect a system training
    dataset that is representative of the problem for which you will actually use
    the system. Research has proven that there is something called **dataset bias**,
    which means that if you train a system on a training set with a specific setup,
    environmental factors, and recording devices, and then apply that system to a
    test set which has been taken from a completely different setup with different
    environmental factors (like lighting sources) and different recording devices,
    then this will produce a decrease in performance of up to 25%. This is a very
    large setback in performance, since you want to make sure that your identification
    system runs with top performance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, there are several things to consider when creating your training
    set for your identification system:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: You should only collect training data with the **known setup** that will be
    used when applying the biometric recognition. This means that you need to decide
    on hardware before you start training models and classifiers.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For biometric login systems, it is important to **constrain your data** as much
    as possible. If you can eliminate lighting changes, different background setups,
    movement, non-equal positioning, and so on, then you can drastically improve the
    performance of your application.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to **normalize your data** orientation-wise. If you align all your training
    data to the same position, you avoid introducing undesired variance in a single
    person's description of the biometric. Research in this field has proven that
    this can increase recognition rates by more than 15%!
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use **multiple training instances** of a single biometric and use the average
    biometric description for authenticating a person. Single-shot training systems
    have the downside that slight differences between two biometric recordings have
    a large influence on the classification rate. Single-shot learning is still a
    very active research topic, and there is yet to be found a very stable solution
    to this problem.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to apply this normalization for specific techniques will be discussed in
    the corresponding subtopics; for example, in the case of face recognition, since
    it can actually depend a lot on the techniques used. Once you get a good training
    set, with sufficient samples, you are ready to move to the second step.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keep in mind that there will be cases where applying constraints is not always
    a good way to go. Consider a laptop login system based on biometric features that
    only works with the lights on like face detection and recognition. That system
    would not work when somebody was working in a dark room. In that case, you would
    reconsider your application and ensure that there were enough biometric checks
    irrelevant to the changing light. You could even check the light intensity yourself
    through the webcam and disable the face check if you could predict that it would
    fail.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The simplification of the application and circumstances involves simplifying
    the algorithm discussed in this chapter, leading to better performance in these
    constrained scenarios.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – creating a descriptor of the recorded biometric
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you get the required training data to build your biometric identification
    system, it is important to find a way to uniquely describe each biometric parameter
    for each individual. This description is called a "unique feature vector" and
    it has several benefits compared to the original recorded image:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'A full scale RGB image with high resolution (which is used a lot in biometrics
    recording) contains a lot of data. If we applied the classification to the complete
    image it would be:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computationally very expensive.
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Not as unique as desired, since regions over different persons can be identical
    or very similar.
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It reduces the important and unique information in an input image to a sparse
    representation based on keypoints which are unique features of each image.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, how you construct the feature descriptor depends on which biometric you
    want to use to authenticate. Some approaches are based on Gabor filter banks,
    local binary pattern descriptions, and keypoint descriptors such as SIFT, SURF,
    and ORB. The possibilities are, again, endless. It all depends on getting the
    best description for your application. We will make suggestions for each biometric,
    but a more exhaustive search will need to be done to find the best solution for
    your application.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – using machine learning to match the retrieved feature vector
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each feature vector created from step 2 needs to be unique to ensure that a
    machine learning technique based on these feature vectors can differentiate between
    the biometrics of different test subjects. Therefore, it is important to have
    a descriptor with enough dimensions. Machine learning techniques are way better
    at separating data in high dimensional spaces than humans are, while they fail
    at separating data at low dimension feature spaces, for which a human brain outperforms
    the system.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the best machine learning approach is very cumbersome. In principle,
    different techniques offer similar results, but getting the best one is a game
    of trial and error. You can apply parameter optimization inside each machine learning
    approach to get even better results. This optimization would be too detailed for
    this chapter. People interested in this should take a deeper look at **hyper parameter
    optimization** techniques.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some interesting publications about this hyper parameter optimization problem
    can be found below:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Bergstra J. S., Bardenet R., Bengio Y., and Kégl B. (2011), *Algorithms for
    hyper-parameter optimization*, in Advances in Neural Information Processing Systems
    (pp. 2546-2554).
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bergstra J. and Bengio Y. (2012), *Random search for hyper-parameter optimization*,
    The Journal of Machine Learning Research, 13(1), 281-305.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snoek J., Larochelle H., and Adams R. P. (2012), *Practical Bayesian optimization
    of machine learning algorithms*, in Advances in Neural Information Processing
    Systems (pp. 2951-2959).
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are many machine learning techniques in OpenCV 3\. Some of the most frequently
    used techniques can be found below in order of complexity:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '**Similarity matching** using distance metrics.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-Nearest neighbors search**: A multi (K) class classification based on distance
    (Euclidean, Hamming, and so on) calculations between feature vectors in a high
    dimensional space.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Naïve Bayes classifier**: A binary classifier that uses Bayesian learning
    to differentiate between different classes.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support vector machines**: Mostly used as a binary classifier learning approach,
    but can be adapted to a multi-class classifier system. This approach depends on
    the separation of data in high dimensional spaces by looking for optimal separation
    plains between training data clouds and separating margins.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting and random forests**: Techniques that combine several weak classifiers
    or learners into a complex model able to separate binary and multi-class problems.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artificial neural networks**: A group of techniques that use the power of
    combining huge amounts of neurons (like the small cells in the brain) that learn
    connections and decisions based on examples. Due to their steeper learning curve
    and complex optimization steps, we will discard their use in this chapter.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you are interested in using neural networks for your classification problems,
    then take a look at this OpenCV documentation page:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.opencv.org/master/d0/dce/classcv_1_1ml_1_1ANN__MLP.html](http://docs.opencv.org/master/d0/dce/classcv_1_1ml_1_1ANN__MLP.html)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – think about your authentication process
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have a machine learning technique that outputs a classification for
    your input feature vector, you need to retrieve a certainty. This certainty is
    needed to be sure how certain a classification result is. For example, if a certain
    output has a match for both entry 2 and entry 5 in a database, then you will need
    to use the certainty to be sure of which of the two matches you should continue
    with.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Here, it is also important to think about how your authentication system will
    operate. It can either be a one-versus-one approach, where you match each database
    entry with your test sample until you get a high enough matching score, or a one-versus-all
    approach, where you match the complete database, then look at the retrieved score
    for each match and take the best match possible.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: One-versus-one can be seen as an iterative version of one-versus-all. They usually
    use the same logic; the difference is in the data structure used during the comparison.
    The one-versus-all approach requires a more complex way of storing and indexing
    the data, while one-versus-one uses a more brute-force approach.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keep in mind that both techniques can yield different results due to the fact
    that a false positive match is always possible in machine learning.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Imagine an input test query for your system. Using one-versus-one matching,
    you would stop analyzing the database when you had a high enough match. However,
    if further down the road there was a match yielding an even higher score, then
    this one would be discarded. With the one-versus-all approach, this could be avoided,
    so in many cases it is better to apply this one-versus-all approach.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: To give an example of which approach to use in a given case, imagine a door
    to a secret lab. If you want to check if a person is allowed to enter the lab,
    then a one-versus-all approach is required to make sure that you match all database
    entries and that the highest matching score has a certainty above a certain threshold.
    However, if this final secret lab door is only used to select who is already allowed
    to enter the room, then a one-versus-one approach is sufficient.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: In order to avoid problems with two individuals who have very similar descriptors
    for a single biometric, multiple biometrics are combined to reduce the occurrence
    of false positive detections. This will be discussed further at the end of this
    chapter, when we combine multiple biometrics in an effective authentication system.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Face detection and recognition
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most existing authentication systems start by detecting a face and trying to
    recognize it by matching it to a database of known people who use the system.
    This subsection will take a closer look at that. We will not dive into every single
    parameter of the software.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want more information about complete face detection and the recognition
    pipeline for both people and cats, then take a look at one of the PacktPub books
    called *OpenCV for Secret Agents*. It looks at the complete process in more detail.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: If you want a very detailed explanation of the parameters used for the face
    detection interface in OpenCV based on the cascade classification pipeline from
    Viola and Jones, then I suggest going to [Chapter 5](part0043_split_000.html#190862-940925703e144daa867f510896bffb69
    "Chapter 5. Generic Object Detection for Industrial Applications"), *Generic Object
    Detection for Industrial Applications*, which discusses the interface generalized
    for generic object detection.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Whenever you are focusing on an authentication system, you want to make sure
    that you are familiar with the different sub-tasks that need to be applied, as
    seen in the figure *An example of face detection software and the cut-out face
    region* in the section *Face detection using the Viola and Jones boosted cascade
    classifier algorithm*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: You should start by using a **general face detector**. This is used to find
    faces in any given input; for example, from your webcam. We will use the Viola
    and Jones face detector inside OpenCV, trained with a cascade classifier based
    on AdaBoost.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Secondly, you should perform some normalization on the image. In our case, we
    will apply some grayscaling, histogram equalization, and some alignment based
    on eye and mouth detection.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the data needs to be passed to a face recognizer interface. We will
    discuss the different options briefly (LBPH, Eigenfaces, and Fisherfaces) and
    walk you through it. This will return the selected user from the database we use
    to match to.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will discuss the possible advantages, disadvantages, and risks of possible
    approaches at all stages. We will also suggest several open source packages that
    give you the chance to further optimize the approach if you want to.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Software for this subsection can be found at the following location:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Face detection using the Viola and Jones boosted cascade classifier algorithm
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most webcam setups nowadays provide a high resolution RGB image as input. However,
    keep in mind that, for all OpenCV based operations, OpenCV formats the input as
    a BGR image. Therefore, we should apply some pre-processing steps to the output
    image before applying a face detector.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Start by converting the image to a grayscale image. The Viola and Jones approach
    uses a HAAR wavelet or local binary pattern-based features, which are both independent
    of color. Both feature types look for regions of changing pixel intensities. Therefore,
    we can omit this extra color information and reduce the amount of data that needs
    to be processed.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the resolution of the image. This depends on the webcam output format
    but, keeping in mind that processing time increases exponentially with increasing
    resolution, a ratio of 640x360 is more than enough for face detection.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply **histogram equalization** to the image to cover invariance under different
    illuminations. Basically, this operation tries to flatten the intensity histogram
    of the complete image. The same was done when training the detection models in
    OpenCV 3 and doing the same works here.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is always good to use different input and output containers for algorithms,
    since inline operations tend to do very nasty things to the output. Avoid problems
    by declaring an extra variable if you are not sure that inline replacement is
    supported.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet illustrates this behavior:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once you have done all the preprocessing, you can apply the following code
    to have an operational face detector on your input image:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can now draw the retrieved rectangles on top of the image to visualize the
    detections.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to know more about used parameters or retrieved detections, have
    a look at [Chapter 5](part0043_split_000.html#190862-940925703e144daa867f510896bffb69
    "Chapter 5. Generic Object Detection for Industrial Applications"), *Generic Object
    Detection for Industrial Applications*, which discusses this interface in much
    more detail.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'The face detections will look like the figure below:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![Face detection using the Viola and Jones boosted cascade classifier algorithm](img/00097.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: An example of face detection software and the cut-out face region
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you should cut out the detected face regions so that they can be passed
    to the interfaces that will process the image. The best approach is to grab these
    face regions from the original resized image, as seen in the preceding figure,
    and not from the visualization matrix, to avoid the red border being cut out and
    polluting the face image.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The software for executing this face detection can be found at:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_detection/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_detection/).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Data normalization on the detected face regions
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are only interested in a basic test setup, then face normalization steps
    are not really necessary. They are mainly used for improving the quality of your
    face recognition software.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: A good way to start is to reduce the amount of variation in the image. You can
    already apply conversion to grayscale and histogram equalization to remove information
    from the image, as described in the previous subtopic. This would be enough if
    you wanted a simple test setup, but it would require the person to keep their
    head positioned in the same way as the training data was grabbed for that person.
    If not, then the slight variation in the data due to a different head position
    would be enough to trigger a false positive match with another person in the database.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this, and to increase the quality of the following face recognition
    system, we propose applying face alignment. This can be done in several ways.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: As a basic approach, one could run an eye and mouth detector based on the existing
    OpenCV detectors, and use the centers of the detections as a way to align faces.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: For a very detailed explanation, refer to chapter 8 of *Mastering OpenCV* by
    Shervan Emami ([https://github.com/MasteringOpenCV/code/tree/master/Chapter8_FaceRecognition](https://github.com/MasteringOpenCV/code/tree/master/Chapter8_FaceRecognition)).
    He discusses several ways to align faces using eye detection.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Also, have a look at the section *Finding the face region in the image* in [Chapter
    3](part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69 "Chapter 3. Recognizing
    Facial Expressions with Machine Learning"), *Recognizing Facial Expressions with
    Machine Learning*.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A more advanced approach would be to apply a facial landmark detector and use
    all those points to normalize and align the faces.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are interested in more advanced techniques, take a look at the flandmark
    library ([http://cmp.felk.cvut.cz/~uricamic/flandmark/](http://cmp.felk.cvut.cz/~uricamic/flandmark/)).
    More information about using the facial landmark techniques can be found in [Chapter
    3](part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69 "Chapter 3. Recognizing
    Facial Expressions with Machine Learning"), *Recognizing Facial Expressions with
    Machine Learning*, which discusses how to install this library, configure the
    software, and then run it on any given face image.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A good discussion about face alignment can be found at the following OpenCV
    Q&A forum: [http://answers.opencv.org/question/24670/how-can-i-align-face-images/](http://answers.opencv.org/question/24670/how-can-i-align-face-images/).
    Multiple active forum users have gathered their OpenCV knowledge to come up with
    a very promising alignment technique, based on basic facial landmark techniques.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'The most basic alignment can be carried out by using the following approach:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Start by detecting the two eyes using the provided eye cascades.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the center points of both eye detections.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the angle between both eyes.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rotate the image around its own center.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code does this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This will generate an Eigenface-based model ready for training, which will use
    all eigenvectors (which could be slow) and without a certainty threshold. To be
    able to use them, you need to use an overloaded interface of the face recognizer.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '`Ptr<` `BasicFaceRecognizer > face_model = createEigenFaceRecognizer(20);`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ptr<` `BasicFaceRecognizer > face_model = createEigenFaceRecognizer(20, 100.0);`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, you need to make a decision on what you actually want to achieve. The
    training will be fast with a low number of eigenvectors, but the accuracy will
    be lower. To increase the accuracy, increase the number of eigenvectors used.
    Getting the correct number of eigenvectors is quite cumbersome since it depends
    a lot on the training data used. As a heuristic, you could train a recognizer
    with a low number of eigenvectors, test the recognition rate on a test set, and
    then increase the number of eigenvectors as long as you do not reach the recognition
    rate goal.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the model can be learned with the following code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you want to have a bit more information on the prediction, like the prediction
    confidence, then you can replace the last line with:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is a basic setup. The things that you need to remember to improve the
    quality of your model are:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Generally the more training faces of a person you have, the better a new sample
    of that person will be recognized. However, keep in mind that your training samples
    should contain as many different situations as possible, regarding lighting conditions,
    facial hair, attributes, and so on.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the number of eigenvectors used for projecting increases accuracy,
    but it also makes the algorithm slower. Finding a good trade-off is very important
    for your application.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To avoid fraud getting in by the best match from the database principle, you
    can use the confidence scores to threshold out matches that are not secure enough
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to do further research on the algorithm specifics, I suggest reading
    a paper that describes the technique in more detail:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Turk, Matthew, and Alex P. Pentland. "Face recognition using eigenfaces" Computer
    Vision and Pattern Recognition, 1991\. Proceedings CVPR'91., IEEE Computer Society
    Conference on. IEEE, 1991.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to play along with the internal data of the Eigenface-based model,
    you can retrieve interesting information using the following code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Some output results on the samples that we used for testing can be seen in the
    figure below. Remember that, if you want to show these images, you will need to
    transform them to the [0 255] range. The OpenCV 3 FaceRecognizer guide shows clearly
    how you should do this. The jet color space is often used to visualize Eigenfaces
    data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The complete and detailed OpenCV 3 FaceRecognizer interface guide can be found
    at the following web page, and discusses further use of these parameters in more
    depth than this chapter:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.opencv.org/master/da/d60/tutorial_face_main.html](http://docs.opencv.org/master/da/d60/tutorial_face_main.html)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenface decomposition through PCA](img/00099.jpeg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: The first ten Eigenfaces visualized in their most common color spaces, grayscale
    and JET. Note the influence of the background.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Linear discriminant analysis using the Fisher criterion
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The downside of using the Eigenface decomposition is that the transformation
    is optimal if you think about the pure reconstruction of the given data, however,
    the technique does not take into account class labels. This could lead to a case
    where the axes of maximal variance are actually created by external sources rather
    than the faces themselves. In order to cope with this problem, the technique of
    using LDA, or linear discriminant analysis, was introduced, based on the Fisher
    criterion. This minimizes variance within a single class, while maximizing variance
    between classes at the same time, which makes the technique more robust in the
    long run.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'The software for executing this face detection can be found at:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_fisher/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_fisher/)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a LDA face recognizer interface using the Fisher criteria, you should
    use the following code snippet in OpenCV 3:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you want to get more specific properties of the model, this can be achieved
    with property-specific functions, as depicted below. Remember that, if you want
    to show these images, you will need to transform them to the [0 255] range. The
    bone color space is often used to visualize Fisherfaces data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear discriminant analysis using the Fisher criterion](img/00100.jpeg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: The first 10 Fisherface dimensions, visualized in their most common color spaces,
    grayscale and BONE.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the background influence is minimal for these Fisherfaces compared
    to the previous Eigenfaces technique. This is the main advantage of Fisherfaces
    over Eigenfaces.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'It is nice to know that both Eigenfaces and Fisherfaces support the reconstruction
    of any given input inside the Eigenspace or Fisherspace at a certain point in
    mapping onto the dimensions selected. This is done by applying the following code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The software for executing this face detection can be found at:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_projection/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_projection/)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: This will result in the output shown in the figure below. We re-project one
    of the test subjects at different stages in the Eigenspace, subsequently adding
    25 eigenvectors to the representation. Here, you can clearly see that we have
    succeeded in reconstructing the individual in 12 steps. We can apply a similar
    procedure to the Fisherfaces. However, due to the fact that Fisherfaces have lower
    dimensionality, and the fact that we only look for features to distinguish between
    labels, we cannot expect a reconstruction that is as pure as it is with Eigenfaces.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear discriminant analysis using the Fisher criterion](img/00101.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: Reprojection result for both Eigenfaces and Fisherfaces
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to do further research on the algorithm specifics, I suggest reading
    a paper that describes the technique in more detail:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Belhumeur Peter N., João P. Hespanha, and David J. Kriegman, *Eigenfaces vs.
    fisherfaces: Recognition using class specific linear projection*, Pattern Analysis
    and Machine Intelligence, IEEE Transactions on 19.7 (1997): 711-720.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Local binary pattern histograms
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of simply reducing the dimensionality to universal axes, another approach
    is the use of local feature extraction. By looking at local features rather than
    a complete global description of the feature, researchers have tried to cope with
    problems like partial occlusion, illumination, and small sample size. The use
    of local binary pattern intensity histograms is a technique that looks at local
    face information rather than looking at global face information for a single individual.
    Local binary patterns have their origin in texture analysis and have proven to
    be efficient at face recognition by focusing on very specific local textured areas.
    This measure is more prone to changing lighting conditions than the previous techniques.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The software for executing this face detection can be found at:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_LBPH/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_LBPH/)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The LBPH features are illustrated below. They clearly show a more local feature
    description than Eigenfaces or Fisherfaces.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![Local binary pattern histograms](img/00102.jpeg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Example face image and its ELBP projection
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The software for executing this LBPH face projection can be found at:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_to_ELBP/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_to_ELBP/)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a LBP face recognizer interface using histograms of local binary patterns,
    you should use the following code snippet in OpenCV 3:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The LBPH interface also has an overload function, but this time related to
    the structure of the LBPH pattern and not the projection axes. This can be seen
    below:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Again, the function can operate with or without a threshold being set in advance.
    Getting or setting the parameters of the model can also be done using the specific
    getter and setter functions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to do further research on the algorithm specifics, I suggest reading
    a paper that describes the technique in more detail:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Ahonen Timo, Abdenour Hadid, and Matti Pietikäinen, *Face recognition with local
    binary patterns*, Computer vision-eccv 2004, Springer Berlin Heidelberg, 2004\.
    469-481.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: We provided functionality for each of the above three interfaces, also calculating
    the number of test samples classified correctly and the ones classified incorrectly,
    as shown below. In the case of LBPH, this means that we have a correct classification
    rate on the test samples of 96.25%, which is quite amazing with the very limited
    training data of only eight samples per person.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![Local binary pattern histograms](img/00103.jpeg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: The number of correctly classified samples is outputted after each run.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The problems with facial recognition in its current OpenCV 3 based implementation
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The techniques discussed enable us to recognize a face and link it to a person
    in the dataset. However, there are still some problems with this system that should
    be addressed:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: When using the Eigenfaces system, it is a general rule that the more Eigenvectors
    you use, the better the system will become, and the higher the accuracy will be.
    Defining how many dimensions you need to get a decent recognition result is frustrating,
    since it depends on how the data is presented to the system. The more variation
    there is in the original data, the more challenging the task will be, and thus
    the more dimensions you will need. The experiments of Philipp Wagner have shown
    that, in the AT&T database, about 300 Eigenvectors should be enough.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can apply thresholding to both Eigenfaces and Fisherfaces. This is a must
    if you want to be certain of classification accuracy. If you do not apply this,
    then the system will basically return the best match. If a given person is not
    part of the dataset, then you want to avoid this, and that can be done by calling
    the interface with a threshold value!
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that, with all face recognition systems, if you train them with
    data in one setup and test them with data containing completely different situations
    and setups, then the drop in accuracy will be huge.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you build a recognition system based on 2D image information, then frauds
    will be able to hack it by simply printing a 2D image of the person and presenting
    it to the system. In order to avoid this, either include 3D knowledge or add extra
    biometrics.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More information on adding 3D information to avoid fraud attempts can be found
    in the following publications:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Akarun Lale, B. Gokberk, and Albert Ali Salah, *3D face recognition for biometric
    applications*, Signal Processing Conference, 2005 13th European. IEEE, 2005.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Abate Andrea F., et al, *2D and 3D face recognition: A survey*, Pattern Recognition
    Letters 28.14 (2007): 1885-1906.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: However, this topic is too specific and complex for the scope of this chapter,
    and will thus not be discussed further.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Fingerprint identification, how is it done?
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we discussed the use of the first biometric, which
    is the face of the person trying to log in to the system. However, since we mentioned
    that using a single biometric is risky, it is better to add secondary biometric
    checks to the system, like a fingerprint. There are several off-the-shelf fingerprint
    scanners that are quite cheap and return you a scanned image. However, you will
    still have to write your own registration software for these scanners, and this
    can be done with OpenCV. Examples of such fingerprint images can be found below:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![Fingerprint identification, how is it done?](img/00104.jpeg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: Examples of single individual thumbprints from different scanners
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset can be downloaded from the FVC2002 competition website released
    by the University of Bologna. The website ([http://bias.csr.unibo.it/fvc2002/databases.asp](http://bias.csr.unibo.it/fvc2002/databases.asp))
    contains four databases of fingerprints available for public download in the following
    format:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Four fingerprint capturing devices, DB1 - DB4
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each device, the prints of 10 individuals are available
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each person, eight different positions of prints were recorded
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use this publicly available dataset to build our system. We will focus
    on the first capturing device, using up to four fingerprints from each individual
    for training the system and making an average descriptor of the fingerprint. Then,
    we will use the other four fingerprints to evaluate our system and make sure that
    the person is still recognized by our system.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: You can apply the same approach to the data grabbed from the other devices if
    you want to investigate the difference between a system that captures binary images
    and one that captures grayscale images. However, we will provide techniques for
    doing the binarization yourself.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the approach in OpenCV 3
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The complete fingerprint software for processing fingerprints obtained from
    a fingerprint scanner can be found at:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/fingerprint/fingerprint_process/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/fingerprint/fingerprint_process/)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'In this subsection, we will describe how you can implement this approach in
    the OpenCV interface. We start by grabbing the image from the fingerprint system
    and applying binarization. This enables us to remove any noise from the image,
    as well as helping us to make the contrast better between the skin and the wrinkled
    surface of the finger:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The Otsu thresholding will automatically choose the best generic threshold for
    the image to obtain a good contrast between foreground and background information.
    This is because the image contains a bimodal distribution (which means that we
    have an image with two peak histograms) of pixel values. For that image, we can
    take an approximate value in the middle of those peaks as the threshold value
    (for images that are not bimodal, binarization won't be accurate). Otsu allows
    us to avoid using a fixed threshold value, making the system more compatible with
    capturing devices. However, we do acknowledge that, if you only have one capturing
    device, then playing around with a fixed threshold value may result in a better
    image for that specific setup. The result of the thresholding can be seen below.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: In order to make the thinning from the next skeletization step as effective
    as possible, we need to invert the binary image.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the approach in OpenCV 3](img/00105.jpeg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Comparison of grayscale and binarized fingerprint images
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a binary image, we are ready to calculate our feature points and
    feature point descriptors. However, in order to improve the process a bit more,
    it is better to skeletize the image. This will create more unique and stronger
    interest points. The following piece of code can apply the skeletization on top
    of the binary image. The skeletization is based on the Zhang-Suen line-thinning
    approach.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Special thanks to `@bsdNoobz` of the OpenCV Q&A forum, who supplied this iteration
    approach.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The code above can then simply be applied to our previous steps by calling
    the thinning function on top of our previous binary-generated image. The code
    for this is:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will result in the following output:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the approach in OpenCV 3](img/00106.jpeg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: Comparison of binarized and thinned fingerprint images using skeletization techniques
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: When we get this skeleton image, the next step is to look for crossing points
    on the ridges of the fingerprint, called minutiae points. We can do this with
    a keypoint detector that looks for large changes in local contrast, like the Harris
    corner detector. Since the Harris corner detector is able to detect strong corners
    and edges, it is ideal for the fingerprint problem, where the most important minutiae
    are short edges and bifurcations—the positions where edges come together.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More information about minutiae points and Harris corner detection can be found
    in the following publications:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Ross Arun A., Jidnya Shah, and Anil K. Jain, *Toward reconstructing fingerprints
    from minutiae points*, Defense and Security. International Society for Optics
    and Photonics, 2005.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Harris Chris and Mike Stephens, *A combined corner and edge detector*, Alvey
    vision conference, Vol. 15, 1988.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Calling the Harris Corner operation on a skeletonized and binarized image in
    OpenCV is quite straightforward. The Harris corners are stored as positions corresponding
    with their cornerness response value in the image. If we want to detect points
    with a certain cornerness, then we should simply threshold the image.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We now have a map with all the available corner responses rescaled to the range
    of [0 255] and stored as float values. We can now manually define a threshold
    which will generate a good number of keypoints for our application. Playing around
    with this parameter could improve performance in other cases. This can be done
    by using the following code snippet:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Implementing the approach in OpenCV 3](img/00107.jpeg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: Comparison of thinned fingerprints and the Harris corner response, as well as
    the selected Harris corners
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a list of keypoints, we need to create some sort of formal
    descriptor of the local region around each keypoint to be able to uniquely identify
    it from other keypoints.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Chapter 3](part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69
    "Chapter 3. Recognizing Facial Expressions with Machine Learning"), *Recognizing
    Facial Expressions with Machine Learning*, discusses in more detail the wide range
    of keypoints out there. In this chapter, we will mainly focus on the process.
    Feel free to adapt the interface to other keypoint detectors and descriptors out
    there, for better or for worse performance.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have an application where the orientation of the thumb can differ
    (since it is not in a fixed position), we want a keypoint descriptor that is good
    at handling these slight differences. One of the most common descriptors for this
    is the SIFT descriptor, which stands for **scale invariant feature transform**.
    However, SIFT is not under a BSD license, which can pose problems when used in
    commercial software. A good alternative in OpenCV is the ORB descriptor. You can
    implement it in the following way:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This enables us to calculate only the descriptors using the ORB approach, since
    we already retrieved the location of the keypoints using the Harris corner approach.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can retrieve a descriptor for each detected keypoint of any
    given fingerprint. The descriptors matrix contains a row for each keypoint containing
    the representation.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the example in which we have just one reference image for each
    fingerprint. We then have a database containing a set of feature descriptors for
    the training persons in the database. We have a single new entry, consisting of
    multiple descriptors for the keypoints found at registration time. We now have
    to match these descriptors to the descriptors stored in the database, to see which
    one has the best match.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to do this is to perform brute-force matching using the hamming
    distance criteria between descriptors of different keypoints.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We now have all the matches stored as DMatch objects. This means that, for each
    matching couple, we will have the original keypoint, the matched keypoint, and
    a floating point score between both matches, representing the distance between
    the matched points.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: This seems pretty straightforward. We take a look at the number of matches that
    have been returned by the matching process and weigh them by their Euclidean distance
    in order to add some certainty. We then look for the matching process that yielded
    the biggest score. This will be our best match, and the match we want to return
    as the selected one from the database.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: If you want to avoid an imposter getting assigned to the best matching score,
    you can add a manual threshold on top of the scoring to avoid matches and ignore
    those that are not good enough. However, it is possible that, if you increase
    the score too much, people with little change will be rejected from the system,
    if, for example, someone cuts their finger and thus changes their pattern drastically.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the approach in OpenCV 3](img/00108.jpeg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: The fingerprint matching process visualized
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Iris identification, how is it done?
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last biometric that we will use is the output of an iris scan. Considering
    our setup, there might be several ways to grab iris data:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: We can separate the face and apply an eye detector using face detection, which
    can be done with a high-resolution camera. We can use the resulting regions to
    perform iris segmentation and classification.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use a specific eye camera, which grabs an eye image to be classified.
    This can be done either with RGB or NIR.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since the first approach is prone to a lot of problems, such as the resulting
    eye image having a low resolution, a more common approach is to use a separate
    eye camera that grabs the eye. This is the method that we will use in this chapter.
    An example of a captured eye in both the RGB (visible colors) and NIR (near infra-red)
    spectrums is visualized below:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![Iris identification, how is it done?](img/00109.jpeg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: An example of both a RGB and a NIR iris-based image
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Using NIR images helps us in several ways:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: First of all, color information is omitted, since a lot of conditions like external
    sources of light can influence color information when grabbing the iris image.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, the pupil center becomes clearer and fully black, which allows us
    to use techniques that depend on this for segmenting the pupil center.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thirdly, the available structure is maintained, under different lighting conditions,
    due to the NIR spectrum.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the outer border of the iris region is clearer, and thus more easily
    separable.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use data from the CASIA eye dataset for the iris recognition, which
    can be found at [http://biometrics.idealtest.org/](http://biometrics.idealtest.org/).
    This dataset is publicly available for research and non-commercial purposes, and
    access can be requested through the site. A small part of it can be found in our
    software repository, where we have a right and a left eye from one individual,
    which we can now treat as two people since no two irises are identical. We have
    10 samples for each eye, of which we will use eight to train and two to test.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: The approach that we will implement for iris recognition is based on the technique
    suggested by John Daugman. The technique is widely accepted and used in commercial
    systems, and has thus proven its quality.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The original paper written by John Daugman can be found at: [http://www.cl.cam.ac.uk/~jgd1000/irisrecog.pdf](http://www.cl.cam.ac.uk/~jgd1000/irisrecog.pdf)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the approach in OpenCV 3
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in getting the iris information is segmenting out the actual
    eye region, containing both the iris and the pupil. We apply a series of operations
    on top of our data to achieve the desired result. This process is necessary to
    keep only the desired data and remove all the redundant eye data that is still
    around.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'We first try to get the pupil. The pupil is the darkest area in NIR images,
    and this information can be used to our advantage. The following steps will lead
    us to the pupil area in an eye image:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to apply segmentation to the darkest regions. We can use the
    `inRange()` image, since the values in which the pupil lie are specific to the
    capturing system. However, due to the fact that they all use NIR, the end result
    will be identical for each separate system.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we apply contour detection to get the outer border of the pupil. We make
    sure that we get the biggest contour from just the outer contours so that we only
    keep one region.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to improve performance, you can also look for the bright spots of
    the IR LED first, remove them from the region, and then run the contour detection.
    This will improve robustness when IR LED spots are close to the pupil border.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the complete process of a single iris can be found at: [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_processing/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_processing/)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'This behavior can be achieved by using the following code snippet:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Implementing the approach in OpenCV 3](img/00110.jpeg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: '[PRE20]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Implementing the approach in OpenCV 3](img/00111.jpeg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: An example of the retrieved Hough Circle result, which gives us the outer border
    of the iris region, for both the left and right eyes.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have succeeded in finding the outer contour, it is pretty straightforward
    to mask the iris region from the original input, as shown in the figure below:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the approach in OpenCV 3](img/00112.jpeg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: An example of the masked iris image
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have our region of interest, meaning only the iris region, as shown
    in the above figure. We acknowledge that there could still be some partial whiskers
    inside the region, but for now we will simply ignore them. Now, we want to encode
    this iris image into a feature vector for comparison. There are two steps still
    to take to reach that level:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Unwrapping of the iris pattern from a polar coordinate system to a Cartesian
    coordinate system for further processing
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying encoding to the iris image and matching it against a database of known
    representations
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We start by providing a code snippet that will unwrap the desired iris region
    from the retrieved final result:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This will result in the following conversion, which gives us the radial unwrapping
    of the iris region, as shown in the figure below:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the approach in OpenCV 3](img/00113.jpeg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: An example of the radially unwrapped iris image for both the left and right
    eyes
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: This radial unwrapping is done for all the eight training images that we have
    for each eye and for the two testing images that we also have for each eye. The
    Daugman approach applies phase quadrant modulation to encode the iris pattern.
    However, this is not yet implemented in OpenCV and is too complex for this chapter.
    Therefore, we decided to look for an available OpenCV implementation that could
    be used to match the irises. A good approach is to use the local binary pattern
    histogram comparison, since we are looking for something that can identify local
    textures, and this was also used for face recognition.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Software for unwrapping a complete set of iris images can be found at:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_processing_batch/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_processing_batch/)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Software for creating the matching interface can be found at:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_recognition/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_recognition/)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, encoding works as follows in OpenCV 3:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We count the testing results again, which yields the result shown in the figure
    below:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the approach in OpenCV 3](img/00114.jpeg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
- en: Encoded iris image and the corresponding iris code visualized.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Combining the techniques to create an efficient people-registration system
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous sections each discussed a specific biometric property. Now, let''s
    combine all this information to create an efficient identification system. The
    approach that we will implement follows the structure described in the figure
    below:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![Combining the techniques to create an efficient people-registration system](img/00115.jpeg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: People authentication pipeline
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown above, the first step is to use a camera interface to check if there
    actually is a person in front of the camera. This is done by performing face detection
    on the input image. We also test to see if the other biometric systems are active.
    This leaves us two checks that need to be performed:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Check if the iris scanner is in use. This, of course, depends on the system
    used. If it depends on the eye retrieved from the face detection, this check should
    be ignored. If the eye is retrieved using an actual eye scanner, then there should
    at least be an eye detected to give a positive signal.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check if the fingerprint scanner is active. Do we actually have a finger available
    for taking a fingerprint picture? This is checked by applying background subtraction
    to the empty scene. If a finger is in place, then there should be a response to
    the background-foreground subtraction.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, we are aware that some of these systems use pressure-based detection
    to find a hand or finger. In such cases, you do not have to perform this check
    yourself, but let the system decide whether to proceed or not.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have all the systems, we can start the individual recognition systems
    described in previous sections. They will all output the identity of a known person
    from the common database that was constructed for this purpose. Then, all these
    outcomes are given to the smart majority voting. This system checks for several
    things:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: It checks if the biometric system checks actually succeeded, by returning their
    match from the database. If not, a person is not granted access and the system
    asks to reconfirm the failing biometrics.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the system has to measure biometrics more than three times in a row, the
    system jams and doesn't work until the owner of the system unlocks it. This is
    to avoid a bug in the current interface that exploits the system and tries to
    get in.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the biometric checks work, a smart majority voting is applied to the results.
    This means that if two biometrics identify person A but one biometric identifies
    person B, then the output result will still be person A. If that person is marked
    as the owner, then the system will allow access.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the individual software provided with the separate subtopics, it should
    be quite straightforward to combine them into a single interface.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the system still fails (this is a case study, not a 100% failproof system),
    there are several things that can be done to achieve the desired results.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should try to improve the detection and matching quality of each separate
    biometric. This can be done by supplying better training data, experimenting with
    different feature extraction methods or different feature comparison methods,
    as discussed in the introduction to the chapter. The variety of combinations is
    endless, so go ahead and give it a try.
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You should try to give each biometric a certainty score on its output. Since
    we have multiple systems voting for the identity of a person, we could take into
    account their certainty on single classifications. For example, when running a
    database, matching the distance to the best match can be wrapped to a scale range
    of [0 100] to give a certainty percentage. We can then multiply the vote of each
    biometric by its weight and do a smart-weighted majority voting.
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned that an authentication system can be more than
    a simple face recognition interface by using multiple biometric properties of
    the person trying to authenticate. We showed you how to perform iris and fingerprint
    recognition using the OpenCV library to make a multi-biometric authentication
    system. One can add even more biometrics to the system, since the possibilities
    are endless.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: The focus of the chapter was to get people interested in the power of biometrics
    and the endless possibilities of the OpenCV library. If you feel inspired by this,
    do experiment further and share your thoughts with the community.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是激发人们对生物识别技术力量和OpenCV库无限可能性的兴趣。如果你因此受到启发，不妨进一步实验，并与社区分享你的想法。
- en: Note
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'I would like to thank the users of the OpenCV Q&A discussion forum who helped
    me to push the limits when I hit brick walls. I would explicitly like to thank
    the following users for their directions: Berak, Guanta, Theodore, and GilLevi.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢OpenCV问答讨论论坛的用户们，在我遇到难题时，他们帮助我突破了限制。我特别想感谢以下用户提供的指导：Berak、Guanta、Theodore和GilLevi。
