- en: Chapter 6. Efficient Person Identification Using Biometric Properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The rise of digital media is greater than ever. People are placing more and
    more of their personal information on digital carriers like laptops, smartphones,
    and tablets. However, many of these systems do not provide efficient identification
    and authentication systems to ensure that strangers cannot access your personal
    data. This is where biometrics-based identification systems come into play and
    try to make your data more secure and less vulnerable to malicious people.
  prefs: []
  type: TYPE_NORMAL
- en: These identification systems can be used to lock down your computer, avoid people
    getting into a secure room, and so on, but, with technology improving each day,
    we are only one step away from further digitalizing our personal lives. How about
    using your facial expressions to unlock your door? How about opening your car
    with your fingerprint? The possibilities are endless.
  prefs: []
  type: TYPE_NORMAL
- en: Many techniques and algorithms are already available in open source computer
    vision and machine learning packages like OpenCV to efficiently use these personal
    identification properties. Of course, this opens up the possibility for enthusiastic
    computer vision programmers to create many different applications based on these
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on techniques that use individual biometrics
    in order to create personal authentication systems that outperform standard available
    login systems based on passwords. We will take a deeper look at iris and fingerprint
    recognition, face detection, and face recognition.
  prefs: []
  type: TYPE_NORMAL
- en: We will first discuss the main principles behind each biometric technique, and
    then we'll show an implementation based on the OpenCV 3 library. For some of the
    biometrics, we will make use of the available open source frameworks out there.
    All datasets used to demonstrate the techniques are available for free online
    for research purposes. However, if you want to apply them to a commercial application,
    be sure to check their licenses!
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will illustrate how you can combine several biometric classifications
    to increase the chance of successfully identifying a specific person based on
    the probability of the individual biometrics.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, you will be able to create a fully functional identification
    system that will help you to avoid your personal details being stolen by any malicious
    party out there.
  prefs: []
  type: TYPE_NORMAL
- en: Biometrics, a general approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The general idea behind identifying a person using a biometric property is the
    same for all biometrics out there. There are several steps that we should follow
    in the correct order if we want to achieve decent results. Moreover, we will point
    out some major points inside these general steps that will help you improve your
    recognition rate with extreme measures.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – getting a good training dataset and applying application-specific normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key to most biometric identification systems is to collect a system training
    dataset that is representative of the problem for which you will actually use
    the system. Research has proven that there is something called **dataset bias**,
    which means that if you train a system on a training set with a specific setup,
    environmental factors, and recording devices, and then apply that system to a
    test set which has been taken from a completely different setup with different
    environmental factors (like lighting sources) and different recording devices,
    then this will produce a decrease in performance of up to 25%. This is a very
    large setback in performance, since you want to make sure that your identification
    system runs with top performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, there are several things to consider when creating your training
    set for your identification system:'
  prefs: []
  type: TYPE_NORMAL
- en: You should only collect training data with the **known setup** that will be
    used when applying the biometric recognition. This means that you need to decide
    on hardware before you start training models and classifiers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For biometric login systems, it is important to **constrain your data** as much
    as possible. If you can eliminate lighting changes, different background setups,
    movement, non-equal positioning, and so on, then you can drastically improve the
    performance of your application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to **normalize your data** orientation-wise. If you align all your training
    data to the same position, you avoid introducing undesired variance in a single
    person's description of the biometric. Research in this field has proven that
    this can increase recognition rates by more than 15%!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use **multiple training instances** of a single biometric and use the average
    biometric description for authenticating a person. Single-shot training systems
    have the downside that slight differences between two biometric recordings have
    a large influence on the classification rate. Single-shot learning is still a
    very active research topic, and there is yet to be found a very stable solution
    to this problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to apply this normalization for specific techniques will be discussed in
    the corresponding subtopics; for example, in the case of face recognition, since
    it can actually depend a lot on the techniques used. Once you get a good training
    set, with sufficient samples, you are ready to move to the second step.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keep in mind that there will be cases where applying constraints is not always
    a good way to go. Consider a laptop login system based on biometric features that
    only works with the lights on like face detection and recognition. That system
    would not work when somebody was working in a dark room. In that case, you would
    reconsider your application and ensure that there were enough biometric checks
    irrelevant to the changing light. You could even check the light intensity yourself
    through the webcam and disable the face check if you could predict that it would
    fail.
  prefs: []
  type: TYPE_NORMAL
- en: The simplification of the application and circumstances involves simplifying
    the algorithm discussed in this chapter, leading to better performance in these
    constrained scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – creating a descriptor of the recorded biometric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you get the required training data to build your biometric identification
    system, it is important to find a way to uniquely describe each biometric parameter
    for each individual. This description is called a "unique feature vector" and
    it has several benefits compared to the original recorded image:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A full scale RGB image with high resolution (which is used a lot in biometrics
    recording) contains a lot of data. If we applied the classification to the complete
    image it would be:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computationally very expensive.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Not as unique as desired, since regions over different persons can be identical
    or very similar.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It reduces the important and unique information in an input image to a sparse
    representation based on keypoints which are unique features of each image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, how you construct the feature descriptor depends on which biometric you
    want to use to authenticate. Some approaches are based on Gabor filter banks,
    local binary pattern descriptions, and keypoint descriptors such as SIFT, SURF,
    and ORB. The possibilities are, again, endless. It all depends on getting the
    best description for your application. We will make suggestions for each biometric,
    but a more exhaustive search will need to be done to find the best solution for
    your application.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – using machine learning to match the retrieved feature vector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each feature vector created from step 2 needs to be unique to ensure that a
    machine learning technique based on these feature vectors can differentiate between
    the biometrics of different test subjects. Therefore, it is important to have
    a descriptor with enough dimensions. Machine learning techniques are way better
    at separating data in high dimensional spaces than humans are, while they fail
    at separating data at low dimension feature spaces, for which a human brain outperforms
    the system.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the best machine learning approach is very cumbersome. In principle,
    different techniques offer similar results, but getting the best one is a game
    of trial and error. You can apply parameter optimization inside each machine learning
    approach to get even better results. This optimization would be too detailed for
    this chapter. People interested in this should take a deeper look at **hyper parameter
    optimization** techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some interesting publications about this hyper parameter optimization problem
    can be found below:'
  prefs: []
  type: TYPE_NORMAL
- en: Bergstra J. S., Bardenet R., Bengio Y., and Kégl B. (2011), *Algorithms for
    hyper-parameter optimization*, in Advances in Neural Information Processing Systems
    (pp. 2546-2554).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bergstra J. and Bengio Y. (2012), *Random search for hyper-parameter optimization*,
    The Journal of Machine Learning Research, 13(1), 281-305.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snoek J., Larochelle H., and Adams R. P. (2012), *Practical Bayesian optimization
    of machine learning algorithms*, in Advances in Neural Information Processing
    Systems (pp. 2951-2959).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are many machine learning techniques in OpenCV 3\. Some of the most frequently
    used techniques can be found below in order of complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Similarity matching** using distance metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-Nearest neighbors search**: A multi (K) class classification based on distance
    (Euclidean, Hamming, and so on) calculations between feature vectors in a high
    dimensional space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Naïve Bayes classifier**: A binary classifier that uses Bayesian learning
    to differentiate between different classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support vector machines**: Mostly used as a binary classifier learning approach,
    but can be adapted to a multi-class classifier system. This approach depends on
    the separation of data in high dimensional spaces by looking for optimal separation
    plains between training data clouds and separating margins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting and random forests**: Techniques that combine several weak classifiers
    or learners into a complex model able to separate binary and multi-class problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artificial neural networks**: A group of techniques that use the power of
    combining huge amounts of neurons (like the small cells in the brain) that learn
    connections and decisions based on examples. Due to their steeper learning curve
    and complex optimization steps, we will discard their use in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you are interested in using neural networks for your classification problems,
    then take a look at this OpenCV documentation page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.opencv.org/master/d0/dce/classcv_1_1ml_1_1ANN__MLP.html](http://docs.opencv.org/master/d0/dce/classcv_1_1ml_1_1ANN__MLP.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – think about your authentication process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have a machine learning technique that outputs a classification for
    your input feature vector, you need to retrieve a certainty. This certainty is
    needed to be sure how certain a classification result is. For example, if a certain
    output has a match for both entry 2 and entry 5 in a database, then you will need
    to use the certainty to be sure of which of the two matches you should continue
    with.
  prefs: []
  type: TYPE_NORMAL
- en: Here, it is also important to think about how your authentication system will
    operate. It can either be a one-versus-one approach, where you match each database
    entry with your test sample until you get a high enough matching score, or a one-versus-all
    approach, where you match the complete database, then look at the retrieved score
    for each match and take the best match possible.
  prefs: []
  type: TYPE_NORMAL
- en: One-versus-one can be seen as an iterative version of one-versus-all. They usually
    use the same logic; the difference is in the data structure used during the comparison.
    The one-versus-all approach requires a more complex way of storing and indexing
    the data, while one-versus-one uses a more brute-force approach.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keep in mind that both techniques can yield different results due to the fact
    that a false positive match is always possible in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine an input test query for your system. Using one-versus-one matching,
    you would stop analyzing the database when you had a high enough match. However,
    if further down the road there was a match yielding an even higher score, then
    this one would be discarded. With the one-versus-all approach, this could be avoided,
    so in many cases it is better to apply this one-versus-all approach.
  prefs: []
  type: TYPE_NORMAL
- en: To give an example of which approach to use in a given case, imagine a door
    to a secret lab. If you want to check if a person is allowed to enter the lab,
    then a one-versus-all approach is required to make sure that you match all database
    entries and that the highest matching score has a certainty above a certain threshold.
    However, if this final secret lab door is only used to select who is already allowed
    to enter the room, then a one-versus-one approach is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: In order to avoid problems with two individuals who have very similar descriptors
    for a single biometric, multiple biometrics are combined to reduce the occurrence
    of false positive detections. This will be discussed further at the end of this
    chapter, when we combine multiple biometrics in an effective authentication system.
  prefs: []
  type: TYPE_NORMAL
- en: Face detection and recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most existing authentication systems start by detecting a face and trying to
    recognize it by matching it to a database of known people who use the system.
    This subsection will take a closer look at that. We will not dive into every single
    parameter of the software.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want more information about complete face detection and the recognition
    pipeline for both people and cats, then take a look at one of the PacktPub books
    called *OpenCV for Secret Agents*. It looks at the complete process in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: If you want a very detailed explanation of the parameters used for the face
    detection interface in OpenCV based on the cascade classification pipeline from
    Viola and Jones, then I suggest going to [Chapter 5](part0043_split_000.html#190862-940925703e144daa867f510896bffb69
    "Chapter 5. Generic Object Detection for Industrial Applications"), *Generic Object
    Detection for Industrial Applications*, which discusses the interface generalized
    for generic object detection.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever you are focusing on an authentication system, you want to make sure
    that you are familiar with the different sub-tasks that need to be applied, as
    seen in the figure *An example of face detection software and the cut-out face
    region* in the section *Face detection using the Viola and Jones boosted cascade
    classifier algorithm*.
  prefs: []
  type: TYPE_NORMAL
- en: You should start by using a **general face detector**. This is used to find
    faces in any given input; for example, from your webcam. We will use the Viola
    and Jones face detector inside OpenCV, trained with a cascade classifier based
    on AdaBoost.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Secondly, you should perform some normalization on the image. In our case, we
    will apply some grayscaling, histogram equalization, and some alignment based
    on eye and mouth detection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the data needs to be passed to a face recognizer interface. We will
    discuss the different options briefly (LBPH, Eigenfaces, and Fisherfaces) and
    walk you through it. This will return the selected user from the database we use
    to match to.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will discuss the possible advantages, disadvantages, and risks of possible
    approaches at all stages. We will also suggest several open source packages that
    give you the chance to further optimize the approach if you want to.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Software for this subsection can be found at the following location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/)'
  prefs: []
  type: TYPE_NORMAL
- en: Face detection using the Viola and Jones boosted cascade classifier algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most webcam setups nowadays provide a high resolution RGB image as input. However,
    keep in mind that, for all OpenCV based operations, OpenCV formats the input as
    a BGR image. Therefore, we should apply some pre-processing steps to the output
    image before applying a face detector.
  prefs: []
  type: TYPE_NORMAL
- en: Start by converting the image to a grayscale image. The Viola and Jones approach
    uses a HAAR wavelet or local binary pattern-based features, which are both independent
    of color. Both feature types look for regions of changing pixel intensities. Therefore,
    we can omit this extra color information and reduce the amount of data that needs
    to be processed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the resolution of the image. This depends on the webcam output format
    but, keeping in mind that processing time increases exponentially with increasing
    resolution, a ratio of 640x360 is more than enough for face detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply **histogram equalization** to the image to cover invariance under different
    illuminations. Basically, this operation tries to flatten the intensity histogram
    of the complete image. The same was done when training the detection models in
    OpenCV 3 and doing the same works here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is always good to use different input and output containers for algorithms,
    since inline operations tend to do very nasty things to the output. Avoid problems
    by declaring an extra variable if you are not sure that inline replacement is
    supported.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet illustrates this behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have done all the preprocessing, you can apply the following code
    to have an operational face detector on your input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can now draw the retrieved rectangles on top of the image to visualize the
    detections.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to know more about used parameters or retrieved detections, have
    a look at [Chapter 5](part0043_split_000.html#190862-940925703e144daa867f510896bffb69
    "Chapter 5. Generic Object Detection for Industrial Applications"), *Generic Object
    Detection for Industrial Applications*, which discusses this interface in much
    more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'The face detections will look like the figure below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Face detection using the Viola and Jones boosted cascade classifier algorithm](img/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of face detection software and the cut-out face region
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you should cut out the detected face regions so that they can be passed
    to the interfaces that will process the image. The best approach is to grab these
    face regions from the original resized image, as seen in the preceding figure,
    and not from the visualization matrix, to avoid the red border being cut out and
    polluting the face image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The software for executing this face detection can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_detection/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_detection/).'
  prefs: []
  type: TYPE_NORMAL
- en: Data normalization on the detected face regions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are only interested in a basic test setup, then face normalization steps
    are not really necessary. They are mainly used for improving the quality of your
    face recognition software.
  prefs: []
  type: TYPE_NORMAL
- en: A good way to start is to reduce the amount of variation in the image. You can
    already apply conversion to grayscale and histogram equalization to remove information
    from the image, as described in the previous subtopic. This would be enough if
    you wanted a simple test setup, but it would require the person to keep their
    head positioned in the same way as the training data was grabbed for that person.
    If not, then the slight variation in the data due to a different head position
    would be enough to trigger a false positive match with another person in the database.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this, and to increase the quality of the following face recognition
    system, we propose applying face alignment. This can be done in several ways.
  prefs: []
  type: TYPE_NORMAL
- en: As a basic approach, one could run an eye and mouth detector based on the existing
    OpenCV detectors, and use the centers of the detections as a way to align faces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: For a very detailed explanation, refer to chapter 8 of *Mastering OpenCV* by
    Shervan Emami ([https://github.com/MasteringOpenCV/code/tree/master/Chapter8_FaceRecognition](https://github.com/MasteringOpenCV/code/tree/master/Chapter8_FaceRecognition)).
    He discusses several ways to align faces using eye detection.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Also, have a look at the section *Finding the face region in the image* in [Chapter
    3](part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69 "Chapter 3. Recognizing
    Facial Expressions with Machine Learning"), *Recognizing Facial Expressions with
    Machine Learning*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A more advanced approach would be to apply a facial landmark detector and use
    all those points to normalize and align the faces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are interested in more advanced techniques, take a look at the flandmark
    library ([http://cmp.felk.cvut.cz/~uricamic/flandmark/](http://cmp.felk.cvut.cz/~uricamic/flandmark/)).
    More information about using the facial landmark techniques can be found in [Chapter
    3](part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69 "Chapter 3. Recognizing
    Facial Expressions with Machine Learning"), *Recognizing Facial Expressions with
    Machine Learning*, which discusses how to install this library, configure the
    software, and then run it on any given face image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A good discussion about face alignment can be found at the following OpenCV
    Q&A forum: [http://answers.opencv.org/question/24670/how-can-i-align-face-images/](http://answers.opencv.org/question/24670/how-can-i-align-face-images/).
    Multiple active forum users have gathered their OpenCV knowledge to come up with
    a very promising alignment technique, based on basic facial landmark techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most basic alignment can be carried out by using the following approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Start by detecting the two eyes using the provided eye cascades.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the center points of both eye detections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the angle between both eyes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rotate the image around its own center.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code does this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This will generate an Eigenface-based model ready for training, which will use
    all eigenvectors (which could be slow) and without a certainty threshold. To be
    able to use them, you need to use an overloaded interface of the face recognizer.
  prefs: []
  type: TYPE_NORMAL
- en: '`Ptr<` `BasicFaceRecognizer > face_model = createEigenFaceRecognizer(20);`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ptr<` `BasicFaceRecognizer > face_model = createEigenFaceRecognizer(20, 100.0);`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, you need to make a decision on what you actually want to achieve. The
    training will be fast with a low number of eigenvectors, but the accuracy will
    be lower. To increase the accuracy, increase the number of eigenvectors used.
    Getting the correct number of eigenvectors is quite cumbersome since it depends
    a lot on the training data used. As a heuristic, you could train a recognizer
    with a low number of eigenvectors, test the recognition rate on a test set, and
    then increase the number of eigenvectors as long as you do not reach the recognition
    rate goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the model can be learned with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to have a bit more information on the prediction, like the prediction
    confidence, then you can replace the last line with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a basic setup. The things that you need to remember to improve the
    quality of your model are:'
  prefs: []
  type: TYPE_NORMAL
- en: Generally the more training faces of a person you have, the better a new sample
    of that person will be recognized. However, keep in mind that your training samples
    should contain as many different situations as possible, regarding lighting conditions,
    facial hair, attributes, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the number of eigenvectors used for projecting increases accuracy,
    but it also makes the algorithm slower. Finding a good trade-off is very important
    for your application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To avoid fraud getting in by the best match from the database principle, you
    can use the confidence scores to threshold out matches that are not secure enough
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to do further research on the algorithm specifics, I suggest reading
    a paper that describes the technique in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Turk, Matthew, and Alex P. Pentland. "Face recognition using eigenfaces" Computer
    Vision and Pattern Recognition, 1991\. Proceedings CVPR'91., IEEE Computer Society
    Conference on. IEEE, 1991.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to play along with the internal data of the Eigenface-based model,
    you can retrieve interesting information using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Some output results on the samples that we used for testing can be seen in the
    figure below. Remember that, if you want to show these images, you will need to
    transform them to the [0 255] range. The OpenCV 3 FaceRecognizer guide shows clearly
    how you should do this. The jet color space is often used to visualize Eigenfaces
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The complete and detailed OpenCV 3 FaceRecognizer interface guide can be found
    at the following web page, and discusses further use of these parameters in more
    depth than this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.opencv.org/master/da/d60/tutorial_face_main.html](http://docs.opencv.org/master/da/d60/tutorial_face_main.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenface decomposition through PCA](img/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The first ten Eigenfaces visualized in their most common color spaces, grayscale
    and JET. Note the influence of the background.
  prefs: []
  type: TYPE_NORMAL
- en: Linear discriminant analysis using the Fisher criterion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The downside of using the Eigenface decomposition is that the transformation
    is optimal if you think about the pure reconstruction of the given data, however,
    the technique does not take into account class labels. This could lead to a case
    where the axes of maximal variance are actually created by external sources rather
    than the faces themselves. In order to cope with this problem, the technique of
    using LDA, or linear discriminant analysis, was introduced, based on the Fisher
    criterion. This minimizes variance within a single class, while maximizing variance
    between classes at the same time, which makes the technique more robust in the
    long run.
  prefs: []
  type: TYPE_NORMAL
- en: 'The software for executing this face detection can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_fisher/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_fisher/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a LDA face recognizer interface using the Fisher criteria, you should
    use the following code snippet in OpenCV 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you want to get more specific properties of the model, this can be achieved
    with property-specific functions, as depicted below. Remember that, if you want
    to show these images, you will need to transform them to the [0 255] range. The
    bone color space is often used to visualize Fisherfaces data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear discriminant analysis using the Fisher criterion](img/00100.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The first 10 Fisherface dimensions, visualized in their most common color spaces,
    grayscale and BONE.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the background influence is minimal for these Fisherfaces compared
    to the previous Eigenfaces technique. This is the main advantage of Fisherfaces
    over Eigenfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is nice to know that both Eigenfaces and Fisherfaces support the reconstruction
    of any given input inside the Eigenspace or Fisherspace at a certain point in
    mapping onto the dimensions selected. This is done by applying the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The software for executing this face detection can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_projection/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_projection/)'
  prefs: []
  type: TYPE_NORMAL
- en: This will result in the output shown in the figure below. We re-project one
    of the test subjects at different stages in the Eigenspace, subsequently adding
    25 eigenvectors to the representation. Here, you can clearly see that we have
    succeeded in reconstructing the individual in 12 steps. We can apply a similar
    procedure to the Fisherfaces. However, due to the fact that Fisherfaces have lower
    dimensionality, and the fact that we only look for features to distinguish between
    labels, we cannot expect a reconstruction that is as pure as it is with Eigenfaces.
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear discriminant analysis using the Fisher criterion](img/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Reprojection result for both Eigenfaces and Fisherfaces
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to do further research on the algorithm specifics, I suggest reading
    a paper that describes the technique in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Belhumeur Peter N., João P. Hespanha, and David J. Kriegman, *Eigenfaces vs.
    fisherfaces: Recognition using class specific linear projection*, Pattern Analysis
    and Machine Intelligence, IEEE Transactions on 19.7 (1997): 711-720.'
  prefs: []
  type: TYPE_NORMAL
- en: Local binary pattern histograms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of simply reducing the dimensionality to universal axes, another approach
    is the use of local feature extraction. By looking at local features rather than
    a complete global description of the feature, researchers have tried to cope with
    problems like partial occlusion, illumination, and small sample size. The use
    of local binary pattern intensity histograms is a technique that looks at local
    face information rather than looking at global face information for a single individual.
    Local binary patterns have their origin in texture analysis and have proven to
    be efficient at face recognition by focusing on very specific local textured areas.
    This measure is more prone to changing lighting conditions than the previous techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The software for executing this face detection can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_LBPH/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_LBPH/)'
  prefs: []
  type: TYPE_NORMAL
- en: The LBPH features are illustrated below. They clearly show a more local feature
    description than Eigenfaces or Fisherfaces.
  prefs: []
  type: TYPE_NORMAL
- en: '![Local binary pattern histograms](img/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Example face image and its ELBP projection
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The software for executing this LBPH face projection can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_to_ELBP/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_to_ELBP/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a LBP face recognizer interface using histograms of local binary patterns,
    you should use the following code snippet in OpenCV 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The LBPH interface also has an overload function, but this time related to
    the structure of the LBPH pattern and not the projection axes. This can be seen
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Again, the function can operate with or without a threshold being set in advance.
    Getting or setting the parameters of the model can also be done using the specific
    getter and setter functions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to do further research on the algorithm specifics, I suggest reading
    a paper that describes the technique in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Ahonen Timo, Abdenour Hadid, and Matti Pietikäinen, *Face recognition with local
    binary patterns*, Computer vision-eccv 2004, Springer Berlin Heidelberg, 2004\.
    469-481.
  prefs: []
  type: TYPE_NORMAL
- en: We provided functionality for each of the above three interfaces, also calculating
    the number of test samples classified correctly and the ones classified incorrectly,
    as shown below. In the case of LBPH, this means that we have a correct classification
    rate on the test samples of 96.25%, which is quite amazing with the very limited
    training data of only eight samples per person.
  prefs: []
  type: TYPE_NORMAL
- en: '![Local binary pattern histograms](img/00103.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The number of correctly classified samples is outputted after each run.
  prefs: []
  type: TYPE_NORMAL
- en: The problems with facial recognition in its current OpenCV 3 based implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The techniques discussed enable us to recognize a face and link it to a person
    in the dataset. However, there are still some problems with this system that should
    be addressed:'
  prefs: []
  type: TYPE_NORMAL
- en: When using the Eigenfaces system, it is a general rule that the more Eigenvectors
    you use, the better the system will become, and the higher the accuracy will be.
    Defining how many dimensions you need to get a decent recognition result is frustrating,
    since it depends on how the data is presented to the system. The more variation
    there is in the original data, the more challenging the task will be, and thus
    the more dimensions you will need. The experiments of Philipp Wagner have shown
    that, in the AT&T database, about 300 Eigenvectors should be enough.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can apply thresholding to both Eigenfaces and Fisherfaces. This is a must
    if you want to be certain of classification accuracy. If you do not apply this,
    then the system will basically return the best match. If a given person is not
    part of the dataset, then you want to avoid this, and that can be done by calling
    the interface with a threshold value!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that, with all face recognition systems, if you train them with
    data in one setup and test them with data containing completely different situations
    and setups, then the drop in accuracy will be huge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you build a recognition system based on 2D image information, then frauds
    will be able to hack it by simply printing a 2D image of the person and presenting
    it to the system. In order to avoid this, either include 3D knowledge or add extra
    biometrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More information on adding 3D information to avoid fraud attempts can be found
    in the following publications:'
  prefs: []
  type: TYPE_NORMAL
- en: Akarun Lale, B. Gokberk, and Albert Ali Salah, *3D face recognition for biometric
    applications*, Signal Processing Conference, 2005 13th European. IEEE, 2005.
  prefs: []
  type: TYPE_NORMAL
- en: 'Abate Andrea F., et al, *2D and 3D face recognition: A survey*, Pattern Recognition
    Letters 28.14 (2007): 1885-1906.'
  prefs: []
  type: TYPE_NORMAL
- en: However, this topic is too specific and complex for the scope of this chapter,
    and will thus not be discussed further.
  prefs: []
  type: TYPE_NORMAL
- en: Fingerprint identification, how is it done?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we discussed the use of the first biometric, which
    is the face of the person trying to log in to the system. However, since we mentioned
    that using a single biometric is risky, it is better to add secondary biometric
    checks to the system, like a fingerprint. There are several off-the-shelf fingerprint
    scanners that are quite cheap and return you a scanned image. However, you will
    still have to write your own registration software for these scanners, and this
    can be done with OpenCV. Examples of such fingerprint images can be found below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fingerprint identification, how is it done?](img/00104.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Examples of single individual thumbprints from different scanners
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset can be downloaded from the FVC2002 competition website released
    by the University of Bologna. The website ([http://bias.csr.unibo.it/fvc2002/databases.asp](http://bias.csr.unibo.it/fvc2002/databases.asp))
    contains four databases of fingerprints available for public download in the following
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: Four fingerprint capturing devices, DB1 - DB4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each device, the prints of 10 individuals are available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each person, eight different positions of prints were recorded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use this publicly available dataset to build our system. We will focus
    on the first capturing device, using up to four fingerprints from each individual
    for training the system and making an average descriptor of the fingerprint. Then,
    we will use the other four fingerprints to evaluate our system and make sure that
    the person is still recognized by our system.
  prefs: []
  type: TYPE_NORMAL
- en: You can apply the same approach to the data grabbed from the other devices if
    you want to investigate the difference between a system that captures binary images
    and one that captures grayscale images. However, we will provide techniques for
    doing the binarization yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the approach in OpenCV 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The complete fingerprint software for processing fingerprints obtained from
    a fingerprint scanner can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/fingerprint/fingerprint_process/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/fingerprint/fingerprint_process/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this subsection, we will describe how you can implement this approach in
    the OpenCV interface. We start by grabbing the image from the fingerprint system
    and applying binarization. This enables us to remove any noise from the image,
    as well as helping us to make the contrast better between the skin and the wrinkled
    surface of the finger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The Otsu thresholding will automatically choose the best generic threshold for
    the image to obtain a good contrast between foreground and background information.
    This is because the image contains a bimodal distribution (which means that we
    have an image with two peak histograms) of pixel values. For that image, we can
    take an approximate value in the middle of those peaks as the threshold value
    (for images that are not bimodal, binarization won't be accurate). Otsu allows
    us to avoid using a fixed threshold value, making the system more compatible with
    capturing devices. However, we do acknowledge that, if you only have one capturing
    device, then playing around with a fixed threshold value may result in a better
    image for that specific setup. The result of the thresholding can be seen below.
  prefs: []
  type: TYPE_NORMAL
- en: In order to make the thinning from the next skeletization step as effective
    as possible, we need to invert the binary image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the approach in OpenCV 3](img/00105.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of grayscale and binarized fingerprint images
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a binary image, we are ready to calculate our feature points and
    feature point descriptors. However, in order to improve the process a bit more,
    it is better to skeletize the image. This will create more unique and stronger
    interest points. The following piece of code can apply the skeletization on top
    of the binary image. The skeletization is based on the Zhang-Suen line-thinning
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Special thanks to `@bsdNoobz` of the OpenCV Q&A forum, who supplied this iteration
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The code above can then simply be applied to our previous steps by calling
    the thinning function on top of our previous binary-generated image. The code
    for this is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the approach in OpenCV 3](img/00106.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of binarized and thinned fingerprint images using skeletization techniques
  prefs: []
  type: TYPE_NORMAL
- en: When we get this skeleton image, the next step is to look for crossing points
    on the ridges of the fingerprint, called minutiae points. We can do this with
    a keypoint detector that looks for large changes in local contrast, like the Harris
    corner detector. Since the Harris corner detector is able to detect strong corners
    and edges, it is ideal for the fingerprint problem, where the most important minutiae
    are short edges and bifurcations—the positions where edges come together.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More information about minutiae points and Harris corner detection can be found
    in the following publications:'
  prefs: []
  type: TYPE_NORMAL
- en: Ross Arun A., Jidnya Shah, and Anil K. Jain, *Toward reconstructing fingerprints
    from minutiae points*, Defense and Security. International Society for Optics
    and Photonics, 2005.
  prefs: []
  type: TYPE_NORMAL
- en: Harris Chris and Mike Stephens, *A combined corner and edge detector*, Alvey
    vision conference, Vol. 15, 1988.
  prefs: []
  type: TYPE_NORMAL
- en: Calling the Harris Corner operation on a skeletonized and binarized image in
    OpenCV is quite straightforward. The Harris corners are stored as positions corresponding
    with their cornerness response value in the image. If we want to detect points
    with a certain cornerness, then we should simply threshold the image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a map with all the available corner responses rescaled to the range
    of [0 255] and stored as float values. We can now manually define a threshold
    which will generate a good number of keypoints for our application. Playing around
    with this parameter could improve performance in other cases. This can be done
    by using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Implementing the approach in OpenCV 3](img/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of thinned fingerprints and the Harris corner response, as well as
    the selected Harris corners
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a list of keypoints, we need to create some sort of formal
    descriptor of the local region around each keypoint to be able to uniquely identify
    it from other keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Chapter 3](part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69
    "Chapter 3. Recognizing Facial Expressions with Machine Learning"), *Recognizing
    Facial Expressions with Machine Learning*, discusses in more detail the wide range
    of keypoints out there. In this chapter, we will mainly focus on the process.
    Feel free to adapt the interface to other keypoint detectors and descriptors out
    there, for better or for worse performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have an application where the orientation of the thumb can differ
    (since it is not in a fixed position), we want a keypoint descriptor that is good
    at handling these slight differences. One of the most common descriptors for this
    is the SIFT descriptor, which stands for **scale invariant feature transform**.
    However, SIFT is not under a BSD license, which can pose problems when used in
    commercial software. A good alternative in OpenCV is the ORB descriptor. You can
    implement it in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This enables us to calculate only the descriptors using the ORB approach, since
    we already retrieved the location of the keypoints using the Harris corner approach.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can retrieve a descriptor for each detected keypoint of any
    given fingerprint. The descriptors matrix contains a row for each keypoint containing
    the representation.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the example in which we have just one reference image for each
    fingerprint. We then have a database containing a set of feature descriptors for
    the training persons in the database. We have a single new entry, consisting of
    multiple descriptors for the keypoints found at registration time. We now have
    to match these descriptors to the descriptors stored in the database, to see which
    one has the best match.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to do this is to perform brute-force matching using the hamming
    distance criteria between descriptors of different keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We now have all the matches stored as DMatch objects. This means that, for each
    matching couple, we will have the original keypoint, the matched keypoint, and
    a floating point score between both matches, representing the distance between
    the matched points.
  prefs: []
  type: TYPE_NORMAL
- en: This seems pretty straightforward. We take a look at the number of matches that
    have been returned by the matching process and weigh them by their Euclidean distance
    in order to add some certainty. We then look for the matching process that yielded
    the biggest score. This will be our best match, and the match we want to return
    as the selected one from the database.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to avoid an imposter getting assigned to the best matching score,
    you can add a manual threshold on top of the scoring to avoid matches and ignore
    those that are not good enough. However, it is possible that, if you increase
    the score too much, people with little change will be rejected from the system,
    if, for example, someone cuts their finger and thus changes their pattern drastically.
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the approach in OpenCV 3](img/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The fingerprint matching process visualized
  prefs: []
  type: TYPE_NORMAL
- en: Iris identification, how is it done?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last biometric that we will use is the output of an iris scan. Considering
    our setup, there might be several ways to grab iris data:'
  prefs: []
  type: TYPE_NORMAL
- en: We can separate the face and apply an eye detector using face detection, which
    can be done with a high-resolution camera. We can use the resulting regions to
    perform iris segmentation and classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use a specific eye camera, which grabs an eye image to be classified.
    This can be done either with RGB or NIR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since the first approach is prone to a lot of problems, such as the resulting
    eye image having a low resolution, a more common approach is to use a separate
    eye camera that grabs the eye. This is the method that we will use in this chapter.
    An example of a captured eye in both the RGB (visible colors) and NIR (near infra-red)
    spectrums is visualized below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Iris identification, how is it done?](img/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of both a RGB and a NIR iris-based image
  prefs: []
  type: TYPE_NORMAL
- en: 'Using NIR images helps us in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, color information is omitted, since a lot of conditions like external
    sources of light can influence color information when grabbing the iris image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, the pupil center becomes clearer and fully black, which allows us
    to use techniques that depend on this for segmenting the pupil center.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thirdly, the available structure is maintained, under different lighting conditions,
    due to the NIR spectrum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the outer border of the iris region is clearer, and thus more easily
    separable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use data from the CASIA eye dataset for the iris recognition, which
    can be found at [http://biometrics.idealtest.org/](http://biometrics.idealtest.org/).
    This dataset is publicly available for research and non-commercial purposes, and
    access can be requested through the site. A small part of it can be found in our
    software repository, where we have a right and a left eye from one individual,
    which we can now treat as two people since no two irises are identical. We have
    10 samples for each eye, of which we will use eight to train and two to test.
  prefs: []
  type: TYPE_NORMAL
- en: The approach that we will implement for iris recognition is based on the technique
    suggested by John Daugman. The technique is widely accepted and used in commercial
    systems, and has thus proven its quality.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The original paper written by John Daugman can be found at: [http://www.cl.cam.ac.uk/~jgd1000/irisrecog.pdf](http://www.cl.cam.ac.uk/~jgd1000/irisrecog.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the approach in OpenCV 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in getting the iris information is segmenting out the actual
    eye region, containing both the iris and the pupil. We apply a series of operations
    on top of our data to achieve the desired result. This process is necessary to
    keep only the desired data and remove all the redundant eye data that is still
    around.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first try to get the pupil. The pupil is the darkest area in NIR images,
    and this information can be used to our advantage. The following steps will lead
    us to the pupil area in an eye image:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to apply segmentation to the darkest regions. We can use the
    `inRange()` image, since the values in which the pupil lie are specific to the
    capturing system. However, due to the fact that they all use NIR, the end result
    will be identical for each separate system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we apply contour detection to get the outer border of the pupil. We make
    sure that we get the biggest contour from just the outer contours so that we only
    keep one region.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to improve performance, you can also look for the bright spots of
    the IR LED first, remove them from the region, and then run the contour detection.
    This will improve robustness when IR LED spots are close to the pupil border.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the complete process of a single iris can be found at: [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_processing/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_processing/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This behavior can be achieved by using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![Implementing the approach in OpenCV 3](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![Implementing the approach in OpenCV 3](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of the retrieved Hough Circle result, which gives us the outer border
    of the iris region, for both the left and right eyes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have succeeded in finding the outer contour, it is pretty straightforward
    to mask the iris region from the original input, as shown in the figure below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the approach in OpenCV 3](img/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of the masked iris image
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have our region of interest, meaning only the iris region, as shown
    in the above figure. We acknowledge that there could still be some partial whiskers
    inside the region, but for now we will simply ignore them. Now, we want to encode
    this iris image into a feature vector for comparison. There are two steps still
    to take to reach that level:'
  prefs: []
  type: TYPE_NORMAL
- en: Unwrapping of the iris pattern from a polar coordinate system to a Cartesian
    coordinate system for further processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying encoding to the iris image and matching it against a database of known
    representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We start by providing a code snippet that will unwrap the desired iris region
    from the retrieved final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following conversion, which gives us the radial unwrapping
    of the iris region, as shown in the figure below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the approach in OpenCV 3](img/00113.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of the radially unwrapped iris image for both the left and right
    eyes
  prefs: []
  type: TYPE_NORMAL
- en: This radial unwrapping is done for all the eight training images that we have
    for each eye and for the two testing images that we also have for each eye. The
    Daugman approach applies phase quadrant modulation to encode the iris pattern.
    However, this is not yet implemented in OpenCV and is too complex for this chapter.
    Therefore, we decided to look for an available OpenCV implementation that could
    be used to match the irises. A good approach is to use the local binary pattern
    histogram comparison, since we are looking for something that can identify local
    textures, and this was also used for face recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Software for unwrapping a complete set of iris images can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_processing_batch/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_processing_batch/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Software for creating the matching interface can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_recognition/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_recognition/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, encoding works as follows in OpenCV 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We count the testing results again, which yields the result shown in the figure
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the approach in OpenCV 3](img/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Encoded iris image and the corresponding iris code visualized.
  prefs: []
  type: TYPE_NORMAL
- en: Combining the techniques to create an efficient people-registration system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous sections each discussed a specific biometric property. Now, let''s
    combine all this information to create an efficient identification system. The
    approach that we will implement follows the structure described in the figure
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Combining the techniques to create an efficient people-registration system](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: People authentication pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown above, the first step is to use a camera interface to check if there
    actually is a person in front of the camera. This is done by performing face detection
    on the input image. We also test to see if the other biometric systems are active.
    This leaves us two checks that need to be performed:'
  prefs: []
  type: TYPE_NORMAL
- en: Check if the iris scanner is in use. This, of course, depends on the system
    used. If it depends on the eye retrieved from the face detection, this check should
    be ignored. If the eye is retrieved using an actual eye scanner, then there should
    at least be an eye detected to give a positive signal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check if the fingerprint scanner is active. Do we actually have a finger available
    for taking a fingerprint picture? This is checked by applying background subtraction
    to the empty scene. If a finger is in place, then there should be a response to
    the background-foreground subtraction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, we are aware that some of these systems use pressure-based detection
    to find a hand or finger. In such cases, you do not have to perform this check
    yourself, but let the system decide whether to proceed or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have all the systems, we can start the individual recognition systems
    described in previous sections. They will all output the identity of a known person
    from the common database that was constructed for this purpose. Then, all these
    outcomes are given to the smart majority voting. This system checks for several
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: It checks if the biometric system checks actually succeeded, by returning their
    match from the database. If not, a person is not granted access and the system
    asks to reconfirm the failing biometrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the system has to measure biometrics more than three times in a row, the
    system jams and doesn't work until the owner of the system unlocks it. This is
    to avoid a bug in the current interface that exploits the system and tries to
    get in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the biometric checks work, a smart majority voting is applied to the results.
    This means that if two biometrics identify person A but one biometric identifies
    person B, then the output result will still be person A. If that person is marked
    as the owner, then the system will allow access.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the individual software provided with the separate subtopics, it should
    be quite straightforward to combine them into a single interface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the system still fails (this is a case study, not a 100% failproof system),
    there are several things that can be done to achieve the desired results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should try to improve the detection and matching quality of each separate
    biometric. This can be done by supplying better training data, experimenting with
    different feature extraction methods or different feature comparison methods,
    as discussed in the introduction to the chapter. The variety of combinations is
    endless, so go ahead and give it a try.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You should try to give each biometric a certainty score on its output. Since
    we have multiple systems voting for the identity of a person, we could take into
    account their certainty on single classifications. For example, when running a
    database, matching the distance to the best match can be wrapped to a scale range
    of [0 100] to give a certainty percentage. We can then multiply the vote of each
    biometric by its weight and do a smart-weighted majority voting.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned that an authentication system can be more than
    a simple face recognition interface by using multiple biometric properties of
    the person trying to authenticate. We showed you how to perform iris and fingerprint
    recognition using the OpenCV library to make a multi-biometric authentication
    system. One can add even more biometrics to the system, since the possibilities
    are endless.
  prefs: []
  type: TYPE_NORMAL
- en: The focus of the chapter was to get people interested in the power of biometrics
    and the endless possibilities of the OpenCV library. If you feel inspired by this,
    do experiment further and share your thoughts with the community.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'I would like to thank the users of the OpenCV Q&A discussion forum who helped
    me to push the limits when I hit brick walls. I would explicitly like to thank
    the following users for their directions: Berak, Guanta, Theodore, and GilLevi.'
  prefs: []
  type: TYPE_NORMAL
