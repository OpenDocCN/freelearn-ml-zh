["```py\n    > install.packages(\"C50\")\n    > library(C50)\n    > data(churn)\n\n    ```", "```py\n    > str(churnTrain)\n\n    ```", "```py\n    > churnTrain = churnTrain[,! names(churnTrain) %in% c(\"state\", \"area_code\", \"account_length\") ]\n\n    ```", "```py\n    > set.seed(2)\n    > ind = sample(2, nrow(churnTrain), replace = TRUE, prob=c(0.7, 0.3))\n    > trainset = churnTrain[ind == 1,]\n    > testset = churnTrain[ind == 2,]\n\n    ```", "```py\n    > dim(trainset)\n    [1] 2315   17\n    > dim(testset)\n    [1] 1018   17\n\n    ```", "```py\n> split.data = function(data, p = 0.7, s = 666){\n+   set.seed(s)\n+   index = sample(1:dim(data)[1])\n+   train = data[index[1:floor(dim(data)[1] * p)], ]\n+   test = data[index[((ceiling(dim(data)[1] * p)) + 1):dim(data)[1]], ]\n+   return(list(train = train, test = test))\n+ } \n\n```", "```py\n    > library(rpart)\n\n    ```", "```py\n    > churn.rp = rpart(churn ~ ., data=trainset)\n\n    ```", "```py\n    > churn.rp \n\n    ```", "```py\n    > printcp(churn.rp)\n\n    Classification tree:\n    rpart(formula = churn ~ ., data = trainset)\n\n    Variables actually used in tree construction:\n    [1] international_plan            number_customer_service_calls\n    [3] total_day_minutes             total_eve_minutes \n    [5] total_intl_calls              total_intl_minutes \n    [7] voice_mail_plan \n\n    Root node error: 342/2315 = 0.14773\n\n    n= 2315 \n\n     CP nsplit rel error  xerror     xstd\n    1 0.076023      0   1.00000 1.00000 0.049920\n    2 0.074561      2   0.84795 0.99708 0.049860\n    3 0.055556      4   0.69883 0.76023 0.044421\n    4 0.026316      7   0.49415 0.52632 0.037673\n    5 0.023392      8   0.46784 0.52047 0.037481\n    6 0.020468     10   0.42105 0.50877 0.037092\n    7 0.017544     11   0.40058 0.47076 0.035788\n    8 0.010000     12   0.38304 0.47661 0.035993\n\n    ```", "```py\n    > plotcp(churn.rp)\n\n    ```", "```py\n    > summary(churn.rp)\n\n    ```", "```py\n    > ?rpart\n    > ?printcp\n    > ?summary.rpart\n\n    ```", "```py\n    > plot(churn.rp, margin= 0.1)\n    > text(churn.rp, all=TRUE, use.n = TRUE)\n\n    ```", "```py\n    > plot(churn.rp, uniform=TRUE, branch=0.6, margin=0.1)\n    > text(churn.rp, all=TRUE, use.n = TRUE)\n\n    ```", "```py\n    > predictions = predict(churn.rp, testset, type=\"class\")\n\n    ```", "```py\n    > table(testset$churn, predictions)\n     predictions\n     yes  no\n     yes 100  41\n     no   18 859\n\n    ```", "```py\n    > library(caret)\n    > confusionMatrix(table(predictions, testset$churn))\n    Confusion Matrix and Statistics\n\n    predictions yes  no\n     yes 100  18\n     no   41 859\n\n     Accuracy : 0.942 \n     95% CI : (0.9259, 0.9556)\n     No Information Rate : 0.8615 \n     P-Value [Acc > NIR] : < 2.2e-16 \n\n     Kappa : 0.7393 \n     Mcnemar's Test P-Value : 0.004181 \n\n     Sensitivity : 0.70922 \n     Specificity : 0.97948 \n     Pos Pred Value : 0.84746 \n     Neg Pred Value : 0.95444 \n     Prevalence : 0.13851 \n     Detection Rate : 0.09823 \n     Detection Prevalence : 0.11591 \n     Balanced Accuracy : 0.84435 \n\n     'Positive' Class : yes \n\n    ```", "```py\n    > min(churn.rp$cptable[,\"xerror\"])\n    [1] 0.4707602\n\n    ```", "```py\n    > which.min(churn.rp$cptable[,\"xerror\"])\n    7 \n\n    ```", "```py\n    > churn.cp = churn.rp$cptable[7,\"CP\"]\n    > churn.cp\n    [1] 0.01754386\n\n    ```", "```py\n    > prune.tree = prune(churn.rp, cp= churn.cp)\n\n    ```", "```py\n    > plot(prune.tree, margin= 0.1)\n    > text(prune.tree, all=TRUE , use.n=TRUE)\n\n    ```", "```py\n    > predictions = predict(prune.tree, testset, type=\"class\")\n    > table(testset$churn, predictions)\n     predictions\n     yes  no\n     yes  95  46\n     no   14 863\n\n    ```", "```py\n    > confusionMatrix(table(predictions, testset$churn))\n    Confusion Matrix and Statistics\n\n    predictions yes  no\n     yes  95  14\n     no   46 863\n\n     Accuracy : 0.9411 \n     95% CI : (0.9248, 0.9547)\n     No Information Rate : 0.8615 \n     P-Value [Acc > NIR] : 2.786e-16 \n\n     Kappa : 0.727 \n     Mcnemar's Test P-Value : 6.279e-05 \n\n     Sensitivity : 0.67376 \n     Specificity : 0.98404 \n     Pos Pred Value : 0.87156 \n     Neg Pred Value : 0.94939 \n     Prevalence : 0.13851 \n     Detection Rate : 0.09332 \n     Detection Prevalence : 0.10707 \n     Balanced Accuracy : 0.82890 \n\n     'Positive' Class : yes \n\n    ```", "```py\n    > library(party)\n    > ctree.model = ctree(churn ~ . , data = trainset)\n\n    ```", "```py\n    > ctree.model\n\n    ```", "```py\n     > help(\"BinaryTree-class\")\n\n    ```", "```py\n    > plot(ctree.model)\n\n    ```", "```py\n    > daycharge.model = ctree(churn ~ total_day_charge, data = trainset)\n    > plot(daycharge.model)\n\n    ```", "```py\n    > ?plot.BinaryTree\n\n    ```", "```py\n    > ctree.predict = predict(ctree.model ,testset)\n    > table(ctree.predict, testset$churn)\n\n    ctree.predict yes  no\n     yes  99  15\n     no   42 862\n\n    ```", "```py\n    > confusionMatrix(table(ctree.predict, testset$churn))\n    Confusion Matrix and Statistics\n\n    ctree.predict yes  no\n     yes  99  15\n     no   42 862\n\n     Accuracy : 0.944 \n     95% CI : (0.9281, 0.9573)\n     No Information Rate : 0.8615 \n     P-Value [Acc > NIR] : < 2.2e-16 \n\n     Kappa : 0.7449 \n     Mcnemar's Test P-Value : 0.0005736 \n\n     Sensitivity : 0.70213 \n     Specificity : 0.98290 \n     Pos Pred Value : 0.86842 \n     Neg Pred Value : 0.95354 \n     Prevalence : 0.13851 \n     Detection Rate : 0.09725 \n     Detection Prevalence : 0.11198 \n     Balanced Accuracy : 0.84251 \n\n     'Positive' Class : yes \n\n    ```", "```py\n    > tr = treeresponse(ctree.model, newdata = testset[1:5,])\n    > tr\n    [[1]]\n    [1] 0.03497409 0.96502591\n\n    [[2]]\n    [1] 0.02586207 0.97413793\n\n    [[3]]\n    [1] 0.02586207 0.97413793\n\n    [[4]]\n    [1] 0.02586207 0.97413793\n\n    [[5]]\n    [1] 0.03497409 0.96502591\n\n    ```", "```py\n    > install.packages(\"class\")\n    > library(class)\n\n    ```", "```py\n    > levels(trainset$international_plan) = list(\"0\"=\"no\", \"1\"=\"yes\")\n    > levels(trainset$voice_mail_plan) = list(\"0\"=\"no\", \"1\"=\"yes\")\n    > levels(testset$international_plan) = list(\"0\"=\"no\", \"1\"=\"yes\")\n    > levels(testset$voice_mail_plan) = list(\"0\"=\"no\", \"1\"=\"yes\")\n\n    ```", "```py\n    > churn.knn  = knn(trainset[,! names(trainset) %in% c(\"churn\")], testset[,! names(testset) %in% c(\"churn\")], trainset$churn, k=3)\n\n    ```", "```py\n    > summary(churn.knn)\n    yes  no\n     77 941\n\n    ```", "```py\n    > table(testset$churn, churn.knn)\n     churn.knn\n     yes  no\n     yes  44  97\n     no   33 844\n\n    ```", "```py\n    > confusionMatrix(table(testset$churn, churn.knn))\n    Confusion Matrix and Statistics\n\n     churn.knn\n     yes  no\n     yes  44  97\n     no   33 844\n\n     Accuracy : 0.8723 \n     95% CI : (0.8502, 0.8922)\n     No Information Rate : 0.9244 \n     P-Value [Acc > NIR] : 1 \n\n     Kappa : 0.339 \n     Mcnemar's Test P-Value : 3.286e-08 \n\n     Sensitivity : 0.57143 \n     Specificity : 0.89692 \n     Pos Pred Value : 0.31206 \n     Neg Pred Value : 0.96237 \n     Prevalence : 0.07564 \n     Detection Rate : 0.04322 \n     Detection Prevalence : 0.13851 \n     Balanced Accuracy : 0.73417 \n\n     'Positive' Class : yes \n\n    ```", "```py\n    > fit = glm(churn ~ ., data = trainset, family=binomial)\n\n    ```", "```py\n    > summary(fit)\n\n    Call:\n    glm(formula = churn ~ ., family = binomial, data = trainset)\n\n    Deviance Residuals: \n     Min       1Q   Median       3Q      Max \n    -3.1519   0.1983   0.3460   0.5186   2.1284 \n\n    Coefficients:\n     Estimate Std. Error z value Pr(>|z|)\n    (Intercept)                    8.3462866  0.8364914   9.978  < 2e-16\n    international_planyes         -2.0534243  0.1726694 -11.892  < 2e-16\n    voice_mail_planyes             1.3445887  0.6618905   2.031 0.042211\n    number_vmail_messages         -0.0155101  0.0209220  -0.741 0.458496\n    total_day_minutes              0.2398946  3.9168466   0.061 0.951163\n    total_day_calls               -0.0014003  0.0032769  -0.427 0.669141\n    total_day_charge              -1.4855284 23.0402950  -0.064 0.948592\n    total_eve_minutes              0.3600678  1.9349825   0.186 0.852379\n    total_eve_calls               -0.0028484  0.0033061  -0.862 0.388928\n    total_eve_charge              -4.3204432 22.7644698  -0.190 0.849475\n    total_night_minutes            0.4431210  1.0478105   0.423 0.672367\n    total_night_calls              0.0003978  0.0033188   0.120 0.904588\n    total_night_charge            -9.9162795 23.2836376  -0.426 0.670188\n    total_intl_minutes             0.4587114  6.3524560   0.072 0.942435\n    total_intl_calls               0.1065264  0.0304318   3.500 0.000464\n    total_intl_charge             -2.0803428 23.5262100  -0.088 0.929538\n    number_customer_service_calls -0.5109077  0.0476289 -10.727  < 2e-16\n\n    (Intercept)                   ***\n    international_planyes         ***\n    voice_mail_planyes            * \n    number_vmail_messages \n    total_day_minutes \n    total_day_calls \n    total_day_charge \n    total_eve_minutes \n    total_eve_calls \n    total_eve_charge \n    total_night_minutes \n    total_night_calls \n    total_night_charge \n    total_intl_minutes \n    total_intl_calls              ***\n    total_intl_charge \n    number_customer_service_calls ***\n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    (Dispersion parameter for binomial family taken to be 1)\n\n     Null deviance: 1938.8  on 2314  degrees of freedom\n    Residual deviance: 1515.3  on 2298  degrees of freedom\n    AIC: 1549.3\n\n    Number of Fisher Scoring iterations: 6\n\n    ```", "```py\n    > fit = glm(churn ~ international_plan + voice_mail_plan+total_intl_calls+number_customer_service_calls, data = trainset, family=binomial)\n    > summary(fit)\n\n    Call:\n    glm(formula = churn ~ international_plan + voice_mail_plan + \n     total_intl_calls + number_customer_service_calls, family = binomial, \n     data = trainset)\n\n    Deviance Residuals: \n     Min       1Q   Median       3Q      Max \n    -2.7308   0.3103   0.4196   0.5381   1.6716 \n\n    Coefficients:\n     Estimate Std. Error z value\n    (Intercept)                    2.32304    0.16770  13.852\n    international_planyes         -2.00346    0.16096 -12.447\n    voice_mail_planyes             0.79228    0.16380   4.837\n    total_intl_calls               0.08414    0.02862   2.939\n    number_customer_service_calls -0.44227    0.04451  -9.937\n     Pr(>|z|) \n    (Intercept)                    < 2e-16 ***\n    international_planyes          < 2e-16 ***\n    voice_mail_planyes            1.32e-06 ***\n    total_intl_calls               0.00329 ** \n    number_customer_service_calls  < 2e-16 ***\n    ---\n    Signif. codes: \n    0  es:    des:  **rvice_calls  < '.  es:    de\n\n    (Dispersion parameter for binomial family taken to be 1)\n\n     Null deviance: 1938.8  on 2314  degrees of freedom\n    Residual deviance: 1669.4  on 2310  degrees of freedom\n    AIC: 1679.4\n\n    Number of Fisher Scoring iterations: 5\n\n    ```", "```py\n    > pred = predict(fit,testset, type=\"response\")\n    > Class = pred >.5\n\n    ```", "```py\n    > summary(Class)\n     Mode   FALSE    TRUE    NA's \n    logical      29     989       0 \n\n    ```", "```py\n    > tb = table(testset$churn,Class)\n    > tbâ€©     Class\n     FALSE TRUE\n     yes    18  123\n     no     11  866\n\n    ```", "```py\n    > churn.mod = ifelse(testset$churn == \"yes\", 1, 0)\n    > pred_class = churn.mod\n    > pred_class[pred<=.5] = 1- pred_class[pred<=.5]\n    > ctb = table(churn.mod, pred_class)\n    > ctb\n     pred_class\n    churn.mod   0   1\n     0 866  11\n     1  18 123\n    > confusionMatrix(ctb)\n    Confusion Matrix and Statistics\n\n     pred_class\n    churn.mod   0   1\n     0 866  11\n     1  18 123\n\n     Accuracy : 0.9715 \n     95% CI : (0.9593, 0.9808)\n     No Information Rate : 0.8684 \n     P-Value [Acc > NIR] : <2e-16 \n\n     Kappa : 0.8781 \n     Mcnemar's Test P-Value : 0.2652 \n\n     Sensitivity : 0.9796 \n     Specificity : 0.9179 \n     Pos Pred Value : 0.9875 \n     Neg Pred Value : 0.8723 \n     Prevalence : 0.8684 \n     Detection Rate : 0.8507 \n     Detection Prevalence : 0.8615 \n     Balanced Accuracy : 0.9488 \n\n     'Positive' Class : 0 \n\n    ```", "```py\n    > library(e1071) \n    > classifier=naiveBayes(trainset[, !names(trainset) %in% c(\"churn\")], trainset$churn)\n\n    ```", "```py\n    > classifier\n\n    Naive Bayes Classifier for Discrete Predictors\n\n    Call:\n    naiveBayes.default(x = trainset[, !names(trainset) %in% c(\"churn\")], \n     y = trainset$churn)\n\n    A-priori probabilities:\n    trainset$churn\n     yes        no \n    0.1477322 0.8522678 \n\n    Conditional probabilities:\n     international_plan\n    trainset$churn         no        yes\n     yes 0.70467836 0.29532164\n     no  0.93512418 0.06487582\n\n    ```", "```py\n    > bayes.table = table(predict(classifier, testset[, !names(testset) %in% c(\"churn\")]), testset$churn)\n    > bayes.table\n\n     yes  no\n     yes  68  45\n     no   73 832\n\n    ```", "```py\n    > confusionMatrix(bayes.table)\n    Confusion Matrix and Statistics\n\n     yes  no\n     yes  68  45\n     no   73 832\n\n     Accuracy : 0.8841 \n     95% CI : (0.8628, 0.9031)\n     No Information Rate : 0.8615 \n     P-Value [Acc > NIR] : 0.01880 \n\n     Kappa : 0.4701 \n     Mcnemar's Test P-Value : 0.01294 \n\n     Sensitivity : 0.4823 \n     Specificity : 0.9487 \n     Pos Pred Value : 0.6018 \n     Neg Pred Value : 0.9193 \n     Prevalence : 0.1385 \n     Detection Rate : 0.0668 \n     Detection Prevalence : 0.1110 \n     Balanced Accuracy : 0.7155 \n\n     'Positive' Class : yes \n\n    ```"]