- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conformal Prediction for Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover conformal prediction for regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: Regression is a cornerstone of machine learning, enabling us to predict continuous
    outcomes from given data. However, as with many predictive tasks, the predictions
    are never free from uncertainty. Traditional regression techniques give us a point
    estimate but fail to measure the uncertainty. This is where the power of conformal
    prediction comes into play, extending our regression models to produce well-calibrated
    prediction intervals.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter delves deep into conformal prediction tailored specifically for
    regression problems. By understanding and appreciating the importance of quantifying
    uncertainty, we will explore how conformal prediction augments regression to provide
    not just a point prediction but an entire interval or even a distribution where
    the actual outcome will likely fall with pre-specified confidence. This is invaluable
    in many real-world scenarios, especially when making decisions based on predictions
    where stakes are high and being “approximately right” isn’t good enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty quantification for regression problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various approaches to produce prediction intervals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conformal prediction for regression problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building prediction intervals and predictive distributions using conformal prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncertainty quantification for regression problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After completing this chapter, whenever you predict any continuous variable,
    you’ll be equipped to add a layer of robustness and reliability to your predictions.
    Understanding and quantifying this uncertainty is crucial for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model interpretability and trust**: Uncertainty quantification helps us understand
    the reliability of our model predictions. By providing a range of possible outcomes,
    we can build trust in our model’s predictions and interpret them more effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision-making**: In many practical applications of regression analysis,
    decision-makers must rely on something other than point estimates. They often
    need to know the range within which the actual value will likely fall with a certain
    probability. This range, or prediction interval, provides crucial information
    about the uncertainty of the prediction and aids in risk management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model improvement**: Uncertainty can highlight the areas where the model
    may benefit from additional data or feature engineering. High uncertainty may
    indicate that the model needs help capturing the underlying relationship, suggesting
    the need for model revision or additional data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier detection**: Uncertainty quantification can also help us identify
    outliers or anomalies in the data. Observations associated with high predictive
    uncertainty may be outliers or indicate a novel situation not captured during
    model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, uncertainty quantification forms an essential part of regression
    problems. It provides a more holistic picture of predictive performance, allows
    for better risk management, and improves model trust and interpretability. Conformal
    prediction for regression, a framework that we will discuss in this chapter, is
    the approach to efficiently quantifying uncertainty in regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the types and sources of uncertainty in regression modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Uncertainty in regression modeling can arise from several sources and manifests
    in different ways. Broadly, these uncertainties can be classified into two main
    types – **aleatoric** and **epistemic**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aleatoric uncertainty**: This type of uncertainty is often called “inherent,”
    “irreducible,” or “random” uncertainty. It arises due to the inherent variability
    in the data itself, which is usually beyond our control. This uncertainty would
    not be eliminated if we were to collect more data or improve our measurements.
    Aleatoric uncertainty reflects the randomness, variability, or heterogeneity in
    our sample population.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epistemic uncertainty**: Also known as “reducible” or “systematic” uncertainty,
    this type originates from the lack of knowledge about the system or process under
    study. It could be due to insufficient data, measurement errors, or incorrect
    assumptions about the underlying data distribution or model structure. Unlike
    aleatoric uncertainty, epistemic uncertainty can be reduced with more information
    or data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The primary sources of these uncertainties in regression modeling are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data uncertainty**: This includes measurement errors, missing values, and
    variability in the data. Data might be collected under different conditions or
    sources, adding more uncertainty to the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model uncertainty**: This comes from the model’s inability to precisely capture
    the proper relationship between predictors and the outcome. Every model makes
    certain assumptions (for example, linearity, independence, normality, and so on),
    and any violation of these assumptions introduces uncertainty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameter uncertainty**: Every regression model involves estimating parameters
    (for example, coefficients). There’s always some uncertainty about these estimates,
    which can contribute to the overall uncertainty in predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structural uncertainty**: This refers to uncertainty due to the choice of
    a specific model form. Different model structures or types (for example, linear
    regression, polynomial regression, and so on) might lead to different interpretations
    and predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Residual uncertainty**: This comes from the residuals or errors of the model.
    It represents the difference between the observed outcomes and the outcomes predicted
    by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of regression modeling, recognizing these types and sources of
    uncertainty can help interpret the model’s predictions more accurately. It can
    guide the process of model refinement and validation.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of prediction intervals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A prediction interval is an interval estimate associated with a regression prediction,
    indicating a range within which the actual outcome will likely fall with a certain
    probability. While a point prediction gives us a singular value as the most likely
    outcome, a prediction interval offers a range, providing a clearer picture of
    the uncertainty associated with that prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need prediction intervals?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantification of uncertainty**: The primary reason for employing prediction
    intervals is to quantify our predictions’ uncertainty. No matter how sophisticated
    the model is, every prediction comes with inherent variability. By using prediction
    intervals, we can communicate this variability effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risk management**: In various industries, particularly finance, healthcare,
    and engineering, understanding the range of potential outcomes is crucial for
    risk assessment and mitigation. A prediction interval helps decision-makers weigh
    their actions’ potential risks and benefits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model transparency**: Providing an interval instead of just a point estimate
    can enhance the transparency of the model. Stakeholders can gauge not only what
    the model predicts but also the confidence the model has in that prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Guided decision-making**: Decision-makers can act more decisively when they
    understand the worst-case and best-case scenarios. For example, knowing the lower
    and upper bounds of predicted sales in sales forecasting can help allocate resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the necessity for prediction intervals in various contexts paves
    the way for a deeper discussion of their nature, particularly in comparision with
    confidence intervals.
  prefs: []
  type: TYPE_NORMAL
- en: How is it different from a confidence interval?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This distinction is crucial. A confidence interval pertains to the uncertainty
    regarding a population parameter based on a sample statistic. For instance, we
    might use a confidence interval to estimate the mean value of a population based
    on a sample mean. On the other hand, a prediction interval is about predicting
    a single future observation and quantifying the uncertainty around that individual
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The components of a prediction interval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A prediction interval typically has the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lower bound**: The minimum value within the predicted range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Upper bound**: The maximum value within the expected range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coverage probability**: The probability of the actual outcome falling within
    the prediction interval. Commonly used probabilities are 90%, 95%, and 99%.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are various approaches to producing prediction intervals. Quantifying
    uncertainty in regression models is crucial for understanding the reliability
    of predictions. Here are some of the most used techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Confidence and prediction intervals**: These are fundamental techniques for
    quantifying uncertainty. Confidence intervals provide a range of values where
    we expect the true regression parameters to fall, given a certain confidence level.
    Prediction intervals, on the other hand, give a range for predicting a new observation,
    incorporating both the uncertainty in the estimate of the mean function and the
    randomness of the new observation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resampling methods**: Techniques such as bootstrapping and cross-validation
    can provide empirical estimates of model uncertainty. Bootstrapping, for example,
    involves repeatedly sampling from the dataset with replacement and recalculating
    the regression estimates to get an empirical distribution of the estimates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian methods**: Bayesian regression analysis provides a probabilistic
    framework for quantifying uncertainty. Instead of single-point estimates, Bayesian
    regression gives a posterior distribution for the model parameters, which can
    be used to construct prediction intervals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conformal prediction**: Conformal prediction is a more recent approach that
    measures the certainty of predictions made by machine learning algorithms. It
    builds prediction regions that attain valid coverage in finite samples without
    making assumptions about data distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantile regression**: Unlike standard regression techniques, which model
    the conditional mean of the response variable given specific values of predictor
    variables, quantile regression models the conditional median or other quantiles.
    It can provide a more comprehensive view of the possible outcomes and their associated
    probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monte Carlo methods**: Monte Carlo methods are a class of computational algorithms
    that use random sampling to obtain numerical results. In the context of uncertainty
    quantification, Monte Carlo methods can be used to propagate the uncertainties
    from the input variables to the response variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensitivity analysis**: Sensitivity analysis is a technique that’s used to
    determine how different values of an independent variable will impact a particular
    dependent variable under a given set of assumptions. This technique is used within
    specific boundaries that depend on one or more input variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the inherent value and significance of prediction intervals makes
    it vital to discern the methodologies and tools to help us generate them. While
    traditional statistical methods have their merits, the dynamic landscape of data-driven
    industries necessitates more adaptive and reliable techniques. Conformal prediction,
    with its roots grounded in algorithmic randomness and validity, offers an enticing
    approach. As we transition into the next section, we will explore how conformal
    prediction tailors itself to regression problems, ensuring that our prediction
    intervals are accurate and theoretically sound. Let’s dive in and unveil the intricacies
    of conformal prediction in the context of regression.
  prefs: []
  type: TYPE_NORMAL
- en: Conformal prediction for regression problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the preceding chapters, we investigated the numerous advantages that conformal
    prediction provides. These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Validity and calibration**: Conformal prediction maintains its validity and
    calibration, irrespective of the dataset’s size. This makes it a robust method
    for prediction across different dataset sizes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distribution-free nature**: One of the significant benefits of conformal
    prediction is its distribution-free nature. It makes no specific assumptions about
    the underlying data distribution, making it a flexible and versatile tool for
    many prediction problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compatibility with various predictors**: Conformal prediction can seamlessly
    integrate with any point predictor, irrespective of its nature. This property
    enhances its adaptability and widens its scope of application in diverse domains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-intrusiveness**: The conformal prediction framework is non-intrusive,
    implying that it does not interfere with or alter the original prediction model.
    Instead, it is an additional layer that quantifies uncertainty, providing a holistic
    perspective of the model’s predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In regression analysis, one paramount concern is the validity and calibration
    of various uncertainty quantification methods. These qualities are particularly
    essential in generating prediction intervals, where it’s expected that these intervals
    provide coverage that matches the specified confidence level, which can sometimes
    be challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the comprehensive study titled *Valid prediction intervals for regression
    problems*, by Nicolas Dewolf, Bernard De Baets, and Willem Waegeman ([https://arxiv.org/abs/2107.00363](https://arxiv.org/abs/2107.00363)),
    the authors explored four broad categories of methods designed for estimating
    prediction intervals in regression scenarios – Bayesian methods, ensemble methods,
    direct estimation methods, and conformal prediction methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayesian methods**: This category encompasses techniques that use Bayes’
    theorem to predict the posterior probability of the intervals. These methods can
    give robust prediction intervals by modeling the entire output distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble methods**: Ensemble methods such as random forest or bagging can
    generate prediction intervals by leveraging the variability among individual models
    in the ensemble.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Direct estimation methods**: These techniques involve the direct calculation
    of prediction intervals. They often necessitate specific assumptions about the
    underlying data or the error distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conformal prediction methods**: Conformal prediction stands out for its distribution-free
    nature and ability to provide valid prediction intervals across diverse scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors of this study underscore that the adoption of artificial intelligence
    systems by humans is significantly tied to the reliability these systems can offer.
    The reliability here refers not only to producing accurate point predictions but
    also to the system’s ability to gauge and communicate its uncertainty level accurately.
    Thus, these systems should be adept at highlighting their areas of knowledge and
    limitations, particularly the aspects they need clarification on.
  prefs: []
  type: TYPE_NORMAL
- en: Converting uncertainties or “what they do not know” becomes even more crucial
    in real-world applications, where predictions often drive significant decisions.
    Therefore, uncertainty quantification aids in making informed and risk-aware decisions,
    contributing to the broader acceptance and trust in artificial intelligence systems.
  prefs: []
  type: TYPE_NORMAL
- en: Calibration refers to the degree to which the actual coverage of a prediction
    interval matches its nominal coverage level. In other words, a prediction interval
    is well-calibrated if it contains the true value of the response variable with
    the expected frequency. Calibration is essential because it ensures that the prediction
    intervals are not too narrow or too broad and that they provide accurate information
    about the uncertainty associated with the predictions. In this study, the authors
    use conformal prediction as a general calibration procedure to ensure the prediction
    intervals are well-calibrated.
  prefs: []
  type: TYPE_NORMAL
- en: Methods other than conformal prediction can suffer from calibration issues because
    they may make assumptions about the distribution of the errors or the model parameters
    that do not hold in practice. For example, Bayesian methods might assume that
    the errors are normally distributed with a fixed variance, which may not be the
    case. Ensemble methods, on the other hand, may not consider the correlation between
    the predictions of the individual models in the ensemble. Outliers or other noise
    sources in the data may also affect direct estimation methods. In addition, some
    interval estimators can, at best, be asymptotically valid; since this is only
    guaranteed for infinitely large datasets, there is no guarantee that it will hold
    for real datasets of final size, especially for medium-sized and smaller datasets.
  prefs: []
  type: TYPE_NORMAL
- en: These issues can lead to prediction intervals that are poorly calibrated, meaning
    that they do not provide accurate information about the uncertainty associated
    with the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, conformal prediction is a non-parametric method that does not make
    any assumptions about the distribution of the errors or the model parameters.
    Instead, it uses the data to construct prediction intervals that are guaranteed
    to be well-calibrated, regardless of the underlying distribution of the errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table provides a summary of the characteristics of the four classes
    of methods for constructing prediction intervals in a regression setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **Marginal Validity** | **Scalability** | **Domain Knowledge**
    | **Validation Set** |'
  prefs: []
  type: TYPE_TB
- en: '| Bayesian methods | No | Only scalable with approximate inference | Yes |
    No |'
  prefs: []
  type: TYPE_TB
- en: '| Ensemble methods | No | Yes (when scalable models are used) | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Direct interval estimation | No | Yes | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Conformal prediction | Yes | Yes (for ICP) | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – Summary of the uncertainty quantification methods
  prefs: []
  type: TYPE_NORMAL
- en: For each class of methods, this table indicates whether the method has marginal
    validity (meaning that it does not require any assumptions about the distribution
    of the errors or the model parameters), whether it is scalable (meaning that it
    can be applied to large datasets), whether it requires domain knowledge (meaning
    that it requires knowledge of the specific problem domain), and whether it requires
    a validation set (meaning that it requires a separate dataset to evaluate the
    performance of the method).
  prefs: []
  type: TYPE_NORMAL
- en: 'This table shows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian methods do not have marginal validity, are only scalable with approximate
    inference, require domain knowledge, and do not require a validation set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble methods do not have marginal validity, are scalable, do not require
    domain knowledge, and do not require a validation set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Direct interval estimation methods do not have marginal validity, are scalable,
    do not require domain knowledge, and require a validation set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conformal prediction has marginal validity, is scalable, does not require domain
    knowledge, and requires a validation set (in the **inductive conformal prediction**
    (**ICP**) version)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Several types of prediction interval estimators for regression problems were
    reviewed and compared, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayesian methods**: Gaussian process and approximate GP'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble methods**: Dropout ensemble, deep ensembles, and mean-variance estimator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Direct interval estimation methods**: Neural network quantile regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conformal prediction**: Neural networks and random forest'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This comparison was based on two main properties – the *coverage degree* and
    the *average width of the* *prediction intervals*.
  prefs: []
  type: TYPE_NORMAL
- en: Before calibration, the performance of prediction interval estimators varied
    significantly across different benchmark datasets, with large fluctuations in
    performance from one dataset to another. This is due to the violation of certain
    assumptions that are inherent to some classes of methods. For example, some methods
    may perform well on datasets with normally distributed errors, but poorly on datasets
    with strongly skewed errors. Similarly, some methods may perform well on datasets
    with low levels of noise, but poorly on datasets with high levels of noise.
  prefs: []
  type: TYPE_NORMAL
- en: The paper also found that the performance of the different methods for constructing
    prediction intervals depends on several factors, such as the size of the dataset,
    the complexity of the model, and the degree of skewness in the data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the paper found that Bayesian methods tend to perform better on
    small datasets, while ensemble methods tend to perform well on large datasets.
    The paper also found that the mean-variance estimator, which is a type of ensemble
    method, can be sensitive to the normality assumption and may perform poorly on
    strongly skewed datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the paper found that direct interval estimation methods, such as neural
    network quantile regression, can be computationally expensive and may require
    many training samples to achieve good performance.
  prefs: []
  type: TYPE_NORMAL
- en: The conformal prediction framework was used for post-hoc calibration, and it
    was found that all methods attained the desired coverage after calibration, and
    in certain cases, the calibrated models even produced intervals with a smaller
    average width.
  prefs: []
  type: TYPE_NORMAL
- en: The authors illustrated how conformal prediction can be used as a general calibration
    procedure for methods that deliver poor results without a calibration step and
    show that it can improve the performance of these methods on a wide range of datasets,
    making conformal prediction a promising framework for constructing prediction
    intervals in a regression setting. In particular, the paper showed that conformal
    prediction can be used as a general calibration procedure for methods that deliver
    poor results without a calibration step. The paper also found that conformal prediction
    has several significant advantages over other methods for constructing prediction
    intervals. For example, conformal prediction has marginal validity, meaning that
    it does not require any assumptions about the distribution of the errors or the
    model parameters. Additionally, conformal prediction is scalable, meaning that
    it can be applied to large datasets. Finally, conformal prediction does not require
    domain knowledge, meaning that it does not require knowledge of the specific problem
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: Conformal prediction is a powerful framework for quantifying uncertainty in
    machine learning predictions. Its principles can be applied to various types of
    problems involving uncertainty quantification, including regression, which involves
    predicting a continuous output variable.
  prefs: []
  type: TYPE_NORMAL
- en: When applied to regression problems, conformal prediction provides prediction
    intervals rather than point predictions. These prediction intervals offer a range
    of plausible values for the target variable, with a specified confidence level.
    For example, a 95% prediction interval indicates that we can be 95% confident
    that the true value of the target variable falls within this range.
  prefs: []
  type: TYPE_NORMAL
- en: The most compelling feature of conformal prediction in regression scenarios
    is its validity, which refers to the fact that if we claim a 95% confidence level,
    the true value will indeed be in the prediction interval 95% of the time. Importantly,
    this validity is assured for unseen test data, any sample size (not just for large
    or infinite samples), and any underlying point regressor model.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, conformal prediction for regression is non-parametric, which means
    it does not make any specific assumptions about the underlying data distribution.
    This makes it broadly applicable across different types of regression problems
    and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: To implement conformal prediction for regression, we need a nonconformity measure,
    which quantifies how much each observation deviates from the norm. Common choices
    for regression include the absolute residuals. Once we have the nonconformity
    scores, we can generate prediction intervals based on the ordered nonconformity
    scores of the calibration set.
  prefs: []
  type: TYPE_NORMAL
- en: Different variants of conformal prediction can be used in regression, such as
    transductive (full) conformal prediction, ICP, which is more computationally efficient
    than classical TCP, jackknife+, and cross-conformal methods, which offer improved
    precision and robustness.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, conformal prediction provides a flexible, robust, and reliable approach
    to uncertainty quantification in regression problems, offering valid and well-calibrated
    prediction intervals.
  prefs: []
  type: TYPE_NORMAL
- en: Building prediction intervals and predictive distributions using conformal prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ICP is a computationally efficient variant of the original transductive conformal
    prediction framework. Like all other models from the conformal prediction family,
    ICP is model-agnostic in terms of the underlying point prediction model and data
    distribution and comes with automatic validity guarantees for final samples of
    any size.
  prefs: []
  type: TYPE_NORMAL
- en: The key advantage of ICP compared to the original variant of conformal prediction
    (transductive conformal prediction) is that ICP requires training the underlying
    regression model only once, leading to efficient computations during the calibration
    and prediction phases. ICP is highly computationally efficient as the conformal
    layer requires very little additional computation overhead compared to training
    the underlying model.
  prefs: []
  type: TYPE_NORMAL
- en: The ICP process involves splitting the dataset into a proper training set and
    a calibration set. The training set is used to create the initial point prediction
    model, while the calibration set is utilized to calculate conformity scores and
    produce the prediction intervals of the unseen points.
  prefs: []
  type: TYPE_NORMAL
- en: ICP automatically guarantees validity and ensures that prediction intervals
    include actual test points with a specified degree of selected coverage.
  prefs: []
  type: TYPE_NORMAL
- en: ICP’s efficiency and flexibility have made it a popular choice for uncertainty
    estimation in various applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the notebook at https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_07.ipynb
    to illustrate how to use ICP:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.1 – Predictions by \uFEFFRandomForestRegressor on the test set](img/B19925_07_01.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Predictions by RandomForestRegressor on the test set
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate prediction intervals by utilizing `RandomForestRegressor` as
    the core predictive model, and employ ICP to transform the point forecasts made
    by the base machine learning model into well-calibrated prediction intervals:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model training**: Initiate the process by training the chosen model using
    the proper training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Making predictions**: Utilize the trained model to generate predictions on
    the calibration and test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Nonconformity metric calculation**: For every observation within the calibration
    set, compute the nonconformity metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Quantile calculation**: Determine the quantile of the nonconformity metrics
    using the formula designed for final sample correction. The formula includes the
    final sample correction factor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Form prediction intervals**: Using the quantile you calculated in the previous
    step, establish prediction intervals for the test set. These intervals should
    be based on the point predictions made by the underlying model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can illustrate the prediction outcomes on a plot that encompasses point
    estimates, actual values, and prediction intervals yielded by ICP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Actual versus predicted values with a prediction interval](img/B19925_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Actual versus predicted values with a prediction interval
  prefs: []
  type: TYPE_NORMAL
- en: We can confirm that in ICP, the width of the prediction interval remains constant
    (as designed).
  prefs: []
  type: TYPE_NORMAL
- en: ICP for regression offers several advantages and a few drawbacks. Let’s consider
    these.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll cover the advantages first:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model-agnostic**: ICP can be applied to any existing regression model. This
    means it’s a flexible method that can be used to enhance a wide variety of regression
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency**: ICP only requires the underlying regression model to be trained
    once, leading to efficient computations during the calibration and prediction
    phases. This makes it computationally more efficient compared to the original
    transductive conformal prediction framework, which requires the model to be retrained
    for every new prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validity**: ICP comes with automatic validity guarantees. If the data distribution
    is exchangeable (that is, the order of the data points does not matter), then
    the prediction intervals produced by ICP will have the desired coverage probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distribution-free**: ICP does not make any assumptions about the distribution
    of the data. This means it can be applied even when the data does not follow any
    known statistical distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s cover the disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance dependency**: The effectiveness of ICP relies heavily on the
    performance of the underlying regression model. If the underlying model does not
    fit the data well, the prediction intervals produced by ICP may be too wide.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assumption of exchangeability**: ICP’s validity guarantees depend on the
    assumption of exchangeability, which may not hold in many real-world scenarios
    (for example, when dealing with time series data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of adaptivity**: Unlike other conformal prediction methods, such as
    **conformalized quantile regression** (**CQR**) and jackknife+ methods, which
    we’ll cover later in this chapter, ICP isn’t inherently adaptive. It does not
    dynamically adjust to the data’s complexity or structure. For example, it won’t
    naturally produce narrower intervals in regions of the data where the model is
    more confident and wider intervals where the model is less confident.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the ensuing section, we’ll deep dive into one of the most popular conformal
    prediction models – CQR. This is a sophisticated technique that amalgamates conformal
    prediction’s robustness with the precision of quantile regression. This fusion
    facilitates the generation of reliable prediction intervals and ensures that these
    intervals are optimally tuned to encapsulate the true values with a high degree
    of certainty. By harnessing the strengths of both conformal prediction and quantile
    regression, CQR emerges as a formidable tool for constructing well-calibrated
    prediction intervals, thereby augmenting the interpretability and trustworthiness
    of predictive models. As we traverse this section, we will unravel the mechanics
    of CQR, elucidate its advantages, and illustrate its application in real-world
    predictive scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Mechanics of CQR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we observed that ICP generates prediction intervals
    of uniform width. Consequently, it doesn’t adjust adaptively to heteroscedastic
    data, where the variability of the response variable isn’t constant across different
    regions of the data.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, not only it is crucial to ensure valid coverage in final samples
    but it is also beneficial to generate the most concise prediction intervals for
    each point within the input space. This helps maintain the informativeness of
    these intervals. When dealing with heteroscedastic data, the model should be capable
    of adjusting the length of prediction intervals to match the local variability
    associated with each point in the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: CQR (developed by Yaniv Romano, Evan Patterson, and Emmanuel Candes and published
    in the paper *Conformalized Quantile Regression* ([https://arxiv.org/abs/1905.03222](https://arxiv.org/abs/1905.03222)))
    is one of the most popular and widely adopted conformal prediction models. It
    was specifically designed to address the need for adaptivity by employing quantile
    regression as the underlying regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Roger Koenker developed quantile regression – a statistical method that estimates
    the conditional quantiles of a response variable given a set of features. Unlike
    classical regression, which focuses on estimating the conditional mean, quantile
    regression provides a more complete picture of the relationship between the predictors
    and the response by estimating specified quantiles. Quantile regression provides
    adaptivity by allowing quantiles to adjust to the local variability of the data.
    This is particularly important when the data is heteroscedastic, meaning that
    the variance of the response variable changes across the range of the predictors.
    However, unlike models in the conformal prediction framework, quantile regression
    does not have automatic validity guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: By combining the concept of quantile regression with the conformal prediction
    framework, CQR inherits the distribution-free validity guarantees for finite samples
    from conformal prediction, as well as the statistical efficiency and adaptivity
    of quantile regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Combining conformal prediction with quantile regression to create CQR provides
    several advantages over existing methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Conformal prediction is a technique for constructing prediction intervals that
    attain valid coverage in finite samples, without making distributional assumptions.
    This means that the prediction intervals produced by CQR are guaranteed to contain
    the true response value with a certain probability, regardless of the underlying
    distribution of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantile regression provides a flexible and efficient way to estimate the conditional
    quantiles of the response variable, which allows CQR to adjust the length of the
    prediction intervals to the local variability of the data. This adaptivity is
    particularly important when the data is heteroscedastic, meaning that the variance
    of the response variable changes across the range of the predictors. By estimating
    the conditional quantiles at each query point in predictor space, CQR can construct
    prediction intervals that are shorter and more informative than those obtained
    from classical regression methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: According to the results published in the paper *Conformalized Quantile Regression*,
    CQR tends to produce shorter intervals than ICP and is adaptive. This is because
    CQR can adjust the length of the prediction intervals to the local variability
    of the data using quantile regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now describe the mechanics of CRQ, starting with classical quantile
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: Quantile regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quantile regression is a statistical method that estimates the conditional quantiles
    of a response variable given a set of predictor variables (features). Unlike classical
    regression, which focuses on estimating the conditional mean, quantile regression
    provides a more complete picture of the relationship between the predictors and
    the response by estimating the entire conditional distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, quantile regression minimizes a loss function that measures
    the difference between the observed response values and the predicted quantiles.
    The loss function that’s used in quantile regression is typically pinball loss,
    a piecewise linear function that places more weight on the residuals at the specified
    quantile level:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Visualization of the pinball loss function, where y=y_hat](img/B19925_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Visualization of the pinball loss function, where y=y_hat
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, quantile regression can be implemented using a variety of algorithms,
    including linear quantile regression, neural networks, quantile random forest,
    and gradient boosting methods.
  prefs: []
  type: TYPE_NORMAL
- en: One common strategy for estimating uncertainty with quantile regression involves
    calculating the lower (*q_lo*) and upper (*q_hi*) quantiles for each X value in
    the test dataset, and then outputting [*q_lo, q_hi*] as a prediction interval.
    While this method can sometimes perform well and adapt to heteroscedasticity,
    it doesn’t come with guarantees for achieving the desired coverage. Without such
    assurance for final samples, the outcomes of using such strategies could be catastrophic,
    particularly in critical applications such as healthcare, finance, and autonomous
    vehicles. This is confirmed by the results of the experiments in the paper, which
    show that prediction intervals produced by neural networks can significantly undercover
    actual values.
  prefs: []
  type: TYPE_NORMAL
- en: Several methods exist to provide asymptotic consistent results for quantile
    regression, including for related methods such as quantile random forests. However,
    none of these methods provide validity guarantees in final samples.
  prefs: []
  type: TYPE_NORMAL
- en: CQR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In previous chapters, we discussed how conformal prediction, in both its transductive
    and inductive forms, can offer validity guarantees in final samples. While both
    variants of conformal prediction can be applied to quantile regression, considering
    the prevalent use of ICP, we will focus solely on the application of ICP in the
    context of CQR.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure from [https://arxiv.org/abs/1905.03222](https://arxiv.org/abs/1905.03222)
    presents a comparison between the prediction intervals produced by the standard
    ICP model (known as split-conformal), its locally adaptive variant, and CQR:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Prediction intervals on simulated heteroscedastic data with
    outliers – (a) the standard split conformal method, (b) its locally adaptive variant,
    and (c) CQR. The interval length as a function of X is shown in (d). The target
    coverage rate is 90%](img/B19925_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Prediction intervals on simulated heteroscedastic data with outliers
    – (a) the standard split conformal method, (b) its locally adaptive variant, and
    (c) CQR. The interval length as a function of X is shown in (d). The target coverage
    rate is 90%
  prefs: []
  type: TYPE_NORMAL
- en: The heteroscedastic dataset, which contains outliers, is modeled by three different
    methods, all achieving a user-specified coverage of 90%. As we discussed when
    we talked about ICP, ICP (split-conformal) generates prediction intervals of a
    constant width, indicating its non-adaptive nature. The locally weighted variant
    of ICP shows partial adaptivity. CQR, however, is fully adaptive and notably produces
    prediction intervals with the shortest average length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s describe the steps involved in CQR:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the data into a proper training set and a calibration set. The proper
    training set is used to fit the quantile regression model, while the calibration
    set is used to construct the prediction intervals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the quantile regression model to the proper training set using any algorithm
    for quantile regression, such as random forest or deep neural networks. This step
    involves estimating the conditional quantiles of the response variable given the
    predictor variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute conformity scores that quantify the errors made by the prediction interval
    obtained from quantile regression [`q_lo`, `q_hi`] by computing nonconformity
    scores using E i : = max{ ˆ q  α lo(X i) − Y i, Y i −  ˆ q  α hi(X i)}. The nonconformity
    score can be interpreted as follows – if the actual label, y, falls below the
    lower bound of the interval, the nonconformity score corresponds to the distance
    from *y* to the lower bound. Conversely, if *y* exceeds the upper bound, the nonconformity
    score is the distance from *y* to the upper bound. However, if the actual label,
    *y*, is within the interval, the nonconformity score is considered negative and
    corresponds to the distance from *y* to the nearest bound.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the prediction intervals for each test point by using empirical quantiles
    of E i.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The last step of the formula for prediction intervals is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C(X n+1) = [ ˆ q  α lo(X n+1) − Q 1−α(E, ℐ 2),  ˆ q  α hi(X n+1) + Q 1−α(E,
    ℐ 2)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here, Q 1−α(E, ℐ 2) : = (1 − α)(1 + 1 / |ℐ 2|) is the empirical quantile of
    E i, ℐ 2 is the calibration set, and |ℐ 2| is just a mathematical notation for
    the number of elements in the calibration set.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The conformal prediction idea here is similar to what we did in ICP – we simply
    use the calibration set to compute some form of nonconformity metric and then
    use the quantiles of the nonconformity metric to produce uncertainty intervals
    around the point forecast from the trained point prediction model. The only difference
    is that instead of adjusting the point predictions produced by the regression
    model, we adjust quantiles produced by the underlying statistical, machine learning,
    or deep learning model. Otherwise, the ideas and the mechanism of quantile calculation
    are the same as in ICP, given the definition of the nonconformity metric that
    we have described.
  prefs: []
  type: TYPE_NORMAL
- en: The key result in the paper is that if the data exchangeable, then the prediction
    interval constructed by the split (inductive) CQR algorithm satisfies the property
    of validity – that is, ℙ{Y n+1 ∈ C(X n+1)} ≥ 1 − α.
  prefs: []
  type: TYPE_NORMAL
- en: This is like in all other models in conformal prediction and says that given
    the user-specified confidence level, 1 − α, the probability of having the actual
    value of *y* within the constructed prediction interval is guaranteed to exceed
    1 − α. So, if the specified user confidence is 90%, the actual points are guaranteed
    to lie within the prediction interval 90% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: The additional bonus point is that if the nonconformity scores, E i, are almost
    surely distinct, then the coverage is also bounded as ℙ{Y n+1 ∈ C(X n+1)} ≤ 1
    − α +  1 _ |ℐ 2| + 1, where |ℐ 2| is the size of the calibration dataset. By way
    of example, for 500 points in the calibration dataset, the coverage is guaranteed
    to be bound between 90% and ~90.2%.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, CQR tends to produce shorter intervals compared to ICP. This is
    because CQR adjusts the length of the prediction intervals to the local variability
    of the data using quantile regression, which is particularly important when the
    data is heteroscedastic.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the combination of conformal prediction and quantile regression in
    CQR provides a powerful and flexible framework for constructing prediction intervals
    that are both distribution-free and adaptive to heteroscedasticity.
  prefs: []
  type: TYPE_NORMAL
- en: Jackknife+
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now describe another widely used conformal prediction method for regression,
    known as jackknife+. The description of the jackknife+ technique aligns closely
    with the details outlined in the seminal paper *Predictive Inference with the
    Jackknife+*, where this method was first introduced.
  prefs: []
  type: TYPE_NORMAL
- en: 'We aim to fit a regression function to the training data, which consists of
    pairs of features (*X**i*, *Y**i*). Our goal is to predict the output, Yn+1, given
    a new feature vector, Xn+1=x, and generate a corresponding uncertainty interval
    for this test point. We require that the interval contains the true Yn+1 given
    the specified target coverage – that is, 1 − α : ℙ{Y n+1 ∈ C(X n+1)} ≥ 1 − α.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A naive solution could be to use the residuals after fitting the underlying
    regressor on the training data, |Y i −  ˆ μ (X i)|, and compute the quantile of
    the residuals to estimate the width of the prediction interval on the new test
    point, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ μ (X n+1) ± ( the (1 − α) quantile of |Y 1 −  ˆ μ (X 1)|, … , |Y n −  ˆ μ (X n)|)
  prefs: []
  type: TYPE_NORMAL
- en: However, in practice, such an approach will underestimate uncertainty due to
    overfitting as the residuals on the training set are typically smaller than residuals
    on unseen test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To circumvent overfitting, statisticians developed a robust technique known
    as the jackknife. This technique was originally designed to reduce bias and estimate
    variance. It functions by iteratively omitting one observation from the dataset
    and recalculating the model. This offers an empirical approach to evaluate the
    model’s stability and resilience to individual data points:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.5 – Illustration of the \uFEFFjackknife prediction method](img/B19925_07_05.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Illustration of the jackknife prediction method
  prefs: []
  type: TYPE_NORMAL
- en: Jackknife regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each time jackknife regression is performed, it fits the model to all data points,
    excluding the one specified by (Xi, Yi). This process allows jackknife regression
    to estimate the leave-one-out residual denoted by |Y i −  ˆ μ  −i(X i)|. By treating
    these leave-one-out residuals as nonconformity scores, we can estimate the 1 −
    α quantile and form prediction intervals similarly to ICP. Conceptually, this
    method is expected to achieve the desired coverage as it effectively tackles the
    issue of overfitting. This is because the residuals, |Y i −  ˆ μ  −i(X i)|, are
    computed in an out-of-sample manner, thus providing a more realistic evaluation
    of model performance.
  prefs: []
  type: TYPE_NORMAL
- en: However, the jackknife procedure does not have universal theoretical guarantees
    and can have poor coverage properties in certain cases, particularly when the
    data is highly skewed or has heavy tails. Although there are statistical results
    under asymptotic settings or assumptions of stability regarding the jackknife
    regression algorithm, it is clear that in situations where the jackknife estimator
    is unstable, the jackknife method can result in a loss of predictive coverage.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the jackknife method can even have zero coverage, meaning that
    the true value is not contained in the estimated prediction interval. Additionally,
    the jackknife method can be computationally intensive as it requires fitting the
    model multiple times on reduced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: These challenges prompted the creation of an improved variant known as jackknife+.
    This method not only aims to bolster the original jackknife method’s coverage
    properties and computational efficiency but also aligns with the conformal prediction
    family of methods. As a result, jackknife+ benefits from all the robust features
    of the conformal prediction framework. It guarantees validity even in final samples
    of any size, exhibits a distribution-free nature, and is versatile enough to be
    applied to any regression model.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between the jackknife and jackknife+ methods for constructing
    predictive intervals is that the jackknife+ method uses leave-one-out predictions
    at the test point to account for the variability in the fitted regression function,
    in addition to the quantiles of leave-one-out residuals used by the jackknife
    method. This modification allows the jackknife+ method to provide rigorous coverage
    guarantees, regardless of the distribution of the data points, for any algorithm
    that treats the training points symmetrically. In contrast, the original jackknife
    method offers no theoretical guarantees without stability assumptions and can
    sometimes have poor coverage properties. Leave-one-out predictions capture the
    uncertainty in the fitted model’s predictions at the target point, allowing the
    intervals to adapt based on model variability. In contrast, the original jackknife
    method offers no theoretical guarantees without stability assumptions and can
    sometimes have poor coverage properties.
  prefs: []
  type: TYPE_NORMAL
- en: The jackknife+ method is related to cross-conformal prediction proposed by Vovk
    in that both methods aim to construct predictive intervals that provide rigorous
    coverage guarantees, regardless of the distribution of the data points, for any
    algorithm that treats the training points symmetrically.
  prefs: []
  type: TYPE_NORMAL
- en: However, the jackknife+ method differs from cross-conformal prediction in that
    it uses leave-one-out predictions at the test point to account for the variability
    in the fitted regression function and the quantiles of leave-one-out residuals
    used by cross-conformal prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Jackknife+ regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To recap, ICP provides validity guarantees at a user-specified 1 − α confidence
    level and is highly computationally efficient as it does not necessitate retraining
    the base regression model. However, these advantages come at the expense of needing
    to divide the data into a separate calibration dataset. As this reduces the amount
    of data available for training the underlying regressor, it can lead to a less
    optimal fit for the regression model and subsequently wider prediction intervals,
    particularly if the original dataset is small. Conversely, if the calibration
    set is small, it could result in higher variability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jackknife+ is a modification of jackknife. Similar to jackknife, we fit the
    regressor *n* times for each of the *n* points in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ μ  −i = 𝒜((X 1, Y 1), … , (X i−1, Y i−1), (X i+1, Y i+1), … , (X n, Y n))
  prefs: []
  type: TYPE_NORMAL
- en: The primary distinction between the jackknife and jackknife+ methods lies in
    the latter’s utilization of leave-one-out predictions at the test point, in addition
    to the quantiles of leave-one-out residuals that the jackknife method employs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The prediction interval generated by the jackknife+ method leverages quantiles
    to construct prediction intervals. However, it not only examines the quantiles
    of leave-one-out prediction errors but it also considers the *n* predictions produced
    by the regression model for the point prediction of X n+1\. This approach effectively
    broadens the scope of the predictive model, taking into account both error estimations
    and individual predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ C  n,α jackknife +(X n+1) = [ ˆ q  n,α − { ˆ μ  −i(X n+1) − R i LOO},  ˆ q  n,α + { ˆ μ  −i(X n+1)
    + R i LOO}]
  prefs: []
  type: TYPE_NORMAL
- en: 'Compare this formula with the formula for jackknife:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ C  n,α jackknife (X n+1) = [ ˆ q  n,α − { ˆ μ (X n+1) − R i LOO},  ˆ q  n,α + { ˆ μ (X n+1)
    + R i LOO}]
  prefs: []
  type: TYPE_NORMAL
- en: The significant divergence between jackknife and jackknife+ lies in the manner
    in which point predictions are made for the X n+1 test point. While the jackknife
    model makes the prediction only once, the jackknife+ model makes *n* predictions,
    each time fitting the model to *n-1* data points while excluding one point at
    a time.
  prefs: []
  type: TYPE_NORMAL
- en: This structure effectively accommodates potential instability in the regression
    algorithm, an issue that previously hindered jackknife from fulfilling theoretical
    validity guarantees. In scenarios where the regression model is highly sensitive
    to the training data, the output can vary substantially. Thus, jackknife+ provides
    a more nuanced and flexible prediction model, catering to possible variability
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure presents a comparative illustration of prediction intervals
    produced by both the jackknife and jackknife+ models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Prediction intervals produced by jackknife and jackknife+](img/B19925_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Prediction intervals produced by jackknife and jackknife+
  prefs: []
  type: TYPE_NORMAL
- en: In scenarios where the regression algorithm is stable, both models perform quite
    similarly, delivering empirical coverage approximately equal to 1 − α. However,
    in situations where the regression model is unstable and sensitive to training
    data, to the extent that removing a single data point can significantly alter
    the predicted value at X n+1, the output from both models can diverge significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to the jackknife method, which lacks theoretical validity guarantees,
    the jackknife+ model will, even in the worst-case scenarios, assure coverage of
    at least 1-2\alpha. Furthermore, in most practical scenarios, barring instances
    involving instability, the jackknife+ model is expected to provide empirical coverage
    of 1-\alpha, making it a robust method for uncertainty prediction.
  prefs: []
  type: TYPE_NORMAL
- en: While the jackknife method provides a robust way of calculating nonconformity
    scores, it has a significant computational overhead as it requires retraining
    the model for each instance in the calibration set. This is where the jackknife+
    method comes in.
  prefs: []
  type: TYPE_NORMAL
- en: The jackknife+ method improves upon the jackknife method by allowing for the
    calculation of nonconformity scores without the need to retrain the model for
    each instance in the calibration set. This is achieved by adjusting the nonconformity
    score calculation to account for the influence of each example on the model’s
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: First, the jackknife+ method trains the model on the entire training dataset.
    Then, for each instance in the calibration set, the method calculates an adjusted
    prediction that approximates the prediction that would have been made if the instance
    had been left out when training the model. The nonconformity score for each instance
    is the absolute difference between its actual value and the adjusted prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The primary benefit of the Jackknife+ method is computational efficiency as
    it doesn’t require the model to be retrained for each instance in the calibration
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Conformal predictive distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Conformal predictive distribution** (**CPD**) is an innovative method that
    applies the principles of conformal prediction to generate predictive distributions.
    These distributions provide a comprehensive view of prediction uncertainty, offering
    not just interval estimates but a complete distribution over all potential outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of CPD was first introduced in the paper *Nonparametric predictive
    distributions based on Conformal Prediction* ([https://link.springer.com/article/10.1007/s10994-018-5755-8](https://link.springer.com/article/10.1007/s10994-018-5755-8)),
    by Vladimir Vovk, Jieli Shen, Valery Manokhin, and Min-ge Xie. In this paper,
    the authors applied conformal prediction to derive valid predictive distributions
    under a nonparametric assumption.
  prefs: []
  type: TYPE_NORMAL
- en: In a subsequent paper, *Conformal predictive distributions with kernels* ([https://arxiv.org/abs/1710.08894](https://arxiv.org/abs/1710.08894)),
    by Vladimir Vovk, Ilia Nouretdinov, Valery Manokhin, and Alex Gammerman, the authors
    reviewed the history of predictive distributions in statistics and discussed two
    key developments. The first was the integration of predictive distributions into
    machine learning, and the second was the combination of predictive distributions
    with kernel methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the paper *Cross-conformal predictive distributions* ([http://proceedings.mlr.press/v91/vovk18a.html](http://proceedings.mlr.press/v91/vovk18a.html)),
    by Vladimir Vovk, Ilia Nouretdinov, Valery Manokhin, and Alexander Gammerman,
    the authors extended CPD to any underlying model, whether it be statistical, machine
    learning, or deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Inductive (split) conformal predictive systems are computationally efficient
    versions of conformal predictive systems that output probability distributions
    for labels of test observations in regression problems. These systems provide
    additional information that can be useful in decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-conformal predictive systems are a novel application of conformal prediction
    that facilitates automatic decision-making. They are built on top of split conformal
    predictive systems, and while they can lose their validity in principle, they
    usually satisfy the validity requirement in practice. Therefore, cross-conformal
    predictive systems differ from traditional conformal predictors in that they are
    more computationally efficient versions that can be used for automatic decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: 'The subsequent figure depicts the CPD prediction process for a single test
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – CPD](img/B19925_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – CPD
  prefs: []
  type: TYPE_NORMAL
- en: Let’s apply these concepts in practice; we will use the notebook *Conformal
    Prediction for* *Regression* ([https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_07.ipynb](https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_07.ipynb)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will follow the MAPIE tutorial for CQR. The target variable of this dataset
    is the median house value for the California districts. This dataset comprises
    eight features, including variables such as the house’s age, the neighborhood’s
    median income, the average number of rooms or bedrooms, and even the location
    in latitude and longitude. In total, there are around 20k observations. We compute
    the correlation between features and also between features and the target. As
    is evident from the analysis, the most significant correlation of house prices
    is with the neighborhood’s median income:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – California housing – correlation matrix](img/B19925_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – California housing – correlation matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also plot the distribution of house prices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – California housing – histogram of house prices](img/B19925_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – California housing – histogram of house prices
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can train and optimize the underlying model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Several methods are used to produce probabilistic predictions, including CQR
    and a variant of jackknife.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot the results that are produced by using different methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Predicting California housing prices using various methods](img/B19925_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Predicting California housing prices using various methods
  prefs: []
  type: TYPE_NORMAL
- en: 'We observe increased flexibility in the prediction intervals for CQR compared
    to other methods that maintain a fixed interval width. Specifically, as the prices
    rise, the prediction intervals expand correspondingly. To substantiate these observations,
    we will examine the conditional coverage and interval width across these intervals,
    segmented by quantiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Prediction interval coverage by binned house prices depending
    on price level](img/B19925_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Prediction interval coverage by binned house prices depending
    on price level
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, CQR adjusts to larger prices more adeptly. Its conditional coverage
    closely aligns with the target coverage for higher prices and lower prices where
    other methods exceed the necessary coverage. This adaptation is likely to influence
    the widths of the intervals.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also plot interval width by bin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Prediction of interval width by binned house prices depending
    on house price level](img/B19925_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – Prediction of interval width by binned house prices depending
    on house price level
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can look at CPD in action using the same notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will demonstrate the use of CPD with the Crepes package. For a quick start
    guide on the Crepes package, refer to https://github.com/henrikbostrom/crepes.
    We will wrap the standard random forest regressor using the `WrapRegressor` class
    from Crepes and fit it (in the usual way) to the proper training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will calibrate the regressor using the calibration set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The conformal regressor is now ready to generate prediction intervals for the
    test set, utilizing a 99% confidence level. The output will be a NumPy array.
    Each row corresponds to a test instance, with two columns indicating each prediction
    interval’s lower and upper bounds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can specify that we wish to trim the intervals to omit impossible values
    – in this scenario, values below 0\. Using the default confidence level (0.95),
    the resulting output intervals will be slightly more concise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will employ `DifficultyEstimator()` from `crepes` to make intervals more
    adaptive. Here, the difficulty is estimated by the standard deviation of the target
    of the default k=25 nearest neighbors in the proper training set for each object
    in the calibration set. First, we will obtain the difficulty estimates for the
    calibration set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'These can now be used for the calibration, which will produce a normalized
    conformal regressor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need difficulty estimates for the test set, which we provide as input
    to `predict_int`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Actual versus predicted values with prediction interval coverage](img/B19925_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Actual versus predicted values with prediction interval coverage
  prefs: []
  type: TYPE_NORMAL
- en: CPD (conformal predictive systems) in the Crepes package
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CPD yields cumulative distribution functions (conformal predictive distributions).
    These not only allow us to create prediction intervals but also enable us to derive
    percentiles, calibrated point predictions, and *p* values for specific target
    values. Let’s explore how to accomplish this.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only modification that’s required is to pass `cps=True` to the `calibrate`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, we can establish normalized Mondrian conformal predictive systems
    by supplying both `bins` and `sigmas` to the `calibrate` method. In this case,
    we will examine Mondrian categories formed by binning the point predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By supplying `bins` (and `sigmas`) for the test objects, we can make predictions
    using the conformal predictive system by utilizing the `predict_cps` method. This
    method offers flexible control over the output. In this instance, we seek to obtain
    prediction intervals with 95% confidence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can instruct the `predict_cps` method to return the complete CPD for each
    test instance, as delineated by the threshold values, by setting `return_cpds=True`.
    The structure of these distributions differs based on the type of conformal predictive
    system. For standard and normalized CPS, the output is an array with a row for
    each test instance and a column for each calibration instance (residual). Conversely,
    for a Mondrian CPS, the default output is a vector with one CPD for each test
    instance, as the number of values may fluctuate between categories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can plot CPD for a random object from the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.14 – CPD for a test object](img/B19925_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – CPD for a test object
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored uncertainty quantification for regression problems, a
    critical aspect of data science and machine learning. It highlighted the importance
    of uncertainty and the methods to handle it effectively to make more reliable
    predictions and decisions.
  prefs: []
  type: TYPE_NORMAL
- en: One of the significant sections of this chapter was dedicated to various approaches
    that can be used to produce prediction intervals. It systematically broke down
    and explained diverse methods, elucidating how each works and their advantages
    and disadvantages. This detailed analysis aids in understanding the mechanisms
    behind these approaches and their practical application in real-world regression
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, this chapter discussed building prediction intervals and predictive
    distributions using conformal prediction. We provided a step-by-step guide to
    constructing these intervals and distributions. This chapter also offered practical
    insights and tips for effectively utilizing conformal prediction to achieve more
    reliable and trustworthy predictions in regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we delved into advanced topics such as CQR, jackknife+, and CPD.
    These advanced techniques were broken down and explained in detail, helping you
    grasp their complexity and utility in handling regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: Practical illustration was crucial in this chapter, offering hands-on experience
    and insights. This chapter utilized housing price datasets to demonstrate the
    application of the discussed models and techniques. Libraries such as MAPIE and
    Crepes were applied to the datasets, providing you with practical knowledge and
    experience beyond theoretical understanding.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, this chapter provided a comprehensive and practical guide that
    covered various topics related to uncertainty quantification, prediction intervals,
    and conformal prediction for regression problems. The realistic illustrations,
    using real-world datasets and libraries, further enhanced the learning experience,
    making this chapter a valuable resource for anyone looking to deepen their understanding
    and strengthen their skills in these critical areas.
  prefs: []
  type: TYPE_NORMAL
