- en: '*Chapter 6*: Preparing for Model Evaluation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a good idea to think through how you will evaluate your model’s performance
    before you begin to run it. A common technique is to separate data into training
    and testing datasets. We do this relatively early in the process to avoid what
    is known as data leakage; that is, conducting analyses based on data that is intended
    to be set aside for model evaluation. In this chapter, we will look at approaches
    for creating training datasets, including how to ensure that training data is
    representative. We will look into cross-validation strategies such as **K-fold**,
    which address some of the limitations of using static training/testing splits.
    We will also begin to look more closely at assessing the performance of models.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why we are discussing model evaluation before going over
    any algorithms in detail. This is because there is a practical consideration.
    We tend to use the same metrics and evaluation techniques across algorithms with
    similar purposes. We examine accuracy and sensitivity when evaluating classification
    models, and mean absolute error and R-squared when examining regression models.
    We do cross-validation with all supervised learning models. So, we will repeat
    the strategies introduced here several times in the following chapters. You may
    even find yourself coming back to these pages when the concepts are re-introduced
    later.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond those practical considerations, our modeling work improves when we do
    not see data extraction, data cleaning, exploratory analysis, feature engineering
    and Preprocessing, model specification, and model evaluation as discrete, sequential
    tasks. If you have been building machine learning models for just 6 months or
    over 30 years, you probably appreciate that such rigid sequencing is inconsistent
    with our workflow as data scientists. We are always preparing for model validation,
    and always cleaning data. This is a good thing. We do better work when we integrate
    these tasks; when we continue to interrogate our data cleaning as we select features,
    and when we look back at bivariate correlations or scatter plots after calculating
    precision or root mean squared error.
  prefs: []
  type: TYPE_NORMAL
- en: We will also spend a fair bit of time constructing visualizations of these concepts.
    It is a good idea to get in the habit of looking at confusion matrices and cumulative
    accuracy profiles when working on classification problems, and plots of residuals
    when working with a continuous target. This, too, will serve us well in subsequent
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, in this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring accuracy, sensitivity, specificity, and precision for binary classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining CAP, ROC, and precision-sensitivity curves for binary classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating multiclass models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using K-fold cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing data with pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will work with the `feature_engine` and `matplotlib` libraries,
    in addition to the scikit-learn library. You can use `pip` to install these packages.
    The code files for this chapter can be found in this book’s GitHub repository
    at [https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning).
  prefs: []
  type: TYPE_NORMAL
- en: Measuring accuracy, sensitivity, specificity, and precision for binary classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When assessing a classification model, we typically want to know how often we
    are right. In the case of a binary target – one where the target has two possible
    categorical values – we calculate **accuracy** as the ratio of times we predict
    the correct classification against the total number of observations.
  prefs: []
  type: TYPE_NORMAL
- en: But, depending on the classification problem, accuracy may not be the most important
    performance measure. Perhaps we are willing to accept more false positives for
    a model that can identify more true positives, even if that means lower accuracy.
    This might be true for a model that would predict the likelihood of having breast
    cancer, a security breach, or structural damage in a bridge. In these cases, we
    may emphasize **sensitivity** (the propensity to identify positive cases) over
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, we may want a model that could identify negative cases with
    high reliability, even if that meant it did not do as good a job of identifying
    positives. **Specificity** is a measure of the percentage of all negatives identified
    by the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**, the percentage of predicted positives that are actually positives,
    is another important measure. For some applications, it is important to limit
    false positives, even if we have to tolerate lower sensitivity. An apple grower,
    using image recognition to identify bad apples, may prefer a high-precision model
    to a more sensitive one, not wanting to discard apples unnecessarily.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be made clearer by looking at a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Confusion matrix ](img/B17978_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'The confusion matrix helps us conceptualize accuracy, sensitivity, specificity,
    and precision. Accuracy is the percentage of observations for which our prediction
    was correct. This can be stated more precisely as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_06_0011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Sensitivity is the number of times we predicted positives correctly divided
    by the number of positives. It might be helpful to glance again at the confusion
    matrix and confirm that actual positive values can either be **predicted positives**
    (**TP**) or **predicted negatives** (**FN**). Sensitivity is also referred to
    as **recall** or the **true positive rate**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Specificity is the number of times we correctly predicted a **negative value**
    (**TN**) divided by the number of actual negative values (**TN + FP**). Specificity
    is also known as the **true negative rate**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Precision is the number of times we correctly predicted a **positive value**
    (**TP**) divided by the number of positive values predicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When there is class imbalance, measures such as accuracy and sensitivity can
    give us very different estimates of the performance of our model. An extreme example
    will illustrate this. Chimpanzees sometimes *termite fish*, putting a stick in
    a termite mound with the hopes of catching a few termites. This is only occasionally
    successful. I am no primatologist, but we can perhaps model a successful fishing
    attempt as a function of the size of the stick used, the time of year, and the
    age of the chimpanzee. In our testing data, fishing attempts are only successful
    2% of the time. (This data has been made up for this demonstration.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also say that we build a classification model of successful termite fishing
    that has a sensitivity of 50%. So, if there are 100 fishing attempts in our testing
    data, we would predict only one of the two successful attempts correctly. There
    is also one false positive, where our model predicted successful fishing when
    the fishing failed. This gives us the following confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Successful termite fishing confusion matrix ](img/B17978_06_0021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Successful termite fishing confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we get a very high accuracy of 98% – that is, (97+1) / 100\. We
    get high accuracy and low sensitivity because a large percentage of the fishing
    attempts are negative and that is easy to predict. A model that just predicts
    failure always would also have an accuracy of 98%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at these model evaluation measures with real data. We can experiment
    with a **k-nearest neighbors** (**KNN**) model to predict bachelor’s degree attainment
    and evaluate its accuracy, sensitivity, specificity, and precision:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading libraries for encoding and standardizing data, and
    for creating training and testing DataFrames. We will also load scikit-learn’s
    KNN classifier and the `metrics` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can create training and testing DataFrames and encode and scale the
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a KNN classification model. We will not worry too much about how
    we specify it since we just want to focus on evaluation measures in this section.
    We will use all of the features listed in `feature_cols` We use the predict method
    of the KNN classifier to generate predictions from the testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use scikit-learn to plot a confusion matrix. We will pass the actual
    values in the testing data (`y_test`) and predicted values to the `confusion_matrix`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This generates the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Confusion matrix of actual and predicted values ](img/B17978_06_0031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Confusion matrix of actual and predicted values
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also just return the true negative, false positive, false negative,
    and true positive counts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now have what we need to calculate accuracy, sensitivity, specificity, and
    precision:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This model has relatively low accuracy, but somewhat better sensitivity; that
    is, it does a better job of identifying those in the testing data who have completed
    a bachelor’s degree than of correctly identifying both degree completers and non-completers
    overall. If we look back at the confusion matrix, we will see that there are a
    fair number of false positives, as our model predicts that 63 individuals in the
    testing data would have a bachelor’s degree who did not.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could have also used scikit-learn handy methods for generating these statistics
    directly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Just for comparison, let’s try a random forest classifier and see if we get
    any better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit a random forest classifier to the same data and call `confusion_matrix`
    again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The second model gets us significantly fewer false negatives and more true positives
    than the first model. It is less likely to predict no bachelor’s degree when individuals
    in the test data have completed a bachelor’s degree, and more likely to predict
    a bachelor’s degree when the person has completed one. The main impact of the
    lower FP and higher TP is a significantly higher sensitivity. The second model
    identifies actual positives 89% of the time, compared with 80% for the first model.
  prefs: []
  type: TYPE_NORMAL
- en: The measures we have discussed in this section – accuracy, sensitivity, specificity,
    and precision – are worth looking at whenever we are evaluating a classification
    model. But it can be hard to get a good sense of the tradeoffs we are sometimes
    confronted with, between precision and sensitivity, for example. Data scientists
    rely on several standard visualizations to improve our sense of these tradeoffs
    when building classification models. We will examine these visualizations in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Examining CAP, ROC, and precision-sensitivity curves for binary classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several ways to visualize the performance of a binary classification
    model. A relatively straightforward visualization is the **Cumulative Accuracy
    Profile** (**CAP**), which shows the ability of our model to identify in-class,
    or positive, cases. It shows the cumulative cases on the *X*-axis and the cumulative
    positive outcomes on the *Y*-axis. A CAP curve is a good way to see how good a
    job our model does at discriminating in-class observations. (When discussing binary
    classification models, I will use the terms *in-class* and *positive* interchangeably.)
  prefs: []
  type: TYPE_NORMAL
- en: '**Receiver operating characteristic** (**ROC**) curves illustrate the tradeoff
    between model sensitivity (being able to identify positive values) and the false
    positive rate as we adjust the threshold for classifying a positive value. Similarly,
    precision-sensitivity curves show the relationship between the reliability of
    our positive predictions (their precision) and sensitivity (our model’s ability
    to identify positive actual values) as we adjust the threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: Constructing CAP curves
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with CAP curves for our bachelor’s completion KNN model. Let’s also
    compare that with a decision tree model. Again, we will not do much with feature
    selection here. The previous chapter went over feature selection in some detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to curves for our models, CAP curves also have plots of a **random
    model** and a **perfect model** to view for comparison. The random model provides
    no information other than the overall distribution of positive values. The perfect
    model predicts positive values precisely. To illustrate how those plots are drawn,
    we will start with a hypothetical example. Imagine that you sample the first six
    cards of a nicely shuffled deck of playing cards. You create a table with the
    cumulative card total in one column and the number of red cards in the next column.
    It may look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Sample of playing cards  ](img/B17978_06_0041.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Sample of playing cards
  prefs: []
  type: TYPE_NORMAL
- en: We can plot a random model based on just our knowledge of the number of red
    cards. The random model has just two points, (0,0) and (6,3), but that is all
    we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'The perfect model plot requires a bit more explanation. If our model predicted
    red cards perfectly and we sorted by the prediction in descending order, we would
    get *Figure 6.5*. The cumulative in-class count matches the number of cards until
    the red cards have been exhausted, which is 3 in this case. A plot of the cumulative
    in-class total with a perfect model would have two slopes; equal to 1 up until
    the in-class total was reached, and then 0 after that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Sample of playing cards ](img/B17978_06_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Sample of playing cards
  prefs: []
  type: TYPE_NORMAL
- en: 'We now know enough to plot both the random model and the perfect model. The
    perfect model will have three points: (0,0), (in-class count, in-class count),
    and (number of cards, in-class count). In this case, in-class count is `3` and
    the number of cards is `6`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – CAP with playing card data ](img/B17978_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – CAP with playing card data
  prefs: []
  type: TYPE_NORMAL
- en: One way to understand the improvement of the perfect model over the random model
    is to consider how many red cards the random model would predict at the midpoint
    – that is, 3 cards. At that point, the random model would predict 1.5 red cards.
    However, the perfect model would predict 3\. (Remember that we have sorted the
    cards by prediction in descending order.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Having constructed plots for random and perfect models with made-up data, let’s
    try it with our bachelor’s degree completion data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import the same modules as in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we load, encode, and scale the NLS bachelor’s degree data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create `KNeighborsClassifier` and `RandomForestClassifier` instances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are now ready to start plotting our CAP curves. We will start by drawing
    a random model and then a perfect model. These are models that use no information
    (other than the overall distribution of positive values) and that provide perfect
    information, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: We count the number of observations in the test data and the number of positive
    values. We will use (0,0) and (the number of observations, in-class count) to
    draw the random model line. For the perfect model, we will plot a line from (0,0)
    to (in-class count, in-class count) since that model can perfectly discriminate
    in-class values (it is never wrong). It is flat to the right of that point since
    there are no more positive values to find.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will also draw a vertical line at the midpoint and a horizontal line where
    that intersects the random model line. This will be more useful later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – CAP with just random and perfect models ](img/B17978_06_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – CAP with just random and perfect models
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define a function to plot a CAP curve for a model we pass to it. We
    will use the `predict_proba` method to get an array with the probability that
    each observation in the test data is in-class (in this case, has completed a bachelor’s
    degree). Then, we will create a DataFrame with those probabilities and the actual
    target value, sort it by probability in reverse order, and calculate a running
    total of positive actual target values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will also get the value of the running total at the middle observation and
    draw a horizontal line at that point. Finally, we will plot a line that has an
    array from 0 to the number of observations as *x* values, and the running in-class
    totals as *y* values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s run the function for the KNN and random forest classifier models
    using the same data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This updates our earlier plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – CAP updated with KNN and random forest models ](img/B17978_06_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – CAP updated with KNN and random forest models
  prefs: []
  type: TYPE_NORMAL
- en: Not surprisingly, the CAP curves show that our KNN and random forest models
    are better than randomly guessing, but not as good as a perfect model. The question
    is, how much better and how much worse, respectively. The horizontal lines give
    us some idea. A perfect model would have correctly identified 138 positive values
    out of 138 observations. (Recall that the observations are sorted so that the
    observations with the highest likelihood of being positive are first.) The random
    model would have identified 70 (line not shown), while the KNN and random forest
    models would have identified 102 and 103, respectively. Our two models are 74%
    and 75% as good as a perfect model would have been at discriminating positive
    values. Anything between 70% and 80% is considered to be a good model; percentages
    above that are very good, while percentages below that are poor.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting a receiver operating characteristic (ROC) curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ROC curves illustrate the tradeoff between the false positive rate and the
    true positive rate (also known as sensitivity) as we adjust the threshold. We
    should discuss the false positive rate before going further. It is the percentage
    of actual negatives (true negatives plus false positives) that our model falsely
    identifies as positive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_06_0051.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, you can see the relationship that the false positive rate has with specificity,
    which was discussed at the beginning of this chapter. The difference is the numerator.
    Specificity is the percentage of actual negatives that our model correctly identifies
    as negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_06_0061.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also compare the false positive rate with sensitivity, which is the
    percentage of actual positives (true positives plus false negatives) that our
    model correctly identifies as positive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_06_0071.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We are typically confronted with a tradeoff between sensitivity and the false
    positive rate. We want our models to be able to identify a large percentage of
    the actual positives, but we do not want a problematically high false positive
    rate. What is *problematically high* depends on your context.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tradeoff between sensitivity and the false positive rate is trickier the
    more difficult it is to discriminate between negative and positive cases. We can
    see this with our bachelor’s degree completion model when we plot the predicted
    probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s fit our random forest classifier again and generate predictions
    and prediction probabilities. We will see that this model predicts that the person
    completes a bachelor’s degree when the predicted probability is greater than `0.500`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is helpful to compare the distribution of these probabilities with the actual
    class values. We can do this with density plots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Density plot of in-class and out-of-class observations ](img/B17978_06_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Density plot of in-class and out-of-class observations
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that our model has some trouble discriminating between actual
    positive and negative values since there is a fair bit of in-class and out-of-class
    overlap. A threshold of 0.500 (the left dotted line) gets us a lot of false positives
    since a good portion of the distribution of out-of-class observations (those not
    completing bachelor’s degrees) have predicted probabilities greater than 0.500\.
    If we move the threshold higher, say to 0.650, we get many more false negatives
    since many in-class observations have probabilities lower than 0.65.
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to construct a ROC curve based on the testing data and the random
    forest model. The `roc_curve` method returns both the false positive rate (`fpr`)
    and sensitivity (true positive rate, `tpr`) at different thresholds (`ths`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, let’s draw separate false positive rate and sensitivity lines by threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – False positive rate and sensitivity lines ](img/B17978_06_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – False positive rate and sensitivity lines
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that increasing the threshold will improve (reduce) our false
    positive rate, but also lower our sensitivity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s draw the associated ROC curve, which plots the false positive rate
    against sensitivity for each threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – ROC curve with false positive rate and sensitivity ](img/B17978_06_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – ROC curve with false positive rate and sensitivity
  prefs: []
  type: TYPE_NORMAL
- en: The ROC curve indicates that the tradeoff between the false positive rate and
    sensitivity is pretty steep until the false positive rate is about 0.5 or higher.
    Let’s see what that means for the threshold of 0.5 that was used for the random
    forest model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s select an index from the threshold array that is near 0.5, and also one
    near 0.4 and 0.6 for comparison. Then, we will draw vertical lines for the false
    positive rate at those indexes, and horizontal lines for the sensitivity values
    at those indexes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This updates our plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – ROC curve with lines for thresholds ](img/B17978_06_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – ROC curve with lines for thresholds
  prefs: []
  type: TYPE_NORMAL
- en: This illustrates the tradeoff between the false positive rate and sensitivity
    at the 0.5 threshold (the blue dashed line) used for predictions. The ROC curve
    has very little slope with thresholds above 0.5, such as with the 0.6 threshold
    (the green dashed line). So, reducing the threshold from 0.6 to 0.5 results in
    a substantially lower false positive rate (from above 0.8 to below 0.6), but not
    much reduction in sensitivity. However, improving (reducing) the false positive
    rate by reducing the threshold from 0.5 to 0.4 (from the blue to the purple line)
    leads to significantly worse sensitivity. It drops from nearly 90% to just above
    70%.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting precision-sensitivity curves
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is often helpful to examine the relationship between precision and sensitivity
    as the threshold is adjusted. Remember that precision tells us the percentage
    of the time we are correct when we predict a positive value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_06_0081.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can improve precision by increasing the threshold for classifying a value
    as positive. However, this will likely mean a reduction in sensitivity. As we
    improve how often we are correct when we predict a positive value (precision),
    we will decrease the number of positive values we are able to identify (sensitivity).
    Precision-sensitivity curves, often called precision-recall curves, illustrate
    this tradeoff.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before drawing the precision-sensitivity curve, let’s look at separate precision
    and sensitivity lines plotted against thresholds:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the points for the precision-sensitivity curves with the `precision_recall_curve`
    method. We remove some squirreliness at the highest threshold values, which can
    sometimes happen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Precision and sensitivity lines ](img/B17978_06_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Precision and sensitivity lines
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that sensitivity declines more steeply with thresholds above
    0.5\. This decline does not buy us much improved precision beyond the 0.6 threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s plot sensitivity against precision to view the precision-sensitivity
    curve:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Precision-sensitivity curve ](img/B17978_06_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Precision-sensitivity curve
  prefs: []
  type: TYPE_NORMAL
- en: The precision-sensitivity curve reflects the fact that sensitivity is much more
    responsive to threshold than is precision with this particular model. This means
    that we could decrease the threshold below 0.5 to get greater sensitivity, without
    a significant reduction in precision.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The choice of threshold is partly a matter of judgment and domain knowledge,
    and is mostly an issue when we have significant class imbalance. However, in [*Chapter
    10*](B17978_10_ePub.xhtml#_idTextAnchor126)*, Logistic Regression* we will explore
    how to calculate an optimal threshold.
  prefs: []
  type: TYPE_NORMAL
- en: This section, and the previous one, demonstrated how to evaluate binary classification
    models. They showed that model evaluation is not just a thumbs up and thumbs down
    process. It is much more like tasting your batter as you make a cake. We make
    good initial assumptions about our model specification and use the model evaluation
    process to make improvements. This often involves tradeoffs between accuracy,
    sensitivity, specificity, and precision, and modeling decisions that resist one-size-fits-all
    recommendations. These decisions are very much domain-dependent and a matter of
    professional judgment.
  prefs: []
  type: TYPE_NORMAL
- en: The discussion in this section, and most of the techniques, apply as much to
    multiclass modeling. We discuss evaluating multiclass models in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating multiclass models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the same principles that we used to evaluate binary classification models
    apply to multiclass model evaluation. Computing a confusion matrix is just as
    important, though a fair bit more difficult to interpret. We also still need to
    examine somewhat competing measures, such as precision and sensitivity. This,
    too, is messier than doing so with binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, we will work with the NLS degree completion data. We will alter
    the target in this case, from bachelor’s degree completion or not to high school
    completion, bachelor’s degree completion, and post-graduate degree completion:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading the necessary libraries. These are the same libraries
    we used in the previous two sections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will load the NLS degree attainment data, create training and testing
    DataFrames, and encode and scale the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will run a KNN model and predict values for each degree level category:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use those predictions to generate a confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This generates the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Confusion matrix with a multiclass target ](img/B17978_06_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Confusion matrix with a multiclass target
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to calculate evaluation measures by hand. Precision is the percentage
    of our in-class predictions that are actually in-class. So, for our prediction
    of high school, it is 48 / (48 + 38 + 8) = 0.51\. Sensitivity for the high school
    class – that is, the percentage of actual values of high school that our model
    predicts – is 48 / (48 + 19 +5) = 0.67\. However, this is fairly tedious. Fortunately,
    scikit-learn can do this for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can call the `classification_report` method to get these statistics, passing
    actual and predicted values (remember that recall and sensitivity are the same
    measure):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In addition to precision and sensitivity rates by class, we get some other statistics.
    The F1-score is the harmonic mean of precision and sensitivity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_06_0091.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *p* is precision and *s* is sensitivity.
  prefs: []
  type: TYPE_NORMAL
- en: To get the average precision, sensitivity, and F1-score across classes, we can
    either use the simple average (macro average) or a weighted average that adjusts
    for class size. Using the weighted average, we get precision, sensitivity, and
    F1-score values of 0.49, 0.50, and 0.49, respectively. (Since the classes are
    relatively balanced here, there is not much difference between the macro average
    and the weighted average.)
  prefs: []
  type: TYPE_NORMAL
- en: This demonstrates how to extend the evaluation measures we discussed for binary
    classification models to multiclass evaluation. The same concepts and techniques
    apply, though they are more difficult to implement.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have focused on metrics and visualizations to help us evaluate classification
    models. We have not examined metrics for evaluating regression models yet. These
    metrics can be somewhat more straightforward than those for classification. We
    will discuss them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics for regression evaluation are typically based on the distance between
    the actual values for the target variable and a model’s predicted values. The
    most common measures – mean squared error, root mean squared error, mean absolute
    error, and R-squared – all track how successfully our predictions capture variation
    in a target.
  prefs: []
  type: TYPE_NORMAL
- en: 'The distance between the actual value and our prediction is known as the residual,
    or error. The **mean squared error** (**MSE**) is the mean of the square of the
    residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_06_0101.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17978_06_011.png) is the actual target variable value at the
    ith observation and ![](img/B17978_06_012.png) is our prediction for the target.
    The residuals are squared to handle negative values, where the predicted value
    is higher than the actual value. To return our measurement to a more meaningful
    scale, we often use the square root of MSE. That is known as **root mean squared
    error** (**RMSE**).
  prefs: []
  type: TYPE_NORMAL
- en: Due to the squaring, MSE will penalize larger residuals much more than it will
    smaller residuals. For example, if we have predictions for five observations,
    with one having a residual of 25, and the other four having a residual of 0, we
    will get an MSE of *(0+0+0+0+625)/5 = 125*. However, if all five observations
    had residuals of 5, the MSE would be *(25+25+25+25+25)/5 = 25*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good alternative to squaring the residuals is to take their absolute value.
    This gives us the mean absolute error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_06_0131.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'R-squared, also known as the coefficient of determination, is an estimate of
    the proportion of the variation in the target variable captured by our model.
    We square the residuals, as we do when calculating MSE, and divide that by the
    deviation of each actual target value from its sample mean. This gives us the
    still unexplained variation, which we subtract from 1 to get the explained variation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_06_0141.jpg)![](img/B17978_06_0151.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Fortunately, scikit-learn makes it easy to generate these statistics. In this
    section, we will build a linear regression model of land temperatures and use
    these statistics to evaluate it. We will work with data from the United States
    National Oceanic and Atmospheric Administration on average annual temperatures,
    elevation, and latitude at weather stations in 2019.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The land temperature dataset contains the average temperature readings (in Celsius)
    in 2019 from over 12,000 stations across the world, though the majority of the
    stations are in the United States. The raw data was retrieved from the Global
    Historical Climatology Network integrated database. It has been made available
    for public use by the United States National Oceanic and Atmospheric Administration
    at [https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4](https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start building a linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading the libraries we need and the land temperatures data.
    We will also create training and testing DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `latabs` feature is the value of latitude without the North or South indicators;
    so, Cairo, Egypt, at approximately 30 degrees north, and Porto Alegre, Brazil,
    at about 30 degrees south, have the same value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we scale our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we instantiate a scikit-learn `LinearRegression` object and fit a model
    on the training data. Our target is the annual average temperature (`avgtemp`),
    while the features are latitude (`latabs`) and `elevation`. The `coef_` attribute
    gives us the coefficient for each feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The interpretation of the `latabs` coefficient is that standardized average
    annual temperature will decline by 0.85 for every one standard deviation increase
    in latitude. (The `LinearRegression` module does not return p-values, a measure
    of the statistical significance of the coefficient estimate. You can use `statsmodels`
    instead to see a full summary of an ordinary least squares model.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can get predicted values. Let’s also join the returned NumPy array
    with the features and the target from the testing data. Then, we can calculate
    the residuals by subtracting the predicted values from the actual values (`avgtemp`).
    The residuals do not look bad, though there is a little negative skew and excessive
    kurtosis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is worth noting that we will be generating predictions and calculated residuals
    in this way most of the time we work with regression models in this book. If you
    feel a little unclear about what we just did in the preceding code block, it may
    be a good idea to go over it again.
  prefs: []
  type: TYPE_NORMAL
- en: We should plot the residuals to get a better sense of how they are distributed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Histogram of residuals for the linear regression model ](img/B17978_06_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Histogram of residuals for the linear regression model
  prefs: []
  type: TYPE_NORMAL
- en: This does not look too bad, but we have more positive residuals, where we have
    predicted a lower temperature in the testing data than the actual temperature,
    than negative residuals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plotting our predictions by the residuals may give us a better sense of what
    is happening:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Scatterplot of predictions by residuals for the linear regression
    model ](img/B17978_06_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Scatterplot of predictions by residuals for the linear regression
    model
  prefs: []
  type: TYPE_NORMAL
- en: This does not look horrible. The residuals hover somewhat randomly around 0. However,
    predictions between 1 and 2 standard deviations are much more likely to be too
    low (to have positive residuals) than too high. Above 2, the predictions are always
    too high (they have negative residuals). This model’s assumption of linearity
    might not be sound. We should explore a couple of the transformations we discussed
    in [*Chapter 4*](B17978_04_ePub.xhtml#_idTextAnchor043), *Encoding, Transforming,
    and Scaling Features*, or try a non-parametric model such as KNN regression.
  prefs: []
  type: TYPE_NORMAL
- en: It is also likely that extreme values are tugging our coefficients around a
    fair bit. A good next move might be to remove outliers, as we discussed in the
    *Identifying extreme values and outliers* section of [*Chapter 1*](B17978_01_ePub.xhtml#_idTextAnchor014),
    *Examining the Distribution of Features and Targets*. We will not do that here,
    however.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some evaluation measures. This can easily be done with scikit-learn’s
    `metrics` library. We can call the same function to get RMSE as MSE. We just need
    to set the squared parameter to `False`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: An MSE of less than 0.2 of a standard deviation and an MAE of less than 0\.
    3 of a standard deviation look pretty decent, especially for such a sparse model.
    An R-squared above 80% is also fairly promising.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what we get if we use a KNN model instead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This model is actually an improvement in both MAE and R-squared.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should also take a look at the residuals again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Scatterplot of predictions by residuals for the KNN model ](img/B17978_06_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Scatterplot of predictions by residuals for the KNN model
  prefs: []
  type: TYPE_NORMAL
- en: This plot of the residuals looks better as well. There are no parts of the target’s
    distribution where we are much more likely to over-predict or under-predict.
  prefs: []
  type: TYPE_NORMAL
- en: This section has introduced key measures for evaluating regression models, and
    how to interpret them. It has also demonstrated how visualizations, particularly
    of model residuals, can improve that interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: However, we have been limited so far, in both our use of regression and classification
    measures, by how we have constructed our training and testing DataFrames. What
    if, for some reason, the testing data is unusual in some way? More generally,
    what is our basis for concluding that our evaluation measures are accurate? We
    can be more confident in these measures if we use K-fold cross-validation, which
    we will cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using K-fold cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have held back 30% of our data for validation. This is not a bad
    strategy. It prevents us from peeking ahead to the testing data as we train our
    model. However, this approach does not take full advantage of all the available
    data, either for training or for testing. If we use K-fold cross-validation instead,
    we can use all of our data while also avoiding data leakage. Perhaps that seems
    too good to be true. But it’s not because of a neat little trick.
  prefs: []
  type: TYPE_NORMAL
- en: '**K-fold cross-validation** trains our model on all but one of the K folds,
    or parts, leaving one out for testing. This is repeated *k* times, each time excluding
    a different fold for testing. Performance metrics are then based on the average
    scores across the K folds.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we start, though, we need to think again about the possibility of data
    leakage. If we scale all of the data that we will use to train our model and then
    split it up into folds, we will be using information from all the folds in our
    training. To avoid this, we need to do the scaling, as well as any other Preprocessing,
    on just the training folds for each iteration. While we could do this manually,
    scikit-learn’s `pipeline` library can do much of this work for us. We will go
    over how to use pipelines for cross-validation in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try evaluating the two models we specified in the previous section using
    K-fold cross-validation. While we are at it, let’s also see how well a random
    forest regressor may work:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the libraries we have worked with so far, we need scikit-learn’s`make_pipeline`,
    `cross_validate`, and `Kfold` libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the land temperatures data again and create training and testing DataFrames.
    We still want to leave some data out for final validation, but this time, we will
    only leave out 10%. We will do both training and testing with the remaining 90%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we create a `KFold` object and indicate that we want five folds and for
    the data to be shuffled (shuffling the data is a good idea if it is not already
    sorted randomly):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define a function to create a pipeline. The function then runs `cross_validate`,
    which takes the pipeline and the `KFold` object we created earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to call the `getscores` function for the linear regression,
    random forest regression, and KNN regression models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can print the `scorelist` list to see our results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The KNN regressor model performs better than either the linear regression or
    random forest regression model, based on R-squared. The random forest regressor
    also has a significant disadvantage in that it has a much longer fit time.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing data with pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just scratched the surface of what we can do with scikit-learn pipelines
    in the previous section. We often need to fold all of our Preprocessing and feature
    engineering into a pipeline, including scaling, encoding, and handling outliers
    and missing values. This can be complicated as different features may need to
    be handled differently. We may need to impute the median for missing values with
    numeric features and the most frequent value for categorical features. We may
    also need to transform our target variable. We will explore how to do that in
    this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading the libraries we have already worked with in this
    chapter. Then, we will add the `ColumnTransformer` and `TransformedTargetRegressor`
    classes. We will use those classes to transform our features and target, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The column transformer is quite flexible. We can even use it with the Preprocessing
    functions that we have defined ourselves. The following code block imports the
    `OutlierTrans` class from the `preprocfunc` module in the `helperfunctions` subfolder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `OutlierTrans` class identifies extreme values by distance from the interquartile
    range. This is a technique we demonstrated in [*Chapter 3*](B17978_03_ePub.xhtml#_idTextAnchor034),
    *Identifying and Fixing Missing Values*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To work in a scikit-learn pipeline, our class has to have fit and transform
    methods. We also need to inherit the `BaseEstimator` and `TransformerMixin` classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this class, almost all of the action happens in the `transform` method.
    Any value that is more than 1.5 times the interquartile range above the third
    quartile or below the first quartile is assigned missing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Our `OutlierTrans` class can be used later in our pipeline in the same way we
    used `StandardScaler` in the previous section. We will do that later.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are ready to load the data that needs to be processed. We will work
    with the NLS weekly wage data in this section. Weekly wages will be our target,
    and we will use high school GPA, mother’s and father’s highest grade completed,
    parent income, gender, and whether the individual completed a bachelor’s degree
    as features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will create lists of features to handle in different ways here. This will
    be helpful later when we instruct our pipeline to carry out different operations
    on numerical, categorical, and binary features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at some descriptive statistics. Some variables have over a thousand
    missing values (`gpascience`, `gpaenglish`, `gpamath`, `gpaoverall`, and `parentincome`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we can set up a column transformer. First, we will create pipelines for
    handling numerical data (`standtrans`), categorical data, and binary data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the numerical data, we want to assign outlier values as missing. Here, we
    will pass a value of `2` to the threshold parameter of `OutlierTrans`, indicating
    that we want values two times the interquartile range above or below that range
    to be set to missing. Recall that the default is 1.5, so we are being somewhat
    more conservative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will create a `ColumnTransformer` object, passing to it the three
    pipelines we just created, and indicating which features to use with which pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can add the column transformer to a pipeline that also includes the
    linear model that we would like to run. We will add KNN imputation to the pipeline
    to handle missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We also need to scale the target, which cannot be done in our pipeline. We
    will use scikit-learn’s `TransformedTargetRegressor` for that. We will pass the
    pipeline we just created to the target regressor’s `regressor` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s do K-fold cross validation using this pipeline. We can pass our pipeline,
    via the target regressor, `ttr`, to the `cross_validate` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These scores are not very good, though that was not quite the point of this
    exercise. The key takeaway here is that we typically want to fold most of the
    Preprocessing we will do into a pipeline. This is the best way to avoid data leakage.
    The column transformer is an extremely flexible tool, allowing us to apply different
    transformations to different features.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced key model evaluation measures and techniques so that
    they will be familiar when we make extensive use of them, and extend them, in
    the remaining chapters of this book. We examined the very different approaches
    to evaluation for classification and regression models. We also explored how to
    use visualizations to improve our analysis of our predictions. Finally, we used
    pipelines and cross-validation to get reliable estimates of model performance.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this chapter also gave you a chance to get used to the general approach
    of this book going forward. Although a large number of algorithms will be discussed
    in the remaining chapters, we will continue to surface the Preprocessing issues
    we have discussed in the first few chapters. We will discuss the core concepts
    of each algorithm, of course. But, in a true *hands-on* fashion, we will also
    deal with the messiness of real-world data. Each chapter will go from relatively
    raw data to feature engineering to model specification and model evaluation, relying
    heavily on scikit-learn’s pipelines to pull it all together.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss regression algorithms in the next few chapters – those algorithms
    that allow us to model a continuous target. We will explore some of the most popular
    regression algorithms – linear regression, support vector regression, K-nearest
    neighbors regression, and decision tree regression. We will also consider making
    modifications to regression models that address underfitting and overfitting,
    including nonlinear transformations and regularization.
  prefs: []
  type: TYPE_NORMAL
