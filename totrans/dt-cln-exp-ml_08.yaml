- en: '*Chapter 6*: Preparing for Model Evaluation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*: 准备模型评估'
- en: It is a good idea to think through how you will evaluate your model’s performance
    before you begin to run it. A common technique is to separate data into training
    and testing datasets. We do this relatively early in the process to avoid what
    is known as data leakage; that is, conducting analyses based on data that is intended
    to be set aside for model evaluation. In this chapter, we will look at approaches
    for creating training datasets, including how to ensure that training data is
    representative. We will look into cross-validation strategies such as **K-fold**,
    which address some of the limitations of using static training/testing splits.
    We will also begin to look more closely at assessing the performance of models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始运行模型之前，思考如何评估模型性能是一个好主意。一种常见的技术是将数据分为训练集和测试集。我们在早期阶段就做这件事，以避免所谓的数据泄露；也就是说，基于原本打算用于模型评估的数据进行分析。在本章中，我们将探讨创建训练集的方法，包括如何确保训练数据具有代表性。我们还将探讨交叉验证策略，如**K折**，它解决了使用静态训练/测试分割的一些局限性。我们还将开始更仔细地评估模型性能。
- en: You might be wondering why we are discussing model evaluation before going over
    any algorithms in detail. This is because there is a practical consideration.
    We tend to use the same metrics and evaluation techniques across algorithms with
    similar purposes. We examine accuracy and sensitivity when evaluating classification
    models, and mean absolute error and R-squared when examining regression models.
    We do cross-validation with all supervised learning models. So, we will repeat
    the strategies introduced here several times in the following chapters. You may
    even find yourself coming back to these pages when the concepts are re-introduced
    later.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么我们在详细讨论任何算法之前讨论模型评估。这是因为有一个实际考虑。我们倾向于在具有相似目的的算法中使用相同的指标和评估技术。在评估分类模型时，我们检查准确率和敏感性，在检查回归模型时，我们检查平均绝对误差和R平方。我们对所有监督学习模型进行交叉验证。因此，我们将在以下章节中多次重复介绍这些策略。你甚至可能会在概念稍后重新引入时回到这些页面。
- en: Beyond those practical considerations, our modeling work improves when we do
    not see data extraction, data cleaning, exploratory analysis, feature engineering
    and Preprocessing, model specification, and model evaluation as discrete, sequential
    tasks. If you have been building machine learning models for just 6 months or
    over 30 years, you probably appreciate that such rigid sequencing is inconsistent
    with our workflow as data scientists. We are always preparing for model validation,
    and always cleaning data. This is a good thing. We do better work when we integrate
    these tasks; when we continue to interrogate our data cleaning as we select features,
    and when we look back at bivariate correlations or scatter plots after calculating
    precision or root mean squared error.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些实际考虑因素之外，当我们不将数据提取、数据清洗、探索性分析、特征工程和预处理、模型指定和模型评估视为离散的、顺序的任务时，我们的建模工作会得到改善。如果你只构建了6个月的机器学习模型，或者超过30年，你可能会欣赏到这种严格的顺序与数据科学家的工作流程不一致。我们总是在准备模型验证，并且总是在清理数据。这是好事。当我们整合这些任务时，我们会做得更好；当我们选择特征时继续审查我们的数据清洗，以及当我们计算精确度或均方根误差后回顾双变量相关或散点图时。
- en: We will also spend a fair bit of time constructing visualizations of these concepts.
    It is a good idea to get in the habit of looking at confusion matrices and cumulative
    accuracy profiles when working on classification problems, and plots of residuals
    when working with a continuous target. This, too, will serve us well in subsequent
    chapters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将花费相当多的时间构建这些概念的可视化。在处理分类问题时，养成查看混淆矩阵和累积准确率轮廓的习惯是一个好主意，而在处理连续目标时，则查看残差图。这同样会在后续章节中对我们大有裨益。
- en: 'Specifically, in this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中，我们将涵盖以下主题：
- en: Measuring accuracy, sensitivity, specificity, and precision for binary classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量二分类的准确率、敏感性、特异性和精确度
- en: Examining CAP, ROC, and precision-sensitivity curves for binary classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查二分类的CAP、ROC和精确度-敏感性曲线
- en: Evaluating multiclass models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估多分类模型
- en: Evaluating regression models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估回归模型
- en: Using K-fold cross-validation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K折交叉验证
- en: Preprocessing data with pipelines
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用管道预处理数据
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will work with the `feature_engine` and `matplotlib` libraries,
    in addition to the scikit-learn library. You can use `pip` to install these packages.
    The code files for this chapter can be found in this book’s GitHub repository
    at [https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用`feature_engine`和`matplotlib`库，以及scikit-learn库。您可以使用`pip`安装这些包。本章的代码文件可以在本书的GitHub仓库中找到，网址为[https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning)。
- en: Measuring accuracy, sensitivity, specificity, and precision for binary classification
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量二元分类的准确率、灵敏度、特异度和精确度
- en: When assessing a classification model, we typically want to know how often we
    are right. In the case of a binary target – one where the target has two possible
    categorical values – we calculate **accuracy** as the ratio of times we predict
    the correct classification against the total number of observations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估分类模型时，我们通常想知道我们正确的情况有多频繁。在二元目标的情况下——目标有两个可能的分类值——我们计算**准确率**为预测正确分类的次数与观察总数之比。
- en: But, depending on the classification problem, accuracy may not be the most important
    performance measure. Perhaps we are willing to accept more false positives for
    a model that can identify more true positives, even if that means lower accuracy.
    This might be true for a model that would predict the likelihood of having breast
    cancer, a security breach, or structural damage in a bridge. In these cases, we
    may emphasize **sensitivity** (the propensity to identify positive cases) over
    accuracy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，根据分类问题，准确率可能不是最重要的性能指标。也许我们愿意接受更多的假阳性，以换取能够识别更多真正正面的模型，即使这意味着较低的准确率。这可能适用于预测患有乳腺癌、安全漏洞或桥梁结构损坏的可能性模型。在这些情况下，我们可能更强调**灵敏度**（识别正案例的倾向）而不是准确率。
- en: On the other hand, we may want a model that could identify negative cases with
    high reliability, even if that meant it did not do as good a job of identifying
    positives. **Specificity** is a measure of the percentage of all negatives identified
    by the model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们可能希望有一个模型能够以高可靠性识别出负面案例，即使这意味着它不能很好地识别正面案例。**特异度**是模型识别出的所有负面案例的百分比。
- en: '**Precision**, the percentage of predicted positives that are actually positives,
    is another important measure. For some applications, it is important to limit
    false positives, even if we have to tolerate lower sensitivity. An apple grower,
    using image recognition to identify bad apples, may prefer a high-precision model
    to a more sensitive one, not wanting to discard apples unnecessarily.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确度**，即预测为正的预测值实际上是正的百分比，是另一个重要的度量。对于某些应用，限制假阳性可能很重要，即使这意味着我们必须容忍较低的灵敏度。一个苹果种植者，使用图像识别来识别坏苹果，可能更倾向于选择高精确度的模型，而不是更灵敏的模型，不希望不必要地丢弃苹果。'
- en: 'This can be made clearer by looking at a confusion matrix:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过查看混淆矩阵来更清楚地说明：
- en: '![Figure 6.1 – Confusion matrix ](img/B17978_06_001.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 混淆矩阵](img/B17978_06_001.jpg)'
- en: Figure 6.1 – Confusion matrix
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 混淆矩阵
- en: 'The confusion matrix helps us conceptualize accuracy, sensitivity, specificity,
    and precision. Accuracy is the percentage of observations for which our prediction
    was correct. This can be stated more precisely as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵帮助我们理解准确率、灵敏度、特异性和精确度。准确率是指我们的预测正确的观察值的百分比。这可以更精确地表述如下：
- en: '![](img/B17978_06_0011.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_06_0011.jpg)'
- en: 'Sensitivity is the number of times we predicted positives correctly divided
    by the number of positives. It might be helpful to glance again at the confusion
    matrix and confirm that actual positive values can either be **predicted positives**
    (**TP**) or **predicted negatives** (**FN**). Sensitivity is also referred to
    as **recall** or the **true positive rate**:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 灵敏度是指我们正确预测正面的次数除以正面的总数。回顾一下混淆矩阵，确认实际的正值可以是**预测正值**（**TP**）或**预测负值**（**FN**）。灵敏度也被称为**召回率**或**真正率**：
- en: '![](img/B17978_06_002.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_06_002.jpg)'
- en: 'Specificity is the number of times we correctly predicted a **negative value**
    (**TN**) divided by the number of actual negative values (**TN + FP**). Specificity
    is also known as the **true negative rate**:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**特异度**是指我们正确预测负值的次数除以实际的负值总数（**TN + FP**）。特异度也被称为**真正率**：'
- en: '![](img/B17978_06_003.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_06_003.jpg)'
- en: 'Precision is the number of times we correctly predicted a **positive value**
    (**TP**) divided by the number of positive values predicted:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度是指我们正确预测**正值**（**TP**）的次数除以预测的正值总数：
- en: '![](img/B17978_06_004.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_06_004.jpg)'
- en: When there is class imbalance, measures such as accuracy and sensitivity can
    give us very different estimates of the performance of our model. An extreme example
    will illustrate this. Chimpanzees sometimes *termite fish*, putting a stick in
    a termite mound with the hopes of catching a few termites. This is only occasionally
    successful. I am no primatologist, but we can perhaps model a successful fishing
    attempt as a function of the size of the stick used, the time of year, and the
    age of the chimpanzee. In our testing data, fishing attempts are only successful
    2% of the time. (This data has been made up for this demonstration.)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在类别不平衡时，如准确率和灵敏度这样的指标可能会给我们关于模型性能的非常不同的估计。一个极端的例子将说明这一点。黑猩猩有时会尝试**白蚁捕捞**，把一根棍子插进蚁丘里，希望能捕获几只白蚁。这只有偶尔会成功。我不是灵长类学家，但我们可以将成功的捕捞尝试建模为使用棍子的大小、年份和时间以及黑猩猩年龄的函数。在我们的测试数据中，捕捞尝试只有2%的时间会成功。（这些数据是为了演示而编造的。）
- en: 'Let’s also say that we build a classification model of successful termite fishing
    that has a sensitivity of 50%. So, if there are 100 fishing attempts in our testing
    data, we would predict only one of the two successful attempts correctly. There
    is also one false positive, where our model predicted successful fishing when
    the fishing failed. This gives us the following confusion matrix:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再假设我们构建了一个成功白蚁捕捞的分类模型，其灵敏度为50%。因此，如果我们测试数据中有100次捕捞尝试，我们只会正确预测其中一次成功的尝试。还有一个假阳性，即我们的模型在捕捞失败时预测了成功的捕捞。这给我们以下混淆矩阵：
- en: '![Figure 6.2 – Successful termite fishing confusion matrix ](img/B17978_06_0021.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 成功白蚁捕捞的混淆矩阵](img/B17978_06_0021.jpg)'
- en: Figure 6.2 – Successful termite fishing confusion matrix
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 成功白蚁捕捞的混淆矩阵
- en: Notice that we get a very high accuracy of 98% – that is, (97+1) / 100\. We
    get high accuracy and low sensitivity because a large percentage of the fishing
    attempts are negative and that is easy to predict. A model that just predicts
    failure always would also have an accuracy of 98%.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们得到了一个非常高的准确率98%——即（97+1）/ 100。我们得到了高准确率和低灵敏度，因为大部分捕捞尝试都是负的，这很容易预测。一个总是预测失败的模型也会有一个98%的准确率。
- en: 'Now, let’s look at these model evaluation measures with real data. We can experiment
    with a **k-nearest neighbors** (**KNN**) model to predict bachelor’s degree attainment
    and evaluate its accuracy, sensitivity, specificity, and precision:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用真实数据来查看这些模型评估指标。我们可以通过实验一个**k最近邻**（**KNN**）模型来预测学士学位的获得，并评估其准确率、灵敏度、特异性和精确率：
- en: 'We will start by loading libraries for encoding and standardizing data, and
    for creating training and testing DataFrames. We will also load scikit-learn’s
    KNN classifier and the `metrics` library:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先加载用于编码和标准化数据的库，以及用于创建训练和测试数据框（DataFrames）的库。我们还将加载scikit-learn的KNN分类器和`metrics`库：
- en: '[PRE0]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we can create training and testing DataFrames and encode and scale the
    data:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以创建训练和测试数据框，并对数据进行编码和缩放：
- en: '[PRE1]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s create a KNN classification model. We will not worry too much about how
    we specify it since we just want to focus on evaluation measures in this section.
    We will use all of the features listed in `feature_cols` We use the predict method
    of the KNN classifier to generate predictions from the testing data:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个KNN分类模型。我们不会太在意如何指定它，因为我们只想关注本节中的评估指标。我们将使用`feature_cols`中列出的所有特征。我们使用KNN分类器的预测方法从测试数据中生成预测：
- en: '[PRE2]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can use scikit-learn to plot a confusion matrix. We will pass the actual
    values in the testing data (`y_test`) and predicted values to the `confusion_matrix`
    method:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用scikit-learn绘制混淆矩阵。我们将传递测试数据中的实际值（`y_test`）和预测值到`confusion_matrix`方法：
- en: '[PRE3]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This generates the following plot:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下图表：
- en: '![Figure 6.3 – Confusion matrix of actual and predicted values ](img/B17978_06_0031.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 实际值和预测值的混淆矩阵](img/B17978_06_0031.jpg)'
- en: Figure 6.3 – Confusion matrix of actual and predicted values
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 实际值和预测值的混淆矩阵
- en: 'We can also just return the true negative, false positive, false negative,
    and true positive counts:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也可以只返回真正的负值、假阳性、假阴性和真正阳性的计数：
- en: '[PRE4]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We now have what we need to calculate accuracy, sensitivity, specificity, and
    precision:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在有了计算准确率、灵敏度、特异性和精确率所需的所有信息：
- en: '[PRE5]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This model has relatively low accuracy, but somewhat better sensitivity; that
    is, it does a better job of identifying those in the testing data who have completed
    a bachelor’s degree than of correctly identifying both degree completers and non-completers
    overall. If we look back at the confusion matrix, we will see that there are a
    fair number of false positives, as our model predicts that 63 individuals in the
    testing data would have a bachelor’s degree who did not.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型相对精度较低，但灵敏度略好；也就是说，它更好地识别了测试数据中完成学士学位的人，而不是正确识别所有学位完成者和未完成者的整体情况。如果我们回顾混淆矩阵，我们会看到有相当数量的假阳性，因为我们的模型预测测试数据中有63个人将会有学士学位，而实际上并没有。
- en: 'We could have also used scikit-learn handy methods for generating these statistics
    directly:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用scikit-learn提供的便捷方法直接生成这些统计数据：
- en: '[PRE6]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Just for comparison, let’s try a random forest classifier and see if we get
    any better results.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 仅为了比较，让我们尝试使用随机森林分类器，看看是否能得到更好的结果。
- en: 'Let’s fit a random forest classifier to the same data and call `confusion_matrix`
    again:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将随机森林分类器拟合到相同的数据，并再次调用`confusion_matrix`：
- en: '[PRE7]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The second model gets us significantly fewer false negatives and more true positives
    than the first model. It is less likely to predict no bachelor’s degree when individuals
    in the test data have completed a bachelor’s degree, and more likely to predict
    a bachelor’s degree when the person has completed one. The main impact of the
    lower FP and higher TP is a significantly higher sensitivity. The second model
    identifies actual positives 89% of the time, compared with 80% for the first model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个模型比第一个模型显著减少了假阴性，并增加了真阳性。它不太可能预测测试数据中的个体没有完成学士学位，而当个人完成了学士学位时，更有可能预测这个人有学士学位。较低的FP和较高的TP的主要影响是灵敏度显著提高。第二个模型有89%的时间识别出实际正值，而第一个模型只有80%。
- en: The measures we have discussed in this section – accuracy, sensitivity, specificity,
    and precision – are worth looking at whenever we are evaluating a classification
    model. But it can be hard to get a good sense of the tradeoffs we are sometimes
    confronted with, between precision and sensitivity, for example. Data scientists
    rely on several standard visualizations to improve our sense of these tradeoffs
    when building classification models. We will examine these visualizations in the
    next section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中讨论的措施——准确性、灵敏度、特异性和精确度——在评估分类模型时都值得一看。但是，例如，在精度和灵敏度之间，我们有时可能会面临难以权衡的情况。数据科学家在构建分类模型时，会依赖几种标准的可视化方法来提高我们对这些权衡的认识。我们将在下一节中探讨这些可视化方法。
- en: Examining CAP, ROC, and precision-sensitivity curves for binary classification
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查二元分类的CAP、ROC和精度-灵敏度曲线
- en: There are several ways to visualize the performance of a binary classification
    model. A relatively straightforward visualization is the **Cumulative Accuracy
    Profile** (**CAP**), which shows the ability of our model to identify in-class,
    or positive, cases. It shows the cumulative cases on the *X*-axis and the cumulative
    positive outcomes on the *Y*-axis. A CAP curve is a good way to see how good a
    job our model does at discriminating in-class observations. (When discussing binary
    classification models, I will use the terms *in-class* and *positive* interchangeably.)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化二元分类模型性能的方法有很多。一种相对直接的可视化方法是**累积准确率曲线**（**CAP**），它显示了我们的模型识别正类（或积极案例）的能力。它显示了X轴上的累积案例和Y轴上的累积积极结果。CAP曲线是了解我们的模型在区分正类观察方面做得如何的好方法。（在讨论二元分类模型时，我将交替使用*正类*和*积极*这两个术语。）
- en: '**Receiver operating characteristic** (**ROC**) curves illustrate the tradeoff
    between model sensitivity (being able to identify positive values) and the false
    positive rate as we adjust the threshold for classifying a positive value. Similarly,
    precision-sensitivity curves show the relationship between the reliability of
    our positive predictions (their precision) and sensitivity (our model’s ability
    to identify positive actual values) as we adjust the threshold.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征**（**ROC**）曲线说明了在调整分类正值的阈值时，模型灵敏度（能够识别正值）与假阳性率之间的权衡。同样，精度-灵敏度曲线显示了在调整阈值时，我们积极预测的可靠性（它们的精度）与灵敏度（我们的模型识别实际正值的能）力之间的关系。'
- en: Constructing CAP curves
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建CAP曲线
- en: Let’s start with CAP curves for our bachelor’s completion KNN model. Let’s also
    compare that with a decision tree model. Again, we will not do much with feature
    selection here. The previous chapter went over feature selection in some detail.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从学士学位完成KNN模型的CAP曲线开始。让我们也将其与决策树模型进行比较。同样，我们在这里不会进行太多的特征选择。上一章详细介绍了特征选择。
- en: 'In addition to curves for our models, CAP curves also have plots of a **random
    model** and a **perfect model** to view for comparison. The random model provides
    no information other than the overall distribution of positive values. The perfect
    model predicts positive values precisely. To illustrate how those plots are drawn,
    we will start with a hypothetical example. Imagine that you sample the first six
    cards of a nicely shuffled deck of playing cards. You create a table with the
    cumulative card total in one column and the number of red cards in the next column.
    It may look something like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们模型的曲线外，CAP曲线还有用于比较的**随机模型**和**完美模型**的图表。随机模型除了提供正值的整体分布信息外，没有其他信息。完美模型精确地预测正值。为了说明这些图表是如何绘制的，我们将从一个假设的例子开始。想象一下，你从一副洗好的牌中抽取前六张牌。你创建一个表格，其中一列是累积牌总数，下一列是红牌的数量。它可能看起来像这样：
- en: '![Figure 6.4 – Sample of playing cards  ](img/B17978_06_0041.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 玩牌样本](img/B17978_06_0041.jpg)'
- en: Figure 6.4 – Sample of playing cards
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 玩牌样本
- en: We can plot a random model based on just our knowledge of the number of red
    cards. The random model has just two points, (0,0) and (6,3), but that is all
    we need.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据我们对红牌数量的了解绘制一个随机模型。随机模型只有两个点，(0,0)和(6,3)，但这就足够了。
- en: 'The perfect model plot requires a bit more explanation. If our model predicted
    red cards perfectly and we sorted by the prediction in descending order, we would
    get *Figure 6.5*. The cumulative in-class count matches the number of cards until
    the red cards have been exhausted, which is 3 in this case. A plot of the cumulative
    in-class total with a perfect model would have two slopes; equal to 1 up until
    the in-class total was reached, and then 0 after that:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 完美模型图表需要更多的解释。如果我们的模型完美预测红牌，并且按预测降序排列，我们会得到*图6.5*。累积in-class count与牌的数量相匹配，直到红牌耗尽，在这个例子中是3张。使用完美模型的累积in-class
    total图表将有两个斜率；在达到in-class total之前等于1，之后为0：
- en: '![Figure 6.5 – Sample of playing cards ](img/B17978_06_005.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 玩牌样本](img/B17978_06_005.jpg)'
- en: Figure 6.5 – Sample of playing cards
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 玩牌样本
- en: 'We now know enough to plot both the random model and the perfect model. The
    perfect model will have three points: (0,0), (in-class count, in-class count),
    and (number of cards, in-class count). In this case, in-class count is `3` and
    the number of cards is `6`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经足够了解如何绘制随机模型和完美模型。完美模型将有三点：(0,0)，(in-class count, in-class count)，和(number
    of cards, in-class count)。在这种情况下，in-class count是`3`，卡片数量是`6`：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This produces the following plot:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 6.6 – CAP with playing card data ](img/B17978_06_006.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – 使用玩牌数据的CAP](img/B17978_06_006.jpg)'
- en: Figure 6.6 – CAP with playing card data
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 使用玩牌数据的CAP
- en: One way to understand the improvement of the perfect model over the random model
    is to consider how many red cards the random model would predict at the midpoint
    – that is, 3 cards. At that point, the random model would predict 1.5 red cards.
    However, the perfect model would predict 3\. (Remember that we have sorted the
    cards by prediction in descending order.)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解完美模型相对于随机模型的改进，可以考虑随机模型在中间点预测多少红牌 – 那就是说，3张牌。在那个点上，随机模型会预测1.5张红牌。然而，完美模型会预测3张。(记住，我们已经按预测顺序降序排列了牌。)
- en: 'Having constructed plots for random and perfect models with made-up data, let’s
    try it with our bachelor’s degree completion data:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用虚构数据构建了随机模型和完美模型的图表后，让我们用我们的学士学位完成数据试试：
- en: 'First, we must import the same modules as in the previous section:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须导入与上一节相同的模块：
- en: '[PRE16]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we load, encode, and scale the NLS bachelor’s degree data:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们加载、编码和缩放NLS学士学位数据：
- en: '[PRE17]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we create `KNeighborsClassifier` and `RandomForestClassifier` instances:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建`KNeighborsClassifier`和`RandomForestClassifier`实例：
- en: '[PRE18]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We are now ready to start plotting our CAP curves. We will start by drawing
    a random model and then a perfect model. These are models that use no information
    (other than the overall distribution of positive values) and that provide perfect
    information, respectively.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始绘制我们的CAP曲线了。我们将首先绘制一个随机模型，然后是一个完美模型。这些模型不使用任何信息（除了正值的整体分布）并且提供完美信息。
- en: We count the number of observations in the test data and the number of positive
    values. We will use (0,0) and (the number of observations, in-class count) to
    draw the random model line. For the perfect model, we will plot a line from (0,0)
    to (in-class count, in-class count) since that model can perfectly discriminate
    in-class values (it is never wrong). It is flat to the right of that point since
    there are no more positive values to find.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算测试数据中的观测数和正值的数量。我们将使用(0,0)和(观测数，类内计数)来绘制随机模型线。对于完美模型，我们将从(0,0)到(类内计数，类内计数)绘制一条线，因为该模型可以完美地区分类内值（它永远不会出错）。在该点右侧是平的，因为再也没有更多的正值可以找到。
- en: 'We will also draw a vertical line at the midpoint and a horizontal line where
    that intersects the random model line. This will be more useful later:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将绘制一条垂直线在中间，以及与随机模型线相交的水平线。这将在以后更有用：
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This produces the following plot:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 6.7 – CAP with just random and perfect models ](img/B17978_06_007.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – 仅使用随机和完美模型的CAP](img/B17978_06_007.jpg)'
- en: Figure 6.7 – CAP with just random and perfect models
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 仅使用随机和完美模型的CAP
- en: Next, we define a function to plot a CAP curve for a model we pass to it. We
    will use the `predict_proba` method to get an array with the probability that
    each observation in the test data is in-class (in this case, has completed a bachelor’s
    degree). Then, we will create a DataFrame with those probabilities and the actual
    target value, sort it by probability in reverse order, and calculate a running
    total of positive actual target values.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数来绘制我们传递给它的模型的CAP曲线。我们将使用`predict_proba`方法来获取一个数组，该数组包含测试数据中每个观测值在类内（在这种情况下，已完成学士学位）的概率。然后，我们将创建一个包含这些概率和实际目标值的DataFrame，按概率降序排序，并计算正实际目标值的累计总和。
- en: 'We will also get the value of the running total at the middle observation and
    draw a horizontal line at that point. Finally, we will plot a line that has an
    array from 0 to the number of observations as *x* values, and the running in-class
    totals as *y* values:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将得到中间观测值的累计值，并在该点绘制一条水平线。最后，我们将绘制一条线，其x值为从0到观测数的数组，y值为累计的类内总数：
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, let’s run the function for the KNN and random forest classifier models
    using the same data:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用相同的数据运行KNN和随机森林分类器模型的函数：
- en: '[PRE21]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This updates our earlier plot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这更新了我们的早期图表：
- en: '![Figure 6.8 – CAP updated with KNN and random forest models ](img/B17978_06_008.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – 使用KNN和随机森林模型的CAP更新](img/B17978_06_008.jpg)'
- en: Figure 6.8 – CAP updated with KNN and random forest models
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 使用KNN和随机森林模型的CAP更新
- en: Not surprisingly, the CAP curves show that our KNN and random forest models
    are better than randomly guessing, but not as good as a perfect model. The question
    is, how much better and how much worse, respectively. The horizontal lines give
    us some idea. A perfect model would have correctly identified 138 positive values
    out of 138 observations. (Recall that the observations are sorted so that the
    observations with the highest likelihood of being positive are first.) The random
    model would have identified 70 (line not shown), while the KNN and random forest
    models would have identified 102 and 103, respectively. Our two models are 74%
    and 75% as good as a perfect model would have been at discriminating positive
    values. Anything between 70% and 80% is considered to be a good model; percentages
    above that are very good, while percentages below that are poor.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，CAP曲线显示我们的KNN和随机森林模型比随机猜测要好，但不如完美模型好。问题是，分别有多好和多差。水平线给我们一些想法。一个完美模型会在138个观测值中正确识别出138个正值。（回想一下，观测值是按最高可能性为正的顺序排序的。）随机模型会识别出70个（线未显示），而KNN和随机森林模型分别会识别出102和103个。我们的两个模型在区分正值方面与完美模型一样好，分别是74%和75%。在70%到80%之间被认为是好的模型；高于这个百分比的模型非常好，而低于这个百分比的模型较差。
- en: Plotting a receiver operating characteristic (ROC) curve
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制接收者操作特征（ROC）曲线
- en: 'ROC curves illustrate the tradeoff between the false positive rate and the
    true positive rate (also known as sensitivity) as we adjust the threshold. We
    should discuss the false positive rate before going further. It is the percentage
    of actual negatives (true negatives plus false positives) that our model falsely
    identifies as positive:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线说明了在调整阈值时，假阳性率与真阳性率（也称为灵敏度）之间的权衡。在进一步讨论之前，我们应该讨论假阳性率。它是模型错误地将实际负例（真负例加上假阳性）识别为正例的百分比：
- en: '![](img/B17978_06_0051.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_06_0051.jpg)'
- en: 'Here, you can see the relationship that the false positive rate has with specificity,
    which was discussed at the beginning of this chapter. The difference is the numerator.
    Specificity is the percentage of actual negatives that our model correctly identifies
    as negative:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到假阳性率与特异性之间的关系，这是在本章开头讨论过的。差异是分子。特异性是我们模型正确地将实际负例识别为负例的百分比：
- en: '![](img/B17978_06_0061.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_06_0061.jpg)'
- en: 'We can also compare the false positive rate with sensitivity, which is the
    percentage of actual positives (true positives plus false negatives) that our
    model correctly identifies as positive:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将假阳性率与灵敏度进行比较，灵敏度是实际正例（真阳性加上假阴性）的百分比，我们模型正确地将它们识别为正例：
- en: '![](img/B17978_06_0071.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_06_0071.jpg)'
- en: We are typically confronted with a tradeoff between sensitivity and the false
    positive rate. We want our models to be able to identify a large percentage of
    the actual positives, but we do not want a problematically high false positive
    rate. What is *problematically high* depends on your context.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常面临灵敏度和假阳性率之间的权衡。我们希望我们的模型能够识别大量实际正例，但我们不希望假阳性率过高。什么是“过高”取决于你的上下文。
- en: 'The tradeoff between sensitivity and the false positive rate is trickier the
    more difficult it is to discriminate between negative and positive cases. We can
    see this with our bachelor’s degree completion model when we plot the predicted
    probabilities:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当区分负例和正例变得更加困难时，灵敏度和假阳性率之间的权衡就变得更加复杂。当我们绘制预测概率时，我们可以通过我们的学士学位完成模型看到这一点：
- en: 'First, let’s fit our random forest classifier again and generate predictions
    and prediction probabilities. We will see that this model predicts that the person
    completes a bachelor’s degree when the predicted probability is greater than `0.500`:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们再次调整我们的随机森林分类器并生成预测和预测概率。我们会看到，当预测概率大于 `0.500` 时，该模型预测这个人将完成学士学位：
- en: '[PRE22]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'It is helpful to compare the distribution of these probabilities with the actual
    class values. We can do this with density plots:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些概率的分布与实际类别值进行比较是有帮助的。我们可以使用密度图来完成这项工作：
- en: '[PRE23]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This produces the following plot:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 6.9 – Density plot of in-class and out-of-class observations ](img/B17978_06_009.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – 在课内和课外观察的密度图](img/B17978_06_009.jpg)'
- en: Figure 6.9 – Density plot of in-class and out-of-class observations
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 在课内和课外观察的密度图
- en: Here, we can see that our model has some trouble discriminating between actual
    positive and negative values since there is a fair bit of in-class and out-of-class
    overlap. A threshold of 0.500 (the left dotted line) gets us a lot of false positives
    since a good portion of the distribution of out-of-class observations (those not
    completing bachelor’s degrees) have predicted probabilities greater than 0.500\.
    If we move the threshold higher, say to 0.650, we get many more false negatives
    since many in-class observations have probabilities lower than 0.65.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的模型在区分实际正例和负例方面有些困难，因为课内和课外重叠的部分相当多。阈值为0.500（左侧虚线）会产生很多假阳性，因为课外观察分布（那些没有完成学士学位的人）中有相当一部分预测概率大于0.500。如果我们提高阈值，比如到0.650，我们会得到更多的假阴性，因为许多课内观察的概率低于0.65。
- en: It is easy to construct a ROC curve based on the testing data and the random
    forest model. The `roc_curve` method returns both the false positive rate (`fpr`)
    and sensitivity (true positive rate, `tpr`) at different thresholds (`ths`).
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于测试数据和随机森林模型，构建ROC曲线很容易。`roc_curve`方法返回不同阈值（`ths`）下的假阳性率（`fpr`）和灵敏度（真阳性率，`tpr`）。
- en: 'First, let’s draw separate false positive rate and sensitivity lines by threshold:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们通过阈值绘制单独的假阳性率和灵敏度线：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This produces the following plot:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 6.10 – False positive rate and sensitivity lines ](img/B17978_06_010.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – 假阳性率和灵敏度线](img/B17978_06_010.jpg)'
- en: Figure 6.10 – False positive rate and sensitivity lines
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 假阳性率和灵敏度线
- en: Here, we can see that increasing the threshold will improve (reduce) our false
    positive rate, but also lower our sensitivity.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到提高阈值将提高（降低）我们的假阳性率，但也会降低我们的灵敏度。
- en: 'Now, let’s draw the associated ROC curve, which plots the false positive rate
    against sensitivity for each threshold:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们绘制相关的ROC曲线，该曲线绘制了每个阈值下的假阳性率与灵敏度：
- en: '[PRE25]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This produces the following plot:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 6.11 – ROC curve with false positive rate and sensitivity ](img/B17978_06_011.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11 – 带有假阳性率和灵敏度的ROC曲线](img/B17978_06_011.jpg)'
- en: Figure 6.11 – ROC curve with false positive rate and sensitivity
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – 带有假阳性率和灵敏度的ROC曲线
- en: The ROC curve indicates that the tradeoff between the false positive rate and
    sensitivity is pretty steep until the false positive rate is about 0.5 or higher.
    Let’s see what that means for the threshold of 0.5 that was used for the random
    forest model predictions.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线表明，假阳性率和灵敏度之间的权衡在假阳性率约为0.5或更高时相当陡峭。让我们看看这对随机森林模型预测中使用的0.5阈值意味着什么。
- en: 'Let’s select an index from the threshold array that is near 0.5, and also one
    near 0.4 and 0.6 for comparison. Then, we will draw vertical lines for the false
    positive rate at those indexes, and horizontal lines for the sensitivity values
    at those indexes:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从阈值数组中选择一个接近0.5的索引，以及一个接近0.4和0.6的索引进行比较。然后，我们将为那些索引绘制垂直线表示假阳性率，以及为那些索引绘制水平线表示灵敏度值：
- en: '[PRE26]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This updates our plot:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这更新了我们的图表：
- en: '![Figure 6.12 – ROC curve with lines for thresholds ](img/B17978_06_012.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12 – 带有阈值线的ROC曲线](img/B17978_06_012.jpg)'
- en: Figure 6.12 – ROC curve with lines for thresholds
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – 带有阈值线的ROC曲线
- en: This illustrates the tradeoff between the false positive rate and sensitivity
    at the 0.5 threshold (the blue dashed line) used for predictions. The ROC curve
    has very little slope with thresholds above 0.5, such as with the 0.6 threshold
    (the green dashed line). So, reducing the threshold from 0.6 to 0.5 results in
    a substantially lower false positive rate (from above 0.8 to below 0.6), but not
    much reduction in sensitivity. However, improving (reducing) the false positive
    rate by reducing the threshold from 0.5 to 0.4 (from the blue to the purple line)
    leads to significantly worse sensitivity. It drops from nearly 90% to just above
    70%.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了在0.5阈值（蓝色虚线）下，预测中假阳性率和灵敏度之间的权衡。ROC曲线在0.5以上的阈值处斜率非常小，例如0.6阈值（绿色虚线）。因此，将阈值从0.6降低到0.5会导致假阳性率显著降低（从超过0.8降低到低于0.6），但灵敏度降低不多。然而，通过将阈值从0.5降低到0.4（从蓝色到紫色线），假阳性率（降低）将导致灵敏度显著下降。它从近90%下降到略高于70%。
- en: Plotting precision-sensitivity curves
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制精度-灵敏度曲线
- en: 'It is often helpful to examine the relationship between precision and sensitivity
    as the threshold is adjusted. Remember that precision tells us the percentage
    of the time we are correct when we predict a positive value:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当调整阈值时，检查精度和灵敏度之间的关系通常很有帮助。记住，精度告诉我们当我们预测一个正值时，我们正确的时间百分比：
- en: '![](img/B17978_06_0081.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_06_0081.jpg)'
- en: We can improve precision by increasing the threshold for classifying a value
    as positive. However, this will likely mean a reduction in sensitivity. As we
    improve how often we are correct when we predict a positive value (precision),
    we will decrease the number of positive values we are able to identify (sensitivity).
    Precision-sensitivity curves, often called precision-recall curves, illustrate
    this tradeoff.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过提高将值分类为正的阈值来提高精度。然而，这可能会意味着灵敏度的降低。随着我们提高预测正值时正确的时间（精度），我们将减少我们能够识别的正值数量（灵敏度）。精度-灵敏度曲线，通常称为精度-召回曲线，说明了这种权衡。
- en: 'Before drawing the precision-sensitivity curve, let’s look at separate precision
    and sensitivity lines plotted against thresholds:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在绘制精度-灵敏度曲线之前，让我们先看看分别针对阈值绘制的精度和灵敏度线：
- en: 'We can get the points for the precision-sensitivity curves with the `precision_recall_curve`
    method. We remove some squirreliness at the highest threshold values, which can
    sometimes happen:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`precision_recall_curve`方法获得精度-灵敏度曲线的点。我们移除最高阈值值的一些不规则性，这有时可能发生：
- en: '[PRE27]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This produces the following plot:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 6.13 – Precision and sensitivity lines ](img/B17978_06_013.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图6.13 – 精确度和灵敏度线](img/B17978_06_013.jpg)'
- en: Figure 6.13 – Precision and sensitivity lines
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – 精确度和灵敏度线
- en: Here, we can see that sensitivity declines more steeply with thresholds above
    0.5\. This decline does not buy us much improved precision beyond the 0.6 threshold.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，当阈值高于0.5时，灵敏度下降更为陡峭。这种下降并没有在0.6阈值以上带来多少精确度的改进。
- en: 'Now, let’s plot sensitivity against precision to view the precision-sensitivity
    curve:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们绘制灵敏度与精确度之间的关系，以查看精确度-灵敏度曲线：
- en: '[PRE28]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This produces the following plot:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下图表：
- en: '![Figure 6.14 – Precision-sensitivity curve ](img/B17978_06_014.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图6.14 – 精确度-灵敏度曲线](img/B17978_06_014.jpg)'
- en: Figure 6.14 – Precision-sensitivity curve
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – 精确度-灵敏度曲线
- en: The precision-sensitivity curve reflects the fact that sensitivity is much more
    responsive to threshold than is precision with this particular model. This means
    that we could decrease the threshold below 0.5 to get greater sensitivity, without
    a significant reduction in precision.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度-灵敏度曲线反映了这样一个事实：对于这个特定的模型，灵敏度对阈值的反应比精确度更敏感。这意味着我们可以将阈值降低到0.5以下，以获得更高的灵敏度，而不会显著降低精确度。
- en: Note
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The choice of threshold is partly a matter of judgment and domain knowledge,
    and is mostly an issue when we have significant class imbalance. However, in [*Chapter
    10*](B17978_10_ePub.xhtml#_idTextAnchor126)*, Logistic Regression* we will explore
    how to calculate an optimal threshold.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值的选择部分是判断和领域知识的问题，并且在存在显著类别不平衡的情况下主要是一个问题。然而，在[*第10章*](B17978_10_ePub.xhtml#_idTextAnchor126)*，逻辑回归*中，我们将探讨如何计算一个最优阈值。
- en: This section, and the previous one, demonstrated how to evaluate binary classification
    models. They showed that model evaluation is not just a thumbs up and thumbs down
    process. It is much more like tasting your batter as you make a cake. We make
    good initial assumptions about our model specification and use the model evaluation
    process to make improvements. This often involves tradeoffs between accuracy,
    sensitivity, specificity, and precision, and modeling decisions that resist one-size-fits-all
    recommendations. These decisions are very much domain-dependent and a matter of
    professional judgment.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本节以及上一节演示了如何评估二元分类模型。它们表明模型评估不仅仅是一个简单的赞成或反对的过程。它更像是在做蛋糕时品尝面糊。我们对模型规格做出良好的初始假设，并使用模型评估过程进行改进。这通常涉及准确度、灵敏度、特异性和精确度之间的权衡，以及抵制一刀切建议的建模决策。这些决策在很大程度上取决于特定领域，并且是专业判断的问题。
- en: The discussion in this section, and most of the techniques, apply as much to
    multiclass modeling. We discuss evaluating multiclass models in the next section.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的讨论以及大多数技术同样适用于多类建模。我们将在下一节讨论如何评估多类模型。
- en: Evaluating multiclass models
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估多类模型
- en: All of the same principles that we used to evaluate binary classification models
    apply to multiclass model evaluation. Computing a confusion matrix is just as
    important, though a fair bit more difficult to interpret. We also still need to
    examine somewhat competing measures, such as precision and sensitivity. This,
    too, is messier than doing so with binary classification.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来评估二元分类模型的所有相同原则也适用于多类模型评估。计算混淆矩阵同样重要，尽管它更难解释。我们还需要检查一些相互竞争的指标，如精确度和灵敏度。这也比二元分类更复杂。
- en: 'Once again, we will work with the NLS degree completion data. We will alter
    the target in this case, from bachelor’s degree completion or not to high school
    completion, bachelor’s degree completion, and post-graduate degree completion:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将使用NLS学位完成数据。在这种情况下，我们将目标从学士学位完成与否更改为高中完成、学士学位完成和研究生学位完成：
- en: 'We will start by loading the necessary libraries. These are the same libraries
    we used in the previous two sections:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先加载必要的库。这些库与前面两节中使用的相同：
- en: '[PRE29]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we will load the NLS degree attainment data, create training and testing
    DataFrames, and encode and scale the data:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载NLS学位达成数据，创建训练和测试数据框，并对数据进行编码和缩放：
- en: '[PRE30]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, we will run a KNN model and predict values for each degree level category:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将运行一个KNN模型，并为每个学位级别类别预测值：
- en: '[PRE31]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can use those predictions to generate a confusion matrix:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用这些预测来生成一个混淆矩阵：
- en: '[PRE32]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This generates the following plot:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下图表：
- en: '![Figure 6.15 – Confusion matrix with a multiclass target ](img/B17978_06_015.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图6.15 – 具有多类目标的混淆矩阵](img/B17978_06_015.jpg)'
- en: Figure 6.15 – Confusion matrix with a multiclass target
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – 具有多类目标的混淆矩阵
- en: It is possible to calculate evaluation measures by hand. Precision is the percentage
    of our in-class predictions that are actually in-class. So, for our prediction
    of high school, it is 48 / (48 + 38 + 8) = 0.51\. Sensitivity for the high school
    class – that is, the percentage of actual values of high school that our model
    predicts – is 48 / (48 + 19 +5) = 0.67\. However, this is fairly tedious. Fortunately,
    scikit-learn can do this for us.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 可以手动计算评估指标。精度是我们预测中实际属于类内的百分比。因此，对于我们的高中预测，它是 48 / (48 + 38 + 8) = 0.51。高中类别的灵敏度——即我们模型预测的实际高中值的百分比——是
    48 / (48 + 19 + 5) = 0.67。然而，这相当繁琐。幸运的是，scikit-learn 可以为我们完成这项工作。
- en: 'We can call the `classification_report` method to get these statistics, passing
    actual and predicted values (remember that recall and sensitivity are the same
    measure):'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过调用 `classification_report` 方法来获取这些统计数据，传递实际和预测值（记住召回率和灵敏度是相同的度量）：
- en: '[PRE33]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In addition to precision and sensitivity rates by class, we get some other statistics.
    The F1-score is the harmonic mean of precision and sensitivity.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 除了按类别计算的精度和灵敏度比率之外，我们还会得到一些其他统计数据。F1 分数是精度和灵敏度的调和平均值。
- en: '![](img/B17978_06_0091.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_06_0091.jpg)'
- en: Here, *p* is precision and *s* is sensitivity.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*p* 代表精度，而 *s* 代表灵敏度。
- en: To get the average precision, sensitivity, and F1-score across classes, we can
    either use the simple average (macro average) or a weighted average that adjusts
    for class size. Using the weighted average, we get precision, sensitivity, and
    F1-score values of 0.49, 0.50, and 0.49, respectively. (Since the classes are
    relatively balanced here, there is not much difference between the macro average
    and the weighted average.)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得类别的平均精度、灵敏度和 F1 分数，我们可以使用简单的平均（宏平均）或调整类别大小的加权平均。使用加权平均，我们得到精度、灵敏度和 F1 分数分别为
    0.49、0.50 和 0.49。（由于这里的类别相对平衡，宏平均和加权平均之间没有太大差异。）
- en: This demonstrates how to extend the evaluation measures we discussed for binary
    classification models to multiclass evaluation. The same concepts and techniques
    apply, though they are more difficult to implement.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这演示了如何将我们讨论的二分类模型评估指标扩展到多类评估。虽然实现起来更困难，但相同的概念和技术同样适用。
- en: So far, we have focused on metrics and visualizations to help us evaluate classification
    models. We have not examined metrics for evaluating regression models yet. These
    metrics can be somewhat more straightforward than those for classification. We
    will discuss them in the next section.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们关注的是指标和可视化，以帮助我们评估分类模型。我们尚未检查评估回归模型的指标。这些指标可能比分类指标更为直接。我们将在下一节中讨论它们。
- en: Evaluating regression models
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估回归模型
- en: Metrics for regression evaluation are typically based on the distance between
    the actual values for the target variable and a model’s predicted values. The
    most common measures – mean squared error, root mean squared error, mean absolute
    error, and R-squared – all track how successfully our predictions capture variation
    in a target.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型评估的指标通常基于目标变量的实际值和模型预测值之间的距离。最常见的指标——均方误差、均方根误差、平均绝对误差和 R 平方——都追踪我们的预测如何成功地捕捉目标变量的变化。
- en: 'The distance between the actual value and our prediction is known as the residual,
    or error. The **mean squared error** (**MSE**) is the mean of the square of the
    residuals:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 实际值和我们的预测值之间的距离被称为残差或误差。**均方误差**（**MSE**）是残差平方的平均值：
- en: '![](img/B17978_06_0101.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_06_0101.jpg)'
- en: Here, ![](img/B17978_06_011.png) is the actual target variable value at the
    ith observation and ![](img/B17978_06_012.png) is our prediction for the target.
    The residuals are squared to handle negative values, where the predicted value
    is higher than the actual value. To return our measurement to a more meaningful
    scale, we often use the square root of MSE. That is known as **root mean squared
    error** (**RMSE**).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![图片](img/B17978_06_011.png) 是第 i 次观察的实际目标变量值，而![图片](img/B17978_06_012.png)
    是我们对目标值的预测。由于预测值高于实际值，残差被平方以处理负值。为了使我们的测量值返回到一个更有意义的尺度，我们通常使用均方误差（MSE）的平方根。这被称为**均方根误差**（**RMSE**）。
- en: Due to the squaring, MSE will penalize larger residuals much more than it will
    smaller residuals. For example, if we have predictions for five observations,
    with one having a residual of 25, and the other four having a residual of 0, we
    will get an MSE of *(0+0+0+0+625)/5 = 125*. However, if all five observations
    had residuals of 5, the MSE would be *(25+25+25+25+25)/5 = 25*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于平方，均方误差（MSE）将对较大的残差进行更多的惩罚，而不是较小的残差。例如，如果我们对五个观测值进行预测，其中一个残差为25，其他四个残差为0，我们将得到均方误差为
    *(0+0+0+0+625)/5 = 125*。然而，如果所有五个观测值的残差都是5，均方误差将是 *(25+25+25+25+25)/5 = 25*。
- en: 'A good alternative to squaring the residuals is to take their absolute value.
    This gives us the mean absolute error:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 将残差的平方作为替代方案是取它们的绝对值。这给我们带来了平均绝对误差：
- en: '![](img/B17978_06_0131.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_06_0131.jpg)'
- en: 'R-squared, also known as the coefficient of determination, is an estimate of
    the proportion of the variation in the target variable captured by our model.
    We square the residuals, as we do when calculating MSE, and divide that by the
    deviation of each actual target value from its sample mean. This gives us the
    still unexplained variation, which we subtract from 1 to get the explained variation:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: R-squared，也称为确定系数，是我们模型捕获目标变量变化的估计比例。我们平方残差，就像我们在计算均方误差（MSE）时做的那样，并将其除以每个实际目标值与其样本均值之间的偏差。这给我们带来了仍然未解释的变异，我们从1中减去它以得到解释的变异：
- en: '![](img/B17978_06_0141.jpg)![](img/B17978_06_0151.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_06_0141.jpg)![](img/B17978_06_0151.jpg)'
- en: Fortunately, scikit-learn makes it easy to generate these statistics. In this
    section, we will build a linear regression model of land temperatures and use
    these statistics to evaluate it. We will work with data from the United States
    National Oceanic and Atmospheric Administration on average annual temperatures,
    elevation, and latitude at weather stations in 2019.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，scikit-learn 使得生成这些统计数据变得容易。在本节中，我们将构建一个关于陆地温度的线性回归模型，并使用这些统计数据来评估它。我们将使用来自美国国家海洋和大气管理局2019年气象站平均年度温度、海拔和纬度的数据。
- en: Note
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The land temperature dataset contains the average temperature readings (in Celsius)
    in 2019 from over 12,000 stations across the world, though the majority of the
    stations are in the United States. The raw data was retrieved from the Global
    Historical Climatology Network integrated database. It has been made available
    for public use by the United States National Oceanic and Atmospheric Administration
    at [https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4](https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 陆地温度数据集包含了2019年来自全球超过12,000个站点的平均温度读数（以摄氏度为单位），尽管大多数站点位于美国。原始数据是从全球历史气候学网络综合数据库中检索的。它已由美国国家海洋和大气管理局在[https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4](https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4)上提供给公众使用。
- en: 'Let’s start building a linear regression model:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建一个线性回归模型：
- en: 'We will start by loading the libraries we need and the land temperatures data.
    We will also create training and testing DataFrames:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先加载所需的库和陆地温度数据。我们还将创建训练和测试数据框：
- en: '[PRE34]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The `latabs` feature is the value of latitude without the North or South indicators;
    so, Cairo, Egypt, at approximately 30 degrees north, and Porto Alegre, Brazil,
    at about 30 degrees south, have the same value.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`latabs` 特征是纬度的值，不带北或南指示符；因此，埃及开罗大约在北纬30度，巴西的波尔图阿雷格里大约在南纬30度，它们具有相同的值。'
- en: 'Now, we scale our data:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们缩放我们的数据：
- en: '[PRE35]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we instantiate a scikit-learn `LinearRegression` object and fit a model
    on the training data. Our target is the annual average temperature (`avgtemp`),
    while the features are latitude (`latabs`) and `elevation`. The `coef_` attribute
    gives us the coefficient for each feature:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实例化一个 scikit-learn `LinearRegression` 对象，并在训练数据上拟合一个模型。我们的目标是年度平均温度 (`avgtemp`)，而特征是纬度
    (`latabs`) 和 `elevation`。`coef_` 属性给我们每个特征的系数：
- en: '[PRE36]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The interpretation of the `latabs` coefficient is that standardized average
    annual temperature will decline by 0.85 for every one standard deviation increase
    in latitude. (The `LinearRegression` module does not return p-values, a measure
    of the statistical significance of the coefficient estimate. You can use `statsmodels`
    instead to see a full summary of an ordinary least squares model.)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`latabs`系数的解释是，标准化平均年温度每增加一个标准差，将下降0.85。（`LinearRegression`模块不返回p值，这是系数估计的统计显著性的度量。你可以使用`statsmodels`来查看普通最小二乘模型的完整摘要。）'
- en: 'Now, we can get predicted values. Let’s also join the returned NumPy array
    with the features and the target from the testing data. Then, we can calculate
    the residuals by subtracting the predicted values from the actual values (`avgtemp`).
    The residuals do not look bad, though there is a little negative skew and excessive
    kurtosis:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以获取预测值。让我们也将测试数据中的特征和目标与返回的NumPy数组结合起来。然后，我们可以通过从实际值（`avgtemp`）中减去预测值来计算残差。尽管存在轻微的负偏斜和过度的峰度，但残差看起来还不错：
- en: '[PRE37]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: It is worth noting that we will be generating predictions and calculated residuals
    in this way most of the time we work with regression models in this book. If you
    feel a little unclear about what we just did in the preceding code block, it may
    be a good idea to go over it again.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在本书中，我们大多数时候在处理回归模型时都会以这种方式生成预测值和计算残差。如果你对前面代码块中我们刚刚做的事情感到有些不清楚，再次回顾一下可能是个好主意。
- en: We should plot the residuals to get a better sense of how they are distributed.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该绘制残差图，以更好地了解它们的分布。
- en: '[PRE38]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This produces the following plot:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 6.16 – Histogram of residuals for the linear regression model ](img/B17978_06_016.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图6.16 – 线性回归模型的残差直方图](img/B17978_06_016.jpg)'
- en: Figure 6.16 – Histogram of residuals for the linear regression model
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – 线性回归模型的残差直方图
- en: This does not look too bad, but we have more positive residuals, where we have
    predicted a lower temperature in the testing data than the actual temperature,
    than negative residuals.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来并不太糟糕，但我们有更多的正残差，在我们预测的测试数据中的温度低于实际温度的情况下，比负残差更多。
- en: 'Plotting our predictions by the residuals may give us a better sense of what
    is happening:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过残差绘制我们的预测可能让我们更好地理解正在发生的情况：
- en: '[PRE39]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This produces the following plot:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 6.17 – Scatterplot of predictions by residuals for the linear regression
    model ](img/B17978_06_017.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17 – 线性回归模型的预测残差散点图](img/B17978_06_017.jpg)'
- en: Figure 6.17 – Scatterplot of predictions by residuals for the linear regression
    model
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 – 线性回归模型的预测残差散点图
- en: This does not look horrible. The residuals hover somewhat randomly around 0. However,
    predictions between 1 and 2 standard deviations are much more likely to be too
    low (to have positive residuals) than too high. Above 2, the predictions are always
    too high (they have negative residuals). This model’s assumption of linearity
    might not be sound. We should explore a couple of the transformations we discussed
    in [*Chapter 4*](B17978_04_ePub.xhtml#_idTextAnchor043), *Encoding, Transforming,
    and Scaling Features*, or try a non-parametric model such as KNN regression.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来并不糟糕。残差在0附近随机波动。然而，在1到2个标准差之间的预测值更有可能过低（具有正残差），而不是过高。超过2，预测值总是过高（它们具有负残差）。这个模型线性假设可能并不合理。我们应该探索我们在[*第四章*](B17978_04_ePub.xhtml#_idTextAnchor043)中讨论的一些转换，或者尝试一个非参数模型，如KNN回归。
- en: It is also likely that extreme values are tugging our coefficients around a
    fair bit. A good next move might be to remove outliers, as we discussed in the
    *Identifying extreme values and outliers* section of [*Chapter 1*](B17978_01_ePub.xhtml#_idTextAnchor014),
    *Examining the Distribution of Features and Targets*. We will not do that here,
    however.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能极端值在相当大的程度上拉动了我们的系数。一个不错的下一步可能是移除异常值，正如我们在[*第一章*](B17978_01_ePub.xhtml#_idTextAnchor014)的*识别极端值和异常值*部分所讨论的，*检查特征和目标的分布*。然而，我们在这里不会这么做。
- en: 'Let’s look at some evaluation measures. This can easily be done with scikit-learn’s
    `metrics` library. We can call the same function to get RMSE as MSE. We just need
    to set the squared parameter to `False`:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看一些评估指标。这可以通过scikit-learn的`metrics`库轻松完成。我们可以调用相同的函数来获取RMSE作为MSE。我们只需要将平方参数设置为`False`：
- en: '[PRE40]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: An MSE of less than 0.2 of a standard deviation and an MAE of less than 0\.
    3 of a standard deviation look pretty decent, especially for such a sparse model.
    An R-squared above 80% is also fairly promising.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差以下0.2的均方误差（MSE）和标准差以下0.3的绝对误差（MAE）看起来相当不错，尤其是对于这样一个稀疏模型。R-squared超过80%也是相当有希望的。
- en: 'Let’s see what we get if we use a KNN model instead:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看如果我们使用KNN模型会得到什么结果：
- en: '[PRE41]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This model is actually an improvement in both MAE and R-squared.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型实际上在MAE和R-squared方面都有所改进。
- en: 'We should also take a look at the residuals again:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也应该再次审视残差：
- en: '[PRE42]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This produces the following plot:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 6.18 – Scatterplot of predictions by residuals for the KNN model ](img/B17978_06_018.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图6.18 – KNN模型的预测残差散点图](img/B17978_06_018.jpg)'
- en: Figure 6.18 – Scatterplot of predictions by residuals for the KNN model
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 – KNN模型的预测残差散点图
- en: This plot of the residuals looks better as well. There are no parts of the target’s
    distribution where we are much more likely to over-predict or under-predict.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这个残差图表看起来也好多了。在目标分布的任何部分，我们都不太可能过度预测或低估。
- en: This section has introduced key measures for evaluating regression models, and
    how to interpret them. It has also demonstrated how visualizations, particularly
    of model residuals, can improve that interpretation.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了评估回归模型的关键措施及其解释方法。同时，还展示了如何通过可视化，尤其是模型残差的可视化，来提高这种解释的准确性。
- en: However, we have been limited so far, in both our use of regression and classification
    measures, by how we have constructed our training and testing DataFrames. What
    if, for some reason, the testing data is unusual in some way? More generally,
    what is our basis for concluding that our evaluation measures are accurate? We
    can be more confident in these measures if we use K-fold cross-validation, which
    we will cover in the next section.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，到目前为止，我们在使用回归和分类度量时都受到了我们构建的训练和测试数据框的限制。如果，出于某种原因，测试数据在某些方面不寻常呢？更普遍地说，我们基于什么结论认为我们的评估措施是准确的？如果我们使用K折交叉验证，我们可以更有信心地使用这些措施，我们将在下一节中介绍。
- en: Using K-fold cross-validation
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用K折交叉验证
- en: So far, we have held back 30% of our data for validation. This is not a bad
    strategy. It prevents us from peeking ahead to the testing data as we train our
    model. However, this approach does not take full advantage of all the available
    data, either for training or for testing. If we use K-fold cross-validation instead,
    we can use all of our data while also avoiding data leakage. Perhaps that seems
    too good to be true. But it’s not because of a neat little trick.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经保留了30%的数据用于验证。这不是一个坏策略。它防止我们在训练模型时提前查看测试数据。然而，这种方法并没有充分利用所有可用数据，无论是用于训练还是测试。如果我们使用K折交叉验证，我们就可以使用所有数据，同时避免数据泄露。也许这听起来太好了，但事实并非如此，这并不是因为一个巧妙的小技巧。
- en: '**K-fold cross-validation** trains our model on all but one of the K folds,
    or parts, leaving one out for testing. This is repeated *k* times, each time excluding
    a different fold for testing. Performance metrics are then based on the average
    scores across the K folds.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**K折交叉验证**在K个折（或部分）中的所有但一个上训练我们的模型，留出一个用于测试。这重复*k*次，每次排除一个不同的折用于测试。性能指标是基于K个折的平均分数。'
- en: Before we start, though, we need to think again about the possibility of data
    leakage. If we scale all of the data that we will use to train our model and then
    split it up into folds, we will be using information from all the folds in our
    training. To avoid this, we need to do the scaling, as well as any other Preprocessing,
    on just the training folds for each iteration. While we could do this manually,
    scikit-learn’s `pipeline` library can do much of this work for us. We will go
    over how to use pipelines for cross-validation in this section.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们还需要再次考虑数据泄露的可能性。如果我们对我们将用于训练模型的所有数据进行缩放，然后将其分成折，我们将在训练中使用所有折的信息。为了避免这种情况，我们需要在每个迭代中仅对训练折进行缩放，以及进行任何其他预处理。虽然我们可以手动完成这项工作，但scikit-learn的`pipeline`库可以为我们做很多这项工作。我们将在本节中介绍如何使用管道进行交叉验证。
- en: 'Let’s try evaluating the two models we specified in the previous section using
    K-fold cross-validation. While we are at it, let’s also see how well a random
    forest regressor may work:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用K折交叉验证评估上一节中指定的两个模型。同时，我们也来看看随机森林回归器可能表现如何：
- en: 'In addition to the libraries we have worked with so far, we need scikit-learn’s`make_pipeline`,
    `cross_validate`, and `Kfold` libraries:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了我们迄今为止已经使用的库之外，我们还需要scikit-learn的`make_pipeline`、`cross_validate`和`Kfold`库：
- en: '[PRE43]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We load the land temperatures data again and create training and testing DataFrames.
    We still want to leave some data out for final validation, but this time, we will
    only leave out 10%. We will do both training and testing with the remaining 90%:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们再次加载陆地温度数据并创建训练和测试DataFrame。我们仍然想留出一些数据用于最终验证，但这次，我们只留出10%。我们将使用剩余的90%进行训练和测试：
- en: '[PRE44]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we create a `KFold` object and indicate that we want five folds and for
    the data to be shuffled (shuffling the data is a good idea if it is not already
    sorted randomly):'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们创建一个`KFold`对象，并指出我们想要五个折，并且数据要打乱（如果数据尚未随机排序，打乱数据是一个好主意）：
- en: '[PRE45]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we define a function to create a pipeline. The function then runs `cross_validate`,
    which takes the pipeline and the `KFold` object we created earlier:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数来创建一个管道。然后，该函数运行`cross_validate`，它接受管道和我们之前创建的`KFold`对象：
- en: '[PRE46]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, we are ready to call the `getscores` function for the linear regression,
    random forest regression, and KNN regression models:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备调用线性回归、随机森林回归和KNN回归模型的`getscores`函数：
- en: '[PRE47]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can print the `scorelist` list to see our results:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将`scorelist`列表打印出来查看我们的结果：
- en: '[PRE48]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The KNN regressor model performs better than either the linear regression or
    random forest regression model, based on R-squared. The random forest regressor
    also has a significant disadvantage in that it has a much longer fit time.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 根据R-squared值，KNN回归模型比线性回归或随机森林回归模型表现更好。随机森林回归模型也有一个显著的缺点，那就是它的拟合时间要长得多。
- en: Preprocessing data with pipelines
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用管道预处理数据
- en: We just scratched the surface of what we can do with scikit-learn pipelines
    in the previous section. We often need to fold all of our Preprocessing and feature
    engineering into a pipeline, including scaling, encoding, and handling outliers
    and missing values. This can be complicated as different features may need to
    be handled differently. We may need to impute the median for missing values with
    numeric features and the most frequent value for categorical features. We may
    also need to transform our target variable. We will explore how to do that in
    this section.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们只是触及了scikit-learn管道可以做的事情的表面。我们经常需要将所有的预处理和特征工程折叠到一个管道中，包括缩放、编码以及处理异常值和缺失值。这可能很复杂，因为不同的特征可能需要不同的处理。我们可能需要用数值特征的中间值和分类特征的众数来填充缺失值。我们可能还需要转换我们的目标变量。我们将在本节中探讨如何做到这一点。
- en: 'Follow these steps:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤：
- en: 'We will start by loading the libraries we have already worked with in this
    chapter. Then, we will add the `ColumnTransformer` and `TransformedTargetRegressor`
    classes. We will use those classes to transform our features and target, respectively:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先加载本章中已经使用过的库。然后，我们将添加`ColumnTransformer`和`TransformedTargetRegressor`类。我们将使用这些类分别转换我们的特征和目标：
- en: '[PRE49]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The column transformer is quite flexible. We can even use it with the Preprocessing
    functions that we have defined ourselves. The following code block imports the
    `OutlierTrans` class from the `preprocfunc` module in the `helperfunctions` subfolder:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列转换器非常灵活。我们甚至可以使用我们自定义的预处理函数。以下代码块从`helperfunctions`子文件夹中的`preprocfunc`模块导入`OutlierTrans`类：
- en: '[PRE50]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The `OutlierTrans` class identifies extreme values by distance from the interquartile
    range. This is a technique we demonstrated in [*Chapter 3*](B17978_03_ePub.xhtml#_idTextAnchor034),
    *Identifying and Fixing Missing Values*.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`OutlierTrans`类通过距离四分位数范围来识别极端值。这是我们演示过的技术，见[*第3章*](B17978_03_ePub.xhtml#_idTextAnchor034)，*识别和修复缺失值*。'
- en: To work in a scikit-learn pipeline, our class has to have fit and transform
    methods. We also need to inherit the `BaseEstimator` and `TransformerMixin` classes.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 要在scikit-learn管道中工作，我们的类必须具有fit和transform方法。我们还需要继承`BaseEstimator`和`TransformerMixin`类。
- en: 'In this class, almost all of the action happens in the `transform` method.
    Any value that is more than 1.5 times the interquartile range above the third
    quartile or below the first quartile is assigned missing:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，几乎所有操作都在`transform`方法中完成。任何超过第三四分位数1.5倍或低于第一四分位数的值都被分配为缺失值：
- en: '[PRE51]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Our `OutlierTrans` class can be used later in our pipeline in the same way we
    used `StandardScaler` in the previous section. We will do that later.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`OutlierTrans`类可以在我们的管道中以与我们在上一节中使用`StandardScaler`相同的方式使用。我们将在稍后这样做。
- en: Now, we are ready to load the data that needs to be processed. We will work
    with the NLS weekly wage data in this section. Weekly wages will be our target,
    and we will use high school GPA, mother’s and father’s highest grade completed,
    parent income, gender, and whether the individual completed a bachelor’s degree
    as features.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好加载需要处理的数据。在本节中，我们将使用NLS每周工资数据。每周工资将是我们的目标，我们将使用高中GPA、母亲和父亲最高学历、家庭收入、性别以及个人是否完成学士学位作为特征。
- en: 'We will create lists of features to handle in different ways here. This will
    be helpful later when we instruct our pipeline to carry out different operations
    on numerical, categorical, and binary features:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个特征列表，以不同的方式处理这些特征。这将在我们指导管道对数值、分类和二进制特征执行不同操作时很有帮助：
- en: '[PRE52]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Let’s look at some descriptive statistics. Some variables have over a thousand
    missing values (`gpascience`, `gpaenglish`, `gpamath`, `gpaoverall`, and `parentincome`):'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看一些描述性统计。一些变量有超过一千个缺失值（`gpascience`、`gpaenglish`、`gpamath`、`gpaoverall`和`parentincome`）：
- en: '[PRE53]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Now, we can set up a column transformer. First, we will create pipelines for
    handling numerical data (`standtrans`), categorical data, and binary data.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以设置一个列转换器。首先，我们将为处理数值数据（`standtrans`）、分类数据和二进制数据创建管道。
- en: For the numerical data, we want to assign outlier values as missing. Here, we
    will pass a value of `2` to the threshold parameter of `OutlierTrans`, indicating
    that we want values two times the interquartile range above or below that range
    to be set to missing. Recall that the default is 1.5, so we are being somewhat
    more conservative.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值数据，我们希望将异常值视为缺失。在这里，我们将`2`这个值传递给`OutlierTrans`的阈值参数，表示我们希望将高于或低于该范围两倍四分位距的值设置为缺失。记住，默认值是1.5，所以我们相对保守一些。
- en: 'Then, we will create a `ColumnTransformer` object, passing to it the three
    pipelines we just created, and indicating which features to use with which pipeline:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个`ColumnTransformer`对象，将其传递给刚刚创建的三个管道，并指明使用哪个管道来处理哪些特征：
- en: '[PRE54]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Now, we can add the column transformer to a pipeline that also includes the
    linear model that we would like to run. We will add KNN imputation to the pipeline
    to handle missing values.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以将列转换器添加到包含我们想要运行的线性模型的管道中。我们将向管道中添加KNN插补来处理缺失值。
- en: 'We also need to scale the target, which cannot be done in our pipeline. We
    will use scikit-learn’s `TransformedTargetRegressor` for that. We will pass the
    pipeline we just created to the target regressor’s `regressor` parameter:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要对目标进行缩放，这在我们管道中无法完成。我们将使用scikit-learn的`TransformedTargetRegressor`来完成这个任务。我们将刚刚创建的管道传递给目标回归器的`regressor`参数：
- en: '[PRE55]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Let’s do K-fold cross validation using this pipeline. We can pass our pipeline,
    via the target regressor, `ttr`, to the `cross_validate` function:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用这个管道进行K折交叉验证。我们可以通过目标回归器`ttr`将我们的管道传递给`cross_validate`函数：
- en: '[PRE56]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: These scores are not very good, though that was not quite the point of this
    exercise. The key takeaway here is that we typically want to fold most of the
    Preprocessing we will do into a pipeline. This is the best way to avoid data leakage.
    The column transformer is an extremely flexible tool, allowing us to apply different
    transformations to different features.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些分数并不理想，但这并不是这次练习的真正目的。关键要点是我们通常希望将大部分预处理工作整合到管道中。这是避免数据泄露的最佳方式。列转换器是一个极其灵活的工具，允许我们对不同的特征应用不同的转换。
- en: Summary
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced key model evaluation measures and techniques so that
    they will be familiar when we make extensive use of them, and extend them, in
    the remaining chapters of this book. We examined the very different approaches
    to evaluation for classification and regression models. We also explored how to
    use visualizations to improve our analysis of our predictions. Finally, we used
    pipelines and cross-validation to get reliable estimates of model performance.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了关键模型评估指标和技术，以便在本书的剩余章节中广泛使用和扩展时，它们将变得熟悉。我们研究了分类和回归模型评估的非常不同的方法。我们还探讨了如何使用可视化来改进我们对预测的分析。最后，我们使用管道和交叉验证来获取模型性能的可靠估计。
- en: I hope this chapter also gave you a chance to get used to the general approach
    of this book going forward. Although a large number of algorithms will be discussed
    in the remaining chapters, we will continue to surface the Preprocessing issues
    we have discussed in the first few chapters. We will discuss the core concepts
    of each algorithm, of course. But, in a true *hands-on* fashion, we will also
    deal with the messiness of real-world data. Each chapter will go from relatively
    raw data to feature engineering to model specification and model evaluation, relying
    heavily on scikit-learn’s pipelines to pull it all together.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这一章也给了你一个机会去适应这本书接下来的一般方法。尽管在接下来的章节中我们将讨论大量算法，但我们仍将继续探讨我们在前几章中讨论的预处理问题。当然，我们将讨论每个算法的核心概念。但是，以真正的*动手实践*方式，我们还将处理现实世界数据的杂乱无章。每一章将从相对原始的数据开始，到特征工程，再到模型指定和模型评估，高度依赖scikit-learn的管道来整合所有内容。
- en: We will discuss regression algorithms in the next few chapters – those algorithms
    that allow us to model a continuous target. We will explore some of the most popular
    regression algorithms – linear regression, support vector regression, K-nearest
    neighbors regression, and decision tree regression. We will also consider making
    modifications to regression models that address underfitting and overfitting,
    including nonlinear transformations and regularization.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，我们将讨论回归算法——那些允许我们建模连续目标的算法。我们将探讨一些最受欢迎的回归算法——线性回归、支持向量回归、K最近邻回归和决策树回归。我们还将考虑对回归模型进行修改，以解决欠拟合和过拟合问题，包括非线性变换和正则化。
