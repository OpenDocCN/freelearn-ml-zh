["```py\nimport math\nimport os, gc\nimport random\nimport mldatasets\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nimport torchvision\nimport torch\nimport pytorch_lightning as pl\nimport efficientnet_pytorch\nfrom torchinfo import summary\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import ScalarMappable \nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\nimport cv2\nfrom tqdm.notebook import tqdm\nfrom captum import attr \n```", "```py\ndataset_file = \"garbage_dataset_sample\"\ndataset_url = f\"https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/raw/main/datasets/{dataset_file}.zip\"\ntorchvision.datasets.utils.download_url(dataset_url, \".\")\ntorchvision.datasets.utils.extract_archive(f\"{dataset_file}.zip\",\\\n                                           remove_finished=True) \n```", "```py\nX_train, norm_mean = (0.485, 0.456, 0.406)\nnorm_std  = (0.229, 0.224, 0.225)\ntransform = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(norm_mean, norm_std),\n    ]\n)\ntrain_data = torchvision.datasets.ImageFolder(\n    f\"{dataset_file}/train\", transform\n)\nval_data = torchvision.datasets.ImageFolder(\n    f\"{dataset_file}/validation\", transform\n)\ntest_data = torchvision.datasets.ImageFolder(\n    f\"{dataset_file}/test\", transform\n)\ntest_400_data = torchvision.datasets.ImageFolder(\n    f\"{dataset_file}/test_400\", transform\n) \n```", "```py\nprint(f\"# Training Samples:    \\t{len(train_data)}\")\nprint(f\"# Validation Samples:  \\t{len(val_data)}\")\nprint(f\"# Test Samples:        \\t{len(test_data)}\")\nprint(f\"Sample Dimension:      \\t{test_data[0][0].shape}\")\nprint(\"=\"*50)\nprint(f\"# Test 400 Samples:    \\t{len(test_400_data)}\")\nprint(f\"# 400 Sample Dimension:\\t{test_400_data[0][0].shape}\") \n```", "```py\n# Training Samples:        3724\n# Validation Samples:      931\n# Test Samples:            120\nSample Dimension:          torch.Size([3, 224, 224])\n==================================================\n# Test 400 Samples:        120 \n# 400 Sample Dimension:    torch.Size([3, 400, 400]) \n```", "```py\nlabels_l = ['battery', 'biological', 'brown-glass', 'cardboard',\\\n            'clothes', 'green-glass', 'metal', 'paper', 'plastic',\\\n            'shoes', 'trash', 'white-glass'] \n```", "```py\ntensor, label = test_400_data[0]\nimg = mldatasets.tensor_to_img(tensor, norm_std, norm_mean)\nplt.figure(figsize=(5,5))\nplt.title(labels_l[label], fontsize=16)\nplt.imshow(img)\nplt.show() \n0) from the higher resolution version of the test dataset (test_400_data) and extracting the tensor and label portion from it. Then, we are using the convenience function tensor_to_img to convert the PyTorch tensor to a numpy array but also reversing the standardization that had been previously performed on the tensor. Then, we plot the image with matplotlib's imshow and use the labels_l list to convert the label into a string, which we print in the title. The result can be seen in *Figure 7.2*:\n```", "```py\ny_test = np.array([l for _, l in test_data])\ny_val = np.array([l for _, l in val_data])\nohe = OneHotEncoder(sparse=False).\\\n              fit(np.array(y_test).reshape(-1, 1)) \n```", "```py\nrand = 42\nos.environ['PYTHONHASHSEED']=str(rand)\nnp.random.seed(rand)\nrandom.seed(rand)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device == 'cuda':\n    torch.cuda.manual_seed(rand)\nelse:\n    torch.manual_seed(rand) \n```", "```py\nplt.subplots(figsize=(14,10))\nfor c, category in enumerate(labels_l):\n    plt.subplot(3, 4, c+1)\n    plt.title(labels_l[c], fontsize=12)\n    idx = np.random.choice(np.where(y_test==c)[0], 1)[0]\n    im = mldatasets.tensor_to_img(test_data[idx][0], norm_std,\\\n                                  norm_mean)\n    plt.imshow(im, interpolation='spline16')\n    plt.axis(\"off\")\nplt.show() \n```", "```py\nclass **EfficientLite**(pl.LightningModule):\n    def __init__(self, lr: float, num_class: int,\\\n                 pretrained=\"efficientnet-b0\", *args, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = efficientnet_pytorch.EfficientNet.\\\n                                      from_pretrained(pretrained)\n        in_features = self.model._fc.in_features\n        self.model._fc = torch.nn.Linear(in_features, num_class)\n    def forward(self, x):\n        return self.model(x)\n    def predict(self, dataset):\n        self.model.eval()\n        device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n                              else \"cpu\")\n        with torch.no_grad():\n            if isinstance(dataset, np.ndarray):\n                if len(dataset.shape) == 3:\n                    dataset = np.expand_dims(dataset, axis=0)\n                dataset = [(x,0) for x in dataset]\n            loader = torch.utils.data.DataLoader(dataset,\\\n                                                 batch_size=32)\n            probs = None\n        for X_batch, _ in tqdm(loader):\n            X_batch = X_batch.to(device, dtype=torch.float32)\n            logits_batch =  self.model(X_batch)\n            probs_batch = torch.nn.functional.softmax(logits_batch,\\\n                                       dim=1).cpu().detach().numpy()\n            if probs is not None:\n                probs = np.concatenate((probs, probs_batch))\n            else:\n                probs = probs_batch\n            clear_gpu_cache()\n        return probs \n```", "```py\nmodel_weights_file = **\"garbage-finetuned-efficientnet-b4\"**\nmodel_url = f\"https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/raw/main/models/{model_weights_file}.ckpt\"\ntorchvision.datasets.utils.download_url(model_url, \".\")\ngarbage_mdl = EfficientLite.load_from_checkpoint(\n    f\"{model_weights_file}.ckpt\"\n)\ngarbage_mdl = garbage_mdl.to(device).eval()\nprint(summary(garbage_mdl)) \n```", "```py\n=======================================================================\nLayer (type:depth-idx) Param # =======================================================================\nEfficientLite -- \n├─EfficientNet: 1-1 – \n│ └─Conv2dStaticSamePadding: 2-1 1,296 \n│ │ └─ZeroPad2d: 3-1 – \n│ └─BatchNorm2d: 2-2 96 \n│ └─ModuleList: 2-3 – \n│ │ └─MBConvBlock: 3-2 2,940 \n│ │ └─MBConvBlock: 3-3 1,206 \n│ │ └─MBConvBlock: 3-4 11,878 \n│ │ └─MBConvBlock: 3-5 18,120 \n│ │ └─MBConvBlock: 3-6 18,120 \n│ │ └─MBConvBlock: 3-7 18,120 \n│ │ └─MBConvBlock: 3-8 25,848 \n│ │ └─MBConvBlock: 3-9 57,246 \n│ │ └─MBConvBlock: 3-10 57,246 \n│ │ └─MBConvBlock: 3-11 57,246 \n│ │ └─MBConvBlock: 3-12 70,798 \n│ │ └─MBConvBlock: 3-13 197,820 \n│ │ └─MBConvBlock: 3-14 197,820 \n│ │ └─MBConvBlock: 3-15 197,820 \n│ │ └─MBConvBlock: 3-16 197,820 \n│ │ └─MBConvBlock: 3-17 197,820 \n│ │ └─MBConvBlock: 3-18 240,924 \n│ │ └─MBConvBlock: 3-19 413,160 \n│ │ └─MBConvBlock: 3-20 413,160 \n│ │ └─MBConvBlock: 3-21 413,160 \n│ │ └─MBConvBlock: 3-22 413,160 \n│ │ └─MBConvBlock: 3-23 413,160 \n│ │ └─MBConvBlock: 3-24 520,904 \n│ │ └─MBConvBlock: 3-25 1,159,332 \n│ │ └─MBConvBlock: 3-26 1,159,332 \n│ │ └─MBConvBlock: 3-27 1,159,332 \n│ │ └─MBConvBlock: 3-28 1,159,332 \n│ │ └─MBConvBlock: 3-29 1,159,332 \n│ │ └─MBConvBlock: 3-30 1,159,332 \n│ │ └─MBConvBlock: 3-31 1,159,332 \n│ │ └─MBConvBlock: 3-32 1,420,804 \n│ │ └─MBConvBlock: 3-33 3,049,200 \n│ └─Conv2dStaticSamePadding: 2-4 802,816 \n│ │ └─Identity: 3-34 – \n│ └─BatchNorm2d: 2-5 3,584 \n│ └─AdaptiveAvgPool2d: 2-6 – \n│ └─Dropout: 2-7 – \n│ └─Linear: 2-8 21,516 \n│ └─MemoryEfficientSwish: 2-9 \n======================================================================\nTotal params: 17,570,132 \nTrainable params: 17,570,132 \nNon-trainable params: 0 ====================================================================== \n```", "```py\ny_val_pred, y_val_prob = mldatasets.evaluate_multiclass_mdl(\n    garbage_mdl, val_data,\\\n    class_l=labels_l, ohe=ohe, plot_roc=False\n) \n```", "```py\nplot_roc=True) but only the averages, and not on a class-by-class basis (plot_roc_class=False) because there are only four pictures per class. Given the small number of samples, we can display the numbers in the confusion matrix rather than percentages (pct_matrix=False):\n```", "```py\ny_test_pred, y_test_prob = mldatasets.evaluate_multiclass_mdl(\n    garbage_mdl, test_data,\\\n    class_l=labels_l, ohe=ohe,\\\n    plot_roc=True, plot_roc_class=False, pct_matrix=False\n) \ngenerated the ROC curve in *Figure 7.7*, the confusion matrix in *Figure 7.8*, and the classification report in *Figure 7.9*:\n```", "```py\npreds_df = pd.DataFrame({'y_true':[labels_l[o] for o in y_test],\\\n                         'y_pred':y_test_pred})\nprobs_df = pd.DataFrame(y_test_prob*100).round(1)\nprobs_df.loc['Total']= probs_df.sum().round(1)\nprobs_df.columns = labels_l\nprobs_df = probs_df.sort_values('Total', axis=1, ascending=False)\nprobs_df.drop(['Total'], axis=0, inplace=True)\nprobs_final_df = probs_df.iloc[:,0:12]\npreds_probs_df = pd.concat([preds_df, probs_final_df], axis=1) \n```", "```py\nnum_cols_l = list(preds_probs_df.columns[2:])\nnum_fmt_dict = dict(zip(num_cols_l, [\"{:,.1f}%\"]*len(num_cols_l)))\npreds_probs_df[\n    (preds_probs_df.y_true!=preds_probs_df.y_pred)\n    | (preds_probs_df.y_true.isin(['battery', 'white-glass']))\n].style.format(num_fmt_dict).apply(\n    lambda x: ['background: lightgreen' if (x[0] == x[1])\\\n                else '' for i in x], axis=1\n).apply(\n    lambda x: ['background: orange' if (x[0] != x[1] and\\\n                x[1] == 'metal' and x[0] == 'battery')\\\n                else '' for i in x], axis=1\n).apply(\n    lambda x: ['background: yellow' if (x[0] != x[1] and\\\n                                        x[0] == 'plastic')\\\n                else '' for i in x], axis=1\n).apply(\n    lambda x: ['font-weight: bold' if isinstance(i, float)\\\n                                                 and i >= 50\\\n                else '' for i in x], axis=1\n).apply(\n    lambda x: ['color:transparent' if i == 0.0\\\n                else '' for i in x], axis=1) \nFigure 7.10. We can tell by the highlights which are the metal false positives and the plastic false negatives, as well as which would be the true positives: #0-6 for battery, and #110-113 and #117-119 for white glass:\n```", "```py\nplastic_FN_idxs = preds_df[\n    (preds_df['y_true'] !=preds_df['y_pred'])\n    & (preds_df['y_true'] == 'plastic')\n].index.to_list()\nmetal_FP_idxs = preds_df[\n    (preds_df['y_true'] != preds_df['y_pred'])\n    & (preds_df['y_pred'] == 'metal')\n    & (preds_df['y_true'] == 'battery')\n].index.to_list()\nbattery_TP_idxs = preds_df[\n    (preds_df['y_true'] ==preds_df['y_pred'])\n    & (preds_df['y_true'] == 'battery')\n].index.to_list()\nwglass_TP_idxs = preds_df[\n    (preds_df['y_true'] == preds_df['y_pred'])\n    & (preds_df['y_true'] == 'white-glass')\n].index.to_list() \n```", "```py\nconv_layers = []\nmodel_children = list(garbage_mdl.model.children())\nfor model_child in model_children:\n    if (type(model_child) ==\\\n                efficientnet_pytorch.utils.Conv2dStaticSamePadding):\n        conv_layers.append(model_child)\n    elif (type(model_child) == torch.nn.modules.container.ModuleList):\n        module_children = list(model_child.children())\n        module_convs = []\n        for module_child in module_children:\n            module_convs.append(list(module_child.children())[0])\n        conv_layers.extend(module_convs[:6])\nprint(conv_layers) \n```", "```py\nidx = battery_TP_idxs[0]\ntensor = test_data[idx][0][None, :].to(device)\nlabel = y_test[idx]\nmethod = attr.LayerActivation(garbage_mdl, conv_layers[layer]\nattribution = method.attribute(tensor).detach().cpu().numpy()\nprint(attribution.shape) \ntensor for the first battery true positive (battery_TP_idxs[0]). Then, it initializes the LayerActivation attribution method with the model (garbage_mdl) and the first convolutional layer (conv_layers[0]). Using the attribute function, it creates an attribution with this method. For the shape of the attribution, we should get (1, 48, 112, 112). The tensor was for a single image, so it makes sense that the first number is a one. The next number corresponds to the number of filters, followed by the width and height dimensions of each filter. Regardless of the kind of attribution, the numbers inside each attribution relate to how a pixel in the input is seen by the model. Interpretation varies according to the method. However, generally, it is interpreted that higher numbers mean more of an impact on the outcome, but attributions may also have negative numbers, which mean the opposite.\n```", "```py\ncbinary_cmap = LinearSegmentedColormap.from_list('custom binary',\n                                                 [(0, '#ffffff'),\n                                                  (0.25, '#777777'),\n                                                  (1, '#000000')]) \n```", "```py\nfilter = 0\nfilter_attr = attribution[0,filter]\nfilter_attr = mldatasets.apply_cmap(filter_attr, cbinary_cmap, 'positive')\ny_true = labels_l[label]\ny_pred = y_test_pred[idx]\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nfig.suptitle(f\"Actual label: {y_true}, Predicted: {y_pred}\", fontsize=16)\nax.set_title(\n    f\"({method.get_name()} Attribution for Filter #{filter+1} for\\\n        Convolutional Layer #{layer+1})\",\n    fontsize=12\n)\nax.imshow(filter_attr)\nax.grid(False)\nfig.colorbar(\n    ScalarMappable(norm='linear', cmap=cbinary_cmap),\n    ax=ax,\n    orientation=\"vertical\"\n)\nplt.show() \nFigure 7.11:\n```", "```py\nfor l, layer in enumerate(conv_layers):\n    layer = conv_layers[l]\n    method = attr.LayerActivation(garbage_mdl, layer)\n    for idx in battery_TP_idxs:\n        orig_img = mldatasets.tensor_to_img(test_400_data[idx][0],\\\n                                            norm_std, norm_mean,\\\n                                            to_numpy=True)\n        tensor = test_data[idx][0][None, :].to(device)\n        label = int(y_test[idx])\n        attribution = method.attribute(tensor).detach().cpu().numpy()\n        viz_img =  mldatasets.**create_attribution_grid**(attribution,\\\n                            cmap='copper', cmap_norm='positive')\n        y_true = labels_l[label]\n        y_pred = y_test_pred[idx]\n        probs_s = probs_df.loc[idx]\n        name = method.get_name()\n        title = f'CNN Layer #{l+1} {name} Attributions for Sample #{idx}'\n        mldatasets.**compare_img_pred_viz**(orig_img, viz_img, y_true,\\\n                                        y_pred, probs_s, title=title)\n    clear_gpu_cache() \nlook fairly familiar. Where it’s different is that it’s placing every attribution map for every filter in a grid (viz_img) with create_attribition_grid. It could just then display it with plt.imshow as before, but instead, we will leverage a utility function called compare_img_pred_viz to visualize the attribution(s) side by side with the original image (orig_img). It also takes the sample’s actual label (y_true) and predicted label (y_pred). Optionally, we can provide a pandas series with the probabilities for this prediction (probs_s) and a title. It generates 56 images in total, including *Figures 7.12*, *7.13*, and *7.14*.\n```", "```py\nmisclass_idxs = metal_FP_idxs + plastic_FN_idxs[-4:]\nmisclass_data = torch.utils.data.Subset(test_data, misclass_idxs)\nmisclass_loader = torch.utils.data.DataLoader(misclass_data,\\\n                                              batch_size = 32)\nX_misclass, y_misclass = next(iter(misclass_loader))\nX_misclass, y_misclass = X_misclass.to(device), y_misclass.to(device) \n```", "```py\ndef get_attribution_maps(**method**, model, device,X,y=None,\\\n                         init_args={}, nt_type=None, nt_samples=10,\\\n                         stdevs=0.2, **kwargs):\n    attr_maps_size = tuple([0] + list(X.shape[1:]))\n    attr_maps = torch.empty(attr_maps_size).to(device)\n    **attr_method** = **method**(model, **init_args)\n    if nt_type is not None:\n        noise_tunnel = attr.NoiseTunnel(attr_method)\n        nt_attr_maps = torch.empty(attr_maps_size).to(device)\n    for i in tqdm(range(len(X))):\n        X_i = X[i].unsqueeze(0).requires_grad_()\n        model.zero_grad()\n        extra_args = {**kwargs}\n        if y is not None:\n            y_i = y[i].squeeze_()\n            extra_args.update({\"target\":y_i})\n\n        attr_map = **attr_method.attribute**(X_i, **extra_args)\n        attr_maps = torch.cat([attr_maps, attr_map])\n        if nt_type is not None:\n            model.zero_grad()\n            nt_attr_map = noise_tunnel.attribute(\n                X_i, nt_type=nt_type, nt_samples=nt_samples,\\\n                stdevs=stdevs, nt_samples_batch_size=1, **extra_args)\n            nt_attr_maps = torch.cat([nt_attr_maps, nt_attr_map])\n        clear_gpu_cache()\n    if nt_type is not None:\n        return attr_maps, nt_attr_maps\n    return attr_maps \n```", "```py\nsaliency_maps = get_attribution_maps(attr.Saliency, garbage_mdl,\\\n                                     device, X_misclass, y_misclass) \n```", "```py\npos = 4\norig_img = mldatasets.tensor_to_img(X_misclass[pos], norm_std,\\\n                                    norm_mean, to_numpy=True)\nattr_map = mldatasets.tensor_to_img(\n    saliency_maps[pos], to_numpy=True,\\\n    cmap_norm='positive'\n)\nfig, axs = plt.subplots(1, 3, figsize=(15,5))\naxs[0].imshow(orig_img)\naxs[0].grid(None)\naxs[0].set_title(\"Original Image\")\naxs[1].imshow(attr_map)\naxs[1].grid(None)\naxs[1].set_title(\"Saliency Heatmap\")\naxs[2].imshow(np.mean(orig_img, axis=2), cmap=\"gray\")\naxs[2].imshow(attr_map, alpha=0.6)\naxs[2].grid(None)\naxs[2].set_title(\"Saliency Overlayed\")\nidx = misclass_idxs[pos]\ny_true = labels_l[int(y_test[idx])]\ny_pred = y_test_pred[idx]\nplt.suptitle(f\"Actual label: {y_true}, Predicted: {y_pred}\")\nplt.show() \n```", "```py\ngradcam_maps = get_attribution_maps(\n    attr.GuidedGradCam, garbage_mdl, device, X_misclass,\\\n    y_misclass, init_args={'layer':conv_layers[3]}\n) \n```", "```py\nig_maps, smooth_ig_maps = get_attribution_maps(\n    attr.IntegratedGradients, garbage_mdl, device, X_misclass,\\\n    y_misclass, nt_type='smoothgrad', nt_samples=20, stdevs=0.2\n) \n```", "```py\nnt_attr_map = mldatasets.tensor_to_img(\n    smooth_ig_maps[pos], to_numpy=True, cmap_norm='positive'\n)\naxs[2].imshow(nt_attr_map)\naxs[2].grid(None)\naxs[2].set_title(\"SmoothGrad Integrated Gradients\") \nFigure 7.17:\n```", "```py\ndeeplift_maps = get_attribution_maps(attr.DeepLift, garbage_mdl,\\\n                                     device, X_misclass, y_misclass) \n```", "```py\nfor pos, idx in enumerate(misclass_idxs):\n    orig_img = mldatasets.tensor_to_img(test_400_data[idx][0],\\\n                                   norm_std, norm_mean, to_numpy=True)\n    bg_img = mldatasets.tensor_to_img(test_data[idx][0],\\\n                                   norm_std, norm_mean, to_numpy=True)\n    map1 = mldatasets.tensor_to_img(\n        saliency_maps[pos], to_numpy=True,\\\n        cmap_norm='positive', overlay_bg=bg_img\n    )\n    map2 = mldatasets.tensor_to_img(\n        smooth_ig_maps[pos],to_numpy=True,\\\n        cmap_norm='positive', overlay_bg=bg_img\n    )\n    map3 = mldatasets.tensor_to_img(\n        gradcam_maps[pos], to_numpy=True,\\\n        cmap_norm='positive', overlay_bg=bg_img\n    )\n    map4 = mldatasets.tensor_to_img(\n        deeplift_maps[pos], to_numpy=True,\\\n        cmap_norm='positive', overlay_bg=bg_img\n    )\n    viz_img = cv2.vconcat([\n        cv2.hconcat([map1, map2]),\n        cv2.hconcat([map3, map4])\n    ])\n    label = int(y_test[idx])\n    y_true = labels_l[label]\n    y_pred = y_test_pred[idx]\n    probs_s = probs_df.loc[idx]\n    title = 'Gradient-Based Attr for Misclassification Sample #{}'.\\\n                                                           format(idx)\n    mldatasets.compare_img_pred_viz(orig_img, viz_img, y_true,\\\n                                    y_pred, probs_s, title=title) \n```", "```py\ncorrectcls_idxs = wglass_TP_idxs[:4] + battery_TP_idxs[:4] \ncorrectcls_data = torch.utils.data.Subset(test_data, correctcls_idxs)\ncorrectcls_loader = torch.utils.data.DataLoader(correctcls_data,\\\n                                                batch_size = 32)\nX_correctcls, y_correctcls = next(iter(correctcls_loader))\nX_correctcls, y_correctcls = X_correctcls.to(device),\\\n                             y_correctcls.to(device) \n```", "```py\nfeature_mask = torch.zeros(3, 224, 224).int().to(device)\ncounter = 0\nstrides = 16\nfor row in range(0, 224, strides):\n    for col in range(0, 224, strides):\n        feature_mask[:, row:row+strides, col:col+strides] = counter\n        counter += 1 \n```", "```py\nbaseline_light = float(X_correctcls.max().detach().cpu())\nbaseline_dark = float(X_correctcls.min().detach().cpu()) \n```", "```py\nablation_maps = get_attribution_maps(\n    attr.FeatureAblation,garbage_mdl,\\\n    device,X_correctcls,y_correctcls,\\\n    feature_mask=feature_mask,\\\n    baselines=baseline_dark\n) \n```", "```py\npos = 2\norig_img = mldatasets.tensor_to_img(X_correctcls[pos], norm_std,\\\n                                    norm_mean, to_numpy=True)\nattr_map = mldatasets.tensor_to_img(occlusion_maps[pos],to_numpy=True,\\\n                         cmap_norm='positive') \nFigure 7.22:\n```", "```py\nocclusion_maps = get_attribution_maps(\n    attr.Occlusion, garbage_mdl,\\\n    device,X_correctcls,y_correctcls,\\\n    baselines=baseline_dark,\\\n    sliding_window_shapes=(3,16,16),\\\n    strides=(3,8,8)\n) \n```", "```py\nsvs_maps = get_attribution_maps(\n    attr.ShapleyValueSampling,garbage_mdl,\\\n    device, X_correctcls, y_correctcls,\\\n    baselines=baseline_dark,\\\n    n_samples=5, feature_mask=feature_mask\n) \nocclusion_maps is replaced by svs_maps. The output is shown in *Figure 7.24*:\n```", "```py\nkshap_light_maps = get_attribution_maps(attr.KernelShap, garbage_mdl,\\\n                                  device, X_correctcls, y_correctcls,\\\n                                  baselines=baseline_light,\\\n                                  n_samples=300,\\\n                                  feature_mask=feature_mask)\nkshap_dark_maps = get_attribution_maps(attr.KernelShap, garbage_mdl,\\\n                                  device, X_correctcls, y_correctcls,\\\n                                  baselines=baseline_dark,\\\n                                  n_samples=300,\\\n                                  feature_mask=feature_mask) \nsvs_maps is replaced by kshap_light_maps, and we modify the code to plot the attributions with the dark baselines in the third position, like this:\n```", "```py\naxs[2].imshow(attr_dark_map)\naxs[2].grid(None)\naxs[2].set_title(\"Kernel Shap Dark Baseline Heatmap\") \nFigure 7.25:\n```", "```py\nfor pos, idx in enumerate(correctcls_idxs):\n    orig_img = mldatasets.tensor_to_img(test_400_data[idx][0],\\\n                                        norm_std, norm_mean,\\\n                                        to_numpy=True)\n    bg_img = mldatasets.tensor_to_img(test_data[idx][0],\\\n                                      norm_std, norm_mean,\\\n                                      to_numpy=True)\n    map1 = mldatasets.tensor_to_img(ablation_maps[pos],\\\n                                    to_numpy=True,\\\n                                    cmap_norm='positive',\\\n                                    overlay_bg=bg_img)\n    map2 = mldatasets.tensor_to_img(svs_maps[pos], to_numpy=True,\\\n                                    cmap_norm='positive',\\\n                                    overlay_bg=bg_img)\n    map3 = mldatasets.tensor_to_img(occlusion_maps[pos],\\\n                                    to_numpy=True,\\\n                                    cmap_norm='positive',\\\n                                    overlay_bg=bg_img)\n    map4 = mldatasets.tensor_to_img(kshap_dark_maps[pos],\\\n                                    to_numpy=True,\\\n                                    cmap_norm='positive',\\\n                                    overlay_bg=bg_img)\n    viz_img = cv2.vconcat([\n            cv2.hconcat([map1, map2]),\n            cv2.hconcat([map3, map4])\n        ])\n    label = int(y_test[idx])\n    y_true = labels_l[label]\n    y_pred = y_test_pred[idx]\n    probs_s = probs_df.loc[idx]\n    title = 'Pertubation-Based Attr for Correct classification #{}'.\\\n                format(idx)\n    mldatasets.compare_img_pred_viz(orig_img, viz_img, y_true,\\\n                                    y_pred, probs_s, title=title) \nFigures 7.26 to *7.28*. For your reference, ablation is in the top-left corner and occlusion is at the bottom left. Then, Shapley is at the top right and KernelSHAP is at the bottom right.\n```"]