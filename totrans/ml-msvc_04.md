# 4

# 关键机器学习算法和概念

在前面的章节中，我们探讨了MSA的不同概念及其在创建企业系统时所起的作用。

在接下来的章节中，我们将开始将我们的关注点从学习MSA概念转移到学习关键机器学习概念。我们还将学习使用Python在机器学习模型中使用到的不同库和包。

在本章中，我们将涵盖以下内容：

+   人工智能、机器学习和深度学习之间的区别

+   在Python中常用的常见深度学习包和库

+   构建回归**模型**

+   构建多类**分类**

+   文本情感分析和主题建模

+   使用机器学习进行模式分析和预测

+   使用深度学习构建增强模型

# 人工智能、机器学习和深度学习之间的区别

尽管人工智能和机器学习在最近几年越来越受欢迎，但人工智能领域自20世纪60年代以来就已经存在。随着不同子领域的出现，能够区分它们并理解它们及其所包含的内容变得很重要。

首先，人工智能是涵盖我们今天看到的所有子领域的总领域，例如机器学习、深度学习等。任何从其环境中感知或接收信息并执行动作以最大化奖励或实现其目标的系统都被认为是人工智能机器。

这在今天与机器人技术相关时非常常见。我们的大多数机器被设计成能够使用它们的传感器，如摄像头、声纳或陀螺仪来捕获数据，并使用捕获到的数据以最高效的方式响应特定任务。这个概念与人类的功能非常相似。我们使用我们的感官来“捕获”来自环境的信息，并根据我们接收到的信息执行某些动作。

人工智能是一个广泛的领域，但它可以被划分为不同的子领域，我们今天普遍知道的一个子领域是机器学习。使机器学习独特的是，这个领域致力于创建可以持续学习和改进其模型而无需明确编程的系统或机器。

机器学习通过收集数据，也称为训练数据，并试图在数据中找到模式以进行准确预测，而不需要被编程来做这样的事情来实现这一点。机器学习中使用了许多不同的方法来学习数据，这些方法针对我们遇到的不同问题进行了定制。

![图4.1：人工智能的不同领域](img/B18934_04_1.jpg)

图4.1：人工智能的不同领域

机器学习问题可以分为三个不同的任务：**监督学习**、**无监督学习**和**强化学习**。现在，我们将专注于监督学习和无监督学习。这种区分基于我们拥有的训练数据。监督学习是我们有特定数据集的输入数据和预期输出时的情况，这也被称为标签。另一方面，无监督学习只包含输入而没有预期输出。

监督学习通过理解输入和输出数据之间的关系来工作。监督学习的一个常见例子是预测某个城市的房价。我们可以通过捕捉现有房屋的规格和它们当前的价格来收集现有房屋的数据，然后学习这些房屋的特征和它们价格之间的模式。然后我们可以取一个不在我们的训练集中的房屋，通过将房屋的特征输入到我们的程序中来测试我们的模型，并让模型预测该房屋的价格。

无监督学习通过使用分组或聚类方法来学习数据的结构。这种方法通常用于营销目的。例如，一家商店想要将其客户聚类到不同的群体中，以便它可以有效地针对不同的细分市场调整其产品。它可以捕捉其客户的购买历史，使用这些数据来学习购买模式，并建议某些可能会引起他们兴趣的项目或商品，从而最大化其收入。

在我们能够理解深度学习——它是机器学习的一个子领域——之前，我们首先必须了解什么是**人工神经网络**（**ANNs**）。ANNs是从大脑中的神经元得到灵感，由一组完全连接的节点组成的模型，也称为人工神经元。它们包含一组输入、连接神经元的隐藏层，以及一个输出节点。每个神经元都有一个输入和输出，这些可以通过整个网络传播。为了计算神经元的输出，我们取所有输入的加权和，乘以神经元的权重，然后通常加上一个偏差项。

我们继续执行这些操作，直到我们达到最后一层，即输出神经元。我们执行一个非线性激活函数，例如Sigmoid函数，以给出最终的预测。然后我们将预测的输出值输入到一个**成本函数**中。这个函数告诉我们我们的网络学习得有多好。我们取这个值，并通过反向传播通过我们的层回到第一层，根据我们的网络表现调整神经元的权重。通过这种方式，我们可以创建强大的模型，可以执行诸如手写识别、游戏AI等任务。

重要提示

如果一个程序能够接受输入数据并学习模式以进行预测，而不需要明确编程，那么它被认为是一个机器学习模型。

![图4.2：一个ANN](img/B18934_04_2.jpg)

图4.2：一个ANN

虽然ANNs能够执行许多任务，但它们在当今市场的使用中存在显著的缺点：

+   理解模型的性能可能很困难。随着你在网络中添加更多的隐藏层，尝试调试网络变得复杂。

+   训练模型需要很长时间，特别是当有大量的训练数据时，并且可能会耗尽硬件资源，因为在CPU上执行所有这些数学运算很困难。

+   ANNs最大的问题是过拟合。随着我们添加更多的隐藏层，存在一个点，此时分配给神经元的权重将高度定制于我们的训练数据。这使得当尝试用之前未见过的数据测试网络时，我们的网络表现非常糟糕。

这就是深度学习发挥作用的地方。深度学习可以根据以下关键特征进行分类：

+   **层的层次结构组成**：而不是在网络中只有全连接层，我们可以创建和组合多个不同的层，包括非线性变换和线性变换。这些不同的层在数据中提取关键特征中发挥作用，这些特征在ANN中可能很难找到。

+   **端到端学习**：网络从一种称为特征提取的方法开始。它查看数据，并找到一种方法来分组冗余信息并识别数据的重要特征。然后，网络使用这些特征通过全连接层进行训练和预测或分类。

+   **神经元的分布式表示**：通过特征提取，网络可以将神经元分组以编码数据的一个更大特征。与ANN不同，没有单个神经元编码所有内容。这允许模型在保留数据中的关键元素的同时，减少它必须学习的参数数量。

深度学习在计算机视觉中非常普遍。由于捕捉照片和视频技术的进步，当涉及到图像检测时，人工神经网络（ANNs）学习和表现良好变得非常困难。首先，当我们使用图像来训练我们的模型时，我们必须将图像中的每一个像素都视为模型的一个输入。因此，对于一个256x256分辨率的图像，我们将会查看超过65,000个输入参数。根据你全连接层中神经元的数量，你可能需要查看数百万个参数。由于参数数量庞大，这必然会导致过拟合，并且可能需要数天的训练时间。

通过深度学习，我们可以创建一组称为**卷积神经网络**（**CNNs**）的层。这些层负责减少我们在模型中需要学习的参数数量，同时仍然保留数据中的关键特征。有了这些添加，我们可以学习如何提取某些特征，并使用这些特征来训练我们的模型，以高效和准确地进行预测。

![图 4.3：一个 CNN](img/B18934_04_3.jpg)

图 4.3：一个 CNN

在下一节中，我们将探讨用于机器学习和深度学习的不同 Python 库及其不同的用例。

# Python 中常用的深度学习和机器学习库

现在我们已经了解了人工智能和机器学习的基本概念，我们可以开始探讨实现这些概念编程方面的内容。在创建机器学习模型时，今天使用的编程语言有很多。常用的有 MATLAB、R 和 Python。其中，Python 由于其作为编程语言的灵活性和丰富的库，已经成为机器学习中最受欢迎的编程语言。在本节中，我们将介绍今天最常用的库。

### NumPy

NumPy 是在 Python 中构建机器学习模型时必不可少的包。在构建模型时，你将主要与大型、多维矩阵打交道。大部分的工作都花在了对矩阵进行转换、拼接以及执行高级数学运算上，而 NumPy 提供了执行这些操作所需的同时保持速度和效率的工具。

想要了解更多关于 NumPy 提供的不同 API 的信息，您可以访问其网站上的文档：https://numpy.org/doc/stable/reference/index.html。

这里，我们将查看示例代码。本节展示了我们如何初始化一个 NumPy 数组。在这个例子中，我们将创建一个 3x3 矩阵，其初始化值为 1 到 9：

[PRE0]

这里，我们将打印出结果：

[PRE1]

现在，我们可以展示如何从我们的数组中拼接和提取某些元素。

这行代码允许我们从数组中提取第二列的所有值。请注意，在 NumPy 中，我们的数组和列表是零索引的，这意味着零索引指的是数组或列表中的第一个元素：

[PRE2]

在这个例子中，我们提取了我们数组中第`2`行的所有值：

[PRE3]

NumPy 数组的另一个有用之处在于，我们可以对矩阵应用数学函数，而无需编写执行基本函数的代码。这不仅更容易，而且更快、更高效。

在这个例子中，我们只是将我们的矩阵与标量值`-1`进行乘法运算：

[PRE4]

### Matplotlib

为了看到您的模型是如何学习和表现的，能够可视化您的结果和数据非常重要。Matplotlib提供了一个简单的方法来绘制您的数据，从简单的线图到更高级的图表，如等高线图和3D图。使这个库如此受欢迎的是它与NumPy工作的无缝性。

关于它们不同功能的更多信息，您可以访问它们的网站：[https://matplotlib.org/stable/index.html](https://matplotlib.org/stable/index.html)。

在这个例子中，我们将创建一个简单的线图。我们首先初始化两个数组，`x`和`y`，这两个数组都将包含从0到9的值。然后，使用Matplotlib的API，我们可以绘制并显示我们的简单图表：

[PRE5]

![图4.4：使用Matplotlib的简单线图](img/B18934_04_4.jpg)

图4.4：使用Matplotlib的简单线图

### Pandas

随着最近将数据存储在CSV文件中的趋势，Pandas因其易用性和多功能性而成为Python社区中的必备工具。Pandas通常用于数据分析。它以表格格式存储数据，并为用户提供简单的函数来预处理和操作数据以满足他们的需求。在处理时间序列数据时，它也变得非常有用，这对于构建预测模型很有帮助。

关于不同函数的更多信息，您可以在其网站上查看文档：[https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/)。

在这个例子中，首先，我们将简单地初始化一个DataFrame。这是在Pandas中以二维表格格式存储我们数据的数据结构。通常，我们存储从读取的文件中读取的数据，但也可以使用自己的数据创建DataFrame：

[PRE6]

![图4.5：我们的DataFrame的输出](img/B18934_04_5.jpg)

图4.5：我们的DataFrame的输出

与NumPy一样，我们可以从我们的DataFrame中提取某些列和行。在这段代码中，我们可以查看我们的DataFrame的第一行：

[PRE7]

![图4.6：我们的DataFrame的第一行输出](img/B18934_04_6.jpg)

图4.6：我们的DataFrame的第一行输出

使用Pandas，我们还可以通过使用列名而不是索引来从我们的DataFrame中提取某些列：

[PRE8]

![图4.7：价格列中所有值的输出](img/B18934_04_7.jpg)

图4.7：价格列中所有值的输出

### TensorFlow和Keras

TensorFlow和Keras是构建深度学习模型的基础。虽然两者都可以单独使用，但Keras被用作TensorFlow框架的接口，使用户能够轻松创建强大的深度学习模型。

由Google创建的TensorFlow在创建机器学习模型时充当后端。它通过创建静态数据流图来工作，这些图指定了数据在深度学习管道中的流动方式。图中包含节点和边，其中节点代表数学运算。它使用称为张量的多维数组传递这些数据。

Keras，后来与TensorFlow集成，可以看作是设计深度学习模型的界面。它被设计成用户友好，允许用户专注于设计他们的神经网络模型，无需处理复杂的后端。它类似于面向对象编程，因为它复制了创建对象的方式。用户可以自由添加不同类型的层、激活函数等。他们甚至可以使用预构建的神经网络进行简单的训练和测试。

在下面的示例代码中，我们可以看到如何创建一个简单的、隐藏的两层神经网络。这段代码允许我们初始化一个`Sequential`模型，它由简单的层堆叠组成：

[PRE9]

根据我们的应用，我们可以添加具有不同配置的多个层，例如节点数、激活函数和核正则化器：

[PRE10]

最后，我们可以编译我们的模型，这实际上是将所有不同的层聚集在一起，并将它们组合成一个简单的神经网络：

[PRE11]

### PyTorch

PyTorch是由Meta（前Facebook）创建的另一个机器学习框架。与Keras/TensorFlow类似，它允许用户创建机器学习模型。该框架非常适合自然语言处理（NLP）和计算机视觉问题，但可以定制以适应大多数应用。PyTorch的独特之处在于其动态计算图。它有一个名为Autograd的模块，允许你动态地进行自动微分，与TensorFlow中的静态微分相比。此外，PyTorch更符合Python语言，这使得它更容易理解，并利用Python的有用特性，如并行编程。有关更多信息，请访问他们网站上的文档：[https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)。

在这段代码中，我们可以创建一个简单的单层神经网络。类似于Keras，我们可以初始化一个`Sequential`模型，并根据需要添加层：

[PRE12]

### SciPy

这个库是为科学计算设计的。它包含许多用于线性代数、优化和积分的内置函数和方法，这些在机器学习中常用。当你构建机器学习模型时，尝试计算某些统计和转换时，这个库很有用。有关它提供的不同函数的更多信息，请查看其网站上的文档：[https://docs.scipy.org/doc/scipy/](https://docs.scipy.org/doc/scipy/)。

在这个示例代码中，我们可以使用NumPy创建一个3x3的数组，然后我们可以使用SciPy来计算行列式：

[PRE13]

### scikit-learn

scikit-learn是一个机器学习库，它是SciPy的扩展，使用NumPy和Matplotlib构建。它包含许多预构建的机器学习模型，如随机森林、K-means和支持向量机。有关它提供的不同API的更多信息，请访问其网站：[https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)。

在下面的例子中，我们将使用scikit-learn提供的示例数据集来构建一个简单的逻辑回归模型。首先，我们导入所有必需的库，然后加载scikit-learn提供的Iris数据集。我们可以使用scikit-learn的一个便捷API将我们的数据分成训练集和测试集：

[PRE14]

我们可以初始化我们的逻辑回归模型，并简单地使用我们的训练数据运行`fit`函数来训练模型。一旦我们训练了我们的模型，我们就可以用它来进行预测，然后测量其准确度：

[PRE15]

在接下来的几节中，我们将开始探讨我们可以使用这些库构建的不同模型。我们将了解是什么使这些模型独特，它们的结构如何，以及它们可以最好地服务于哪些目的和应用程序来满足我们的需求。

# 构建回归模型

首先，我们将探讨回归模型。回归模型或回归分析是用于寻找独立变量和依赖变量之间关系的建模技术。回归模型的输出通常是连续值，也称为定量变量。一些常见的例子是根据房屋的特征预测房价，或者根据之前的销售信息预测新商店中某种产品的销售情况。

在构建回归模型之前，我们首先必须理解数据和它的结构。大多数回归模型涉及监督学习。这包括特征和一个输出变量，称为标签。这将通过调整权重来帮助模型更好地拟合我们迄今为止观察到的数据。我们通常将我们的特征表示为X，将我们的标签表示为Y，以帮助我们理解用于解决回归模型的数学模型：

![图4.8：监督学习数据结构的示例](img/B18934_04_8.jpg)

图4.8：监督学习数据结构的示例

通常，我们的数据被分成两个子集，**训练集**和**测试集**。训练集通常包含原始数据的70-80%，而测试集包含剩余的部分。这是为了让模型在训练集上学习，并在测试集上验证其结果以展示其性能。从结果中，我们可以推断出我们的模型在数据集上的表现。

为了线性回归模型能够有效运行，我们的数据必须以线性方式组织。模型使用以下公式在数据上进行训练和学习：

![公式4.1](img/Formula_04_001.jpg)

在这个方程中，![公式 4.002](img/Formula_04_002.png)代表模型的输出，或者我们通常所说的预测。预测是通过取截距![公式 4.003](img/Formula_04_003.png)和斜率![公式 4.004](img/Formula_04_004.png)来计算的。斜率，也称为权重，应用于数据中的所有特征，代表![公式 4.005](img/Formula_04_005.png)。在处理数据时，我们通常将其表示为矩阵，这使得它易于理解，并且在使用Python时易于操作：

![公式 4.06](img/Formula_04_006.jpg)![公式 4.07](img/Formula_04_007.jpg)![公式 4.08](img/Formula_04_008.jpg)![公式 4.09](img/Formula_04_009.jpg)

特征数量描述了你正在解决什么类型的问题。如果你的数据只有一个特征，它被认为是一个简单的线性回归模型。虽然它可以解决直接的问题，但对于更复杂的数据和问题，映射关系可能会很困难。因此，你可以通过添加更多特征来创建一个多元线性回归模型（![公式 4.10](img/Formula_04_010.png)）。这使得模型更加稳健，并能找到更深层次的关系。

![图 4.9：一个简单的线性回归模型](img/B18934_04_9.jpg)

图 4.9：一个简单的线性回归模型

一旦我们训练了我们的模型，我们需要学习如何评估我们的模型并理解它在测试数据上的表现。当涉及到线性回归时，我们用来评估模型的两个常用指标是**均方根误差**（**RMSE**）和**R**2指标。

RMSE是预测中残差误差的标准差。残差是实际数据点到回归线的距离的度量。所有点的平均距离越远，误差就越高。这表明模型较弱，因为它无法找到数据点之间的相关性。这个指标可以通过以下公式计算，其中![公式 4.11](img/Formula_04_011.png)是实际值，![公式 4.12](img/Formula_04_012.png)是预测值，![公式 4.13](img/Formula_04_013.png)是数据点的数量：

![公式 4.14](img/Formula_04_014.jpg)![图 4.10：计算线性回归模型的残差](img/B18934_04_10.jpg)

图 4.10：计算线性回归模型的残差

R2，也称为确定系数，衡量了因变量（Y）的方差中可以被自变量（X）解释的比例。它本质上告诉我们数据与模型拟合得有多好。与RMSE不同，RMSE可以是一个任意数，R2以百分比的形式给出，这可能更容易理解。百分比越高，数据的关联性越好。尽管很有用，但高百分比并不总是意味着模型强大。决定一个好的R2值取决于应用和用户如何理解数据。R2可以通过以下公式计算：

![公式 4.15](img/Formula_04_015.jpg)

有许多更多的指标可以评估你的回归模型的有效性，但这两个指标已经足够让你了解你的模型是如何表现的。在构建和评估你的模型时，重要的是绘制和可视化你的数据和模型，因为这可以识别关键点。图表可以帮助你确定你的模型是**过度拟合**还是**欠拟合**。

当你的模型过于适合你的训练数据时，会发生过度拟合。你的RMSE将会非常低，并且你的训练准确率几乎达到100%。虽然这看起来很有吸引力，但这表明模型不好。这可能是由于以下两个原因之一：数据不足或参数过多。因此，当你用之前未见过的新数据测试你的模型时，由于它无法泛化数据，它的表现会非常差。

![图4.11：过度拟合的线性回归模型](img/B18934_04_11.jpg)

图4.11：过度拟合的线性回归模型

为了解决过度拟合问题，你可以尝试增加训练数据量，或者使模型更简单。在将数据分成训练集和测试集之前随机打乱数据也有帮助。另一个重要的技术称为**正则化**。虽然根据模型的不同，有许多不同的正则化技术（L1或L2正则化），但它们在防止过度拟合方面的工作方式相似，即它们向模型中添加**偏差**或噪声。在我们之前看到的回归方程中，我们可以添加另一个项，![](img/Formula_04_016.png)，以表明正则化正在应用于我们的模型：

![](img/Formula_04_017.jpg)

在另一端，当你的模型无法在数据中找到任何有意义的相关性时，会发生欠拟合。由于在大多数数据中很容易找到模式，所以这不如过度拟合常见。如果发生这种情况，要么是因为你的数据噪声太多且严重不相关，要么是因为你的模型太简单且参数不足，或者模型对当前的应用不有效。在调试代码并确保在预处理数据或设置模型时没有错误也是很有用的：

![图4.12：欠拟合的线性回归模型](img/B18934_04_12.jpg)

图4.12：欠拟合的线性回归模型

因此，目标是找到一个最佳拟合模型，介于过度拟合和欠拟合之间。找到适合你需求的模型需要时间和实验，但使用这里讨论的关键指标和度量可以帮助你指引正确的方向：

![图4.13：最佳拟合的线性回归模型](img/B18934_04_13.jpg)

图4.13：最佳拟合的线性回归模型

重要提示

特征工程是构建全面模型的关键部分。理解你的数据可以帮助确定在模型中包含哪些特征或参数，这样你就可以捕捉独立变量和因变量之间的关系，而不会导致过度拟合。

在收集和处理模型数据时，有一些关键点需要注意：

+   **归一化数据**：可能存在具有非常高的或很低的数值的特征，为了防止它们压倒模型并产生偏差，将所有数据归一化以使它们在特征上保持一致是至关重要的。

+   **清理数据**：在现实世界中，我们收集的数据并不总是完美的，可能包含缺失或严重的错误数据。处理这些问题很重要，因为它们可能导致异常值并负面影响模型。

+   **理解数据**：对数据进行统计分析，也称为**探索性数据分析**（**EDA**），是常见的做法，以更好地了解数据如何影响模型。这可以包括绘制图表、运行统计方法，甚至使用机器学习技术来降低数据的维度，这些将在本章后面讨论。

在下一节中，我们将讨论分类模型。

# 构建多类分类

与产生连续输出的回归模型不同，当模型产生有限输出时，它们被认为是分类模型。一些例子包括垃圾邮件检测、图像分类和语音识别。

分类模型被认为是多才多艺的，因为它们可以应用于监督学习和无监督学习，而回归模型主要用于监督学习。有一些回归模型（如逻辑回归和支持向量机）也被认为是分类模型，因为它们使用阈值将连续值的输出分割成不同的类别。

无监督学习是当今市场上常用的应用。尽管监督学习通常表现更好，并提供了有意义的成果，因为我们知道预期的输出，但我们收集的大多数数据都是未标记的。对于公司来说，让人类专家筛选数据并进行标记既耗时又费钱。无监督学习通过让模型尝试为数据确定标签并提取有意义的信息，有助于降低成本和时间。有时它们甚至可以比人类表现得更好。

分类模型输出的类别数量决定了它的类型。对于只有两个输出（即垃圾邮件和非垃圾邮件）的模型，这被称为二元分类器，而具有两个以上输出的模型被称为多类分类器：

![图4.14：二元和多类分类器](img/B18934_04_14.jpg)

图4.14：二元和多类分类器

从那些分类器中，有两种类型的学习者：**懒惰学习者**和**急切学习者**。

懒惰学习者基本上存储训练数据，并等待接收到新的测试数据。一旦他们获得测试数据，模型就会根据已有的数据对新数据进行分类。这些类型的学习者训练时所需时间较少，因为你可以连续添加新数据，而无需重新训练整个模型，但在执行分类时需要更多时间，因为他们必须通过所有数据点。一种常见的懒惰学习者类型是**K-最近邻**（**KNN**）算法。

另一方面，急切的学习者以相反的方式工作。每当有新数据添加到模型中时，他们必须重新训练模型。虽然与懒惰学习者相比，这需要更多的时间，但查询模型的速度要快得多，因为他们不必通过所有数据点。一些急切学习者的例子是决策树、朴素贝叶斯和人工神经网络。

重要提示

监督学习通常比无监督学习表现更好，因为我们知道在训练过程中预期的输出应该是什么，但是收集和标记数据成本高昂，因此无监督学习在训练未标记数据方面表现出色。

在接下来的几节中，我们将探讨一些用于解决大多数基本分类或回归模型无法解决的独特问题的利基模型。

# 文本情感分析和主题建模

机器学习领域的一个热门领域是主题建模和文本分析。随着互联网上文本的激增，能够理解这些数据并创建复杂的模型，如聊天机器人和翻译服务，已经成为一个热门话题。使用软件与人类语言交互被称为**NLP**。

尽管我们可以使用大量数据来训练我们的模型，但创建有意义的模型是一项困难的任务。语言本身很复杂，包含许多语法规则，尤其是在尝试翻译不同语言时。某些强大的技术可以帮助我们在创建NLP模型时。

重要提示

在实施任何NLP模型之前，以某种方式预处理数据至关重要。文档和文本往往包含一些无关数据，如停用词（the/a/and）或随机字符，这些可能会影响模型并产生错误的结果。

我们将要讨论的第一个想法是**主题建模**。这是一个将文档或文本中的文本或单词分组到不同主题或领域的进程。当你有一份文档或文本，并希望将其分类和分组到某个特定类型，而不必逐个阅读文档时，这非常有用。用于主题建模有许多不同的模型：

+   **潜在语义分析**（**LSA**）

+   **概率潜在语义分析**（**PLSA**）

+   **潜在狄利克雷分配**（**LDA**）

我们将重点关注 LDA。LDA 使用统计学来寻找单词或短语的重复出现模式，并将它们分组到相应的主题中。它假设每个文档包含主题的混合，每个主题包含单词的混合。LDA 首先开始处理文档的过程，并保持一个词矩阵，其中包含每个文档中每个单词的计数：

![图 4.15：词矩阵](img/B18934_04_15.jpg)

图 4.15：词矩阵

在创建词矩阵后，我们确定要分割单词的主题数量，![](img/Formula_04_018.png)，并使用统计学来找到单词属于某个主题的概率。使用贝叶斯统计学，我们可以计算出概率，并使用该概率将单词聚类到不同的主题中：

![图 4.16：LDA 模型](img/B18934_04_16.jpg)

图 4.16：LDA 模型

另一个在自然语言处理（NLP）中日益增长的应用是**情感分析**。这涉及对单词或文本进行理解，以了解用户的意图或情感。这在处理在线评论或社交媒体帖子时很常见。它决定了文本是否包含积极、中性或消极的情感。

许多不同的方法和模型可以解决这个问题。最简单的方法是通过使用贝叶斯定理进行统计学。这个公式用于预测分析，因为它使用文本中的先前单词来更新模型。可以使用此公式计算概率：

![](img/Formula_04_019.jpg)

深度学习已成为 NLP 的强大工具，对情感分析很有用。卷积神经网络（CNNs）和**循环神经网络**（**RNNs**）是两种可以显著提高 NLP 模型，特别是情感分析模型的深度学习模型。我们将在本章后面讨论这些神经网络及其性能。

# 机器学习中的模式分析和预测

随着时间的不确定性，能够预测某些趋势和模式已成为当今行业的热门话题。尽管大多数回归模型功能强大，但它们无法做出自信的时间预测。因此，一些研究人员设计了在做出某些预测时考虑时间的模型，例如油价、股市和销售预测。在我们深入探讨不同的模型之前，我们首先必须了解时间序列分析中的不同概念。

处理时间序列问题的第一步是熟悉数据。数据通常包含以下四种数据组件之一：

+   **趋势** – 数据遵循增加或减少的连续时间线，没有周期性变化

+   **季节性** – 数据在一定的周期性时间线上发生变化

+   **周期性** – 数据发生变化，但没有固定的周期性时间线

+   **不规则** – 数据随机变化，没有模式

![图 4.17：时间序列数据的不同组成部分](img/B18934_04_17.jpg)

图4.17：时间序列数据的不同组件

这些不同的趋势可以分为两种不同的数据类型：

+   **平稳** – 数据的某些属性，如均值、方差和协方差，随时间不变

+   **非平稳** – 数据的属性随时间变化

通常，你将处理非平稳数据，使用这类数据创建机器学习模型将产生不可靠的结果。为了解决这个问题，我们使用某些技术将我们的数据转换为平稳数据。它们包括以下方法：

+   **差分** – 一种用于标准化均值并消除方差数学方法。它可以通过以下公式计算：![](img/Formula_04_020.jpg)

+   **转换** – 使用数学方法来消除方差的变化。在转换方法中，以下三种是常用的：

    +   对数变换

    +   平方根

    +   幂变换

![图4.18：差分非平稳数据](img/B18934_04_18.jpg)

图4.18：差分非平稳数据

重要提示

时间本身就是不确定的，这使得创建一个可以自信预测未来趋势的模型几乎不可能。我们能在数据中消除的不确定性越多，我们的模型就能更好地找到数据中的关系。

一旦我们可以转换我们的数据，我们就可以开始考虑使用模型来进行预测。在众多模型中，用于时间序列分析最流行的模型是**自回归积分移动平均**（**ARIMA**）模型。这个线性回归模型由三个子组件组成：

+   **自回归**（**AR**） – 使用当前时间和前一时间依赖性进行预测的回归模型

+   **积分**（**I**） – 差分的过程，以便使数据平稳

+   **移动平均**（**MA**） – 通过计算滞后观测数据的移动平均来模型预期数据和残差误差

除了ARIMA，其他机器学习模型也可以用于时间序列问题。另一个著名的模型是RNN模型。这是一种用于具有某种序列数据的深度学习模型。我们将在下一节中更详细地介绍它们是如何工作的。

# 使用深度学习增强模型

在本章前面，我们简要讨论了深度学习及其在增强简单机器学习模型时带来的优势。在本节中，我们将更深入地介绍不同的深度学习模型。

在我们构建模型之前，我们将简要介绍深度学习模型的结构。一个简单的ANN模型通常包含大约两到三个全连接层，并且通常足够强大，可以模拟大多数复杂的线性函数，但随着我们添加更多层，模型改进的幅度显著减小，并且由于过拟合，它无法执行更复杂的应用。

深度学习使我们能够在我们的ANN中添加多个隐藏层，同时减少训练模型的时间并提高性能。我们可以通过添加深度学习中常见的隐藏层类型之一或两种来实现这一点——CNN或RNN。

由于神经网络的结构，CNN主要应用于图像检测和视频识别领域。CNN架构包括以下关键特性：

+   卷积层

+   激活层

+   池化层

卷积层是CNN模型的核心元素。其主要任务是使用**核**对数据进行卷积或分组，并产生一个称为**特征图**的输出。这个图包含了从数据中提取的所有关键特征，这些特征随后可以在全连接层中进行训练。特征图中的每个元素都表示一个感受野，用于表示输入的哪一部分被用来映射到输出。随着你添加更多的卷积层，你可以提取更多的特征，这允许你的模型适应更复杂的模型：

![图4.19：卷积层输出](img/B18934_04_19.jpg)

图4.19：卷积层输出

在**图4.19**中，我们可以看到特征图是如何在卷积层中创建的。该层将核在输入数据上滑动，执行点积操作，并产生一个输出矩阵，这是卷积函数的结果。核的大小和步长可以决定特征图的输出大小。在这个例子中，我们使用2x2的核大小和2的步长或步长大小，根据我们的输入大小得到一个2x2大小的特征图。特征图的输出可以根据用户的需求用于更多的未来卷积层。

在我们将新创建的特征图作为另一个卷积层或全连接层的输入之前，我们需要将特征图通过一个激活层。在训练过程中，通过某种类型的非线性函数传递数据非常重要，因为这允许模型映射到更复杂的函数。在整个模型中可以使用许多不同类型的激活函数，并且根据你计划构建的模型类型具有各自的优势。在众多激活函数中，以下是最常用的：

+   **线性** **修正** **激活** （**ReLU**）

+   **逻辑（sigmoid**）

+   双曲正切（**Tanh**）

ReLU函数是目前最流行的激活函数。它非常简单，有助于模型比大多数其他激活函数更快地学习和收敛。它是通过以下函数计算的：

![](img/Formula_04_021.jpg)![](img/B18934_04_20.jpg)

图4.20：ReLU函数

逻辑函数是另一种常用的激活函数。这是在逻辑回归模型中使用的相同函数。这个函数有助于将特征图的输出限制在0到1之间。虽然有用，但这个函数计算量大，可能会减慢训练过程。它使用以下函数计算：

![](img/Formula_04_022.jpg)![](img/B18934_04_21.jpg)

图4.21：Sigmoid函数

Tanh函数与sigmoid函数类似，它限制了特征图的值。而不是从0到1限制，它从-1到1限制值，并且通常比sigmoid函数表现更好。该函数使用以下公式计算：

![](img/Formula_04_023.jpg)![](img/B18934_04_22.jpg)

图4.22：Tanh函数

每个激活函数都有其用途和好处，这取决于手头的任务或模型。ReLU函数在CNN模型中常用，而sigmoid和Tanh主要在RNN模型中找到，但它们可以互换使用并产生不同的结果。

在我们将特征图通过激活层处理后，我们来到了最后一部分——池化层。正如之前提到的，深度学习中的一个关键元素是参数的减少。这允许模型在保持从卷积层提取的重要特征的同时，使用更少的参数进行训练。池化层负责这一步缩小我们的参数大小。有许多常见的池化函数，但最常用的是最大池化。这与卷积层类似，我们在输入数据上滑动一个核或过滤器，并只从每个窗口中取最大值：

![Figure 4.23: Max pooling layer output](img/B18934_04_23.jpg)

图4.23：最大池化层输出

在*图4**.23*中，我们使用2x2的核大小和2的步长大小。在这里，我们可以看到我们的输出，其中只选择了每个窗口的最大值。

可以向模型添加其他层和函数来帮助解决某些问题或应用，例如批量归一化层，但有了这三个基础层，我们仍然可以添加不同大小的多层，并构建一个强大的模型。

在最后一层，我们将输出输入到全连接层。到那时，我们能够提取数据的重要特征，并且与简单的ANN模型相比，在更短的时间内学习更复杂的模型。

接下来，我们将介绍RNN架构。由于RNN模型的结构特性，它被设计用于需要考虑一系列序列数据的任务，其中序列中的数据后依赖于前面的数据。这个模型通常用于某些领域，如NLP和信号处理。

RNN的基本结构是通过在隐藏层中有一个输出，该输出被送回到同一个隐藏层来构建的。这样，模型能够根据先前数据学习，并相应地调整权重。

![图4.24：RNN架构](img/B18934_04_24.jpg)

图4.24：RNN架构

为了更好地理解模型的工作原理，你可以想象每个数据点都有一个单独的层。每一层将数据点作为输入 ![](img/Formula_04_024.png) 并产生一个输出 ![](img/Formula_04_025.png)。然后我们在层之间传递权重，并取模型中所有层的总成本函数的平均值。

![图4.25：展开的RNN](img/B18934_04_25.jpg)

图4.25：展开的RNN

这个模型适用于大多数简单问题。随着序列的增加，它遇到了一些问题：

+   **梯度消失**：在训练过程中，当梯度接近零时发生这种情况。结果权重没有得到适当的更新，模型表现不佳。

+   **缺乏上下文**：该模型是单向的，不能进一步或先前查看数据。因此，模型只能根据当前序列点周围的数据进行预测，并且更有可能基于错误上下文做出较差的预测。

为了解决这里提到的一些问题，已经创建了不同版本的循环神经网络（RNNs）。其中，今天最常用的一个是**长短期记忆**（**LSTM**）模型。LSTM模型由三个组件组成：

+   输入门

+   一个输出门

+   一个忘记门

![图4.26：LSTM神经网络](img/B18934_04_26.jpg)

图4.26：LSTM神经网络

这些门通过调节哪些数据点需要上下文化序列来工作。这样，模型可以更准确地预测，而不容易被操纵。

忘记门专门负责移除不再需要的前置数据或上下文。这个门使用sigmoid函数来确定是否使用或“忘记”数据。

输入门用于确定新数据是否与当前序列相关。这样，只有重要的数据被用来训练模型，而不是冗余或不相关信息。

最后，输出门的主要功能是过滤当前状态的信息，并且只将相关信息发送到下一个状态。与其他门一样，它使用先前状态中的上下文来应用过滤器，这有助于模型正确地上下文化数据。

卷积神经网络（CNN）和循环神经网络（RNN）模型主要设计用于监督学习问题。当涉及到无监督学习时，需要不同的模型来解决某些问题。让我们讨论**自编码器**。

自编码器通过接收输入数据，压缩它，然后通过解压缩它来重建它。虽然简单，但它可以用于一些高级应用，例如生成音频或图像，或者它可以用作异常检测器。

自动编码器由两部分组成：

+   编码器

+   解码器

![图4.27：自动编码器的组件](img/B18934_04_27.jpg)

图4.27：自动编码器的组件

编码器和解码器通常使用单层人工神经网络（ANN）构建。编码器负责接收数据并压缩或展平数据。然后，解码器处理展平的数据，并尝试重建输入数据。

编码器和解码器中间的隐藏层通常被称为瓶颈。隐藏层中的节点数必须少于编码器和解码器中的节点数。这迫使模型尝试在输入数据中找到模式或表示，以便用少量信息重建数据。因此，成本函数用于计算并最小化输入和输出数据之间的差异。

自动编码器的一个方面是**降维**，这是深度学习的一个基本组成部分。这是在训练模型时减少参数或特征数量的过程。正如本章前面提到的，为了构建一个能够构建数据更深层次表示的复杂模型，包括更多特征是很重要的。然而，添加过多的特征可能导致过拟合，那么我们如何找到在模型中使用最佳特征数量呢？

有许多模型和技术，例如自动编码器，可以执行降维以帮助我们找到在模型中使用最佳特征。在众多技术中，**主成分分析**（**PCA**）是最受欢迎的。这项技术可以将N维数据集通过线性代数减少数据的维度。在用数据训练模型之前使用降维技术是一种常见做法，因为这有助于去除数据中的噪声并避免过拟合。

# 摘要

在本章中，我们讨论了什么是人工智能以及它包含的不同子领域。

我们讨论了回归和分类模型的不同特性。从那里，我们还了解了数据结构以及模型在数据上训练时的表现。然后，我们讨论了分析模型性能的不同方法以及解决在训练模型时可能遇到的不同问题的方法。

我们简要地了解了今天在机器学习模型中使用的不同包和库及其不同的用例。

我们还分析了不同主题，如主题建模和时间序列分析及其包含的内容。有了这些，我们能够查看解决这些类型问题的不同方法和技术。

最后，我们深入探讨了深度学习及其如何改进机器学习。我们讨论了两种不同的神经网络类型——卷积神经网络（CNNs）和循环神经网络（RNNs）——它们的结构以及它们的优点和用例。

在下一章中，我们将运用所学知识，开始探讨如何设计和构建一个端到端的机器学习系统及其包含的不同组件。
