- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Machine Learning with C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different approaches to making computers solve tasks. One of them
    is to define an explicit algorithm, and another one is to use implicit strategies
    based on mathematical and statistical methods. **Machine learning** (**ML**) is
    one of the implicit methods that uses mathematical and statistical approaches
    to solve tasks. It is an actively growing discipline, and a lot of scientists
    and researchers find it to be one of the best ways to move forward toward systems
    acting as human-level **artificial** **intelligence** (**AI**).
  prefs: []
  type: TYPE_NORMAL
- en: In general, ML approaches have the idea of searching patterns in a given dataset
    as their basis. Consider a recommendation system for a news feed, which provides
    the user with a personalized feed based on their previous activity or preferences.
    The software gathers information about the type of news article the user reads
    and calculates some statistics. For example, it could be the frequency of some
    topics appearing in a set of news articles. Then, it performs some predictive
    analytics, identifies general patterns, and uses them to populate the user’s news
    feed. Such systems periodically track a user’s activity, update the dataset, and
    calculate new trends for recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: ML is a rapidly growing field with diverse applications across various industries.
    In healthcare, it analyzes medical data to detect patterns and predict diseases
    and treatment outcomes. In finance, it aids in credit scoring, fraud detection,
    risk assessment, portfolio optimization, and algorithmic trading, enhancing decision-making
    and operations. E-commerce benefits from recommendation systems that suggest products
    based on customer behavior, boosting sales and satisfaction. Autonomous vehicles
    use ML for environmental perception, decision-making, and safe navigation.
  prefs: []
  type: TYPE_NORMAL
- en: Customer service is improved with chatbots and virtual assistants that handle
    queries and tasks. Cybersecurity leverages ML to detect and prevent cyberattacks
    by analyzing network traffic and identifying threats. Language translation tools
    use ML for accurate and efficient text translation. Image recognition, powered
    by computer vision algorithms, identifies objects, faces, and scenes in images
    and videos, supporting applications such as facial recognition and content moderation.
    Speech recognition in voice assistants such as Siri, Google Assistant, and Alexa
    relies on ML for understanding and responding to user commands. These examples
    illustrate the vast potential of ML in shaping our lives.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter describes what ML is and which tasks can be solved with ML, and
    discusses different approaches used in ML. It aims to show the minimally required
    math to start implementing ML algorithms. It also covers how to perform basic
    `Eigen`, `xtensor`, `ArrayFire`, `Blaze`, and `Dlib`, and also explains the linear
    regression task as an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the fundamentals of ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of linear algebra
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of a linear regression example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the fundamentals of ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different approaches to creating and training ML models. In this section,
    we show what these approaches are and how they differ. Apart from the approach
    we use to create an ML model, there are also parameters that manage how this model
    behaves in the training and evaluation processes. Model parameters can be divided
    into two distinct groups, which should be configured in different ways. The first
    group of parameters is the model weights in ML algorithms that are used to adjust
    the model’s predictions. They are assigned numerical values during the training
    process, and these values determine how the model makes decisions or predictions
    based on new data. The second group is the model hyperparameters that control
    the behavior of an ML model during training. They are not learned from the data
    like other parameters in the model but, rather, are set by the user or algorithm
    before training begins. The last crucial part of the ML process is the technique
    that we use to train a model. Usually, the training technique uses some numerical
    optimization algorithm that finds the minimal value of a target function. In ML,
    the target function is usually called a loss function and is used for penalizing
    the training algorithm when it makes errors. We discuss these concepts more precisely
    in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Venturing into the techniques of ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can divide ML approaches into two techniques, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning** is an approach based on the use of labeled data. Labeled
    data is a set of known data samples with corresponding known target outputs. Such
    data is used to build a model that can predict future outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning** is an approach that does not require labeled data
    and can search hidden patterns and structures in an arbitrary kind of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s have a look at each of the techniques in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Supervised ML algorithms usually take a limited set of labeled data and build
    models that can make reasonable predictions for new data. We can split supervised
    learning algorithms into two main parts, classification and regression techniques,
    described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification models predict some finite and distinct types of categories—this
    could be a label that identifies whether an email is spam or not, or whether an
    image contains a human face or not. Classification models are applied in speech
    and text recognition, object identification on images, credit scoring, and others.
    Typical algorithms for creating classification models are **support vector machine**
    (**SVM**), decision tree approaches, **k-nearest neighbors** (**KNN**), logistic
    regression, Naive Bayes, and neural networks. The following chapters describe
    the details of some of these algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression models predict continuous responses such as changes in temperature
    or values of currency exchange rates. Regression models are applied in algorithmic
    trading, forecasting of electricity load, revenue prediction, and others. Creating
    a regression model usually makes sense if the output of the given labeled data
    is real numbers. Typical algorithms for creating regression models are linear
    and multivariate regressions, polynomial regression models, and stepwise regressions.
    We can use decision tree techniques and neural networks to create regression models
    too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following chapters describe the details of some of these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms do not use labeled datasets. They create models
    that use intrinsic relations in data to find hidden patterns that they can use
    for making predictions. The most well-known unsupervised learning technique is
    **clustering**. Clustering involves dividing a given set of data into a limited
    number of groups according to some intrinsic properties of data items. Clustering
    is applied in market research, different types of exploratory analysis, **deoxyribonucleic
    acid** (**DNA**) analysis, image segmentation, and object detection. Typical algorithms
    for creating models for performing clustering are k-means, k-medoids, Gaussian
    mixture models, hierarchical clustering, and hidden Markov models. Some of these
    algorithms are explained in the following chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with ML models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can interpret ML models as functions that take different types of parameters.
    Such functions provide outputs for given inputs based on the values of these parameters.
    Developers can configure the behavior of ML models for solving problems by adjusting
    model parameters. Training an ML model can usually be treated as a process of
    searching for the best combination of its parameters. We can split the ML model’s
    parameters into two types. The first type consists of parameters internal to the
    model, and we can estimate their values from the training (input) data. The second
    type consists of parameters external to the model, and we cannot estimate their
    values from training data. Parameters that are external to the model are usually
    called **hyperparameters**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Internal parameters have the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: They are necessary for making predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They define the quality of the model on the given problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can learn them from training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually, they are a part of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model contains a fixed number of internal parameters, it is called **parametric**.
    Otherwise, we can classify it as **non-parametric**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of internal parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Weights of **artificial neural** **networks** (**ANNs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector values for SVM models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial coefficients for linear regression or logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ANNs are computer systems inspired by the structure and function of biological
    neural networks in the human brain. They are composed of interconnected nodes,
    or neurons, that process and transmit information. ANNs are designed to learn
    patterns and relationships from data, allowing them to make predictions or decisions
    based on new inputs. The learning process involves adjusting the weights and biases
    of the connections between neurons to improve the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, hyperparameters have the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: They are used to configure algorithms that estimate model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The practitioner usually specifies them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Their estimation is often based on using heuristics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are specific to a concrete modeling problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is hard to know the best values for a model’s hyperparameters for a specific
    problem. Also, practitioners usually need to perform additional research on how
    to tune required hyperparameters so that a model or a training algorithm behaves
    in the best way. Practitioners use rules of thumb, copying values from similar
    projects, as well as special techniques such as grid search for hyperparameter
    estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of hyperparameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: C and sigma parameters used in the SVM algorithm for a classification quality
    configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate parameter that is used in the neural network training process
    to configure algorithm convergence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *k* value that is used in the KNN algorithm to configure the number of neighbors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model parameter estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Model parameter estimation** usually uses some optimization algorithm. The
    speed and quality of the resulting model can significantly depend on the optimization
    algorithm chosen. Research on optimization algorithms is a popular topic in industry,
    as well as in academia. ML often uses optimization techniques and algorithms based
    on the optimization of a loss function. A function that evaluates how well a model
    predicts the data is called a **loss function**. If predictions are very different
    from the target outputs, the loss function will return a value that can be interpreted
    as a bad one, usually a large number. In such a way, the loss function penalizes
    an optimization algorithm when it moves in the wrong direction. So, the general
    idea is to minimize the value of the loss function to reduce penalties. There
    is no single universal loss function for optimization algorithms. Different factors
    determine how to choose a loss function. Examples of such factors are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Specifics of the given problem—for example, whether it is a regression or a
    classification model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ease of calculating derivatives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percentage of outliers in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In ML, the term **optimizer** is used to define an algorithm that connects
    a loss function and a technique for updating model parameters in response to the
    values of the loss function. So, optimizers tune ML models to predict target values
    for new data in the most accurate way by fitting model parameters. Optimizers
    or optimization algorithms play a crucial role in training ML models. They help
    find the best parameters for a model, which can improve its performance and accuracy.
    They have wide applications in various fields such as image recognition, natural
    language processing, and fraud detection. For example, in image classification
    tasks, optimization algorithms can be used to train deep neural networks to accurately
    identify objects in images. There are many optimizers: gradient descent, Adagrad,
    RMSProp, Adam, and others. Moreover, developing new optimizers is an active area
    of research. For example, there is the *ML and Optimization* research group at
    Microsoft (located in Redmond) whose research areas include combinatorial optimization,
    convex and non-convex optimization, and their application in ML and AI. Other
    companies in the industry also have similar research groups; there are many publications
    from Facebook Research, Amazon Research, and OpenAI groups.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will learn about what ML is and what its main conceptual parts are.
    So let’s learn the most important part of its mathematical basement: linear algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of linear algebra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concepts of linear algebra are essential for understanding the theory behind
    ML because they help us understand how ML algorithms work under the hood. Also,
    most ML algorithm definitions use linear algebra terms.
  prefs: []
  type: TYPE_NORMAL
- en: Linear algebra is not only a handy mathematical instrument but also the concepts
    of linear algebra can be very efficiently implemented with modern computer architectures.
    The rise of ML, and especially deep learning, began after significant performance
    improvement of the modern `Cuda` and `OpenCL`, and one example of a specialized
    linear algebra library is `cuBLAS`. Moreover, it became more common to use **general-purpose
    graphics processing units** (**GPGPUs**) because these turn the computational
    power of a modern GPU into a powerful general-purpose computing resource.
  prefs: []
  type: TYPE_NORMAL
- en: Also, `AVx`, `SSE`, and `MMx`. There is also the term `Eigen`, `xtensor`, `VienaCL`,
    and others, use them to improve computational performance.
  prefs: []
  type: TYPE_NORMAL
- en: Learning the concepts of linear algebra
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linear algebra is a big area. It is the section of algebra that studies objects
    of a linear nature: vector (or linear) spaces, linear representations, and systems
    of linear equations. The main tools used in linear algebra are determinants, matrices,
    conjugation, and tensor calculus.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand ML algorithms, we only need a small set of linear algebra concepts.
    However, to do research on new ML algorithms, a practitioner should have a deep
    understanding of linear algebra and calculus.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list contains the most valuable linear algebra concepts for understanding
    ML algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalar**: This is a single number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector**: This is an array of ordered numbers. Each element has a distinct
    index. Notation for vectors is a bold lowercase typeface for names and an italic
    typeface with a subscript for elements, as shown in the following example:![](img/B19849_Formula_01.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matrix**: This is a two-dimensional array of numbers. Each element has a
    distinct pair of indices. Notation for matrices is a bold uppercase typeface for
    names and an italic but not bold typeface with a comma-separated list of indices
    in subscripts for elements, as shown in the following example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Tensor**: This is an array of numbers arranged in a multidimensional regular
    grid, and represents generalizations of matrices. It is like a multidimensional
    matrix. For example, tensor *A* with dimensions 2 x 2 x 2 can look like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Linear algebra libraries and ML frameworks usually use the concept of a tensor
    instead of a matrix because they implement general algorithms, and a matrix is
    just a special case of a tensor with two dimensions. Also, we can consider a vector
    as a matrix of size *n* x *1*.
  prefs: []
  type: TYPE_NORMAL
- en: Basic linear algebra operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most common operations used for programming linear algebra algorithms are
    the following ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Element-wise operations**: These are performed in an element-wise manner
    on vectors, matrices, or tensors of the same size. The resulting elements will
    be the result of operations on corresponding input elements, as shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_04.jpg)![](img/B19849_Formula_05.jpg)![](img/B19849_Formula_06.jpg)![](img/B19849_Formula_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following example shows the element-wise summation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Dot product**: There are two types of multiplication for tensors and matrices
    in linear algebra—one is just element-wise, and the second is the dot product.
    The dot product deals with two equal-length series of numbers and returns a single
    number. This operation applied on matrices or tensors requires that the matrix
    or tensor *A* has the same number of columns as the number of rows in the matrix
    or tensor *B*. The following example shows the dot-product operation in the case
    when *A* is an *n x m* matrix and *B* is an *m x* *p* matrix:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Transposing**: The transposing of a matrix is an operation that flips the
    matrix over its diagonal, which leads to the flipping of the column and row indices
    of the matrix, resulting in the creation of a new matrix. In general, it is swapping
    matrix rows with columns. The following example shows how transposing works:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Norm**: This operation calculates the size of the vector; the result of this
    is a non-negative real number. The norm formula is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The generic name of this type of norm is an ![](img/B19849_Formula_12.png) norm
    for ![](img/B19849_Formula_13.png). Usually, we use more concrete norms such as
    an ![](img/B19849_Formula_14.png) norm with *p = 2*, which is known as the Euclidean
    norm, and we can interpret it as the Euclidean distance between points. Another
    widely used norm is the squared ![](img/B19849_Formula_14.png) norm, whose calculation
    formula is ![](img/B19849_Formula_16.png). The squared ![](img/B19849_Formula_14.png)
    norm is more suitable for mathematical and computational operations than the ![](img/B19849_Formula_14.png)
    norm. Each partial derivative of the squared ![](img/B19849_Formula_14.png) norm
    depends only on the corresponding element of *x*, in comparison to the partial
    derivatives of the ![](img/B19849_Formula_14.png) norm, which depends on the entire
    vector; this property plays a vital role in optimization algorithms. Another widely
    used norm operation is the ![](img/B19849_Formula_21.png) norm with *p = 1*, which
    is commonly used in ML when we care about the difference between zero and nonzero
    elements. The *L^1* norm is also known as the **Manhattan distance**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inverting**: The inverse matrix is such a matrix that ![](img/B19849_Formula_22.png),
    where *I* is an identity matrix. The **identity matrix** is a matrix that does
    not change any vector when we multiply that vector by that matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have considered the main linear algebra concepts as well as operations on
    them. Using this math apparatus, we can define and program many ML algorithms.
    For example, we can use tensors and matrices to define training datasets for training,
    and scalars can be used as different types of coefficients. We can use element-wise
    operations to perform arithmetic operations with a whole dataset (a matrix or
    a tensor). For example, we can use element-wise multiplication to scale a dataset.
    We usually use transposing to change a view of a vector or matrix to make them
    suitable for the dot-product operation. The dot product is usually used to apply
    a linear function with weights expressed as matrix coefficients to a vector; for
    example, this vector can be a training sample. Also, dot-product operations are
    used to update model parameters expressed as matrix or tensor coefficients according
    to an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The norm operation is often used in formulas for loss functions because it naturally
    expresses the distance concept and can measure the difference between target and
    predicted values. The inverse matrix is a crucial concept for the analytical solving
    of linear equations systems. Such systems often appear in different optimization
    problems. However, calculating the inverse matrix is very computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor representation in computing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can represent tensor objects in computer memory in different ways. The most
    obvious method is a simple linear array in computer memory (**random-access memory**,
    or **RAM**). However, the linear array is also the most computationally effective
    data structure for modern CPUs. There are two standard practices to organize tensors
    with a linear array in memory: **row-major ordering** and **column-major ordering**.'
  prefs: []
  type: TYPE_NORMAL
- en: In row-major ordering, we place consecutive elements of a row in linear order
    one after the other, and each row is also placed after the end of the previous
    one. In column-major ordering, we do the same but with the column elements. Data
    layouts have a significant impact on computational performance because the speed
    of traversing an array relies on modern CPU architectures that work with sequential
    data more efficiently than with non-sequential data. CPU caching effects are the
    reasons for such behavior. Also, a contiguous data layout makes it possible to
    use SIMD vectorized instructions that work with sequential data more efficiently,
    and we can use them as a type of parallel processing.
  prefs: []
  type: TYPE_NORMAL
- en: Different libraries, even in the same programming language, can use different
    ordering. For example, `Eigen` uses column-major ordering, but `PyTorch` uses
    row-major ordering. So, developers should be aware of internal tensor representation
    in libraries they use, and also take care of this when performing data loading
    or implementing algorithms from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, in the row-major data layout, members of the matrix will have the following
    layout in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **0** | **1** | **2** | **3** | **4** | **5** |'
  prefs: []
  type: TYPE_TB
- en: '| a11 | a12 | a13 | a21 | a22 | a23 |'
  prefs: []
  type: TYPE_TB
- en: Table 1.1 – The row-major data layout example
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the column-major data layout, order layout will be next, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **0** | **1** | **2** | **3** | **4** | **5** |'
  prefs: []
  type: TYPE_TB
- en: '| a11 | a21 | a12 | a22 | a13 | a23 |'
  prefs: []
  type: TYPE_TB
- en: Table 1.2 – The column-major data layout example
  prefs: []
  type: TYPE_NORMAL
- en: Linear algebra API samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s consider some C++ linear algebra **application programming interfaces**
    (**APIs**) and look at how we can use them for creating linear algebra primitives
    and performing algebra operations with them.
  prefs: []
  type: TYPE_NORMAL
- en: Using Eigen
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`Eigen` is a general-purpose linear algebra C++ library. In Eigen, all matrices
    and vectors are objects of the `Matrix` template class, and the vector is a specialization
    of the matrix type, with either one row or one column. Tensor objects are not
    presented in official APIs but exist as submodules.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define the type for a matrix with known 3 x 3 dimensions and a floating-point
    data type like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can define a column vector in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Eigen already has a lot of predefined types for vector and matrix objects—for
    example, `Eigen::Matrix3f` (floating-point 3 x 3 matrix type) or `Eigen::RowVector2f`
    (floating-point 1 x 2 vector type). Also, Eigen is not limited to matrices whose
    dimensions we know at compile time. We can define matrix types that will take
    the number of rows or columns at initialization during runtime. To define such
    types, we can use a special type variable for the `Matrix` class template argument
    named `Eigen::Dynamic`. For example, to define a matrix of doubles with dynamic
    dimensions, we can use the following definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Objects initialized from the types we defined will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To put some values into these objects, we can use several approaches. We can
    use special predefined initialization functions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the *comma-initializer* syntax, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This code construction initializes the matrix values in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use direct element access to set or change matrix coefficients. The
    following code sample shows how to use the `()` operator for such an operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the object of the `Map` type to wrap an existent C++ array or vector
    in the `Matrix` type object. This kind of mapping object will use memory and values
    from the underlying object, and will not allocate the additional memory and copy
    the values. The following snippet shows how to use the `Map` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use initialized matrix objects in mathematical operations. Matrix and
    vector arithmetic operations in the `Eigen` library are offered either through
    overloads of standard C++ arithmetic operators such as `+`, `-`, or `*`, or through
    methods such as `dot()` and `cross()`. The following code sample shows how to
    express general math operations in Eigen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Notice that, in Eigen, arithmetic operators such as `+` do not perform any computation
    by themselves. These operators return an *expression object*, which describes
    what computation to perform. The actual computation happens later when the whole
    expression is evaluated, typically in the `=` arithmetic operator. It can lead
    to some strange behaviors, primarily if a developer uses the `auto` keyword too
    frequently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, we need to perform operations only on a part of the matrix. For
    this purpose, Eigen provides the `block` method, which takes four parameters:
    `i,j,p,q`. These parameters are the block size, `p,q`, and the starting point,
    `i,j`. The following code shows how to use this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two more methods to access rows and columns by index, which are also
    a type of `block` operation. The following snippet shows how to use the `col`
    and `row` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Another important feature of linear algebra libraries is broadcasting, and
    Eigen supports this with the `colwise` and `rowwise` methods. Broadcasting can
    be interpreted as a matrix by replicating it in one direction. Take a look at
    the following example of how to add a vector to each column of the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This operation has the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using xtensor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `xtensor` library is a C++ library for numerical analysis with multidimensional
    array expressions. Containers of xtensor are inspired by NumPy, the Python array
    programming library. ML algorithms are mainly described using Python and NumPy,
    so this library can make it easier to move them to C++. The following container
    classes implement multidimensional arrays in the `xtensor` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `xarray` type is a dynamically sized multidimensional array, as shown in
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Dynamic size for the `xarray` type means that this shape can be changed at compilation
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `xtensor` type is a multidimensional array whose range is fixed at compilation
    time. Exact dimension values can be configured in the initialization step, as
    shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `xtensor_fixed` type is a multidimensional array with a dimension shape
    fixed at compile time, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `xtensor` library also implements arithmetic operators with expression template
    techniques such as Eigen (this is a common approach for math libraries implemented
    in C++). So, the computation happens lazily, and the actual result is calculated
    when the whole expression is evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Lazy computation**, also known as **lazy evaluation** or **call-by-need evaluation**,
    is a strategy in programming where the evaluation of an expression is delayed
    until its value is actually needed.'
  prefs: []
  type: TYPE_NORMAL
- en: This contrasts with eager evaluation, where expressions are evaluated immediately
    upon encountering them. The container definitions are also expressions. There
    is also a function to force an expression evaluation named `xt::eval` in the `xtensor`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different kinds of container initialization in the `xtensor` library.
    Initialization of `xtensor` arrays can be done with C++ initializer lists, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `xtensor` library also has builder functions for special tensor types.
    The following snippet shows some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we can map existing C++ arrays into the `xtensor` container with the
    `xt::adapt` function. This function returns the object that uses the memory and
    values from the underlying object, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use direct access to container elements, with the `()` operator, to
    set or change tensor values, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `xtensor` library implements linear algebra arithmetic operations through
    overloads of standard C++ arithmetic operators such as `+`, `-`, and `*`. To use
    other operations such as dot-product operations, we have to link an application
    with the library named `xtensor-blas`. These operators are declared in the `xt::linalg`
    namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the use of arithmetic operations with the `xtensor`
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To get partial access to the `xtensor` containers, we can use the `xt::view`
    function. The `view` function returns a new tensor object that shares the same
    underlying data with the original tensor but with a different shape or strides.
    This allows you to access the data in the tensor in a different way, without actually
    changing the underlying data itself. The following sample shows how this function
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This operation takes a rectangular block from the tensor, which looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `xtensor` library implements automatic broadcasting in most cases. When
    the operation involves two arrays of different dimensions, it transmits the array
    with the smaller dimension across the leading dimension of the other array, so
    we can directly add a vector to a matrix. The following code sample shows how
    easy it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Using Blaze
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`Blaze` is a general-purpose high-performance C++ library for dense and sparse
    linear algebra. There are different classes to represent matrices and vectors
    in Blaze.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define the type for a matrix with known dimensions and a floating-point
    data type, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can define a vector in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, Blaze is not limited to matrices whose dimensions we know at compile
    time. We can define matrix types that will take the number of rows or columns
    at initialization during runtime. To define such types, we can use the `blaze::DynamicMatrix`
    or `blaze::DynamicVector` classes. For example, to define a matrix of doubles
    with dynamic dimensions, we can use the following definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Objects initialized from the types we defined will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To put some values into these objects, we can use several approaches. We can
    use special predefined initialization functions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This code construction initializes the matrix values in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use direct element access to set or change matrix coefficients. The
    following code sample shows how to use the `()` operator for such an operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the object of the `blaze::CustomVector` type to wrap an existent
    C++ array or vector into the `Matrix` or `Vector` type object. This kind of mapping
    object will use memory and values from the underlying object, and will not allocate
    the additional memory and copy the values. The following snippet shows how to
    use this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use initialized matrix objects in mathematical operations. Notice that
    we used two parameters: `blaze::unaligned` and `blaze::unpadded`. The `unpadded`
    parameter may be used in some functions or methods to control the behavior of
    padding or truncation of arrays. This parameter can be important in certain scenarios
    where you want to avoid unnecessary padding or truncating of data during operations
    such as reshaping, slicing, or concatenating arrays. The `blaze::unaligned` parameter
    allows users to perform operations on unaligned data, which can be useful in certain
    scenarios where the data is not aligned to specific memory boundaries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrix and vector arithmetic operations in the `Blaze` library are offered
    either through overloads of standard C++ arithmetic operators such as `+`, `-`,
    or `*`, or through methods such as `dot()` and `cross()`. The following code sample
    shows how to express general math operations in Blaze:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Notice that, in Blaze, arithmetic operators such as `+` do not perform any computation
    by themselves. These operators return an *expression object*, which describes
    what computation to perform. The actual computation happens later when the whole
    expression is evaluated, typically in the `=` arithmetic operator or in a constructor
    of a concrete object. It can lead to some non-obvious behaviors, primarily if
    a developer uses the `auto` keyword too frequently. The library provides two functions,
    `eval()` and `evaluate()`, to evaluate a given expression. The `evaluate()` function
    assists in deducing the exact result type of the operation via the `auto` keyword,
    and the `eval()` function should be used to explicitly evaluate a sub-expression
    within a larger expression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, we need to perform operations only on a part of the matrix. For
    this purpose, Blaze provides the `blaze::submatrix` and `blaze::subvector` classes,
    which can be parameterized with template parameters. These parameters are the
    top-left starting point and the width and height of a region. Also, there are
    functions with the same names that take the same arguments and can be used in
    a runtime. The following code shows how to use this class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two more functions to access rows and columns by index, which are
    also a type of `block` operation. The following snippet shows how to use the `col`
    and `row` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In contrast to Eigen, Blaze doesn’t support implicit broadcasting. But there
    is the `blaze::expand()` function that can virtually expand a matrix or vector
    without actual memory allocation. The following code shows how to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this operation will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Using ArrayFire
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`ArrayFire` is a general-purpose high-performance C++ library for parallel
    computing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallel computing** is a method of solving complex problems by dividing
    them into smaller tasks and executing them simultaneously across multiple processors
    or cores. This approach can significantly speed up the processing time compared
    to sequential computing, making it an essential tool for data-intensive applications
    such as ML.'
  prefs: []
  type: TYPE_NORMAL
- en: It provides a single `array` type to represent matrices, volumes, and vectors.
    This array-based notation expresses computational algorithms in readable mathematical
    notation so users don’t need to express parallel computations explicitly. It has
    extensive vectorization and parallel batched operations. This library supports
    accelerated execution on CUDA and OpenCL devices. Another interesting feature
    of the `ArrayFire` library is that it optimizes memory usage and arithmetic calculations
    by runtime analysis of the executed code. This becomes possible by avoiding many
    temporary allocations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define the type for a matrix with known dimensions and a floating-point
    data type like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can define a 64-bit floating vector in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'To put some values into these objects, we can use several approaches. We can
    use special predefined initialization functions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This code construction initializes the matrix values in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the matrix was initialized in the column-major format. The `ArrayFire`
    library doesn’t support row-major initialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use direct element access to set or change matrix coefficients. The
    following code sample shows how to use the `()` operator for such an operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'One important difference from other libraries that we discussed is that you
    can’t map existent C/C++ array data to the ArrayFire `array` object, as it will
    be copied. The following snippet shows this situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Only memory allocated in CUDA or OpenCL devices will not be copied, but ArrayFire
    will take ownership of a pointer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use initialized array objects in mathematical operations. Arithmetic
    operations in the `ArrayFire` library are offered either through overloads of
    standard C++ arithmetic operators such as `+`, `-`, or `*`, or through methods
    such as `af::matmul`. The following code sample shows how to express general math
    operations in ArrayFire:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: In contrast to other libraries we already discussed, ArrayFire doesn’t extensively
    use template expression for joining arithmetical operations. This library uses
    a **just-in-time** (**JIT**) compilation engine that converts mathematical expressions
    into computational kernels for CUDA, OpenCL, or CPU devices. Also, this engine
    merges different operations together to provide the best performance. This operation
    fusion technology decreases the number of kernel calls and reduces global memory
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, we need to perform operations only on a part of the array. For this
    purpose, ArrayFire provides a special indexing technique. There are particular
    classes that can be used to express index sub-ranges for given dimensions. They
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows an example of how to access only the central part
    of a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'To access and update a particular row or column in the array, there are `row(i)`
    and `col(i)` methods specifying a single row or column. They can be used in the
    following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Also, to work with several rows or columns, there are the `rows(first,last)`
    and `cols(first,last)` methods specifying a span of rows or columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'ArrayFire doesn’t support implicit broadcasting but there is the `af::batchFunc`
    function that can be used to simulate and parallelize such functionality. In general,
    this function finds a batch dimension of data and applies the given function to
    multiple data chunks in parallel. The following code shows how to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this operation will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the vector was a column-major one.
  prefs: []
  type: TYPE_NORMAL
- en: Using Dlib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`Dlib` is a modern C++ toolkit containing ML algorithms and tools for creating
    computer vision software in C++. Most of the linear algebra tools in Dlib deal
    with dense matrices. However, there is also limited support for working with sparse
    matrices and vectors. In particular, the `Dlib` tools represent sparse vectors
    using the containers from the C++ **standard template** **library** (**STL**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main container types in Dlib to work with linear algebra: the
    `matrix` and `vector` classes. Matrix operations in Dlib are implemented using
    the expression templates technique, which allows them to eliminate the temporary
    matrix objects that would usually be returned from expressions such as `M =` `A+B+C+D`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a matrix sized at compile time in the following way, by specifying
    dimensions as template arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can create dynamically sized matrix objects. In such a case,
    we pass the matrix dimensions to the constructor, as shown in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Later, we can change the size of this matrix with the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We can initialize matrix values with a comma operator, as shown in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the previous libraries, we can wrap an existing C++ array to the matrix
    object, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we can access matrix elements with the `()` operator to modify or get
    a particular value, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Dlib` library has a set of predefined functions to initialize a matrix
    with values such as the identity matrix, ones, or random values, as illustrated
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Most linear algebra arithmetic operations in the `Dlib` library are implemented
    through overloads of standard C++ arithmetic operators such as `+`, `-`, or `*`.
    Other complex operations are provided by the library as standalone functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows the use of arithmetic operations in the `Dlib`
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'To work with partial access to matrices, Dlib provides a set of special functions.
    The following code sample shows how to use some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Broadcasting in the `Dlib` library can be modeled with the `set_rowm()`, `set_colm()`,
    and `set_subm()` functions that give modifier objects for a particular matrix
    row, column, or rectangular part of the original matrix. Objects returned from
    these functions support all set or arithmetic operations. The following code snippet
    shows how to add a vector to the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we learned about the main concepts of linear algebra and their
    implementation in different C++ libraries. We saw how to create matrices and tensors,
    and how to perform different mathematical operations with them. In the following
    section, we will see our first complete ML example—solving the regression problem
    with the linear regression approach.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider an example of the real-world supervised ML algorithm called linear
    regression. In general, **linear regression** is an approach for modeling a target
    value (dependent value) based on an explanatory value (independent value). This
    method is used for forecasting and finding relationships between values. We can
    classify regression methods by the number of inputs (independent variables) and
    the type of relationship between the inputs and outputs (dependent variables).
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression is the case where the number of independent variables
    is *1*, and there is a linear relationship between the independent (*x*) and dependent
    (*y*) variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear regression is widely used in different areas such as scientific research,
    where it can describe relationships between variables, as well as in applications
    within industry, such as revenue prediction. For example, it can estimate a trend
    line that represents the long-term movement in the stock price time-series data.
    It tells whether the interest value of a specific dataset has increased or decreased
    over the given period, as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Linear regression visualization](img/B19849_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Linear regression visualization
  prefs: []
  type: TYPE_NORMAL
- en: If we have one input variable (independent variable) and one output variable
    (dependent variable), the regression is called simple, and we use the term **simple
    linear regression** for it. With multiple independent variables, we call this
    **multiple linear regression** or **multivariable linear regression**. Usually,
    when we are dealing with real-world problems, we have a lot of independent variables,
    so we model such problems with multiple regression models. Multiple regression
    models have a universal definition that covers other types, so even simple linear
    regression is often defined using the multiple regression definition.
  prefs: []
  type: TYPE_NORMAL
- en: Solving linear regression tasks with different libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assume that we have a dataset, ![](img/B19849_Formula_28.png), so that we can
    express the linear relation between *y* and *x* with mathematical formula in the
    following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *p* is the dimension of the independent variable, and *T* denotes the
    transpose, so that ![](img/B19849_Formula_30.png) is the inner product between
    vectors ![](img/B19849_Formula_31.png) and *β*. Also, we can rewrite the previous
    expression in matrix notation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_32.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B19849_Formula_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ','
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_34.jpg)![](img/B19849_Formula_35.jpg)![](img/B19849_Formula_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding matrix notation can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y*: This is a vector of observed target values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*: This is a matrix of row-vectors, ![](img/B19849_Formula_31.png), which
    are known as explanatory or independent values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ß: This is a (*p+1*) dimensional parameters vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ε: This is called an error term or noise. This variable captures all other
    factors that influence the *y*-dependent variable other than the regressors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we are considering simple linear regression, *p* is equal to 1, and the
    equation will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The goal of the linear regression task is to find parameter vectors that satisfy
    the previous equation. Usually, there is no exact solution to such a system of
    linear equations, so the task is to estimate parameters that satisfy these equations
    with some assumptions. One of the most popular estimation approaches is one based
    on the principle of least squares: minimizing the sum of the squares of the differences
    between the observed dependent variable in the given dataset and those predicted
    by the linear function. This is called the **ordinary least squares** (**OLS**)
    estimator. So, the task can be formulated with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding formula, the objective function, *S*, is given by the following
    matrix notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This minimization problem has a unique solution, in the case that the *p* columns
    of the *x* matrix are linearly independent. We can get this solution by solving
    the *normal equation*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Linear algebra libraries can solve such equations directly with an analytical
    approach, but it has one significant disadvantage—computational cost. In the case
    of large dimensions of *y* and *x*, requirements for computer memory amount and
    computational time are too big to solve real-world tasks.
  prefs: []
  type: TYPE_NORMAL
- en: So, usually, this minimization task is solved with iterative approaches. **gradient
    descent** (**GD**) is an example of such an algorithm. GD is a technique based
    on the observation that if the function ![](img/B19849_Formula_42.png) is defined
    and is differentiable in a neighborhood of a point ![](img/B19849_Formula_43.png),
    then ![](img/B19849_Formula_44.png) decreases fastest when it goes in the direction
    of the negative gradient of *S* at point ![](img/B19849_Formula_43.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can change our ![](img/B19849_Formula_46.png) objective function to a form
    more suitable for an iterative approach. We can use the **mean squared error**
    (**MSE**) function, which measures the difference between the estimator and the
    estimated value, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the case of multiple regression, we take partial derivatives for this function
    for each of *x* components, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, in the case of linear regression, we take the following derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_49.jpg)![](img/B19849_Formula_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The whole algorithm has the following description:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize β with zeros.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a value for the learning rate parameter that controls how much we are
    adjusting parameters during the learning procedure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the following values of β:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_51.jpg)![](img/B19849_Formula_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Repeat steps 1–3 a number of times or until the MSE value reaches a reasonable
    amount.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The previously described algorithm is one of the simplest supervised ML algorithms.
    We described it with the linear algebra concepts we introduced earlier in the
    chapter. Later, it became more evident that almost all ML algorithms use linear
    algebra under the hood. Linear regression is widely used in various industries
    for predictive analysis, forecasting, and decision-making. Here are some real-world
    examples of linear regression applications in finance, marketing, and healthcare.
    Linear regression can be used to predict stock prices based on historical data
    such as company earnings, interest rates, and economic indicators. This helps
    investors make informed decisions about when to buy or sell stocks. Linear regression
    models can be built to predict customer behavior based on demographic information,
    purchase history, and other relevant data. This allows marketers to target their
    campaigns more effectively and optimize their marketing spend. Linear regression
    is used to analyze medical data to identify patterns and relationships that can
    help improve patient outcomes. For example, it can be used to study the impact
    of certain treatments on patient health.
  prefs: []
  type: TYPE_NORMAL
- en: The following samples show the higher-level API in different linear algebra
    libraries for solving the *linear regression* task, and we provide them to show
    how libraries can simplify the complicated math used underneath. We will give
    the details of the APIs used in these samples in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Solving linear regression tasks with Eigen
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several iterative methods for solving problems of the ![](img/B19849_Formula_53.png)
    form in the `Eigen` library. The `LeastSquaresConjugateGradient` class is one
    of them, which allows us to solve linear regression problems with the conjugate
    gradient algorithm. The `ConjugateGradient` algorithm can converge more quickly
    to the function’s minimum than regular GD but requires that matrix *A* is positively
    defined to guarantee numerical stability. The `LeastSquaresConjugateGradient`
    class has two main settings: the maximum number of iterations and a tolerance
    threshold value that is used as a stopping criterion as an upper bound to the
    relative residual error, as illustrated in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'For new `x` inputs, we can predict new `y` values with matrix operations, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we can calculate the parameter’s `b` vector (the linear regression task
    solution) by solving the *normal equation* directly, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Solving linear regression tasks with Blaze
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since Blaze is just a mathematical library, there are no special classes or
    functions to solve linear regression tasks. However, the normal equation approach
    can be easily implemented. Let’s see how to define and solve linear regression
    tasks with Blaze:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume we have our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'So we can find linear regression coefficients in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can use estimated coefficients for making predictions on new data.
    The following code snippet shows how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we virtually expand the coefficients vector to perform element-wise
    multiplication with the x data.
  prefs: []
  type: TYPE_NORMAL
- en: Solving linear regression tasks with ArrayFire
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ArrayFire also doesn’t have special functions and classes to solve this type
    of problem. However, as it has all the necessary mathematical abstraction, the
    normal equation approach can be applied. Another approach is an iterative one
    that uses GD. This algorithm was described in the first section of this chapter.
    Such a technique eliminates the necessity of calculating inverse matrices, making
    it possible to apply it to a larger amount of training data. Calculating an inverse
    for a large matrix is a very performance-expensive operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define a lambda function to calculate prediction values from data and
    coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Assume that we have training data defined in the `x` and `y` variables. We
    define the `train_weights` variable to hold and update coefficients that want
    to learn from the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can define the GD loop where we will iteratively update coefficients.
    The following code snippet shows how to implement it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The most important part of this loop is calculating the prediction error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Another important part is calculating the gradient values based on partial
    derivatives related to each of the coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We join these values into one vector to make a single expression for updating
    the training parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We can stop this training loop after a number of iterations or when the cost
    function value reaches some appropriate convergence value. The cost function value
    can be calculated in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: You can see that this is just a sum of squared errors for all training samples.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with Dlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Dlib` library provides the `krr_trainer` class, which can get the template
    argument of the `linear_kernel` type to solve linear regression tasks. This class
    implements direct analytical solving for this type of problem with the kernel
    ridge regression algorithm, as illustrated in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'For new `x` inputs, we can predict new `y` values in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we learned how to solve the linear regression problem with
    different C++ libraries. We saw that some of them contain the complete algorithm
    implementation that can be easily applied, and we saw how to implement this approach
    from scratch using just basic linear algebra primitives.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned what ML is, how it differs from other computer algorithms,
    and how it became so popular. We also became familiar with the necessary mathematical
    background required to begin working with ML algorithms. We looked at software
    libraries that provide APIs for linear algebra and also implemented our first
    ML algorithm—linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: There are other linear algebra libraries for C++. Moreover, the popular deep
    learning frameworks use their own implementations of linear algebra libraries.
    For example, the MXNet framework is based on the `mshadow` library, and the PyTorch
    framework is based on the ATen library. Some of these libraries can use GPU or
    special CPU instructions to speed up calculations. Such features do not usually
    change the API but require some additional library initialization settings or
    explicit object conversion to different backends such as CPUs or GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Real ML projects can be challenging and complex. Common pitfalls include data
    quality issues, overfitting and underfitting, choosing the wrong model, and not
    having enough computing resources. Poor data quality can also affect model performance.
    It’s essential to clean and preprocess data to remove outliers, handle missing
    values, and transform features for better representation. The model should be
    chosen based on the nature of the problem and the available data. Overfitting
    occurs when the model memorizes the training data instead of learning general
    patterns, while underfitting happens when the model cannot capture the underlying
    structure of the data. To avoid these pitfalls, it is important to have a clear
    understanding of the problem, use appropriate preprocessing techniques for data,
    choose the right model, and evaluate the performance using metrics that are relevant
    to the task. Best practices in ML also include monitoring the model’s performance
    in production and making adjustments as needed. We will discuss the details of
    these techniques throughout the book. In the next two chapters, we will learn
    more about available software tools that are necessary to implement more complicated
    algorithms, and we will also learn more theoretical background on how to manage
    ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Basic Linear Algebra for Deep* *Learning*: [https://towardsdatascience.com/linear-algebra-for-deep-learning-f21d7e7d7f23](https://towardsdatascience.com/linear-algebra-for-deep-learning-f21d7e7d7f23)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Learning*, an MIT Press book: [https://www.deeplearningbook.org/contents/linear_algebra.html](https://www.deeplearningbook.org/contents/linear_algebra.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What is Machine* *Learning?*: [https://www.mathworks.com/discovery/machine-learning.html](https://www.mathworks.com/discovery/machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Eigen` library documentation: [https://gitlab.com/libeigen/eigen](https://gitlab.com/libeigen/eigen)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `xtensor` library documentation: [https://xtensor.readthedocs.io/en/latest/](https://xtensor.readthedocs.io/en/latest/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Dlib` library documentation: [http://dlib.net/](http://dlib.net/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `blaze` library documentation: [https://bitbucket.org/blaze-lib/blaze/wiki/Home](https://bitbucket.org/blaze-lib/blaze/wiki/Home)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ArrayFire library documentation: [https://arrayfire.org/docs/index.htm](https://arrayfire.org/docs/index.htm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
