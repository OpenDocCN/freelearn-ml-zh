- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Introduction to Machine Learning with C++
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用C++介绍机器学习
- en: There are different approaches to making computers solve tasks. One of them
    is to define an explicit algorithm, and another one is to use implicit strategies
    based on mathematical and statistical methods. **Machine learning** (**ML**) is
    one of the implicit methods that uses mathematical and statistical approaches
    to solve tasks. It is an actively growing discipline, and a lot of scientists
    and researchers find it to be one of the best ways to move forward toward systems
    acting as human-level **artificial** **intelligence** (**AI**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机解决任务的方法有很多种。其中一种方法是定义一个明确的算法，另一种方法是使用基于数学和统计方法的隐式策略。**机器学习**（**ML**）是使用数学和统计方法解决任务的隐式方法之一。它是一个正在迅速发展的学科，许多科学家和研究人员认为它是向具有人类水平**人工智能**（**AI**）行为的系统迈进的最佳途径之一。
- en: In general, ML approaches have the idea of searching patterns in a given dataset
    as their basis. Consider a recommendation system for a news feed, which provides
    the user with a personalized feed based on their previous activity or preferences.
    The software gathers information about the type of news article the user reads
    and calculates some statistics. For example, it could be the frequency of some
    topics appearing in a set of news articles. Then, it performs some predictive
    analytics, identifies general patterns, and uses them to populate the user’s news
    feed. Such systems periodically track a user’s activity, update the dataset, and
    calculate new trends for recommendations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，机器学习方法的基本思想是在给定的数据集中寻找模式。考虑一个新闻推送的推荐系统，它根据用户的先前活动或偏好为用户提供个性化的推送。软件收集有关用户阅读的新闻文章类型的资料，并计算一些统计数据。例如，它可能是某些主题在一组新闻文章中出现的频率。然后，它执行一些预测分析，识别一般模式，并使用这些模式来填充用户的新闻推送。这样的系统定期跟踪用户的活动，更新数据集，并为推荐计算新的趋势。
- en: ML is a rapidly growing field with diverse applications across various industries.
    In healthcare, it analyzes medical data to detect patterns and predict diseases
    and treatment outcomes. In finance, it aids in credit scoring, fraud detection,
    risk assessment, portfolio optimization, and algorithmic trading, enhancing decision-making
    and operations. E-commerce benefits from recommendation systems that suggest products
    based on customer behavior, boosting sales and satisfaction. Autonomous vehicles
    use ML for environmental perception, decision-making, and safe navigation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个快速发展的领域，在各个行业中都有广泛的应用。在医疗保健领域，它通过分析医疗数据来检测模式和预测疾病及治疗效果。在金融领域，它有助于信用评分、欺诈检测、风险评估、投资组合优化和算法交易，从而增强决策和运营。电子商务通过基于客户行为的推荐系统来提升销售和满意度。自动驾驶汽车使用机器学习进行环境感知、决策和安全的导航。
- en: Customer service is improved with chatbots and virtual assistants that handle
    queries and tasks. Cybersecurity leverages ML to detect and prevent cyberattacks
    by analyzing network traffic and identifying threats. Language translation tools
    use ML for accurate and efficient text translation. Image recognition, powered
    by computer vision algorithms, identifies objects, faces, and scenes in images
    and videos, supporting applications such as facial recognition and content moderation.
    Speech recognition in voice assistants such as Siri, Google Assistant, and Alexa
    relies on ML for understanding and responding to user commands. These examples
    illustrate the vast potential of ML in shaping our lives.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 客户服务通过聊天机器人和虚拟助手得到改善，它们可以处理查询和任务。网络安全利用机器学习通过分析网络流量和识别威胁来检测和预防网络攻击。语言翻译工具使用机器学习进行准确高效的文字翻译。图像识别，借助计算机视觉算法，在图像和视频中识别对象、人脸和场景，支持人脸识别和内容审核等应用。语音助手如Siri、Google
    Assistant和Alexa中的语音识别依赖于机器学习来理解和响应用户指令。这些例子展示了机器学习在塑造我们生活方面的巨大潜力。
- en: This chapter describes what ML is and which tasks can be solved with ML, and
    discusses different approaches used in ML. It aims to show the minimally required
    math to start implementing ML algorithms. It also covers how to perform basic
    `Eigen`, `xtensor`, `ArrayFire`, `Blaze`, and `Dlib`, and also explains the linear
    regression task as an example.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了机器学习的概念以及哪些任务可以使用机器学习来解决，并讨论了机器学习中使用的不同方法。它旨在展示开始实现机器学习算法所需的最基本的数学知识。它还涵盖了如何执行基本的`Eigen`、`xtensor`、`ArrayFire`、`Blaze`和`Dlib`操作，并以线性回归任务为例进行解释。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding the fundamentals of ML
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解机器学习的基本原理
- en: An overview of linear algebra
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性代数概述
- en: An overview of a linear regression example
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归示例概述
- en: Understanding the fundamentals of ML
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习的基本原理
- en: There are different approaches to creating and training ML models. In this section,
    we show what these approaches are and how they differ. Apart from the approach
    we use to create an ML model, there are also parameters that manage how this model
    behaves in the training and evaluation processes. Model parameters can be divided
    into two distinct groups, which should be configured in different ways. The first
    group of parameters is the model weights in ML algorithms that are used to adjust
    the model’s predictions. They are assigned numerical values during the training
    process, and these values determine how the model makes decisions or predictions
    based on new data. The second group is the model hyperparameters that control
    the behavior of an ML model during training. They are not learned from the data
    like other parameters in the model but, rather, are set by the user or algorithm
    before training begins. The last crucial part of the ML process is the technique
    that we use to train a model. Usually, the training technique uses some numerical
    optimization algorithm that finds the minimal value of a target function. In ML,
    the target function is usually called a loss function and is used for penalizing
    the training algorithm when it makes errors. We discuss these concepts more precisely
    in the following sections.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和训练机器学习模型有不同的方法。在本节中，我们展示了这些方法是什么以及它们之间的区别。除了我们用来创建机器学习模型的方法之外，还有一些参数管理模型在训练和评估过程中的行为。模型参数可以分为两组，它们应该以不同的方式配置。第一组参数是机器学习算法中的模型权重，用于调整模型的预测。这些权重在训练过程中被分配数值，这些数值决定了模型如何根据新数据做出决策或预测。第二组是模型超参数，它们控制机器学习模型在训练过程中的行为。这些超参数不像模型中的其他参数那样从数据中学习，而是在训练开始之前由用户或算法设置。机器学习过程的最后关键部分是我们用来训练模型的技术。通常，训练技术使用某种数值优化算法来找到目标函数的最小值。在机器学习中，目标函数通常被称为损失函数，用于在训练算法出错时对其进行惩罚。我们将在以下章节中更精确地讨论这些概念。
- en: Venturing into the techniques of ML
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索机器学习技术
- en: 'We can divide ML approaches into two techniques, as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将机器学习方法分为两种技术，如下所示：
- en: '**Supervised learning** is an approach based on the use of labeled data. Labeled
    data is a set of known data samples with corresponding known target outputs. Such
    data is used to build a model that can predict future outputs.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**是一种基于使用标记数据的方法。标记数据是一组已知数据样本及其对应的已知目标输出。此类数据用于构建一个可以预测未来输出的模型。'
- en: '**Unsupervised learning** is an approach that does not require labeled data
    and can search hidden patterns and structures in an arbitrary kind of data.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**是一种不需要标记数据的处理方法，可以在任意类型的数据中搜索隐藏的模式和结构。'
- en: Let’s have a look at each of the techniques in detail.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看每种技术。
- en: Supervised learning
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'Supervised ML algorithms usually take a limited set of labeled data and build
    models that can make reasonable predictions for new data. We can split supervised
    learning algorithms into two main parts, classification and regression techniques,
    described as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法通常使用有限的一组标记数据来构建可以对新数据进行合理预测的模型。我们可以将监督学习算法分为两个主要部分，即分类和回归技术，具体描述如下：
- en: Classification models predict some finite and distinct types of categories—this
    could be a label that identifies whether an email is spam or not, or whether an
    image contains a human face or not. Classification models are applied in speech
    and text recognition, object identification on images, credit scoring, and others.
    Typical algorithms for creating classification models are **support vector machine**
    (**SVM**), decision tree approaches, **k-nearest neighbors** (**KNN**), logistic
    regression, Naive Bayes, and neural networks. The following chapters describe
    the details of some of these algorithms.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类模型预测一些有限且不同的类别类型——这可能是标识电子邮件是否为垃圾邮件的标签，或者图像中是否包含人脸的标签。分类模型应用于语音和文本识别、图像中的对象识别、信用评分等领域。创建分类模型的典型算法包括**支持向量机**（**SVM**）、决策树方法、**k-最近邻**（**KNN**）、逻辑回归、朴素贝叶斯和神经网络。以下章节将描述这些算法的一些细节。
- en: Regression models predict continuous responses such as changes in temperature
    or values of currency exchange rates. Regression models are applied in algorithmic
    trading, forecasting of electricity load, revenue prediction, and others. Creating
    a regression model usually makes sense if the output of the given labeled data
    is real numbers. Typical algorithms for creating regression models are linear
    and multivariate regressions, polynomial regression models, and stepwise regressions.
    We can use decision tree techniques and neural networks to create regression models
    too.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归模型预测连续响应，例如温度变化或货币汇率值。回归模型应用于算法交易、电力负荷预测、收入预测等领域。如果给定标记数据的输出是实数，则创建回归模型通常是有意义的。创建回归模型的典型算法包括线性回归和多变量回归、多项式回归模型以及逐步回归。我们也可以使用决策树技术和神经网络来创建回归模型。
- en: The following chapters describe the details of some of these algorithms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节描述了这些算法的一些细节。
- en: Unsupervised learning
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Unsupervised learning algorithms do not use labeled datasets. They create models
    that use intrinsic relations in data to find hidden patterns that they can use
    for making predictions. The most well-known unsupervised learning technique is
    **clustering**. Clustering involves dividing a given set of data into a limited
    number of groups according to some intrinsic properties of data items. Clustering
    is applied in market research, different types of exploratory analysis, **deoxyribonucleic
    acid** (**DNA**) analysis, image segmentation, and object detection. Typical algorithms
    for creating models for performing clustering are k-means, k-medoids, Gaussian
    mixture models, hierarchical clustering, and hidden Markov models. Some of these
    algorithms are explained in the following chapters of this book.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法不使用标记数据集。它们创建使用数据内在关系来寻找隐藏模式并用于做出预测的模型。最著名的无监督学习技术是**聚类**。聚类涉及根据数据项的一些内在属性将给定的数据集划分为有限数量的组。聚类应用于市场研究、不同类型的探索性分析、**脱氧核糖核酸**（**DNA**）分析、图像分割和目标检测。创建执行聚类模型的典型算法包括k-means、k-medoids、高斯混合模型、层次聚类和隐马尔可夫模型。本书的以下章节中解释了其中一些算法。
- en: Dealing with ML models
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理机器学习模型
- en: We can interpret ML models as functions that take different types of parameters.
    Such functions provide outputs for given inputs based on the values of these parameters.
    Developers can configure the behavior of ML models for solving problems by adjusting
    model parameters. Training an ML model can usually be treated as a process of
    searching for the best combination of its parameters. We can split the ML model’s
    parameters into two types. The first type consists of parameters internal to the
    model, and we can estimate their values from the training (input) data. The second
    type consists of parameters external to the model, and we cannot estimate their
    values from training data. Parameters that are external to the model are usually
    called **hyperparameters**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将机器学习模型解释为接受不同类型参数的函数。这些函数根据这些参数的值，为给定的输入提供输出。开发者可以通过调整模型参数来配置机器学习模型的行为，以解决特定问题。训练机器学习模型通常可以被视为寻找其参数最佳组合的过程。我们可以将机器学习模型的参数分为两种类型。第一种类型是模型内部的参数，我们可以从训练（输入）数据中估计它们的值。第二种类型是模型外部的参数，我们无法从训练数据中估计它们的值。模型外部的参数通常被称为**超参数**。
- en: 'Internal parameters have the following characteristics:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 内部参数具有以下特征：
- en: They are necessary for making predictions
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们对于做出预测是必要的
- en: They define the quality of the model on the given problem
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们定义了模型在给定问题上的质量
- en: We can learn them from training data
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以从训练数据中学习它们
- en: Usually, they are a part of the model
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，它们是模型的一部分
- en: If the model contains a fixed number of internal parameters, it is called **parametric**.
    Otherwise, we can classify it as **non-parametric**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型包含固定数量的内部参数，则称为**参数化**。否则，我们可以将其归类为**非参数化**。
- en: 'Examples of internal parameters are as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 内部参数的例子如下：
- en: Weights of **artificial neural** **networks** (**ANNs**)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANNs**）的权重'
- en: Support vector values for SVM models
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM模型的支撑向量值
- en: Polynomial coefficients for linear regression or logistic regression
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归或逻辑回归的多项式系数
- en: ANNs are computer systems inspired by the structure and function of biological
    neural networks in the human brain. They are composed of interconnected nodes,
    or neurons, that process and transmit information. ANNs are designed to learn
    patterns and relationships from data, allowing them to make predictions or decisions
    based on new inputs. The learning process involves adjusting the weights and biases
    of the connections between neurons to improve the accuracy of the model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs（人工神经网络）是受人类大脑中生物神经网络的架构和功能启发的计算机系统。它们由相互连接的节点或神经元组成，这些节点处理和传输信息。ANNs旨在从数据中学习模式和关系，使它们能够根据新的输入进行预测或决策。学习过程涉及调整神经元之间连接的权重和偏差，以提高模型的准确性。
- en: 'On the other hand, hyperparameters have the following characteristics:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，超参数具有以下特征：
- en: They are used to configure algorithms that estimate model parameters
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们用于配置估计模型参数的算法
- en: The practitioner usually specifies them
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践者通常指定它们
- en: Their estimation is often based on using heuristics
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的估计通常基于使用启发式方法
- en: They are specific to a concrete modeling problem
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们针对具体的建模问题
- en: It is hard to know the best values for a model’s hyperparameters for a specific
    problem. Also, practitioners usually need to perform additional research on how
    to tune required hyperparameters so that a model or a training algorithm behaves
    in the best way. Practitioners use rules of thumb, copying values from similar
    projects, as well as special techniques such as grid search for hyperparameter
    estimation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定问题，很难知道模型超参数的最佳值。此外，从业者通常需要研究如何调整所需的超参数，以便模型或训练算法以最佳方式运行。从业者使用经验法则、从类似项目中复制值以及如网格搜索等特殊技术来进行超参数估计。
- en: 'Examples of hyperparameters are as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数的例子如下：
- en: C and sigma parameters used in the SVM algorithm for a classification quality
    configuration
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分类质量配置的SVM算法中的C和sigma参数
- en: The learning rate parameter that is used in the neural network training process
    to configure algorithm convergence
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在神经网络训练过程中用于配置算法收敛的学习率参数
- en: The *k* value that is used in the KNN algorithm to configure the number of neighbors
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在KNN算法中用于配置邻居数量的*k*值
- en: Model parameter estimation
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型参数估计
- en: '**Model parameter estimation** usually uses some optimization algorithm. The
    speed and quality of the resulting model can significantly depend on the optimization
    algorithm chosen. Research on optimization algorithms is a popular topic in industry,
    as well as in academia. ML often uses optimization techniques and algorithms based
    on the optimization of a loss function. A function that evaluates how well a model
    predicts the data is called a **loss function**. If predictions are very different
    from the target outputs, the loss function will return a value that can be interpreted
    as a bad one, usually a large number. In such a way, the loss function penalizes
    an optimization algorithm when it moves in the wrong direction. So, the general
    idea is to minimize the value of the loss function to reduce penalties. There
    is no single universal loss function for optimization algorithms. Different factors
    determine how to choose a loss function. Examples of such factors are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型参数估计**通常使用某些优化算法。结果模型的速度和质量可能显著取决于所选的优化算法。优化算法的研究在工业界和学术界都是一个热门话题。机器学习（ML）通常使用基于损失函数优化的优化技术和算法。评估模型预测数据好坏的函数称为**损失函数**。如果预测与目标输出非常不同，损失函数将返回一个可以解释为不良的值，通常是一个大数字。这样，损失函数在优化算法向错误方向移动时对其进行惩罚。因此，一般思想是使损失函数的值最小化以减少惩罚。没有单一的通用损失函数适用于优化算法。不同的因素决定了如何选择损失函数。以下是一些这样的因素的例子：'
- en: Specifics of the given problem—for example, whether it is a regression or a
    classification model
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定问题的具体细节——例如，它是一个回归模型还是分类模型
- en: Ease of calculating derivatives
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算导数的容易程度
- en: Percentage of outliers in the dataset
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集中异常值的百分比
- en: 'In ML, the term **optimizer** is used to define an algorithm that connects
    a loss function and a technique for updating model parameters in response to the
    values of the loss function. So, optimizers tune ML models to predict target values
    for new data in the most accurate way by fitting model parameters. Optimizers
    or optimization algorithms play a crucial role in training ML models. They help
    find the best parameters for a model, which can improve its performance and accuracy.
    They have wide applications in various fields such as image recognition, natural
    language processing, and fraud detection. For example, in image classification
    tasks, optimization algorithms can be used to train deep neural networks to accurately
    identify objects in images. There are many optimizers: gradient descent, Adagrad,
    RMSProp, Adam, and others. Moreover, developing new optimizers is an active area
    of research. For example, there is the *ML and Optimization* research group at
    Microsoft (located in Redmond) whose research areas include combinatorial optimization,
    convex and non-convex optimization, and their application in ML and AI. Other
    companies in the industry also have similar research groups; there are many publications
    from Facebook Research, Amazon Research, and OpenAI groups.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习（ML）中，术语 **优化器** 用于定义一个将损失函数与更新模型参数的技术相连接的算法，该技术是对损失函数值的响应。因此，优化器通过调整模型参数来调整机器学习模型，以最准确的方式预测新数据的目标值。优化器或优化算法在训练机器学习模型中起着至关重要的作用。它们帮助找到模型的最佳参数，这可以提高其性能和准确性。它们在各个领域都有广泛的应用，例如图像识别、自然语言处理和欺诈检测。例如，在图像分类任务中，可以使用优化算法来训练深度神经网络，以准确识别图像中的对象。有许多优化器：梯度下降、Adagrad、RMSProp、Adam
    等。此外，开发新的优化器是一个活跃的研究领域。例如，微软（位于雷德蒙德）有一个名为 *ML 和优化* 的研究小组，其研究领域包括组合优化、凸和非凸优化，以及它们在机器学习和人工智能中的应用。该行业中的其他公司也有类似的研究小组；Facebook
    Research、Amazon Research 和 OpenAI 小组都有许多出版物。
- en: 'Now, we will learn about what ML is and what its main conceptual parts are.
    So let’s learn the most important part of its mathematical basement: linear algebra.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将了解机器学习是什么以及它的主要概念部分。因此，让我们学习其数学基础最重要的部分：线性代数。
- en: An overview of linear algebra
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性代数的概述
- en: The concepts of linear algebra are essential for understanding the theory behind
    ML because they help us understand how ML algorithms work under the hood. Also,
    most ML algorithm definitions use linear algebra terms.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数的概念对于理解机器学习背后的理论至关重要，因为它们帮助我们理解机器学习算法在底层是如何工作的。此外，大多数机器学习算法的定义都使用了线性代数的术语。
- en: Linear algebra is not only a handy mathematical instrument but also the concepts
    of linear algebra can be very efficiently implemented with modern computer architectures.
    The rise of ML, and especially deep learning, began after significant performance
    improvement of the modern `Cuda` and `OpenCL`, and one example of a specialized
    linear algebra library is `cuBLAS`. Moreover, it became more common to use **general-purpose
    graphics processing units** (**GPGPUs**) because these turn the computational
    power of a modern GPU into a powerful general-purpose computing resource.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数不仅是一个实用的数学工具，而且线性代数的概念也可以用现代计算机架构非常有效地实现。机器学习（特别是深度学习）的兴起是在现代 `Cuda` 和 `OpenCL`
    显著性能提升之后开始的，一个专门的线性代数库的例子是 `cuBLAS`。此外，使用 **通用图形处理单元**（**GPGPUs**）变得更加普遍，因为这些将现代
    GPU 的计算能力转化为强大的通用计算资源。
- en: Also, `AVx`, `SSE`, and `MMx`. There is also the term `Eigen`, `xtensor`, `VienaCL`,
    and others, use them to improve computational performance.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有 `AVx`、`SSE` 和 `MMx`。还有 `Eigen`、`xtensor`、`VienaCL` 等术语，它们被用来提高计算性能。
- en: Learning the concepts of linear algebra
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习线性代数的概念
- en: 'Linear algebra is a big area. It is the section of algebra that studies objects
    of a linear nature: vector (or linear) spaces, linear representations, and systems
    of linear equations. The main tools used in linear algebra are determinants, matrices,
    conjugation, and tensor calculus.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数是一个很大的领域。它是研究线性对象的代数部分：向量（或线性）空间、线性表示和线性方程组。线性代数中使用的工具主要有行列式、矩阵、共轭和张量计算。
- en: To understand ML algorithms, we only need a small set of linear algebra concepts.
    However, to do research on new ML algorithms, a practitioner should have a deep
    understanding of linear algebra and calculus.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解机器学习算法，我们只需要一组小的线性代数概念。然而，要研究新的机器学习算法，从业者应该对线性代数和微积分有深入的理解。
- en: 'The following list contains the most valuable linear algebra concepts for understanding
    ML algorithms:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表包含了理解机器学习算法最有价值的线性代数概念：
- en: '**Scalar**: This is a single number.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标量**：这是一个单独的数字。'
- en: '**Vector**: This is an array of ordered numbers. Each element has a distinct
    index. Notation for vectors is a bold lowercase typeface for names and an italic
    typeface with a subscript for elements, as shown in the following example:![](img/B19849_Formula_01.jpg)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量**：这是一个有序数字的数组。每个元素都有一个独特的索引。向量的表示法是名称使用粗体小写字母，元素使用带有下标的斜体字母，如下例所示！[](img/B19849_Formula_01.jpg)'
- en: '**Matrix**: This is a two-dimensional array of numbers. Each element has a
    distinct pair of indices. Notation for matrices is a bold uppercase typeface for
    names and an italic but not bold typeface with a comma-separated list of indices
    in subscripts for elements, as shown in the following example:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矩阵**：这是一个二维数字数组。每个元素都有一个独特的索引对。矩阵的表示法是名称使用粗体大写字母，元素使用带有逗号分隔的索引列表的下标斜体字母，如下例所示：'
- en: '![](img/B19849_Formula_02.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_02.jpg)'
- en: '**Tensor**: This is an array of numbers arranged in a multidimensional regular
    grid, and represents generalizations of matrices. It is like a multidimensional
    matrix. For example, tensor *A* with dimensions 2 x 2 x 2 can look like this:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量**：这是一个以多维规则网格排列的数字数组，代表了矩阵的推广。它就像一个多维矩阵。例如，具有2 x 2 x 2维度的张量 *A* 可以看起来像这样：'
- en: '![](img/B19849_Formula_03.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_03.jpg)'
- en: Linear algebra libraries and ML frameworks usually use the concept of a tensor
    instead of a matrix because they implement general algorithms, and a matrix is
    just a special case of a tensor with two dimensions. Also, we can consider a vector
    as a matrix of size *n* x *1*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数库和机器学习框架通常使用张量的概念而不是矩阵，因为它们实现的是通用算法，而矩阵只是具有两个维度的张量的特例。此外，我们可以将向量视为大小为 *n*
    x *1* 的矩阵。
- en: Basic linear algebra operations
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本线性代数操作
- en: 'The most common operations used for programming linear algebra algorithms are
    the following ones:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 编程线性代数算法最常用的操作如下：
- en: '**Element-wise operations**: These are performed in an element-wise manner
    on vectors, matrices, or tensors of the same size. The resulting elements will
    be the result of operations on corresponding input elements, as shown here:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逐元素操作**：这些操作以逐元素的方式在相同大小的向量、矩阵或张量上执行。结果元素将是相应输入元素上操作的输出，如下所示：'
- en: '![](img/B19849_Formula_04.jpg)![](img/B19849_Formula_05.jpg)![](img/B19849_Formula_06.jpg)![](img/B19849_Formula_07.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_04.jpg)![](img/B19849_Formula_05.jpg)![](img/B19849_Formula_06.jpg)![](img/B19849_Formula_07.jpg)'
- en: 'The following example shows the element-wise summation:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了逐元素求和：
- en: '![](img/B19849_Formula_08.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_08.jpg)'
- en: '**Dot product**: There are two types of multiplication for tensors and matrices
    in linear algebra—one is just element-wise, and the second is the dot product.
    The dot product deals with two equal-length series of numbers and returns a single
    number. This operation applied on matrices or tensors requires that the matrix
    or tensor *A* has the same number of columns as the number of rows in the matrix
    or tensor *B*. The following example shows the dot-product operation in the case
    when *A* is an *n x m* matrix and *B* is an *m x* *p* matrix:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点积**：在线性代数中，张量和矩阵有两种乘法类型——一种是逐元素，另一种是点积。点积处理两个等长数字序列，并返回一个数字。在矩阵或张量上应用此操作要求矩阵或张量
    *A* 的列数与矩阵或张量 *B* 的行数相同。以下示例显示了当 *A* 是一个 *n x m* 矩阵且 *B* 是一个 *m x* *p* 矩阵时的点积操作：'
- en: '![](img/B19849_Formula_09.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_09.jpg)'
- en: '**Transposing**: The transposing of a matrix is an operation that flips the
    matrix over its diagonal, which leads to the flipping of the column and row indices
    of the matrix, resulting in the creation of a new matrix. In general, it is swapping
    matrix rows with columns. The following example shows how transposing works:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转置**：矩阵的转置是一种翻转矩阵对角线上的操作，这会导致矩阵的列和行索引翻转，从而创建一个新的矩阵。一般来说，它是交换矩阵的行和列。以下示例显示了转置的工作原理：'
- en: '![](img/B19849_Formula_10.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_10.jpg)'
- en: '**Norm**: This operation calculates the size of the vector; the result of this
    is a non-negative real number. The norm formula is as follows:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**范数**：这个操作计算向量的大小；这个结果是一个非负实数。范数公式如下：'
- en: '![](img/B19849_Formula_11.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_11.jpg)'
- en: The generic name of this type of norm is an ![](img/B19849_Formula_12.png) norm
    for ![](img/B19849_Formula_13.png). Usually, we use more concrete norms such as
    an ![](img/B19849_Formula_14.png) norm with *p = 2*, which is known as the Euclidean
    norm, and we can interpret it as the Euclidean distance between points. Another
    widely used norm is the squared ![](img/B19849_Formula_14.png) norm, whose calculation
    formula is ![](img/B19849_Formula_16.png). The squared ![](img/B19849_Formula_14.png)
    norm is more suitable for mathematical and computational operations than the ![](img/B19849_Formula_14.png)
    norm. Each partial derivative of the squared ![](img/B19849_Formula_14.png) norm
    depends only on the corresponding element of *x*, in comparison to the partial
    derivatives of the ![](img/B19849_Formula_14.png) norm, which depends on the entire
    vector; this property plays a vital role in optimization algorithms. Another widely
    used norm operation is the ![](img/B19849_Formula_21.png) norm with *p = 1*, which
    is commonly used in ML when we care about the difference between zero and nonzero
    elements. The *L^1* norm is also known as the **Manhattan distance**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这种范数的通用名称是 ![](img/B19849_Formula_12.png) 范数，用于 ![](img/B19849_Formula_13.png)。通常，我们使用更具体的范数，例如
    *p = 2* 的 ![](img/B19849_Formula_14.png) 范数，这被称为欧几里得范数，我们可以将其解释为点之间的欧几里得距离。另一种广泛使用的范数是平方
    ![](img/B19849_Formula_14.png) 范数，其计算公式为 ![](img/B19849_Formula_16.png)。平方 ![](img/B19849_Formula_14.png)
    范数比 ![](img/B19849_Formula_14.png) 范数更适合数学和计算操作。平方 ![](img/B19849_Formula_14.png)
    范数的每一阶偏导数只依赖于 *x* 的对应元素，而 ![](img/B19849_Formula_14.png) 范数的偏导数则依赖于整个向量；这一特性在优化算法中起着至关重要的作用。另一种广泛使用的范数操作是
    *p = 1* 的 ![](img/B19849_Formula_21.png) 范数，在机器学习中，当我们关注零和非零元素之间的差异时常用。*L^1* 范数也称为**曼哈顿距离**。
- en: '**Inverting**: The inverse matrix is such a matrix that ![](img/B19849_Formula_22.png),
    where *I* is an identity matrix. The **identity matrix** is a matrix that does
    not change any vector when we multiply that vector by that matrix.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**求逆**：逆矩阵是这样的矩阵，![](img/B19849_Formula_22.png)，其中 *I* 是单位矩阵。**单位矩阵**是一个当我们用该矩阵乘以一个向量时不会改变该向量的矩阵。'
- en: We have considered the main linear algebra concepts as well as operations on
    them. Using this math apparatus, we can define and program many ML algorithms.
    For example, we can use tensors and matrices to define training datasets for training,
    and scalars can be used as different types of coefficients. We can use element-wise
    operations to perform arithmetic operations with a whole dataset (a matrix or
    a tensor). For example, we can use element-wise multiplication to scale a dataset.
    We usually use transposing to change a view of a vector or matrix to make them
    suitable for the dot-product operation. The dot product is usually used to apply
    a linear function with weights expressed as matrix coefficients to a vector; for
    example, this vector can be a training sample. Also, dot-product operations are
    used to update model parameters expressed as matrix or tensor coefficients according
    to an algorithm.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经考虑了主要的线性代数概念及其操作。使用这个数学工具，我们可以定义和编程许多机器学习算法。例如，我们可以使用张量和矩阵来定义训练数据集，标量可以用作不同类型的系数。我们可以使用逐元素操作对整个数据集（矩阵或张量）进行算术运算。例如，我们可以使用逐元素乘法来缩放数据集。我们通常使用转置来改变向量或矩阵的视图，使其适合点积操作。点积通常用于将权重表示为矩阵系数的线性函数应用于向量；例如，这个向量可以是一个训练样本。此外，点积操作也用于根据算法更新表示为矩阵或张量系数的模型参数。
- en: The norm operation is often used in formulas for loss functions because it naturally
    expresses the distance concept and can measure the difference between target and
    predicted values. The inverse matrix is a crucial concept for the analytical solving
    of linear equations systems. Such systems often appear in different optimization
    problems. However, calculating the inverse matrix is very computationally expensive.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 范数操作常用于损失函数的公式中，因为它自然地表达了距离概念，可以衡量目标和预测值之间的差异。逆矩阵是线性方程组解析求解的关键概念。这类系统常出现在不同的优化问题中。然而，计算逆矩阵的计算成本非常高。
- en: Tensor representation in computing
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机中的张量表示
- en: 'We can represent tensor objects in computer memory in different ways. The most
    obvious method is a simple linear array in computer memory (**random-access memory**,
    or **RAM**). However, the linear array is also the most computationally effective
    data structure for modern CPUs. There are two standard practices to organize tensors
    with a linear array in memory: **row-major ordering** and **column-major ordering**.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用不同的方式在计算机内存中表示张量对象。最明显的方法是在计算机内存中（**随机存取存储器**，或 **RAM**）使用简单的线性数组。然而，线性数组也是现代
    CPU 最有效的计算数据结构。有两种标准做法在内存中使用线性数组组织张量：**行主序排列**和**列主序排列**。
- en: In row-major ordering, we place consecutive elements of a row in linear order
    one after the other, and each row is also placed after the end of the previous
    one. In column-major ordering, we do the same but with the column elements. Data
    layouts have a significant impact on computational performance because the speed
    of traversing an array relies on modern CPU architectures that work with sequential
    data more efficiently than with non-sequential data. CPU caching effects are the
    reasons for such behavior. Also, a contiguous data layout makes it possible to
    use SIMD vectorized instructions that work with sequential data more efficiently,
    and we can use them as a type of parallel processing.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在行主序排列中，我们将一行的连续元素依次线性排列，并且每一行也排在上一行的末尾之后。在列主序排列中，我们以相同的方式处理列元素。数据布局对计算性能有重大影响，因为遍历数组的速度依赖于现代
    CPU 架构，它们在处理顺序数据时比处理非顺序数据更有效。CPU 缓存效应是这种行为的理由。此外，连续的数据布局使得可以使用与顺序数据更有效地工作的 SIMD
    向量化指令，我们可以将它们用作一种并行处理方式。
- en: Different libraries, even in the same programming language, can use different
    ordering. For example, `Eigen` uses column-major ordering, but `PyTorch` uses
    row-major ordering. So, developers should be aware of internal tensor representation
    in libraries they use, and also take care of this when performing data loading
    or implementing algorithms from scratch.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的库，即使在同一编程语言中，也可以使用不同的排列。例如，`Eigen` 使用列主序排列，但 `PyTorch` 使用行主序排列。因此，开发者应该注意他们使用的库中内部张量表示，并在执行数据加载或从头实现算法时注意这一点。
- en: 'Consider the following matrix:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下矩阵：
- en: '![](img/B19849_Formula_23.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_23.jpg)'
- en: 'Then, in the row-major data layout, members of the matrix will have the following
    layout in memory:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在行主序数据布局中，矩阵的成员在内存中的布局如下：
- en: '| **0** | **1** | **2** | **3** | **4** | **5** |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **0** | **1** | **2** | **3** | **4** | **5** |'
- en: '| a11 | a12 | a13 | a21 | a22 | a23 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| a11 | a12 | a13 | a21 | a22 | a23 |'
- en: Table 1.1 – The row-major data layout example
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.1 – 行主序数据布局示例
- en: 'In the case of the column-major data layout, order layout will be next, as
    shown here:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在列主序数据布局的情况下，顺序布局将紧随其后，如下所示：
- en: '| **0** | **1** | **2** | **3** | **4** | **5** |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **0** | **1** | **2** | **3** | **4** | **5** |'
- en: '| a11 | a21 | a12 | a22 | a13 | a23 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| a11 | a21 | a12 | a22 | a13 | a23 |'
- en: Table 1.2 – The column-major data layout example
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.2 – 列主序数据布局示例
- en: Linear algebra API samples
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性代数 API 示例
- en: Let’s consider some C++ linear algebra **application programming interfaces**
    (**APIs**) and look at how we can use them for creating linear algebra primitives
    and performing algebra operations with them.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些 C++ 线性代数 **应用程序编程接口**（**API**）并看看我们如何使用它们来创建线性代数原语以及使用它们进行代数运算。
- en: Using Eigen
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Eigen
- en: '`Eigen` is a general-purpose linear algebra C++ library. In Eigen, all matrices
    and vectors are objects of the `Matrix` template class, and the vector is a specialization
    of the matrix type, with either one row or one column. Tensor objects are not
    presented in official APIs but exist as submodules.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`Eigen` 是一个通用的线性代数 C++ 库。在 Eigen 中，所有矩阵和向量都是 `Matrix` 模板类的对象，向量是矩阵类型的特化，具有一行或一列。张量对象在官方
    API 中没有表示，但作为子模块存在。'
- en: 'We can define the type for a matrix with known 3 x 3 dimensions and a floating-point
    data type like this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样定义一个已知 3 x 3 维度和浮点数据类型的矩阵类型：
- en: '[PRE0]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can define a column vector in the following way:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样定义一个列向量：
- en: '[PRE1]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Eigen already has a lot of predefined types for vector and matrix objects—for
    example, `Eigen::Matrix3f` (floating-point 3 x 3 matrix type) or `Eigen::RowVector2f`
    (floating-point 1 x 2 vector type). Also, Eigen is not limited to matrices whose
    dimensions we know at compile time. We can define matrix types that will take
    the number of rows or columns at initialization during runtime. To define such
    types, we can use a special type variable for the `Matrix` class template argument
    named `Eigen::Dynamic`. For example, to define a matrix of doubles with dynamic
    dimensions, we can use the following definition:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Eigen已经为向量和矩阵对象提供了许多预定义的类型——例如，`Eigen::Matrix3f`（浮点3 x 3矩阵类型）或`Eigen::RowVector2f`（浮点1
    x 2向量类型）。此外，Eigen并不限于在编译时已知维度的矩阵。我们可以在运行时初始化时定义矩阵类型，它将根据行数或列数动态调整。为了定义此类类型，我们可以使用名为`Eigen::Dynamic`的特殊类型变量作为`Matrix`类模板参数。例如，为了定义一个具有动态维度的双精度矩阵，我们可以使用以下定义：
- en: '[PRE2]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Objects initialized from the types we defined will look like this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们定义的类型初始化的对象将看起来像这样：
- en: '[PRE3]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To put some values into these objects, we can use several approaches. We can
    use special predefined initialization functions, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这些值放入这些对象中，我们可以使用几种方法。我们可以使用特殊预定义的初始化函数，如下所示：
- en: '[PRE4]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can use the *comma-initializer* syntax, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用*逗号初始化器*语法，如下所示：
- en: '[PRE5]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This code construction initializes the matrix values in the following way:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码构造以以下方式初始化矩阵值：
- en: '![](img/B19849_Formula_24.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_24.jpg)'
- en: 'We can use direct element access to set or change matrix coefficients. The
    following code sample shows how to use the `()` operator for such an operation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用直接元素访问来设置或更改矩阵系数。以下代码示例展示了如何使用`()`运算符进行此类操作：
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can use the object of the `Map` type to wrap an existent C++ array or vector
    in the `Matrix` type object. This kind of mapping object will use memory and values
    from the underlying object, and will not allocate the additional memory and copy
    the values. The following snippet shows how to use the `Map` type:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`Map`类型的对象将现有的C++数组或向量包装在`Matrix`类型对象中。此类映射对象将使用底层对象的内存和值，而不会分配额外的内存并复制值。以下代码片段展示了如何使用`Map`类型：
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can use initialized matrix objects in mathematical operations. Matrix and
    vector arithmetic operations in the `Eigen` library are offered either through
    overloads of standard C++ arithmetic operators such as `+`, `-`, or `*`, or through
    methods such as `dot()` and `cross()`. The following code sample shows how to
    express general math operations in Eigen:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在数学运算中使用初始化的矩阵对象。`Eigen`库中的矩阵和向量算术运算通过标准C++算术运算符的重载提供，如`+`、`-`、`*`，或者通过方法如`dot()`和`cross()`。以下代码示例展示了如何在Eigen中表示通用数学运算：
- en: '[PRE8]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice that, in Eigen, arithmetic operators such as `+` do not perform any computation
    by themselves. These operators return an *expression object*, which describes
    what computation to perform. The actual computation happens later when the whole
    expression is evaluated, typically in the `=` arithmetic operator. It can lead
    to some strange behaviors, primarily if a developer uses the `auto` keyword too
    frequently.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在Eigen中，算术运算符如`+`本身并不执行任何计算。这些运算符返回一个*表达式对象*，它描述了要执行的计算。实际的计算发生在整个表达式评估之后，通常是在`=`算术运算符中。这可能会导致一些奇怪的行为，主要如果开发者频繁使用`auto`关键字的话。
- en: 'Sometimes, we need to perform operations only on a part of the matrix. For
    this purpose, Eigen provides the `block` method, which takes four parameters:
    `i,j,p,q`. These parameters are the block size, `p,q`, and the starting point,
    `i,j`. The following code shows how to use this method:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们只需要对矩阵的一部分执行操作。为此，Eigen提供了`block`方法，它接受四个参数：`i,j,p,q`。这些参数是块大小`p,q`和起始点`i,j`。以下代码展示了如何使用此方法：
- en: '[PRE9]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'There are two more methods to access rows and columns by index, which are also
    a type of `block` operation. The following snippet shows how to use the `col`
    and `row` methods:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种更多通过索引访问行和列的方法，它们也是一种`block`操作。以下代码片段展示了如何使用`col`和`row`方法：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Another important feature of linear algebra libraries is broadcasting, and
    Eigen supports this with the `colwise` and `rowwise` methods. Broadcasting can
    be interpreted as a matrix by replicating it in one direction. Take a look at
    the following example of how to add a vector to each column of the matrix:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数库的另一个重要特性是广播，Eigen通过`colwise`和`rowwise`方法支持这一特性。广播可以理解为通过在一个方向上复制矩阵来解释为一个矩阵。以下是一个如何将向量添加到矩阵每一列的示例：
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This operation has the following result:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作的结果如下：
- en: '![](img/B19849_Formula_25.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_25.jpg)'
- en: Using xtensor
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用xtensor
- en: The `xtensor` library is a C++ library for numerical analysis with multidimensional
    array expressions. Containers of xtensor are inspired by NumPy, the Python array
    programming library. ML algorithms are mainly described using Python and NumPy,
    so this library can make it easier to move them to C++. The following container
    classes implement multidimensional arrays in the `xtensor` library.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`xtensor`库是一个用于数值分析的C++库，具有多维数组表达式。`xtensor`的容器灵感来源于NumPy，Python数组编程库。机器学习算法主要使用Python和NumPy进行描述，因此这个库可以使得将它们移动到C++更加容易。以下容器类在`xtensor`库中实现了多维数组。'
- en: 'The `xarray` type is a dynamically sized multidimensional array, as shown in
    the following code snippet:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`xarray`类型是一个动态大小的多维数组，如下面的代码片段所示：'
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Dynamic size for the `xarray` type means that this shape can be changed at compilation
    time.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`xarray`类型的动态大小意味着这个形状可以在编译时改变。'
- en: 'The `xtensor` type is a multidimensional array whose range is fixed at compilation
    time. Exact dimension values can be configured in the initialization step, as
    shown in the following code snippet:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`xtensor`类型是一个在编译时固定范围的多维数组。精确的维度值可以在初始化步骤中配置，如下面的代码片段所示：'
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `xtensor_fixed` type is a multidimensional array with a dimension shape
    fixed at compile time, as shown in the following code snippet:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`xtensor_fixed`类型是一个在编译时固定维度形状的多维数组，如下面的代码片段所示：'
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `xtensor` library also implements arithmetic operators with expression template
    techniques such as Eigen (this is a common approach for math libraries implemented
    in C++). So, the computation happens lazily, and the actual result is calculated
    when the whole expression is evaluated.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`xtensor`库还通过Eigen等表达式模板技术实现了算术运算符，这是一个在C++中实现的数学库的常见方法。因此，计算是延迟的，整个表达式评估时才会计算实际结果。'
- en: '**Lazy computation**, also known as **lazy evaluation** or **call-by-need evaluation**,
    is a strategy in programming where the evaluation of an expression is delayed
    until its value is actually needed.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**延迟计算**，也称为**延迟评估**或**按需评估**，是编程中的一种策略，其中表达式的评估被延迟到其实际需要时。'
- en: This contrasts with eager evaluation, where expressions are evaluated immediately
    upon encountering them. The container definitions are also expressions. There
    is also a function to force an expression evaluation named `xt::eval` in the `xtensor`
    library.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这与即时求值相对立，即时求值是在遇到表达式时立即进行评估。容器定义也是表达式。`xtensor`库中还有一个名为`xt::eval`的函数，用于强制表达式评估。
- en: 'There are different kinds of container initialization in the `xtensor` library.
    Initialization of `xtensor` arrays can be done with C++ initializer lists, as
    follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在`xtensor`库中存在不同种类的容器初始化。`xtensor`数组的初始化可以使用C++初始化列表来完成，如下所示：
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `xtensor` library also has builder functions for special tensor types.
    The following snippet shows some of them:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`xtensor`库还为特殊张量类型提供了构建函数。以下代码片段展示了其中的一些：'
- en: '[PRE16]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Also, we can map existing C++ arrays into the `xtensor` container with the
    `xt::adapt` function. This function returns the object that uses the memory and
    values from the underlying object, as shown in the following code snippet:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以使用`xt::adapt`函数将现有的C++数组映射到`xtensor`容器中。此函数返回一个使用底层对象的内存和值的对象，如下面的代码片段所示：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can use direct access to container elements, with the `()` operator, to
    set or change tensor values, as shown in the following code snippet:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`()`运算符直接访问容器元素，以设置或更改张量值，如下面的代码片段所示：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `xtensor` library implements linear algebra arithmetic operations through
    overloads of standard C++ arithmetic operators such as `+`, `-`, and `*`. To use
    other operations such as dot-product operations, we have to link an application
    with the library named `xtensor-blas`. These operators are declared in the `xt::linalg`
    namespace.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`xtensor`库通过标准C++算术运算符（如`+`、`-`和`*`）的重载来实现线性代数算术运算。要使用其他操作，如点积运算，我们必须将应用程序与名为`xtensor-blas`的库链接起来。这些运算符在`xt::linalg`命名空间中声明。'
- en: 'The following code shows the use of arithmetic operations with the `xtensor`
    library:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了使用`xtensor`库进行算术运算的用法：
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To get partial access to the `xtensor` containers, we can use the `xt::view`
    function. The `view` function returns a new tensor object that shares the same
    underlying data with the original tensor but with a different shape or strides.
    This allows you to access the data in the tensor in a different way, without actually
    changing the underlying data itself. The following sample shows how this function
    works:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取对 `xtensor` 容器的部分访问权限，我们可以使用 `xt::view` 函数。`view` 函数返回一个新的张量对象，它与原始张量共享相同的基本数据，但具有不同的形状或步长。这允许您以不同的方式访问张量中的数据，而实际上并不改变底层数据本身。以下示例展示了此函数的工作原理：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This operation takes a rectangular block from the tensor, which looks like
    this:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作从张量中取一个矩形块，看起来像这样：
- en: '![](img/B19849_Formula_26.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_26.jpg)'
- en: 'The `xtensor` library implements automatic broadcasting in most cases. When
    the operation involves two arrays of different dimensions, it transmits the array
    with the smaller dimension across the leading dimension of the other array, so
    we can directly add a vector to a matrix. The following code sample shows how
    easy it is:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`xtensor` 库在大多数情况下实现了自动广播。当操作涉及不同维度的两个数组时，它会将具有较小维度的数组传输到另一个数组的前导维度，这样我们就可以直接将一个向量加到一个矩阵上。以下代码示例展示了这有多么简单：'
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Using Blaze
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Blaze
- en: '`Blaze` is a general-purpose high-performance C++ library for dense and sparse
    linear algebra. There are different classes to represent matrices and vectors
    in Blaze.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`Blaze` 是一个通用高性能 C++ 库，用于密集和稀疏线性代数。Blaze 中有不同类来表示矩阵和向量。'
- en: 'We can define the type for a matrix with known dimensions and a floating-point
    data type, like this:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义具有已知维度和浮点数据类型的矩阵类型，如下所示：
- en: '[PRE22]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can define a vector in the following way:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以下方式定义一个向量：
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Also, Blaze is not limited to matrices whose dimensions we know at compile
    time. We can define matrix types that will take the number of rows or columns
    at initialization during runtime. To define such types, we can use the `blaze::DynamicMatrix`
    or `blaze::DynamicVector` classes. For example, to define a matrix of doubles
    with dynamic dimensions, we can use the following definition:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Blaze 并不仅限于在编译时已知维度的矩阵。我们可以在运行时初始化期间定义将接受行数或列数的矩阵类型。为了定义此类类型，我们可以使用 `blaze::DynamicMatrix`
    或 `blaze::DynamicVector` 类。例如，为了定义具有动态维度的双精度矩阵，我们可以使用以下定义：
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Objects initialized from the types we defined will look like this:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们定义的类型初始化的对象将如下所示：
- en: '[PRE25]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To put some values into these objects, we can use several approaches. We can
    use special predefined initialization functions, as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这些值放入这些对象中，我们可以使用几种方法。我们可以使用特殊预定义的初始化函数，如下所示：
- en: '[PRE26]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This code construction initializes the matrix values in the following way:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码构造以以下方式初始化矩阵值：
- en: '![](img/B19849_Formula_27.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_27.jpg)'
- en: 'We can use direct element access to set or change matrix coefficients. The
    following code sample shows how to use the `()` operator for such an operation:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用直接元素访问来设置或更改矩阵系数。以下代码示例展示了如何使用 `()` 操作符进行此类操作：
- en: '[PRE27]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can use the object of the `blaze::CustomVector` type to wrap an existent
    C++ array or vector into the `Matrix` or `Vector` type object. This kind of mapping
    object will use memory and values from the underlying object, and will not allocate
    the additional memory and copy the values. The following snippet shows how to
    use this approach:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `blaze::CustomVector` 类型的对象将现有的 C++ 数组或向量包装到 `Matrix` 或 `Vector` 类型对象中。此类映射对象将使用底层对象的内存和值，而不会分配额外的内存并复制值。以下代码片段展示了如何使用这种方法：
- en: '[PRE28]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can use initialized matrix objects in mathematical operations. Notice that
    we used two parameters: `blaze::unaligned` and `blaze::unpadded`. The `unpadded`
    parameter may be used in some functions or methods to control the behavior of
    padding or truncation of arrays. This parameter can be important in certain scenarios
    where you want to avoid unnecessary padding or truncating of data during operations
    such as reshaping, slicing, or concatenating arrays. The `blaze::unaligned` parameter
    allows users to perform operations on unaligned data, which can be useful in certain
    scenarios where the data is not aligned to specific memory boundaries.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在数学运算中使用初始化的矩阵对象。注意，我们使用了两个参数：`blaze::unaligned`和`blaze::unpadded`。`unpadded`参数可以在某些函数或方法中使用，以控制填充或截断数组的行为。在某些场景中，当您希望在重塑、切片或连接数组等操作中避免不必要的填充或截断数据时，此参数可能很重要。`blaze::unaligned`参数允许用户对未对齐的数据执行操作，这在数据没有对齐到特定内存边界的情况下可能很有用。
- en: 'Matrix and vector arithmetic operations in the `Blaze` library are offered
    either through overloads of standard C++ arithmetic operators such as `+`, `-`,
    or `*`, or through methods such as `dot()` and `cross()`. The following code sample
    shows how to express general math operations in Blaze:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Blaze`库中，矩阵和向量的算术运算可以通过标准C++算术运算符的重载来实现，例如`+`、`-`或`*`，或者通过`dot()`和`cross()`等方法。以下代码示例展示了如何在Blaze中表达一般的数学运算：
- en: '[PRE29]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Notice that, in Blaze, arithmetic operators such as `+` do not perform any computation
    by themselves. These operators return an *expression object*, which describes
    what computation to perform. The actual computation happens later when the whole
    expression is evaluated, typically in the `=` arithmetic operator or in a constructor
    of a concrete object. It can lead to some non-obvious behaviors, primarily if
    a developer uses the `auto` keyword too frequently. The library provides two functions,
    `eval()` and `evaluate()`, to evaluate a given expression. The `evaluate()` function
    assists in deducing the exact result type of the operation via the `auto` keyword,
    and the `eval()` function should be used to explicitly evaluate a sub-expression
    within a larger expression.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在Blaze中，算术运算符如`+`本身并不执行任何计算。这些运算符返回一个*表达式对象*，它描述了要执行的计算。实际的计算发生在整个表达式评估之后，通常是在`=`算术运算符或具体对象的构造函数中。这可能导致一些不明显的行为，主要如果开发者频繁使用`auto`关键字。该库提供了两个函数`eval()`和`evaluate()`来评估给定的表达式。`evaluate()`函数通过`auto`关键字帮助推断操作的精确结果类型，而`eval()`函数应用于显式评估较大表达式中的子表达式。
- en: 'Sometimes, we need to perform operations only on a part of the matrix. For
    this purpose, Blaze provides the `blaze::submatrix` and `blaze::subvector` classes,
    which can be parameterized with template parameters. These parameters are the
    top-left starting point and the width and height of a region. Also, there are
    functions with the same names that take the same arguments and can be used in
    a runtime. The following code shows how to use this class:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们只需要对矩阵的一部分执行操作。为此，Blaze提供了`blaze::submatrix`和`blaze::subvector`类，这些类可以用模板参数进行参数化。这些参数是一个区域的左上角起始点、宽度和高度。还有具有相同名称的函数，它们接受相同的参数，可以在运行时使用。以下代码展示了如何使用此类：
- en: '[PRE30]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'There are two more functions to access rows and columns by index, which are
    also a type of `block` operation. The following snippet shows how to use the `col`
    and `row` functions:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个额外的函数可以通过索引访问行和列，它们也是一种`block`操作。以下代码片段展示了如何使用`col`和`row`函数：
- en: '[PRE31]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In contrast to Eigen, Blaze doesn’t support implicit broadcasting. But there
    is the `blaze::expand()` function that can virtually expand a matrix or vector
    without actual memory allocation. The following code shows how to use it:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 与Eigen相比，Blaze不支持隐式广播。但有一个`blaze::expand()`函数可以在不实际分配内存的情况下虚拟地扩展矩阵或向量。以下代码展示了如何使用它：
- en: '[PRE32]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The result of this operation will be the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作的输出结果如下：
- en: '[PRE33]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Using ArrayFire
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用ArrayFire
- en: '`ArrayFire` is a general-purpose high-performance C++ library for parallel
    computing.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`ArrayFire`是一个用于并行计算的高性能通用C++库。'
- en: '**Parallel computing** is a method of solving complex problems by dividing
    them into smaller tasks and executing them simultaneously across multiple processors
    or cores. This approach can significantly speed up the processing time compared
    to sequential computing, making it an essential tool for data-intensive applications
    such as ML.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**并行计算**是一种通过将复杂问题分解为更小的任务并在多个处理器或核心上同时执行来解决复杂问题的方法。与顺序计算相比，这种方法可以显著加快处理时间，使其成为数据密集型应用（如机器学习）的必要工具。'
- en: It provides a single `array` type to represent matrices, volumes, and vectors.
    This array-based notation expresses computational algorithms in readable mathematical
    notation so users don’t need to express parallel computations explicitly. It has
    extensive vectorization and parallel batched operations. This library supports
    accelerated execution on CUDA and OpenCL devices. Another interesting feature
    of the `ArrayFire` library is that it optimizes memory usage and arithmetic calculations
    by runtime analysis of the executed code. This becomes possible by avoiding many
    temporary allocations.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供了一个单一的 `array` 类型来表示矩阵、体积和向量。这种基于数组的表示法以可读的数学符号表达计算算法，因此用户不需要显式地表达并行计算。它具有广泛的向量化和平行批量操作。此库支持在
    CUDA 和 OpenCL 设备上加速执行。`ArrayFire` 库的另一个有趣特性是，它通过执行代码的运行时分析来优化内存使用和算术计算。通过避免许多临时分配，这成为可能。
- en: 'We can define the type for a matrix with known dimensions and a floating-point
    data type like this:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样定义具有已知维度和浮点数据类型的矩阵类型：
- en: '[PRE34]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can define a 64-bit floating vector in the following way:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样定义一个 64 位浮点向量：
- en: '[PRE35]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'To put some values into these objects, we can use several approaches. We can
    use special predefined initialization functions, as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这些值放入这些对象中，我们可以使用几种方法。我们可以使用特殊预定义的初始化函数，如下所示：
- en: '[PRE36]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This code construction initializes the matrix values in the following way:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码构造以以下方式初始化矩阵值：
- en: '[PRE37]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Notice that the matrix was initialized in the column-major format. The `ArrayFire`
    library doesn’t support row-major initialization.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到矩阵是以列主序格式初始化的。`ArrayFire` 库不支持行主序初始化。
- en: 'We can use direct element access to set or change matrix coefficients. The
    following code sample shows how to use the `()` operator for such an operation:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用直接元素访问来设置或更改矩阵系数。以下代码示例展示了如何使用 `()` 运算符进行此类操作：
- en: '[PRE38]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'One important difference from other libraries that we discussed is that you
    can’t map existent C/C++ array data to the ArrayFire `array` object, as it will
    be copied. The following snippet shows this situation:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前讨论的其他库的一个重要区别是，你不能将现有的 C/C++ 数组数据映射到 ArrayFire 的 `array` 对象，因为它将被复制。以下代码片段展示了这种情况：
- en: '[PRE39]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Only memory allocated in CUDA or OpenCL devices will not be copied, but ArrayFire
    will take ownership of a pointer.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在 CUDA 或 OpenCL 设备上分配的内存将不会被复制，但 ArrayFire 将接管指针的所有权。
- en: 'We can use initialized array objects in mathematical operations. Arithmetic
    operations in the `ArrayFire` library are offered either through overloads of
    standard C++ arithmetic operators such as `+`, `-`, or `*`, or through methods
    such as `af::matmul`. The following code sample shows how to express general math
    operations in ArrayFire:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在数学运算中使用初始化的数组对象。`ArrayFire` 库中的算术运算可以通过标准 C++ 算术运算符的重载，如 `+`、`-` 或 `*`，或通过如
    `af::matmul` 这样的方法提供。以下代码示例展示了如何在 ArrayFire 中表达通用数学运算：
- en: '[PRE40]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In contrast to other libraries we already discussed, ArrayFire doesn’t extensively
    use template expression for joining arithmetical operations. This library uses
    a **just-in-time** (**JIT**) compilation engine that converts mathematical expressions
    into computational kernels for CUDA, OpenCL, or CPU devices. Also, this engine
    merges different operations together to provide the best performance. This operation
    fusion technology decreases the number of kernel calls and reduces global memory
    operations.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前讨论的其他库相比，ArrayFire 并不广泛使用模板表达式来连接算术运算。此库使用一个 **即时**（**JIT**）编译引擎，将数学表达式转换为
    CUDA、OpenCL 或 CPU 设备的计算内核。此外，此引擎将不同的操作合并在一起，以提供最佳性能。这种操作融合技术减少了内核调用次数，并减少了全局内存操作。
- en: 'Sometimes, we need to perform operations only on a part of the array. For this
    purpose, ArrayFire provides a special indexing technique. There are particular
    classes that can be used to express index sub-ranges for given dimensions. They
    are as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following code shows an example of how to access only the central part
    of a matrix:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To access and update a particular row or column in the array, there are `row(i)`
    and `col(i)` methods specifying a single row or column. They can be used in the
    following way:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Also, to work with several rows or columns, there are the `rows(first,last)`
    and `cols(first,last)` methods specifying a span of rows or columns.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'ArrayFire doesn’t support implicit broadcasting but there is the `af::batchFunc`
    function that can be used to simulate and parallelize such functionality. In general,
    this function finds a batch dimension of data and applies the given function to
    multiple data chunks in parallel. The following code shows how to use it:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The result of this operation will be the following:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Notice that the vector was a column-major one.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Using Dlib
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`Dlib` is a modern C++ toolkit containing ML algorithms and tools for creating
    computer vision software in C++. Most of the linear algebra tools in Dlib deal
    with dense matrices. However, there is also limited support for working with sparse
    matrices and vectors. In particular, the `Dlib` tools represent sparse vectors
    using the containers from the C++ **standard template** **library** (**STL**).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main container types in Dlib to work with linear algebra: the
    `matrix` and `vector` classes. Matrix operations in Dlib are implemented using
    the expression templates technique, which allows them to eliminate the temporary
    matrix objects that would usually be returned from expressions such as `M =` `A+B+C+D`.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a matrix sized at compile time in the following way, by specifying
    dimensions as template arguments:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Alternatively, we can create dynamically sized matrix objects. In such a case,
    we pass the matrix dimensions to the constructor, as shown in the following code
    snippet:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Later, we can change the size of this matrix with the following method:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We can initialize matrix values with a comma operator, as shown in the following
    code snippet:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As in the previous libraries, we can wrap an existing C++ array to the matrix
    object, as shown in the following code snippet:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Also, we can access matrix elements with the `()` operator to modify or get
    a particular value, as shown in the following code snippet:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The `Dlib` library has a set of predefined functions to initialize a matrix
    with values such as the identity matrix, ones, or random values, as illustrated
    in the following code snippet:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Most linear algebra arithmetic operations in the `Dlib` library are implemented
    through overloads of standard C++ arithmetic operators such as `+`, `-`, or `*`.
    Other complex operations are provided by the library as standalone functions.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows the use of arithmetic operations in the `Dlib`
    library:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'To work with partial access to matrices, Dlib provides a set of special functions.
    The following code sample shows how to use some of them:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Broadcasting in the `Dlib` library can be modeled with the `set_rowm()`, `set_colm()`,
    and `set_subm()` functions that give modifier objects for a particular matrix
    row, column, or rectangular part of the original matrix. Objects returned from
    these functions support all set or arithmetic operations. The following code snippet
    shows how to add a vector to the columns:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: In this section, we learned about the main concepts of linear algebra and their
    implementation in different C++ libraries. We saw how to create matrices and tensors,
    and how to perform different mathematical operations with them. In the following
    section, we will see our first complete ML example—solving the regression problem
    with the linear regression approach.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: An overview of linear regression
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider an example of the real-world supervised ML algorithm called linear
    regression. In general, **linear regression** is an approach for modeling a target
    value (dependent value) based on an explanatory value (independent value). This
    method is used for forecasting and finding relationships between values. We can
    classify regression methods by the number of inputs (independent variables) and
    the type of relationship between the inputs and outputs (dependent variables).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression is the case where the number of independent variables
    is *1*, and there is a linear relationship between the independent (*x*) and dependent
    (*y*) variables.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear regression is widely used in different areas such as scientific research,
    where it can describe relationships between variables, as well as in applications
    within industry, such as revenue prediction. For example, it can estimate a trend
    line that represents the long-term movement in the stock price time-series data.
    It tells whether the interest value of a specific dataset has increased or decreased
    over the given period, as illustrated in the following screenshot:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Linear regression visualization](img/B19849_01_01.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Linear regression visualization
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: If we have one input variable (independent variable) and one output variable
    (dependent variable), the regression is called simple, and we use the term **simple
    linear regression** for it. With multiple independent variables, we call this
    **multiple linear regression** or **multivariable linear regression**. Usually,
    when we are dealing with real-world problems, we have a lot of independent variables,
    so we model such problems with multiple regression models. Multiple regression
    models have a universal definition that covers other types, so even simple linear
    regression is often defined using the multiple regression definition.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Solving linear regression tasks with different libraries
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assume that we have a dataset, ![](img/B19849_Formula_28.png), so that we can
    express the linear relation between *y* and *x* with mathematical formula in the
    following way:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_29.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: 'Here, *p* is the dimension of the independent variable, and *T* denotes the
    transpose, so that ![](img/B19849_Formula_30.png) is the inner product between
    vectors ![](img/B19849_Formula_31.png) and *β*. Also, we can rewrite the previous
    expression in matrix notation, as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_32.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: '![](img/B19849_Formula_33.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: ','
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_34.jpg)![](img/B19849_Formula_35.jpg)![](img/B19849_Formula_36.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: 'The preceding matrix notation can be explained as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '*y*: This is a vector of observed target values.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*: This is a matrix of row-vectors, ![](img/B19849_Formula_31.png), which
    are known as explanatory or independent values.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ß: This is a (*p+1*) dimensional parameters vector.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ε: This is called an error term or noise. This variable captures all other
    factors that influence the *y*-dependent variable other than the regressors.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we are considering simple linear regression, *p* is equal to 1, and the
    equation will look like this:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_38.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'The goal of the linear regression task is to find parameter vectors that satisfy
    the previous equation. Usually, there is no exact solution to such a system of
    linear equations, so the task is to estimate parameters that satisfy these equations
    with some assumptions. One of the most popular estimation approaches is one based
    on the principle of least squares: minimizing the sum of the squares of the differences
    between the observed dependent variable in the given dataset and those predicted
    by the linear function. This is called the **ordinary least squares** (**OLS**)
    estimator. So, the task can be formulated with the following formula:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_39.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding formula, the objective function, *S*, is given by the following
    matrix notation:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_40.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: 'This minimization problem has a unique solution, in the case that the *p* columns
    of the *x* matrix are linearly independent. We can get this solution by solving
    the *normal equation*, as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_41.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: Linear algebra libraries can solve such equations directly with an analytical
    approach, but it has one significant disadvantage—computational cost. In the case
    of large dimensions of *y* and *x*, requirements for computer memory amount and
    computational time are too big to solve real-world tasks.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: So, usually, this minimization task is solved with iterative approaches. **gradient
    descent** (**GD**) is an example of such an algorithm. GD is a technique based
    on the observation that if the function ![](img/B19849_Formula_42.png) is defined
    and is differentiable in a neighborhood of a point ![](img/B19849_Formula_43.png),
    then ![](img/B19849_Formula_44.png) decreases fastest when it goes in the direction
    of the negative gradient of *S* at point ![](img/B19849_Formula_43.png).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'We can change our ![](img/B19849_Formula_46.png) objective function to a form
    more suitable for an iterative approach. We can use the **mean squared error**
    (**MSE**) function, which measures the difference between the estimator and the
    estimated value, as illustrated here:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_47.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: 'In the case of multiple regression, we take partial derivatives for this function
    for each of *x* components, as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_48.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: 'So, in the case of linear regression, we take the following derivatives:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_49.jpg)![](img/B19849_Formula_50.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: 'The whole algorithm has the following description:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Initialize β with zeros.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a value for the learning rate parameter that controls how much we are
    adjusting parameters during the learning procedure.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the following values of β:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_51.jpg)![](img/B19849_Formula_52.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: Repeat steps 1–3 a number of times or until the MSE value reaches a reasonable
    amount.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The previously described algorithm is one of the simplest supervised ML algorithms.
    We described it with the linear algebra concepts we introduced earlier in the
    chapter. Later, it became more evident that almost all ML algorithms use linear
    algebra under the hood. Linear regression is widely used in various industries
    for predictive analysis, forecasting, and decision-making. Here are some real-world
    examples of linear regression applications in finance, marketing, and healthcare.
    Linear regression can be used to predict stock prices based on historical data
    such as company earnings, interest rates, and economic indicators. This helps
    investors make informed decisions about when to buy or sell stocks. Linear regression
    models can be built to predict customer behavior based on demographic information,
    purchase history, and other relevant data. This allows marketers to target their
    campaigns more effectively and optimize their marketing spend. Linear regression
    is used to analyze medical data to identify patterns and relationships that can
    help improve patient outcomes. For example, it can be used to study the impact
    of certain treatments on patient health.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: The following samples show the higher-level API in different linear algebra
    libraries for solving the *linear regression* task, and we provide them to show
    how libraries can simplify the complicated math used underneath. We will give
    the details of the APIs used in these samples in the following chapters.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Solving linear regression tasks with Eigen
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several iterative methods for solving problems of the ![](img/B19849_Formula_53.png)
    form in the `Eigen` library. The `LeastSquaresConjugateGradient` class is one
    of them, which allows us to solve linear regression problems with the conjugate
    gradient algorithm. The `ConjugateGradient` algorithm can converge more quickly
    to the function’s minimum than regular GD but requires that matrix *A* is positively
    defined to guarantee numerical stability. The `LeastSquaresConjugateGradient`
    class has two main settings: the maximum number of iterations and a tolerance
    threshold value that is used as a stopping criterion as an upper bound to the
    relative residual error, as illustrated in the following code block:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'For new `x` inputs, we can predict new `y` values with matrix operations, as
    follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Also, we can calculate the parameter’s `b` vector (the linear regression task
    solution) by solving the *normal equation* directly, as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Solving linear regression tasks with Blaze
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since Blaze is just a mathematical library, there are no special classes or
    functions to solve linear regression tasks. However, the normal equation approach
    can be easily implemented. Let’s see how to define and solve linear regression
    tasks with Blaze:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume we have our training data:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'So we can find linear regression coefficients in the following way:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Then, we can use estimated coefficients for making predictions on new data.
    The following code snippet shows how to do it:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Notice that we virtually expand the coefficients vector to perform element-wise
    multiplication with the x data.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Solving linear regression tasks with ArrayFire
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ArrayFire also doesn’t have special functions and classes to solve this type
    of problem. However, as it has all the necessary mathematical abstraction, the
    normal equation approach can be applied. Another approach is an iterative one
    that uses GD. This algorithm was described in the first section of this chapter.
    Such a technique eliminates the necessity of calculating inverse matrices, making
    it possible to apply it to a larger amount of training data. Calculating an inverse
    for a large matrix is a very performance-expensive operation.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define a lambda function to calculate prediction values from data and
    coefficients:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Assume that we have training data defined in the `x` and `y` variables. We
    define the `train_weights` variable to hold and update coefficients that want
    to learn from the training data:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Then, we can define the GD loop where we will iteratively update coefficients.
    The following code snippet shows how to implement it:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The most important part of this loop is calculating the prediction error:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Another important part is calculating the gradient values based on partial
    derivatives related to each of the coefficients:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We join these values into one vector to make a single expression for updating
    the training parameters:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We can stop this training loop after a number of iterations or when the cost
    function value reaches some appropriate convergence value. The cost function value
    can be calculated in the following way:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: You can see that this is just a sum of squared errors for all training samples.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with Dlib
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Dlib` library provides the `krr_trainer` class, which can get the template
    argument of the `linear_kernel` type to solve linear regression tasks. This class
    implements direct analytical solving for this type of problem with the kernel
    ridge regression algorithm, as illustrated in the following code block:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'For new `x` inputs, we can predict new `y` values in the following way:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: In this section, we learned how to solve the linear regression problem with
    different C++ libraries. We saw that some of them contain the complete algorithm
    implementation that can be easily applied, and we saw how to implement this approach
    from scratch using just basic linear algebra primitives.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned what ML is, how it differs from other computer algorithms,
    and how it became so popular. We also became familiar with the necessary mathematical
    background required to begin working with ML algorithms. We looked at software
    libraries that provide APIs for linear algebra and also implemented our first
    ML algorithm—linear regression.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: There are other linear algebra libraries for C++. Moreover, the popular deep
    learning frameworks use their own implementations of linear algebra libraries.
    For example, the MXNet framework is based on the `mshadow` library, and the PyTorch
    framework is based on the ATen library. Some of these libraries can use GPU or
    special CPU instructions to speed up calculations. Such features do not usually
    change the API but require some additional library initialization settings or
    explicit object conversion to different backends such as CPUs or GPUs.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Real ML projects can be challenging and complex. Common pitfalls include data
    quality issues, overfitting and underfitting, choosing the wrong model, and not
    having enough computing resources. Poor data quality can also affect model performance.
    It’s essential to clean and preprocess data to remove outliers, handle missing
    values, and transform features for better representation. The model should be
    chosen based on the nature of the problem and the available data. Overfitting
    occurs when the model memorizes the training data instead of learning general
    patterns, while underfitting happens when the model cannot capture the underlying
    structure of the data. To avoid these pitfalls, it is important to have a clear
    understanding of the problem, use appropriate preprocessing techniques for data,
    choose the right model, and evaluate the performance using metrics that are relevant
    to the task. Best practices in ML also include monitoring the model’s performance
    in production and making adjustments as needed. We will discuss the details of
    these techniques throughout the book. In the next two chapters, we will learn
    more about available software tools that are necessary to implement more complicated
    algorithms, and we will also learn more theoretical background on how to manage
    ML algorithms.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Basic Linear Algebra for Deep* *Learning*: [https://towardsdatascience.com/linear-algebra-for-deep-learning-f21d7e7d7f23](https://towardsdatascience.com/linear-algebra-for-deep-learning-f21d7e7d7f23)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Learning*, an MIT Press book: [https://www.deeplearningbook.org/contents/linear_algebra.html](https://www.deeplearningbook.org/contents/linear_algebra.html)'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What is Machine* *Learning?*: [https://www.mathworks.com/discovery/machine-learning.html](https://www.mathworks.com/discovery/machine-learning.html)'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Eigen` library documentation: [https://gitlab.com/libeigen/eigen](https://gitlab.com/libeigen/eigen)'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `xtensor` library documentation: [https://xtensor.readthedocs.io/en/latest/](https://xtensor.readthedocs.io/en/latest/)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Dlib` library documentation: [http://dlib.net/](http://dlib.net/)'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `blaze` library documentation: [https://bitbucket.org/blaze-lib/blaze/wiki/Home](https://bitbucket.org/blaze-lib/blaze/wiki/Home)'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ArrayFire library documentation: [https://arrayfire.org/docs/index.htm](https://arrayfire.org/docs/index.htm)'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
