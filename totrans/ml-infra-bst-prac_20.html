<html><head></head><body>
		<div id="_idContainer139">
			<h1 class="chapter-number"><a id="_idTextAnchor187"/>16</h1>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor188"/>Integrating ML Systems  in Ecosystems</h1>
			<p>ML systems have gained a lot of popularity for two reasons – their ability to learn from data (which we’ve explored throughout this book), and their ability to be packaged into <span class="No-Break">web services.</span></p>
			<p>Packaging these ML systems into web services allows us to integrate them into workflows in a very flexible way. Instead of compiling or using dynamically linked libraries, we can deploy ML components that communicate over HTTP protocols using JSON protocols. We have already seen how to use that protocol by using the GPT-3 model that is hosted by OpenAI. In this chapter, we’ll explore the possibility of creating a Docker container with a pre-trained ML model, deploying it, and integrating it with <span class="No-Break">other components.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>ML system of systems – <span class="No-Break">software ecosystems</span></li>
				<li>Creating web services over ML models <span class="No-Break">using Flask</span></li>
				<li>Deploying ML models <span class="No-Break">using Docker</span></li>
				<li>Combining web services <span class="No-Break">into ecosystems</span></li>
			</ul>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor189"/>Ecosystems</h1>
			<p>In the<a id="_idIndexMarker605"/> dynamic realm of software engineering, the tools, methodologies, and paradigms are in a constant state of evolution. Among the most influential forces driving this transformation is ML. While ML itself is a marvel of computational prowess, its true genius emerges when integrated into the broader software engineering ecosystems. This chapter delves into the nuances of embedding ML within an ecosystem. Ecosystems are groups of software that work together but are not connected at compile time. A well-known ecosystem is the PyTorch ecosystem, where a set of libraries work together in the context of ML. However, there is much more than that to ML ecosystems in <span class="No-Break">software engineering.</span></p>
			<p>From automated testing systems that learn from each iteration to recommendation engines that adapt to user behaviors, ML is redefining how software is designed, developed, and deployed. However, integrating ML into software engineering is not a mere plug-and-play operation. It demands a rethinking of traditional workflows, a deeper understanding of data-driven decision-making, and a commitment to continuous learning <span class="No-Break">and adaptation.</span></p>
			<p>As we delve deeper into the integration of ML within software engineering, it becomes imperative to discuss two pivotal components that are reshaping the landscape: web services and Docker containers. These technologies, while not exclusive to ML applications, play a crucial role in the seamless deployment and scaling of ML-driven solutions in the <span class="No-Break">software ecosystem.</span></p>
			<p>Web services, especially in the era of microservices architecture, provide a modular approach to building software applications. By encapsulating specific functionalities into distinct services, they allow for greater flexibility and scalability. When combined with ML models, web services can deliver dynamic, real-time responses based on the insights derived from data. For instance, a web service might leverage an ML model to provide personalized content recommendations to users or to detect fraudulent activities in <span class="No-Break">real time.</span></p>
			<p>Docker containers, on the other hand, have revolutionized the way software, including ML models, is packaged and deployed. Containers encapsulate an application along with all its dependencies into a standardized unit, ensuring consistent behavior across different environments. For ML practitioners, this means the painstaking process of setting up environments, managing dependencies, and ensuring compatibility is vastly simplified. Docker containers ensure that an ML model trained on a developer’s machine will run with the same efficiency and accuracy on a production server or any <span class="No-Break">other platform.</span></p>
			<p>Furthermore, when web services and Docker containers are combined, they pave the way for <a id="_idIndexMarker606"/>ML-driven microservices. Such architectures allow for the rapid deployment of scalable, isolated services that can be updated independently without disrupting the entire system. This is especially valuable in the realm of ML, where models might need frequent updates based on new data or <span class="No-Break">improved algorithms.</span></p>
			<p>In this chapter, we’ll learn how to use both technologies to package models and create an ecosystem based on Docker containers. After reading this chapter, we shall have a good understanding of how we can scale up our development by using ML as part of a larger system of systems – <span class="No-Break">ecosystems.</span></p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor190"/>Creating web services over ML models using Flask</h1>
			<p>In<a id="_idIndexMarker607"/> this book, we’ve mostly focused on training, evaluating, and deploying ML models. However, we did not discuss the need to structure them flexibly. We worked with monolithic software. Monolithic software is characterized by its unified, single code base structure where all the functionalities, from the user interface to data processing, are tightly interwoven and operate as one cohesive unit. This design simplifies initial development and deployment since everything is bundled together and they are compiled together. Any change, however minor, requires the entire application to be rebuilt and redeployed. This makes it problematic when the evolution of contemporary software <span class="No-Break">is fast.</span></p>
			<p>On the other hand, web service-based software, which is often associated with microservices architecture, breaks down the application into smaller, independent services that communicate over the web, typically using protocols such as HTTP and REST. Each service is responsible for a specific functionality and operates independently. This modular approach offers greater flexibility. Services can be scaled, updated, or redeployed individually without affecting the entire system. Moreover, failures in one service don’t necessarily bring down the whole application. <span class="No-Break"><em class="italic">Figure 16</em></span><em class="italic">.1</em> presents how the difference between these two types of software can <span class="No-Break">be seen:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer135">
					<img alt="Figure 16.1 – Monolithic software versus web service-based software" src="image/B19548_16_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.1 – Monolithic software versus web service-based software</p>
			<p>On the <a id="_idIndexMarker608"/>left-hand side, we have all the components bundled together into one product. Users interact with the product through the user interface. Only one user can interact with the software, and more users require more installations of <span class="No-Break">the software.</span></p>
			<p>On the right-hand side, we have a decentralized architecture where each component is a separate web service. The coordination of these components is done by a thin client. If more users/clients want to use the same services, they can just connect them using the HTTP REST <span class="No-Break">protocol (API).</span></p>
			<p>Here is my first <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #75</p>
			<p class="callout">Use web services (RESTful API) when deploying ML models <span class="No-Break">for production.</span></p>
			<p>Although it takes additional effort to create web services, it is worth using them. They provide a great separation of concerns and asynchronous access and also provide great possibilities for load balancing. We can use different servers to run the same web service and therefore balance <span class="No-Break">the load.</span></p>
			<p>So, let’s create the first web service <span class="No-Break">using Flask.</span></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor191"/>Creating a web service using Flask</h2>
			<p>Flask is a <a id="_idIndexMarker609"/>framework that allows <a id="_idIndexMarker610"/>us to provide easy access to internal APIs via the REST interface over HTTP protocol. First, we need to <span class="No-Break">install it:</span></p>
			<pre class="console">
pip install flask
pip install flask-restful</pre>			<p>Once we’ve<a id="_idIndexMarker611"/> installed the interface, we can write our programs. In this example, our first web service calculates the lines of code and complexity of the program sent to it. The following code fragment <span class="No-Break">exemplifies this:</span></p>
			<pre class="source-code">
from fileinput import filename
from flask import *
from radon.complexity import cc_visit
from radon.cli.harvest import CCHarvester
app = Flask(__name__)
# Dictionary to store the metrics for the file submitted
# Metrics: lines of code and McCabe complexity
metrics = {}
def calculate_metrics(file_path):
    with open(file_path, 'r') as file:
        content = file.read()
    # Count lines of code
    lines = len(content.splitlines())
    # Calculate McCabe complexity
    complexity = cc_visit(content)
    # Store the metrics in the dictionary
    metrics[file_path] = {
        'lines_of_code': lines,
        'mccabe_complexity': complexity
    }
@app.route('/')
def main():
    return render_template("index.html")
@app.route('/success', methods=['POST'])
def success():
    if request.method == 'POST':
        f = request.files['file']
        # Save the file to the server
        file_path = f.filename
        f.save(file_path)
        # Calculate metrics for the file
        calculate_metrics(file_path)
        # Return the metrics for the file
        return metrics[file_path]
@app.route('/metrics', methods=['GET'])
def get_metrics():
    if request.method == 'GET':
        return metrics
if __name__ == '__main__':
    app.run(host='0.0.0.0', debug=True)</pre>			<p>First, the code<a id="_idIndexMarker612"/> requires a few imports and<a id="_idIndexMarker613"/> then initializes the application in <strong class="source-inline">app = Flask(__name__)</strong>. Then, it creates the routes – that is, the places where the program will be able to communicate via the <span class="No-Break">REST API:</span></p>
			<ul>
				<li><strong class="source-inline">@app.route('/')</strong>: This is a decorator that defines a route for the root URL (<strong class="source-inline">"/"</strong>). When users access the root URL, it renders the <strong class="source-inline">"</strong><span class="No-Break"><strong class="source-inline">index.html"</strong></span><span class="No-Break"> template.</span></li>
				<li><strong class="source-inline">@app.route('/success', methods=['POST'])</strong>: This decorator defines a route for the <strong class="source-inline">"/success"</strong> URL, which expects HTTP POST requests. This route is used to handle file uploads, count lines of code, and calculate <span class="No-Break">McCabe complexity.</span></li>
				<li><strong class="source-inline">@app.route('/metrics', methods=['GET'])</strong>: This decorator defines a route for the <strong class="source-inline">"/metrics"</strong> URL, which expects HTTP GET requests. It is used to retrieve and display <span class="No-Break">the metrics.</span></li>
				<li><strong class="source-inline">def main()</strong>: This function is associated with the root (<strong class="source-inline">"/"</strong>) route. It returns an HTML template called <strong class="source-inline">"index.html"</strong> when users access the <span class="No-Break">root URL.</span></li>
				<li><strong class="source-inline">def success()</strong>: This function is associated with the <strong class="source-inline">"/success"</strong> route, which<a id="_idIndexMarker614"/> handles <span class="No-Break">file uploads:</span><ul><li>It checks if the request method <span class="No-Break">is POST</span></li><li>It saves the uploaded file to <span class="No-Break">the server</span></li><li>It counts the lines of code in the <span class="No-Break">uploaded file</span></li><li>It calculates the McCabe complexity using the <span class="No-Break">radon library</span></li><li>It stores the metrics (lines of code and McCabe complexity) in the <span class="No-Break">metrics dictionary</span></li><li>It <a id="_idIndexMarker615"/>returns the metrics for the uploaded file as a <span class="No-Break">JSON response</span></li></ul></li>
				<li><strong class="source-inline">def get_metrics()</strong>: This function is associated with the <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">metrics</strong></span><span class="No-Break"> route:</span><ul><li>It checks if the request method <span class="No-Break">is GET.</span></li><li>It returns the entire metrics dictionary as a JSON response. This is used for debugging purposes to see the metrics for all files that are uploaded during <span class="No-Break">the session.</span></li></ul></li>
				<li><strong class="source-inline">if __name__ == '__main__':</strong>: This block ensures that the web application is only run when this script is executed directly (not when it’s imported as <span class="No-Break">a module).</span></li>
				<li><strong class="source-inline">app.run(host='0.0.0.0', debug=True)</strong>: This starts the Flask application in debug mode, allowing you to see detailed error messages <span class="No-Break">during development.</span></li>
			</ul>
			<p>Then, the application is executed – <strong class="source-inline">app.run(debug=True)</strong> starts the Flask application with debugging mode enabled. This means that any changes made to the code will automatically reload the server, and any errors will be displayed in the browser. Once<a id="_idIndexMarker616"/> we execute it, the following web page appears (note that the code of the page has to be in the <strong class="source-inline">templates</strong> subfolder<a id="_idIndexMarker617"/> and should contain the <span class="No-Break">following code):</span></p>
			<pre class="source-code">
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Machine learning best practices for software engineers: Chapter 16 - Upload a file to make predictions&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;h1&gt;Machine learning best practices for software engineers - Chapter 16&lt;/h1&gt;
    &lt;p&gt;This page allows to upload a file to a web service that has been written using Flask. The web application behind this interface calculates metrics that are important for the predictions. It returns a JSON string with the metrics.  &lt;/p&gt;
    &lt;p&gt;We need another web app that contains the model in order to actually obtain predictions if the file can contain defects. &lt;/p&gt;
    &lt;h1&gt;Upload a file to make predictions&lt;/h1&gt;
    &lt;p&gt;The file should be a .c or .cpp file&lt;/p&gt;
    &lt;form action = "/success" method = "post" enctype="multipart/form-data"&gt;
        &lt;input type="file" name="file" /&gt;
        &lt;input type = "submit" value="Upload"&gt;
    &lt;/form&gt;
    &lt;p&gt;Disclaimer: the container saves the file it its local folder, so don't send any sensitive files for analysis.&lt;/p&gt;
    &lt;p&gt;This is a research prototype&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</pre>			<p>The page <a id="_idIndexMarker618"/>contains a simple form that<a id="_idIndexMarker619"/> allows us to upload the file to <span class="No-Break">the server:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer136">
					<img alt="Figure 16.2 – The web page where we can send the file to calculate lines of code. This is only one way of sending this information to the web service" src="image/B19548_16_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.2 – The web page where we can send the file to calculate lines of code. This is only one way of sending this information to the web service</p>
			<p>After uploading the file, we get <span class="No-Break">the results:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer137">
					<img alt="Figure 16.3 – Results of calculating the lines of code" src="image/B19548_16_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.3 – Results of calculating the lines of code</p>
			<p>The <a id="_idIndexMarker620"/>majority of the transmission is done by the Flask framework, which makes the development a really pleasant <a id="_idIndexMarker621"/>experience. However, just counting lines of code and complexity is not a great ML model. Therefore, we need to create another web service with the code of the ML <span class="No-Break">model itself.</span></p>
			<p>Therefore, my next best practice is about <span class="No-Break">dual interfaces.</span></p>
			<p class="callout-heading">Best practice #76</p>
			<p class="callout">Use both a website and an API for the <span class="No-Break">web services.</span></p>
			<p>Although we can always design web services so that they only accept JSON/REST calls, we should try to provide different interfaces. The web interface (presented previously) allows us to test the web service and even send data to it without the need to write a <span class="No-Break">separate program.</span></p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor192"/>Creating a web service that contains a pre-trained ML model</h2>
			<p>The code of the ML<a id="_idIndexMarker622"/> model web service follows the same template. It uses the Flask framework to provide the REST API of the web service for the software. Here is the code fragment that shows this <span class="No-Break">web service:</span></p>
			<pre class="source-code">
#
# This is a flask web service to make predictions on the data
# that is sent to it. It is meant to be used with the measurement instrument
#
from flask import *
from joblib import load
import pandas as pd
app = Flask(__name__)   # create an app instance
# entry point where we send JSON with two parameters:
# LOC and MCC
# and make prediction using make_pre<a id="_idTextAnchor193"/>diction method
@app.route('/predict/&lt;loc&gt;/&lt;mcc&gt;')
def predict(loc,mcc):
    return {'Defect': make_prediction(loc, mcc)}
@app.route('/')
def hello():
    return 'Welcome to the predictor! You need to send a GET request with two parameters: LOC (lines of code) and MCC (McCabe complexity))'
# the main method for making the prediction
# using the model that is stored in the joblib file
def make_prediction(loc, mcc):
    # now read the model from the joblib file
    # and predict the defects for the X_test data
    dt = load('dt.joblib')
    # input data to the model
    input = {'LOC': loc,
             'MCC': mcc}
    # convert input data into dataframe
    X_pred = pd.DataFrame(input, index=[0])
    # make prediction
    y_pred = dt.predict(X_pred)
    # return the prediction
    # as an integer
    return int(y_pred[0])
# run the application
if __name__ == '__main__':
    app.run(debug=True)</pre>			<p>The main<a id="_idIndexMarker623"/> entry point to this web service takes two parameters: <strong class="source-inline">@app.route('/predict/&lt;loc&gt;/&lt;mcc&gt;')</strong>. It uses these two parameters as parameters of the method that instantiates the models and uses it to make a prediction – <strong class="source-inline">make_prediction(loc, mcc)</strong>. The <strong class="source-inline">make_prediction</strong> method reads a model from a <strong class="source-inline">joblib</strong> file and uses it to predict whether the module will contain a defect or not. I use <strong class="source-inline">joblib</strong> for this model as it is based on a NumPy array. However, if a model is based on a Python object (for example, when it is an estimator from a scikit-learn library), then it is better to use pickle instead of <strong class="source-inline">joblib</strong>. It returns the JSON string containing the result. <span class="No-Break"><em class="italic">Figure 16</em></span><em class="italic">.4</em> illustrates how we can use a web browser to invoke this web service – not the <span class="No-Break">address bar:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer138">
					<img alt="Figure 16.4 – Using the prediction endpoint to get the predicted number of defects in this module" src="image/B19548_16_4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.4 – Using the prediction endpoint to get the predicted number of defects in this module</p>
			<p>The <a id="_idIndexMarker624"/>address bar sends the parameters to the model and the response is a JSON string that says that this module will, most probably, contain a defect. Well, this is not a surprise as we say that the module has 10 lines of code and a complexity of 100 (unrealistic, <span class="No-Break">but possible).</span></p>
			<p>These two web services already give us an example of how powerful the REST API can be. Now, let’s learn how to package that with Docker so that we can deploy these web services even <span class="No-Break">more easily.</span></p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor194"/>Deploying ML models using Docker</h1>
			<p>To create a <a id="_idIndexMarker625"/>Docker container with our newly created <a id="_idIndexMarker626"/>web service (or two of them), we need to install Docker on <a id="_idIndexMarker627"/>our system. Once we’ve installed Docker, we can use it to compile <span class="No-Break">the container.</span></p>
			<p>The crucial part of packaging the web service into the Docker container is the Dockerfile. It is a recipe for how to assemble the container and how to start it. If you’re interested, I’ve suggested a good book about Docker containers in the <em class="italic">Further reading</em> section so that you can learn more about how to create more advanced components than the ones in <span class="No-Break">this book.</span></p>
			<p>In our example, we <a id="_idIndexMarker628"/>need two containers. The first one will be<a id="_idIndexMarker629"/> the container for the measurement instrument. The code for that container is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
FROM alpine:latest
RUN apk update
RUN apk add py-pip
RUN apk add --no-cache python3-dev
RUN pip install --upgrade pip
WORKDIR /app
COPY . /app
RUN pip --no-cache-dir install -r requirements.txt
CMD ["python3", "main.py"]</pre>			<p>This Dockerfile is setting up an Alpine Linux-based environment, installing Python and the necessary development packages, copying your application code into the image, and then running the Python script as the default command when the container starts. It’s a common pattern for creating Docker images for Python applications. Let’s take a <span class="No-Break">closer look:</span></p>
			<ol>
				<li><strong class="source-inline">FROM alpine:latest</strong>: This line specifies the base image for the Docker image. In this case, it uses the Alpine Linux distribution, which is a lightweight and minimalistic distribution that’s often used in Docker containers. <strong class="source-inline">latest</strong> refers to the latest version of the Alpine image available on <span class="No-Break">Docker Hub.</span></li>
				<li><strong class="source-inline">RUN apk update</strong>: This command updates the package index of the Alpine Linux package manager (<strong class="source-inline">apk</strong>) to ensure it has the latest information about <span class="No-Break">available packages.</span></li>
				<li><strong class="source-inline">RUN apk add py-pip</strong>: Here, it installs the <strong class="source-inline">py-pip</strong> package, which is the package manager for Python packages. This step is necessary to be able to install Python packages <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">pip</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">RUN apk add --no-cache python3-dev</strong>: This installs the <strong class="source-inline">python3-dev</strong> package, which provides development files for Python. These development files are often<a id="_idIndexMarker630"/> needed when compiling or building Python packages that have native <span class="No-Break">code extensions.</span></li>
				<li><strong class="source-inline">RUN pip install --upgrade pip</strong>: This upgrades the <strong class="source-inline">pip</strong> package manager to the <span class="No-Break">latest version.</span></li>
				<li><strong class="source-inline">WORKDIR /app</strong>: This sets the working directory for the subsequent commands to <strong class="source-inline">/app</strong>. This directory is where the application code will be copied, and it becomes the default directory for <span class="No-Break">running commands.</span></li>
				<li><strong class="source-inline">COPY . /app</strong>: This copies the contents of the current directory (where the Dockerfile is located) into the <strong class="source-inline">/app</strong> directory in the Docker image. This typically includes the application code, <span class="No-Break">including </span><span class="No-Break"><strong class="source-inline">requirements.txt</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">RUN pip --no-cache-dir install -r requirements.txt</strong>: This installs Python<a id="_idIndexMarker631"/> dependencies specified in the <strong class="source-inline">requirements.txt</strong> file. The <strong class="source-inline">--no-cache-dir</strong> flag is used to ensure that no cache is used during the installation, which can help reduce the size of the <span class="No-Break">Docker image.</span></li>
				<li><strong class="source-inline">CMD ["python3", "main.py"]</strong>: This specifies the default command to run when a container is started from this image. In this case, it runs the <strong class="source-inline">main.py</strong> Python script using <strong class="source-inline">python3</strong>. This is the command that will be executed when we run a container based on this <span class="No-Break">Docker image.</span></li>
			</ol>
			<p>In <em class="italic">step 8</em>, we need the <strong class="source-inline">requirements.txt</strong> file. In this case, the file does not have to be too complex – it needs to use the same imports as the web <span class="No-Break">service script:</span></p>
			<pre class="source-code">
flask
flask-restful</pre>			<p>Now, we are ready to compile the Docker container. We can do that with the following command from the <span class="No-Break">command line:</span></p>
			<pre class="console">
docker build -t measurementinstrument .</pre>			<p>Once the compilation process is complete, we can start <span class="No-Break">the container:</span></p>
			<pre class="console">
docker run -t -p 5000:5000 measurementinstrument</pre>			<p>The preceding<a id="_idIndexMarker632"/> command tells the Docker environment that we want to start the container, <strong class="source-inline">measurementinstrument</strong>, and map the port of the web service (<strong class="source-inline">5000</strong>) to the same port in <strong class="source-inline">localmachine</strong>. Now, if we navigate to <a id="_idIndexMarker633"/>the address, we can upload the file just like we could when the web service was running without the <span class="No-Break">Docker containers.</span></p>
			<p class="callout-heading">Best practice #77</p>
			<p class="callout">Dockerize your web services for both version control <span class="No-Break">and portability.</span></p>
			<p>Using Docker is one way we can ensure the portability of our web services. Once we package our web service into the container, we can be sure that it will behave the same on every system that is capable of running Docker. This makes our lives much easier even more than using <strong class="source-inline">requirements.txt</strong> files to set up <span class="No-Break">Python environments.</span></p>
			<p>Once we have the container with the measurement instrument, we can package the second web service – with the prediction model – into another web service. The following Dockerfile <span class="No-Break">does this:</span></p>
			<pre class="source-code">
FROM ubuntu:latest
RUN apt update &amp;&amp; apt install python3 python3-pip -y
WORKDIR /app
COPY . /app
RUN pip --no-cache-dir install -q -r requirements.txt
CMD ["python3", "main.py"]</pre>			<p>This Dockerfile sets up an Ubuntu-based environment, installs Python 3 and <strong class="source-inline">pip</strong>, copies your application code into the image, installs Python dependencies from <strong class="source-inline">requirements.txt</strong>, and then runs the Python script as the default command when the container starts. Please note that we use Ubuntu here and not Alpine Linux. This is no accident. There is no scikit-learn package for Alpine Linux, so we need to use Ubuntu (for which that Python package <span class="No-Break">is available):</span></p>
			<ul>
				<li><strong class="source-inline">FROM ubuntu:latest</strong>: This line specifies the base image for the Docker image. In this case, it uses the latest version of the Ubuntu Linux distribution as the base image. <strong class="source-inline">latest</strong> refers to the latest version of the Ubuntu image available on <span class="No-Break">Docker Hub.</span></li>
				<li><strong class="source-inline">RUN apt update &amp;&amp; apt install python3 python3-pip -y</strong>: This command is used to update the package index of the Ubuntu package manager (<strong class="source-inline">apt</strong>) and then install Python 3 and Python 3 <strong class="source-inline">pip</strong>. The <strong class="source-inline">-y</strong> flag is used to automatically answer “yes” to any prompts during the <span class="No-Break">installation process.</span></li>
				<li><strong class="source-inline">WORKDIR /app</strong>: This sets the working directory for the subsequent commands to <strong class="source-inline">/app</strong>. This directory is where the application code will be copied, and it <a id="_idIndexMarker634"/>becomes the default directory for <span class="No-Break">running commands.</span></li>
				<li><strong class="source-inline">COPY . /app</strong>: This<a id="_idIndexMarker635"/> copies the contents of the current directory (where the Dockerfile is located) into the <strong class="source-inline">/app</strong> directory in the <span class="No-Break">Docker image.</span></li>
				<li><strong class="source-inline">RUN pip --no-cache-dir install -q -r requirements.txt</strong>: This installs Python dependencies specified in the <strong class="source-inline">requirements.txt</strong> file using pip. The flags that are used here are <span class="No-Break">as follows:</span><ul><li><strong class="source-inline">--no-cache-dir</strong>: This ensures that no cache is used during the installation, which can help reduce the size of the <span class="No-Break">Docker image</span></li><li><strong class="source-inline">-q</strong>: This flag runs <strong class="source-inline">pip</strong> in quiet mode, meaning it will produce less output, which can make the Docker build process <span class="No-Break">less verbose</span></li></ul></li>
				<li><strong class="source-inline">CMD ["python3", "main.py"]</strong>: This specifies the default command to run when a container is started from this image. In this case, it runs the <strong class="source-inline">main.py</strong> Python script using <strong class="source-inline">python3</strong>. This is the command that will be executed when we run a container based on this <span class="No-Break">Docker image.</span></li>
			</ul>
			<p>In this case, the<a id="_idIndexMarker636"/> requirements code is a bit longer, although not <span class="No-Break">extremely complex:</span></p>
			<pre class="source-code">
scikit-learn
scipy
flask
flask-restful
joblib
pandas
numpy</pre>			<p>We compile<a id="_idIndexMarker637"/> the Docker container with a <span class="No-Break">similar command:</span></p>
			<pre class="console">
docker build -t predictor .</pre>			<p>We execute it with a <span class="No-Break">similar command:</span></p>
			<pre class="console">
docker run -t -p 5001:5000 predictor</pre>			<p>Now, we should be able to use the same browser commands to connect. Please note that we use a different port so that this new web service does not collide with the <span class="No-Break">previous one.</span></p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor195"/>Combining web services into ecosystems</h1>
			<p>Now, let’s <a id="_idIndexMarker638"/>develop the software that will connect these two web services. For this, we’ll create a new file that will send one file to the<a id="_idIndexMarker639"/> first web service, get the data, and then send it to the second web service to <span class="No-Break">make predictions:</span></p>
			<pre class="source-code">
import requests
# URL of the Flask web service for file upload
upload_url = 'http://localhost:5000/success'  # Replace with the actual URL
# URL of the Flask web service for predictions
prediction_url = 'http://localhost:5001/predict/'  # Replace with the actual URL
def upload_file_and_get_metrics(file_path):
    try:
        # Open and read the file
        with open(file_path, 'rb') as file:
            # Create a dictionary to hold the file data
            files = {'file': (file.name, file)}
            # Send a POST request with the file to the upload URL
            response = requests.post(upload_url, files=files)
            response.raise_for_status()
            # Parse the JSON response
            json_result = response.json()
            # Extract LOC and mccabe_complexity from the JSON result
            loc = json_result.get('lines_of_code')
            mccabe_complexity = json_result.get('mccabe_complexity')[0][-1]
            if loc is not None and mccabe_complexity is not None:
                print(f'LOC: [3], McCabe Complexity: {mccabe_complexity}')
                return loc, mccabe_complexity
            else:
                print('LOC or McCabe Complexity not found in JSON result.')
    except Exception as e:
        print(f'Error: {e}')
def send_metrics_for_prediction(loc, mcc):
    try:
        # Create the URL for making predictions
        predict_url = f'{prediction_url}[3]/[4]'
        # Send a GET request to the prediction web service
        response = requests.get(predict_url)
        response.raise_for_status()
        # Parse the JSON response to get the prediction
        prediction = response.json().get('Defect')
        print(f'Prediction: {prediction}')
    except Exception as e:
        print(f'Error: {e}')
if __name__ == '__main__':
    # Specify the file path you want to upload
    file_path = './main.py'  # Replace with the actual file path
    # Upload the specified file and get LOC and McCabe Complexity
    loc, mcc = upload_file_and_get_metrics(file_path)
    send_metrics_for_prediction(loc, mcc)</pre>			<p>This code <a id="_idIndexMarker640"/>demonstrates how to upload a file to a Flask <a id="_idIndexMarker641"/>web service to obtain metrics and then send those metrics to another Flask web service for making predictions. It uses the <strong class="source-inline">requests</strong> library to handle HTTP requests and JSON responses between the <span class="No-Break">two services:</span></p>
			<ul>
				<li><strong class="source-inline">import requests</strong>: This line imports the <strong class="source-inline">requests</strong> library, which is used to send HTTP requests to <span class="No-Break">web services.</span></li>
				<li><strong class="source-inline">upload_url</strong>: This variable stores the URL of the Flask web service for <span class="No-Break">file upload.</span></li>
				<li><strong class="source-inline">prediction_url</strong>: This variable stores the URL of the Flask web service <span class="No-Break">for predictions.</span></li>
				<li><span class="No-Break"><strong class="source-inline">upload_file_and_get_metrics</strong></span><span class="No-Break">:</span><ul><li>This function takes <strong class="source-inline">file_path</strong> as input, which should be the path to the file you want to upload and obtain <span class="No-Break">metrics for</span></li><li>It sends a POST request to <strong class="source-inline">upload_url</strong> to upload the <span class="No-Break">specified file</span></li><li>After uploading the file, it parses the JSON response received from the file <span class="No-Break">upload service</span></li><li>It extracts the <strong class="source-inline">"lines_of_code"</strong> and <strong class="source-inline">"mccabe_complexity"</strong> fields from the <span class="No-Break">JSON response</span></li><li>The extracted metrics are printed, and the function <span class="No-Break">returns them</span></li></ul></li>
				<li><span class="No-Break"><strong class="source-inline">send_metrics_for_prediction</strong></span><span class="No-Break">:</span><ul><li>This<a id="_idIndexMarker642"/> function takes <strong class="source-inline">loc</strong> (lines of code) and <strong class="source-inline">mcc</strong> (McCabe complexity) values <span class="No-Break">as input</span></li><li>It constructs the URL for making predictions by appending the <strong class="source-inline">loc</strong> and <strong class="source-inline">mcc</strong> values <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">prediction_url</strong></span></li><li>It sends a GET request to the prediction service using the <span class="No-Break">constructed URL</span></li><li>After receiving the prediction, it parses the JSON response to obtain the <strong class="source-inline">"</strong><span class="No-Break"><strong class="source-inline">Defect"</strong></span><span class="No-Break"> value</span></li><li>The prediction is printed to <span class="No-Break">the console</span></li></ul></li>
				<li><strong class="source-inline">if __name__ == '__main__'</strong>: This block specifies the file path (<strong class="source-inline">file_path</strong>) that you want to upload and obtain metrics for. It calls the <strong class="source-inline">upload_file_and_get_metrics</strong> function to upload the file and obtain the metrics (lines of code and McCabe complexity). Then, it calls the <strong class="source-inline">send_metrics_for_prediction</strong> function to send these metrics for prediction and prints <span class="No-Break">the prediction.</span></li>
			</ul>
			<p>This <a id="_idIndexMarker643"/>program shows that we can package our model into a web service (with or without a container) and then use it, just like <span class="No-Break"><em class="italic">Figure 16</em></span><em class="italic">.1</em> suggested. This way of designing the entire system allows us to make the software more scalable and robust. Depending on the usage scenario, we can adapt the web services and deploy them on several different servers for scalability and <span class="No-Break">load balancing.</span></p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor196"/>Summary</h1>
			<p>In this chapter, we learned how to deploy ML models using web services and Docker. Although we only deployed two web services, we can see that it can become an ecosystem for ML. By separating predictions and measurements, we can separate the computational-heavy workloads (prediction) and the data collection parts of the pipeline. Since the model can be deployed on any server, we can reuse the servers and therefore reduce the energy consumption of <span class="No-Break">these models.</span></p>
			<p>With that, we have come to was last technical chapter of this book. In the next chapter, we’ll take a look at the newest trends in ML and peer into our crystal ball to predict, or at least guess, <span class="No-Break">the future.</span></p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor197"/>References</h1>
			<ul>
				<li><em class="italic">Masse, M., REST API design rulebook: designing consistent RESTful web service interfaces. 2011: “O’Reilly </em><span class="No-Break"><em class="italic">Media, Inc.”.</em></span></li>
				<li><em class="italic">Raj, P., J.S. Chelladhurai, and V. Singh, Learning Docker. 2015: Packt </em><span class="No-Break"><em class="italic">Publishing Ltd.</em></span></li>
				<li><em class="italic">Staron, M., et al. Robust Machine Learning in Critical Care—Software Engineering and Medical Perspectives. In 2021 IEEE/ACM 1st Workshop on AI Engineering-Software Engineering for AI (WAIN). </em><span class="No-Break"><em class="italic">2021. IEEE.</em></span></li>
				<li><em class="italic">McCabe, T.J., A complexity measure. IEEE Transactions on Software Engineering, 1976(4): </em><span class="No-Break"><em class="italic">p. 308-320.</em></span></li>
			</ul>
		</div>
	</body></html>