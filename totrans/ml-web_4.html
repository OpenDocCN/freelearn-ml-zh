<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Web Mining Techniques">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch04"/>&#13;
 Chapter 4. Web Mining Techniques</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Web data mining techniques are used to explore the data available online and then extract the relevant information from the Internet. Searching on the web is a complex process that requires different algorithms, and they will be the main focus of this chapter. Given a search query, the relevant pages are obtained using the data available on each web page, which is usually divided in the page content and the page hyperlinks to other pages. Usually, a search engine has multiple components:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">A web crawler or spider for collecting web pages</li>&#13;
<li class="listitem">A parser that extracts content and preprocesses web pages</li>&#13;
<li class="listitem">An indexer to organize the web pages in data structures</li>&#13;
<li class="listitem">A retrieval information system to score the most important documents related to a query</li>&#13;
<li class="listitem">A ranking algorithm to order the web pages in a meaningful manner</li>&#13;
</ul>&#13;
</div>&#13;
<p>These parts can be divided into web structure mining techniques and web content mining techniques.</p>&#13;
<p>The web crawler, indexer, and ranking procedures refer to the web structure (the network of hyperlinks). The other parts (parser and retrieval system) of a search engine are web content analysis methods because the text information on web pages is used to perform such operations.</p>&#13;
<p>Furthermore, the content of a collection of web pages can be further analyzed using some natural language processing techniques, such as <span class="strong">&#13;
<strong>latent Dirichlet allocation</strong>&#13;
</span>&#13;
 opinion mining or sentiment analysis tools. These techniques are especially important for extracting subjective information about web users, and so they are widely found in many commercial applications, from marketing to consultancy. These sentiment analysis techniques will be discussed at the end of the chapter. Now we will start discussing the web structure mining category.</p>&#13;
<div class="section" title="Web structure mining">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch04lvl1sec25"/>&#13;
 Web structure mining</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>This <a id="id326" class="indexterm"/>&#13;
 field of web mining focuses on the discovery of the relationships among web pages and how to use this link structure to find the relevance of web pages. For the first task, usually a spider is employed, and the links and the collected web pages are stored in a indexer. For the the last task, the web page ranking evaluates the importance of the web pages.</p>&#13;
<div class="section" title="Web crawlers (or spiders)">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch04lvl2sec25"/>&#13;
 Web crawlers (or spiders)</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>A spider starts <a id="id327" class="indexterm"/>&#13;
 from a set of URLs (seed pages) and then extracts the URL inside<a id="id328" class="indexterm"/>&#13;
 them to fetch more pages. New links are then extracted from the new pages and the process continues until some criteria are matched. The unvisited URLs are stored in a list called <a id="id329" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>frontier</strong>&#13;
</span>&#13;
 , and depending on how the list is used, we can have different crawler algorithms, such as breadth-first and preferential spiders. In the <a id="id330" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>breadth-first</strong>&#13;
</span>&#13;
 algorithm, the next URL to crawl comes from the head of the frontier while the new URLs are appended to the frontier tail. Preferential spider instead employs a certain importance estimate on the list of unvisited URLs to determine which page to crawl first. Note that the extraction of links from a page is performed using a parser, and this operation is discussed in more detail in the related paragraph of the web content mining section.</p>&#13;
<p>A web crawler is essentially a graph search algorithm in which the structure of the neighborhood of the starting pages is retrieved, following certain criteria such as the maximum number of links to follow (depth of the graph), maximum number of pages to crawl, or time limit. A spider can then extract a portion of the Web that has interesting structures, such as hubs and authorities. A hub is a web page that contains a large number of links, while an authority is defined as, a page, with a large number of times that its URL occurs on other web pages (it is a measure of the page's popularity). A popular Python implementation of the crawler is given by the Scrapy library, which also employs concurrency methods (asynchronous programming using Twisted) to speed up operations. A tutorial on this module is given in <a class="link" title="Chapter 7. Movie Recommendation System Web Application" href="text00050.html#page">Chapter 7</a>&#13;
 , <span class="emphasis">&#13;
<em>Movie Recommendation System Web Application</em>&#13;
</span>&#13;
 when the crawler will be used to extract information about movie reviews.</p>&#13;
</div>&#13;
<div class="section" title="Indexer">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch04lvl2sec26"/>&#13;
 Indexer</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>An indexer<a id="id331" class="indexterm"/>&#13;
 is a way to store web pages found by the crawler in a structured database to<a id="id332" class="indexterm"/>&#13;
 allow subsequent fast retrieval on a given search query. The simplest indexing approach is to directly store all the pages and, at query time, just scan for all the documents that contain the keywords in the query. However, this method is not feasible if the number of pages is large (which in practice, it is) due to high computational costs. The most common method to speed up the retrieval is called <a id="id333" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>inverted index scheme</strong>&#13;
</span>&#13;
 , which is used by the most popular search engines.</p>&#13;
<p>Given a set of <a id="id334" class="indexterm"/>&#13;
 web pages <span class="emphasis">&#13;
<em>p<sub>1</sub>&#13;
 , …, p<sub>k</sub>&#13;
</em>&#13;
</span>&#13;
 and a <a id="id335" class="indexterm"/>&#13;
 vocabulary <span class="emphasis">&#13;
<em>V</em>&#13;
</span>&#13;
 containing all the words <span class="inlinemediaobject"><img src="Image00368.jpg" alt="Indexer"/>&#13;
</span>&#13;
 in the pages, the inverted index database is obtained by storing lists such as <span class="inlinemediaobject"><img src="Image00369.jpg" alt="Indexer"/>&#13;
</span>&#13;
 , …, <span class="inlinemediaobject"><img src="Image00370.jpg" alt="Indexer"/>&#13;
</span>&#13;
 ,</p>&#13;
<p>Here, <span class="inlinemediaobject"><img src="Image00371.jpg" alt="Indexer"/>&#13;
</span>&#13;
 is the ID of the web page <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 . Extra information can be stored for each word, for example, the frequency count of the word or its position on each page. The implementation of the indexer is beyond the scope of this book, but the general concepts have been described in this paragraph for completeness.</p>&#13;
<p>Therefore, a search query with a list of words will retrieve all the inverted lists related to each word and then merge the lists. The order of the final lists will be chosen using the ranking algorithm together with an information retrieval system to measure the relevance of the documents to the query.</p>&#13;
</div>&#13;
<div class="section" title="Ranking – PageRank algorithm">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch04lvl2sec27"/>&#13;
 Ranking – PageRank algorithm</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>A ranking algorithm<a id="id336" class="indexterm"/>&#13;
 is<a id="id337" class="indexterm"/>&#13;
 important because the usual number of web <a id="id338" class="indexterm"/>&#13;
 pages that a single information retrieval query can return may be huge, so there is a problem of how to choose the most relevant pages. Furthermore, the information retrieval model can easily be spammed by just inserting many keywords into the page to make the page relevant to a large number of queries. So, the problem to evaluate the importance (that is, ranking score) of a web page on the Internet has been addressed considering the fact that the web has a graph in which the hyperlinks—links from a page to another—are the primary source of information to estimate the relevance of web pages. The hyperlinks can be divided as:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">in-links of page <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 : hyperlinks that point to page <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">out-links of page <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 : hyperlinks that point to other pages from page <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
<p>Intuitively, the more in-links a web page has, the more important the page should be. The study of this hyperlink structure is part of social network analysis, and many algorithms have been used and proposed. But for historical reasons, we will explain the most well known algorithm, called <span class="strong">&#13;
<strong>PageRank</strong>&#13;
</span>&#13;
 , which was presented by Sergey Brin and Larry Page (the founders of Google) in 1998. The whole idea is to calculate the prestige of a page as the sum of the prestiges of the pages that point to it. If the prestige of a page <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 is <span class="emphasis">&#13;
<em>P(j)</em>&#13;
</span>&#13;
 it is equally <a id="id339" class="indexterm"/>&#13;
 distributed to all the pages <span class="emphasis">&#13;
<em>N<sub>j</sub>&#13;
</em>&#13;
</span>&#13;
 that it points to so that each out-link <a id="id340" class="indexterm"/>&#13;
 receives a portion of prestige equal to <span class="emphasis">&#13;
<em>P(j)|N<sub>j</sub>&#13;
</em>&#13;
</span>&#13;
 . Formally, the prestige or page rank score of a page <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 can be defined as:</p>&#13;
<div class="mediaobject"><img src="Image00372.jpg" alt="Ranking – PageRank algorithm"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, <span class="inlinemediaobject"><img src="Image00373.jpg" alt="Ranking – PageRank algorithm"/>&#13;
</span>&#13;
 if page <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 points to page <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 ; otherwise it is equal to <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 . <span class="emphasis">&#13;
<em>A<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 is called adjacency matrix and it represents the portion of prestige that propagates from node <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 to node <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 . Considering <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 total nodes in the graph, the preceding equation can be rewritten in matrix form:</p>&#13;
<div class="mediaobject"><img src="Image00374.jpg" alt="Ranking – PageRank algorithm"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Note that this equation is equivalent to an eigensystem with eigenvalue <span class="inlinemediaobject"><img src="Image00375.jpg" alt="Ranking – PageRank algorithm"/>&#13;
</span>&#13;
 if the adjacency matrix satisfies certain conditions. Another way to interpret the preceding equation is to use the Markov chain terminology—the entry <span class="emphasis">&#13;
<em>A<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 becomes the transition probability from node <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 to node <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 and the prestige of node <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>p(i)</em>&#13;
</span>&#13;
 , is the probability to visit node <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 . In this scenario, it may happen that two nodes (or more) point to each other but do not point to other pages. Once one of these two nodes has been visited, a loop will occur and the user will be trapped in it. This situation is called <a id="id341" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>rank sink</strong>&#13;
</span>&#13;
 , (the matrix <span class="emphasis">&#13;
<em>A</em>&#13;
</span>&#13;
 is called <a id="id342" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>periodic</strong>&#13;
</span>&#13;
 ) and the solution is to add a transition matrix term that allows jumping from each page to another page at random without following the Markov chain described by <span class="emphasis">&#13;
<em>A</em>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00376.jpg" alt="Ranking – PageRank algorithm"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, <span class="emphasis">&#13;
<em>E=ee<sup>T</sup>&#13;
</em>&#13;
</span>&#13;
 is a matrix of one entry of dimensions <span class="emphasis">&#13;
<em>N´N</em>&#13;
</span>&#13;
 (<span class="emphasis">&#13;
<em>e</em>&#13;
</span>&#13;
 is a unit vector), and <span class="emphasis">&#13;
<em>d</em>&#13;
</span>&#13;
 (also called the<a id="id343" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>damping factor</strong>&#13;
</span>&#13;
 ) is the probability to follow the transition given by the transition matrix <span class="emphasis">&#13;
<em>A</em>&#13;
</span>&#13;
 . <span class="emphasis">&#13;
<em>(1-d)</em>&#13;
</span>&#13;
 is the probability to visit a page randomly. In this final form, all <a id="id344" class="indexterm"/>&#13;
 the nodes are linked to each other so that even if the <a id="id345" class="indexterm"/>&#13;
 adjacency matrix has a row with many <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 entries for a particular node <span class="emphasis">&#13;
<em>s</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>A<sub>sj</sub>&#13;
</em>&#13;
</span>&#13;
 , there is always a small probability equal to <span class="inlinemediaobject"><img src="Image00377.jpg" alt="Ranking – PageRank algorithm"/>&#13;
</span>&#13;
 that <span class="emphasis">&#13;
<em>s</em>&#13;
</span>&#13;
 is visited from all the <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 nodes in the graph. Note that <span class="emphasis">&#13;
<em>A</em>&#13;
</span>&#13;
 has to be stochastic, which means each row has to sum to <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 ; <span class="inlinemediaobject"><img src="Image00378.jpg" alt="Ranking – PageRank algorithm"/>&#13;
</span>&#13;
 (at least one entry per row different from <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 or at least one out-link per page). The equation can be simplified by normalizing the <span class="emphasis">&#13;
<em>P</em>&#13;
</span>&#13;
 vector as <span class="emphasis">&#13;
<em>e<sup>T</sup>&#13;
 P=N</em>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00379.jpg" alt="Ranking – PageRank algorithm"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>This can be solved using the power iteration method. This algorithm will be used in <a class="link" title="Chapter 8. Sentiment Analyser Application for Movie Reviews" href="text00059.html#ch08">Chapter 8</a>&#13;
 , <span class="emphasis">&#13;
<em>Sentiment Analyser Application on Movie Reviews</em>&#13;
</span>&#13;
 to implement an example of a movie review sentiment analysis system. The main advantages of this algorithm is that it does not depend on the query (so the PageRank scores can be computed offline and retrieved at query time), and it is very robust to spamming since it is not feasible for a spammer to insert in-links to their page on influential pages.</p>&#13;
</div>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Web content mining">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch04lvl1sec26"/>&#13;
 Web content mining</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>This type of mining focuses <a id="id346" class="indexterm"/>&#13;
 on extracting information from the content of web pages. Each page is usually gathered and organized (using a parsing technique), processed to remove the unimportant parts from the text (natural language processing), and then analyzed using an information retrieval system to match the relevant documents to a given query. These three components are discussed in the following paragraphs.</p>&#13;
<div class="section" title="Parsing">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch04lvl2sec28"/>&#13;
 Parsing</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>A web page is <a id="id347" class="indexterm"/>&#13;
 written in HTML format, so the first <a id="id348" class="indexterm"/>&#13;
 operation is to extract the relevant pieces of information. An HTML parser builds a tree of tags from which the content can be extracted. Nowadays, there are many parsers available, but as an example, we use the Scrapy library see <a class="link" title="Chapter 7. Movie Recommendation System Web Application" href="text00050.html#page">Chapter 7</a>&#13;
 , <span class="emphasis">&#13;
<em>Movie Recommendation System Web Application</em>&#13;
</span>&#13;
 which provides a command-line parser. Let's say we want to parse the main page of Wikipedia, <a class="ulink" href="https://en.wikipedia.org/wiki/Main_Page">https://en.wikipedia.org/wiki/Main_Page</a>&#13;
 . We simply type this in a terminal:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>scrapy shell 'https://en.wikipedia.org/wiki/Main_Page' </strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>A prompt will be ready to parse the page using the <code class="literal">response</code>&#13;
 object and the <code class="literal">xpath</code>&#13;
 language. For example we want to obtain the title's page:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>In [1]: response.xpath('//title/text()').extract()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Out[1]: [u'Wikipedia, the free encyclopedia']</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>Or we want to extract all the embedded links in page (this operation is needed for the crawler to work), which are usually put on <code class="literal">&lt;a&gt;</code>&#13;
 , and the URL value is on an <code class="literal">href</code>&#13;
 attribute:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>In [2]: response.xpath("//a/@href").extract()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Out[2]:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>[u'#mw-head',</strong>&#13;

</span>&#13;


<span class="strong">
<strong> u'#p-search',</strong>&#13;

</span>&#13;


<span class="strong">
<strong> u'/wiki/Wikipedia',</strong>&#13;

</span>&#13;


<span class="strong">
<strong> u'/wiki/Free_content',</strong>&#13;

</span>&#13;


<span class="strong">
<strong> u'/wiki/Encyclopedia',</strong>&#13;

</span>&#13;


<span class="strong">
<strong> u'/wiki/Wikipedia:Introduction',</strong>&#13;

</span>&#13;


<span class="strong">
<strong>…</strong>&#13;

</span>&#13;


<span class="strong">
<strong> u'//wikimediafoundation.org/',</strong>&#13;

</span>&#13;


<span class="strong">
<strong> u'//www.mediawiki.org/']</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>Note that a more robust way to parse content can be used since the web pages are usually written by non-programmers, so the HTML may contain syntax errors that browsers typically repair. Note also that web pages may contain a large amount of data due to advertisements, making the parsing of relevant information complicated. Different algorithms have been proposed (for instance, tree matching) to identify the main content of a page but no Python libraries are available at the moment, so we have decided not to discuss this topic further. However, note that a nice parsing implementation for extracting the body of a web article can be found in the newspaper library and it will also be used in <a class="link" title="Chapter 7. Movie Recommendation System Web Application" href="text00050.html#page">Chapter 7</a>&#13;
 , <span class="emphasis">&#13;
<em>Movie Recommendation System Web Application</em>&#13;
</span>&#13;
 .</p>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Natural language processing">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch04lvl1sec27"/>&#13;
 Natural language processing</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Once the text content of a web page <a id="id349" class="indexterm"/>&#13;
 has been extracted, the text data is usually preprocessed to remove parts that do not bring any relevant information. A text is tokenized, that is, transformed into a list of words (tokens), and all the punctuation marks are removed. Another usual operation is to remove all the <span class="emphasis">&#13;
<em>stopwords</em>&#13;
</span>&#13;
 , that is, all the words used to construct the syntax of a sentence but not containing text information (such as conjunctions, articles, and prepositions) such as <span class="emphasis">&#13;
<em>a</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>about</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>an</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>are</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>as</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>at</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>be</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>by</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>for</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>from</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>how</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>in</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>is</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>of</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>on</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>or</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>that</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>the</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>these</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>this</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>to</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>was</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>what</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>when</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>where</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>who</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>will</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>with</em>&#13;
</span>&#13;
 , and many others.</p>&#13;
<p>Many words in English (or any language) share the same root but have different suffixes or prefixes. For example, the words <span class="emphasis">&#13;
<em>think</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>thinking</em>&#13;
</span>&#13;
 , and <span class="emphasis">&#13;
<em>thinker</em>&#13;
</span>&#13;
 all share the same root—<span class="emphasis">&#13;
<em>think</em>&#13;
</span>&#13;
 indicating that the meaning is the same—but the role in a sentence is different (verb, noun, and so on). The procedure to reduce all the words in a set to its roots it is called <a id="id350" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>stemming</strong>&#13;
</span>&#13;
 , and many algorithms have been invented to do so (Porter, Snowball, and Lancaster). All of these techniques are parts of a broader range of algorithms called <a id="id351" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>natural language processing</strong>&#13;
</span>&#13;
 , and they are implemented in Python on the <code class="literal">nltk</code>&#13;
 library (installed as usual through <code class="literal">sudo pip install nltk</code>&#13;
 ). As an example, the following code preprocesses a sample text using the techniques described previously (using the Python interface terminal):</p>&#13;
<div class="mediaobject"><img src="Image00380.jpg" alt="Natural language processing"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Note that the <code class="literal">stopwords</code>&#13;
 list has<a id="id352" class="indexterm"/>&#13;
 been downloaded using the <code class="literal">nltk dowloader nltk.download('stopwords')</code>&#13;
 .</p>&#13;
<div class="section" title="Information retrieval models">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch04lvl2sec29"/>&#13;
 Information retrieval models</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>The<a id="id353" class="indexterm"/>&#13;
 information retrieval methods are needed to find the most relevant documents to a given query. The words contained in the web pages can be modeled using different approaches such as Boolean models, vector space models, and probabilistic models, and in this book, we have decided to discuss the vector space models and how to implement them. Formally, given a vocabulary of <span class="emphasis">&#13;
<em>V</em>&#13;
</span>&#13;
 words, each web page <span class="emphasis">&#13;
<em>d<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 (or document) in a collection of <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 pages, can be thought of as a vector of words, <span class="inlinemediaobject"><img src="Image00381.jpg" alt="Information retrieval models"/>&#13;
</span>&#13;
 , where each word <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 belonging to the document <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 is represented by <span class="emphasis">&#13;
<em>w<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 , which can be either a number (weight) or a vector depending on the chosen algorithm:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">Term frequency-inverse document frequency (TF-IDF), <span class="emphasis">&#13;
<em>w<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 , is a real number</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>Latent Semantic Analysis</strong>&#13;
</span>&#13;
 (<span class="strong">&#13;
<strong>LSA</strong>&#13;
</span>&#13;
 ), <span class="emphasis">&#13;
<em>w<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 , is a <a id="id354" class="indexterm"/>&#13;
 real number (representation independent of the document <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 )</li>&#13;
<li class="listitem">Doc2Vec (or word2vec), <span class="emphasis">&#13;
<em>w<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 , is a vector of real numbers (representation independent of the document <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 )</li>&#13;
</ul>&#13;
</div>&#13;
<p>Since the query can also be <a id="id355" class="indexterm"/>&#13;
 represented by a vector of words, <span class="inlinemediaobject"><img src="Image00382.jpg" alt="Information retrieval models"/>&#13;
</span>&#13;
 , the web pages most similar to the vector <span class="emphasis">&#13;
<em>q</em>&#13;
</span>&#13;
 are found by calculating a similarity measure between the query vector and each document. The most used similarity measure is called cosine similarity, for any document <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 given by:</p>&#13;
<div class="mediaobject"><img src="Image00383.jpg" alt="Information retrieval models"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Note that there are other measures used in literature (okapi and pivoted normalization weighting), but for the purpose of this book, they are not necessary.</p>&#13;
<p>The following sections will give some details about the three methods before being applied in a text case in the final paragraph of the section.</p>&#13;
<div class="section" title="TF-IDF">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch04lvl3sec19"/>&#13;
 TF-IDF</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>This method calculates <span class="emphasis">&#13;
<em>w<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 , taking<a id="id356" class="indexterm"/>&#13;
 into account the fact that a word that appears many times and in a <a id="id357" class="indexterm"/>&#13;
 large number of pages is likely to be less important than a word that occurs many times but only in a subset of documents. It is given by the multiplication of two factors:</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00384.jpg" alt="TF-IDF"/>&#13;
</span>&#13;
 where:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00385.jpg" alt="TF-IDF"/>&#13;
</span>&#13;
 is the normalized frequency of the word <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 in the document <span class="emphasis">&#13;
<em>I</em>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00386.jpg" alt="TF-IDF"/>&#13;
</span>&#13;
 is the inverse<a id="id358" class="indexterm"/>&#13;
 document frequency and <span class="emphasis">&#13;
<em>df<sub>j</sub>&#13;
</em>&#13;
</span>&#13;
 is the number of web<a id="id359" class="indexterm"/>&#13;
 pages that contain the word <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div>&#13;
<div class="section" title="Latent Semantic Analysis (LSA)">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch04lvl3sec20"/>&#13;
 Latent Semantic Analysis (LSA)</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>The name of this algorithm<a id="id360" class="indexterm"/>&#13;
 comes from the idea that there is a latent<a id="id361" class="indexterm"/>&#13;
 space in which each word (and each document) can be efficiently described, assuming that words with similar meanings also occur in similar text positions. Projection on this subspace is performed using the (truncated) SVD method already discussed in <a class="link" title="Chapter 2. Unsupervised Machine Learning" href="text00020.html#ch02">Chapter 2</a>&#13;
 , <span class="emphasis">&#13;
<em>Machine Learning Techniques – Unsupervised Learning</em>&#13;
</span>&#13;
 . We contextualize the method for LSA as follows: the web pages are collected together in matrix <span class="emphasis">&#13;
<em>X (V ´N</em>&#13;
</span>&#13;
 ), in which each column is a document:</p>&#13;
<div class="mediaobject"><img src="Image00387.jpg" alt="Latent Semantic Analysis (LSA)"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, <span class="emphasis">&#13;
<em>U<sub>t</sub>&#13;
</em>&#13;
</span>&#13;
 (<span class="emphasis">&#13;
<em>V ´d</em>&#13;
</span>&#13;
 ) is the matrix of the words projected in the new latent space with <span class="emphasis">&#13;
<em>d</em>&#13;
</span>&#13;
 dimensions, <span class="inlinemediaobject"><img src="Image00388.jpg" alt="Latent Semantic Analysis (LSA)"/>&#13;
</span>&#13;
 (<span class="emphasis">&#13;
<em>d</em>&#13;
</span>&#13;
 ´<span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 ) is the transpose matrix of the documents transformed into the subspace, and <span class="inlinemediaobject"><img src="Image00389.jpg" alt="Latent Semantic Analysis (LSA)"/>&#13;
</span>&#13;
 (<span class="emphasis">&#13;
<em>d</em>&#13;
</span>&#13;
 ´<span class="emphasis">&#13;
<em>d</em>&#13;
</span>&#13;
 ) is the diagonal matrix with singular values. The query vector itself is projected into the latent space by:</p>&#13;
<div class="mediaobject"><img src="Image00390.jpg" alt="Latent Semantic Analysis (LSA)"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Now, each document represented by each row of <span class="emphasis">&#13;
<em>V<sub>t</sub>&#13;
</em>&#13;
</span>&#13;
 can be compared with <span class="emphasis">&#13;
<em>q<sub>t</sub>&#13;
</em>&#13;
</span>&#13;
 using the cosine similarity. Note that the true mathematical representation of the documents on the latent space is given by <span class="inlinemediaobject"><img src="Image00391.jpg" alt="Latent Semantic Analysis (LSA)"/>&#13;
</span>&#13;
 (not <span class="emphasis">&#13;
<em>V<sub>t</sub>&#13;
</em>&#13;
</span>&#13;
 ) because the singular values are the scaling factors of the space axis components and they must be taken into account. Therefore, this matrix should be compared with <span class="inlinemediaobject"><img src="Image00392.jpg" alt="Latent Semantic Analysis (LSA)"/>&#13;
</span>&#13;
 . Nevertheless, it usually computes the similarity between <span class="emphasis">&#13;
<em>V<sub>t</sub>&#13;
</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>q<sub>t</sub>&#13;
</em>&#13;
</span>&#13;
 , and in practice, it is <a id="id362" class="indexterm"/>&#13;
 still unknown<a id="id363" class="indexterm"/>&#13;
 which method returns the best results.</p>&#13;
</div>&#13;
<div class="section" title="Doc2Vec (word2vec)">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch04lvl3sec21"/>&#13;
 Doc2Vec (word2vec)</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>This method represents each word <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>w<sub>j</sub>&#13;
</em>&#13;
</span>&#13;
 , as a vector <span class="inlinemediaobject"><img src="Image00393.jpg" alt="Doc2Vec (word2vec)"/>&#13;
</span>&#13;
 but independent of the document <span class="emphasis">&#13;
<em>d<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 it occurs in. Doc2Vec <a id="id364" class="indexterm"/>&#13;
 is an extension of the word2vec algorithm originally proposed<a id="id365" class="indexterm"/>&#13;
 by Mikolov and others, and it employs neuron networks and backpropagation to generate the word (and document) vectors. Due to the increasing importance of neuron networks (especially deep learning) in many machine learning applications, we decided to include the main concepts and formulas of this quite advanced method here to give you an introduction to a subject that will become extremely important in the future of machine learning in various fields. The following description is based on the paper Rong (2014) and Le and Mikolov (2014), and the notation also reflects the name currently used in literature.</p>&#13;
</div>&#13;
<div class="section" title="Word2vec – continuous bag of words and skip-gram architectures">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch04lvl3sec22"/>&#13;
 Word2vec – continuous bag of words and skip-gram architectures</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Each word <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 in the vocabulary <span class="emphasis">&#13;
<em>V</em>&#13;
</span>&#13;
 is<a id="id366" class="indexterm"/>&#13;
 represented by a vector of length <span class="emphasis">&#13;
<em>|V|</em>&#13;
</span>&#13;
 , with binary entries <span class="emphasis">&#13;
<em>x<sub>j</sub>&#13;
 =(x<sub>1j</sub>&#13;
 , …, x<sub>Vj</sub>&#13;
 )</em>&#13;
</span>&#13;
 , where only <span class="emphasis">&#13;
<em>x<sub>jj</sub>&#13;
 =1</em>&#13;
</span>&#13;
 ; otherwise, <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 . The word2vec method trains a <a id="id367" class="indexterm"/>&#13;
 single (hidden) layer of <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 neurons (weights), choosing between two different network architectures (shown in the following figure). Note that both architectures have only one layer of <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 neurons or weights, <span class="emphasis">&#13;
<em>h</em>&#13;
</span>&#13;
 . This means that the method has to be considered <span class="emphasis">&#13;
<em>shallow</em>&#13;
</span>&#13;
 learning and not deep, which typically refers to networks with many hidden layers. The <a id="id368" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>Continuous Bag Of Words</strong>&#13;
</span>&#13;
 (<span class="strong">&#13;
<strong>CBOW</strong>&#13;
</span>&#13;
 ) method (displayed to the right in the following figure) trains the model using a set of <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 words as an input called <span class="emphasis">&#13;
<em>context</em>&#13;
</span>&#13;
 trying to predict the word (target) that occurs adjacent to the input text. The reverse approach is called <span class="strong">&#13;
<strong>Skip-gram</strong>&#13;
</span>&#13;
 , <a id="id369" class="indexterm"/>&#13;
 in which the input is the target word and the network is trained to predict the context set (displayed to the left in the following figure ). Note that <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 is called the window parameter and it sets how far<a id="id370" class="indexterm"/>&#13;
 from the target word the context words are selected:</p>&#13;
<div class="mediaobject"><img src="Image00394.jpg" alt="Word2vec – continuous bag of words and skip-gram architectures"/>&#13;
<div class="caption">&#13;
<p>Skip-gram (left) and CBOW (right) architectures of the word2vec algorithm; figures taken from word2vec Parameter Learning Explained by X Rong (2015)</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>In both cases, the matrix <span class="emphasis">&#13;
<em>W</em>&#13;
</span>&#13;
 transforms<a id="id371" class="indexterm"/>&#13;
 the input vectors into the hidden layer and <span class="emphasis">&#13;
<em>W'</em>&#13;
</span>&#13;
 transforms from the hidden layer to the output layer <span class="emphasis">&#13;
<em>y</em>&#13;
</span>&#13;
 , where the target (or context) is evaluated. In the training phase, the error from the true target (or context) is computed and used to calculate a stochastic gradient descent to update both the matrices <span class="emphasis">&#13;
<em>W</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>W'</em>&#13;
</span>&#13;
 . We will give a more mathematical description of the CBOW method in the following section. Note that the Skip-gram equations are similar and we will refer to the Rong (2015) paper for further details.</p>&#13;
</div>&#13;
<div class="section" title="Mathematical description of the CBOW model">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch04lvl3sec23"/>&#13;
 Mathematical description of the CBOW model</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Starting from the <a id="id372" class="indexterm"/>&#13;
 input layer, the hidden layer <span class="emphasis">&#13;
<em>h</em>&#13;
</span>&#13;
 can be obtained by computing, <span class="inlinemediaobject"><img src="Image00395.jpg" alt="Mathematical description of the CBOW model"/>&#13;
</span>&#13;
 , where <span class="inlinemediaobject"><img src="Image00396.jpg" alt="Mathematical description of the CBOW model"/>&#13;
</span>&#13;
 is a vector of length <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 that represents the word <span class="emphasis">&#13;
<em>w<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 on the hidden layer and <span class="emphasis">&#13;
<em>w<sub>C</sub>&#13;
</em>&#13;
</span>&#13;
 is the average of the <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 context vectors <span class="inlinemediaobject"><img src="Image00396.jpg" alt="Mathematical description of the CBOW model"/>&#13;
</span>&#13;
 . Choosing a target word <span class="emphasis">&#13;
<em>w<sub>j</sub>&#13;
</em>&#13;
</span>&#13;
 , the score at the output layer <span class="emphasis">&#13;
<em>u<sub>j</sub>&#13;
</em>&#13;
</span>&#13;
 is obtained by multiplying the vector <span class="inlinemediaobject"><img src="Image00397.jpg" alt="Mathematical description of the CBOW model"/>&#13;
</span>&#13;
 (the j-th column of <span class="emphasis">&#13;
<em>W'</em>&#13;
</span>&#13;
 ) by <span class="emphasis">&#13;
<em>h</em>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00398.jpg" alt="Mathematical description of the CBOW model"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>This is not the final value <a id="id373" class="indexterm"/>&#13;
 on the output layer <span class="emphasis">&#13;
<em>y<sub>j</sub>&#13;
</em>&#13;
</span>&#13;
 because we want to evaluate the posterior conditional probability to have the target word <span class="emphasis">&#13;
<em>w<sub>j</sub>&#13;
</em>&#13;
</span>&#13;
 given the context <span class="emphasis">&#13;
<em>C</em>&#13;
</span>&#13;
 expressed by the <a id="id374" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>softmax</strong>&#13;
</span>&#13;
 formula:</p>&#13;
<div class="mediaobject"><img src="Image00399.jpg" alt="Mathematical description of the CBOW model"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Now the training objective is to maximize this probability for all the words in the vocabulary, which is equivalent to <span class="inlinemediaobject"><img src="Image00400.jpg" alt="Mathematical description of the CBOW model"/>&#13;
</span>&#13;
 , where <span class="inlinemediaobject"><img src="Image00401.jpg" alt="Mathematical description of the CBOW model"/>&#13;
</span>&#13;
 and the index <span class="emphasis">&#13;
<em>j<sup>M</sup>&#13;
</em>&#13;
</span>&#13;
 represents the vector of <span class="emphasis">&#13;
<em>W'</em>&#13;
</span>&#13;
 in which the product is maximum, that is, the most probable target word.</p>&#13;
<p>The stochastic gradient descent equations are then obtained by calculating the derivatives of <span class="emphasis">&#13;
<em>E</em>&#13;
</span>&#13;
 with respect to the entries of <span class="emphasis">&#13;
<em>W (w<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 ) and <span class="emphasis">&#13;
<em>W' (w'<sub>ij'</sub>&#13;
</em>&#13;
</span>&#13;
 ). The final equations for each output target word <span class="emphasis">&#13;
<em>w<sub>j</sub>&#13;
</em>&#13;
</span>&#13;
 are:</p>&#13;
<div class="mediaobject"><img src="Image00402.jpg" alt="Mathematical description of the CBOW model"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00403.jpg" alt="Mathematical description of the CBOW model"/>&#13;
</span>&#13;
 where <span class="inlinemediaobject"><img src="Image00404.jpg" alt="Mathematical description of the CBOW model"/>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>a</em>&#13;
</span>&#13;
 is the learning rate of the gradient <a id="id375" class="indexterm"/>&#13;
 descent. The derivative <span class="inlinemediaobject"><img src="Image00405.jpg" alt="Mathematical description of the CBOW model"/>&#13;
</span>&#13;
 represents the error of the network with respect to the true target word so that the error is back propagated on the system, which can learn iteratively. Note that the vectors <span class="inlinemediaobject"><img src="Image00406.jpg" alt="Mathematical description of the CBOW model"/>&#13;
</span>&#13;
 are the usual word representations used to perform the semantic operations.</p>&#13;
<p>Further details can be found in the Rong (2015) paper.</p>&#13;
</div>&#13;
<div class="section" title="Doc2Vec extension">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch04lvl3sec24"/>&#13;
 Doc2Vec extension</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>As explained in <a id="id376" class="indexterm"/>&#13;
 Le and Mikolov (2014), Doc2Vec is a natural extension of the word2vec method in which a document is considered as an additional word vector. So in the case of the CBOW architecture, the hidden layer vector <span class="emphasis">&#13;
<em>h</em>&#13;
</span>&#13;
 is just the average of the context vectors and the document vector <span class="emphasis">&#13;
<em>d<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00407.jpg" alt="Doc2Vec extension"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>This architecture is shown in the following figure, and it is called the <a id="id377" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>distributed memory model</strong>&#13;
</span>&#13;
 (<span class="strong">&#13;
<strong>DM</strong>&#13;
</span>&#13;
 ) because the document <span class="emphasis">&#13;
<em>d<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 vector just remembers the information of the document not represented by the context words. The vector <span class="inlinemediaobject"><img src="Image00408.jpg" alt="Doc2Vec extension"/>&#13;
</span>&#13;
 is shared with all the context words sampled from the document <span class="emphasis">&#13;
<em>d<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 but the matrix <span class="emphasis">&#13;
<em>W</em>&#13;
</span>&#13;
 (and <span class="emphasis">&#13;
<em>W'</em>&#13;
</span>&#13;
 ) is the same for all the documents:</p>&#13;
<div class="mediaobject"><img src="Image00409.jpg" alt="Doc2Vec extension"/>&#13;
<div class="caption">&#13;
<p>A distributed memory model example with a context of three words (window=3); figure taken from Distributed Representations of Sentences and Documents by Le and Mikolov (2014)</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The other proposed<a id="id378" class="indexterm"/>&#13;
 architecture is called <a id="id379" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>distributed bag of words</strong>&#13;
</span>&#13;
 (<span class="strong">&#13;
<strong>DBOW</strong>&#13;
</span>&#13;
 ), which considers only a document vector in the input layer and a set of context words sampled from the document in the output layer. It has been shown that the DM architecture performs better than DBOW, and it is therefore the default model in the <code class="literal">gensim</code>&#13;
 library implementation. The reader is advised to read the paper of Le and Mikolov (2014) for further details.</p>&#13;
</div>&#13;
<div class="section" title="Movie review query example">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch04lvl3sec25"/>&#13;
 Movie review query example</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>To show in action the three <a id="id380" class="indexterm"/>&#13;
 information retrieval methods discussed previously, we use the IMBD movie reviews in the <span class="emphasis">&#13;
<em>polarity dataset v2.0</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>Pool of 27886 unprocessed html files</em>&#13;
</span>&#13;
 at <a class="ulink" href="http://www.cs.cornell.edu/people/pabo/movie-review-data/">http://www.cs.cornell.edu/people/pabo/movie-review-data/</a>&#13;
 , provided by Bo Pang and Lillian Lee (the dataset and the code are also stored in the GitHub account of the author at <a class="ulink" href="https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/">https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/</a>&#13;
 . Download and unzip the <code class="literal">movie.zip</code>&#13;
 file from the website (called <code class="literal">polarity_html.zip</code>&#13;
 ), which creates the <code class="literal">movie</code>&#13;
 folder with all the web page movie reviews (about 2000 files). First of all, we need to prepare the data from the files:</p>&#13;
<div class="mediaobject"><img src="Image00410.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>This time we use<a id="id381" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>BeautifulSoup</strong>&#13;
</span>&#13;
 to<a id="id382" class="indexterm"/>&#13;
 parse the title of the movie from each HTML web page and create a dictionary, <code class="literal">moviedict</code>&#13;
 . The <code class="literal">polarity dataset v2.0.tar.gz</code>&#13;
 contains a folder, <code class="literal">review_polarity</code>&#13;
 , which is inside the <code class="literal">txt_sentoken/</code>&#13;
 folder that split the positive and negative reviews into two separate subfolders (pros and cons). These files are preprocessed using the following code:</p>&#13;
<div class="mediaobject"><img src="Image00411.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Now all the 2,000 reviews are stored in the <code class="literal">tot_textreviews</code>&#13;
 list and the corresponding titles in <code class="literal">tot_titles</code>&#13;
 . The TF-IDF model can be trained using <code class="literal">sklearn</code>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00412.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>After the <code class="literal">PreprocessTfidf</code>&#13;
 function, apply all the preprocessing techniques (removing stop words, tokenizing, and stemming) to each document. In the same way, we can train the LSA model using the <code class="literal">gensim</code>&#13;
 library, specifying 10 latent dimensions:</p>&#13;
<div class="mediaobject"><img src="Image00413.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Note that the <code class="literal">GenSimCorpus</code>&#13;
 function <a id="id383" class="indexterm"/>&#13;
 just preprocesses the documents with the usual techniques and transforms them into a format that the gensim LSA implementation can read. From the <code class="literal">lsi</code>&#13;
 object, it is possible to obtain the matrices <span class="emphasis">&#13;
<em>U</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>V</em>&#13;
</span>&#13;
 , and S that are needed to transform the query into the latent space:</p>&#13;
<div class="mediaobject"><img src="Image00414.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Also the indexed dictionary <a id="id384" class="indexterm"/>&#13;
 of words, <code class="literal">dict_words</code>&#13;
 , has been calculated to transform a query word into the corresponding index word in <code class="literal">dict_corpus</code>&#13;
 .</p>&#13;
<p>The last model to train is Doc2Vec. First, we prepare the data in a format that the gensim Doc2Vec implementation can handle:</p>&#13;
<div class="mediaobject"><img src="Image00415.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Each review has been placed in a <code class="literal">namedtuple</code>&#13;
 object, which contains the words preprocessed by the <code class="literal">PreprocessDoc2Vec</code>&#13;
 function (stopwords removed and tokenization performed) and the tag that is the name of the file. Note that we chose not to apply a stemmer because the results are generally better without it (the reader can test the results by applying the stemmer, setting the Boolean flag <code class="literal">doc2vecstem</code>&#13;
 to <code class="literal">True</code>&#13;
 ). The Doc2Vec training is finally performed by the following code:</p>&#13;
<div class="mediaobject"><img src="Image00416.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>We set the <a id="id385" class="indexterm"/>&#13;
 DM architecture (<code class="literal">dm</code>&#13;
 =1), the hidden layer with 500 dimensions (<code class="literal">size</code>&#13;
 ), a window size of 10 words, and all the words that occur at least once have been taken into account by the model (<code class="literal">min_count</code>&#13;
 =1).The other parameters are related to the efficiency optimization method (<code class="literal">negative</code>&#13;
 for negative sampling and <code class="literal">hs</code>&#13;
 for hierarchical softmax). The training lasted for <code class="literal">20</code>&#13;
 epochs, with a learning rate equal to <code class="literal">0.99</code>&#13;
 .</p>&#13;
<p>We can now verify which results each method returns, defining a query to retrieve all the web documents related to sci-fi movies, that is, movies usually described by this list of words:</p>&#13;
<div class="mediaobject"><img src="Image00417.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The TF-IDF method returns the five most similar web pages using the following script:</p>&#13;
<div class="mediaobject"><img src="Image00418.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Note that the model uses a sparse matrix format to store data, so the <code class="literal">cosine_similarity</code>&#13;
 function converts the vectors into regular vectors. Then it computes the similarity. In a similar way, the query is converted in a <span class="emphasis">&#13;
<em>q<sub>k</sub>&#13;
</em>&#13;
</span>&#13;
 in LSA terminology and the five most similar web pages are printed out:</p>&#13;
<div class="mediaobject"><img src="Image00419.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Finally, the <code class="literal">doc2vec</code>&#13;
 model transforms the query list into a vector using the <code class="literal">infer_vector</code>&#13;
 function, and the most similar reviews are returned by the <code class="literal">most_similar</code>&#13;
 function:</p>&#13;
<div class="mediaobject"><img src="Image00420.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Note that the <code class="literal">random</code>&#13;
 parameter <a id="id386" class="indexterm"/>&#13;
 of the model needs to be set up to a fixed value to return deterministic results whenever an optimization approach is used (negative sampling or hierarchical softmax). The results are as follows:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>TF-IDF</strong>&#13;
</span>&#13;
 :<div class="mediaobject"><img src="Image00421.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>LSA</strong>&#13;
</span>&#13;
 :<div class="mediaobject"><img src="Image00422.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</li>&#13;
<li class="listitem">Doc2vec:<div class="mediaobject"><img src="Image00423.jpg" alt="Movie review query example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
<p>All three methods show<a id="id387" class="indexterm"/>&#13;
 movies related to the query. Interestingly, TF-IDF performs better than the more advanced LSA and Doc2Vec algorithms because <span class="emphasis">&#13;
<em>In the Heat of the Night</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>Pokemon</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>Rocky Horror Picture Show</em>&#13;
</span>&#13;
 , and <span class="emphasis">&#13;
<em>Wild Things</em>&#13;
</span>&#13;
 are not related to the query compared with the TF-IDF results which show only one movie (<span class="emphasis">&#13;
<em>No Telling</em>&#13;
</span>&#13;
 ) as unrelated. The movies <span class="emphasis">&#13;
<em>Charlie's Angels</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>Batman &amp; Robin</em>&#13;
</span>&#13;
 are action movies, so they are mostly related to the single query word <span class="emphasis">&#13;
<em>action</em>&#13;
</span>&#13;
 . Doc2Vec returns the worst results mostly because the training dataset is too small to learn good vector representations (as an example, Google released a word2vec trained dataset based on billions of documents, or more). The website <a class="ulink" href="http://www.cs.cornell.edu/people/pabo/movie-review-data/">http://www.cs.cornell.edu/people/pabo/movie-review-data/</a>&#13;
 provides a larger dataset, so the reader can try to train Doc2Vec with more data as an exercise.</p>&#13;
</div>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Postprocessing information">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch04lvl1sec28"/>&#13;
 Postprocessing information</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Once the <a id="id388" class="indexterm"/>&#13;
 web pages are collected from the Web, there are natural language processing algorithms that are able to extract relevant information for different commercial purposes apart from building a web search engine. We will discuss here algorithms that are able to extract the main topics on the collection of documents (latent Dirichlet analysis) and to extract the sentiment or opinion of each web page (opinion mining techniques).</p>&#13;
<div class="section" title="Latent Dirichlet allocation">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch04lvl2sec30"/>&#13;
 Latent Dirichlet allocation</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>&#13;
<span class="strong">&#13;
<strong>Latent Dirichlet allocation</strong>&#13;
</span>&#13;
 (<span class="strong">&#13;
<strong>LDA</strong>&#13;
</span>&#13;
 )<a id="id389" class="indexterm"/>&#13;
 is a natural language processing algorithm<a id="id390" class="indexterm"/>&#13;
 that belongs to the generative model category. The technique is based on the observations of some variables that can be explained by other underlined unobserved variables, which are the reasons the observed data is similar or different.</p>&#13;
<p>For example, consider text documents in which words are the observations. Each document can be the result of a mixture of topics (unobserved variables) and each word refers to a specific topic.</p>&#13;
<p>For instance, consider the two following documents describing two companies:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>doc1</strong>&#13;
</span>&#13;
 : Changing how people search for fashion items and, share and buy fashion via visual recognition, TRUELIFE is going to become the best approach to search the ultimate trends …</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>doc2</strong>&#13;
</span>&#13;
 : Cinema4you enabling any venue to be a cinema is a new digital filmed media distribution company currently in the testing phase. It applies technologies used in Video on Demand and broadcasting to ...</li>&#13;
</ul>&#13;
</div>&#13;
<p>LDA is a way of automatically discovering latent topics that these documents contain. For example, given these documents and asked for two topics, LDA might return the following words associated with each topic:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>topic 1</strong>&#13;
</span>&#13;
 : people Video fashion media…</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>topic 2</strong>&#13;
</span>&#13;
 : Cinema technologies recognition broadcasting…</li>&#13;
</ul>&#13;
</div>&#13;
<p>Therefore, the second topic can be labeled as <span class="emphasis">&#13;
<em>technology</em>&#13;
</span>&#13;
 while the first as <span class="emphasis">&#13;
<em>business</em>&#13;
</span>&#13;
 .</p>&#13;
<p>Documents are then represented as mixtures of topics that spit out words with certain probabilities:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>doc1</strong>&#13;
</span>&#13;
 : topic 1 42%,topic 2 64%</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>doc2</strong>&#13;
</span>&#13;
 : topic 1 21%, topic 2 79%</li>&#13;
</ul>&#13;
</div>&#13;
<p>This representation of the documents can be useful in various applications such as clustering of pages in different groups, or to extract the main common subjects of a collection of pages. The mathematical model behind this algorithm is explained in the next paragraph.</p>&#13;
<div class="section" title="Model">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch04lvl3sec26"/>&#13;
 Model</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Documents are represented<a id="id391" class="indexterm"/>&#13;
 as random mixtures over latent topics, where each topic is characterized by a distribution over words. LDA assumes the following process for a corpus consisting of <span class="emphasis">&#13;
<em>M</em>&#13;
</span>&#13;
 documents, <span class="emphasis">&#13;
<em>d=(d<sub>1</sub>&#13;
 , …, d<sub>M</sub>&#13;
 )</em>&#13;
</span>&#13;
 , with each <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 containing <span class="emphasis">&#13;
<em>N<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 words. If <span class="emphasis">&#13;
<em>V</em>&#13;
</span>&#13;
 is the length of the vocabulary, a word of document <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 is represented by a vector <span class="emphasis">&#13;
<em>w<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 of length <span class="emphasis">&#13;
<em>V</em>&#13;
</span>&#13;
 , where only an element <span class="emphasis">&#13;
<em>w<sub>i</sub>&#13;
 <sub>v</sub>&#13;
 =1</em>&#13;
</span>&#13;
 and the others are <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00424.jpg" alt="Model"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The number of latent dimensions (topics) is <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 , and for each document, <span class="inlinemediaobject"><img src="Image00425.jpg" alt="Model"/>&#13;
</span>&#13;
 is the vector of topics associated with each word <span class="emphasis">&#13;
<em>w<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 , where <span class="emphasis">&#13;
<em>z<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 is a vector of <span class="emphasis">&#13;
<em>0's</em>&#13;
</span>&#13;
 of length <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 except for the element <span class="emphasis">&#13;
<em>j, z<sub>i</sub>&#13;
 <sup>j</sup>&#13;
 =1</em>&#13;
</span>&#13;
 , that represents the topic <span class="emphasis">&#13;
<em>w<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 has been drawn from.</p>&#13;
<p>b indicates the <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 ´<span class="emphasis">&#13;
<em>V</em>&#13;
</span>&#13;
 <a id="id392" class="indexterm"/>&#13;
 matrix, where <span class="emphasis">&#13;
<em>b<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 represents the probability that each word <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 in the vocabulary is drawn from topic <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 : <span class="inlinemediaobject"><img src="Image00426.jpg" alt="Model"/>&#13;
</span>&#13;
 .</p>&#13;
<p>So, each row <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 of b is the word's distribution of topic <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 , while each column <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 is the topic's distribution of word <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 . Using these definitions, the process is described as follows:</p>&#13;
<div class="orderedlist">&#13;
<ol class="orderedlist arabic">&#13;
<li class="listitem" value="1">From a chosen distribution (usually Poisson), draw the length of each document <span class="emphasis">&#13;
<em>N<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 .</li>&#13;
<li class="listitem" value="2">For each document <span class="emphasis">&#13;
<em>d<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 , draw the topic distribution q<span class="emphasis">&#13;
<em>&#13;
<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 , as a Dirichlet distribution <span class="emphasis">&#13;
<em>Dir(a)</em>&#13;
</span>&#13;
 , where <span class="inlinemediaobject"><img src="Image00427.jpg" alt="Model"/>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>a</em>&#13;
</span>&#13;
 is a parameter vector of length <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 such that <span class="inlinemediaobject"><img src="Image00428.jpg" alt="Model"/>&#13;
</span>&#13;
 .</li>&#13;
<li class="listitem" value="3">For each document <span class="emphasis">&#13;
<em>d<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 , for each word <span class="emphasis">&#13;
<em>n</em>&#13;
</span>&#13;
 , draw a topic from the multinomial <span class="inlinemediaobject"><img src="Image00429.jpg" alt="Model"/>&#13;
</span>&#13;
 .</li>&#13;
<li class="listitem" value="4">For each document <span class="emphasis">&#13;
<em>d<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 , for each word <span class="emphasis">&#13;
<em>n</em>&#13;
</span>&#13;
 , and for each topic <span class="emphasis">&#13;
<em>z<sub>n</sub>&#13;
</em>&#13;
</span>&#13;
 , draw a word <span class="emphasis">&#13;
<em>w<sub>n</sub>&#13;
</em>&#13;
</span>&#13;
 from a multinomial given by the row <span class="emphasis">&#13;
<em>z<sub>n</sub>&#13;
</em>&#13;
</span>&#13;
 of b, <span class="inlinemediaobject"><img src="Image00430.jpg" alt="Model"/>&#13;
</span>&#13;
 .</li>&#13;
</ol>&#13;
<div style="height:10px; width: 1px"/>&#13;
</div>&#13;
<p>The objective of the algorithm is to maximize the posterior probability for each document:</p>&#13;
<div class="mediaobject"><img src="Image00431.jpg" alt="Model"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Applying the conditional<a id="id393" class="indexterm"/>&#13;
 probability definition, the numerator becomes the following:</p>&#13;
<div class="mediaobject"><img src="Image00432.jpg" alt="Model"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>So, the probability that the document <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 is given by topic vector <span class="emphasis">&#13;
<em>z</em>&#13;
</span>&#13;
 and word probability matrix b can be expressed as a multiplication of the single word probabilities:</p>&#13;
<div class="mediaobject"><img src="Image00433.jpg" alt="Model"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Considering that <span class="emphasis">&#13;
<em>z<sub>n</sub>&#13;
</em>&#13;
</span>&#13;
 is a vector with only one component <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 different from <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>z<sup>j</sup>&#13;
 <sub>n</sub>&#13;
 =1</em>&#13;
</span>&#13;
 , then <span class="inlinemediaobject"><img src="Image00434.jpg" alt="Model"/>&#13;
</span>&#13;
 . Substituting these expressions on (2):</p>&#13;
<div class="mediaobject"><img src="Image00435.jpg" alt="Model"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The denominator of (1) is obtained simply by integration over q<span class="emphasis">&#13;
<em>&#13;
<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 and summation over <span class="emphasis">&#13;
<em>z</em>&#13;
</span>&#13;
 . The final values of the topic distribution <span class="emphasis">&#13;
<em>q<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 and the words per topic distribution (rows of <span class="emphasis">&#13;
<em>b</em>&#13;
</span>&#13;
 ) are obtained by calculating this probability by approximated inference techniques; those are beyond the scope of this book.</p>&#13;
<p>The parameter <span class="emphasis">&#13;
<em>a</em>&#13;
</span>&#13;
 is called the <a id="id394" class="indexterm"/>&#13;
 concentration parameter, and it indicates how much the distribution is <a id="id395" class="indexterm"/>&#13;
 spread over the possible values. A concentration parameter of <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 (or <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 , the dimension of the Dirichlet distribution, by the definition used in topic modeling literature) results in all sets of probabilities being equally probable. Meanwhile, in the limit as the concentration parameter tends towards zero, only distributions with nearly the entire mass concentrated on one of their components are likely (the words are less shared among different topics and they concentrate on a few topics).</p>&#13;
<p>As an example, a 100,000-dimension categorical distribution has a vocabulary of 100,000 words even though a topic may be represented by a couple of hundred words. As a consequence, typical values for the concentration parameter are between 0.01 and 0.001, or lower if the vocabulary's size is millions of words or higher.</p>&#13;
<p>According to L. Li and Y. Zhang's paper <span class="emphasis">&#13;
<em>An empirical study of text classification using Latent Dirichlet Allocation</em>&#13;
</span>&#13;
 , LDA can be used as an effective dimension reduction method for text modeling. However, even though the method has performed well in various applications, there are certain issues to consider. The initialization of the model is random, which means it can lead to different results in each run. Also, the choice of concentration parameters is important, but there is no standard method to choose them.</p>&#13;
</div>&#13;
<div class="section" title="Example">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch04lvl3sec27"/>&#13;
 Example</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Consider again the <a id="id396" class="indexterm"/>&#13;
 movie reviews' web pages, <code class="literal">textreviews</code>&#13;
 , already preprocessed in the <span class="emphasis">&#13;
<em>Movie review query example</em>&#13;
</span>&#13;
 section, and LDA is applied to test whether it is possible to gather reviews on different topics. As usual, the following code is available in <code class="literal">postprocessing.ipynb</code>&#13;
 at <a class="ulink" href="https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/">https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/</a>&#13;
 :</p>&#13;
<div class="mediaobject"><img src="Image00436.jpg" alt="Example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>As usual we have<a id="id397" class="indexterm"/>&#13;
 transformed each document in tokens (a different tokenizer has been used) and the stop words have been removed. To achieve better results, we filter out the most frequent words (such as <code class="literal">movie</code>&#13;
 and <code class="literal">film</code>&#13;
 ) that do not add any information to the pages. We ignore all the words with more than 1,000 occurrences or observed less than three times:</p>&#13;
<div class="mediaobject"><img src="Image00437.jpg" alt="Example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Now we can train the LDA model with 10 topics (<code class="literal">passes</code>&#13;
 is the number of training passes through the corpus):</p>&#13;
<div class="mediaobject"><img src="Image00438.jpg" alt="Example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The code returns the following 10 most probable words associated with each topic:</p>&#13;
<div class="mediaobject"><img src="Image00439.jpg" alt="Example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Although not all the topics<a id="id398" class="indexterm"/>&#13;
 have an easy interpretation, we can definitely see that topic 2 is associated with the words <code class="literal">disney</code>&#13;
 , <code class="literal">mulan</code>&#13;
 (a Disney movie), <code class="literal">love</code>&#13;
 , and <code class="literal">life</code>&#13;
 is a topic about animation movies, topic 6 is associated with the words <code class="literal">action</code>&#13;
 , <code class="literal">alien</code>&#13;
 , <code class="literal">bad</code>&#13;
 , and <code class="literal">planet</code>&#13;
 is related to fantasy sci-fi movies. In fact, we can query all the movies with most probable topic equal to 6 like this:</p>&#13;
<div class="mediaobject"><img src="Image00440.jpg" alt="Example"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>This will return:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>Rock Star (2001)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Star Wars: Episode I - The Phantom Menace (1999)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Zoolander (2001)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Star Wars: Episode I - The Phantom Menace (1999)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Matrix, The (1999)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Volcano (1997)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Return of the Jedi (1983)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Daylight (1996)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Blues Brothers 2000 (1998)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Alien&amp;#179; (1992)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Fallen (1998)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Planet of the Apes (2001)</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>Most of these titles are clearly sci-fi and fantasy movies, so the LDA algorithm clusters them correctly.</p>&#13;
<p>Note that with the <a id="id399" class="indexterm"/>&#13;
 documents' representations in the topic space (<code class="literal">lda_lfq[corpus]</code>&#13;
 ), it would be possible to apply a cluster algorithm (see <a class="link" title="Chapter 2. Unsupervised Machine Learning" href="text00020.html#ch02">Chapter 2</a>&#13;
 , <span class="emphasis">&#13;
<em>Machine Learning Techniques – Unsupervised Learning</em>&#13;
</span>&#13;
 ) but this is left to the reader as an exercise. Note also that each time the LDA algorithm is run, it may lead to different results due to the random initialization of the model (that is, it's normal if your results are different from what it is shown in this paragraph).</p>&#13;
</div>&#13;
</div>&#13;
<div class="section" title="Opinion mining (sentiment analysis)">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch04lvl2sec31"/>&#13;
 Opinion mining (sentiment analysis)</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Opinion mining or <a id="id400" class="indexterm"/>&#13;
 sentiment analysis <a id="id401" class="indexterm"/>&#13;
 is the field of study of text to extract the opinion of the writer, which can usually be positive or negative (or neutral). This analysis is particularly useful especially in marketing to find the public opinion on products or services. The standard approach is to consider the sentiment (or polarity), negative or positive, as the target of a classification problem. A dataset of documents will have as many features as the number of different words contained in the vocabulary, and classification algorithms such as SVM and Naive Bayes are typically used. As an example, we consider the 2,000 movie reviews already used for testing LDA and information retrieval models that are already labeled (positive or negative). All of the code discussed in this paragraph is available on the <code class="literal">postprocessing.ipynb</code>&#13;
 IPython notebook at <a class="ulink" href="https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/">https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/</a>&#13;
 . As before, we import the data and preprocess:</p>&#13;
<div class="mediaobject"><img src="Image00441.jpg" alt="Opinion mining (sentiment analysis)"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The data is then split into<a id="id402" class="indexterm"/>&#13;
 a training set (80%) and a test set (20%) in a way the <code class="literal">nltk</code>&#13;
 library can process (a list of tuples each or <a id="id403" class="indexterm"/>&#13;
 those with a dictionary containing the document words and the label):</p>&#13;
<div class="mediaobject"><img src="Image00442.jpg" alt="Opinion mining (sentiment analysis)"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Now we can train and test a <code class="literal">NaiveBayesClassifier</code>&#13;
 (multinomial) using the <code class="literal">nltk</code>&#13;
 library and check the error:</p>&#13;
<div class="mediaobject"><img src="Image00443.jpg" alt="Opinion mining (sentiment analysis)"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The code returns an <a id="id404" class="indexterm"/>&#13;
 error of 28.25%, but it <a id="id405" class="indexterm"/>&#13;
 is possible to improve the result by computing the best bigrams in each document. A bigram is defined as a pair of consecutive words, and the <span class="emphasis">&#13;
<em>X<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 test is used to find bigrams that do not occur by chance but with a larger frequency. These particular bigrams contain relevant information for the text and are called collocations in natural language processing terminology. For example, given a bigram of two words, <span class="strong">&#13;
<strong>w1</strong>&#13;
</span>&#13;
 and <span class="strong">&#13;
<strong>w2</strong>&#13;
</span>&#13;
 , in our corpus with a total number of N possible bigrams, under the null hypothesis that <span class="strong">&#13;
<strong>w1</strong>&#13;
</span>&#13;
 and <span class="strong">&#13;
<strong>w2</strong>&#13;
</span>&#13;
 occur independently to each other, we can fill a two-dimensional matrix <span class="emphasis">&#13;
<em>O</em>&#13;
</span>&#13;
 by collecting the occurrences of the bigram (<span class="strong">&#13;
<strong>w1</strong>&#13;
</span>&#13;
 , <span class="strong">&#13;
<strong>w2</strong>&#13;
</span>&#13;
 ) and the rest of the possible bigrams, such as these:</p>&#13;
<div class="informaltable">&#13;
<table border="1">&#13;
<colgroup><col/>&#13;
<col/>&#13;
<col/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th valign="bottom"> </th>&#13;
<th valign="bottom">&#13;
<p>w1</p>&#13;
</th>&#13;
<th valign="bottom">&#13;
<p>Not w1</p>&#13;
</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td valign="top">&#13;
<p>&#13;
<span class="strong">&#13;
<strong>w2</strong>&#13;
</span>&#13;
</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>10</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>901</p>&#13;
</td>&#13;
</tr>&#13;
<tr>&#13;
<td valign="top">&#13;
<p>&#13;
<span class="strong">&#13;
<strong>Not w2</strong>&#13;
</span>&#13;
</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>345</p>&#13;
</td>&#13;
<td valign="top">&#13;
<p>1,111,111</p>&#13;
</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
</div>&#13;
<p>The <span class="emphasis">&#13;
<em>X<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 measure is then given by <span class="inlinemediaobject"><img src="Image00444.jpg" alt="Opinion mining (sentiment analysis)"/>&#13;
</span>&#13;
 , where <span class="emphasis">&#13;
<em>O<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 is the number of occurrences of the bigram given by the words <span class="emphasis">&#13;
<em>(i, j)</em>&#13;
</span>&#13;
 (so that <span class="emphasis">&#13;
<em>O<sub>00</sub>&#13;
 =10</em>&#13;
</span>&#13;
 and so on) and <span class="emphasis">&#13;
<em>E<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 is the expected frequency of the bigram <span class="emphasis">&#13;
<em>(i, j)</em>&#13;
</span>&#13;
 (for example, <span class="inlinemediaobject"><img src="Image00445.jpg" alt="Opinion mining (sentiment analysis)"/>&#13;
</span>&#13;
 ). Intuitively, <span class="emphasis">&#13;
<em>X<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 is higher the more the observed frequency <span class="emphasis">&#13;
<em>O<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 differs from the expected mean <span class="emphasis">&#13;
<em>E<sub>ij</sub>&#13;
</em>&#13;
</span>&#13;
 , so the null hypothesis is likely to be rejected. The bigram is a good collocation and it contains <span class="emphasis">&#13;
<em>more information</em>&#13;
</span>&#13;
 than a bigram that follows the expected means. It can be shown that the <span class="emphasis">&#13;
<em>X<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 can be calculated as the f test (also called <span class="strong">&#13;
<strong>mean square contingency coefficient</strong>&#13;
</span>&#13;
 ) multiplied by the total number of bigram occurrences <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 , as follows:</p>&#13;
<div class="mediaobject"><img src="Image00446.jpg" alt="Opinion mining (sentiment analysis)"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>More information about the collocations and the <span class="emphasis">&#13;
<em>X<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 methods can be found in <span class="emphasis">&#13;
<em>Foundations of Statistical Natural Language Processing</em>&#13;
</span>&#13;
 by C. D. Manning and H. Schuetze (1999). Note also that the <span class="emphasis">&#13;
<em>X<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 , as the <a id="id406" class="indexterm"/>&#13;
 information gain measure (not discussed here), can be thought of as a feature <a id="id407" class="indexterm"/>&#13;
 selection method as defined in <a class="link" title="Chapter 3. Supervised Machine Learning" href="text00024.html#page">Chapter 3</a>&#13;
 , <span class="emphasis">&#13;
<em>Supervised Machine Learning</em>&#13;
</span>&#13;
 . Using the <code class="literal">nltk</code>&#13;
 library, we can use the <span class="emphasis">&#13;
<em>X<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 measure to select the 500 best bigrams per document and then train a Naive Bayes classifier again, as follows:</p>&#13;
<div class="mediaobject"><img src="Image00447.jpg" alt="Opinion mining (sentiment analysis)"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>This time the error rate is 20%, which is lower than in the normal method. The <span class="emphasis">&#13;
<em>X<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 test can also be used to extract the most informative words from the whole corpus. We can measure how much the single word frequency differs from the frequency of the positive (or negative) documents to score its importance (for example, if the word <code class="literal">great</code>&#13;
 has a high <span class="emphasis">&#13;
<em>X<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 value on positive reviews but low on negative reviews, it means that the word gives information that the review is positive). The 10,000 most significant words of the corpus can be extracted <a id="id408" class="indexterm"/>&#13;
 by calculating for <a id="id409" class="indexterm"/>&#13;
 each of them, the the overall frequency on the entire corpus and the frequencies over the positive and negative subsets:</p>&#13;
<div class="mediaobject"><img src="Image00448.jpg" alt="Opinion mining (sentiment analysis)"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Now we can simply train a Naive Bayes classifier again using only the words in the <code class="literal">bestwords</code>&#13;
 set for each document:</p>&#13;
<div class="mediaobject"><img src="Image00449.jpg" alt="Opinion mining (sentiment analysis)"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The error rate is 12.75%, which<a id="id410" class="indexterm"/>&#13;
 is remarkably<a id="id411" class="indexterm"/>&#13;
 low considering the relatively small dataset. Note that to have a more reliable result, a cross-validation method (see <a class="link" title="Chapter 3. Supervised Machine Learning" href="text00024.html#page">Chapter 3</a>&#13;
 , <span class="emphasis">&#13;
<em>Supervised Machine Learning</em>&#13;
</span>&#13;
 ) should be applied, but this is given to the reader as an exercise. Also note that the Doc2Vec vectors (compute in the <span class="emphasis">&#13;
<em>Movie review query example</em>&#13;
</span>&#13;
 section) can be used to train a classifier. Assuming that the Doc2Vec vectors have already been trained and stored in the <code class="literal">model_d2v.doc2vec</code>&#13;
 object, as usual we split the data into a training dataset (80%) and a test set (20%):</p>&#13;
<div class="mediaobject"><img src="Image00450.jpg" alt="Opinion mining (sentiment analysis)"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Then we can train an SVM classifier (<span class="strong">&#13;
<strong>radial basis function kernel</strong>&#13;
</span>&#13;
 (<span class="strong">&#13;
<strong>RBF</strong>&#13;
</span>&#13;
 ) kernel) or a logistic regression model:</p>&#13;
<div class="mediaobject"><img src="Image00451.jpg" alt="Opinion mining (sentiment analysis)"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Logistic regression and<a id="id412" class="indexterm"/>&#13;
 SVM give very<a id="id413" class="indexterm"/>&#13;
 low accuracies, of <code class="literal">0.5172</code>&#13;
 and <code class="literal">0.5225</code>&#13;
 respectively. This is mostly due to the small size of the training dataset, which does not allow us to train algorithms that have a large number of parameters to train, such as neuron networks.</p>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Summary">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch04lvl1sec29"/>&#13;
 Summary</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>In this chapter both the most common and advanced algorithms used to manage web data were discussed and implemented using a series of Python libraries. Now you should have a clear understanding of the challenges faced in the web mining area and should be able to handle some of these issues with Python. In the next chapter, we will discuss the most important recommendation systems algorithms used to date in the commercial environment.</p>&#13;
</div>&#13;

<h1>读累了记得休息一会哦~</h1>
<p> </p>
<p><strong>公众号：古德猫宁李</strong></p>
<ul>
<li>电子书搜索下载</li>
<li>书单分享</li>
<li>书友学习交流</li>

<p> </p>
</ul>
<p><strong>网站：</strong><a href="https://www.chenjin5.com">沉金书屋 https://www.chenjin5.com</a></p>
<ul>
<li>电子书搜索下载</li>
<li>电子书打包资源分享</li>
<li>学习资源分享</li>

</ul>
</body></html>