- en: Real-Time Machine Learning Using Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will extend our deployment of machine learning models beyond
    batch processing in order to learn from data, make predictions, and identify trends
    in real time! We will develop and deploy a real-time stream processing and machine
    learning application comprised of the following high-level technologies:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka producer application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Kafka consumer application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark's Structured Streaming engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark's machine learning library, `MLlib`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed streaming platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far in this book, we have been performing batch processing—that is, we have
    been provided with bounded raw data files and processed that data as a group.
    As we saw in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The Big
    Data Ecosystem*, stream processing differs from batch processing in the fact that
    data is processed as and when individual units, or streams, of data arrive. We
    also saw in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The Big
    Data Ecosystem*, how **Apache Kafka**, as a distributed *streaming platform*,
    allows us to move real-time data between systems and applications in a fault-tolerant
    and reliable manner via a logical streaming architecture comprising of the following
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Producers**: Applications that generate and send messages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumers**: Applications that subscribe to and consume messages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topics**: Streams of records belonging to a particular category and stored
    as a sequence of ordered and immutable records partitioned and replicated across
    a distributed cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stream processors**: Applications that process messages in a certain manner,
    such as data transformations and machine learning models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A simplified illustration of this logical streaming architecture is shown in *Figure
    8.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92b5f057-8b17-4e0f-a931-9f5cb7851f82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Apache Kafka logical streaming architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Distributed stream processing engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Kafka allows us to *move* real-time data reliably between systems and
    applications. But we still need some sort of processing engine to process and
    transform that real-time data in order ultimately to derive value from it based
    on the use case in question. Fortunately, there are a number of *stream processing
    engines* available to allow us to do this, including—but not limited—to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Spark:** [https://spark.apache.org/](https://spark.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Storm:** [http://storm.apache.org/](http://storm.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Flink: **[https://flink.apache.org/](https://flink.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Samza:** [http://samza.apache.org/](http://samza.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Kafka (via its Streams API):** [https://kafka.apache.org/documentation/](https://kafka.apache.org/documentation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KSQL:** [https://www.confluent.io/product/ksql/](https://www.confluent.io/product/ksql/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though a detailed comparison of the available stream processing engines is beyond
    the scope of this book, you are encouraged to explore the preceding links and
    study the differing architectures available. For the purposes of this chapter,
    we will be using Apache Spark's Structured Streaming engine as our stream processing
    engine of choice.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming using Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the time of writing, there are two stream processing APIs available in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark Streaming (DStreams): **[https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structured Streaming:** [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming (DStreams)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Spark Streaming (DStreams)* extends the core Spark API and works by dividing
    real-time data streams into *input batches* that are then processed by Spark''s
    core API, resulting in a final stream of *processed batches*, as illustrated in
    *Figure 8.2*. A sequence of RDDs form what is known as a *discretized stream*
    (or DStream), which represents the continuous stream of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfd910ee-aea3-4890-92e1-10daa23bb5c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Spark Streaming (DStreams)'
  prefs: []
  type: TYPE_NORMAL
- en: Structured Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Structured Streaming*, on the other hand, is a newer and highly optimized
    stream processing engine built on the Spark SQL engine in which streaming data
    can be stored and processed using Spark''s Dataset/DataFrame API (see [Chapter
    1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The Big Data Ecosystem*). As
    of Spark 2.3, Structured Streaming offers the ability to process data streams
    using both micro-batch processing, with latencies as low as 100 milliseconds,
    and *continuous processing*, with latencies as low as 1 millisecond (thereby providing
    *true* real-time processing). Structured Streaming works by modelling data streams
    as an unbounded table that is being continuously appended. When a transformation
    or other type of query is processed on this unbounded table, a results table will
    be generated that is representative of that moment in time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After a configurable trigger interval, new data in the data stream is modeled
    as new rows appended to this unbounded table and the results table is subsequently
    updated, as illustrated in *Figure* *8.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b60d2e4-726c-40ca-ab4f-a886e3e85b3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Spark Structured Streaming logical model'
  prefs: []
  type: TYPE_NORMAL
- en: As streaming data is exposed via the Dataset/DataFrame API, both SQL-like operations
    (including aggregations and joins) and RDD operations (including map and filtering)
    can easily be executed on real-time streams of data. Furthermore, Structured Streaming
    offers features that are designed to cater for data that arrives late, the management
    and monitoring of streaming queries, and the ability to recover from failures.
    As such, Structured Streaming is an extremely versatile, efficient, and reliable
    way to process streaming data with extremely low latencies, and is the stream
    processing engine that we will use for the remainder of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In general, it is advised that developers use this newer and highly optimized
    engine over Spark Streaming (DStreams). However, since it is a newer API, there
    may be certain features that are not yet available as of Spark 2.3.2, which will
    mean the continued occasional usage of the DStreams RDD-based approach while the
    newer API is being developed.
  prefs: []
  type: TYPE_NORMAL
- en: Stream processing pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will develop an end-to-end stream processing pipeline that
    is capable of streaming data from a source system that generates continuous data,
    and thereafter able to publish those streams to an Apache Kafka distributed cluster.
    Our stream processing pipeline will then use Apache Spark to both consume data
    from Apache Kafka, using its Structured Streaming engine, and apply trained machine
    learning models to these streams in order to derive insights in real time using
    `MLlib`. The end-to-end stream processing pipeline that we will develop is illustrated
    in *Figure 8.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79b87bac-111a-4861-8808-9654b33c059c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Our end-to-end stream processing pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: Case study – real-time sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the case study for this chapter, we will extend the sentiment analysis model
    that we developed in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural
    Language Processing Using Apache Spark*, to operate in real time. In [Chapter
    6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural Language Processing Using
    Apache Spark*, we trained a decision tree classifier to predict and classify the
    underlying sentiment of tweets based on a training dataset of historic tweets
    about airlines. In this chapter, we will apply this trained decision tree classifier
    to real-time tweets in order to predict their sentiment and identify negative
    tweets so that airlines may act on them as soon as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our end-to-end stream processing pipeline can therefore be extended, as illustrated
    in *Figure 8.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07bd36e1-f7f5-4696-9710-517f5f63d164.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: Our end-to-end stream processing pipeline for real-time sentiment
    analysis'
  prefs: []
  type: TYPE_NORMAL
- en: 'The core stages of our stream processing pipeline for real-time sentiment analysis
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kafka producer:** We will develop a Python application, using the `pykafka`
    (an Apache Kafka client for Python) and `tweepy` (a Python library for accessing
    the Twitter API) libraries that we installed in [Chapter 2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml), *Setting
    Up a Local Development Environment*, to capture tweets about airlines that are
    being tweeted in real time and to then publish those tweets to an Apache Kafka
    topic called `twitter`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Kafka consumer:** We will then develop a Spark application, using its Structured
    Streaming API, to subscribe to and then consume tweets from the `twitter` topic
    into a Spark dataframe.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Stream processor and** `MLlib`**:** We will then preprocess the raw textual
    content of the tweets stored in this Spark dataframe using the same pipeline of
    feature transformers and feature extractors that we studied and developed in [Chapter
    6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural Language Processing Using
    Apache Spark*, namely tokenization, removing stop words, stemming, and normalization—before
    applying the HashingTF transformer to generate feature vectors in real time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Trained decision tree classifier:** Next, we will load the decision tree
    classifier that we trained in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural
    Language Processing Using Apache Spark,* and persisted to the local filesystem
    of our single development node. Once loaded, we will apply this trained decision
    tree classifier to the Spark dataframe containing our preprocessed feature vectors
    derived from real time tweets in order to predict and classify their underlying
    sentiment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Output sink:** Finally, we will output the results of our sentiment analysis
    model applied to real-time tweets to a target destination, called an output *sink*.
    In our case, the output sink will be the *console* sink, one of the built-in output
    sinks provided natively by the Structured Streaming API. By using this sink, the
    output is printed to the console/**standard output** (**stdout***)* every time
    there is a trigger. From this console, we will be able to read both the raw textual
    content of the original tweets and the predicted sentiment classification from
    our model, namely negative or non-negative. To learn more about the various output
    sinks available, please visit [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following subsections describe the technical steps that we will follow to
    develop, deploy, and run our end-to-end stream processing pipeline for real-time
    sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Note that for the purposes of this case study, we will not be using Jupyter
    notebooks for development. This is because separate code files are required for
    the separate components, as described previously. This case study therefore provides
    another glimpse into how a production-grade pipeline should be developed and executed.
    Rather than instantiating a `SparkContext` explicitly within a notebook, we will
    instead submit our Python code files and all dependencies to `spark-submit` via
    the Linux command line.
  prefs: []
  type: TYPE_NORMAL
- en: Start Zookeeper and Kafka Servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step is to ensure that our single-node Kafka cluster is up and running.
    As described in [Chapter 2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml), *Setting
    Up a Local Development Environment*, please execute the following commands to
    start Apache Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Kafka topic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we need to create a Kafka topic to which our Python Kafka producer application
    (which we will develop later on) will publish real-time tweets about airlines.
    In our case, we will call the topic `twitter`. As demonstrated in [Chapter 2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml),* Setting
    Up a Local Development Environment*, this can be achieved as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Twitter developer account
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order for our Python Kafka producer application to capture tweets in real
    time, we require access to the Twitter API. As of July 2018, a Twitter *developer
    account*, in addition to a normal Twitter account, must be created and approved
    in order to access its API. In order to apply for a developer account, please
    go to [https://apps.twitter.com/](https://apps.twitter.com/), click on the Apply
    for a Developer Account button, and fill in the required details.
  prefs: []
  type: TYPE_NORMAL
- en: Twitter apps and the Twitter API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you have created your Twitter developer account, in order to use the Twitter
    API, a Twitter *app* must be created. A Twitter app provides authenticated and
    authorized access to the Twitter API based on the specific purpose of the app
    that you intend to create. In order to create a Twitter app for the purposes of
    our real-time sentiment analysis model, please go through the following instructions
    (valid at the time of writing):'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to [https://developer.twitter.com/en/apps](https://developer.twitter.com/en/apps).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the Create an App button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Provide the following mandatory app details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Click the Create button to create your Twitter app.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once your Twitter app has been created, navigate to the Keys and Tokens tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a note of your Consumer API Key and Consumer API Secret Key strings respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then click the Create button under Access Token & Access Token Secret to generate
    access tokens for your Twitter app. Set the access level to Read-only as this
    Twitter app will only read tweets, and will not generate any of its own.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a note of the resulting **Access Token** and Access Token Secret strings
    respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The consumer API keys and access tokens will be used to provision our Python-based
    Kafka producer application read-only access to the stream of real-time tweets
    via the Twitter API, so it is important that you make a note of them.
  prefs: []
  type: TYPE_NORMAL
- en: Application configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to start developing our end-to-end stream processing pipeline!
    First, let''s create a configuration file in Python that will store all environmental
    and application-level options pertinent to our pipeline and local development
    node, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The following Python configuration file, called `config.py`, can be found in
    the GitHub repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This Python configuration file defines the following pertinent options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bootstrap_servers`: A comma-delimited list of the hostname/IP address and
    port number pairings for the Kafka brokers. In our case, this is just the hostname/IP
    address of our single-node development environment at port `9092` by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`consumer_api_key`: Enter the consumer API key associated with your Twitter
    app here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`consumer_api_secret`: Enter the consumer API secret key associated with your
    Twitter app here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`access_token`: Enter the access token associated with your Twitter app here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`access_token_secret`: Enter the access token secret associated with your Twitter
    app here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`twitter_kafka_topic_name`: The name of the Kafka topic to which our Kafka
    producer will publish tweets and from which our Structured Streaming Spark application
    will consume tweets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`twitter_stream_filter`: A keyword, Twitter handle, or hashtag to use in order
    to filter the stream of real-time tweets being captured from the Twitter API.
    In our case, we are filtering for real-time tweets directed at `@British_Airways`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trained_classification_model_path`: The absolute path where we saved our trained
    decision tree classifier in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml),
    *Natural Language Processing Using Apache Spark*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Twitter producer application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to develop our Python-based Kafka producer application that
    will capture tweets about airlines that are being tweeted in real-time and then
    publish those tweets to the Apache Kafka `twitter` topic that we created previously.
    We will be using the following two Python libraries in order to develop our Kafka
    producer:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tweepy`: This library allows us to access the Twitter API programmatically using
    Python and the consumer API keys and access tokens that we generated earlier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pykafka`: This library allow us to instantiate a Python-based Apache Kafka
    client through which we can communicate and transact with our single-node Kafka
    cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following Python code file, called `kafka_twitter_producer.py`, can be found
    in the GitHub repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In regards to our Python-based Kafka producer application, we perform the following
    steps (numbered to correspond to the numbered comments in our Python code file):'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the required modules from the `tweepy` and `pykafka` libraries
    respectively, as shown in the following code. We also import the configuration
    from our `config.py` file, which we created earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we instantiate a `tweepy` wrapper for the Twitter API using the consumer
    API keys and access tokens defined in `config.py` to provide us authenticated
    and authorized programmatic access to the Twitter API, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We then define a class in Python called `KafkaTwitterProducer`, which once instantiated,
    provides us with a `pykafka` client to our single-node Apache Kafka cluster, as
    shown in the following code. When this class is instantiated, it initially executes
    the code defined in the `__init__` function, which creates a `pykafka` client
    using the bootstrap servers, the locations of which may be found in `config.py`. It
    then creates a Kafka producer that associates the producer to the `twitter_kafka_topic_name`
    Kafka topic also defined in `config.py`. When data is captured by our `pykafka`
    producer, the `on_data` function is invoked, which physically publishes the data
    to the Kafka topic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If our `pykafka` producer encounters an error, then the `on_error` function
    is invoked, which, in our case, simply prints the error to the console and goes
    on to process the next message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we instantiate a Twitter stream using the `Stream` module of the `tweepy`
    library. To achieve this, we simply pass our Twitter app authentication details
    and an instance of our `KafkaTwitterProducer` class to the Stream module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have instantiated a Twitter stream, the final step is to filter
    the stream to deliver tweets of interest, based on the `twitter_stream_filter`
    option found in `config.py`, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to run our Kafka producer application! Since it is a Python
    application, the easiest way to run it is simply to use the Linux command line,
    navigate to the directory containing `kafka_twitter_producer.py`, and execute
    it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To check that it is actually capturing and publishing real-time tweets to Kafka,
    as described in [Chapter 2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml), *Setting
    Up a Local Development Environment*, you can start a command-line consumer application
    to consume messages from the Twitter topic and print them to the console, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Hopefully, you will see tweets printed to the console in real time. In our case,
    these tweets are all directed to `"@British_Airways"`.
  prefs: []
  type: TYPE_NORMAL
- en: The tweets themselves are captured via the Twitter API in JSON format, and contain
    not only the raw textual content of the tweet, but also associated metadata, such
    as the tweet ID, the username of the tweeter, the timestamp, and so on. For a
    full description of the JSON schema, please visit [https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html).
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing and feature vectorization pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As described earlier, in order to be able to apply our trained decision tree
    classifier to these real-time tweets, we first need to preprocess and vectorize
    them exactly as we did with our training and test datasets in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml),
    *Natural Language Processing Using Apache Spark*. However, rather than duplicating
    the preprocessing and vectorization pipeline logic within our Kafka consumer application
    itself, we will define our pipeline logic in a separate Python module and within
    Python *functions*. This way, any time we need to preprocess text as we did in
    [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural Language Processing
    Using Apache Spark*, we simply call the relevant Python function, thereby avoiding
    the need to duplicate the same code across different Python code files.
  prefs: []
  type: TYPE_NORMAL
- en: The following Python code file, called `model_pipelines.py`, can be found in
    the GitHub repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following Python module, we define two functions. The first function
    applies the exact same pipeline of `MLlib` and `spark-nlp` feature transformers
    that we studied in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural
    Language Processing Using Apache Spark*, in order to preprocess the raw textual
    content of the tweets. The second function then takes a preprocessed Spark dataframe
    and applies the HashingTF transformer to it in order to generate feature vectors
    based on term frequencies, exactly as we studied in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml),
    *Natural Language Processing Using Apache Spark*. The result is a Spark dataframe
    containing the original raw text of the tweet in a column called `text` and term
    frequency feature vectors in a column called `features`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Kafka Twitter consumer application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are finally ready to develop our Kafka consumer application using the Spark
    Structured Streaming engine in order to apply our trained decision tree classifier
    to the stream of real-time tweets in order to deliver real-time sentiment analysis!
  prefs: []
  type: TYPE_NORMAL
- en: The following Python code file, called `kafka_twitter_consumer.py`, can be found
    in the GitHub repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In regards to our Spark Structured-Streaming-based Kafka consumer application,
    we perform the following steps (numbered to correspond to the numbered comments
    in our Python code file):'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the configuration from our `config.py` file. We also import
    the Python functions containing the logic for our preprocessing and vectorization
    pipelines that we created earlier, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlike our Jupyter notebook case studies, there is no need explicitly to instantiate
    a `SparkContext` as this will be done for us when we execute our Kafka consumer
    application via `spark-submit` in the command line. In this case study, we create
    a `SparkSession`, as shown in the following code that acts as an entry point into
    the Spark execution environment—even if it is already running—and which subsumes
    `SQLContext`. We can therefore use `SparkSession` to undertake the same SQL-like
    operations over data that we have seen previously, while still using the Spark
    Dataset/DataFrame API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we load the decision tree classifier that we trained in [Chapter
    6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural Language Processing Using
    Apache Spark*, (which used the *HashingTF* feature extractor) from the local filesystem
    into a `DecisionTreeClassificationModel` object so that we can apply it later
    on, as shown in the following code. Note that the absolute path to the trained
    decision tree classifier has been defined in `config.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We are almost ready to start consuming messages from our single-node Kafka
    cluster. However, before doing so, we must note that Spark does not yet support
    the automatic inference and parsing of JSON key values into Spark dataframe columns.
    We must therefore explicitly define the JSON schema, or the subset of the JSON
    schema that we wish to retain, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have defined our JSON schema, we are ready to start consuming messages.
    To do this, we invoke the `readStream` method on our `SparkSession` instance to
    consume streaming data. We specify that the source of our stream will be a Kafka
    cluster using the `format` method, after which we define the Kafka bootstrap servers
    and the name of the Kafka topic to which we want to subscribe, both of which have
    been defined in `config.py`. Finally, we invoke the `load` method to stream the
    latest messages consumed from the `twitter` topic to an unbounded Spark dataframe
    called `tweets_df`, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Records stored in a Kafka topic are persisted in binary format. In order to
    process the JSON representing our tweets, which are stored in a Kafka record under
    a field called `value`, we must first `CAST` the contents of `value` as a string.
    We then apply our defined schema to this JSON string and extract the fields of
    interest, as shown in the following code. In our case, we are only interested
    in the tweet ID, stored in the JSON key called `id`, and its raw textual content,
    stored in the JSON key called `text`. The resulting Spark dataframe will therefore
    have two string columns, `id` and `text`, containing these respective fields of
    interest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have consumed raw tweets from our Kafka topic and parsed them into
    a Spark dataframe, we can apply our preprocessing pipeline as we did in [Chapter
    6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml),* Natural Language Processing Using
    Apache Spark*. However, rather than duplicating the same code from [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural
    Language Processing Using Apache Spark*, into our Kafka consumer application,
    we simply call the relevant function that we defined in `model_pipelines.py`,
    namely `preprocessing_pipeline()`, as shown in the following code. This preprocessing
    pipeline tokenizes the raw text, removes stop words, applies a stemming algorithm,
    and normalizes the resulting tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we generate feature vectors from these tokens, as we did in [Chapter
    6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural Language Processing Using
    Apache Spark*. We call the `vectorizer_pipeline()` function from `model_pipelines.py`
    to generate feature vectors based on term frequencies, as shown in the following
    code. The resulting Spark dataframe, called `features_df`, contains three pertinent
    columns, namely `id` (raw tweet ID), `text` (raw tweet text), and `features` (term-frequency
    feature vectors):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have generated feature vectors from our stream of tweets, we can
    apply our trained decision tree classifier to this stream in order to predict
    and classify their underlying sentiment. We do this as normal, by invoking the
    `transform` method on the `features_df` dataframe, resulting in a new Spark dataframe
    called `predictions_df`, containing the columns `id` and `text` as before, and
    a new column called `prediction` that contains our predicted classification, as
    shown in the following code. As described in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural
    Language Processing Using Apache Spark*, a prediction of `1` implies non-negative
    sentiment, and a prediction of `0` implies negative sentiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we write our predicted results dataframe to an output sink. In our
    case, we define the output sink as simply the console that is used to execute
    the Kafka consumer PySpark application—that is, the console from which we execute
    `spark-submit`. We achieve this by invoking the `writeStream` method on the relevant
    Spark dataframe and stating `console` as the `format` of choice. We start our
    output stream by invoking the `start` method, and invoke the `awaitTermination`
    method, which tells Spark to continue processing our streaming pipeline indefinitely
    until it is explicitly interrupted and stopped, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `outputMode` method defines what gets written to the output sink,
    and can take one of the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`complete`: The entire (updated) results table is written to the output sink'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`append`: Only the new rows appended to the results table since the last trigger
    is written to the output sink'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update`: Only the rows that were updated in the results table since the last
    trigger is written to the output sink'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are now ready to run our Kafka consumer application! Since it is a Spark
    application, we can execute it via *spark-submit* on the Linux command line. To
    do this, navigate to the directory where we installed Apache Spark (see [Chapter
    2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml), *Setting Up a Local Development
    Environment*). Thereafter we can execute the `spark-submit` program by passing
    to it the following command-line arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--master`: The Spark master URL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--packages`: The third-party libraries and dependencies required for the given
    Spark application to work. In our case, our Kafka consumer application is dependent
    on the availability of two third-party libraries, namely `spark-sql-kafka` (Spark
    Kafka integration) and `spark-nlp` (NLP algorithms, as studied in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml),
    *Natural Language Processing Using Apache Spark*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--py-files`: Since our Kafka consumer is a PySpark application, we can use
    this argument to pass a comma-delimited list of filesystem paths to any Python
    code files that our application is dependent on. In our case, our Kafka consumer
    application is dependent on `config.py` and `model_pipelines.py` respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final argument is the path to the Python code file containing our Spark
    Structured Streaming driver program, in our case, `kafka_twitter_consumer.py`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The final commands to execute therefore look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming that the Python-based Kafka producer application is running as well,
    the results table should periodically be written to the console and contain the
    real-time prediction and classification of the underlying sentiment behind the
    stream of airline tweets being consumed by the Twitter API from across the world
    and written by real Twitter users! A selection of real-world classified tweets
    that was processed when filtering using `"@British_Airways"` is shown in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tweet Raw Contents** | **Predicted Sentiment** |'
  prefs: []
  type: TYPE_TB
- en: '| @British_Airways @HeathrowAirport I mean I''m used to cold up in Shetland
    but this is a whole different kind of cold!! | Non-negative |'
  prefs: []
  type: TYPE_TB
- en: '| @British_Airways have just used the app to check-in for our flight bari to
    lgw but the app shows no hold luggage | Negative |'
  prefs: []
  type: TYPE_TB
- en: '| She looks more Beautiful at Night &#124; A380 takeoff London Heathrow @HeathrowAviYT
    @HeathrowAirport @British_Airways | Non-negative |'
  prefs: []
  type: TYPE_TB
- en: '| The @British_Airways #B747 landing into @HeathrowAirport | Non-negative |'
  prefs: []
  type: TYPE_TB
- en: '| @British_Airways trying to check in online for my flight tomorrow and receiving
    a ''sorry we are unable to offer you online check-in for this flight'' message.
    Any idea why?? | Negative |'
  prefs: []
  type: TYPE_TB
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we developed Apache Kafka producer and consumer applications
    and utilized Spark's Structured Streaming engine to process streaming data consumed
    from a Kafka topic. In our real-world case study, we designed, developed, and
    deployed an end-to-end stream processing pipeline that was capable of consuming
    real tweets being authored across the world and then classified their underlying
    sentiment using machine learning, all of which was done in real time.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we went on both a theoretical and a hands-on journey through some
    of the most important and exciting technologies and frameworks that underpin the
    data-intelligence-driven revolution being seen across industry today. We started
    out by describing a new breed of distributed and scalable technologies that allow
    us to store, process, and analyze huge volumes of structured, semi-structured,
    and unstructured data. Using these technologies as a base, we established the
    context for artificial intelligence and how it relates to machine learning and
    deep learning. We then went on to explore some of the key concepts in, and applications
    of, machine learning, including supervised learning, unsupervised learning, natural
    language processing, and deep learning. We illustrated these key concepts through
    a wide variety of relevant and exciting use cases that were implemented using
    our big data processing engine of choice—Apache Spark. Finally, because timely
    decisions are critical to many businesses and organizations in the modern world,
    we extended our deployment of machine learning models beyond batch processing
    to real-time streaming applications!
  prefs: []
  type: TYPE_NORMAL
