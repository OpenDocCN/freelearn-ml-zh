- en: '*Chapter 7*: Data and Feature Management'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will add a feature management data layer to the machine
    learning platform being built. We will leverage the features of the MLflow Projects
    module to structure our data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will look at the following sections in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Structuring your data pipeline project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acquiring stock data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking data quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will acquire relevant data to provide datasets for training.
    Our primary resource will be the Yahoo Finance Data for BTC dataset. Alongside
    that data, we will acquire the following extra datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Leveraging our productionization architecture introduced in [*Chapter 6*](B16783_06_Final_SB_epub.xhtml#_idTextAnchor106),
    *Introducing ML Systems Architecture*, represented in *Figure 7.1*, the feature
    and data component is responsible for acquiring data from sources and making the
    data available in a format consumable by the different components of the platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – High-level architecture with a data layer reference'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – High-level architecture with a data layer reference
  prefs: []
  type: TYPE_NORMAL
- en: Let's delve into this chapter and see how we will structure and populate the
    data layer with relevant data to be used for training models and generating features.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need the following prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of Docker installed on your machine. If you don't already
    have it installed, please follow the instructions at [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of docker-compose installed. Please follow the instructions
    at [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to Git on the command line and installed as described at [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a Bash terminal (Linux or Windows).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.5+ installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of your machine learning installed locally as described in
    [*Chapter 3*](B16783_03_Final_SB_epub.xhtml#_idTextAnchor066), *Your Data Science
    Workbench*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will describe the structure of our data pipeline, the
    data sources, and the different steps that we will execute to implement our practical
    example leveraging MLflow project features to package the project.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Copying and pasting directly from the code snippets might cause issues with
    your editor. Please refer to the GitHub repository of the chapter available at
    https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter07
  prefs: []
  type: TYPE_NORMAL
- en: Structuring your data pipeline project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At a high level, our data pipeline will run weekly, collecting data for the
    preceding 7 days and storing it in a way that can be run by machine learning jobs
    to generate models upstream. We will structure our data folders into three types
    of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Raw data**: A dataset generated by retrieving data from the Yahoo Finance
    API for the last 90 days. We will store the data in CSV format – the same format
    that it was received in from the API. We will log the run in MLflow and extract
    the number of rows collected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Staged data**: Over the raw data, we will run quality checks, schema verification,
    and confirm that the data can be used in production. This information about data
    quality will be logged in MLflow Tracking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training data**: The training data is the final product of the data pipeline.
    It must be executed over data that is deemed as clean and suitable to execute
    models. The data contains the data processed into features that can be consumed
    directly for the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This folder structure will be implemented initially on the filesystem and will
    be transposed to the relevant environment (examples: AWS S3, Kubernetes PersistentVolume,
    and so on) during deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to execute our data pipeline project, we will use the **MLflow Project**
    module to package the data pipeline in an execution environment-independent format.
    We will use the Docker format to package the **MLflow Project**. The Docker format
    provides us with different options to deploy our project in the cloud or on-premises
    depending on the available infrastructure to deploy our project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image0024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – High-level architecture with a data layer reference
  prefs: []
  type: TYPE_NORMAL
- en: 'Our workflow will execute the following steps, as illustrated in *Figure 7.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '`data/raw/data.csv folder`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`data/staged/data.csv file`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`data/training/data.csv location`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these three distinct phases, we ensure the reproducibility of the training
    data generation process, visibility, and a clear separation of the different steps
    of the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by organizing our MLflow project into steps and creating placeholders
    for each of the components of the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new folder on your local machine with the name `psytock-data-features`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the `MLProject file`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following `conda.yaml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now add a sample `main.py` file to the folder to ensure that the basic
    structure of the project is working:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test the basic structure by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command will build your project based on the environment created by your
    `conda.yaml` file and run the basic project that you just created. It should error
    out as we need to add the missing files. The *file not found* error will look
    like the following :'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: At this stage, we have the basic blocks of the MLflow project of the data pipeline
    that we will be building in this chapter. We will next fill in the Python script
    to acquire the data in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring stock data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our script to acquire the data will be based on the `pandas-datareader Python
    package`. It provides a simple abstraction to remote financial APIs we can leverage
    in the future in the pipeline. The abstraction is very simple. Given a data source
    such as Yahoo Finance, you provide the stock ticker/pair and date range, and the
    data is provided in a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now create the `load_raw_data.py file`, which will be responsible for
    loading the data and saving it in the `raw` folder. You can look at the contents
    of the file in the repository at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter07/psystock-data-features-main/load_raw_data.py.
    Execute the following steps to implement the file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you should add a function to retrieve the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we've acquired the data, we need to apply the best practices we will
    address in the next section – an approach to check the data quality of the data
    acquired.
  prefs: []
  type: TYPE_NORMAL
- en: Checking data quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Checking data quality as part of your machine learning system is extremely critical
    to ensure the integrity and correctness of your model training and inference.
    Principles of software testing and quality should be borrowed and used on the
    data layer of machine learning platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a data quality perspective, in a dataset there are a couple of critical
    dimensions with which to assess and profile our data, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Schema compliance**: Ensuring the data is from the expected types; making
    sure that numeric values don''t contain any other types of data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Valid data**: Assessing from a data perspective whether the data is valid
    from a business perspective'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing data**: Assessing whether all the data needed to run analytics and
    algorithms is available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For data validation, we will use the *Great Expectations* Python package (available
    at [https://github.com/great-expectations/great_expectations](https://github.com/great-expectations/great_expectations)).
    It allows making assertions on data with many data-compatible packages, such as
    pandas, Spark, and cloud environments. It provides a DSL in JSON with which to
    declare the rules that we want our data to be compliant with.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our current project, we want the following rules/constraints to be verifiable:'
  prefs: []
  type: TYPE_NORMAL
- en: Date values should be valid dates and cannot be missing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check numeric and long values are correctly typed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All columns are present in the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now create the `check_verify_data.py file`, which will be responsible
    for loading the data and saving it in the `staging` folder where all the data
    is valid and ready to be used for ML training. You can look at the contents of
    the file in the repository at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter07/psystock-data-features-main/check_verify_data.py.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to convert the preceding rules so they can be relied on by our system,
    we will need to import the following dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will implement the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can progress to do a little bit of cleaning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Having verified the quality of the data and staging to be used, it can now be
    utilized for feature generation with a high degree of confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Generating a feature set and training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will refactor a bit of the code previously developed in our local environment
    to generate features for training to add to our MLflow project the data pipelineof
    our MLflow project .
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now create the `feature_set_generation.py file`, which will be responsible
    for generating our features and saving them in the `training` folder where all
    the data is valid and ready to be used for ML training. You can look at the contents
    in the file in the repository https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter07/psystock-data-features-main/feature_set_generation.py:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to import the following dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before delving into the main component of the code, we''ll now proceed to implement
    a critical function to generate the features by basically transforming the difference
    with each *n* preceding day in a feature that we will use to predict the next
    day, very similar to the approach that we used in previous chapters of the book
    for our running use case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll proceed to read the staged file that is deemed as clean and ready
    to be used by upstream processes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We generate the feature set and features. We are now able to run all of the
    end-to-end pipeline from data acquisition to feature generation.
  prefs: []
  type: TYPE_NORMAL
- en: Running your end-to-end pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will run the complete example, which you can retrieve from
    the following address for the book''s GitHub repository in the folder at `/Chapter07/psytock-data-features-main`.
    *Figure 7.3* illustrates the complete folder structure of the project that you
    can inspect in GitHub and compare with your local version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Folder structure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0034.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – Folder structure
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the pipeline end to end, you should execute the following command in
    the directory with the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It will basically execute the end-to-end pipeline and you can inspect it directly
    in the MLflow UI, running each of the steps of the pipeline in order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You can run and explore the tracking information in MLflow at [http://localhost:5000](http://localhost:5000).
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7.4*, you can see the different runs of the main project and subprojects
    of the stages of the pipeline in a nested workflow format that you can browse
    to inspect the details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – High-level architecture with a data layer reference'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0044.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4 – High-level architecture with a data layer reference
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7.5*, you can see the reference to the `load_raw_data` phase of
    the data pipeline and check when it was started and stopped and the parameters
    used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – High-level architecture with a data layer reference'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0054.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.5 – High-level architecture with a data layer reference
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure.7.6*, you can see the reference to the `check_verify_data` phase
    of the data pipeline where we logged some basic statistical information of the
    dataset obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – High-level architecture with a data layer reference'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0063.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – High-level architecture with a data layer reference
  prefs: []
  type: TYPE_NORMAL
- en: 'If any data quality issues are detected, the workflow will fail with a clear
    indication of which section failed, as represented in *Figure 7.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Checking errors'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0073.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 – Checking errors
  prefs: []
  type: TYPE_NORMAL
- en: With this section, we have concluded the description of the process of data
    management and feature generation in a data pipeline implemented with the `MLProjects`
    module in MLflow. We will now look at how to manage the data in a feature store.
  prefs: []
  type: TYPE_NORMAL
- en: Using a feature store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A feature store is a software layer on top of your data to abstract all the
    production and management processes for data by providing inference systems with
    an interface to retrieve a feature set that can be used for inference or training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will illustrate the concept of a feature store by using
    Feast (a feature store), an operational data system for managing and serving machine
    learning features to models in production:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Feast Architecture (retrieved from https://docs.feast.dev/)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0083.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.8 – Feast Architecture (retrieved from https://docs.feast.dev/)
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to understand how Feast works and how it can fit into your data layer
    component (code available at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter07/psystock_feature_store,
    execute the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install `feast`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize a feature repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create your feature definitions by replacing the `yaml` file generated automatically:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now proceed to import dependencies of the feature definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now load the feature files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now add a feature view:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To deploy the feature store with the configurations added so far, we need to
    run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: At this stage, the feature store is deployed in your environment (locally in
    this case) and the feature store is available to be used from your MLflow job.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can now do feature retrieval, now that all the features are stored in a
    feature store:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can now integrate your feature store repository into your MLflow workloads.
  prefs: []
  type: TYPE_NORMAL
- en: With this section, we have concluded the description of the process of data
    management and feature generation in a data pipeline implemented with the `MLProjects
    module` in MLflow. We are now ready to deal with production environment deployments
    in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered MLflow and its integration with the feature management
    data layer of our reference architecture. We leveraged the features of the MLflow
    Projects module to structure our data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The important layer of data and feature management was introduced, and the need
    for feature generation was made clear, as were the concepts of data quality, validation,
    and data preparation.
  prefs: []
  type: TYPE_NORMAL
- en: We applied the different stages of producing a data pipeline to our own project.
    We then formalized data acquisition and quality checks. In the last section, we
    introduced the concept of a feature store and how to create and use one.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapters and following section of the book, we will focus on applying
    the data pipeline and features to the process of training and deploying the data
    pipeline in production.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to further your knowledge, you can consult the documentation at the
    following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/mlflow/mlflow/blob/master/examples/multistep_workflow/MLproject](https://github.com/mlflow/mlflow/blob/master/examples/multistep_workflow/MLproject)'
  prefs: []
  type: TYPE_NORMAL
