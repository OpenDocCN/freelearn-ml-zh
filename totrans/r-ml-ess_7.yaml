- en: Chapter 7. Overview of Machine Learning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different machine learning techniques and this chapter gives an overview
    of the most relevant ones. Some of them have already been introduced in the previous
    chapters and some are new.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most relevant branches of techniques: supervised and unsupervised learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making predictions with supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying hidden patterns and structures with unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pros and cons of these techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are different categories of machine learning techniques and in this chapter
    we will see the two most relevant branches—supervised and unsupervised learning,
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Overview](img/7740OS_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The supervised and unsupervised learning techniques deal with objects described
    by features. An example of supervised learning techniques is decision tree learning,
    and an example of unsupervised technique is k-means. In both cases, the algorithms
    learn from a set of objects and the difference is their target: supervised techniques
    predict attributes whose nature is already known and unsupervised techniques identify
    new patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The supervised learning techniques predict an attribute of the objects. The
    algorithms learn from a training set of objects whose attribute is known and they
    predict the attribute of other objects. There are two categories of supervised
    learning techniques: classification and regression. We talk about classification
    if the predicted attribute is categoric and about regression if the attribute
    is numeric.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The unsupervised learning techniques identify patterns and structures of a
    set of objects. The two main branches of unsupervised learning are clustering
    and dimensionality reduction. The clustering techniques identify homogeneous groups
    of objects on the basis of their attributes and an example is k-means. Dimensionality
    reduction techniques identify a small set of significant features describing the
    objects and an example is the principal component analysis. The difference between
    clustering and dimensionality reduction depends on the identified attribute that
    is categoric or numeric respectively, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Overview](img/7740OS_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This chapter will show you some popular techniques for each branch. In order
    to illustrate the techniques, we will reuse the flag dataset of [Chapters 4](ch04.html
    "Chapter 4. Step 1 – Data Exploration and Feature Engineering"), *Step 1 – Data
    Exploration and Feature Engineering*; [Chapter 5](ch05.html "Chapter 5. Step 2
    – Applying Machine Learning Techniques"), *Step 2 – Applying Machine Learning
    Techniques*; and [Chapter 6](ch06.html "Chapter 6. Step 3 – Validating the Results"),
    *Step 3 – Validating the Results* that can be found in the supporting code bundle
    with this book.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will show you some examples of popular supervised learning algorithms.
    These techniques are very useful for facing business problems because they make
    predictions about future attributes and outcomes. In addition, it is possible
    to measure the accuracy of each technique and/or parameter in order to choose
    the most suitable one and set it up in the best way.
  prefs: []
  type: TYPE_NORMAL
- en: 'As anticipated, there are two categories of techniques: classification and
    regression. However, most of the techniques can be used in both the contexts.
    Each of the following subsections introduces a different algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: The k-nearest neighbors algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KNN is a supervised learning algorithm that performs classification or regression.
    Given a new object, the algorithm predicts its attribute starting from its *k*
    neighbors that are its most similar objects. KNN is a lazy learning algorithm
    in the sense that it directly queries the training data to make a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a categoric attribute, the algorithm estimates it as the most
    common among the similar objects. In the case of a numeric attribute, it computes
    the median or average between them. In order to state which are the *k* most *similar*
    objects, KNN uses a similarity function that evaluates how similar two objects
    are. In order to measure similarity, the starting point is often a distance matrix
    expressing the dissimilarity. Then, the algorithm computes the similarity between
    the new object and each other and picks the *k* most similar objects.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we will use the flag dataset and the features are the number
    of stripes and the number of colors in the flags. The attribute that we want to
    predict starting from its flag attributes is the language of a new country.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training set is composed of some countries in such a way that there are
    no two countries with the same flag features. First, let''s visualize the data.
    We can show the countries in a chart whose dimensions are the two features and
    whose color is the language, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The k-nearest neighbors algorithm](img/7740OS_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We have two new countries with:'
  prefs: []
  type: TYPE_NORMAL
- en: 7 stripes and 4 colors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3 stripes and 7 colors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We want to determine the language of two new countries using a 4-nearest-neighbor
    algorithm. We can add the two countries to the chart and determine the 4 closest
    points for each of them, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The k-nearest neighbors algorithm](img/7740OS_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With regard to the country on the right-hand side of the chart, all its 4 closest
    neighbors belong to **Others**, so we estimate that its language is **Others**.
    The other country has a mixed neighborhood: 1 English, 1 Other Indo-European,
    and 2 Spanish countries. The most common language is Spanish, so we estimate that
    it is a Spanish-speaking country.'
  prefs: []
  type: TYPE_NORMAL
- en: The KNN is a simple and scalable algorithm that achieves good results in many
    contexts. However, in the presence of many features, the similarity function takes
    account of all of them, including the less relevant, making it difficult to use
    the distance. In that context, the KNN is not able to identify the meaningful
    nearest neighbors and this issue is called the curse of dimensionality. A solution
    is to reduce the dimensionality by selecting the most relevant features or using
    a dimensionality reduction technique (this is the topic of the next section).
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision tree learning is a supervised learning algorithm that builds a classification
    or regression tree. Each leaf of the tree represents the attribute estimation
    and each node splits the data accordingly with a condition of the features.
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree learning is an eager method in the sense that it uses a training
    set to build a model that doesn't require you to query the data. All the other
    supervised learning techniques are eager as well.
  prefs: []
  type: TYPE_NORMAL
- en: The target of the algorithm is to define the most relevant feature and split
    the set in two groups accordingly with it. Then, for each group, the algorithm
    identifies its most relevant feature and divides the objects of the groups into
    two parts. This procedure goes on until we identify the leaves as small groups
    of objects. For each leaf, the algorithm estimates the feature as a mode, if it
    is categoric, or average, if it is numeric. After building the tree, if we have
    too many leaves, we can define a level in which we stop splitting the tree. In
    this way, each leaf will contain a reasonably big group. This procedure of stopping
    splitting is called pruning. In this way, we find a less complex and more accurate
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we want to determine the language of a new country starting
    from different flag attributes, such as colors and patterns. The algorithm builds
    the tree learning from a training set. Let''s visualize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision tree learning](img/7740OS_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In any node, if the answer is **true**, we go to the left, and if the answer
    is **false**, we go to the right. First, the model identifies the most relevant
    attribute that is **saltires**. If a flag contains a saltire, we go to the left
    and we determine that the related country is English. Otherwise, we go to the
    right and we check if the flag contains the blue color. Then, we go on checking
    the conditions until we reach a leaf.
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose that we built the tree without taking account of the Spanish flag.
    How do we estimate the language of Spain? Starting from the top, we check the
    conditions on each node we encounter.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The flag doesn't contain a saltire, so we go to the left.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The flag contains the blue color, so we go to the right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The flag doesn't contain a cross, so `crosses = no` is `true` and we go to the
    left.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The flag doesn't contain an animated image, so we go to the right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The flag has two main colors, so `number of colors not equal to 4 or 5` is `true`
    and we go to the left.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The flag doesn't contain any bars, so we go to the left.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The flag doesn''t have any vertical stripes, so `nStrp0 = no` is `true` and
    we go to the left, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision tree learning](img/7740OS_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the end, the estimated language is `Spanish`.
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree learning can deal with numeric and/or categoric features and
    attributes, so it can be applied in different contexts with just a little data
    preparation. In addition, it is applicable when there are many features, different
    from other algorithms. A disadvantage is that the algorithm can overfit in the
    sense that the model is too close to the data and is more complicated than the
    reality, although pruning can help with this.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression is a statistical model identifying a relationship between
    numeric variables. Given a set of objects described by the *y* attribute and the
    `x1, …,` and `xn` features, the model defines a relationship between the features
    and the attribute. The relationship is described by the linear function *y = a0
    + a1 * x1 + … + an * xn*, and `a0, …,` and `an` are parameters defined by the
    method in such a way that the relationship is as close as possible to the data.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of machine learning, linear regression can be used to predict a
    numeric attribute. The algorithm learns from the training dataset to determine
    the parameters. Then, given a new object, the model inserts its features into
    the linear function to estimate the attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we want to estimate the population of a country starting from
    its area. First, let''s visualize the data about the area (in thousand km2) and
    the population (in millions), as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/7740OS_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Most of the countries have an area below 3000 thousand km2 and a population
    below 200 million and just a few countries have a much higher area and/or population.
    For this reason, most of the points are concentrated in the bottom-left area of
    the chart. In order to spread the points, we can transform the features using
    the logarithmic area and population, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/7740OS_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The target of linear regression is to identify a linear relationship that is
    as close to the data as possible. In our case, we have two dimensions, so we can
    visualize the relationship using a line. Given the area, the linear regression
    estimates that the population is on the line. Let''s see it in the chart with
    the logarithmic features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/7740OS_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Given a new country about which we know the area, we can estimate its population
    using the regression line. In the chart, there is a new country of which we know
    the area. The linear regression estimates that its point is on the red line.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is a very simple and basic technique. The disadvantage is
    that it requires numeric features and attributes, so there are many contexts in
    which it is not applicable. However, it is possible to convert the categoric features
    into a numeric format using dummy variables or other techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Another disadvantage is that the model makes strong assumptions on how the features
    and the attributes are related. The function estimating the output is linear,
    so in some contexts it might be far from the real relationship. In addition, if
    in reality the features interact with each other, the model is not able to keep
    track of the interaction. It's possible to solve this problem using a transformation
    that makes the relationship linear. It is also possible to define new features
    expressing non-linear interactions.
  prefs: []
  type: TYPE_NORMAL
- en: The linear regression is very basic and it is the starting point of some other
    techniques. For instance, the logistic regression predicts an attribute whose
    value is in the 0 to 1 range.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial Neural Networks** (**ANN**) are the supervised learning techniques
    whose logic is similar to biological neural systems. A simple ANN technique is
    the single-layer perceptron and it is a classification technique estimating a
    binary attribute whose value can be 0 or 1\. The perceptron works like a neuron
    in the sense that it sums the impact of all the inputs and outputs to 1 if the
    sum is above a defined threshold. The model is based on the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: A weight for each feature, defining its impact
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A threshold above which the estimated output is 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting from the features, the model estimates the attribute through these
    steps
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the output through a linear regression: multiply each feature by its
    weight and sum all of them'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimate the attribute to 1 if the output is above the threshold and to 0 otherwise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The models are as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptron](img/7740OS_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the beginning, the algorithm builds the perceptron with a defined set of
    coefficients and with a threshold. Then, the algorithm iteratively improves the
    coefficients using the training set. At each step, the algorithm estimates the
    attribute of each object. Then, the algorithm computes the difference between
    the real and the estimated attribute and uses the difference to modify the coefficients.
    In many situations, the algorithm does not reach a stable set of coefficients
    that are not modified, so we need to define at which point we stop. In the end,
    we have a perception defined by a set of coefficients and we can use it to estimate
    the attribute of new objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The perceptron is a simple example of a neural network and it allows us to
    easily understand the impact of the variables. However, the perceptron depends
    on a linear regression, so it is limited in the same way: the feature impact is
    linear and the features can''t interact with each other.'
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each algorithm has some weaknesses leading to incorrect results. What if we
    were able to solve the same problem using different algorithms and to pick the
    best result? If just a few algorithms commit the same mistake, we can just ignore
    them. It is not possible to determine which result is correct and which is not,
    but there is another option. By performing supervised learning on a new object,
    we can apply different algorithms and pick the most common or average result among
    them. In this way, if most of the algorithms identify the correct estimation,
    we will take it into account. The ensemble methods are based on this principle:
    they combine different classification or regression algorithms to increase their
    accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An ensemble method requires variability between the results coming from different
    algorithms and/or training datasets. Some options are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Changing the algorithm configuration**: The algorithm is the same and its
    parameters vary within a range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Changing the algorithm**: We predict the attribute using different techniques.
    In addition, for each technique, we can use different configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using different data subsets**: The algorithm is the same and every time
    it learns from a different random subset of the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using different data samples (bagging)**: The algorithm is the same and it
    learns from a bootstrap sample, that is, a set of objects picked randomly from
    the training dataset. The same object can be picked more than once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final result combines the output of all the algorithms. In the case of classification,
    we use the mode, and in the case of regression, we use the average or median.
  prefs: []
  type: TYPE_NORMAL
- en: We can build an ensemble algorithm using any combination of supervised learning
    techniques, so there are several options. An example is a random forest that combines
    decision tree learning algorithms using bagging (the technique explained in the
    last bullet point in the previous list).
  prefs: []
  type: TYPE_NORMAL
- en: The ensemble methods often perform much better than the single algorithms. In
    the case of classification, the ensemble removes the biases affecting just a small
    part of the algorithms. However, the logic of different algorithms is often related
    and the same bias might be common. In this case, the ensemble keeps the bias.
  prefs: []
  type: TYPE_NORMAL
- en: The ensemble methods don't always work in the case of regression problems since
    the biases affect the final result. For instance, if there is just an algorithm
    computing a very biased result, the average will be highly affected by that. In
    this context, the median works better since it is much more stable and it is not
    affected by outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter shows some unsupervised learning techniques. When facing a business
    problem, these techniques allow us to identify hidden structures and patterns
    and perform exploratory data analysis. In addition, unsupervised learning can
    simplify the problem, allowing us to build more accurate and less elaborated solutions.
    These techniques can also be used in the solution of the problem itself.
  prefs: []
  type: TYPE_NORMAL
- en: The two branches of techniques are clustering and dimensionality reduction and
    most of them are not applicable in both the contexts. This chapter shows some
    popular techniques.
  prefs: []
  type: TYPE_NORMAL
- en: k-means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: k-means is a centroid-based clustering technique. Given a set of objects, the
    algorithm identifies *k* homogeneous clusters. k-means is centroid-based in the
    sense that each cluster is defined by its centroid representing its average object.
  prefs: []
  type: TYPE_NORMAL
- en: The target of the algorithm is to identify *k* centroids. Then, the k-means
    associates each object to the closest centroid, defining *k* clusters. The algorithm
    starts with a random set of centroids and it iteratively changes them, improving
    the clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, the data is about the country flags and the two features are
    the number of stripes and the number of colors. We select a subset of the countries
    in such a way that there are no two flags with the same value of the attributes.
    Our target is to identify two homogeneous groups of countries. The first step
    of the k-means is identifying two random centroids. Let''s visualize the data
    and the centroids in a chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![k-means](img/7740OS_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **o** represents the country flags and the **x** represents the centroids.
    Before running k-means, we need to define a distance that is a way of determining
    dissimilarity between objects. For instance, in the preceding chart, we can use
    the Euclidean distance that expresses the length of the line connecting two points.
    The algorithm is iterative and each step consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: For each point, determine the centroid whose distance is the minimum. Then,
    assign the point to the cluster related to the closest centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recompute the centroid of each cluster in such a way that it is the average
    between its objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the end, we have two clusters with the related centroids representing average
    objects. Let''s visualize them, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![k-means](img/7740OS_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The colors represent the clusters and the black **x** represents the final centroids.
  prefs: []
  type: TYPE_NORMAL
- en: k-means is one of the most popular clustering techniques because it is easy
    to understand and it doesn't require a lot of computational power. However, the
    algorithm has some limitations. It contains a random component, so if we run it
    twice on the same set of data it will probably identify different clusters. Another
    disadvantage is that it is not able to identify the clusters in some specific
    contexts, for instance, when the clusters have different sizes or elaborated shapes.
    k-means is a very simple and basic algorithm and it is the starting point to some
    more elaborate techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hierarchical clustering is a branch of clustering techniques. Starting from
    a set of objects, the target builds a hierarchy of clusters. In agglomerative
    hierarchical clustering, each object belongs to a different cluster in the beginning.
    Then, the algorithm merges the clusters until there is one cluster containing
    all the objects. After having identified the hierarchy, we can define the clusters
    and stop their merging at any point.
  prefs: []
  type: TYPE_NORMAL
- en: 'During each agglomeration step, the algorithm merges the two most similar clusters
    and there are some parameters defining the similarity. First, we need to define
    a way to measure how similar two objects are. There are several options, depending
    on the situation. Then, we need to define the similarity between clusters; the
    methods are called **linkage**. In order to measure the similarity, we start defining
    a distance function that is the opposite. To determine the distance between cluster1
    and cluster2, we measure the distance between every possible object of cluster1
    and every object of cluster2\. Some options to measure the distance between the
    two clusters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single linkage**: This is the minimum distance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete** **linkage**: This is the maximum distance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average** **linkage**: This is the average distance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the linkage, the results of the algorithms will be different.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example uses the same data as k-means. The country flags are represented
    by the number of stripes and colors and we want to identify homogeneous groups.
    The distance that we use is the Euclidean (just the distance between two points)
    and the linkage is complete. First, let''s identify the clusters from their hierarchy,
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical clustering](img/7740OS_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The chart is called a **dendrogram** and at the bottom of the chart every object
    belongs to a different cluster. Then, going up, we merge the clusters until all
    the objects belong to the same cluster. The height is the distance at which the
    algorithm merges the clusters. For instance, at a height of 3, all the clusters
    whose distance is below 3 are already merged.
  prefs: []
  type: TYPE_NORMAL
- en: 'The red line is at a height of 6 and it defines when we stop merging and below
    it the objects are divided in 4 clusters. Now we can visualize the clusters in
    a chart as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical clustering](img/7740OS_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The colors of the points represent the clusters. The algorithm has correctly
    identified the group on the right and has split the group on the left in three
    parts in a good way.
  prefs: []
  type: TYPE_NORMAL
- en: There are different options for the hierarchic cluster and some of them produce
    very good results in some contexts. Different from the k-means, the algorithm
    is deterministic, so it always leads to the same result.
  prefs: []
  type: TYPE_NORMAL
- en: A big disadvantage of hierarchic cluster is the computational time (`O(n3)`)
    that makes it impossible to apply it on large datasets. Another lack is the manual
    component to choose the algorithm configuration and the dendrogram cut. In order
    to identify a good solution, we usually need to run the algorithm with different
    configurations and to visualize the dendrogram to define its cut.
  prefs: []
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Principal Components Analysis** (**PCA**) is a statistical procedure transforming
    the features. The PCA logic is based on the concepts of linear correlation and
    variance. In a machine learning context, the PCA is a dimensionality reduction
    technique.'
  prefs: []
  type: TYPE_NORMAL
- en: Starting with the features describing a set of objects, the target defines other
    variables that are linearly uncorrelated with each other. The output is a new
    set of variables defined as linear combinations of the initial features. In addition,
    the new variables are ranked on the basis of their relevance. The number of the
    new variables is less than or equal to the initial number of features and it is
    possible to select the most relevant features. Then, we are able to define a smaller
    set of features, reducing the problem dimension.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm starts defining the feature combination with the highest variance.
    Then, at each step, it iteratively defines another feature combination maximizing
    the variance, under the condition that the new combination is not linearly correlated
    with the others.
  prefs: []
  type: TYPE_NORMAL
- en: In the example of [Chapter 4](ch04.html "Chapter 4. Step 1 – Data Exploration
    and Feature Engineering"), *Step 1 – Data Exploration and Feature Engineering*,
    [Chapter 5](ch05.html "Chapter 5. Step 2 – Applying Machine Learning Techniques"),
    *Step 2 – Applying Machine Learning Techniques*, and [Chapter 6](ch06.html "Chapter 6. Step
    3 – Validating the Results"), *Step 3 – Validating the Results*, we have defined
    37 attributes describing each country flag. Applying the PCA, we can define 37
    new attributes defined as linear combination of the variables. The attributes
    are ranked by relevance, so we can select the top six and in this way have a small
    table describing the flag. In this way, we are able to build a supervised learning
    model estimating the language on the basis of six relevant features.
  prefs: []
  type: TYPE_NORMAL
- en: In the presence of a lot of features, the PCA allows us to define a smaller
    set of relevant variables. However, this technique is not applicable in all the
    contexts. A lack is that the result depends on how the features are scaled, so
    it is necessary to standardize the variables first.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with a supervised learning problem, we can use the PCA to reduce its
    dimensionality. However, the PCA only takes into account the features, ignoring
    how they are related with the attribute to predict, so it might select feature
    combinations that are not very relevant to the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned about the main branches of machine learning techniques:
    supervised and unsupervised learning. We saw how to estimate a numeric or categoric
    attribute using supervised learning techniques such as KNN, decision tree, linear
    regression, and neural networks. We saw that it is possible to increase performance
    using ensembles that are techniques combining different supervised learning algorithms.
    We learned how to identify homogeneous groups using clustering techniques such
    as k-means and hierarchic clustering. We have also understood the importance of
    dimensionality reduction techniques such as the PCA to transform the features
    defining a smaller set of variables.'
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter shows an example of a business problem that can be faced using
    machine learning techniques. We will also see examples of both supervised and
    unsupervised learning techniques.
  prefs: []
  type: TYPE_NORMAL
