<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer225">&#13;
			<h1 id="_idParaDest-252" class="chapter-number"><a id="_idTextAnchor258"/>12</h1>&#13;
			<h1 id="_idParaDest-253"><a id="_idTextAnchor259"/>Linear Regression in R</h1>&#13;
			<p>In this chapter, we will introduce linear regression, a fundamental statistical approach that’s used to model the relationship between a target variable and multiple explanatory (also called independent) variables. We will cover the basics of linear regression, starting with simple linear regression and then extending the concepts to multiple linear regression. We will learn how to estimate the model coefficients, evaluate the goodness of fit, and test the significance of the coefficients using hypothesis testing. Additionally, we will discuss the assumptions underlying linear regression and explore techniques to address potential issues, such as nonlinearity, interaction effect, multicollinearity, and heteroskedasticity. We will also introduce two widely used regularization <a id="_idIndexMarker1128"/>techniques: the ridge and <strong class="bold">Least Absolute Shrinkage and Selection Operator</strong> (<span class="No-Break"><strong class="bold">lasso</strong></span><span class="No-Break">) penalties.</span></p>&#13;
			<p>By the end of this chapter, you will learn the core principles of linear regression, its extensions to regularized linear regression, and the implementation <span class="No-Break">details involved.</span></p>&#13;
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>&#13;
			<ul>&#13;
				<li>Introducing <span class="No-Break">linear regression</span></li>&#13;
				<li>Introducing penalized <span class="No-Break">linear regression</span></li>&#13;
				<li>Working with <span class="No-Break">ridge regression</span></li>&#13;
				<li>Working with <span class="No-Break">lasso regression</span></li>&#13;
			</ul>&#13;
			<p>To run the code in this chapter, you will need to have the latest versions of the <span class="No-Break">following packages:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break"><strong class="source-inline">ggplot2</strong></span><span class="No-Break">, 3.4.0</span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">tidyr</strong></span><span class="No-Break">, 1.2.1</span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">dplyr</strong></span><span class="No-Break">, 1.0.10</span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">car</strong></span><span class="No-Break">, 3.1.1</span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">lmtest</strong></span><span class="No-Break">, 0.9.40</span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">glmnet</strong></span><span class="No-Break">, 4.1.7</span></li>&#13;
			</ul>&#13;
			<p>Please note that the versions of the packages mentioned in the preceding list are the latest ones at the time of writing this chapter. All the code and data for this chapter are available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_12/working.R"><span class="No-Break">https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_12/working.R</span></a><span class="No-Break">.</span></p>&#13;
			<h1 id="_idParaDest-254"><a id="_idTextAnchor260"/>Introducing linear regression</h1>&#13;
			<p>At the core of<a id="_idIndexMarker1129"/> linear regression is the concept of fitting a straight line – or more generally, a hyperplane – to the data points. Such fitting aims to minimize the deviation between the observed and predicted values. When it comes to simple linear regression, one target variable is regressed by one predictor, and the goal is to fit a straight line that best mimics the relationship between the two variables. For multiple linear regression, there is more than one predictor, and the goal is to fit a hyperplane that best describes the relationship among the variables. Both tasks can be achieved by minimizing a measure of deviation between the predictions and the <span class="No-Break">corresponding targets.</span></p>&#13;
			<p>In linear regression, obtaining an optimal model means identifying the best coefficients that define the relationship between the target variable and the input predictors. These coefficients represent the change in the target associated with a single unit change in the associated predictor, assuming all other variables are constant. This allows us to quantify the magnitude (size of the coefficient) and direction (sign of the coefficient) of the relationship between the variables, which can be used for inference (highlighting explainability) <span class="No-Break">and prediction.</span></p>&#13;
			<p>When it comes to inference, we often look at the relative impact on the target variable given a unit change to the input variable. Examples of such explanatory modeling include how marketing spend affects quarterly sales, how smoker status affects insurance premiums, and how education affects income. On the other hand, predictive modeling focuses on predicting a target quantity. Examples include predicting quarterly sales given the marketing spend, predicting the insurance premium given a policyholder’s profile information, such as age and gender, and predicting income given someone’s education, age, work experience, <span class="No-Break">and industry.</span></p>&#13;
			<p>In <a id="_idIndexMarker1130"/>linear regression, the expected outcome is modeled as a weighted sum of all the input variables. It also assumes that the change in the output is linearly proportional to the change in any input variable. This is the simplest form of the <span class="No-Break">regression method.</span></p>&#13;
			<p>Let’s start with <strong class="bold">simple linear </strong><span class="No-Break"><strong class="bold">regression</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">SLR</strong></span><span class="No-Break">).</span></p>&#13;
			<h2 id="_idParaDest-255"><a id="_idTextAnchor261"/>Understanding simple linear regression</h2>&#13;
			<p>SLR <a id="_idIndexMarker1131"/>is a <a id="_idIndexMarker1132"/>powerful and widely used statistical model that specifies the relationship between two continuous variables, including one input and one output. It allows us to understand how a response variable (also referred to as the dependent or target variable) changes as the explanatory variable (also called the independent variable or the input variable) varies. By fitting a straight line to the observed data, SLR quantifies the strength and direction of the linear association between the two variables. This straight line is called the<a id="_idIndexMarker1133"/> SLR <strong class="bold">model</strong>. It enables us to make predictions and infer the impact of the predictor on the <span class="No-Break">target variable.</span></p>&#13;
			<p>Specifically, in an SLR model, we assume a linear relationship between the target variable (<span class="_-----MathTools-_Math_Variable">y</span>) and the input variable (<span class="_-----MathTools-_Math_Variable">x</span>). The model can be represented mathematically <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span></p>&#13;
			<p>Here, <span class="_-----MathTools-_Math_Variable">y</span> is called the response variable, dependent variable, explained variable, predicted variable, target variable, or regressand. <span class="_-----MathTools-_Math_Variable">x</span> is called the explanatory variable, independent variable, control variable, predictor variable, or regressor. <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> is the intercept of the linear line that represents the expected value of <span class="_-----MathTools-_Math_Variable">y</span> when <span class="_-----MathTools-_Math_Variable">x</span> is 0. <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> is the slope that represents the change in <span class="_-----MathTools-_Math_Variable">y</span> for a one-unit increase in <span class="_-----MathTools-_Math_Variable">x</span>. Finally, <span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span> is the random error term that accounts for the variability in the target, <span class="_-----MathTools-_Math_Variable">y</span>, that the predictor, <span class="_-----MathTools-_Math_Variable">x</span>,  <span class="No-Break">cannot explain.</span></p>&#13;
			<p>The main objective of SLR is to estimate the <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> and <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> parameters. An optimal set of <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> and <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> would minimize the total squared deviations between the observed target values, <span class="_-----MathTools-_Math_Variable">y</span>, and the predicted values, <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span>, using the model. This is called the <strong class="bold">least squares method</strong>, where we<a id="_idIndexMarker1134"/> seek the optimal <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> and <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> parameters that correspond to the<a id="_idIndexMarker1135"/> minimum <strong class="bold">sum of squared </strong><span class="No-Break"><strong class="bold">error</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">SSR</strong></span><span class="No-Break">):</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Function_v-normal">min</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Function_v-normal">min</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Function_v-normal">min</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>&#13;
			<p>Here, each residual, <span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span>, is the difference between the observation, <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span>, and its fitted value, <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span>. In simple terms, the objective is to locate the straight line that is closest to the data points given. <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.1</em> illustrates a collection of data points (in blue) and the linear model (<span class="No-Break">in red):</span></p>&#13;
			<div>&#13;
				<div id="_idContainer217" class="IMG---Figure">&#13;
					<img src="Images/B18680_12_001.jpg" alt="Figure 12.1 – The SLR model, where the linear model appears as a line and is trained by minimizing the SSR" width="728" height="471"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – The SLR model, where the linear model appears as a line and is trained by minimizing the SSR</p>&#13;
			<p>Once we have<a id="_idIndexMarker1136"/> estimated the model coefficients, <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> and <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>, we can use the model to<a id="_idIndexMarker1137"/> make predictions and inferences on the intensity of the linear relationship between the variables. Such a linear relationship indicates the goodness of fit, which is often measured using the coefficient of determination, or <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>. <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> ranges from <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong> and quantifies the proportion of the total variation in <span class="_-----MathTools-_Math_Variable">y</span> that can be explained by <span class="_-----MathTools-_Math_Variable">x</span>. It is defined <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span></p>&#13;
			<p>Here, <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span> denotes the average value of the observed target <span class="No-Break">variable, </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break">.</span></p>&#13;
			<p>Besides<a id="_idIndexMarker1138"/> this, we can also use hypothesis testing to test the significance of the resulting coefficients, <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> and <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>, thus helping us determine whether the observed <a id="_idIndexMarker1139"/>relationship between the variables is <span class="No-Break">statistically significant.</span></p>&#13;
			<p>Let’s go through an example of building a simple linear model using a <span class="No-Break">simulated dataset.</span></p>&#13;
			<h3>Exercise 12.1 – building an SLR model</h3>&#13;
			<p>In this exercise, we will demonstrate<a id="_idIndexMarker1140"/> the implementation of an SLR model in R. We’ll be using a combination of built-in functions and packages to accomplish this task using a <span class="No-Break">simulated dataset:</span></p>&#13;
			<ol>&#13;
				<li>Simulate a dataset such that the response variable, <strong class="source-inline">Y</strong>, is linearly dependent on the explanatory variable, <strong class="source-inline">X</strong>, with some <span class="No-Break">added noise:</span><pre class="source-code">&#13;
# Set seed for reproducibility&#13;
set.seed(123)&#13;
# Generate independent variable X&#13;
X = runif(100, min = 1, max = 100) # 100 random uniform numbers between 1 and 100&#13;
# Generate some noise&#13;
noise = rnorm(100, mean = 0, sd = 10) # 100 random normal numbers with mean 0 and standard deviation 10&#13;
# Generate dependent variable Y&#13;
Y = 5 + 0.5 * X + noise&#13;
# Combine X and Y into a data frame&#13;
data = data.frame(X, Y)</pre><p class="list-inset">Here, we use the <strong class="source-inline">runif()</strong> function to generate the independent variable, <strong class="source-inline">X</strong>, which is a vector of random uniform numbers. Then, we add some “noise” to the dependent variable, <strong class="source-inline">Y</strong>, making the observed data more realistic and less perfectly linear. This is achieved using the <strong class="source-inline">rnorm()</strong> function, which creates a vector of random normal numbers. The target variable, <strong class="source-inline">Y</strong>, is then created as a function of <strong class="source-inline">X</strong>, plus <span class="No-Break">the noise.</span></p><p class="list-inset">Besides this, we use a seed (<strong class="source-inline">set.seed(123)</strong>) at the beginning to ensure reproducibility. This means that we will get the same set of random numbers every time we run this code. Each run will produce a different list of random numbers if we don’t set <span class="No-Break">a seed.</span></p><p class="list-inset">In this simulation, the true intercept (<span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span>) is 5, the true slope (<span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>) is 0.5, and the noise is normally distributed with a mean of 0 and a standard deviation <span class="No-Break">of 10.</span></p></li>				<li>Train a linear <a id="_idIndexMarker1141"/>regression model based on the simulated dataset using the <span class="No-Break"><strong class="source-inline">lm()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
# Fit a simple linear regression model&#13;
model = lm(Y ~ X, data = data)&#13;
# Print the model summary&#13;
&gt;&gt;&gt; summary(model)&#13;
Call:&#13;
lm(formula = Y ~ X, data = data)&#13;
Residuals:&#13;
     Min       1Q   Median       3Q      Max&#13;
-22.3797  -6.1323  -0.1973   5.9633  22.1723&#13;
Coefficients:&#13;
            Estimate Std. Error t value Pr(&gt;|t|)&#13;
(Intercept)  4.91948    1.99064   2.471   0.0152 *&#13;
X            0.49093    0.03453  14.218   &lt;2e-16 ***&#13;
---&#13;
Signif. codes:&#13;
0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&#13;
Residual standard error: 9.693 on 98 degrees of freedom&#13;
Multiple R-squared:  0.6735,  Adjusted R-squared:  0.6702&#13;
F-statistic: 202.2 on 1 and 98 DF,  p-value: &lt; 2.2e-16</pre><p class="list-inset">Here, we use <a id="_idIndexMarker1142"/>the <strong class="source-inline">lm()</strong> function to fit the data, where <strong class="source-inline">lm</strong> stands for “linear model.” This function creates our SLR model. The <strong class="source-inline">Y ~ X</strong> syntax is how we specify our model: it tells the function that <strong class="source-inline">Y</strong> is being modeled as a function <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">X</strong></span><span class="No-Break">.</span></p><p class="list-inset">The <strong class="source-inline">summary()</strong> function provides a comprehensive overview of the model, including the estimated coefficients, the standard errors, the t-values, and the p-values, among other statistics. Since the resulting p-value is extremely low, we can conclude that the input variable is predictive with strong <span class="No-Break">statistical significance.</span></p></li>				<li>Use the <strong class="source-inline">plot()</strong> and <strong class="source-inline">abline()</strong> functions to visualize the data and the fitted <span class="No-Break">regression line:</span><pre class="source-code">&#13;
# Plot the data&#13;
plot(data$X, data$Y, main = "Simple Linear Regression", xlab = "X", ylab = "Y")&#13;
# Add the fitted regression line&#13;
abline(model, col = "red")</pre><p class="list-inset">Running this code generates <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer218" class="IMG---Figure">&#13;
					<img src="Images/B18680_12_002.jpg" alt="Figure 12.2 – Visualizing the data and the fitted regression line" width="949" height="801"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Visualizing the data and the fitted regression line</p>&#13;
			<p class="list-inset">Here, the <strong class="source-inline">plot()</strong> function<a id="_idIndexMarker1143"/> creates a scatter plot of our data, and the <strong class="source-inline">abline()</strong> function adds the regression line to this plot. Such a visual representation is very useful for understanding the quality of <span class="No-Break">the fitting.</span></p>&#13;
			<p>We’ll move on to the <strong class="bold">multiple linear regression</strong> (<strong class="bold">MLR</strong>) model in the <span class="No-Break">next section.</span></p>&#13;
			<h2 id="_idParaDest-256"><a id="_idTextAnchor262"/>Introducing multiple linear regression</h2>&#13;
			<p>MLR<a id="_idIndexMarker1144"/> expands the<a id="_idIndexMarker1145"/> single predictor in SLR to predict the target outcome based on multiple predictor variables. Here, the term “multiple” in MLR refers to the multiple predictors in the model, where each feature is given a coefficient. A specific coefficient, <span class="_-----MathTools-_Math_Variable">β</span>, represents the change in the outcome variable for a single unit change in the associated predictor variable, assuming all other predictors are <span class="No-Break">held constant.</span></p>&#13;
			<p>One of the great advantages of MLR is its ability to include multiple predictors, allowing for a more complex and realistic (linear) representation of the real world. It can provide a holistic view of the connection between the target and all input variables. This is particularly useful in fields where the outcome variable is likely influenced by more than one predictor variable. It is modeled via the <span class="No-Break">following formula:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span> <span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span></p>&#13;
			<p>Here, we have a total of <span class="_-----MathTools-_Math_Variable">p</span> features, and therefore, <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">)</span> coefficients due to the intercept term. <span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span> is the usual noise term that represents the unexplained part. In other words, our prediction using MLR is <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span> <span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span></p>&#13;
			<p>We can perform ceteris paribus analysis with this formulation, which is a Latin way of saying all other things are equal, and we only change one input variable to assess its impact on the outcome variable. In other words, MLR allows us to explicitly control (that is, keep unchanged) many other factors that simultaneously affect the target variable and observe the impact of only <span class="No-Break">one factor.</span></p>&#13;
			<p>For example, suppose we add a small increment, <span class="_-----MathTools-_Math_Variable_v-normal">Δ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span>, to the feature, <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span>, and keep all other features unchanged. The new prediction, <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">w</span>, is obtained by the <span class="No-Break">following formula:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">j</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">Δ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span> <span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span></p>&#13;
			<p>We know that<a id="_idIndexMarker1146"/> the original prediction is <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span> <span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span></p>&#13;
			<p>The difference<a id="_idIndexMarker1147"/> between these two gives us the change in the <span class="No-Break">output variable:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable_v-normal">Δ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">Δ</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">j</span></span></p>&#13;
			<p>What we are doing here is essentially controlling all other input variables but only bumping <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span> to see the impact on the prediction, <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span>. So, the coefficient, <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">j</span>, measures the sensitivity of the outcome to a specific feature. When we have a unit change, with <span class="_-----MathTools-_Math_Variable_v-normal">Δ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>, the<a id="_idIndexMarker1148"/> change is exactly the coefficient itself, giving <a id="_idIndexMarker1149"/>us <span class="_-----MathTools-_Math_Variable_v-normal">Δ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">β</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">j</span></span><span class="No-Break">.</span></p>&#13;
			<p>The next section discusses the measure of the predictiveness of the <span class="No-Break">MLR model.</span></p>&#13;
			<h2 id="_idParaDest-257"><a id="_idTextAnchor263"/>Seeking a higher coefficient of determination</h2>&#13;
			<p>MLR tends<a id="_idIndexMarker1150"/> to perform better than SLR due to the multiple predictors used in the model, such as a higher coefficient of determination (<span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>). However, a regression model with more input variables and a higher <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> does not necessarily mean that the model is a better fit and can predict better for the <span class="No-Break">test set.</span></p>&#13;
			<p>A higher <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>, as a result of more input features, could likely be due to overfitting. Overfitting occurs when a model is excessively complex, including too many predictors or even interaction terms between predictors. In such cases, the model may fit the observed data well (thus leading to a high <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>), but it may perform poorly when applied to new, unseen test data. This is because the model might have learned not only the underlying structure of the training data but also the random noise specific to <span class="No-Break">the dataset.</span></p>&#13;
			<p>Let’s look at the metric of <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> more closely. While <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> measures how well the model explains the variance in the outcome variable, it has a major limitation: it tends to get bigger as more predictors enter the model, even if those predictors are irrelevant. As a remedy, we can use the adjusted <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>. Unlike <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>, the adjusted <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> explicitly considers the number of predictors and adjusts the resulting statistic accordingly. If a predictor improves the model substantially, the adjusted <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> will increase, but if a predictor does not improve the model by a significant amount, the adjusted <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> may <span class="No-Break">even decrease.</span></p>&#13;
			<p>When building statistical models, simpler models are usually preferred when they perform similarly to more complex models. This principle of parsimony, also known as Occam’s razor, suggests that among models with similar predictive power, the simplest one should be chosen. In other words, adding more predictors to the model makes it more complex, harder to interpret, and more likely <span class="No-Break">to overfit.</span></p>&#13;
			<h2 id="_idParaDest-258"><a id="_idTextAnchor264"/>More on adjusted <span class="_-----MathTools-_Math_Variable_v-bold-italic">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span></h2>&#13;
			<p>The adjusted <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> improves<a id="_idIndexMarker1151"/> upon <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> by adjusting for the number of features in the selected model. Specifically, the value of the adjusted <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> only increases if adding this feature is worth more than what would have been expected from adding a random feature. Essentially, the additional predictors that are added to the model must be meaningful and predictive to lead to a higher adjusted <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>. These additional predictors, however, would always increase <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> when added to <span class="No-Break">th<a id="_idTextAnchor265"/>e model.</span></p>&#13;
			<p>The adjusted <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> addresses this issue by incorporating the model’s degree of freedom. Here, the degree of freedom refers to the number of values in a statistical calculation that is free to vary. In the context of regression models, this typically means the number of predictors. The adjusted <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> can be expressed <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable_v-normal">A</span><span class="_-----MathTools-_Math_Variable_v-normal">d</span><span class="_-----MathTools-_Math_Variable_v-normal">j</span><span class="_-----MathTools-_Math_Variable_v-normal">u</span><span class="_-----MathTools-_Math_Variable_v-normal">s</span><span class="_-----MathTools-_Math_Variable_v-normal">t</span><span class="_-----MathTools-_Math_Variable_v-normal">e</span><span class="_-----MathTools-_Math_Variable_v-normal">d</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span></p>&#13;
			<p>Here, <span class="_-----MathTools-_Math_Variable">n</span> denotes the number of observations and <span class="_-----MathTools-_Math_Variable">p</span> represents the number of features in <span class="No-Break">the model.</span></p>&#13;
			<p>The formula works by adjusting the scale of <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> based on the count of observations and predictors. The term <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> is a ratio that reflects the degrees of freedom in the model, where <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">)</span> represents the total degrees of freedom in the model. We subtract by 1 because we are estimating the mean of the dependent variable from the data. <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">)</span> represents the degrees of freedom for the error, which, in turn, represents the number of observations left over after estimating the model parameters. The whole term, <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>, denotes the error variance that’s been adjusted for the count <span class="No-Break">of predictors.</span></p>&#13;
			<p>Subtracting this<a id="_idIndexMarker1152"/> from 1 results in the proportion of the total variance explained by the model, after adjusting for the number of predictors in the model. In other words, it’s a version of <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> that penalizes the addition of unnecessary predictors. This helps to prevent overfitting and makes adjusted <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> a more balanced measure of a model’s explanatory power when comparing models with different numbers <span class="No-Break">of predictors.</span></p>&#13;
			<p>Let’s look at how to develop an MLR model <span class="No-Break">in R.</span></p>&#13;
			<h2 id="_idParaDest-259"><a id="_idTextAnchor266"/>Developing an MLR model</h2>&#13;
			<p>In this section, we will develop an <a id="_idIndexMarker1153"/>MLR model using the same <strong class="source-inline">lm()</strong> function in R based on the <strong class="source-inline">mtcars</strong> dataset, which comes preloaded with R and was used in previous exercises. Again, the <strong class="source-inline">mtcars</strong> dataset contains measurements for 32 vehicles from a 1974 Motor Trend issue. These measurements include attributes such as miles per gallon (<strong class="source-inline">mpg</strong>), number of cylinders (<strong class="source-inline">cyl</strong>), horsepower (<strong class="source-inline">hp</strong>), and <span class="No-Break">weight (</span><span class="No-Break"><strong class="source-inline">wt</strong></span><span class="No-Break">).</span></p>&#13;
			<h3>Exercise 12.2 – building an MLR model</h3>&#13;
			<p>In this exercise, we will develop an<a id="_idIndexMarker1154"/> MLR model to predict <strong class="source-inline">mpg</strong> using <strong class="source-inline">cyl</strong>, <strong class="source-inline">hp</strong>, and <strong class="source-inline">wt</strong>. We will then interpret the <span class="No-Break">model results:</span></p>&#13;
			<ol>&#13;
				<li>Load the <strong class="source-inline">mtcars</strong> dataset and build an MLR that predicts <strong class="source-inline">mpg</strong> based on <strong class="source-inline">cyl</strong>, <strong class="source-inline">hp</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">wt</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
# Load the data&#13;
data(mtcars)&#13;
# Build the model&#13;
model = lm(mpg ~ cyl + hp + wt, data = mtcars)</pre><p class="list-inset">Here, we first load the <strong class="source-inline">mtcars</strong> dataset using the <strong class="source-inline">data()</strong> function, then construct the MLR model using the <strong class="source-inline">lm()</strong> function. The <strong class="source-inline">mpg ~ cyl + hp + wt</strong> formula is used to specify the model. This formula tells R that we want to model <strong class="source-inline">mpg</strong> as a function of <strong class="source-inline">cyl</strong>, <strong class="source-inline">hp</strong>, and <strong class="source-inline">wt</strong>. The <strong class="source-inline">data = mtcars</strong> argument tells R to look for these variables in the <strong class="source-inline">mtcars</strong> dataset. The <strong class="source-inline">lm()</strong> function fits the model to the data and returns a model object, which we store in the <span class="No-Break">variable model.</span></p></li>				<li>View the summary of <span class="No-Break">the model:</span><pre class="source-code">&#13;
# Print the summary of the model&#13;
&gt;&gt;&gt; summary(model)&#13;
Call:&#13;
lm(formula = mpg ~ cyl + hp + wt, data = mtcars)&#13;
Residuals:&#13;
    Min      1Q  Median      3Q     Max&#13;
-3.9290 -1.5598 -0.5311  1.1850  5.8986&#13;
Coefficients:&#13;
            Estimate Std. Error t value Pr(&gt;|t|)&#13;
(Intercept) 38.75179    1.78686  21.687  &lt; 2e-16 ***&#13;
cyl         -0.94162    0.55092  -1.709 0.098480 .&#13;
hp          -0.01804    0.01188  -1.519 0.140015&#13;
wt          -3.16697    0.74058  -4.276 0.000199 ***&#13;
---&#13;
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&#13;
Residual standard error: 2.512 on 28 degrees of freedom&#13;
Multiple R-squared:  0.8431,  Adjusted R-squared:  0.8263&#13;
F-statistic: 50.17 on 3 and 28 DF,  p-value: 2.184e-11</pre><p class="list-inset">The summary <a id="_idIndexMarker1155"/>includes the model’s coefficients (the intercept and the slopes for each predictor), the residuals (differences between the actual observations and predicted values for the target), and several statistics that tell us how well the model fits the data, including <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> and the adjusted <span class="No-Break"><span class="_-----MathTools-_Math_Variable">R</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break">.</span></p><p class="list-inset">Let’s interpret the output. Each coefficient represents the expected change in <strong class="source-inline">mpg</strong> for a single unit increase in the associated predictor, assuming all other predictors are constant. The <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> value, which is <strong class="source-inline">0.8431</strong>, denotes the proportion of variance (over 84%) in <strong class="source-inline">mpg</strong> that can be explained by the predictors together. Again, the adjusted <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> value, which is <strong class="source-inline">0.8263</strong>, is a modified <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> that accounts for the number of features in <span class="No-Break">the model.</span></p><p class="list-inset">In addition, the p-values for each predictor test the null hypothesis that the true value of the coefficient is zero. If a predictor’s p-value is smaller than a preset significance level (such as 0.05), we would reject this null hypothesis and conclude that the predictor is statistically significant. In this case, <strong class="source-inline">wt</strong> is the only statistically significant factor compared with others using a significance level <span class="No-Break">of 5%.</span></p></li>			</ol>&#13;
			<p>In the MLR model, all<a id="_idIndexMarker1156"/> coefficients are negative, indicating a reverse direction of travel between the input variable and the target. However, we cannot conclude that all the predictors negatively correlate with the target variable. The correlation between the individual predictor and the target variable could be positive or negative <span class="No-Break">in SLR.</span></p>&#13;
			<p>The next section provides more context on <span class="No-Break">this phenomenon.</span></p>&#13;
			<h2 id="_idParaDest-260"><a id="_idTextAnchor267"/>Introducing Simpson’s Paradox</h2>&#13;
			<p>Simpson’s Paradox says<a id="_idIndexMarker1157"/> that a trend appears in different data groups but disappears or changes when combined. In the context of regression analysis, Simpson’s Paradox can appear when a variable that seems positively correlated with the outcome might be negatively correlated when we control <span class="No-Break">other variables.</span></p>&#13;
			<p>Essentially, this paradox illustrates the importance of considering confounding variables and not drawing conclusions from aggregated data without understanding the context. The confounding variables are those not among the explanatory variables under consideration but are related to both the target variable and <span class="No-Break">the predictors.</span></p>&#13;
			<p>Let’s consider a simple example through the <span class="No-Break">following exercise.</span></p>&#13;
			<h3>Exercise 12.3 – illustrating Simpson’s Paradox</h3>&#13;
			<p>In this exercise, we will<a id="_idIndexMarker1158"/> look at two scenarios with opposite signs of coefficient values for the same feature in both SLR <span class="No-Break">and MLR:</span></p>&#13;
			<ol>&#13;
				<li>Create a dummy dataset with two predictors and one <span class="No-Break">output variable:</span><pre class="source-code">&#13;
set.seed(123)&#13;
x1 = rnorm(100)&#13;
x2 = -3 * x1 + rnorm(100)&#13;
y = 2 + x1 + x2 + rnorm(100)&#13;
df = data.frame(y = y, x1 = x1, x2 = x2)</pre><p class="list-inset">Here, <strong class="source-inline">x1</strong> is a set of 100 numbers randomly generated from a standard normal distribution. <strong class="source-inline">x2</strong> is a linear function of <strong class="source-inline">x1</strong> but with a negative correlation, and some random noise is added (via <strong class="source-inline">rnorm(100)</strong>). <strong class="source-inline">y</strong> is then generated as a linear function of <strong class="source-inline">x1</strong> and <strong class="source-inline">x2</strong>, again with some random noise added. All three variables are compiled into a <span class="No-Break">DataFrame, </span><span class="No-Break"><strong class="source-inline">df</strong></span><span class="No-Break">.</span></p></li>				<li>Train an<a id="_idIndexMarker1159"/> SLR model with <strong class="source-inline">y</strong> as the outcome and <strong class="source-inline">x1</strong> as the input features. Check the summary of <span class="No-Break">the model:</span><pre class="source-code">&#13;
# Single linear regression&#13;
single_reg = lm(y ~ x1, data = df)&#13;
&gt;&gt;&gt; summary(single_reg)&#13;
Call:&#13;
lm(formula = y ~ x1, data = df)&#13;
Residuals:&#13;
    Min      1Q  Median      3Q     Max&#13;
-2.7595 -0.8365 -0.0564  0.8597  4.3211&#13;
Coefficients:&#13;
            Estimate Std. Error t value Pr(&gt;|t|)&#13;
(Intercept)   2.0298     0.1379   14.72   &lt;2e-16 ***&#13;
x1           -2.1869     0.1511  -14.47   &lt;2e-16 ***&#13;
---&#13;
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&#13;
Residual standard error: 1.372 on 98 degrees of freedom&#13;
Multiple R-squared:  0.6813,  Adjusted R-squared:  0.678&#13;
F-statistic: 209.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16</pre><p class="list-inset">The result shows that <strong class="source-inline">x1</strong> is negatively correlated with <strong class="source-inline">y</strong> due to a negative coefficient <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">-2.1869</strong></span><span class="No-Break">.</span></p></li>				<li>Train an <a id="_idIndexMarker1160"/>SLR model with <strong class="source-inline">y</strong> as the target and <strong class="source-inline">x1</strong> and <strong class="source-inline">x2</strong> as the input features. Check the summary of <span class="No-Break">the model:</span><pre class="source-code">&#13;
# Multiple linear regression&#13;
multi_reg = lm(y ~ x1 + x2, data = df)&#13;
&gt;&gt;&gt; summary(multi_reg)&#13;
Call:&#13;
lm(formula = y ~ x1 + x2, data = df)&#13;
Residuals:&#13;
    Min      1Q  Median      3Q     Max&#13;
-1.8730 -0.6607 -0.1245  0.6214  2.0798&#13;
Coefficients:&#13;
            Estimate Std. Error t value Pr(&gt;|t|)&#13;
(Intercept)  2.13507    0.09614  22.208  &lt; 2e-16 ***&#13;
x1           0.93826    0.31982   2.934  0.00418 **&#13;
x2           1.02381    0.09899  10.342  &lt; 2e-16 ***&#13;
---&#13;
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&#13;
Residual standard error: 0.9513 on 97 degrees of freedom&#13;
Multiple R-squared:  0.8484,  Adjusted R-squared:  0.8453&#13;
F-statistic: 271.4 on 2 and 97 DF,  p-value: &lt; 2.2e-16</pre><p class="list-inset">The result shows that the estimated coefficient for <strong class="source-inline">x1</strong> is now a positive quantity. Does this suggest that <strong class="source-inline">x1</strong> is suddenly positively correlated with <strong class="source-inline">y</strong>? No, since there are likely other confounding variables that lead to a <span class="No-Break">positive coefficient.</span></p></li>			</ol>&#13;
			<p>The key takeaway is<a id="_idIndexMarker1161"/> that we can only make inferences on the (positive or negative) correlation between a predictor and a target outcome in an SLR setting. For example, if we build an SLR model to regress <strong class="source-inline">y</strong> against <strong class="source-inline">x</strong>, we can conclude that <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> are positively correlated if the resulting coefficient is positive (<span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span>). Similarly, if <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span>, we can conclude that <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> are positively correlated. The same applies to the case of <span class="No-Break">negative correlation.</span></p>&#13;
			<p>However, such inference breaks in an MLR setting – that is, we cannot conclude a positive correlation if <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span>, and <span class="No-Break">vice versa.</span></p>&#13;
			<p>Let’s take this opportunity to interpret the results. The <strong class="source-inline">Estimate</strong> column shows the estimated regression coefficients. These values indicate how much the <strong class="source-inline">y</strong> variable is expected to increase when the corresponding predictor variable increases by one unit while holding all other features constant. In this case, for each unit increase in <strong class="source-inline">x1</strong>, <strong class="source-inline">y</strong> is expected to increase by approximately <strong class="source-inline">0.93826</strong> units, and for each unit increase in <strong class="source-inline">x2</strong>, <strong class="source-inline">y</strong> is expected to increase by approximately <strong class="source-inline">1.02381</strong> units. The <strong class="source-inline">(Intercept)</strong> row shows the estimated value of <strong class="source-inline">y</strong> when all predictor variables in the model are zero. In this model, the estimated intercept <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">2.13507</strong></span><span class="No-Break">.</span></p>&#13;
			<p><strong class="source-inline">Std. Error</strong> represents the standard errors for the estimates. Smaller values here indicate more precise estimates. The <strong class="source-inline">t value</strong> column shows the t-statistics for the hypothesis test that the corresponding population regression coefficient is zero, given that the other predictors are in the model. A larger absolute value of the t-statistic indicates stronger evidence against the null hypothesis. The <strong class="source-inline">Pr(&gt;|t|)</strong> column gives the p-values for the hypothesis tests. In this case, both <strong class="source-inline">x1</strong> and <strong class="source-inline">x2</strong> have p-values below <strong class="source-inline">0.05</strong>, indicating that both are statistically significant predictors of <strong class="source-inline">y</strong> at the 5% <span class="No-Break">significance level.</span></p>&#13;
			<p>Finally, the multiple R-squared and adjusted R-squared values provide measures of how well the model fits the data. The multiple R-squared value is <strong class="source-inline">0.8484</strong>, indicating that this model explains approximately 84.84% of the variability in <strong class="source-inline">y</strong>. The adjusted R-squared value adjusts this measure for the number of features in the model. As discussed, it is a better measure when comparing models with different numbers of predictors. Here, the adjusted R-squared value is <strong class="source-inline">0.8453</strong>. The F-statistic and its associated p-value are used to test the hypothesis that all population regression coefficients are zero. A small p-value (less than <strong class="source-inline">0.05</strong>) indicates that we can reject this hypothesis, and conclude that at least one of the predictors is useful in <span class="No-Break">predicting </span><span class="No-Break"><strong class="source-inline">y</strong></span><span class="No-Break">.</span></p>&#13;
			<p>The next section looks at the situation when we have a categorical predictor in the <span class="No-Break">MLR model.</span></p>&#13;
			<h2 id="_idParaDest-261"><a id="_idTextAnchor268"/>Working with categorical variables</h2>&#13;
			<p>In MLR, the process of including <a id="_idIndexMarker1162"/>a binary predictor is similar to including a numeric predictor. However, the interpretation differs. Consider a dataset where <span class="_-----MathTools-_Math_Variable">y</span> is the target variable, <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> is a numeric predictor, and <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> is a <span class="No-Break">binary predictor:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span></p>&#13;
			<p>In this model, <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> is coded as 0 and 1, and its corresponding coefficient, <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>, represents the difference in the mean values of <span class="_-----MathTools-_Math_Variable">y</span> between the two groups identified by <span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break">.</span></p>&#13;
			<p>For example, if <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> is a binary variable representing sex (0 for males, 1 for females), and <span class="_-----MathTools-_Math_Variable">y</span> is salary, then <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> represents the average difference in salary between females and males, after accounting for the value of <span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">1</span></span><span class="No-Break">.</span></p>&#13;
			<p>Note that the interpretation of the coefficient of a binary predictor is dependent on the other variables in the model. So, in the preceding example, <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> is the difference in salary between females and males for given values of <span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">1</span></span><span class="No-Break">.</span></p>&#13;
			<p>On the implementation side, R automatically creates dummy variables when a factor is used in a regression model. So, if <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> were a factor with levels of “male” and “female,” R would handle the conversion to 0 and 1 internally when fitting <span class="No-Break">the model.</span></p>&#13;
			<p>Let’s look at a <a id="_idIndexMarker1163"/>concrete example. In the following code, we’re building an MLR model to predict <strong class="source-inline">mpg</strong> using <strong class="source-inline">qsec</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">am</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
# Fit the model&#13;
model &lt;- lm(mpg ~ qsec + am, data = mtcars)&#13;
# Display the summary of the model&#13;
&gt;&gt;&gt; summary(model)&#13;
Call:&#13;
lm(formula = mpg ~ qsec + am, data = mtcars)&#13;
Residuals:&#13;
    Min      1Q  Median      3Q     Max&#13;
-6.3447 -2.7699  0.2938  2.0947  6.9194&#13;
Coefficients:&#13;
            Estimate Std. Error t value Pr(&gt;|t|)&#13;
(Intercept) -18.8893     6.5970  -2.863  0.00771 **&#13;
qsec          1.9819     0.3601   5.503 6.27e-06 ***&#13;
am            8.8763     1.2897   6.883 1.46e-07 ***&#13;
---&#13;
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&#13;
Residual standard error: 3.487 on 29 degrees of freedom&#13;
Multiple R-squared:  0.6868,  Adjusted R-squared:  0.6652&#13;
F-statistic:  31.8 on 2 and 29 DF,  p-value: 4.882e-08</pre>			<p>Note that the <strong class="source-inline">am</strong> variable <a id="_idIndexMarker1164"/>is treated as a numeric variable. Since it represents the type of transmission in the car (<strong class="source-inline">0</strong> = automatic, <strong class="source-inline">1</strong> = manual), it should have been treated as a categorical variable. This can be achieved by converting it into a factor, as <span class="No-Break">shown here:</span></p>&#13;
			<pre class="source-code">&#13;
# Convert am to categorical var&#13;
mtcars$am_cat = as.factor(mtcars$am)&#13;
# Fit the model&#13;
model &lt;- lm(mpg ~ qsec + am_cat, data = mtcars)&#13;
# Display the summary of the model&#13;
&gt;&gt;&gt; summary(model)&#13;
Call:&#13;
lm(formula = mpg ~ qsec + am_cat, data = mtcars)&#13;
Residuals:&#13;
    Min      1Q  Median      3Q     Max&#13;
-6.3447 -2.7699  0.2938  2.0947  6.9194&#13;
Coefficients:&#13;
            Estimate Std. Error t value Pr(&gt;|t|)&#13;
(Intercept) -18.8893     6.5970  -2.863  0.00771 **&#13;
qsec          1.9819     0.3601   5.503 6.27e-06 ***&#13;
am_cat1       8.8763     1.2897   6.883 1.46e-07 ***&#13;
---&#13;
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&#13;
Residual standard error: 3.487 on 29 degrees of freedom&#13;
Multiple R-squared:  0.6868,  Adjusted R-squared:  0.6652&#13;
F-statistic:  31.8 on 2 and 29 DF,  p-value: 4.882e-08</pre>			<p>Note that only one variable, <strong class="source-inline">am_cat1</strong>, is created for the categorical variable, <strong class="source-inline">am_cat</strong>. This is because <strong class="source-inline">am_cat</strong> is binary, thus we only need one dummy column (keeping only <strong class="source-inline">am_cat1</strong> and removing <strong class="source-inline">am_cat0</strong> in this case) to represent the original categorical variable. In general, for a categorical variable with <span class="_-----MathTools-_Math_Variable">k</span> categorical, R will automatically create (<span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>) dummy variables in <span class="No-Break">the model.</span></p>&#13;
			<p>This process<a id="_idIndexMarker1165"/> is called <strong class="bold">one-hot encoding</strong>, which involves converting categorical data into a format that can be used by the regression model. For categorical <a id="_idIndexMarker1166"/>variables where there is no ordinal relationship, the integer encoding (as originally used) may not be sufficient. It could even be misleading to some extent. For these cases, one-hot encoding can be applied, where each unique original value is replaced with a binary variable. Specifically, each of these new variables (also known as “dummy” variables) would have a value of 1 for the observations where <strong class="source-inline">am</strong> was equal to the corresponding level, and 0 otherwise. This essentially creates a set of indicators that capture the presence or absence of each category. Finally, since we can infer the last dummy variable based on the values of the previous (<strong class="source-inline">k-1</strong>) dummy variables, we can remove the last dummy variable in the resulting one-hot encoded set <span class="No-Break">of variables.</span></p>&#13;
			<p>Using a categorical variable introduces a vertical shift to the model estimate, as discussed in the following section. To see this, let’s look more closely at the impact of the categorical variable, <strong class="source-inline">am_cat1</strong>. Our MLR model now assumes the <span class="No-Break">following form:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">q</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">a</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">m</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">_</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">c</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">a</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">t</span></span></p>&#13;
			<p>We know that <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">t</span> is a binary variable. When <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span>, the prediction becomes <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">q</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">s</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">e</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">c</span></span></p>&#13;
			<p>When <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>, the prediction is <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">q</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">q</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">s</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">e</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">c</span></span></p>&#13;
			<p>By comparing these two quantities, we can see that they are two linear models parallel to each other since the slope is the same and the only difference is <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> in the <span class="No-Break">intercept term.</span></p>&#13;
			<p>A <a id="_idIndexMarker1167"/>visual illustration helps here. In the following code snippet, we first create a new DataFrame, <strong class="source-inline">newdata</strong>, that covers the range of <strong class="source-inline">qsec</strong> values in the original data, for each of the <strong class="source-inline">am_cat</strong> values (0 and 1). Then, we use the <strong class="source-inline">predict()</strong> function to get the predicted <strong class="source-inline">mpg</strong> values from the model for this new data. Next, we plot the original data points with <strong class="source-inline">geom_point()</strong> and add two regression lines with <strong class="source-inline">geom_line()</strong>, where the lines are based on the predicted values in <strong class="source-inline">newdata</strong>. The <strong class="source-inline">color = am_cat</strong> aesthetic setting adds different colors to the points and lines for the different <strong class="source-inline">am_cat</strong> values, and the labels are adjusted in <strong class="source-inline">scale_color_discrete()</strong> so that 0 corresponds to “Automatic” and 1 corresponds <span class="No-Break">to “Manual”:</span></p>&#13;
			<pre class="source-code">&#13;
# Load required library&#13;
library(ggplot2)&#13;
# Create new data frame for the predictions&#13;
newdata = data.frame(qsec = seq(min(mtcars$qsec), max(mtcars$qsec), length.out = 100),&#13;
                      am_cat = c(rep(0, 100), rep(1, 100)))&#13;
newdata$am_cat = as.factor(newdata$am_cat)&#13;
# Get predictions&#13;
newdata$mpg_pred = predict(model, newdata)&#13;
# Plot the data and the regression lines&#13;
ggplot(data = mtcars, aes(x = qsec, y = mpg, color = am_cat)) +&#13;
  geom_point() +&#13;
  geom_line(data = newdata, aes(y = mpg_pred)) +&#13;
  labs(title = "mpg vs qsec by Transmission Type",&#13;
       x = "Quarter Mile Time (qsec)",&#13;
       y = "Miles per Gallon (mpg)",&#13;
       color = "Transmission Type") +&#13;
  scale_color_discrete(labels = c("Automatic", "Manual")) +&#13;
  theme(text = element_text(size = 16),  # Default text size&#13;
        title = element_text(size = 15),  # Title size&#13;
        axis.title = element_text(size = 18),  # Axis title size&#13;
        legend.title = element_text(size = 16),  # Legend title size&#13;
        legend.text = element_text(size = 16), # Legend text size&#13;
        legend.position = "bottom")  # Legend position</pre>			<p>Running this code generates <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>&#13;
			<p class="IMG---Figure"> </p>&#13;
			<div>&#13;
				<div id="_idContainer219" class="IMG---Figure">&#13;
					<img src="Images/B18680_12_003.jpg" alt="Figure 12.3 – Visualizing the two linear regression models based on different transmission types. These two lines are parallel to each other due to a shift in the intercept term" width="621" height="499"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Visualizing the two linear regression models based on different transmission types. These two lines are parallel to each other due to a shift in the intercept term</p>&#13;
			<p>What this figure suggests<a id="_idIndexMarker1168"/> is that manual transmission cars have the same miles per gallon (<strong class="source-inline">mpg</strong>) more than automatic transmission cars given the same quarter-mile time (<strong class="source-inline">qsec</strong>). However, this is unlikely in practice since different car types (manual versus automatic) will likely have different quarter-mile times. In other words, there is an interaction effect between these <span class="No-Break">two variables.</span></p>&#13;
			<p>The following section introduces the interaction term as a remedy to <span class="No-Break">this situation.</span></p>&#13;
			<h2 id="_idParaDest-262"><a id="_idTextAnchor269"/>Introducing the interaction term</h2>&#13;
			<p>In regression analysis, an interaction<a id="_idIndexMarker1169"/> occurs when the effect of one predictor on the target variable differs depending on the level of another predictor variable. In our running example, we are essentially looking at whether the relationship between <strong class="source-inline">mpg</strong> and <strong class="source-inline">qsec</strong> is different for different values of <strong class="source-inline">am</strong>. In other words, we are testing whether the slope of the line relating <strong class="source-inline">mpg</strong> and <strong class="source-inline">qsec</strong> differs for manual (<strong class="source-inline">am</strong>=1) and automatic (<span class="No-Break"><strong class="source-inline">am</strong></span><span class="No-Break">=0) transmissions.</span></p>&#13;
			<p>For example, if there is no interaction effect, then the impact of <strong class="source-inline">qsec</strong> on <strong class="source-inline">mpg</strong> is the same, regardless of whether the car has a manual or automatic transmission. This would mean that the lines depicting the relationship between <strong class="source-inline">mpg</strong> and <strong class="source-inline">qsec</strong> for manual and automatic cars would <span class="No-Break">be parallel.</span></p>&#13;
			<p>If there is an interaction effect, then the effect of <strong class="source-inline">qsec</strong> on <strong class="source-inline">mpg</strong> differs for manual and automatic cars. This would mean that the lines depicting the relationship between <strong class="source-inline">mpg</strong> and <strong class="source-inline">qsec</strong> for manual and automatic cars would not be parallel. They could either cross or, more commonly, just have <span class="No-Break">different slopes.</span></p>&#13;
			<p>To depict these differences in <a id="_idIndexMarker1170"/>relationships, we can add an interaction term to the model, which is done using the <strong class="source-inline">*</strong> operator. For example, the formula for a regression model with an interaction between <strong class="source-inline">qsec</strong> and <strong class="source-inline">am_cat</strong> would be <strong class="source-inline">mpg ~ qsec * am_cat</strong>. This is equivalent to <strong class="source-inline">mpg ~ qsec + am</strong><strong class="source-inline">_cat + qsec:am_cat</strong>, where <strong class="source-inline">qsec:am_cat</strong> represents the interaction term. The following code shows <span class="No-Break">the details:</span></p>&#13;
			<pre class="source-code">&#13;
# Adding interaction term&#13;
model_interaction &lt;- lm(mpg ~ qsec * am_cat, data = mtcars)&#13;
# Print model summary&#13;
&gt;&gt;&gt; summary(model_interaction)&#13;
Call:&#13;
lm(formula = mpg ~ qsec * am_cat, data = mtcars)&#13;
Residuals:&#13;
    Min      1Q  Median      3Q     Max&#13;
-6.4551 -1.4331  0.1918  2.2493  7.2773&#13;
Coefficient s:&#13;
             Estimate Std. Error t value Pr(&gt;|t|)&#13;
(Intercept)   -9.0099     8.2179  -1.096  0.28226&#13;
qsec           1.4385     0.4500   3.197  0.00343 **&#13;
am_cat1      -14.5107    12.4812  -1.163  0.25481&#13;
qsec:am_cat1   1.3214     0.7017   1.883  0.07012 .&#13;
Signif. code s:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&#13;
Residual standard error: 3.343 on 28 degrees of freedom&#13;
Multiple R-squared:  0.722,  Adjusted R-squared:  0.6923&#13;
F-statistic: 24.24 on 3 and 28 DF,  p-value: 6.129e-08</pre>			<p>Let’s also plot the updated <a id="_idIndexMarker1171"/>model, which consists of two intersecting lines due to the interaction effect. In the following code snippet, <strong class="source-inline">geom_smooth(method =""l"", se = FALSE)</strong> is used to fit different linear lines to each group of points (automatic and manual cars). <strong class="source-inline">as.factor(am_cat)</strong> is used to treat <strong class="source-inline">am_cat</strong> as a factor (categorical) variable so that a separate line is fit for <span class="No-Break">each category:</span></p>&#13;
			<pre class="source-code">&#13;
# Create scatter plot with two intersecting lines&#13;
ggplot(mtcars, aes(x = qsec, y = mpg, color = as.factor(am_cat))) +&#13;
  geom_point() +&#13;
  geom_smooth(method =""l"", se = FALSE) + # fit separate regression lines per group&#13;
  scale_color_discrete(name =""Transmission Typ"",&#13;
                       labels = c""Automati"",""Manua"")) +&#13;
  labs(x =""Quarter mile time (seconds"",&#13;
       y =""Miles per gallo"",&#13;
       title =""Separate regression lines fit for automatic and manual car"") +&#13;
  theme(text = element_text(size = 16),&#13;
        title = element_text(size = 15),&#13;
        axis.title = element_text(size = 20),&#13;
        legend.title = element_text(size = 16),&#13;
        legend.text = element_text(size = 16))</pre>			<p>Running this code generates <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer220" class="IMG---Figure">&#13;
					<img src="Images/B18680_12_004.jpg" alt="Figure 12.4 – Two intersecting lines due to the intersection term between quarter-mile time and transmission type" width="1168" height="814"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Two intersecting lines due to the intersection term between quarter-mile time and transmission type</p>&#13;
			<p>The next section focuses on another related topic: <span class="No-Break">nonlinear terms.</span></p>&#13;
			<h2 id="_idParaDest-263"><a id="_idTextAnchor270"/>Handling nonlinear terms</h2>&#13;
			<p>Linear regression is a widely used<a id="_idIndexMarker1172"/> model for understanding the linear relationships between a response and explanatory variables. However, not all underlying relationships in the data are linear. In many situations, a feature and a response variable may not have a straight-line relationship, thus necessitating the handling of nonlinear terms in the linear regression model to increase <span class="No-Break">its flexibility.</span></p>&#13;
			<p>The simplest way to incorporate nonlinearity, and therefore build a curve instead of a straight line, is by including polynomial terms of predictors in the regression model. In polynomial regression, some or all predictors are raised to a specific polynomial term – for example, transforming a feature, <span class="_-----MathTools-_Math_Variable">x</span>, into <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> or <span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">3</span></span><span class="No-Break">.</span></p>&#13;
			<p>Let’s go through an exercise to understand the impact of adding polynomial features to a linear <span class="No-Break">regression model.</span></p>&#13;
			<h3>Exercise 12.4 – adding polynomial features to a linear regression model</h3>&#13;
			<p>In this exercise, we <a id="_idIndexMarker1173"/>will create a simple dataset <a id="_idIndexMarker1174"/>where the relationship between the input feature, <span class="_-----MathTools-_Math_Variable">x</span>, and the target variable, <span class="_-----MathTools-_Math_Variable">y</span>, is not linear, but quadratic. First, we will fit an SLR model, then add a quadratic term and compare <span class="No-Break">the results:</span></p>&#13;
			<ol>&#13;
				<li>Generate a sequence of <strong class="source-inline">x</strong> values ranging from -10 to 10. For each <strong class="source-inline">x</strong>, compute the corresponding <strong class="source-inline">y</strong> as the square of <strong class="source-inline">x</strong>, plus some random noise to show a (noisy) quadratic relationship. Put <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> in <span class="No-Break">a DataFrame:</span><pre class="source-code">&#13;
# Create a quadratic dataset&#13;
set.seed(1)&#13;
x = seq(-10, 10, by = 0.5)&#13;
y = x^2 + rnorm(length(x), sd = 5)&#13;
# Put it in a dataframe&#13;
df = data.frame(x = x, y = y)&#13;
Fit a simple linear regression to the data and print the summary.&#13;
lm1 &lt;- lm(y ~ x, data = df)&#13;
&gt;&gt;&gt; summary(lm1)&#13;
Call:&#13;
lm(formula = y ~ x, data = df)&#13;
Residuals:&#13;
    Min      1Q  Median      3Q     Max&#13;
-43.060 -29.350  -5.451  19.075  64.187&#13;
Coefficient s:&#13;
            Estimate Std. Error t value Pr(&gt;|t|)&#13;
(Intercept) 35.42884    5.04627   7.021 2.01e-08 ***&#13;
x           -0.04389    0.85298  -0.051    0.959&#13;
Signif. code s:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&#13;
Residual standard error: 32.31 on 39 degrees of freedom&#13;
Multiple R-squared:  6.787e-05,  Adjusted R-squared:  -0.02557&#13;
F-statistic: 0.002647 on 1 and 39 DF,  p-value: 0.9592</pre><p class="list-inset">The result suggests a not-so-good model fitting to the data, which possesses a <span class="No-Break">nonlinear relationship.</span></p></li>				<li>Fit a <a id="_idIndexMarker1175"/>quadratic model to<a id="_idIndexMarker1176"/> the data by including <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> as a predictor using the <strong class="source-inline">I()</strong> function. Print the summary of <span class="No-Break">the model:</span><pre class="source-code">&#13;
lm2 &lt;- lm(y ~ x + I(x^2), data = df)&#13;
&gt;&gt;&gt; summary(lm2)&#13;
Call:&#13;
lm(formula = y ~ x + I(x^2), data = df)&#13;
Residuals:&#13;
    Min      1Q  Median      3Q     Max&#13;
-11.700  -2.134  -0.078   2.992   7.247&#13;
Coefficients:&#13;
            Estimate Std. Error t value Pr(&gt;|t|)&#13;
(Intercept)  0.49663    1.05176   0.472    0.639&#13;
x           -0.04389    0.11846  -0.370    0.713&#13;
I(x^2)       0.99806    0.02241  44.543   &lt;2e-16 ***&#13;
Signif. code s:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&#13;
Residual standard error: 4.487 on 38 degrees of freedom&#13;
Multiple R-squared:  0.9812,  Adjusted R-squared:  0.9802&#13;
F-statistic: 992.1 on 2 and 38 DF,  p-value: &lt; 2.2e-16</pre><p class="list-inset">The result shows that the polynomial model fits the data better than the simple linear model. Thus, adding nonlinear terms can improve model fit when the relationship between predictors and the response is not <span class="No-Break">strictly linear.</span></p></li>				<li>Plot the<a id="_idIndexMarker1177"/> linear and quadratic <a id="_idIndexMarker1178"/>models together with <span class="No-Break">the data:</span><pre class="source-code">&#13;
ggplot(df, aes(x = x, y = y)) +&#13;
  geom_point() +&#13;
  geom_line(aes(y = linear_pred), color =""blu"", linetype =""dashe"") +&#13;
  geom_line(aes(y = quadratic_pred), color =""re"") +&#13;
  labs(title =""Scatter plot with linear and quadratic fit"",&#13;
       x ="""",&#13;
       y ="""") +&#13;
  theme(text = element_text(size = 15)) +&#13;
  scale_color_discrete(name =""Mode"",&#13;
                       labels = c"«Linear Mode"»,"«Quadratic Mode"»)) +&#13;
  annotate""tex"", x = 0, y = 40, label =""Linear Mode"", color =""blu"") +&#13;
  annotate""tex"", x = 6, y = 80, label =""Quadratic Mode"", color =""re"")</pre><p class="list-inset">Here, we first calculate the predicted values for both models and add them to the DataFrame. Then, we create a scatter plot of the data and add two lines representing the predicted values from the linear model (in blue) and the quadratic model (<span class="No-Break">in red).</span></p><p class="list-inset">Running this<a id="_idIndexMarker1179"/> code generates <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.5</em>. The<a id="_idIndexMarker1180"/> result suggests that adding a polynomial feature could extend the flexibility of a <span class="No-Break">linear model:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer221" class="IMG---Figure">&#13;
					<img src="Images/B18680_12_005.jpg" alt="Figure 12.5 – Visualizing the linear and quadratic fits to the nonlinear data" width="1074" height="842"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – Visualizing the linear and quadratic fits to the nonlinear data</p>&#13;
			<p>Other common<a id="_idIndexMarker1181"/> ways to introduce nonlinearity<a id="_idIndexMarker1182"/> include the logarithmic transformation (<span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Variable">x</span>) or a square root transformation (<span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable"> </span>). These transformations can also be applied to the target variable, <span class="_-----MathTools-_Math_Variable">y</span>, and we can have multiple transformed features in the same <span class="No-Break">linear model.</span></p>&#13;
			<p>Note that the model with transformed features remains a linear model. If there is a nonlinear transformation to the coefficients, the model would be a <span class="No-Break">nonlinear one.</span></p>&#13;
			<p>The next section sheds more light on a widely used type of transformation: the <span class="No-Break">logarithmic transformation.</span></p>&#13;
			<h2 id="_idParaDest-264"><a id="_idTextAnchor271"/>More on the logarithmic transformation</h2>&#13;
			<p>The<a id="_idIndexMarker1183"/> logarithmic transformation, or log transformation, maps an input to the corresponding output based on the logarithmic function, giving <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Variable">x</span>. A popular reason behind using such a transformation is to introduce nonlinearity in the linear regression model. When the relationship between the input features and the target output is nonlinear, applying a transformation can sometimes linearize the relationship, making it possible to model the relationship with a linear regression model. For the logarithmic transformation, it can help when the rate of change in the outcome variable increases or decreases as the value of the <span class="No-Break">predictor increases.</span></p>&#13;
			<p>To be specific, the rate of change decreases as the input becomes more extreme. The natural consequence of such a transformation is that potential outliers in the input data are squeezed so that they appear less extreme in the transformed column. In other words, the resulting linear regression model will be less sensitive to the original outliers due to the <span class="No-Break">log transformation.</span></p>&#13;
			<p>Another side benefit of using<a id="_idIndexMarker1184"/> log transformation is its ability to deal with heteroscedasticity. Heteroscedasticity is when the variability of the error term in a regression model is not constant across all levels of the predictors. This violates one of the assumptions of linear regression models and can lead to inefficient and biased estimates. In this case, log transformations can stabilize the variance of the error term by shrinking the potential big error terms, making it more constant across different levels of <span class="No-Break">the predictors.</span></p>&#13;
			<p>Lastly, when the relationship between predictors and the outcome is multiplicative rather than additive, taking the log of the predictors and/or the outcome variable can convert the relationship into an additive one, which can be modeled using <span class="No-Break">linear regression.</span></p>&#13;
			<p>Let’s consider an example where we predict the miles per gallon (<strong class="source-inline">mpg</strong>) from horsepower (<strong class="source-inline">hp</strong>). We’ll compare the model where we predict <strong class="source-inline">mpg</strong> directly from <strong class="source-inline">hp</strong> and another model where we predict the log of <strong class="source-inline">mpg</strong> from <strong class="source-inline">hp</strong>, as shown in the following <span class="No-Break">code snippet:</span></p>&#13;
			<pre class="source-code">&#13;
# Fit the original model&#13;
model_original = lm(mpg ~ hp, data = mtcars)&#13;
# Fit the log-transformed model&#13;
mtcars$log_mpg = log(mtcars$mpg)&#13;
model_log = lm(log_mpg ~ hp, data = mtcars)&#13;
# Predictions from the original model&#13;
mtcars$pred_original = predict(model_original, newdata = mtcars)&#13;
# Predictions from the log-transformed model (back-transformed to the original scale using exp)&#13;
mtcars$pred_log = exp(predict(model_log, newdata = mtcars))&#13;
library(tidyr)&#13;
library(dplyr)&#13;
# Reshape data to long format&#13;
df_long &lt;- mtcars %&gt;%&#13;
  gather(key =""Mode"", value =""Predictio"", pred_original, pred_log)&#13;
# Create plot&#13;
ggplot(df_long, aes(x = hp, y = mpg)) +&#13;
  geom_point(data = mtcars, aes(x = hp, y = mpg)) +&#13;
  geom_line(aes(y = Prediction, color = Model)) +&#13;
  labs(&#13;
    x =""Horsepower (hp"",&#13;
    y =""Miles per gallon (mpg"",&#13;
    color =""Mode""&#13;
  ) +&#13;
  scale_color_manual(values = c""pred_origina"" =""blu"",""pred_lo"" =""re"")) +&#13;
  theme(&#13;
    legend.position =""botto"",&#13;
    text = element_text(size = 16),&#13;
    legend.title = element_text(size = 16),&#13;
    axis.text = element_text(size = 16),  # control the font size of axis labels&#13;
    legend.text = element_text(size = 16)  # control the font size of legend text&#13;
  )</pre>			<p>Running this <a id="_idIndexMarker1185"/>code generates <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.6</em>, where we can see a slight curvature in the <span class="No-Break">blue line:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer222" class="IMG---Figure">&#13;
					<img src="Images/B18680_12_006.jpg" alt="Figure 12.6 – Visualizing the original and log-transformed model" width="1133" height="926"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – Visualizing the original and log-transformed model</p>&#13;
			<p>Note that the log transformation can only be applied to positive data. In the case of <strong class="source-inline">mtcars$mpg</strong>, all values are positive, so we can safely apply the log transformation. If the variable included zero or negative values, we would need to consider a different transformation <span class="No-Break">or approach.</span></p>&#13;
			<p>The next section focuses on deriving and using the closed-form solution to the linear <span class="No-Break">regression model.</span></p>&#13;
			<h2 id="_idParaDest-265"><a id="_idTextAnchor272"/>Working with the closed-form solution</h2>&#13;
			<p>When developing a linear <a id="_idIndexMarker1186"/>regression model, the available training set <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base">)</span> is given, and the only unknown parameters are the coefficients, <span class="_-----MathTools-_Math_Variable">β</span>. Here, a bold lowercase letter means a vector (such as <span class="_-----MathTools-_Math_Variable">β</span> and <span class="_-----MathTools-_Math_Variable">y</span>), and a bold uppercase letter denotes a matrix (such as <span class="_-----MathTools-_Math_Variable">X</span>). It turns out that the closed-form solution to a linear regression model can be derived using the concept of the <strong class="bold">ordinary least squares</strong> (<strong class="bold">OLS</strong>) estimator, which <a id="_idIndexMarker1187"/>aims to minimize the sum of the squared residuals in the model. Having the closed-form solution means we can simply plug in the required elements (in this case, <span class="_-----MathTools-_Math_Variable">X</span> and <span class="_-----MathTools-_Math_Variable">y</span>) and perform the calculation to obtain the solution, without resorting to any <span class="No-Break">optimization procedure.</span></p>&#13;
			<p>Specifically, given a data matrix, <span class="_-----MathTools-_Math_Variable">X</span> (which includes a column of ones for the intercept term and is in bold to indicate more than one feature), of dimensions <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span> (where <span class="_-----MathTools-_Math_Variable">n</span> is the number of observations and <span class="_-----MathTools-_Math_Variable">p</span> is the number of predictors) and a response vector, <span class="_-----MathTools-_Math_Variable">y</span>, of length n, the OLS estimator for the coefficient vector, <span class="_-----MathTools-_Math_Variable">β</span>, is given by the <span class="No-Break">following formula:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Variable_v-normal">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">T</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">y</span></span></p>&#13;
			<p>This solution assumes that the term (<span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span>) is invertible, meaning it should be a full-rank matrix. If this is not the case, the solution either does not exist or is <span class="No-Break">not unique.</span></p>&#13;
			<p>Now, let’s look at how to derive this solution. We start with the minimization problem for the least squares: minimizing <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Variable_v-normal">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base">)</span> over <span class="_-----MathTools-_Math_Variable_v-normal">β</span>. This quadratic form can be expanded to <span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Variable_v-normal">β</span>. Note that <span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Variable_v-normal">β</span> since both terms are scalars and therefore are equal to each other after the transpose operation. We can write <a id="_idIndexMarker1188"/>the <strong class="bold">residual sum of squares</strong> (<strong class="bold">RSS</strong>) expression as <span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">T</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">X</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">β</span></span><span class="No-Break">.</span></p>&#13;
			<p>Here, we apply the first-order condition to solve for the value of <span class="_-----MathTools-_Math_Variable_v-normal">β</span> that minimizes this expression (recall that the point that either minimizes or maximizes a graph has a derivative of 0). This means that we would set its first derivative to zero, leading to the <span class="No-Break">following formula:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">___________________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">T</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">y</span></span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable_v-normal">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Variable_v-normal">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">T</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">y</span></span></p>&#13;
			<p>Thus, we have derived the closed-form solution of <span class="_-----MathTools-_Math_Variable_v-normal">β</span> that minimizes the sum of the squared residuals. Let’s go through an example to see how it can <span class="No-Break">be implemented.</span></p>&#13;
			<h3>Implementing the closed-form solution</h3>&#13;
			<p>Let’s look at implementing<a id="_idIndexMarker1189"/> the OLS estimation in R for an SLR model. An example that uses synthetic data is shown in the following <span class="No-Break">code snippet:</span></p>&#13;
			<pre class="source-code">&#13;
# Set seed for reproducibility&#13;
set.seed(123)&#13;
# Generate synthetic data&#13;
n = 100 # number of observations&#13;
x = runif(n, -10, 10) # predictors&#13;
beta0 = 2 # intercept&#13;
beta1 = 3 # slope&#13;
epsilon = rnorm(n, 0, 2) # random error term&#13;
y = beta0 + beta1*x + epsilon # response variable&#13;
# Design matrix X&#13;
X = cbind(1, x)</pre>			<p>Here, we generate 100 observations with a single input feature, where the observation is noise-perturbed and follows a process given by <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span>. The error term assumes a normal distribution that’s parameterized by a mean of 0 and a standard deviation <span class="No-Break">of 2.</span></p>&#13;
			<p>Before proceeding to the estimation, note that we also appended a column of 1s on the left of the input feature, <strong class="source-inline">x</strong>, to form a matrix, <strong class="source-inline">X</strong>. This column of 1s is used to indicate the intercept term and is often referred to as the bias trick. That is, the coefficient, <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span>, for the intercept term will be part of the coefficient vector, and there is no need to create a separate coefficient just for <span class="No-Break">the intercept.</span></p>&#13;
			<p>Let’s calculate the result using the <span class="No-Break">closed-form solution:</span></p>&#13;
			<pre class="source-code">&#13;
beta_hat = solve(t(X) %*% X) %*% t(X) %*% y&#13;
&gt;&gt;&gt; print(beta_hat)&#13;
       [,1]&#13;
  1.985344&#13;
x 3.053152</pre>			<p>Here, <strong class="source-inline">%*%</strong> is used for<a id="_idIndexMarker1190"/> matrix multiplication, <strong class="source-inline">t(X)</strong> is the transpose of <strong class="source-inline">X</strong>, and <strong class="source-inline">solve()</strong> is used to calculate the inverse of <span class="No-Break">a matrix.</span></p>&#13;
			<p>We can also run the linear regression procedure using the <strong class="source-inline">lm()</strong> function <span class="No-Break">for comparison:</span></p>&#13;
			<pre class="source-code">&#13;
# Fit linear regression model for comparison&#13;
model = lm(y ~ x)&#13;
&gt;&gt;&gt; print(coef(model))&#13;
(Intercept)           x&#13;
   1.985344    3.053152</pre>			<p>The results are the same as the ones that we obtained via the <span class="No-Break">manual approach.</span></p>&#13;
			<p>The next two sections cover two common issues in linear regression settings: multicollinearity <span class="No-Break">and heteroskedasticity.</span></p>&#13;
			<h2 id="_idParaDest-266"><a id="_idTextAnchor273"/>Dealing with multicollinearity</h2>&#13;
			<p>Multicollinearity refers to the<a id="_idIndexMarker1191"/> case when two (or more) predictors are highly correlated in a multiple regression model. This means that one independent variable can be linearly predicted from the others with a high degree of accuracy. This is a situation that we do not want to fall into. In other words, we would like to see a high degree of correlation between the predictors and the target variable, while a low degree of correlation among these <span class="No-Break">predictors themselves.</span></p>&#13;
			<p>In the face of multicollinearity in a linear regression model, the resultant model tends to generate unreliable and unstable estimates of the regression coefficients. It can inflate the coefficients of the parameters, making them statistically insignificant, even though they might be substantively important. In addition, multicollinearity makes it difficult to assess the effect of each independent variable on the dependent variable as the effects are intertwined. However, it does not affect the predictive power or interpretability of the model; instead, it only changes the calculations for <span class="No-Break">individual features.</span></p>&#13;
			<p>Detecting any potential multicollinearity among the predictors can be performed by examining the pair-wise correlation. Alternatively, we can resort to a particular test statistic called<a id="_idIndexMarker1192"/> the <strong class="bold">variance inflation factor</strong> (<strong class="bold">VIF</strong>), which quantifies how much the variance is increased due to<a id="_idIndexMarker1193"/> multicollinearity. A VIF of 1 indicates that two variables are not correlated, while a VIF greater than 5 (in many fields) would suggest a problematic amount <span class="No-Break">of multicollinearity.</span></p>&#13;
			<p>When multicollinearity exists in the linear regression model, we could choose to keep one predictor only and remove all other highly correlated predictors. We can also combine these correlated variables into a few <a id="_idIndexMarker1194"/>uncorrelated ones via <strong class="bold">principle component analysis</strong> (<strong class="bold">PCA</strong>), a widely used technique for dimension reduction. Besides this, we can resort to ridge regression to control the magnitude of the coefficients; this will be introduced later in <span class="No-Break">this chapter.</span></p>&#13;
			<p>To check multicollinearity using VIF, we can use the <strong class="source-inline">vif()</strong> function from the <strong class="source-inline">car</strong> package, as shown in the following <span class="No-Break">code snippet:</span></p>&#13;
			<pre class="source-code">&#13;
# install the package if not already installed&#13;
if(!require(car)) install.packages('car')&#13;
# load the package&#13;
library(car)&#13;
# fit a linear model&#13;
model = lm(mpg ~ hp + wt + disp, data = mtcars)&#13;
# calculate VIF&#13;
vif_values = vif(model)&#13;
&gt;&gt;&gt; print(vif_values)&#13;
      hp       wt     disp&#13;
2.736633 4.844618 7.324517</pre>			<p>Looking at the result, <strong class="source-inline">disp</strong> seems to have high multicollinearity (VIF = 7.32 &gt; 5), suggesting that it has a strong correlation with <strong class="source-inline">hp</strong> and <strong class="source-inline">wt</strong>. This implies that <strong class="source-inline">disp</strong> is not providing much information that is not already contained in the other <span class="No-Break">two predictors.</span></p>&#13;
			<p>To handle the multicollinearity here, we can consider removing <strong class="source-inline">disp</strong> from the model since it has the highest VIF, applying PCA to combine the three predictors, or using ridge or lasso <a id="_idIndexMarker1195"/>regression (more on this in the last two sections of <span class="No-Break">this chapter).</span></p>&#13;
			<p>The next section focuses on the issue <span class="No-Break">of heteroskedasticity.</span></p>&#13;
			<h2 id="_idParaDest-267"><a id="_idTextAnchor274"/>Dealing with heteroskedasticity</h2>&#13;
			<p>Heteroskedasticity (or heteroscedasticity) refers to the situation in which the variability of the error term, or residuals, is not the <a id="_idIndexMarker1196"/>same across all levels of the independent variables. This violates one of the key assumptions of OLS linear regression, which assumes that the residuals have a constant variance – in other words, the residuals are homoskedastic. Violating this assumption could lead to incorrect inferences on the statistical significance of the coefficients since the resulting standard errors of the regression coefficients could be larger or smaller than they <span class="No-Break">should be.</span></p>&#13;
			<p>There are a few ways to handle heteroskedasticity. First, we can transform the outcome variable using the logarithmic function, as introduced earlier. Other functions, such as taking the square root or inverse of the original outcome variable, could also help reduce heteroskedasticity. Advanced regression models<a id="_idIndexMarker1197"/> such as <strong class="bold">weighted least squares</strong> (<strong class="bold">WLS</strong>) or <strong class="bold">generalized least squares</strong> (<strong class="bold">GLS</strong>) may also be<a id="_idIndexMarker1198"/> explored to reduce the impact <span class="No-Break">of heteroskedasticity.</span></p>&#13;
			<p>To formally test for heteroskedasticity, we can conduct a Breusch-Pagan test using the <strong class="source-inline">bptest()</strong> function from the <strong class="source-inline">lmtest</strong> package. In the following code snippet, we fit an MLR model to predict <strong class="source-inline">mpg</strong> using <strong class="source-inline">wt</strong> and <strong class="source-inline">hp</strong>, followed by performing the <span class="No-Break">Breusch-Pagan test:</span></p>&#13;
			<pre class="source-code">&#13;
# Load library&#13;
library(lmtest)&#13;
# Fit a simple linear regression model on mtcars dataset&#13;
model = lm(mpg ~ wt + hp, data = mtcars)&#13;
# Perform a Breusch-Pagan test to formally check for heteroskedasticity&#13;
&gt;&gt;&gt; bptest(model)&#13;
  studentized Breusch-Pagan test&#13;
data:  model&#13;
BP = 0.88072, df = 2, p-value = 0.6438</pre>			<p>Since the p-value (<strong class="source-inline">0.6438</strong>) is greater <a id="_idIndexMarker1199"/>than 0.05, we do not reject the null hypothesis of the Breusch-Pagan test. This suggests that there is not enough evidence to say that heteroskedasticity is present in the regression model. So, we would conclude that the variances of the residuals are not significantly different from being constant, <span class="No-Break">or homoskedastic.</span></p>&#13;
			<p>The next section shifts to looking at regularized linear regression models and ridge and <span class="No-Break">lasso penalties.</span></p>&#13;
			<h1 id="_idParaDest-268"><a id="_idTextAnchor275"/>Introducing penalized linear regression</h1>&#13;
			<p>Penalized regression models, such <a id="_idIndexMarker1200"/>as ridge and lasso, are techniques that are used to handle problems such as multicollinearity, reduce overfitting, and even perform variable selection, especially when dealing with high-dimensional data with multiple <span class="No-Break">input features.</span></p>&#13;
			<p>Ridge regression (also called L2 regularization) is a method that adds a penalty equivalent to the square of the magnitude of coefficients. We would add this term to the loss function after weighting it by an additional hyperparameter, often denoted as <span class="_-----MathTools-_Math_Variable">λ</span>, to control the strength of the <span class="No-Break">penalty term.</span></p>&#13;
			<p>Lasso regression (L1 regularization), on the other hand, is a method that, similar to ridge regression, adds a penalty for non-zero coefficients, but unlike ridge regression, it can force some coefficients to be exactly equal to zero when the penalty tuning parameter is large enough. The larger the value of the hyperparameter, <span class="_-----MathTools-_Math_Variable">λ</span>, the greater the amount of shrinkage. The penalty on the size of coefficients helps reduce model complexity and multicollinearity, leading to a model that can generalize better on unseen data. However, ridge regression includes all the features in the final model, so it doesn’t induce any sparsity. Therefore, it’s not particularly useful for <span class="No-Break">variable selection.</span></p>&#13;
			<p>In summary, ridge <a id="_idIndexMarker1201"/>and lasso regression are both penalized linear regression methods that add a constraint regarding the magnitude of the estimated coefficients to the model optimization process, which helps prevent overfitting, manage multicollinearity, and reduce model complexity. However, ridge tends to include all predictors in the model and helps reduce their effect, while lasso can exclude predictors from the model altogether, leading to a simpler and more <span class="No-Break">interpretable model.</span></p>&#13;
			<p>Let’s start with ridge regression and look at its loss function <span class="No-Break">more closely.</span></p>&#13;
			<h1 id="_idParaDest-269"><a id="_idTextAnchor276"/>Working with ridge regression</h1>&#13;
			<p>Ridge regression, also <a id="_idIndexMarker1202"/>referred to as L2 regularization, is a commonly used technique to alleviate overfitting in linear regression models by penalizing the magnitude of the estimated coefficients in the <span class="No-Break">resulting model.</span></p>&#13;
			<p>Recall that in an SLR model, we seek to minimize the sum of the squared differences between our predicted and actual values, which we refer to as the least squares method. The loss function we wish to minimize is <span class="No-Break">the RSS:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">j</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>&#13;
			<p>Here, <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span> is the<a id="_idIndexMarker1203"/> actual target value, <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> is the intercept term, <span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base">}</span> are the coefficient estimates for each predictor, <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">j</span>, and the summations are overall observations <span class="No-Break">and predictors.</span></p>&#13;
			<p>Purely minimizing the RSS would give us an overfitting model, as represented by the high magnitude of the resulting coefficients. As a remedy, we could apply ridge regression by adding a penalty term to this loss function. This penalty term is the sum of the squares of each coefficient multiplied <a id="_idIndexMarker1204"/>by a tuning parameter, <span class="_-----MathTools-_Math_Variable">λ</span>. So, the ridge regression loss function (also known as the <strong class="bold">cost function</strong>) is <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Variable">g</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">j</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>&#13;
			<p>Here, the <span class="_-----MathTools-_Math_Variable">λ</span> parameter is a user-defined tuning parameter. A larger <span class="_-----MathTools-_Math_Variable">λ</span> means a higher penalty and a smaller <span class="_-----MathTools-_Math_Variable">λ</span> means less regularization effect. <span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> gives the ordinary least squares regression result, while as <span class="_-----MathTools-_Math_Variable">λ</span> approaches infinity, the impact of the penalty term dominates, and the coefficient estimates <span class="No-Break">approach zero.</span></p>&#13;
			<p>By adding this penalty term, ridge regression tends to decrease the size of the coefficients, which can help mitigate the problem of multicollinearity (where predictors are highly correlated). It does this by spreading the coefficient estimates of correlated predictors across each other, which can lead to a more stable and <span class="No-Break">interpretable model.</span></p>&#13;
			<p>However, it’s important to note that ridge regression does not typically produce sparse solutions and does not perform variable selection. In other words, it will not result in a model where some coefficients are exactly zero (unless <span class="_-----MathTools-_Math_Variable">λ</span> is infinite), thus all predictors are included in the model. If feature selection is important, methods such as lasso (L1 regularization) or <a id="_idIndexMarker1205"/>elastic net (a combination of L1 and L2 regularization) might be <span class="No-Break">more appropriate.</span></p>&#13;
			<p>Note that we would often penalize the intercept, <span class="No-Break"><span class="_-----MathTools-_Math_Variable">β</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0</span></span><span class="No-Break">.</span></p>&#13;
			<p>Let’s go through an exercise to learn how to develop a ridge <span class="No-Break">regression model.</span></p>&#13;
			<h3>Exercise 12.5 – implementing ridge regression</h3>&#13;
			<p>In this exercise, we will <a id="_idIndexMarker1206"/>implement a ridge regression model and compare the estimated coefficients with the OLS model. Our implementation will be based on the <span class="No-Break"><strong class="source-inline">glmnet</strong></span><span class="No-Break"> package:</span></p>&#13;
			<ol>&#13;
				<li>Install and load the <span class="No-Break"><strong class="source-inline">glmnet</strong></span><span class="No-Break"> package:</span><pre class="source-code">&#13;
# install the package if not already installed&#13;
if(!require(glmnet)) install.packages('glmnet')&#13;
library(glmnet)</pre><p class="list-inset">Here, we use an <strong class="source-inline">if-else</strong> statement to detect if the <strong class="source-inline">glmnet</strong> package <span class="No-Break">is installed.</span></p></li>				<li>Store all columns other than <strong class="source-inline">mpg</strong> as predictors in <strong class="source-inline">X</strong> and <strong class="source-inline">mpg</strong> as the target variable <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">y</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
# Prepare data&#13;
data(mtcars)&#13;
X = as.matrix(mtcars[, -1]) # predictors&#13;
y = mtcars[, 1] # response</pre></li>				<li>Fit a ridge regression model using the <span class="No-Break"><strong class="source-inline">glmnet()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
# Fit ridge regression model&#13;
set.seed(123) # for reproducibility&#13;
ridge_model = glmnet(X, y, alpha = 0)</pre><p class="list-inset">Here, the <strong class="source-inline">alpha</strong> parameter controls the type of model we fit. <strong class="source-inline">alpha = 0</strong> fits a ridge regression model, <strong class="source-inline">alpha = 1</strong> fits a lasso model, and any value in between fits an elastic <span class="No-Break">net model.</span></p></li>				<li>Use cross-validation to choose the best <span class="No-Break"><strong class="source-inline">lambda</strong></span><span class="No-Break"> value:</span><pre class="source-code">&#13;
# Use cross-validation to find the optimal lambda&#13;
cv_ridge = cv.glmnet(X, y, alpha = 0)&#13;
best_lambda = cv_ridge$lambda.min&#13;
&gt;&gt;&gt; best_lambda&#13;
2.746789</pre><p class="list-inset">Here, we use the cross-validation approach to identify the optimal <strong class="source-inline">lambda</strong> that gives the lowest error on the cross-validation set on average. All repeated cross-validation steps are completed via the <span class="No-Break"><strong class="source-inline">cv.glmnet()</strong></span><span class="No-Break"> function.</span></p></li>				<li>Fit a new<a id="_idIndexMarker1207"/> ridge regression model using the optimal <strong class="source-inline">lambda</strong> and extract its coefficients without <span class="No-Break">the intercept:</span><pre class="source-code">&#13;
# Fit a new ridge regression model using the optimal lambda&#13;
opt_ridge_model = glmnet(X, y, alpha = 0, lambda = best_lambda)&#13;
# Get coefficients&#13;
ridge_coefs = coef(opt_ridge_model)[-1]  # remove intercept&#13;
&gt;&gt;&gt; ridge_coefs&#13;
[1] -0.371840170 -0.005260088 -0.011611491  1.054511975 -1.233657799  0.162231830&#13;
 [7]  0.771141047  1.623031037  0.544153807 -0.547436697</pre></li>				<li>Fit a linear regression model and extract <span class="No-Break">its coefficients:</span><pre class="source-code">&#13;
# Ordinary least squares regression&#13;
ols_model = lm(mpg ~ ., data = mtcars)&#13;
# Get coefficients&#13;
ols_coefs = coef(ols_model)[-1] # remove intercept&#13;
&gt;&gt;&gt; ols_coefs&#13;
        cyl        disp          hp        drat          wt        qsec          vs&#13;
-0.11144048  0.01333524 -0.02148212  0.78711097 -3.71530393  0.82104075  0.31776281&#13;
         am        gear        carb&#13;
 2.52022689  0.65541302 -0.19941925</pre></li>				<li>Plot the<a id="_idIndexMarker1208"/> coefficients of both models on the <span class="No-Break">same graph:</span><pre class="source-code">&#13;
plot(1:length(ols_coefs), ols_coefs, type="b", col="blue", pch=19, xlab="Coefficient", ylab="Value", ylim=c(min(ols_coefs, ridge_coefs), max(ols_coefs, ridge_coefs)))&#13;
lines(1:length(ridge_coefs), ridge_coefs, type="b", col="red", pch=19)&#13;
legend("bottomright", legend=c("OLS", "Ridge"), col=c("blue", "red"), pch=19)</pre><p class="list-inset">Running this code generates <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer223" class="IMG---Figure">&#13;
					<img src="Images/B18680_12_007.jpg" alt="Figure 12.7 – Visualizing the estimated coefficients from the ridge and OLS models" width="1340" height="973"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – Visualizing the estimated coefficients from the ridge and OLS models</p>&#13;
			<p class="list-inset">This plot<a id="_idIndexMarker1209"/> shows that the estimated coefficients from the ridge regression model are, in general, smaller than those from the <span class="No-Break">OLS model.</span></p>&#13;
			<p>The next section focuses on <span class="No-Break">lasso regression.</span></p>&#13;
			<h1 id="_idParaDest-270"><a id="_idTextAnchor277"/>Working with lasso regression</h1>&#13;
			<p>Lasso regression <a id="_idIndexMarker1210"/>is another type of regularized linear regression. It is similar to ridge regression but differs in terms of the specific process of calculating the magnitude of the coefficients. Specifically, it uses the L1 norm of the coefficients, which consists of the total sum of absolute values of the coefficients, as the penalty that’s added to the OLS <span class="No-Break">loss function.</span></p>&#13;
			<p>The <a id="_idIndexMarker1211"/>lasso regression cost function can be written <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">β</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">j</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">|</span></span></p>&#13;
			<p>The key characteristic of lasso regression is that it can reduce some coefficients exactly to 0, effectively performing variable selection. This is a consequence of the L1 penalty term and is not the case for ridge regression, which can only shrink coefficients close to 0. Therefore, lasso regression is particularly useful when we believe that only a subset of the predictors matters when it comes to predicting <span class="No-Break">the outcome.</span></p>&#13;
			<p>In addition, in contrast to ridge regression, which can’t perform variable selection and therefore may be less interpretable, lasso regression automatically selects the most important features and discards the rest, which can make the final model easier <span class="No-Break">to interpret.</span></p>&#13;
			<p>Like ridge <a id="_idIndexMarker1212"/>regression, the lasso regression penalty term is also subject to a tuning parameter, <span class="_-----MathTools-_Math_Variable">λ</span>. The optimal <span class="_-----MathTools-_Math_Variable">λ</span> parameter is typically chosen via cross-validation or a more intelligent search policy such as <span class="No-Break">Bayesian optimization.</span></p>&#13;
			<p>Let’s go through an exercise to understand how to develop a lasso <span class="No-Break">regression model.</span></p>&#13;
			<h3>Exercise 12.6 – implementing lasso regression</h3>&#13;
			<p>To implement a<a id="_idIndexMarker1213"/> lasso regression model, we can follow a similar process as we did for the ridge <span class="No-Break">regression model:</span></p>&#13;
			<ol>&#13;
				<li>To fit a lasso regression model, set <strong class="source-inline">alpha = 1</strong> in the <span class="No-Break"><strong class="source-inline">glmnet()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
lasso_model = glmnet(X, y, alpha = 1)</pre></li>				<li>Use the same cross-validation procedure to identify the optimal value <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">lambda</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
# Use cross-validation to find the optimal lambda&#13;
cv_lasso = cv.glmnet(X, y, alpha = 1)&#13;
best_lambda = cv_lasso$lambda.min&#13;
&gt;&gt;&gt; best_lambda&#13;
0.8007036</pre></li>				<li>Fit a new lasso regression model using the <span class="No-Break">optimal </span><span class="No-Break"><strong class="source-inline">lambda</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
# Fit a new lasso regression model using the optimal lambda&#13;
opt_lasso_model = glmnet(X, y, alpha = 1, lambda = best_lambda)</pre><p class="list-inset">The resulting coefficients can also be extracted using the <strong class="source-inline">coef()</strong> function, followed by <strong class="source-inline">[-1]</strong> to remove the <span class="No-Break">intercept term:</span></p><pre class="source-code"># Get coefficients&#13;
lasso_coefs = coef(opt_lasso_model)[-1]  # remove intercept&#13;
&gt;&gt;&gt; lasso_coefs&#13;
[1] -0.88547684  0.00000000 -0.01169485  0.00000000 -2.70853300  0.00000000  0.00000000&#13;
 [8]  0.00000000  0.00000000  0.00000000</pre></li>				<li>Plot the estimated coefficients together with the previous <span class="No-Break">two models:</span><pre class="source-code">&#13;
plot(1:length(ols_coefs), ols_coefs, type="b", col="blue", pch=19, xlab="Coefficient", ylab="Value", ylim=c(min(ols_coefs, ridge_coefs), max(ols_coefs, ridge_coefs)))&#13;
lines(1:length(ridge_coefs), ridge_coefs, type="b", col="red", pch=19)&#13;
lines(1:length(lasso_coefs), lasso_coefs, type="b", col="green", pch=19)&#13;
legend("bottomright", legend=c("OLS", "Ridge", "Lasso"), col=c("blue", "red", "green"), pch=19)</pre><p class="list-inset">Running this <a id="_idIndexMarker1214"/>code generates <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.8</em>, which suggests that only two variables are kept in the resultant model. So, the lasso regression model can produce a sparse solution by setting the coefficients of some features equal <span class="No-Break">to zero:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer224" class="IMG---Figure">&#13;
					<img src="Images/B18680_12_008.jpg" alt="Figure 12.8 – Visualizing the estimated coefficients from the ridge, lasso, and OLS regression models" width="1187" height="846"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – Visualizing the estimated coefficients from the ridge, lasso, and OLS regression models</p>&#13;
			<p>In summary, the<a id="_idIndexMarker1215"/> lasso regression model gives us a sparse model by setting the coefficients of non-significant variables to zero, thus achieving feature selection and model estimation at the <span class="No-Break">same time.</span></p>&#13;
			<h1 id="_idParaDest-271"><a id="_idTextAnchor278"/>Summary</h1>&#13;
			<p>In this chapter, we covered the nuts and bolts of the linear regression model. We started by introducing the SLR model, which consists of only one input variable and one target variable, and then extended to the MLR model with two or more predictors. Both models can be assessed using <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>, or more preferably, the adjusted <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> metric. Next, we discussed specific scenarios, such as working with categorical variables and interaction terms, handling nonlinear terms via transformations, working with the closed-form solution, and dealing with multicollinearity and heteroskedasticity. Lastly, we introduced widely used regularization techniques such as ridge and lasso penalties, which can be incorporated into the loss function as a penalty term and generate a regularized model, and, additionally, a sparse solution in the case of <span class="No-Break">lasso regression.</span></p>&#13;
			<p>In the next chapter, we will cover another type of widely used linear model: the logistic <span class="No-Break">regression model.</span></p>&#13;
		</div>&#13;
	</div></body></html>