<html><head></head><body>
		<div id="_idContainer146">
			<h1 id="_idParaDest-156"><em class="italic"><a id="_idTextAnchor162"/>Chapter 8</em>: Evaluating and Optimizing Models</h1>
			<p>It is now time to learn how to evaluate and optimize machine learning models. During the process of modeling, or even after model completion, you might want to understand how your model is performing. Each type of model has its own set of metrics that can be used to evaluate performance, and that is what we are going to study in this chapter.</p>
			<p>Apart from model evaluation, as a data scientist, you might also need to improve your model's performance by tuning the hyperparameters of your algorithm. We will take a look at some nuances of this modeling task.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Introducing model evaluation</li>
				<li>Evaluating classification models</li>
				<li>Evaluating regression models</li>
				<li>Model optimization</li>
			</ul>
			<p>Alright, let's do it!</p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor163"/>Introducing model evaluation</h1>
			<p>There are several different scenarios in <a id="_idIndexMarker711"/>which we might want to evaluate model performance, some of them are as follows.</p>
			<ul>
				<li>You are creating a model and testing different approaches and/or algorithms. Therefore, you need to compare these models to select the best one.</li>
				<li>You have just completed your model and you need to document your work, which includes specifying the model's performance metrics that you have reached out to during the modeling phase.</li>
				<li>Your model is running in a production environment and you need to track its performance. If you encounter model drift, then you might want to retrain the model.<p class="callout-heading">Important note</p><p class="callout">The term <strong class="bold">model drift</strong> is used to <a id="_idIndexMarker712"/>refer to the problem of model deterioration. When you are building a machine learning model, you must use data to train the algorithm. This set of data is known as training data, and it reflects the business rules at a particular point in time. If these business rules change over time, your model will probably fail to adapt to that change. This is because it was trained on top of another dataset, which was reflecting another business scenario. To solve this problem, you must retrain the model so that it can consider the rules of the new business scenario. Reinforcement learning systems might not suffer from this issue since they can adapt to the new data by themselves. </p></li>
			</ul>
			<p>We perform model evaluations by <a id="_idIndexMarker713"/>designing a testing approach. We have learned about holdout validation and cross-validation before. However, both testing approaches share the same requirement: they need a metric in order to evaluate performance. </p>
			<p>This metric is specific to the problem domain, for example, there are specific metrics for regression models, classification models, clustering, natural language processing, and more. Therefore, during the design of your testing approach, you have to consider what type of model you are building in order to define the evaluation metrics.</p>
			<p>In the following sections, we will take a look at the most important metrics and concepts that you should know to evaluate your models. </p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor164"/>Evaluating classification models</h1>
			<p>Classification models are one of <a id="_idIndexMarker714"/>the most traditional classes of problems that you <a id="_idIndexMarker715"/>might face, either during the exam or during your journey as a data scientist. A very important artifact that you might want to generate during the classification model evaluation is known as a <strong class="bold">confusion matrix</strong>. </p>
			<p>A confusion matrix compares your model predictions against the real values of each class under evaluation. <em class="italic">Figure 8.1</em> shows what a <a id="_idIndexMarker716"/>confusion matrix looks like in a binary classification problem:</p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B16735_08_01.jpg" alt="Figure 8.1 – A confusion matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – A confusion matrix</p>
			<p>We find the following <a id="_idIndexMarker717"/>components in a confusion matrix:</p>
			<ul>
				<li><strong class="bold">TP</strong>: This is the number of <strong class="bold">True Positive</strong> cases. Here, we have to count the number of cases that have been predicted as true and are, indeed, true. For example, in a fraud detection system, this would be the number of fraudulent transactions that were correctly predicted as fraud.</li>
				<li><strong class="bold">TN</strong>: This is the number of <strong class="bold">True Negative</strong> cases. Here, we have to count the number of cases that have been predicted as false and are, indeed, false. For example, in a fraud detection system, this would be the number of non-fraudulent transactions that were correctly predicted as not fraud.</li>
				<li><strong class="bold">FN</strong>: This is the number of <strong class="bold">False Negative</strong> cases. Here, we have to count the number of cases that have been predicted as false but are, instead, true. For example, in a fraud detection system, this would be the number of fraudulent transactions that were wrongly predicted as not fraud.</li>
				<li><strong class="bold">FP</strong>: This is the number of <strong class="bold">False Positive</strong> cases. Here, we have to count the number of cases that have been <a id="_idIndexMarker718"/>predicted as true but are, instead, false. For example, in a fraud detection system, this would be the number of non-fraudulent transactions that were wrongly predicted as fraud.</li>
			</ul>
			<p>In a perfect scenario, your confusion matrix will have only true positive and true negative cases, which means that your model has an accuracy of 100%. In practical terms, if that type of scenario <a id="_idIndexMarker719"/>occurs, you should be skeptical instead of happy since it is expected that your model will contain errors. If your model does not contain errors, you are likely to be suffering from overfitting issues, so be careful.</p>
			<p>Once false negatives and false positives are expected, the most that you can do is to prioritize one of them. For example, you can reduce the number of false negatives by increasing the number of false positives and vice versa. This is known as the precision versus recall trade-off. Let's take a look at these metrics next.</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor165"/>Extracting metrics from a confusion matrix</h2>
			<p>The simplest metric that we can <a id="_idIndexMarker720"/>extract from a <a id="_idIndexMarker721"/>confusion matrix is known as <strong class="bold">accuracy</strong>. Accuracy is given by the number of true positives plus true <a id="_idIndexMarker722"/>negatives over the total number of cases. In <em class="italic">Figure 8.2</em>, we have filled out all the components of the confusion matrix for the sake of this demonstration:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B16735_08_02.jpg" alt="Figure 8.2 – A confusion matrix filled with some examples&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – A confusion matrix filled with some examples</p>
			<p>According to <em class="italic">Figure 8.2</em>, the accuracy would be (100 + 90) / 210, which is equal to 0.90. There is a common issue that occurs when utilizing an accuracy metric, which is related to the balance of each class. Problems with highly imbalanced classes, such as 99% of positive cases and 1% of negative cases, will <a id="_idIndexMarker723"/>impact the accuracy score and make it useless.</p>
			<p>For example, if your <a id="_idIndexMarker724"/>training data has 99% of positive cases (the majority class), your model is likely to correctly classify most of the positive cases but go badly in the classification of negative cases (the minority class). The accuracy will be very high (due to the correctness of the classification of the positive cases), regardless of the bad results in the minority class classification.</p>
			<p>The point is that on highly imbalanced problems, we usually have more interest in correctly classifying the minority class, not the majority class. That's the case on most fraud detection systems, for example, where the minority class corresponds to fraudulent cases. For imbalanced problems, you should look for other types of metrics, which we will cover next.</p>
			<p>Another important metric that we <a id="_idIndexMarker725"/>can extract from a <a id="_idIndexMarker726"/>confusion matrix is known as <strong class="bold">recall</strong>, which is the number of true positives over the number of true positives plus false negatives. In other words, recall is given by the number of true positive and overall <a id="_idIndexMarker727"/>positive cases. Recall is also known as <strong class="bold">sensitivity</strong>.</p>
			<p>According to <em class="italic">Figure 8.2</em>, recall is given by 100 / 112, which is <a id="_idIndexMarker728"/>equal to 0.89. <strong class="bold">Precision</strong>, on the other hand, is given by the number of true positives over the number of true positives plus false positives. In other words, precision is given by the number of true positive and overall predicted positive cases. Precision <a id="_idIndexMarker729"/>is also known as <strong class="bold">positive predictive power</strong>.</p>
			<p>According to <em class="italic">Figure 8.2</em>, precision is given by 100 / 108, which is equal to 0.93. In general, we can increase <a id="_idIndexMarker730"/>precision by the cost of decrease recall and vice versa. There is another model evaluation artifact in which we can play around with this precision versus recall trade-off. It is <a id="_idIndexMarker731"/>known as a <strong class="bold">precision-recall curve</strong>. </p>
			<p>Precision-recall curves summarize the precision versus recall trade-off by using different probability thresholds. For example, the default threshold is 0.5, where any prediction above 0.5 will be considered as true; otherwise, it is false. You can change the default threshold according to your need so that you can prioritize recall or precision. <em class="italic">Figure 8.3</em> shows an example of a precision-recall curve:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B16735_08_03.jpg" alt="Figure 8.3 – A precision-recall curve&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – A precision-recall curve</p>
			<p>As you can see in <em class="italic">Figure 8.3</em>, increasing the precision will reduce the amount of recall and vice versa. <em class="italic">Figure 8.3</em> shows the precision/recall for each threshold for a gradient boosting model (as shown by the orange line) compared to a no-skill model (as shown by the blue dashed line). A perfect model will <a id="_idIndexMarker732"/>approximate the curve to the point (1,1), forming a squared corner in the top right-hand side of the chart. </p>
			<p>Another visual analysis we can do on top of <a id="_idIndexMarker733"/>confusion matrixes is <a id="_idIndexMarker734"/>known as a <strong class="bold">Receiver Operating Characteristic</strong> (<strong class="bold">ROC</strong>) curve. ROC curves summarize <a id="_idIndexMarker735"/>the trade-off between the <strong class="bold">true positive rate</strong> and the <strong class="bold">false positive rate</strong> according to <a id="_idIndexMarker736"/>different thresholds, as in the precision-recall curve.</p>
			<p>You already know about the true positive rate, or sensitivity, which is the same as what we have just learned in the precision-recall curve. The other dimension of an ROC curve is the <strong class="bold">false positive rate</strong>, which is the number of false positives over the number of false positives plus true negatives. </p>
			<p>In literature, you might find the false positive rate <a id="_idIndexMarker737"/>referred to as inverted <strong class="bold">specificity</strong>, represented by <em class="italic">1 – specificity</em>. Specificity is given as the number of true negatives over the number of true negatives plus false positives. Furthermore, false-positive rates or inverted specificity are the same. <em class="italic">Figure 8.4</em> shows what an ROC curve looks like:</p>
			<p class="figure-caption"><img src="image/B16735_08_04.png" alt="Figure 8.4 – An ROC curve&#13;&#10;"/></p>
			<p class="figure-caption">Figure 8.4 – An ROC curve</p>
			<p>A perfect model will approximate the <a id="_idIndexMarker738"/>curve to the point (0,1), forming a squared corner in the top left-hand side of the chart. The orange line represents the <a id="_idIndexMarker739"/>trade-off between the true positive rate and the false positive rate of a gradient boosting classifier. The dashed blue line represents a no-skill model, which cannot predict the classes properly. </p>
			<p>To summarize, you can use ROC curves for fairly balanced datasets and precision-recall curves for moderate to imbalanced datasets.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor166"/>Summarizing precision and recall</h2>
			<p>Sometimes, we might want to <a id="_idIndexMarker740"/>use a metric that summarizes precision and <a id="_idIndexMarker741"/>recall, instead of prioritizing one over the other. Two very popular metrics can be used to summarize precision and recall: <strong class="bold">F1 score</strong> and <strong class="bold">Area Under Curve</strong> (<strong class="bold">AUC</strong>).</p>
			<p>The F1 score, also <a id="_idIndexMarker742"/>known as <strong class="bold">F-measure</strong>, computes the <a id="_idIndexMarker743"/>harmonic mean of precision and recall. AUC summarizes the <a id="_idIndexMarker744"/>approximation of the area under the precision-recall curve.</p>
			<p>That brings us to the end of this section on classification metrics. Let's now take a look at the evaluation metrics for regression models.</p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor167"/>Evaluating regression models</h1>
			<p>Regression models are quite <a id="_idIndexMarker745"/>different from classification models since the outcome of the model is a continuous number. Therefore, the metrics around regression models aim to monitor the difference between real and predicted values.</p>
			<p>The simplest way to check the difference between a predicted value (<em class="italic">yhat</em>) and its actual value (<em class="italic">y</em>) is by performing a simple subtraction operation, where the error will be equal to the absolute value of <em class="italic">yhat – y</em>. This metric is <a id="_idIndexMarker746"/>known as the <strong class="bold">Mean Absolute Error</strong> (<strong class="bold">MAE</strong>).</p>
			<p>Since we usually have to evaluate the error of each prediction, <em class="italic">i</em>, we have to take the mean value of the errors. The following formula shows how this error can be formally defined: </p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/Formula_09_001.jpg" alt=""/>
				</div>
			</div>
			<p>Sometimes, you might want to penalize bigger errors over smaller errors. To achieve this, you can use another metric, which is <a id="_idIndexMarker747"/>known as the <strong class="bold">Mean Squared Error</strong> (<strong class="bold">MSE</strong>). MSE will square each error and return the mean value. </p>
			<p>By squaring errors, MSE will penalize the bigger ones. The following formula shows how MSE can be formally defined:</p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B16735_08_05.jpg" alt=""/>
				</div>
			</div>
			<p>There is a potential <a id="_idIndexMarker748"/>interpretation problem with MSE. Since it has to compute the squared error, it might be difficult to interpret the final results from a business perspective. The <strong class="bold">Root Mean Squared Error</strong> (<strong class="bold">RMSE</strong>) works around this <a id="_idIndexMarker749"/>interpretation issue, by taking the square root of MSE. Here is the RMSE equation:</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/Formula_09_002.jpg" alt=""/>
				</div>
			</div>
			<p>RMSE is probably the most used metric for regression models since it can either penalize bigger errors, yet still be easily interpreted.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor168"/>Exploring other regression metrics</h2>
			<p>There are many more metrics <a id="_idIndexMarker750"/>that are suitable for regression problems aside from the ones that we have just learned. We will not be able to cover most of them here, but there are a few more metrics that might be important for you to know. </p>
			<p>One of these metrics is <a id="_idIndexMarker751"/>known as the <strong class="bold">Mean Absolute Percentage Error</strong> (<strong class="bold">MAPE</strong>). As the name suggests, MAPE will compute the absolute percentage error of each prediction and then take the average value. The following formula shows how this metric is computed:</p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B16735_08_06.jpg" alt=""/>
				</div>
			</div>
			<p>MAPE is broadly used on forecasting models since it is <a id="_idIndexMarker752"/>very simple to interpret, and it provides a very good sense of how far (or close) the predictions are from the actual values (in terms of a percentage).</p>
			<p>We have now completed this section on regression metrics. Next, we will talk about model optimizations.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor169"/>Model optimization</h1>
			<p>As you know, understanding evaluation metrics is very important in order to measure your model's performance and document your work. In the same way, when we want to optimize our current models, evaluating metrics also plays a very important role in defining the baseline performance that we want to challenge. </p>
			<p>The process of model optimization <a id="_idIndexMarker753"/>consists of finding the best configuration (also known as hyperparameters) of the machine learning algorithm for a particular data distribution. We don't want to find hyperparameters that overfit the training data in the same way that we don't want to find hyperparameters that underfit the training data.</p>
			<p>You learned about overfitting and underfitting in <a href="B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Fundamentals</em>. In the same chapter, you also learned how to avoid these two types of modeling issues. </p>
			<p>In this section, we will learn about some techniques that you can use to find the best configuration for a particular algorithm and dataset. You can combine these techniques of model optimization with other methods, such as cross-validation, to find the best set of hyperparameters for your model and avoid fitting issues.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Always remember that you don't want to optimize your algorithm to the underlying training data but to the data distribution behind the training data. This is so that your model will work in the training data as well as in the production data (that is, the data that has never been exposed to your model during the training process). A machine learning model that works only in the training data is useless. That's why combining model-tuning techniques (such as the ones we will learn about next) with sampling techniques (such as cross-validation) makes all the difference when it comes to creating a good model.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor170"/>Grid search</h2>
			<p><strong class="bold">Grid search</strong> is probably the most <a id="_idIndexMarker754"/>popular method for <a id="_idIndexMarker755"/>model optimization. It consists of testing different combinations of the algorithm and selecting the best one. Here, we have two important points that we need to pay attention to:</p>
			<ul>
				<li>How to define the best one?</li>
				<li>How many combinations should we test?</li>
			</ul>
			<p>The best model is defined based on an evaluation metric. In other words, you have to first define which metric you are going to use to evaluate the model's performance. Secondly, you have to define how you are going to evaluate the model. Usually, we use cross-validation to evaluate the model on multiple datasets that have never been used for training.</p>
			<p>In terms of the number of combinations, this is the most challenging part when playing with grid search. Each hyperparameter of an algorithm may have multiple or, sometimes, infinite possibilities of values. If you consider that an algorithm will usually have multiple hyperparameters, this becomes a function with quadratic cost, where the number of unique combinations to test (also known as a model for testing) is given as <em class="italic">the number of values of hyperparameter a * the number of values of hyperparameter b * the number of values of hyperparameter i</em>. <em class="italic">Figure 8.5</em> shows how you could potentially set a grid search configuration for a decision tree model:</p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B16735_08_Table_1.jpg" alt="Figure 8.5 – Grid search configuration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – Grid search configuration</p>
			<p>In <em class="italic">Figure 8.5</em>, there are three hyperparameters: <strong class="bold">Criterion</strong>, <strong class="bold">Max depth</strong>, and <strong class="bold">Min samples leaf</strong>. Each of these hyperparameters has a list of values for testing: 2, 3, and 3 values, respectively. That means, by the end of the grid search process, we will have tested 18 models (2 * 3 * 3), where only the best one will be selected.</p>
			<p>As you might have noticed, all the <a id="_idIndexMarker756"/>different combinations of those three hyperparameters will be tested, for example, consider the following:</p>
			<ul>
				<li>Criterion = Gini, Max depth = 2, Min samples leaf = 10</li>
				<li>Criterion = Gini, Max depth = 5, Min samples leaf = 10</li>
				<li>Criterion = Gini, Max depth = 10, Min samples leaf = 10</li>
			</ul>
			<p>Other questions that you might be wondering could include the following: </p>
			<ul>
				<li>Considering that a particular algorithm might have several hyperparameters, which ones should I tune?</li>
				<li>Considering that a particular hyperparameter might accept infinite values, which values should I test?</li>
			</ul>
			<p>These are good questions and grid search <a id="_idIndexMarker757"/>will not give you a straight answer for them. Instead, this is closer to an empirical process, where you have to test as much as you need to achieve your target performance.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Of course, grid search cannot guarantee that you will come up with your target performance. That depends on the algorithm and the training data.</p>
			<p>A common practice, though, is to define the values for testing by <a id="_idIndexMarker758"/>using a <strong class="bold">linear space</strong> or <strong class="bold">log space</strong>, where you can <a id="_idIndexMarker759"/>manually set the limits of the hyperparameter you want to test and the number of values for testing. Then, the intermediate values will be drawn by a linear or log function.</p>
			<p>As you might imagine, grid search can <a id="_idIndexMarker760"/>take a long time to run. A number of alternative methods have been proposed to work around this time issue. <strong class="bold">Random search</strong> is one of them, where <a id="_idIndexMarker761"/>the list of values for testing is randomly selected from the search space.</p>
			<p>Another method that has gained rapid adoption across <a id="_idIndexMarker762"/>the industry is known as <strong class="bold">Bayesian optimization</strong>. Algorithm optimizations, such as <strong class="bold">gradient descent</strong>, try to find what is <a id="_idIndexMarker763"/>called the <strong class="bold">global minima</strong>, by calculating <a id="_idIndexMarker764"/>derivatives of the cost function. Global minima are the points where you find the algorithm configuration with the least associated cost.</p>
			<p>Bayesian optimization is useful when <a id="_idIndexMarker765"/>calculating derivatives is not an option. So, we <a id="_idIndexMarker766"/>can use the <strong class="bold">Bayes theorem</strong>, a probabilistic approach, to find the global minima using the smallest number of steps. </p>
			<p>In practical terms, Bayesian optimization will start testing the entire search space to find the most promising set of optimal hyperparameters. Then, it will perform more tests specifically in the place where the global minima are likely to be.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor171"/>Summary</h1>
			<p>In this chapter, you learned about the main metrics for model evaluation. We first started with the metrics for classification problems and then we moved on to the metrics for regression problems. </p>
			<p>In terms of classification metrics, you have been introduced to the well-known confusion matrix, which is probably the most important artifact to perform a model evaluation on classification models. </p>
			<p>Aside from knowing what true positive, true negative, false positive, and false negative are, we have learned how to combine these components to extract other metrics, such as accuracy, precision, recall, the F1 score, and AUC.</p>
			<p>We went even deeper and learned about ROC curves, as well as precision-recall curves. We learned that we can use ROC curves to evaluate fairly balanced datasets and precision-recall curves for moderate to imbalanced datasets.</p>
			<p>By the way, when you are dealing with imbalanced datasets, remember that using accuracy might not be a good idea.</p>
			<p>In terms of regression metrics, we learned that the most popular ones, and the most likely to be present in the <em class="italic">AWS Machine Learning Specialty</em> exam, are MAE, MSE, RMSE, and MAPE. Make sure you know the basics of each of them before taking the exam.</p>
			<p>Finally, you learned about methods for hyperparameter optimization, where grid search and Bayesian optimization are the primary ones. In the next chapter, we will take a look at SageMaker and learn how it can be used for modeling. But first, let's take a moment to practice these questions on model evaluation and model optimization.</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor172"/>Questions</h1>
			<ol>
				<li>You are working as a data scientist for a pharmaceutical company. You are collaborating with other teammates to create a machine learning model to classify certain types of diseases on image exams. The company wants to prioritize the assertiveness rate of positive cases, even if they have to wrongly return false negatives. Which type of metric would you use to optimize the underlying model?<p>a. Recall</p><p>b. Precision</p><p>c. R-squared</p><p>d. RMSE</p><p class="callout-heading">Answer</p><p class="callout">In this scenario, the company prefers to have a higher probability to be right on positive outcomes at the cost of wrongly classifying some positive cases as negative. Technically, they prefer to increase precision at the cost of reducing recall.</p></li>
				<li>You are working as a data scientist for a pharmaceutical company. You are collaborating with other teammates to create a machine learning model to classify certain types of diseases on image exams. The company wants to prioritize the capture of positive cases, even if they have to wrongly return false positives. Which type of metric would you use to optimize the underlying model?<p>a. Recall</p><p>b. Precision</p><p>c. R-squared</p><p>d. RMSE</p><p class="callout-heading">Answer</p><p class="callout">In this scenario, the company prefers to find most of the positive cases at the cost of wrongly classifying some negative cases as positive. Technically, they prefer to increase recall at the cost of reducing precision.</p></li>
				<li>You are working in a fraud identification system, where one of the components is a classification model. You want to check the model's performance. Which of the following metrics could be used and why?<p>a. Accuracy. Since fraudulent system datasets are naturally unbalanced, this metric is good to take into consideration the assertiveness of both positive and negative classes.</p><p>b. Precision. Since fraudulent system datasets are naturally unbalanced, this metric is good to take into consideration the assertiveness of both positive and negative classes.</p><p>c. Recall. Since fraudulent system datasets are naturally unbalanced, this metric is good to take into consideration the assertiveness of both positive and negative classes.</p><p>d. The F1 score. Since fraudulent system datasets are naturally unbalanced, this metric is good to take into consideration the assertiveness of both positive and negative classes.</p><p class="callout-heading">Answer</p><p class="callout">Option "d" is the only one that matches the explanation of the proposed metric and provides a valid measure to the problem. Accuracy cannot be used in this problem due to the unbalanced issue. Precision and recall could be potentially used together to provide a quality view of the problem, but there is no such option in the list of answers.</p></li>
				<li>You are building a machine learning model to predict house prices. You have approached the problem as a regression model. Which of the following metrics are not applicable for regression models? (Select all correct answers.)<p>a. Recall</p><p>b. Precision</p><p>c. MAPE</p><p>d. RMSE</p><p class="callout-heading">Answer</p><p class="callout">Recall and precision are applicable for classification problems; that's why they are the correct answers. On the other hand, MAPE and RMSE are applicable for regression models. </p></li>
				<li>Which of the following metrics help us to penalize bigger errors on regression models? <p>a. Recall</p><p>b. Precision</p><p>c. MAPE</p><p>d. RMSE</p><p class="callout-heading">Answer</p><p class="callout">RMSE computes the squared error of each prediction. Then, it takes the squared root of the MSE. By computing the squared error, RMSE will penalize bigger errors over smaller errors.</p></li>
				<li> You are working as a data scientist for a financial services company and you have created a regression model to predict credit utilization. If you decide to include more features in the model, what will happen to R-squared and Adjusted R-squared?<p>a. Adjusted R-squared will increase, whereas R-squared can either increase or decrease.</p><p>b. R-squared will decrease, whereas Adjusted R-squared can either increase or decrease.</p><p>c. R-squared will increase, whereas Adjusted R-squared can either increase or decrease.</p><p>d. Adjusted R-squared will decrease, whereas R-squared can either increase or decrease.</p><p class="callout-heading">Answer</p><p class="callout">R-squared will increase since the extra information will help the model to capture more variance in the data. However, Adjusted R-squared can either increase or decrease, depending on the gain of adding the extra variable.</p></li>
				<li>Which of the following metrics will compute the percentage of errors instead of absolute errors?<p>a. Recall</p><p>b. Precision</p><p>c. MAPE</p><p>d. RMSE</p><p class="callout-heading">Answer</p><p class="callout">MAPE is applicable for regression models and it will compute the error as a percentage number.</p></li>
				<li>You are the lead data scientist of the company. Your team wants to optimize a model that is no longer performing well in production. The team has decided to use grid search to retrain the hyperparameters; however, the process is taking a long time and does not complete. Which approach could you take to speed up the process of tuning and still maximize your chances of finding a better model?<p>a) Reduce the search space to speed up the training process.</p><p>b) Use Bayesian optimization instead of grid search.</p><p>c) Increase the search space to speed up the training process.</p><p>d) None of the above.</p><p class="callout-heading">Answer</p><p class="callout">Reducing the search space of grid search will help to speed up the process of tuning, but you will test fewer models. This will reduce your chances of finding the best model for the problem. Increasing the search space will increase the time for tuning. Option "b" is the most resealable one since Bayesian optimization can focus on the most important search space, potentially reducing the time for processing and increasing your chances of finding the best model.</p></li>
				<li>You are using grid search to tune a machine learning model. During the tuning process, you obtain good performance metrics. However, when you execute the model in production, the model performance is not acceptable. You have to troubleshoot the problem. Which of the following options are valid reasons for this issue? (Select all correct answers.)<p>a) You are tuning and evaluating the model in the training data, which is causing overfitting.</p><p>b) The production data does not have the same distribution as the training data.</p><p>c) You are not using cross-validation in the training process.</p><p>d) You are not tuning the right hyperparameters.</p><p class="callout-heading">Answer</p><p class="callout">You can't tune and test the model in the same dataset at the risk of overfitting it. That's why option "a" is correct. If the production data does not follow the same distribution of the training data, the model will not work, so option "b" is also correct. Option "c" is not valid because cross-validation is not mandatory for model evaluation. Option "d" would be correct if you find bad results in the training data, not in the production data. </p></li>
				<li>You are working for a global financial company. Your team has created a binary classification model to identify fraudulent transactions. The model has been put into production and is automatically flagging fraudulent transactions and sending them for further screening. The operation team is complaining that this model is blocking too many transactions and they would prefer to flag a smaller number of transactions. According to the preceding scenario, what is the expectation of the operation team?<p>a) They want to calibrate the model threshold at 0.5.</p><p>b) They want to prioritize precision over recall.</p><p>b) They want to prioritize recall over precision.</p><p>b) They want to use F-measure.</p><p class="callout-heading">Answer</p><p class="callout">We always have to match model usage with business goals and capacity. In this scenario, the model is flagging a lot of potentially fraudulent transactions, but there isn't a big enough human workforce to evaluate all of those blocked transactions. Furthermore, what makes more sense is "calibrating" the model to the real business scenario, where it will flag fewer (but more likely) fraudulent cases for further screening.</p></li>
			</ol>
		</div>
	</body></html>