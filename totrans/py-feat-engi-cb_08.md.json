["```py\n from sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\nprint(data.DESCR)\n```", "```py\n     import pandas as pd\n    from feature_engine.creation import MathFeatures\n    from sklearn.datasets import load_breast_cancer\n    ```", "```py\n     data = load_breast_cancer()\n    df = pd.DataFrame(data.data,\n        columns=data.feature_names)\n    ```", "```py\n     features = [\n        «mean smoothness\",\n        «mean compactness\",\n        «mean concavity\",\n        «mean concave points\",\n        «mean symmetry\",\n    ]\n    ```", "```py\n     df[\"mean_features\"] = df[features].mean(axis=1)\n    df[\"mean_features\"].head()\n    ```", "```py\n    <st c=\"5539\">0    0.21702</st>\n    <st c=\"5548\">1    0.10033</st>\n    <st c=\"5558\">2    0.16034</st>\n    <st c=\"5568\">3    0.20654</st>\n    <st c=\"5578\">4    0.14326</st>\n    <st c=\"5588\">Name: mean_features, dtype: float64</st>\n    ```", "```py\n     df[\"std_features\"] = df[features].std(axis=1)\n    df[\"std_features\"].head()\n    ```", "```py\n    <st c=\"5949\">0    0.080321</st>\n    <st c=\"5960\">1    0.045671</st>\n    <st c=\"5971\">2    0.042333</st>\n    <st c=\"5982\">3    0.078097</st>\n    <st c=\"5993\">4    0.044402</st>\n    <st c=\"6004\">Name: std_features, dtype: float64</st>\n    ```", "```py\n     math_func = [\n        \"sum\", \"prod\", \"mean\", \"std\", \"max\", \"min\"]\n    ```", "```py\n     df_t = df[features].agg(math_func, axis=\"columns\")\n    ```", "```py\n     new_feature_names = [\n        \"sum_f\", \"prod_f\", \"mean_f\",\n        „std_f\", „max_f\", „min_f\"]\n    ```", "```py\n     create = MathFeatures(\n        variables=features,\n        func=math_func,\n        new_variables_names=new_feature_names,\n    )\n    ```", "```py\n     df_t = create.fit_transform(df)\n    ```", "```py\n     import pandas as pd\n    from feature_engine.creation import RelativeFeatures\n    from sklearn.datasets import load_breast_cancer\n    ```", "```py\n     data = load_breast_cancer()\n    df = pd.DataFrame(data.data,\n        columns=data.feature_names)\n    ```", "```py\n     df[\"difference\"] = df[\"worst compactness\"].sub(\n        df[\"mean compactness\"])\n    df[\"difference\"].head()\n    ```", "```py\n    <st c=\"12841\">0    0.38800</st>\n    <st c=\"12851\">1    0.10796</st>\n    <st c=\"12861\">2    0.26460</st>\n    <st c=\"12871\">3    0.58240</st>\n    <st c=\"12881\">4    0.07220</st>\n    <st c=\"12891\">Name: difference, dtype: float64</st>\n    ```", "```py\n     df[\"quotient\"] = df[\"worst radius\"].div(\n        df[\"mean radius\"])\n    df[\"quotient\"].head()\n    ```", "```py\n    <st c=\"13472\">0    1.410784</st>\n    <st c=\"13483\">1    1.214876</st>\n    <st c=\"13494\">2    1.197054</st>\n    <st c=\"13505\">3    1.305604</st>\n    <st c=\"13516\">4    1.110892</st>\n    <st c=\"13527\">Name: quotient, dtype: float64</st>\n    ```", "```py\n     features = [\n        \"mean smoothness\",\n        «mean compactness\",\n        \"mean concavity\",\n        \"mean symmetry\"\n    ]\n    ```", "```py\n     reference = [\"mean radius\", \"mean area\"]\n    ```", "```py\n     creator = RelativeFeatures(\n        variables=features,\n        reference=reference,\n        func=[\"sub\", \"div\"],\n    )\n    ```", "```py\n     df_t = creator.fit_transform(df)\n    ```", "```py\n     all_feat = creator.feature_names_in_\n    new_features = [\n        f for f in df_t.columns if f not in all_feat]\n    ```", "```py\n ['mean smoothness_sub_mean radius',\n'mean compactness_sub_mean radius',\n'mean concavity_sub_mean radius',\n'mean symmetry_sub_mean radius',\n'mean smoothness_sub_mean area',\n'mean compactness_sub_mean area',\n'mean concavity_sub_mean area',\n'mean symmetry_sub_mean area',\n'mean smoothness_div_mean radius',\n'mean compactness_div_mean radius',\n'mean concavity_div_mean radius',\n'mean symmetry_div_mean radius',\n'mean smoothness_div_mean area',\n'mean compactness_div_mean area',\n'mean concavity_div_mean area',\n'mean symmetry_div_mean area']\n```", "```py\n     import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn import set_config\n    from sklearn.preprocessing import PolynomialFeatures\n    ```", "```py\n     set_config(transform_output=\"pandas\")\n    ```", "```py\n     df = pd.DataFrame(np.linspace(\n        0, 10, 11), columns=[\"var\"])\n    ```", "```py\n     poly = PolynomialFeatures(\n        degree=3,\n        interaction_only=False,\n        include_bias=False)\n    ```", "```py\n     dft = poly.fit_transform(df)\n    ```", "```py\n     dft = pd.DataFrame(\n        dft, columns=poly.get_feature_names_out())\n    plt.plot(df[\"var\"], dft)\n    plt.legend(dft.columns)\n    plt.xlabel(\"original variable\")\n    plt.ylabel(\"new variables\")\n    plt.show()\n    ```", "```py\n     df[\"col\"] = np.linspace(0, 5, 11)\n    df[\"feat\"] = np.linspace(0, 5, 11)\n    ```", "```py\n     poly = PolynomialFeatures(\n        degree=2, interaction_only=True,\n        include_bias=False)\n    dft = poly.fit_transform(df)\n    ```", "```py\n     import pandas as pd\n    from sklearn.datasets import load_breast_cancer\n    from sklearn.compose import ColumnTransformer\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import PolynomialFeatures\n    ```", "```py\n     data = load_breast_cancer()\n    df = pd.DataFrame(data.data,\n        columns=data.feature_names)\n    X_train, X_test, y_train, y_test = train_test_split(\n        df, data.target, test_size=0.3, random_state=0\n    )\n    ```", "```py\n     features = [\n        \"mean smoothness\",\n        \"mean compactness\",\n        \"mean concavity\"]\n    ```", "```py\n     poly = PolynomialFeatures(\n        degree=3,\n        interaction_only=False,\n        include_bias=False)\n    ```", "```py\n     ct = ColumnTransformer([(\"poly\", poly, features)])\n    ```", "```py\n     train_t = ct.fit_transform(X_train)\n    test_t = ct.transform(X_test)\n    ```", "```py\n     import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.datasets import fetch_california_housing\n    from sklearn.model_selection import train_test_split\n    from <st c=\"34339\">feature_engine.creation</st>, import DecisionTreeFeatures\n    ```", "```py\n     X, y = fetch_california_housing(\n        return_X_y=True, as_frame=True)\n    X.drop(labels=[\n        \"Latitude\", \"Longitude\"], axis=1, inplace=True)\n    ```", "```py\n     X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=0)\n    ```", "```py\n     for var in X_train.columns:\n        pearson = np.corrcoef(X_train[var], y_train)[0, 1]\n        pearson = np.round(pearson, 2)\n        print(\n            f\"corr {var} vs target: {pearson}\")\n    ```", "```py\n    <st c=\"35234\">corr MedInc vs target: 0.69</st>\n    <st c=\"35262\">corr HouseAge vs target: 0.1</st>\n    <st c=\"35291\">corr AveRooms vs target: 0.16</st>\n    <st c=\"35321\">corr AveBedrms vs target: -0.05</st>\n    <st c=\"35353\">corr Population vs target: -0.03</st>\n    <st c=\"35422\">feature-engine</st> library’s <st c=\"35447\">DecisionTreeFeatures()</st> selects the best tree by using cross-validation.\n    ```", "```py\n     param_grid = {\"max_depth\": [2, 3, 4, None]}\n    ```", "```py\n     variables = [\"AveRooms\", \"AveBedrms\"]\n    ```", "```py\n     dtf = DecisionTreeFeatures(\n        variables=variables,\n        features_to_combine=None,\n        cv=5,\n        param_grid=param_grid,\n        scoring=\"neg_mean_squared_error\",\n        regression=True,\n    )\n    ```", "```py\n     dtf.fit(X_train, y_train)\n    ```", "```py\n     dtf.input_features_\n    ```", "```py\n    <st c=\"36971\">['AveRooms', 'AveBedrms', ['AveRooms', 'AveBedrms']]</st>\n    ```", "```py\n     train_t = dtf.transform(X_train)\n    test_t = dtf.transform(X_test)\n    ```", "```py\n     tree_features = [\n        var for var in test_t.columns if \"tree\" in var ]\n    ```", "```py\n     test_t[tree_features].head()\n    ```", "```py\n     for var in tree_features:\n        pearson = np.corrcoef(test_t[var], y_test)[0, 1]\n        pearson = np.round(pearson, 2)\n        print(\n            f\"corr {var} vs target: {pearson}\")\n    ```", "```py\n    corr tree(AveRooms) vs target: 0.37\n    corr tree(AveBedrms) vs target: 0.12\n    corr tree(['AveRooms', 'AveBedrms']) vs target: 0.47\n    ```", "```py\n     features = (('Population'), ('Population','AveOccup'),\n        ('Population', 'AveOccup', 'HouseAge'))\n    ```", "```py\n     dtf = DecisionTreeFeatures(\n        variables=None,\n        features_to_combine=features,\n        cv=5,\n        param_grid=param_grid,\n        scoring=\"neg_mean_squared_error\"\n    )\n    dtf.fit(X_train, y_train)\n    ```", "```py\n     train_t = dtf.transform(X_train)\n    test_t = dtf.transform(X_test)\n    ```", "```py\n     tree_features = [\n        var for var in test_t.columns if \"tree\" in var]\n    test_t[tree_features].head()\n    ```", "```py\n     from sklearn.linear_model import Lasso\n    from sklearn.model_selection import cross_validate\n    ```", "```py\n     lasso = Lasso(random_state=0, alpha=0.0001)\n    ```", "```py\n     cv_results = cross_validate(lasso, X_train, y_train,\n        cv=3)\n    mean = cv_results['test_score'].mean()\n    std = cv_results['test_score'].std()\n    print(f\"Results: {mean} +/- {std}\")\n    ```", "```py\n    <st c=\"40809\">Results: 0.5480403481478856 +/- 0.004214649109293269</st>\n    ```", "```py\n     variables = [\"AveRooms\", \"AveBedrms\", \"Population\"]\n    train_t = train_t.drop(variables, axis=1)\n    cv_results = cross_validate(lasso, train_t, y_train,\n        cv=3)\n    mean = cv_results['test_score'].mean()\n    std = cv_results['test_score'].std()\n    print(f\"Results: {mean} +/- {std}\")\n    ```", "```py\n    <st c=\"41447\">Results: 0.5800993721099441 +/- 0.002845475651622909</st>\n    ```", "```py\n     import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    ```", "```py\n     df = pd.DataFrame([i for i in range(24)],\n        columns=[\"hour\"])\n    ```", "```py\n     df[\"hour_sin\"] = np.sin(\n        df[\"hour\"] / df[\"hour\"].max() * 2 * np.pi)\n    df[\"hour_cos\"] = np.cos(\n        df[\"hour\"] / df[\"hour\"].max() * 2 * np.pi)\n    ```", "```py\n     plt.scatter(df[\"hour\"], df[\"hour_sin\"])\n    plt.ylabel(\"Sine of hour\")\n    plt.xlabel(\"Hour\")\n    plt.title(\"Sine transformation\")\n    ```", "```py\n     plt.scatter(df[\"hour\"], df[\"hour_cos\"])\n    plt.ylabel(\"Cosine of hour\")\n    plt.xlabel(\"Hour\")\n    plt.title(\"Cosine transformation\")\n    ```", "```py\n     fig, ax = plt.subplots(figsize=(7, 5))\n    sp = ax.scatter(\n        df[\"hour_sin\"], df[\"hour_cos\"], c=df[\"hour\"])\n    ax.set(\n        xlabel=\"sin(hour)\",\n        ylabel=\"cos(hour)\",\n    )\n    _ = fig.colorbar(sp)\n    ```", "```py\n     from feature_engine.creation import CyclicalFeatures\n    ```", "```py\n     df = pd.DataFrame()\n    df[\"hour\"] = pd.Series([i for i in range(24)])\n    df[\"month\"] = pd.Series([i for i in range(1, 13)]*2)\n    df[\"week\"] = pd.Series([i for i in range(7)]*4)\n    ```", "```py\n     cyclic = CyclicalFeatures(\n        variables=None,\n        drop_original=False,\n    )\n    ```", "```py\n     dft = cyclic.fit_transform(df)\n    ```", "```py\n     import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.linear_model import Ridge\n    from sklearn.preprocessing import SplineTransformer\n    ```", "```py\n     X = np.linspace(-1, 11, 20)\n    y = np.sin(X)\n    ```", "```py\n     plt.plot(X, y)\n    plt.ylabel(\"y\")\n    plt.xlabel(\"X\")\n    ```", "```py\n     linmod = Ridge(random_state=10)\n    linmod.fit(X.reshape(-1, 1), y)\n    pred = linmod.predict(X.reshape(-1, 1))\n    ```", "```py\n     plt.plot(X, y)\n    plt.plot(X, pred)\n    plt.ylabel(\"y\")\n    plt.xlabel(\"X\")\n    plt.legend(\n        [\"y\", \"linear\"],\n        bbox_to_anchor=(1, 1),\n        loc=\"upper left\")\n    ```", "```py\n     spl = SplineTransformer(degree=3, n_knots=5)\n    ```", "```py\n     X_t = spl.fit_transform(X.reshape(-1, 1))\n    X_df = pd.DataFrame(\n        X_t,\n        columns=spl.get_feature_names_out([\"var\"])\n    )\n    ```", "```py\n     plt.plot(X, X_t)\n    plt.legend(\n        spl.get_feature_names_out([\"var\"]),\n        bbox_to_anchor=(1, 1),\n        loc=\"upper left\")\n    plt.xlabel(\"X\")\n    plt.ylabel(\"Splines values\")\n    plt.title(\"Splines\")\n    plt.show()\n    ```", "```py\n     linmod = Ridge(random_state=10)\n    linmod.fit(X_t, y)\n    pred = linmod.predict(X_t)\n    ```", "```py\n     plt.plot(X, y)\n    plt.plot(X, pred)\n    plt.ylabel(\"y\")\n    plt.xlabel(\"X\")\n    plt.legend(\n        [\"y\", \"splines\"],\n        bbox_to_anchor=(1, 1),\n        loc=\"upper left\")\n    ```", "```py\n     from sklearn.datasets import fetch_california_housing\n    from sklearn.compose import ColumnTransformer\n    from sklearn.model_selection import cross_validate\n    ```", "```py\n     X, y = fetch_california_housing(\n        return_X_y=True, as_frame=True)\n    X.drop([\"Latitude\", \"Longitude\"], axis=1,\n        inplace=True)\n    ```", "```py\n     linmod = Ridge(random_state=10)\n    cv = cross_validate(linmod, X, y)\n    mean_, std_ = np.mean(\n        cv[«test_score\"]), np.std(cv[\"test_score\"])\n    print(f\"Model score: {mean_} +- {std_}\")\n    ```", "```py\n    <st c=\"61415\">SplineTransformer()</st> to obtain spline features from four variables by utilizing third-degree polynomials and 50 knots, and then fit the pipeline to the data:\n\n    ```", "```py\n\n    ```", "```py\n     cv = cross_validate(linmod, ct.transform(X), y)\n    mean_, std_ = np.mean(\n        cv[«test_score\"]), np.std(cv[\"test_score\"])\n    print(f\"Model score: {mean_} +- {std_}\")\n    ```", "```py\n    <st c=\"62565\">Model score: 0.5553526813919297 +- 0.02244513992785257</st>\n    ```"]