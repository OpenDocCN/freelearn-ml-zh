- en: Artificial Neural Network Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial Neural Networks** (**ANNs**) or, simply NNs, are arguably the
    most popular **machine learning** (**ML**) tool today, if not necessarily the
    most widely used. The tech media and commentary of the day love to focus on neural
    networks, and they are seen by many as the magical algorithm. It is believed that
    neural networks will pave the way to **Artificial General Intelligence** (**AGI**)—but
    the technical reality is much different.'
  prefs: []
  type: TYPE_NORMAL
- en: While they are powerful, neural networks are highly specialized ML models that
    focus on solving individual tasks or problems—they are not magical *brains* that
    can solve problems out of the box. A model that exhibits 90% accuracy is typically
    considered good. Neural networks are slow to train and require thoughtful design
    and implementation. That said, they are indeed highly proficient problem solvers
    that can unravel even very difficult problems, such as object identification in
    images.
  prefs: []
  type: TYPE_NORMAL
- en: It is likely that neural networks will play a large part in achieving AGI. However,
    many other fields of ML and **natural language processing** (**NLP**) will need
    to be involved. Because ANNs are only specialized problem solvers, it is popularly
    believed that the way toward AGI is with a large ensemble of thousands of ANNs,
    each specialized for an individual task. I personally believe that we will see
    something resembling AGI surprisingly soon. However, AGI will only be achievable
    initially through immense resources—not in terms of computation power, but rather
    in terms of training data.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn the basics of neural networks in this chapter. There are many
    ways to use neural networks, and many possible topologies for them—we'll discuss
    a number of these in this chapter and [Chapter 9](4b1941f5-8d1d-4796-9dcb-98067bcc5625.xhtml), *Deep
    Neural Networks*. Each neural network topology has its own purpose, strengths,
    and weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: First, we'll discuss neural networks conceptually. We'll examine their components
    and construction and explore their applications and strengths. We'll have a discussion
    on the backpropagation algorithm and how ANNs are trained. Then we'll take a brief
    peek at the mathematics of ANNs, before diving into some practical advice for
    neural networks in the wild. Finally, we'll demonstrate an example of a simple
    neural network using the `TensorFlow.js` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the topics that we will be covering in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Conceptual overview of neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example—XOR in `TensorFlow.js`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conceptual overview of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ANNs have been around almost as long as computers have, and indeed were originally
    constructed out of electrical hardware. One of the first ANNs was developed in
    the 1970s to adaptively filter echoes out of phone line transmissions. Despite
    their initial early success, ANNs waned in popularity until the mid-1980s, when
    the backpropagation training algorithm was popularized.
  prefs: []
  type: TYPE_NORMAL
- en: 'ANNs are modeled on our understanding of biological brains. An ANN contains
    many neurons that connect to one another. The manner, structure, and organization
    of these neuronal connections is called the **topology** (or **shape**) of the
    network. Each individual neuron is a simple construct: it accepts several numerical
    input values and outputs a single numerical value, which may in turn be transmitted
    to several other neurons. The following is a simple, conceptual example of a neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8aff5fa-7f2c-459c-bff1-a23a2a686c65.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Neurons are typically, but not always, arranged into layers. The specific arrangement
    and connections between neurons is defined by the network''s topology. However,
    most ANNs will have three or four fully-connected layers, or layers where each
    neuron in the layer connects to every neuron in the next layer. In these common
    topologies, the first layer is the input layer and the last layer is the output
    layer. Input data is fed directly to the input neurons, and the results of the
    algorithm are read from the output neurons. In between the input and output layers,
    there are typically one or two hidden layers made up of neurons that the user
    or programmer doesn''t interact with directly. The following diagram shows a neural
    network with three layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e64e195-6ec8-4b10-917b-34bd80678c4c.png)'
  prefs: []
  type: TYPE_IMG
- en: The input layer has four neurons, the single hidden layer has six neurons, and
    the output layer has two neurons. A shorthand for describing this type of network
    is to list the number of neurons in each layer, so this could be called a **4-6-2
    network** for short. Such a network is capable of accepting four different features
    and can output two pieces of information, such as X/Y coordinates, true/false
    values for two properties, or even the numbers 0-3 if the output is taken as binary
    bits.
  prefs: []
  type: TYPE_NORMAL
- en: When using an ANN to make a prediction, you are using the network in feed-forward
    mode, which is actually quite straightforward. We will discuss the mechanics of
    neurons in depth, but for now all you need to know is that a neuron takes a number
    of inputs and generates a single output based on simple weighted sums and a smoothing
    function (called the **activation function**).
  prefs: []
  type: TYPE_NORMAL
- en: To make a prediction, you load your input data directly into the input neurons.
    If your problem is an image recognition problem, then each input neuron might
    be fed the grayscale intensity of a single pixel (you would need 2,500 input neurons
    to process a 50 x 50 pixel grayscale image). The input neurons are activated,
    meaning their inputs are summed up, weighted, biased, and the result fed into
    an activation function which will return a numerical value (typically between
    -1 and +1, or between 0 and +1). The input neurons in turn send their activation
    outputs to the neurons in the hidden layer, which experience the same process,
    and send their results to the output layer, which again becomes activated. The
    result of the algorithm is the values of the activation functions at the output
    layer. If your image recognition problem is a classification problem with 15 possible
    classes, you would have 15 neurons in the output layer, each representing a class
    label. The output neurons will either return values of 1 or 0 (or fractions in
    between), and the output neurons with the highest values are the classes that
    are most likely represented by the image.
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand how networks like these actually produce results, we
    need to take a closer look at the neuron. Neurons in ANNs have a few different
    properties. First, a neuron maintains a set (a vector) of weights. Each input
    to the neuron is multiplied by its corresponding weight. If you look at the topmost
    neuron in the hidden layer in the preceding image, you can see that it receives
    four inputs from the neurons in the input layer. The hidden-layer neurons therefore
    must each have a vector of four weights, one for each of the neurons in the previous
    layer that send it signals. The weights essentially determine how important a
    specific input signal is to the neuron in question. For instance, the topmost
    hidden-layer neuron might have a weight of 0 for the bottom-most input neuron;
    in that case, the two neurons are essentially unconnected. On the other hand,
    the next hidden neuron might have a very high weight for the bottom-most input
    neuron, meaning that it considers its input very strongly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each neuron also has a bias. The bias does not apply to any one single input,
    but instead is added to the sum of the weighted inputs before the activation function
    is invoked. The bias can be seen as a modifier to the threshold of the neuron''s
    activation. We''ll discuss activation functions shortly, but let''s take a look
    at an updated diagram of the neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19a65ada-0abf-4e14-a74d-fa097f51a1f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A mathematical form for the description of the neuron goes something like this,
    where bold figures **w** and **x** represent vectors of inputs and weights (that
    is, [x[1], x[2], x[3]]), non-bold *b* and *y* represent the bias of the neuron
    and output of the neuron, respectively, and *fn(...)* represents the activation
    function. Here goes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = fn( **w·x** + b)*'
  prefs: []
  type: TYPE_NORMAL
- en: The dot in between **w** and **x** is the vector dot product of the two vectors.
    Another way to write **w·x** would be *w[1]*x[1] + w[2]*x[2] + w[3]*x[3] + … +
    w[n]*x[n]*, or simply Σ*[j] w[j]*x[j]* .
  prefs: []
  type: TYPE_NORMAL
- en: Taken together, it is the weights and the biases of the neurons in the network
    that actually do the learning and calculation. When you train a neural network,
    you are gradually updating the weights and biases with the goal of configuring
    them to solve your problem. Two neural networks with the same topology (for example,
    two fully-connected 10-15-5 networks) but different weights and biases are different
    networks that will solve different problems.
  prefs: []
  type: TYPE_NORMAL
- en: How does the activation function factor into all of this? The original model
    for an artificial neuron was called the perceptron, and its activation function
    was a step function. Basically, if ***w·x** + b* for a neuron was greater than
    zero, the neuron would output 1\. If, on the other hand, ***w·x** + b* were less
    than zero, the neuron would output zero.
  prefs: []
  type: TYPE_NORMAL
- en: This early perceptron model was powerful because it was possible to represent
    logic gates with an artificial neuron. If you've ever taken a course on Boolean
    logic or circuits, you would have learned that you can use NAND gates to build
    any other type of logic gate, and it is trivially easy to build a NAND gate with
    a perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a perceptron that takes two inputs, each with a weight of -2\. The perceptron's
    bias is +3\. If both inputs are 0, then ***w·x** + b = +3* (just the weight, because
    all inputs are zero). Since the perceptron's activation function is a step function,
    the output of the neuron in this case will be 1 (+3 is greater than zero, so the
    step function returns +1).
  prefs: []
  type: TYPE_NORMAL
- en: 'If the inputs are 1 and 0, in any order, then ***w·x** + b = +1*, and therefore
    the output of the perceptron will also be 1\. However, if both inputs are instead
    1, then ***w·x** + b = -1*. The two inputs, both weighted -2, will overcome the
    neuron''s bias of +3, and the activation function (which returns 1 or 0), will
    return 0\. This is the logic of the NAND gate: the perceptron will return 0 if
    both inputs are 1, otherwise it will return 1 for any other combination of inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: These early results excited the computer science and electronics community in
    the 1970s, and ANNs received a lot of hype. However, we had difficulty automatically
    training neural networks. Perceptron's could be crafted by hand to represent logic
    gates, and some amount of automated training for neural networks was possible,
    but large-scale problems remained inaccessible.
  prefs: []
  type: TYPE_NORMAL
- en: The problem was the step function used as the perceptron's activation function.
    When training an ANN, you want small changes to the weights or biases of the network
    to similarly result in only small changes to the network's output. But the step
    function gets in the way of the process; one small change to the weights might
    result in no change to the output, but the next small change to the weights could
    result in a huge change to the output! This happens because the step function
    is not a smooth function—it has an abrupt jump from 0 to 1 once the threshold
    is crossed, and it is exactly 0 or exactly 1 at all other points. This limitation
    of the perceptron, and thus the major limitation of ANNs, resulted in over a decade
    of research stagnation.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, researchers in 1986 rediscovered a training technique that had been
    discovered a few years prior. They found that this technique, called **backpropagation**,
    made training much faster and more reliable. Thus artificial neural networks experienced
    their second wind.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There was one key insight that brought neural network research out of stagnation
    and into the modern era: the choice of a better activation function for neurons.
    Step functions caused issues with the automated training of networks because tiny
    changes in the network parameters (the weights and biases) could alternately have
    either no effect or an abrupt major effect on the network. Obviously, this is
    not a desired property of a trainable system.'
  prefs: []
  type: TYPE_NORMAL
- en: The general approach to automatically training ANNs is to start with the output
    layer and work backwards. For each example in your training set, you run the network
    in feed-forward mode (that is, **prediction mode**) and compare the actual output
    to the desired output. A good metric to use for comparing desired versus actual
    results is **mean squared error** (**MSE**); test all training examples, and for
    each calculate and square the difference in output from the desired values. Sum
    all the squared errors up and average over the number of training examples, and
    you have a cost function or loss function. The cost function is a function of
    the weights and biases of a given network topology. The goal in training ANNs
    is to reduce the cost function to—ideally—zero. You could potentially use the
    ANN's accuracy over all training examples as a cost function, but mean-squared
    error has better mathematical properties for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The backpropagation algorithm hinges on the following insight: if you know
    the weights and biases of all neurons, if you know the inputs and desired outputs,
    and if you know the activation function the neurons use, you can work backwards
    from an output neuron to discover which weights or biases are contributing to
    a large error. That is, if neuron Z has neuron inputs A, B, and C with weights
    of 100, 10, and 0, respectively, you would know that neuron C has no effect on
    neuron Z and therefore neuron C is not contributing to neuron Z''s error. On the
    other hand, neuron A has an outsized impact on neuron Z, so if neuron Z has a
    large error it is likely that neuron A is to blame. The backpropagation algorithm
    is named such because it propagates the error in output neurons backwards through
    the network.'
  prefs: []
  type: TYPE_NORMAL
- en: Taking this concept a step further, if you also know the activation function
    and its relationship between the weights, biases, and the errors, you can determine
    how much a weight would need to change in order to get a corresponding change
    to a neuron's output. Of course, there are many weights in an ANN and it is a
    highly complex system, so the approach we use is to make tiny changes to the weights—we
    can only predict changes to the network’s output if we use a simplifying approximation
    for small changes in weights. This part of the approach is called gradient descent,
    which is named such because we are trying to descend the gradient (the slope)
    of the cost function by making small modifications to weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: 'To picture this, imagine a nylon hammock hanging between two trees. The hammock
    represents the cost function, and the *x* and *y* axes (viewed from the sky) abstractly
    represent the biases and weights of the network (in reality, this is a multi-thousand-dimensional
    picture). There is some combination of weights and biases where the hammock hangs
    the lowest: that point is our goal. We are a tiny ant sitting somewhere on the
    surface of the hammock. We don''t know where the lowest point of the hammock is,
    and we''re so small that even wrinkles or creases in the fabric can throw us off.
    But we do know that the hammock is smooth and continuous, and we can feel around
    our immediate area. As long as we keep heading downhill for each individual step
    we take, we will eventually find the lowest point in the hammock—or, at least,
    a low point close to where we started (a local minimum), depending on how complex
    the shape of the hammock is.'
  prefs: []
  type: TYPE_NORMAL
- en: This approach of gradient descent requires us to mathematically understand and
    be able to describe the gradient of the cost function, which means we must also
    understand the gradient of the activation function. The gradient of a function
    is essentially its slope, or derivative. The reason we can’t use the perceptron's
    original step function as an activation function is because the step function
    is not differentiable at all points; the giant, instantaneous leap between 0 and
    1 in the step function is a non-differentiable discontinuity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we figured out that we should be using gradient descent and backpropagation
    to train our neural networks, the rest came easily. Instead of using a step function
    for neuron activation functions, we started using sigmoid functions. Sigmoid functions
    are generally shaped like step functions, except they are smoothed out, continuous,
    and differentiable at all points. Here''s an example of a sigmoid function versus
    a step function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9cd7a8e7-fa84-425a-966c-3efa806bc7ff.png)'
  prefs: []
  type: TYPE_IMG
- en: There are many types of sigmoid functions; the preceding one is described by
    the equation *y = 1 / 1+e^(-x)* and is called the **logistic function** or **logistic
    curve**. Other popular sigmoid functions are hyperbolic tangent (that is, tanh),
    which has a range from -1 to +1 as opposed to the logistic function's range of
    0 to +1\. Another popular activation function is the **rectified linear unit**
    (**ReLU**), which is often used in image processing and output layers. There is
    also the *softplus* function, whose derivative is in fact the logistic function
    itself. The activation function you choose will depend on the specific mathematical
    properties you desire. It is also not uncommon to use different activation functions
    in different layers of the network; hidden layers will often use logistic or tanh
    activation functions, while the output layer might use *softmax* and the input
    layer might use ReLU. You can invent your own activation function for your neurons,
    however, you must be able to differentiate the function and determine its gradient
    in order to integrate it with the backpropagation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: This singular, minor change to the neuron's activation function made a world
    of difference to our training of ANNs. Once we started using differentiable activation
    functions, we were able to calculate the gradient of the cost and activation functions,
    and use that information to determine precisely how to update weights in the backpropagation
    algorithm. Neural network training became faster and more powerful, and neural
    networks were propelled into the modern era, though they still had to wait for
    hardware and software libraries to catch up. More importantly, neural network
    training became a study in mathematics—particularly vector calculus—rather than
    being limited to study by computer scientists.
  prefs: []
  type: TYPE_NORMAL
- en: Example - XOR in TensorFlow.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we're going to solve the XOR problem using a `TensorFlow.js`
    feedforward neural network. First, let's explore the XOR problem, and why it's
    a good starting point for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'The XOR, or *exclusive or* operation, is a Boolean operator that returns true
    if only one, but not both, of its inputs is truth. Compare this to the regular
    Boolean OR that you''re more commonly familiar with, which will return true if
    both inputs are true—the XOR will return false if both inputs are true. Here is
    a table comparing XOR to OR; I''ve highlighted the case where OR and XOR differ:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input 1** | **Input 2** | **OR** | **XOR** |'
  prefs: []
  type: TYPE_TB
- en: '| False | False | False | False |'
  prefs: []
  type: TYPE_TB
- en: '| False | True | True | True |'
  prefs: []
  type: TYPE_TB
- en: '| True | False | True | True |'
  prefs: []
  type: TYPE_TB
- en: '| True | True | **True** | **False** |'
  prefs: []
  type: TYPE_TB
- en: 'Why is the XOR problem a good test for us? Let''s plot the XOR operations on
    a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/899339f7-7f6e-42d9-b834-adee57e8fd30.png)'
  prefs: []
  type: TYPE_IMG
- en: Viewing the preceding graph, we can see that the two classes involved in the
    **XOR** operation are not linearly separable. In other words, it is impossible
    to draw a straight line that separates the circles from the X in the preceding
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: The combined facts that the XOR operation is very simple but also that the classes
    are not linearly separable make the XOR operation an excellent entry point when
    testing out a new classification algorithm. You don't need a fancy dataset to
    test out whether a new library or algorithm will work for you.
  prefs: []
  type: TYPE_NORMAL
- en: Before jumping into the TensorFlow example, let's first discuss how we might
    build an XOR-solving neural network by hand. We will design our own weights and
    biases and see whether we can develop a manual neural network that solves XOR.
  prefs: []
  type: TYPE_NORMAL
- en: First, we know that the network requires two inputs and one output. We know
    that the inputs and output are binary, so we must pick activations functions that
    have the range [0, 1]; ReLU or sigmoid would be appropriate, while tanh, whose
    range is [-1, 1] would be less appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we know that XOR is not linearly separable and thus cannot be trivially
    solved; we need a hidden layer in our network. Let''s therefore attempt to build
    a 2-2-1 neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46fef899-238c-4a22-9621-3586a55b7732.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we need to think about the weights and biases of the neurons in the network.
    We know that the network needs to be designed such that there is a penalty for
    both inputs being true. Therefore, one hidden-layer neuron should represent a
    weakly positive signal (that is, it activates when the inputs are activated) and
    the other hidden-layer neuron should represent a strongly negative signal (that
    is, if both inputs are true, this neuron should overwhelm the weakly-positive
    neuron).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example of one set of weights that would work to achieve XOR:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06a64a8c-ae56-4816-94a8-2307194ee0dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's run a few example calculations. I'll start with the differentiating case
    of both inputs being true. The hidden **h1** neuron will have a total weighted
    input of 4, because the weight for each input is 2 and both inputs are true. The
    h1 neuron also has a bias of -1, however, the bias is not enough to deactivate
    the neuron. The total sum of the biased inputs is therefore 3 for the h1 neuron;
    since we haven't decided on a specific activation function, we will not try to
    guess what the actual activation will become—suffice to say that an input of +3
    is enough to activate the neuron.
  prefs: []
  type: TYPE_NORMAL
- en: We now turn our attention to the hidden **h2** neuron. It also receives input
    from both input neurons, however, these weights are negative, and so the unbiased
    input sum it receives is -4\. The bias on h2 is +3, so the total biased input
    for h2 is -1\. If we were to choose the ReLU activation function, the neuron's
    output would be zero. In any case, h2 is not activated.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we look at the output node. It receives a weighted input of +2 from
    h1, but receives no input from h2\. Since the output node’s bias is -3 (essentially
    requiring both h1 and h2 to be activated), the output node will return 0 or false.
    This is the expected result for XOR with both inputs being set to true or 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s similarly tabulate the results for the other XOR cases. The columns
    for `h1`, `h2`, and `Out` represent the weighted and biased input to the neuron,
    before being applied to an activation function (since we haven''t chosen one).
    Just remember that each neuron will transmit values in [0, 1] to the next neuron;
    it will not send values such as -1 or 3 after the activation function is applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **In 1** | **In 2** | **h1** | **h2** | **Out** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | -1 | 3 | -1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 3 | -1 | -1 |'
  prefs: []
  type: TYPE_TB
- en: The preceding table proves that the handcrafted ANN works for all XOR test cases.
    It also gives us a little insight into the inner workings of a network. The hidden
    h1 and h2 neurons have specific roles. The h1 neuron is off by default but easily
    satisfied and will become activated if any of the inputs are active; h1 is essentially
    a typical OR operation. On the other hand, h2 is on by default and can only be
    deactivated if both inputs are on; h2 is essentially a NAND operation. The output
    neuron requires both h1 and h2 to be active, therefore the output neuron is an
    AND operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now use the `TensorFlow.js` library and see whether we can achieve the
    same success. On your computer, create a new folder called `Ch8-ANN`. Add the
    following `package.json` file and then issue `yarn install`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now add the `src/index.js` file and import TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow is not simply an ANN library. The TensorFlow library provides a number
    of building blocks that are useful in both ANNs and general ML and linear algebra
    (that is, vector/matrix math) problems. Because TensorFlow is more of a toolbox
    than a singular tool, there will always be many ways to solve any given problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating a sequential model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow *models* are high-level containers that essentially run functions;
    they are mapping from input to output. You can use TensorFlow's low-level operators
    (the linear algebra tools that come with the library) to build your model, or
    you can use one of the higher-level model classes. In this case, we are building
    a *sequential model*, which is a special case of TensorFlow's generic model. You
    may think of a sequential model as a neural network that only feeds forward, and
    does not involve any recurrences or feedback loops internally. A sequential model
    is essentially a vanilla neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s add layers to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We are adding three layers to our model. All layers are **dense** layers, meaning
    they are fully connected to the next layer. This is what you'd expect from a vanilla
    neural network. We've specified the *units* for each layer—units is TensorFlow's
    name for neurons, since TensorFlow can be used outside of ANN contexts. I've set
    this example up with four neurons per layer rather than two, because I found that
    the extra neurons greatly improve the speed and resilience of the training process.
    We've specified `inputDim`in the first layer, telling the layer that it should
    expect two inputs per data point. The first and second layers use the ReLU activation
    function. The third layer, which is the output layer, has only one unit/neuron
    and uses the familiar sigmoid activation function, since I would like the results
    to snap more readily toward 0 or 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we must compile the model before we can use it. We will specify a loss
    function, which can be either a prebuilt loss function that ships with the library
    or a custom loss function that we provide. We will also specify our optimizer;
    we discussed gradient descent earlier in this chapter, but there are many other
    optimizers available, such as Adam, Adagrad, and Adadelta. In this case, we will
    use the stochastic gradient descent optimizer (typical for vanilla neural networks),
    however, we will choose the `binaryCrossentropy` loss function, which is more
    appropriate than mean squared error for our binary classification task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We've also set the learning rate for the gradient descent optimizer; the learning
    rate dictates how much the backpropagation training algorithm will modify the
    weights and biases in each training generation or epoch. A lower learning rate
    will result in a longer time to train the network, but will be more stable. A
    higher learning rate will train the network more quickly, but less reliably; your
    network may not converge at all if the learning rate is too high.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we''ve added `metrics: [''accuracy'']` to the compilation step. This
    allows us to get a report on the network''s accuracy when we eventually call `model.evaluate`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we''ll set up our training data, which is simply four data points. TensorFlow
    operates on *tensors*, which are essentially mathematical matrices. TensorFlow
    tensors are immutable, and all operations performed on tensors will return new
    tensors rather than modifying the existing tensor. If you need to modify tensors
    in place, you must use TensorFlow''s *variables,* which are mutable wrappers around
    tensors. TensorFlow requires all math be performed through tensors so that the
    library can optimize calculations for GPU processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Because tensors are matrices, every tensor has a *shape*. Shape, for 2D tensors,
    is defined as *[rows, cols]*. For 3D tensors, shape is *[rows, cols, depth]*;
    image processing typically uses 3D tensors, where rows and cols represent pixel
    Y and X coordinates, and depth represents a color channel (for example, RGBA)
    for that pixel. Since we have four training examples, and each training example
    requires two fields for inputs, our input tensor has a shape of four rows by two
    columns. Similarly, our target values tensor has a shape of four rows by one column.
    TensorFlow will throw errors if you attempt to run calculations or train models
    with the wrong input and output shapes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our last step is to train the model with the data and then evaluate the model.
    TensorFlow''s `model.fit` method is what trains the model, and once trained we
    can use `model.evaluate` to get statistics, such as accuracy and loss, and we
    can also use `model.predict` to run the model in feedforward or prediction mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you''ve added the code, run `yarn start` from the command line. Running
    this model takes about 60 seconds for me. When the model finishes, you should
    see something similar to the following as output. Note that ANNs and the stochastic
    gradient descent optimizer use random values for initialization and processing,
    and therefore some runs of the model may be unsuccessful, depending on the specific
    random initial conditions. The following is the output that will be obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output shows that the model has learned to emulate XOR. The loss
    value is very low, while the accuracy is 1.0, which is what's required for such
    a simple problem. In real-world problems, accuracies of 80-90% are more realistic.
    Additionally, the program's output shows the individual predictions for each of
    the four test cases. You can see the effect of the sigmoid activation function
    in that the values get very close to 0 and 1, but don't quite get there. Internally,
    TensorFlow is rounding these values in order to determine whether the classification
    was correct.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should play with the network parameters a bit. What happens
    if you reduce the number of training epochs? What happens if you switch the ReLU
    layers to sigmoid layers? What happens if you reduce the number of units/neurons
    in the first two layers to two? Does it work if you increase the number of training
    epochs? What is the effect of the learning rate on the training process? These
    are things that are best discovered through trial and error rather than lecture.
    This is an infinitely flexible neural network model, capable of handling much
    more complex problems than the simple XOR example, so you should become intimately
    familiar with all of these properties and parameters through experimentation and
    research.
  prefs: []
  type: TYPE_NORMAL
- en: While this example was only a simple XOR sample, this approach can also be used
    for many other types of ANN problems. We've created a three-layered binary classifier
    that automatically trains and evaluates itself—it's the ultimate vanilla neural
    network. I will leave it to you to take these concepts and apply them to your
    real-world problems, though in the next chapter we'll try out some advanced neural
    models, such as convolutional and recurrent networks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced and described the concept of artificial neural networks.
    We first discussed ANNs from a conceptual standpoint. You learned that neural
    networks are made of individual neurons, which are simple weighted adding machines
    that can apply an activation function to their output. You learned that neural
    networks can have many topologies, and it is the topology and the weights and
    biases between neurons in the network that do the actual work. You also learned
    about the backpropagation algorithm, which is the method by which neural networks
    are automatically trained.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at the classic XOR problem and looked at it through the lens
    of neural networks. We discussed the challenges and the approach to solving XOR
    with ANNs, and we even built—by hand!—a fully-trained ANN that solves the XOR
    problem. We then introduced the `TensorFlow.js` library and built a vanilla neural
    network with it, and successfully used that NN to train and solve the XOR problem.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to take a deeper look at the advanced ANN topologies.
    In particular, we'll discuss the **Convolutional Neural Network** (**CNN**), which
    is widely used in image processing, and we'll also look at **recurrent neural
    networks** (**RNN**), which are commonly used in artificial intelligence and natural
    language tasks.
  prefs: []
  type: TYPE_NORMAL
