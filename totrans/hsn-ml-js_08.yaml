- en: Artificial Neural Network Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络算法
- en: '**Artificial Neural Networks** (**ANNs**) or, simply NNs, are arguably the
    most popular **machine learning** (**ML**) tool today, if not necessarily the
    most widely used. The tech media and commentary of the day love to focus on neural
    networks, and they are seen by many as the magical algorithm. It is believed that
    neural networks will pave the way to **Artificial General Intelligence** (**AGI**)—but
    the technical reality is much different.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络（ANNs**）或简称NNs，可能是今天最流行的**机器学习（ML**）工具，如果不是最广泛使用的。当时的科技媒体和评论喜欢关注神经网络，许多人认为它们是神奇的算法。人们相信神经网络将为**通用人工智能（AGI**）铺平道路——但技术现实却大不相同。'
- en: While they are powerful, neural networks are highly specialized ML models that
    focus on solving individual tasks or problems—they are not magical *brains* that
    can solve problems out of the box. A model that exhibits 90% accuracy is typically
    considered good. Neural networks are slow to train and require thoughtful design
    and implementation. That said, they are indeed highly proficient problem solvers
    that can unravel even very difficult problems, such as object identification in
    images.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然神经网络功能强大，但它们是高度专业化的机器学习模型，专注于解决单个任务或问题——它们不是可以现成解决问题的神奇**大脑**。一个表现出90%准确率的模型通常被认为是好的。神经网络训练缓慢，需要精心设计和实现。尽管如此，它们确实是高度熟练的问题解决者，可以解开甚至非常困难的问题，例如图像中的物体识别。
- en: It is likely that neural networks will play a large part in achieving AGI. However,
    many other fields of ML and **natural language processing** (**NLP**) will need
    to be involved. Because ANNs are only specialized problem solvers, it is popularly
    believed that the way toward AGI is with a large ensemble of thousands of ANNs,
    each specialized for an individual task. I personally believe that we will see
    something resembling AGI surprisingly soon. However, AGI will only be achievable
    initially through immense resources—not in terms of computation power, but rather
    in terms of training data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可能在实现通用人工智能（AGI）中扮演重要角色。然而，许多其他机器学习（ML）和**自然语言处理（NLP**）领域也将需要参与其中。因为人工神经网络（ANNs）仅仅是专门的问题解决者，普遍认为通往AGI的道路是通过成千上万的人工神经网络的大集合，每个网络针对一个特定的任务进行优化。我个人认为，我们可能会很快看到类似AGI的东西。然而，AGI最初只能通过巨大的资源来实现——不是指计算能力，而是指训练数据。
- en: You will learn the basics of neural networks in this chapter. There are many
    ways to use neural networks, and many possible topologies for them—we'll discuss
    a number of these in this chapter and [Chapter 9](4b1941f5-8d1d-4796-9dcb-98067bcc5625.xhtml), *Deep
    Neural Networks*. Each neural network topology has its own purpose, strengths,
    and weaknesses.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习神经网络的基础知识。神经网络有许多使用方式，以及许多可能的拓扑结构——我们将在本章和第9章[深度神经网络](4b1941f5-8d1d-4796-9dcb-98067bcc5625.xhtml)中讨论其中的一些。每种神经网络拓扑都有其自身的目的、优势和劣势。
- en: First, we'll discuss neural networks conceptually. We'll examine their components
    and construction and explore their applications and strengths. We'll have a discussion
    on the backpropagation algorithm and how ANNs are trained. Then we'll take a brief
    peek at the mathematics of ANNs, before diving into some practical advice for
    neural networks in the wild. Finally, we'll demonstrate an example of a simple
    neural network using the `TensorFlow.js` library.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从概念上讨论神经网络。我们将检查它们的组件和构建，并探讨它们的应用和优势。我们将讨论反向传播算法以及如何训练人工神经网络。然后，我们将简要地看一下人工神经网络的数学，接着深入探讨野外神经网络的实用建议。最后，我们将使用`TensorFlow.js`库演示一个简单神经网络的例子。
- en: 'The following are the topics that we will be covering in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Conceptual overview of neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的概念概述
- en: Backpropagation training
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播训练
- en: Example—XOR in `TensorFlow.js`
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例——`TensorFlow.js`中的XOR
- en: Conceptual overview of neural networks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的概念概述
- en: ANNs have been around almost as long as computers have, and indeed were originally
    constructed out of electrical hardware. One of the first ANNs was developed in
    the 1970s to adaptively filter echoes out of phone line transmissions. Despite
    their initial early success, ANNs waned in popularity until the mid-1980s, when
    the backpropagation training algorithm was popularized.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）几乎与计算机一样历史悠久，最初确实是由电子硬件构建的。第一个ANN是在20世纪70年代开发的，用于自适应地过滤电话线路传输中的回声。尽管它们最初取得了早期成功，但ANNs在1985年中期之前一直不太受欢迎，那时反向传播训练算法被普及。
- en: 'ANNs are modeled on our understanding of biological brains. An ANN contains
    many neurons that connect to one another. The manner, structure, and organization
    of these neuronal connections is called the **topology** (or **shape**) of the
    network. Each individual neuron is a simple construct: it accepts several numerical
    input values and outputs a single numerical value, which may in turn be transmitted
    to several other neurons. The following is a simple, conceptual example of a neuron:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs是基于我们对生物大脑的理解构建的。ANN包含许多相互连接的神经元。这些神经元连接的方式、结构和组织被称为网络的**拓扑结构**（或**形状**）。每个单独的神经元是一个简单的结构：它接受几个数值输入值，并输出一个数值，这个数值可能随后被传输到几个其他神经元。以下是一个简单的、概念性的神经元示例：
- en: '![](img/f8aff5fa-7f2c-459c-bff1-a23a2a686c65.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图](img/f8aff5fa-7f2c-459c-bff1-a23a2a686c65.png)'
- en: 'Neurons are typically, but not always, arranged into layers. The specific arrangement
    and connections between neurons is defined by the network''s topology. However,
    most ANNs will have three or four fully-connected layers, or layers where each
    neuron in the layer connects to every neuron in the next layer. In these common
    topologies, the first layer is the input layer and the last layer is the output
    layer. Input data is fed directly to the input neurons, and the results of the
    algorithm are read from the output neurons. In between the input and output layers,
    there are typically one or two hidden layers made up of neurons that the user
    or programmer doesn''t interact with directly. The following diagram shows a neural
    network with three layers:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元通常但并非总是排列成层。神经元的具体排列和连接由网络的拓扑结构定义。然而，大多数人工神经网络（ANNs）将具有三到四个全连接层，或者每一层的每个神经元都连接到下一层的每个神经元的层。在这些常见的拓扑结构中，第一层是输入层，最后一层是输出层。输入数据直接馈送到输入神经元，算法的结果从输出神经元读取。在输入层和输出层之间，通常有一到两个用户或程序员不直接交互的隐藏层。以下图显示了具有三个层的神经网络：
- en: '![](img/0e64e195-6ec8-4b10-917b-34bd80678c4c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图](img/0e64e195-6ec8-4b10-917b-34bd80678c4c.png)'
- en: The input layer has four neurons, the single hidden layer has six neurons, and
    the output layer has two neurons. A shorthand for describing this type of network
    is to list the number of neurons in each layer, so this could be called a **4-6-2
    network** for short. Such a network is capable of accepting four different features
    and can output two pieces of information, such as X/Y coordinates, true/false
    values for two properties, or even the numbers 0-3 if the output is taken as binary
    bits.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层有四个神经元，单个隐藏层有六个神经元，输出层有两个神经元。描述这种网络的简写方法是列出每层的神经元数量，因此可以简称为**4-6-2网络**。这样的网络能够接受四个不同的特征，并输出两份数据，例如X/Y坐标，两个属性的布尔值，或者如果输出被视为二进制位，甚至可以是0-3的数字。
- en: When using an ANN to make a prediction, you are using the network in feed-forward
    mode, which is actually quite straightforward. We will discuss the mechanics of
    neurons in depth, but for now all you need to know is that a neuron takes a number
    of inputs and generates a single output based on simple weighted sums and a smoothing
    function (called the **activation function**).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用ANN进行预测时，你实际上是在使用前馈模式下的网络，这实际上相当简单。我们将深入讨论神经元的机制，但就目前而言，你需要知道的是，一个神经元接受多个输入，并根据简单的加权总和和光滑函数（称为**激活函数**）生成单个输出。
- en: To make a prediction, you load your input data directly into the input neurons.
    If your problem is an image recognition problem, then each input neuron might
    be fed the grayscale intensity of a single pixel (you would need 2,500 input neurons
    to process a 50 x 50 pixel grayscale image). The input neurons are activated,
    meaning their inputs are summed up, weighted, biased, and the result fed into
    an activation function which will return a numerical value (typically between
    -1 and +1, or between 0 and +1). The input neurons in turn send their activation
    outputs to the neurons in the hidden layer, which experience the same process,
    and send their results to the output layer, which again becomes activated. The
    result of the algorithm is the values of the activation functions at the output
    layer. If your image recognition problem is a classification problem with 15 possible
    classes, you would have 15 neurons in the output layer, each representing a class
    label. The output neurons will either return values of 1 or 0 (or fractions in
    between), and the output neurons with the highest values are the classes that
    are most likely represented by the image.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做出预测，你直接将输入数据加载到输入神经元中。如果你的问题是图像识别问题，那么每个输入神经元可能被提供单个像素的灰度强度（处理一个50 x 50像素的灰度图像可能需要2,500个输入神经元）。输入神经元被激活，意味着它们的输入被求和、加权、加偏差，然后将结果输入到激活函数中，该函数将返回一个数值（通常在-1和+1之间，或0和+1之间）。输入神经元随后将它们的激活输出发送到隐藏层的神经元，这些神经元经历相同的过程，并将结果发送到输出层，输出层再次被激活。算法的结果是输出层激活函数的值。如果你的图像识别问题是具有15个可能类别的分类问题，那么输出层将有15个神经元，每个神经元代表一个类别标签。输出神经元将返回1或0的值（或介于两者之间的分数），具有最高值的输出神经元是图像最可能代表的类别。
- en: In order to understand how networks like these actually produce results, we
    need to take a closer look at the neuron. Neurons in ANNs have a few different
    properties. First, a neuron maintains a set (a vector) of weights. Each input
    to the neuron is multiplied by its corresponding weight. If you look at the topmost
    neuron in the hidden layer in the preceding image, you can see that it receives
    four inputs from the neurons in the input layer. The hidden-layer neurons therefore
    must each have a vector of four weights, one for each of the neurons in the previous
    layer that send it signals. The weights essentially determine how important a
    specific input signal is to the neuron in question. For instance, the topmost
    hidden-layer neuron might have a weight of 0 for the bottom-most input neuron;
    in that case, the two neurons are essentially unconnected. On the other hand,
    the next hidden neuron might have a very high weight for the bottom-most input
    neuron, meaning that it considers its input very strongly.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解像这样的网络实际上是如何产生结果的，我们需要更仔细地研究神经元。在人工神经网络中，神经元有几个不同的特性。首先，一个神经元保持一组（一个向量）的权重。每个输入到神经元的值都乘以其相应的权重。如果你看前面图像中隐藏层最顶部的神经元，你可以看到它从输入层的神经元接收四个输入。因此，隐藏层中的每个神经元必须有一个包含四个权重的向量，每个权重对应于前一层发送信号的神经元。权重基本上决定了特定输入信号对相关神经元的重要性。例如，最顶部的隐藏层神经元可能对最底部的输入神经元有一个权重为0；在这种情况下，两个神经元基本上是未连接的。另一方面，下一个隐藏神经元可能对最底部的输入神经元有一个非常高的权重，这意味着它非常重视其输入。
- en: 'Each neuron also has a bias. The bias does not apply to any one single input,
    but instead is added to the sum of the weighted inputs before the activation function
    is invoked. The bias can be seen as a modifier to the threshold of the neuron''s
    activation. We''ll discuss activation functions shortly, but let''s take a look
    at an updated diagram of the neuron:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元还有一个偏差。偏差不适用于任何一个单独的输入，而是在激活函数被调用之前添加到加权输入的总和中。偏差可以看作是神经元激活阈值的修饰器。我们很快就会讨论激活函数，但让我们先看看神经元的更新图示：
- en: '![](img/19a65ada-0abf-4e14-a74d-fa097f51a1f8.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/19a65ada-0abf-4e14-a74d-fa097f51a1f8.png)'
- en: 'A mathematical form for the description of the neuron goes something like this,
    where bold figures **w** and **x** represent vectors of inputs and weights (that
    is, [x[1], x[2], x[3]]), non-bold *b* and *y* represent the bias of the neuron
    and output of the neuron, respectively, and *fn(...)* represents the activation
    function. Here goes:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 描述神经元的数学形式大致如下，其中粗体数字 **w** 和 **x** 代表输入和权重的向量（即[x[1], x[2], x[3]]），非粗体 *b*
    和 *y* 分别代表神经元的偏差和输出，而 *fn(...)* 代表激活函数。下面是具体内容：
- en: '*y = fn( **w·x** + b)*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = fn( **w·x** + b)*'
- en: The dot in between **w** and **x** is the vector dot product of the two vectors.
    Another way to write **w·x** would be *w[1]*x[1] + w[2]*x[2] + w[3]*x[3] + … +
    w[n]*x[n]*, or simply Σ*[j] w[j]*x[j]* .
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**w** 和 **x** 之间的点表示两个向量的向量点积。另一种写**w·x**的方式是 *w[1]*x[1] + w[2]*x[2] + w[3]*x[3]
    + … + w[n]*x[n]*，或者简单地表示为 Σ*[j] w[j]*x[j]*。'
- en: Taken together, it is the weights and the biases of the neurons in the network
    that actually do the learning and calculation. When you train a neural network,
    you are gradually updating the weights and biases with the goal of configuring
    them to solve your problem. Two neural networks with the same topology (for example,
    two fully-connected 10-15-5 networks) but different weights and biases are different
    networks that will solve different problems.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，网络中神经元的权重和偏置实际上负责学习和计算。当你训练一个神经网络时，你是在逐渐更新权重和偏置，目的是将它们配置来解决你的问题。具有相同拓扑结构（例如，两个全连接的10-15-5网络）但不同权重和偏置的两个神经网络是不同的网络，将解决不同的问题。
- en: How does the activation function factor into all of this? The original model
    for an artificial neuron was called the perceptron, and its activation function
    was a step function. Basically, if ***w·x** + b* for a neuron was greater than
    zero, the neuron would output 1\. If, on the other hand, ***w·x** + b* were less
    than zero, the neuron would output zero.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数在这个过程中的作用是什么？人工神经元的原始模型被称为感知器，其激活函数是阶跃函数。基本上，如果一个神经元的***w·x** + b*大于零，那么神经元将输出1。另一方面，如果***w·x**
    + b*小于零，那么神经元将输出零。
- en: This early perceptron model was powerful because it was possible to represent
    logic gates with an artificial neuron. If you've ever taken a course on Boolean
    logic or circuits, you would have learned that you can use NAND gates to build
    any other type of logic gate, and it is trivially easy to build a NAND gate with
    a perceptron.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个早期的感知器模型之所以强大，是因为可以用人工神经元来表示逻辑门。如果你曾经上过布尔逻辑或电路的课程，你就会知道你可以使用NAND门来构建任何其他类型的逻辑门，并且用感知器构建NAND门是极其容易的。
- en: Imagine a perceptron that takes two inputs, each with a weight of -2\. The perceptron's
    bias is +3\. If both inputs are 0, then ***w·x** + b = +3* (just the weight, because
    all inputs are zero). Since the perceptron's activation function is a step function,
    the output of the neuron in this case will be 1 (+3 is greater than zero, so the
    step function returns +1).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个接受两个输入的感知器，每个输入的权重为-2。感知器的偏置为+3。如果两个输入都是0，那么***w·x** + b = +3*（仅仅是权重，因为所有输入都是零）。由于感知器的激活函数是阶跃函数，在这种情况下神经元的输出将是1（+3大于零，因此阶跃函数返回+1）。
- en: 'If the inputs are 1 and 0, in any order, then ***w·x** + b = +1*, and therefore
    the output of the perceptron will also be 1\. However, if both inputs are instead
    1, then ***w·x** + b = -1*. The two inputs, both weighted -2, will overcome the
    neuron''s bias of +3, and the activation function (which returns 1 or 0), will
    return 0\. This is the logic of the NAND gate: the perceptron will return 0 if
    both inputs are 1, otherwise it will return 1 for any other combination of inputs.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入是1和0，无论顺序如何，那么***w·x** + b = +1*，因此感知器的输出也将是1。然而，如果两个输入都是1，那么***w·x** +
    b = -1*。两个输入，权重都是-2，将克服神经元的偏置+3，激活函数（返回1或0）将返回0。这就是NAND门的逻辑：如果两个输入都是1，感知器将返回0，对于任何其他输入组合，它将返回1。
- en: These early results excited the computer science and electronics community in
    the 1970s, and ANNs received a lot of hype. However, we had difficulty automatically
    training neural networks. Perceptron's could be crafted by hand to represent logic
    gates, and some amount of automated training for neural networks was possible,
    but large-scale problems remained inaccessible.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些早期结果在20世纪70年代激发了计算机科学和电子学界的兴趣，人工神经网络（ANNs）受到了大量的炒作。然而，我们很难自动训练神经网络。感知器可以通过手工制作来表示逻辑门，并且对神经网络进行一定程度的自动训练是可能的，但对于大规模问题仍然难以接近。
- en: The problem was the step function used as the perceptron's activation function.
    When training an ANN, you want small changes to the weights or biases of the network
    to similarly result in only small changes to the network's output. But the step
    function gets in the way of the process; one small change to the weights might
    result in no change to the output, but the next small change to the weights could
    result in a huge change to the output! This happens because the step function
    is not a smooth function—it has an abrupt jump from 0 to 1 once the threshold
    is crossed, and it is exactly 0 or exactly 1 at all other points. This limitation
    of the perceptron, and thus the major limitation of ANNs, resulted in over a decade
    of research stagnation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于用作感知器激活函数的阶跃函数。在训练人工神经网络时，你希望网络权重或偏置的微小变化只会导致网络输出的微小变化。但阶跃函数阻碍了这个过程；权重的一个微小变化可能不会改变输出，但下一个微小变化可能导致输出发生巨大变化！这是因为阶跃函数不是一个平滑函数——一旦越过阈值，它就会从0突然跳到1，而在所有其他点上它恰好是0或恰好是1。这种感知器的限制，以及因此人工神经网络的重大限制，导致了十多年的研究停滞。
- en: Eventually, researchers in 1986 rediscovered a training technique that had been
    discovered a few years prior. They found that this technique, called **backpropagation**,
    made training much faster and more reliable. Thus artificial neural networks experienced
    their second wind.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，1986年，研究人员重新发现了几年前就已经发现的一种训练技术。他们发现，这种称为**反向传播**的技术使训练变得更快、更可靠。因此，人工神经网络经历了第二次发展。
- en: Backpropagation training
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播训练
- en: 'There was one key insight that brought neural network research out of stagnation
    and into the modern era: the choice of a better activation function for neurons.
    Step functions caused issues with the automated training of networks because tiny
    changes in the network parameters (the weights and biases) could alternately have
    either no effect or an abrupt major effect on the network. Obviously, this is
    not a desired property of a trainable system.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个关键的洞察力将神经网络研究从停滞中带入了现代时代：为神经元选择更好的激活函数。阶跃函数导致网络自动训练出现问题，因为网络参数（权重和偏置）的微小变化可能会交替产生没有效果或突然的重大效果。显然，这不是一个可训练系统的期望属性。
- en: The general approach to automatically training ANNs is to start with the output
    layer and work backwards. For each example in your training set, you run the network
    in feed-forward mode (that is, **prediction mode**) and compare the actual output
    to the desired output. A good metric to use for comparing desired versus actual
    results is **mean squared error** (**MSE**); test all training examples, and for
    each calculate and square the difference in output from the desired values. Sum
    all the squared errors up and average over the number of training examples, and
    you have a cost function or loss function. The cost function is a function of
    the weights and biases of a given network topology. The goal in training ANNs
    is to reduce the cost function to—ideally—zero. You could potentially use the
    ANN's accuracy over all training examples as a cost function, but mean-squared
    error has better mathematical properties for training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 自动训练人工神经网络的一般方法是从输出层开始，逆向工作。对于训练集中每个示例，你以前馈模式（即**预测模式**）运行网络，并将实际输出与期望输出进行比较。用于比较期望结果与实际结果的好指标是**均方误差**（**MSE**）；测试所有训练示例，并对每个示例计算输出与期望值之间的差异并平方。将所有平方误差相加，并除以训练示例的数量，你就得到了一个成本函数或损失函数。成本函数是给定网络拓扑的权重和偏置的函数。训练人工神经网络的目标是将成本函数降低到——理想情况下——零。你可以使用人工神经网络在所有训练示例上的准确率作为成本函数，但均方误差在训练中具有更好的数学特性。
- en: 'The backpropagation algorithm hinges on the following insight: if you know
    the weights and biases of all neurons, if you know the inputs and desired outputs,
    and if you know the activation function the neurons use, you can work backwards
    from an output neuron to discover which weights or biases are contributing to
    a large error. That is, if neuron Z has neuron inputs A, B, and C with weights
    of 100, 10, and 0, respectively, you would know that neuron C has no effect on
    neuron Z and therefore neuron C is not contributing to neuron Z''s error. On the
    other hand, neuron A has an outsized impact on neuron Z, so if neuron Z has a
    large error it is likely that neuron A is to blame. The backpropagation algorithm
    is named such because it propagates the error in output neurons backwards through
    the network.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法的关键在于以下洞察：如果你知道所有神经元的权重和偏置，如果你知道输入和期望的输出，以及如果你知道神经元使用的激活函数，你可以从输出神经元开始反向工作，以发现哪些权重或偏置对大的误差有贡献。也就是说，如果神经元Z有来自神经元A、B和C的输入，它们的权重分别为100、10和0，你就会知道神经元C对神经元Z没有影响，因此神经元C没有对神经元Z的误差做出贡献。另一方面，神经元A对神经元Z有巨大的影响，所以如果神经元Z有一个大的误差，那么很可能是神经元A的责任。反向传播算法之所以得名，是因为它通过网络将输出神经元的误差反向传播。
- en: Taking this concept a step further, if you also know the activation function
    and its relationship between the weights, biases, and the errors, you can determine
    how much a weight would need to change in order to get a corresponding change
    to a neuron's output. Of course, there are many weights in an ANN and it is a
    highly complex system, so the approach we use is to make tiny changes to the weights—we
    can only predict changes to the network’s output if we use a simplifying approximation
    for small changes in weights. This part of the approach is called gradient descent,
    which is named such because we are trying to descend the gradient (the slope)
    of the cost function by making small modifications to weights and biases.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个概念进一步扩展，如果你还知道激活函数及其与权重、偏置和误差之间的关系，你可以确定权重需要改变多少才能使神经元的输出产生相应的变化。当然，在人工神经网络中有很多权重，它是一个高度复杂的系统，所以我们使用的方法是对权重进行微小的调整——我们只能使用对权重微小变化的简化近似来预测网络输出的变化。这个方法的一部分被称为梯度下降，之所以得名，是因为我们试图通过调整权重和偏置来降低成本函数的梯度（斜率）。
- en: 'To picture this, imagine a nylon hammock hanging between two trees. The hammock
    represents the cost function, and the *x* and *y* axes (viewed from the sky) abstractly
    represent the biases and weights of the network (in reality, this is a multi-thousand-dimensional
    picture). There is some combination of weights and biases where the hammock hangs
    the lowest: that point is our goal. We are a tiny ant sitting somewhere on the
    surface of the hammock. We don''t know where the lowest point of the hammock is,
    and we''re so small that even wrinkles or creases in the fabric can throw us off.
    But we do know that the hammock is smooth and continuous, and we can feel around
    our immediate area. As long as we keep heading downhill for each individual step
    we take, we will eventually find the lowest point in the hammock—or, at least,
    a low point close to where we started (a local minimum), depending on how complex
    the shape of the hammock is.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了形象地理解这一点，想象一个挂在两棵树之间的尼龙吊床。吊床代表成本函数，而*x*和*y*轴（从天空看）抽象地代表网络的偏置和权重（实际上，这是一个多千维度的图像）。存在一些权重和偏置的组合，使得吊床挂得最低：那个点就是我们的目标。我们是一只坐在吊床表面某处的微小蚂蚁。我们不知道吊床最低点在哪里，而且我们太小了，即使布料上的褶皱或折痕也能让我们偏离方向。但我们知道吊床是光滑且连续的，我们可以在我们周围摸索。只要我们在每一步都朝下山方向前进，我们最终会在吊床中找到最低点——或者至少，一个接近我们起始点（局部最小值）的低点，这取决于吊床形状的复杂程度。
- en: This approach of gradient descent requires us to mathematically understand and
    be able to describe the gradient of the cost function, which means we must also
    understand the gradient of the activation function. The gradient of a function
    is essentially its slope, or derivative. The reason we can’t use the perceptron's
    original step function as an activation function is because the step function
    is not differentiable at all points; the giant, instantaneous leap between 0 and
    1 in the step function is a non-differentiable discontinuity.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种梯度下降的方法要求我们数学上理解和能够描述成本函数的梯度，这意味着我们也必须理解激活函数的梯度。函数的梯度本质上是其斜率或导数。我们不能使用感知器原始的步函数作为激活函数的原因是步函数在所有点上都是不可微分的；步函数在
    0 和 1 之间巨大的、瞬间的跳跃是一个不可微分的间断。
- en: 'Once we figured out that we should be using gradient descent and backpropagation
    to train our neural networks, the rest came easily. Instead of using a step function
    for neuron activation functions, we started using sigmoid functions. Sigmoid functions
    are generally shaped like step functions, except they are smoothed out, continuous,
    and differentiable at all points. Here''s an example of a sigmoid function versus
    a step function:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们弄清楚我们应该使用梯度下降和反向传播来训练我们的神经网络，其他事情就变得容易了。我们不再使用步函数作为神经元激活函数，而是开始使用 Sigmoid
    函数。Sigmoid 函数通常呈阶梯函数形状，但它们被平滑了，是连续的，并且在所有点上都是可微分的。以下是一个 Sigmoid 函数与步函数的例子：
- en: '![](img/9cd7a8e7-fa84-425a-966c-3efa806bc7ff.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9cd7a8e7-fa84-425a-966c-3efa806bc7ff.png)'
- en: There are many types of sigmoid functions; the preceding one is described by
    the equation *y = 1 / 1+e^(-x)* and is called the **logistic function** or **logistic
    curve**. Other popular sigmoid functions are hyperbolic tangent (that is, tanh),
    which has a range from -1 to +1 as opposed to the logistic function's range of
    0 to +1\. Another popular activation function is the **rectified linear unit**
    (**ReLU**), which is often used in image processing and output layers. There is
    also the *softplus* function, whose derivative is in fact the logistic function
    itself. The activation function you choose will depend on the specific mathematical
    properties you desire. It is also not uncommon to use different activation functions
    in different layers of the network; hidden layers will often use logistic or tanh
    activation functions, while the output layer might use *softmax* and the input
    layer might use ReLU. You can invent your own activation function for your neurons,
    however, you must be able to differentiate the function and determine its gradient
    in order to integrate it with the backpropagation algorithm.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多类型的 Sigmoid 函数；前一个函数由方程 *y = 1 / (1+e^(-x))* 描述，被称为**逻辑函数**或**逻辑曲线**。其他流行的
    Sigmoid 函数是双曲正切（即 tanh），其范围从 -1 到 +1，与逻辑函数的范围 0 到 +1 相比。另一个流行的激活函数是**修正线性单元**（**ReLU**），它常用于图像处理和输出层。还有
    *softplus* 函数，其导数实际上是逻辑函数本身。你选择的激活函数将取决于你想要的特定数学属性。在不同的网络层中使用不同的激活函数也很常见；隐藏层通常会使用逻辑或
    tanh 激活函数，而输出层可能会使用 *softmax*，输入层可能会使用 ReLU。然而，你可以为你的神经元发明自己的激活函数，但你必须能够微分该函数并确定其梯度，以便将其与反向传播算法集成。
- en: This singular, minor change to the neuron's activation function made a world
    of difference to our training of ANNs. Once we started using differentiable activation
    functions, we were able to calculate the gradient of the cost and activation functions,
    and use that information to determine precisely how to update weights in the backpropagation
    algorithm. Neural network training became faster and more powerful, and neural
    networks were propelled into the modern era, though they still had to wait for
    hardware and software libraries to catch up. More importantly, neural network
    training became a study in mathematics—particularly vector calculus—rather than
    being limited to study by computer scientists.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对神经元激活函数的这种微小改变对我们训练人工神经网络产生了巨大的影响。一旦我们开始使用可微分的激活函数，我们就能计算成本和激活函数的梯度，并利用这些信息来确定在反向传播算法中如何精确地更新权重。神经网络训练变得更快、更强大，神经网络被推进到现代时代，尽管它们仍然需要等待硬件和软件库的跟进。更重要的是，神经网络训练成为了一项数学研究——尤其是矢量微积分——而不是仅限于计算机科学家的研究。
- en: Example - XOR in TensorFlow.js
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 - TensorFlow.js 中的 XOR 操作
- en: In this example, we're going to solve the XOR problem using a `TensorFlow.js`
    feedforward neural network. First, let's explore the XOR problem, and why it's
    a good starting point for us.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用`TensorFlow.js`前馈神经网络来解决XOR问题。首先，让我们探索XOR问题，以及为什么它对我们来说是一个好的起点。
- en: 'The XOR, or *exclusive or* operation, is a Boolean operator that returns true
    if only one, but not both, of its inputs is truth. Compare this to the regular
    Boolean OR that you''re more commonly familiar with, which will return true if
    both inputs are true—the XOR will return false if both inputs are true. Here is
    a table comparing XOR to OR; I''ve highlighted the case where OR and XOR differ:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: XOR，或称为*排他或*操作，是一个布尔运算符，当且仅当其输入中只有一个为真时返回真。与您更熟悉的常规布尔OR运算符相比，后者在两个输入都为真时返回真——XOR在两个输入都为真时返回假。以下是一个比较XOR和OR的表格；我已经突出显示了OR和XOR不同的情况：
- en: '| **Input 1** | **Input 2** | **OR** | **XOR** |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **输入1** | **输入2** | **OR** | **XOR** |'
- en: '| False | False | False | False |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| False | False | False | False |'
- en: '| False | True | True | True |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| False | True | True | True |'
- en: '| True | False | True | True |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| True | False | True | True |'
- en: '| True | True | **True** | **False** |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| True | True | **True** | **False** |'
- en: 'Why is the XOR problem a good test for us? Let''s plot the XOR operations on
    a graph:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么XOR问题对我们来说是一个好的测试？让我们在图上绘制XOR操作：
- en: '![](img/899339f7-7f6e-42d9-b834-adee57e8fd30.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/899339f7-7f6e-42d9-b834-adee57e8fd30.png)'
- en: Viewing the preceding graph, we can see that the two classes involved in the
    **XOR** operation are not linearly separable. In other words, it is impossible
    to draw a straight line that separates the circles from the X in the preceding
    graph.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 观察前面的图表，我们可以看到在XOR操作中涉及的两个类别在图上不是线性可分的。换句话说，不可能画出一条直线来将前面的图中的圆圈与X分开。
- en: The combined facts that the XOR operation is very simple but also that the classes
    are not linearly separable make the XOR operation an excellent entry point when
    testing out a new classification algorithm. You don't need a fancy dataset to
    test out whether a new library or algorithm will work for you.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: XOR操作非常简单，但类别不是线性可分的事实，使得XOR操作在测试新的分类算法时是一个极好的起点。您不需要一个复杂的数据集来测试新的库或算法是否适合您。
- en: Before jumping into the TensorFlow example, let's first discuss how we might
    build an XOR-solving neural network by hand. We will design our own weights and
    biases and see whether we can develop a manual neural network that solves XOR.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在跳入TensorFlow示例之前，让我们首先讨论我们如何手动构建一个解决XOR的神经网络。我们将设计自己的权重和偏差，看看我们是否能开发出一个手动神经网络来解决XOR。
- en: First, we know that the network requires two inputs and one output. We know
    that the inputs and output are binary, so we must pick activations functions that
    have the range [0, 1]; ReLU or sigmoid would be appropriate, while tanh, whose
    range is [-1, 1] would be less appropriate.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们知道网络需要两个输入和一个输出。我们知道输入和输出是二进制的，因此我们必须选择范围在[0, 1]的激活函数；ReLU或sigmoid是合适的，而tanh，其范围是[-1,
    1]，则不太合适。
- en: 'Finally, we know that XOR is not linearly separable and thus cannot be trivially
    solved; we need a hidden layer in our network. Let''s therefore attempt to build
    a 2-2-1 neural network:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们知道XOR不是线性可分的，因此不能轻易解决；我们需要在网络上添加一个隐藏层。因此，让我们尝试构建一个2-2-1神经网络：
- en: '![](img/46fef899-238c-4a22-9621-3586a55b7732.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46fef899-238c-4a22-9621-3586a55b7732.png)'
- en: Next, we need to think about the weights and biases of the neurons in the network.
    We know that the network needs to be designed such that there is a penalty for
    both inputs being true. Therefore, one hidden-layer neuron should represent a
    weakly positive signal (that is, it activates when the inputs are activated) and
    the other hidden-layer neuron should represent a strongly negative signal (that
    is, if both inputs are true, this neuron should overwhelm the weakly-positive
    neuron).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要考虑网络中神经元的权重和偏差。我们知道网络需要设计成对两个输入都为真时有一个惩罚。因此，一个隐藏层神经元应该表示一个弱正信号（即，当输入被激活时它会被激活），而另一个隐藏层神经元应该表示一个强负信号（即，如果两个输入都为真，这个神经元应该压倒弱正神经元）。
- en: 'Here''s an example of one set of weights that would work to achieve XOR:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可以用来实现XOR的一组权重的示例：
- en: '![](img/06a64a8c-ae56-4816-94a8-2307194ee0dc.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06a64a8c-ae56-4816-94a8-2307194ee0dc.png)'
- en: Let's run a few example calculations. I'll start with the differentiating case
    of both inputs being true. The hidden **h1** neuron will have a total weighted
    input of 4, because the weight for each input is 2 and both inputs are true. The
    h1 neuron also has a bias of -1, however, the bias is not enough to deactivate
    the neuron. The total sum of the biased inputs is therefore 3 for the h1 neuron;
    since we haven't decided on a specific activation function, we will not try to
    guess what the actual activation will become—suffice to say that an input of +3
    is enough to activate the neuron.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: We now turn our attention to the hidden **h2** neuron. It also receives input
    from both input neurons, however, these weights are negative, and so the unbiased
    input sum it receives is -4\. The bias on h2 is +3, so the total biased input
    for h2 is -1\. If we were to choose the ReLU activation function, the neuron's
    output would be zero. In any case, h2 is not activated.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we look at the output node. It receives a weighted input of +2 from
    h1, but receives no input from h2\. Since the output node’s bias is -3 (essentially
    requiring both h1 and h2 to be activated), the output node will return 0 or false.
    This is the expected result for XOR with both inputs being set to true or 1.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s similarly tabulate the results for the other XOR cases. The columns
    for `h1`, `h2`, and `Out` represent the weighted and biased input to the neuron,
    before being applied to an activation function (since we haven''t chosen one).
    Just remember that each neuron will transmit values in [0, 1] to the next neuron;
    it will not send values such as -1 or 3 after the activation function is applied:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '| **In 1** | **In 2** | **h1** | **h2** | **Out** |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | -1 | 3 | -1 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 | 1 | 1 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 | 1 | 1 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 3 | -1 | -1 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: The preceding table proves that the handcrafted ANN works for all XOR test cases.
    It also gives us a little insight into the inner workings of a network. The hidden
    h1 and h2 neurons have specific roles. The h1 neuron is off by default but easily
    satisfied and will become activated if any of the inputs are active; h1 is essentially
    a typical OR operation. On the other hand, h2 is on by default and can only be
    deactivated if both inputs are on; h2 is essentially a NAND operation. The output
    neuron requires both h1 and h2 to be active, therefore the output neuron is an
    AND operator.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now use the `TensorFlow.js` library and see whether we can achieve the
    same success. On your computer, create a new folder called `Ch8-ANN`. Add the
    following `package.json` file and then issue `yarn install`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now add the `src/index.js` file and import TensorFlow:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: TensorFlow is not simply an ANN library. The TensorFlow library provides a number
    of building blocks that are useful in both ANNs and general ML and linear algebra
    (that is, vector/matrix math) problems. Because TensorFlow is more of a toolbox
    than a singular tool, there will always be many ways to solve any given problem.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating a sequential model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: TensorFlow *models* are high-level containers that essentially run functions;
    they are mapping from input to output. You can use TensorFlow's low-level operators
    (the linear algebra tools that come with the library) to build your model, or
    you can use one of the higher-level model classes. In this case, we are building
    a *sequential model*, which is a special case of TensorFlow's generic model. You
    may think of a sequential model as a neural network that only feeds forward, and
    does not involve any recurrences or feedback loops internally. A sequential model
    is essentially a vanilla neural network.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s add layers to the model:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We are adding three layers to our model. All layers are **dense** layers, meaning
    they are fully connected to the next layer. This is what you'd expect from a vanilla
    neural network. We've specified the *units* for each layer—units is TensorFlow's
    name for neurons, since TensorFlow can be used outside of ANN contexts. I've set
    this example up with four neurons per layer rather than two, because I found that
    the extra neurons greatly improve the speed and resilience of the training process.
    We've specified `inputDim`in the first layer, telling the layer that it should
    expect two inputs per data point. The first and second layers use the ReLU activation
    function. The third layer, which is the output layer, has only one unit/neuron
    and uses the familiar sigmoid activation function, since I would like the results
    to snap more readily toward 0 or 1.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we must compile the model before we can use it. We will specify a loss
    function, which can be either a prebuilt loss function that ships with the library
    or a custom loss function that we provide. We will also specify our optimizer;
    we discussed gradient descent earlier in this chapter, but there are many other
    optimizers available, such as Adam, Adagrad, and Adadelta. In this case, we will
    use the stochastic gradient descent optimizer (typical for vanilla neural networks),
    however, we will choose the `binaryCrossentropy` loss function, which is more
    appropriate than mean squared error for our binary classification task:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We've also set the learning rate for the gradient descent optimizer; the learning
    rate dictates how much the backpropagation training algorithm will modify the
    weights and biases in each training generation or epoch. A lower learning rate
    will result in a longer time to train the network, but will be more stable. A
    higher learning rate will train the network more quickly, but less reliably; your
    network may not converge at all if the learning rate is too high.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we''ve added `metrics: [''accuracy'']` to the compilation step. This
    allows us to get a report on the network''s accuracy when we eventually call `model.evaluate`.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we''ll set up our training data, which is simply four data points. TensorFlow
    operates on *tensors*, which are essentially mathematical matrices. TensorFlow
    tensors are immutable, and all operations performed on tensors will return new
    tensors rather than modifying the existing tensor. If you need to modify tensors
    in place, you must use TensorFlow''s *variables,* which are mutable wrappers around
    tensors. TensorFlow requires all math be performed through tensors so that the
    library can optimize calculations for GPU processing:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Because tensors are matrices, every tensor has a *shape*. Shape, for 2D tensors,
    is defined as *[rows, cols]*. For 3D tensors, shape is *[rows, cols, depth]*;
    image processing typically uses 3D tensors, where rows and cols represent pixel
    Y and X coordinates, and depth represents a color channel (for example, RGBA)
    for that pixel. Since we have four training examples, and each training example
    requires two fields for inputs, our input tensor has a shape of four rows by two
    columns. Similarly, our target values tensor has a shape of four rows by one column.
    TensorFlow will throw errors if you attempt to run calculations or train models
    with the wrong input and output shapes.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Our last step is to train the model with the data and then evaluate the model.
    TensorFlow''s `model.fit` method is what trains the model, and once trained we
    can use `model.evaluate` to get statistics, such as accuracy and loss, and we
    can also use `model.predict` to run the model in feedforward or prediction mode:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once you''ve added the code, run `yarn start` from the command line. Running
    this model takes about 60 seconds for me. When the model finishes, you should
    see something similar to the following as output. Note that ANNs and the stochastic
    gradient descent optimizer use random values for initialization and processing,
    and therefore some runs of the model may be unsuccessful, depending on the specific
    random initial conditions. The following is the output that will be obtained:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding output shows that the model has learned to emulate XOR. The loss
    value is very low, while the accuracy is 1.0, which is what's required for such
    a simple problem. In real-world problems, accuracies of 80-90% are more realistic.
    Additionally, the program's output shows the individual predictions for each of
    the four test cases. You can see the effect of the sigmoid activation function
    in that the values get very close to 0 and 1, but don't quite get there. Internally,
    TensorFlow is rounding these values in order to determine whether the classification
    was correct.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should play with the network parameters a bit. What happens
    if you reduce the number of training epochs? What happens if you switch the ReLU
    layers to sigmoid layers? What happens if you reduce the number of units/neurons
    in the first two layers to two? Does it work if you increase the number of training
    epochs? What is the effect of the learning rate on the training process? These
    are things that are best discovered through trial and error rather than lecture.
    This is an infinitely flexible neural network model, capable of handling much
    more complex problems than the simple XOR example, so you should become intimately
    familiar with all of these properties and parameters through experimentation and
    research.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: While this example was only a simple XOR sample, this approach can also be used
    for many other types of ANN problems. We've created a three-layered binary classifier
    that automatically trains and evaluates itself—it's the ultimate vanilla neural
    network. I will leave it to you to take these concepts and apply them to your
    real-world problems, though in the next chapter we'll try out some advanced neural
    models, such as convolutional and recurrent networks.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced and described the concept of artificial neural networks.
    We first discussed ANNs from a conceptual standpoint. You learned that neural
    networks are made of individual neurons, which are simple weighted adding machines
    that can apply an activation function to their output. You learned that neural
    networks can have many topologies, and it is the topology and the weights and
    biases between neurons in the network that do the actual work. You also learned
    about the backpropagation algorithm, which is the method by which neural networks
    are automatically trained.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at the classic XOR problem and looked at it through the lens
    of neural networks. We discussed the challenges and the approach to solving XOR
    with ANNs, and we even built—by hand!—a fully-trained ANN that solves the XOR
    problem. We then introduced the `TensorFlow.js` library and built a vanilla neural
    network with it, and successfully used that NN to train and solve the XOR problem.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to take a deeper look at the advanced ANN topologies.
    In particular, we'll discuss the **Convolutional Neural Network** (**CNN**), which
    is widely used in image processing, and we'll also look at **recurrent neural
    networks** (**RNN**), which are commonly used in artificial intelligence and natural
    language tasks.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
