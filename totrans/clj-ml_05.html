<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Selecting and Evaluating Data</h1></div></div></div><p>In the previous chapter, we studied <strong>Artificial Neural Networks</strong> (<strong>ANNs</strong>) and how they can be used to effectively model nonlinear sample data. So far, we've discussed several machine learning techniques that can be used to model a given training set of data. In this chapter, we will explore the following topics that focus on how to select appropriate features from the sample data:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We will study methods to evaluate or quantify how accurately a formulated model fits the supplied training data. These techniques will be useful when we have to extend or debug an existing model.</li><li class="listitem" style="list-style-type: disc">We will also explore how we can use the <code class="literal">clj-ml</code> library to perform this process on a given machine learning model.</li><li class="listitem" style="list-style-type: disc">Towards the end of the chapter, we will implement a working spam classifier that incorporates a model evaluation technique.</li></ul></div><p>The term <strong>machine learning diagnostic</strong><a id="id433" class="indexterm"/> is often used to describe a test that can be run to gain insight about what is and isn't working in a machine learning model. This information generated by the diagnostic can then be used to improve the performance of the given model. Generally, when designing a machine learning model, it's advisable to formulate a diagnostic for the model in parallel. Implementing a diagnostic for a given model can take around the same time as formulating the model itself, but implementing a diagnostic is a good investment of time since it would help in quickly determining what needs to be changed in the model in order to improve it. Thus, machine learning diagnostics are helpful in saving time with respect to debugging or improving a formulated learning model.</p><p>Another interesting aspect of machine learning is that without knowing the nature of the data we are trying to fit, we can make no assumption about which machine learning model we can use to fit the sample data. This axiom is known as the <strong>No Free Lunch</strong> theorem,<a id="id434" class="indexterm"/> and can be summarized as follows:</p><div><blockquote class="blockquote"><p>"Without prior assumptions about the nature of a learning algorithm, no learning algorithm is superior or inferior to any other (or even random guessing)."</p></blockquote></div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec35"/>Understanding underfitting and overfitting</h1></div></div></div><p>In the previous chapters, we've talked about minimizing the error or cost function of a formulated machine learning model. It's apt for the overall error of the estimated model to be low, but a low error is generally not enough to determine how well the model fits the supplied training data. In this section, we will revisit the concepts of <em>overfitting</em> and <em>underfitting</em>.</p><p>An estimated model is said to be <strong>underfit</strong><a id="id435" class="indexterm"/> if it exhibits a large error in prediction. Ideally, we should strive to minimize this error in the<a id="id436" class="indexterm"/> model. However, <a id="id437" class="indexterm"/>a formulated model with a low error or cost function could also indicate that the model doesn't understand<a id="id438" class="indexterm"/> the underlying relationship between the given features of the model. Rather, the model is <em>memorizing</em> the supplied data, and this could even result in modeling random noise. In this case, the model is said to be <strong>overfit</strong><a id="id439" class="indexterm"/>. A general symptom of an overfit model is<a id="id440" class="indexterm"/> failure to correctly predict the output variable from unseen data. An underfit model is also said to exhibit <a id="id441" class="indexterm"/>
<strong>high bias</strong> and an overfit model is said to have <a id="id442" class="indexterm"/>
<strong>high variance</strong>.</p><p>Suppose we are modeling a single dependent and independent variable in our model. Ideally, the model should fit the training data while generalizing on data that hasn't yet been observed in the training data. </p><p>The variance of the dependent variables with the independent variable in an underfit model can be represented using the following plot:</p><div><img src="img/4351OS_05_01.jpg" alt="Understanding underfitting and overfitting"/></div><p>In the preceding diagram,<a id="id443" class="indexterm"/> the red crosses represent data points in our sample data. As shown in the diagram, an underfit model will exhibit a large overall error, and we must <a id="id444" class="indexterm"/>try to reduce this error by appropriately selecting the features for our model and using regularization.</p><p>On the other hand, a model could also be overfit, in which the overall error in the model has a low value, but the estimated model fails to correctly predict the dependent variable from previously unseen data. An overfit model can be depicted using the following plot:</p><div><img src="img/4351OS_05_03.jpg" alt="Understanding underfitting and overfitting"/></div><p>As shown in the preceding diagram, the estimated model plot closely but inappropriately fits the training data and thus has a low overall error. But, the model fails to respond correctly to new data.</p><p>The model that describes <a id="id445" class="indexterm"/>a good fit for the sample data will have a low overall error and <a id="id446" class="indexterm"/>can predict the dependent variable correctly from previously unseen values for the independent variables in our model. An appropriately fit model should have a plot similar to the following diagram:</p><div><img src="img/4351OS_05_05.jpg" alt="Understanding underfitting and overfitting"/></div><p>ANNs can also be underfit or overfit on the provided sample data. For example, an ANN with a few hidden nodes and layers could be an underfit model, while an ANN with a large number of hidden nodes<a id="id447" class="indexterm"/> and<a id="id448" class="indexterm"/> layers could exhibit overfitting.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec09"/>Evaluating a model</h2></div></div></div><p>We can plot the variance of the dependent and independent variables of a model to determine if the model is<a id="id449" class="indexterm"/> underfit or overfit. However, with a larger number of features, we need a better way to visualize how well the model generalizes the relationship of the dependent and independent variables of the model over the training data.</p><p>We can evaluate a trained machine learning model by determining the cost function of the model on some different data. Thus, we need to split the available sample data into two subsets—one for training the model and another for testing it. The latter subset is also called the <strong>test set</strong><a id="id450" class="indexterm"/> of our model.</p><p>The cost function is then calculated for the <img src="img/4351OS_05_06.jpg" alt="Evaluating a model"/> samples in the test set. This gives us a measure of the overall error in the model when used on previously unseen data. This value is represented by the term <img src="img/4351OS_05_07.jpg" alt="Evaluating a model"/> of the estimated model <img src="img/4351OS_05_08.jpg" alt="Evaluating a model"/> and is also called the <strong>test error</strong><a id="id451" class="indexterm"/> of the<a id="id452" class="indexterm"/> formulated model. The overall error in the training data is called the <strong>training error</strong><a id="id453" class="indexterm"/> of the model and is represented by the term <img src="img/4351OS_05_09.jpg" alt="Evaluating a model"/>. A linear regression model's test error can be calculated as follows:</p><div><img src="img/4351OS_05_10.jpg" alt="Evaluating a model"/></div><p>Similarly, the test error in a binary classification model can be formally expressed as follows:</p><div><img src="img/4351OS_05_11.jpg" alt="Evaluating a model"/></div><div><img src="img/4351OS_05_12.jpg" alt="Evaluating a model"/></div><div><img src="img/4351OS_05_13.jpg" alt="Evaluating a model"/></div><p>The problem of determining the features of a model such that the test error is low is termed as <strong>model selection</strong> or <strong>feature selection</strong>. Also, to avoid overfitting, we must measure how well the model generalizes over the training data. The test error on its own is an optimistic estimate of the generalization error in the model over the training data. However, we must also measure the generalization error in data that hasn't yet been seen by the model. If the model has a low error over unseen data as well, we can be certain that the model does not overfit the data. This process is termed as <strong>cross-validation</strong>.</p><p>Thus, to ensure that the model can perform well on unseen data, we will require an additional set of data, called the <strong>cross-validation set</strong><a id="id454" class="indexterm"/>. The number of samples in the cross-validation set is represented by the term <img src="img/4351OS_05_14.jpg" alt="Evaluating a model"/>. Typically, the sample data is partitioned into the training, test, and cross-validation sets such that the number of samples in the training data are significantly greater than those in the test and cross-validate sets. The error in generalization, or rather the cross-validation error <img src="img/4351OS_05_15.jpg" alt="Evaluating a model"/>, thus indicates how well the estimated model fits unseen data. Note that we don't modify the estimated model when we use the cross-validation and test sets on it. We will study more about cross-validation in the following sections of this chapter. As we will see later, we can also use cross-validation to determine the features of a model from some sample data.</p><p>For example, suppose we<a id="id455" class="indexterm"/> have 100 samples in our training data. We partition this sample data into three sets. The first 60 samples will be used to estimate a model that fits the data appropriately. Out of the 40 remaining samples, 20 will be used to cross-validate the estimated model, and the other 20 will be used to finally test the cross-validated model.</p><p>In the context of classification, a good representation of the accuracy of a given classifier is a <em>confusion matrix</em>.<a id="id456" class="indexterm"/> This representation is often used to visualize the performance of a given classifier based on a supervised machine learning algorithm. Each column in this matrix represents the number of samples that belong to a particular class as predicted by the given classifier. The rows of the confusion matrix represent the actual classes of the samples. The confusion matrix is also called the <strong>contingency matrix</strong> or the <strong>error matrix</strong> of the trained classifier.</p><p>For example, say we have two classes in a given classification model. The confusion matrix of this model might look like the following:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th rowspan="2" colspan="2" style="text-align: center" valign="bottom"> </th><th colspan="2" style="text-align: center" valign="bottom">
<p>Predicted class</p>
</th></tr><tr><th style="text-align: left">
<p>A</p>
</th><th style="text-align: left">
<p>B</p>
</th></tr></thead><tbody><tr><td rowspan="2" style="text-align: left" valign="top">
<p>
<strong>Actual class</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>A</p>
</td><td style="text-align: left" valign="top">
<p>45</p>
</td><td style="text-align: left" valign="top">
<p>15</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>B</p>
</td><td style="text-align: left" valign="top">
<p>30</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td></tr></tbody></table></div><p>In a confusion matrix, the predicted classes in our model are represented by vertical columns and the actual classes are represented by horizontal rows. In the preceding example of a confusion matrix, there are a total of 100 samples. Out of these, 45 samples from class A and 10 samples from class B were predicted to have the correct class. However, 15 samples of class A have been classified as class B and similarly 30 samples of class B have been predicted to have class A.</p><p>Let's consider the following confusion matrix of a different classifier that uses the same data as the previous example:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th rowspan="2" colspan="2" style="text-align: center" valign="bottom"> </th><th colspan="2" style="text-align: center" valign="bottom">
<p>Predicted class</p>
</th></tr><tr><th style="text-align: left">
<p>A</p>
</th><th style="text-align: left">
<p>B</p>
</th></tr></thead><tbody><tr><td rowspan="2" style="text-align: left" valign="top">
<p>
<strong>Actual class</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>A</p>
</td><td style="text-align: left" valign="top">
<p>45</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>B</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>50</p>
</td></tr></tbody></table></div><p>In the preceding confusion matrix, the classifier classifies all samples of class B correctly. Also, only 5 samples of class A are classified incorrectly. Thus, this classifier better understands the<a id="id457" class="indexterm"/> distinction between the two classes of data when compared to the classifier used in the previous example. In practice, we must strive to train a classifier such that it has values close to <em>0</em> for all the elements other than the diagonal elements in its confusion matrix.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec10"/>Understanding feature selection</h2></div></div></div><p>As we mentioned earlier, we need to determine an appropriate set of features from the sample data on which we must<a id="id458" class="indexterm"/> base our model. We can use cross-validation to determine which set of features to use from<a id="id459" class="indexterm"/> the training data, which can be explained as follows.</p><p>For each set or combination of feature variables, we determine the training and cross-validation error of a model based on the selected set of features. For example, we might want to add polynomial features derived from the independent variables of our model. We evaluate the training and cross-validation errors for each set of features depending on the highest degree of polynomial used to model the training data. We can plot the variance of these error functions over the degree of polynomial used, similar to the following diagram:</p><div><img src="img/4351OS_05_16.jpg" alt="Understanding feature selection"/></div><p>From the preceding diagram, we can determine which set of features produce an underfit or overfit estimated model. If a selected model has a high value for both the training and cross-validation errors, which<a id="id460" class="indexterm"/> is found towards the left of the plot, then the model is underfitting the supplied training data. On the other hand, a low training error and a high<a id="id461" class="indexterm"/> cross-validation error, as shown towards the right of the plot, indicates that the model is overfit. Ideally, we must select the set of features with the lowest possible values of the training and cross-validation errors.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec36"/>Varying the regularization parameter</h1></div></div></div><p>To produce a better fit of the training data, we can use regularization to avoid the problem of overfitting our data. The <a id="id462" class="indexterm"/>value  <img src="img/4351OS_05_17.jpg" alt="Varying the regularization parameter"/> of a given model must be appropriately selected depending on the behavior of the model. Note that a high regularization parameter could result in a high training error, which is an undesirable effect. We can vary the regularization parameter in a formulated machine learning model to produce the following plot of the error values over the value of the regularization parameter in our model:</p><div><img src="img/4351OS_05_18.jpg" alt="Varying the regularization parameter"/></div><p>Thus, as shown in the preceding plot, we can also minimize the training and cross-validation error in the model by <a id="id463" class="indexterm"/>changing the regularization parameter. If a model exhibits a high value for both these error values, we must consider reducing the value of the regularization parameter until both the error values are significantly low for the supplied sample data.</p></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec37"/>Understanding learning curves</h1></div></div></div><p>Another useful way to visualize the performance of a machine learning model is to use learning curves<a id="id464" class="indexterm"/>. A <strong>learning curve</strong> is essentially a plot of the<a id="id465" class="indexterm"/> error values in a model over the number of samples by which it is trained and cross-validated. For example, a model could have the following learning curve for the training and cross-validation errors:</p><div><img src="img/4351OS_05_19.jpg" alt="Understanding learning curves"/></div><p>Learning curves can be<a id="id466" class="indexterm"/> used to diagnose an underfit and overfit model. For example, the training error could be observed to increase quickly and converge towards a value close to the cross-validation with<a id="id467" class="indexterm"/> the number of samples provided to the model. Also, both the error values in our model have a significantly high value. A model that exhibits this kind of variance of error with the number of samples is underfit and has a learning curve similar to the following plot:</p><div><img src="img/4351OS_05_20.jpg" alt="Understanding learning curves"/></div><p>On the other hand,<a id="id468" class="indexterm"/> a model's training error could be observed to increase slowly with the number of samples provided<a id="id469" class="indexterm"/> to the model, and there might also be a large difference between the training and cross-validation errors in the model. This model is said to be overfit and has a learning curve similar to the following plot:</p><div><img src="img/4351OS_05_21.jpg" alt="Understanding learning curves"/></div><p>Thus, learning curve is a<a id="id470" class="indexterm"/> good supplementary tool<a id="id471" class="indexterm"/> to cross-validation for determining what is not working and what needs to be changed in a given machine learning model.</p></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec38"/>Improving a model</h1></div></div></div><p>Once we have determined whether a model is underfit or overfit over the given sample data, we must decide on how to<a id="id472" class="indexterm"/> improve the model's understanding of the relationship between the independent and dependent variables in our model. Let's briefly discuss a few of these techniques, as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Add or remove some features. As we will explore later, this technique can be used to improve both an underfit and an overfit model.</li><li class="listitem" style="list-style-type: disc">
Vary the value of the regularization parameter <img src="img/4351OS_05_17.jpg" alt="Improving a model"/>. Like adding or removing features, this method can be applied to both underfit and overfit models.
</li><li class="listitem" style="list-style-type: disc">Gather more training data. This method is a fairly obvious solution for improving an overfit model as it's needed to formulate a more generalized model to fit the training data.</li><li class="listitem" style="list-style-type: disc">
Add features which are polynomial terms of other features in the model. This method can be used to improve an underfit model. For example, if we are modeling two<a id="id473" class="indexterm"/> independent feature variables, <img src="img/4351OS_05_22.jpg" alt="Improving a model"/> and <img src="img/4351OS_05_23.jpg" alt="Improving a model"/>, we could add the terms <img src="img/4351OS_05_24.jpg" alt="Improving a model"/> as additional features to improve the model. The polynomial terms could be of even higher degrees, such as <img src="img/4351OS_05_25.jpg" alt="Improving a model"/> and <img src="img/4351OS_05_26.jpg" alt="Improving a model"/> , although this could result in overfitting the training data.
</li></ul></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec39"/>Using cross-validation</h1></div></div></div><p>As we briefly mentioned earlier, cross-validation<a id="id474" class="indexterm"/> is a common validation technique that can be used to evaluate machine learning models. Cross-validation essentially measures how well the estimated model will generalize some given data. This data is <a id="id475" class="indexterm"/>different from the training data supplied to our model, and is called the <strong>cross-validation set</strong>,<a id="id476" class="indexterm"/> or simply <strong>validation set</strong>, of our model. Cross-validation of a given model is also called <strong>rotation estimation</strong>.</p><p>If an estimated model performs well during cross-validation, we can assume that the model can understand the relationship between its various independent and dependent variables. The goal of cross-validation is to provide a test to determine if a formulated model is overfit on the training data. In the perspective of implementation, cross-validation is a kind of unit test for a machine learning system.</p><p>A single round of cross-validation generally involves partitioning all the available sample data into two subsets and then performing training on one subset and validation and/or testing on the other subset. Several such rounds, or <em>folds</em>, of cross-validation must be performed using different sets of data to reduce the variance of the overall cross-validation error of the given model. Any particular measure of the cross-validation error should be calculated as the average of this error over the different folds in cross-validation.</p><p>There are several types<a id="id477" class="indexterm"/> of cross-validation we can implement as a diagnostic for a given machine learning model or system. Let's briefly explore a few of them as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">
A common type is <em>k-fold</em> cross-validation<a id="id478" class="indexterm"/>, in which we partition the cross-validation data into <em>k</em> equal subsets. The training of the model is then performed on <img src="img/4351OS_05_27.jpg" alt="Using cross-validation"/> subsets of the data and the cross-validation is performed on a single subset.
</li><li class="listitem" style="list-style-type: disc">A simple variation of <em>k-fold</em> cross-validation is <em>2-fold</em> cross-validation<a id="id479" class="indexterm"/>, which is also called the <em>holdout method</em>. <a id="id480" class="indexterm"/>In <em>2-fold</em> cross-validation, the training and cross-validation subsets of data will be almost equal in proportion.</li><li class="listitem" style="list-style-type: disc"><strong>Repeated random subsampling</strong><a id="id481" class="indexterm"/> is another simple variant of cross-validation in which the sample data is first randomized or shuffled and then used as training and cross-validation data. This method is notably not dependent on the number of folds used for cross-validation.</li><li class="listitem" style="list-style-type: disc">Another form of <em>k-fold</em> cross-validation is <strong>leave-one-out</strong> cross-validation<a id="id482" class="indexterm"/>, in which only a single record from the available sample data is used<a id="id483" class="indexterm"/> for cross-validation. Leave-one-out cross-validation is essentially <em>k-fold</em> cross-validation in which <em>k</em> is equal to the number of samples or observations in the sample data.</li></ul></div><p>Cross-validation basically<a id="id484" class="indexterm"/> treats the estimated model as a black box, that is, it makes no assumptions about the implementation of the model. We can also use cross-validation to select features in a given model<a id="id485" class="indexterm"/> by using cross-validation to determine the feature set that produces the best fit model over the given sample data. Of course, there are a couple of limitations of classification, which can be summarized as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If a given model is needed to perform feature selection internally, we must perform cross-validation for each selected feature set in the given model. This can be computationally expensive depending on the amount of available sample data.</li><li class="listitem" style="list-style-type: disc">Cross-validation is not very useful if the sample data comprises exactly or nearly equal samples.</li></ul></div><p>In summary, it's a good practice to implement cross-validation for any machine learning system that we build. Also, we can choose an appropriate cross-validation technique depending on the problem we are trying to model as well as the nature of the collected sample data.</p><div><div><h3 class="title"><a id="note25"/>Note</h3><p>For the example that will follow, the namespace declaration should look similar to the following declaration:</p><div><pre class="programlisting">(ns my-namespace
  (:use [clj-ml classifiers data]))</pre></div></div></div><p>We can use the <code class="literal">clj-ml</code> library to cross-validate the classifier we built for the fish packaging plant in <a class="link" href="ch03.html" title="Chapter 3. Categorizing Data">Chapter 3</a>, <em>Categorizing Data</em>. Essentially, we built a classifier to determine whether a fish is a salmon or a sea bass using the <code class="literal">clj-ml</code> library. To recap, a fish is represented as a vector containing the category of the fish and values for the various features of the fish. The attributes of a fish are its length, width, and lightness of skin. We also described a template for a sample fish, which is defined as follows:</p><div><pre class="programlisting">(def fish-template
  [{:category [:salmon :sea-bass]}
   :length :width :lightness])</pre></div><p>The <code class="literal">fish-template</code> vector<a id="id486" class="indexterm"/> defined in the preceding code can be used to train a classifier with some sample data. For now, we will not bother about which classification algorithm we have used to model the given training data. We can only assume that the classifier was created using the <code class="literal">make-classifier</code> function<a id="id487" class="indexterm"/> from the <code class="literal">clj-ml</code> library. This classifier is stored in the <code class="literal">*classifier*</code> variable as follows:</p><div><pre class="programlisting">(def *classifier* (make-classifier ...))</pre></div><p>Suppose the classifier was trained with some sample data. We must now evaluate this trained classification model. To do this, we must first create some sample data to cross-validate. For the sake of simplicity, we will use randomly generated data in this example. We can generate this data using the <code class="literal">make-sample-fish</code> function<a id="id488" class="indexterm"/>, which we defined in <a class="link" href="ch03.html" title="Chapter 3. Categorizing Data">Chapter 3</a>, <em>Categorizing Data</em>. This function simply creates a new vector of some random values representing a fish. Of course, we must not forget the fact that the <code class="literal">make-sample-fish</code> function has an in-built partiality, so we create a meaningful pattern in a number of samples created using this function as follows:</p><div><pre class="programlisting">(def fish-cv-data
  (for [i (range 3000)] (make-sample-fish)))</pre></div><p>We will need to use a dataset<a id="id489" class="indexterm"/> from the <code class="literal">clj-ml</code> library, and we can create one using the <code class="literal">make-dataset</code> function, as shown in the following code:</p><div><pre class="programlisting">(def fish-cv-dataset
  (make-dataset "fish-cv" fish-template fish-cv-data))</pre></div><p>To cross-validate the classifier, we must use the <code class="literal">classifier-evaluate</code> function<a id="id490" class="indexterm"/> from the <code class="literal">clj-ml.classifiers</code> namespace. This function essentially performs <em>k-fold</em> cross-validation on the given data. Other than the classifier and the cross-validation dataset, this function requires the number of folds that we must perform on the data to be specified as the last parameter. Also, we will first need to set the class field of the records in <code class="literal">fish-cv-dataset</code> using the <code class="literal">dataset-set-class</code> function<a id="id491" class="indexterm"/>. We can define a single function to perform these operations as follows:</p><div><pre class="programlisting">(defn cv-classifier [folds]
  (dataset-set-class fish-cv-dataset 0)
  (classifier-evaluate *classifier* :cross-validation
                       fish-cv-dataset folds))</pre></div><p>We will use 10 folds of cross-validation on the classifier. Since the <code class="literal">classifier-evaluate</code> function returns a map, we bind this return value to a variable for further use, as follows:</p><div><pre class="programlisting">user&gt; (def cv (cv-classifier 10))
#'user/cv</pre></div><p>We can fetch and print the summary of the preceding cross-validation using the <code class="literal">:summary</code> keyword as follows:</p><div><pre class="programlisting">user&gt; (print (:summary cv))

Correctly Classified Instances        2986              99.5333 %
Incorrectly Classified Instances        14               0.4667 %
Kappa statistic                          0.9888
Mean absolute error                      0.0093
Root mean squared error                  0.0681
Relative absolute error                  2.2248 %
Root relative squared error             14.9238 %
Total Number of Instances             3000     
nil</pre></div><p>As shown in the preceding code, we can view several statistical measures of performance for our trained classifier. Apart from the correctly and incorrectly classified records, this summary also describes the <strong>Root Mean Squared Error</strong> (<strong>RMSE</strong>) and several other measures of error in our classifier. <a id="id492" class="indexterm"/>For a more detailed view of the correctly and incorrectly classified instances in the classifier, we can print the confusion matrix of the cross-validation using the <code class="literal">:confusion-matrix</code> keyword, as shown in the following code:</p><div><pre class="programlisting">user&gt; (print (:confusion-matrix cv))
=== Confusion Matrix ===

    a    b   &lt;-- classified as
 2129    0 |    a = salmon
    9  862 |    b = sea-bass
nil</pre></div><p>As shown in the preceding example, we can use the <code class="literal">clj-ml</code> library's <code class="literal">classifier-evaluate</code> function to perform a <em>k-fold</em> cross-validation on any given classifier. Although we are restricted to using classifiers from the <code class="literal">clj-ml</code> library when using the <code class="literal">classifier-evaluate</code> function, we must strive to implement similar diagnostics in any machine learning system we build.</p></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec40"/>Building a spam classifier</h1></div></div></div><p>Now that we are familiar<a id="id493" class="indexterm"/> with cross-validation, we will build a working machine learning system that incorporates cross-validation. The problem at hand will be that of <strong>spam classification</strong><a id="id494" class="indexterm"/>, in which we will have to determine the likelihood of a given e-mail being a spam e-mail. Essentially, the problem boils down to binary classification with a few tweaks to make the machine learning system more sensitive to spam (for more information, refer to <em>A Plan for Spam</em>). Note that we will not be implementing a classification engine that is integrated with an e-mail server, but rather we will be concentrating on the aspects of training the engine with some data and classifying a given e-mail.</p><p>The way this would be used in practice can be briefly explained as follows. A user will receive and read a new e-mail, and will decide whether to mark the e-mail as spam or not. Depending on the user's decision, we must train the e-mail service's spam engine using the new e-mail as data.</p><p>In order to train our<a id="id495" class="indexterm"/> spam classifier<a id="id496" class="indexterm"/> in a more automated manner, we'll have to simply gather data to feed into the classifier. We will need a large amount of data to effectively train a classifier with the English language. Luckily for us, sample data for spam classification can be found easily on the Web. For this implementation, we will use data from the <a id="id497" class="indexterm"/>
<strong>Apache SpamAssassin</strong> project.</p><div><div><h3 class="title"><a id="note26"/>Note</h3><p>The Apache SpamAssassin project is an open source implementation of a spam classification engine in Perl. For our implementation, we will use the sample data from this project. You can download this data from <a class="ulink" href="http://spamassassin.apache.org/publiccorpus/">http://spamassassin.apache.org/publiccorpus/</a>. For our example, we have used the <code class="literal">spam_2</code> and <code class="literal">easy_ham_2</code> datasets. A Clojure Leiningen project housing our spam classifier implementation will require that these datasets be extracted and placed in the <code class="literal">ham/</code> and <code class="literal">spam/</code> subdirectories of the <code class="literal">corpus/</code> folder. The <code class="literal">corpus/</code> folder should be placed in the root directory of the Leiningen project that is the same folder of the <code class="literal">project.clj</code> file.</p></div></div><p>The features<a id="id498" class="indexterm"/> of our spam classifier will be the number of occurrences of all previously encountered words in spam and ham e-mails. By the term <strong>ham</strong>, we mean "not spam". Thus, there are effectively two independent variables in our model. Also, each word has an associated probability of occurrence in e-mails, which can be calculated from the number of times it's found in spam and ham e-mails and the total number of e-mails processed by the classifier. A new e-mail would be classified by finding all known words in the e-mail's header and body and then somehow combining the probabilities of occurrences of these words in spam and ham e-mails.</p><p>For a given word feature in our classifier, we must calculate the total probability of occurrence of the word by taking into account the total number of e-mails analyzed by the classifier (for more information, refer to <em>Better Bayesian Filtering</em>). Also, an unseen term is neutral in the sense that it is neither spam nor ham. Thus, the initial probability of occurrence of any word in the untrained classifier is 0.5. Hence, we use a <strong>Bayesian probability</strong> function<a id="id499" class="indexterm"/> to model the occurrence of a particular word.</p><p>In order to classify a new e-mail, we also need to combine the probabilities of occurrences of all the known words found in it. For this implementation, we will use <strong>Fisher's method</strong>, or <strong>Fisher's combined probability test</strong>, to combine the calculated probabilities. Although the mathematical proof of this test is beyond the scope of this book, it's important to know that this method essentially estimates the probabilities of several independent probabilities in a given model as a <img src="img/4351OS_05_28.jpg" alt="Building a spam classifier"/> (pronounced as <strong>chi-squared</strong>) distribution (for more information, refer to <em>Statistical Methods for Research Workers</em>). Such a distribution has an associated<a id="id500" class="indexterm"/> number of degrees of freedom. It can be shown that an <img src="img/4351OS_05_28.jpg" alt="Building a spam classifier"/> distribution with degrees of freedom equal to twice the number of combined probabilities <em>k</em> can be formally expressed as follows:</p><div><img src="img/4351OS_05_29.jpg" alt="Building a spam classifier"/></div><p>This means that using an <img src="img/4351OS_05_28.jpg" alt="Building a spam classifier"/> distribution with <img src="img/4351OS_05_30.jpg" alt="Building a spam classifier"/> degrees of freedom, the <strong>Cumulative Distribution Function</strong> (<strong>CDF</strong>)<a id="id501" class="indexterm"/>, of the probabilities of the e-mail being a spam or a ham can be combined to reflect a total probability that is high when there are a large number of probabilities with values close to 1.0. Thus, an e-mail is classified as spam only when most of the words in the e-mail have been previously found in spam e-mails. Similarly, a large number of ham keywords would indicate the e-mail is in fact a ham e-mail. On the other hand, a low number of occurrences of spam keywords in an e-mail would have a probability closer to 0.5, in which case the classifier will be unsure of whether the e-mail is spam or ham.</p><div><div><h3 class="title"><a id="note27"/>Note</h3><p>For the example that will follow, we will require the <code class="literal">file</code> and <code class="literal">cdf-chisq</code> functions from the <code class="literal">clojure.java.io</code> and <code class="literal">Incanter</code> libraries, respectively. The namespace declaration of the example should look similar to the following declaration:</p><div><pre class="programlisting">(ns my-namespace
  (:use [clojure.java.io :only [file]]
        [incanter.stats :only [cdf-chisq]])</pre></div></div></div><p>A classifier trained using Fisher's method<a id="id502" class="indexterm"/>, as described earlier, will be very sensitive to new spam e-mails. We represent the dependent variable of our model by the probability of a given e-mail being spam. This probability is also termed as the <strong>spam score</strong><a id="id503" class="indexterm"/> of the e-mail. A low score indicates that an e-mail is ham, while a high score indicates that the e-mail is spam. Of course, we must also include a third class to represent an unknown value in our model. We can define some reasonable limits for the scores of these categories as follows:</p><div><pre class="programlisting">(def min-spam-score 0.7)
(def max-ham-score 0.4)

(defn classify-score [score]
  [(cond
    (&lt;= score max-ham-score) :ham
    (&gt;= score min-spam-score) :spam
    :else :unsure)
   score])</pre></div><p>As defined earlier, if an e-mail has a score of 0.7 or more, it's a spam e-mail. And a score of 0.5 or less indicates that the e-mail is ham. Also, if the score lies between these two values, we can't effectively decide whether the e-mail is spam or not. We represent these three categories using the keywords <code class="literal">:ham</code>, <code class="literal">:spam</code>, and <code class="literal">:unsure</code>.</p><p>The spam classifier must read <a id="id504" class="indexterm"/>several e-mails, determine all the words, or <em>tokens</em>, in the e-mails' text and header, and store this information as empirical knowledge to use later. We need to store the number of occurrences a particular word is found in spam and ham e-mails. Thus, every word that the classifier has encountered represents a feature. To represent this information for a single word, we will use a record with three fields as shown in the following code:</p><div><pre class="programlisting">(defrecord TokenFeature [token spam ham])

(defn new-token [token]
  (TokenFeature. token 0 0))

(defn inc-count [token-feature type]
  (update-in token-feature [type] inc))</pre></div><p>The record <code class="literal">TokenFeature</code> defined in the preceding code can be used to store the needed information for our spam classifier. The <code class="literal">new-token</code> function<a id="id505" class="indexterm"/> simply creates a new record for a given token by invoking the records, constructor. Obviously, a word is initially seen zero times in both spam and ham e-mails. We will also need to update these values, and we define the <code class="literal">inc-count</code> function to perform an update on the record using the <code class="literal">update-in</code> function. Note that the <code class="literal">update-in</code> function expects a function to apply to a particular field in the record as the last parameter. We are already dealing with a small amount of a mutable state in our implementation, so let's delegate access to this state through an agent. We would also like to keep track of the total number of ham and spam e-mails; so, we'll wrap these values with agents as well, as shown in the following code:</p><div><pre class="programlisting">(def feature-db
  (agent {} :error-handler #(println "Error: " %2)))

(def total-ham (agent 0))
(def total-spam (agent 0))</pre></div><p>The <code class="literal">feature-db</code> agent defined in the preceding code will be used to store all word features. We define a simple error handler for this agent using the <code class="literal">:error-handler</code> keyword parameter. The agent's <code class="literal">total-ham</code> and <code class="literal">total-spam</code> functions will keep track of the total number of ham and spam e-mails, respectively. We will now define a couple of functions to access these agents as follows:</p><div><pre class="programlisting">(defn clear-db []
  (send feature-db (constantly {}))
  (send total-ham  (constantly 0))
  (send total-spam (constantly 0)))

(defn update-feature!
  "Looks up a TokenFeature record in the database and
  creates it if it doesn't exist, or updates it."
  [token f &amp; args]
  (send feature-db update-in [token]
        #(apply f (if %1 %1 (new-token token))
                args)))</pre></div><p>In case you are not familiar with agents in Clojure, we can use the <code class="literal">send</code> function to alter the value contained in an agent. This function expects a single argument, that is, the function to apply to its encapsulated value. The agent applies this function on its contained value and updates it if<a id="id506" class="indexterm"/> there are no errors. The <code class="literal">clear-db</code> function<a id="id507" class="indexterm"/> simply initializes all the agents we've defined with an initial value. This is done by using the <code class="literal">constantly</code> function<a id="id508" class="indexterm"/> that wraps a value in a function that returns the same value. The <code class="literal">update-feature!</code> function<a id="id509" class="indexterm"/> modifies the value of a given token in the <code class="literal">feature-db</code> map and creates a new token if the supplied token is not present in the map of <code class="literal">feature-db</code>. Since we will only be incrementing the number of occurrences of a given token, we will pass the <code class="literal">inc-count</code> function<a id="id510" class="indexterm"/> as a parameter to the <code class="literal">update-feature!</code> function.</p><p>Now, let's define how the classifier will extract words from a given e-mail. We'll use regular expressions to do this. If we want to extract all the words from a given string, we can use the regular expression <code class="literal">[a-zA-Z]{3,}</code>. We can define this regular expression using a literal syntax in Clojure, as shown in the following code. Note that we could also use the <code class="literal">re-pattern</code> function<a id="id511" class="indexterm"/> to create a regular expression. We will also define all the MIME header fields from which we should also extract tokens. We will do all this with the help of the following code:</p><div><pre class="programlisting">(def token-regex #"[a-zA-Z]{3,}")

(def header-fields
  ["To:"
   "From:"
   "Subject:"
   "Return-Path:"])</pre></div><p>To match tokens with the regular expression defined by <code class="literal">token-regex</code>, we will use the <code class="literal">re-seq</code> function, which returns all matching tokens in a given string as a sequence of strings. For the MIME headers of an e-mail, we need to use a different regular expression to extract tokens. For example, we can extract tokens from the <code class="literal">"From"</code> MIME header as follows:</p><div><pre class="programlisting">user&gt; (re-seq #"From:(.*)\n"
              "From: someone@host.org\n")
(["From: someone@host.org\n" " someone@host.org"])</pre></div><div><div><h3 class="title"><a id="note28"/>Note</h3><p>Note the use of the newline character at the end of the regular expression, which is used to indicate the end of a MIME header in an e-mail.</p></div></div><p>We can then proceed to extract<a id="id512" class="indexterm"/> words from the values returned by matching the regular expression defined in the preceding code. Let's define the following few functions to extract tokens from a given e-mail's headers and body using this logic:</p><div><pre class="programlisting">(defn header-token-regex [f]
  (re-pattern (str f "(.*)\n")))

(defn extract-tokens-from-headers [text]
  (for [field header-fields]
    (map #(str field %1)  ; prepends field to each word from line
         (mapcat (fn [x] (-&gt;&gt; x second (re-seq token-regex)))
                 (re-seq (header-token-regex field)
                         text)))))

(defn extract-tokens [text]
  (apply concat
         (re-seq token-regex text)
         (extract-tokens-from-headers text)))</pre></div><p>The <code class="literal">header-token-regex</code> function<a id="id513" class="indexterm"/> defined in the preceding code returns a regular expression for a given header, such as <code class="literal">From:(.*)\n</code> for the <code class="literal">"From"</code> header. The <code class="literal">extract-tokens-from-headers</code> function uses this regular expression to determine all words in the various header fields of an e-mail and appends the header name to all the tokens found in the header text. The <code class="literal">extract-tokens</code> function applies the regular expression over the text and headers of an e-mail and then flattens the resulting lists into a single list using the <code class="literal">apply</code> and <code class="literal">concat</code> functions. Note that the <code class="literal">extract-tokens-from-headers</code> function returns empty lists for the headers defined in <code class="literal">header-fields</code>, which are not present in the supplied e-mail header. Let's try this function out in the REPL with the help of the following code:</p><div><pre class="programlisting">user&gt; (def sample-text
        "From: 12a1mailbot1@web.de
         Return-Path: &lt;12a1mailbot1@web.de&gt;
         MIME-Version: 1.0")

user&gt; (extract-tokens-from-headers sample-text)
(() ("From:mailbot" "From:web")
 () ("Return-Path:mailbot" "Return-Path:web"))</pre></div><p>Using the <code class="literal">extract-tokens-from-headers</code> function<a id="id514" class="indexterm"/> and the regular expression defined by <code class="literal">token-regex</code>, we can extract all words comprising of three or more characters from an e-mail's header and text. Now, let's<a id="id515" class="indexterm"/> define a function to apply the <code class="literal">extract-tokens</code> function<a id="id516" class="indexterm"/> on a given e-mail and update the feature map using the <code class="literal">update-feature!</code> function with all the words found in the e-mail. We will do all this with the help of the following code:</p><div><pre class="programlisting">(defn update-features!
  "Updates or creates a TokenFeature in database
  for each token in text."
  [text f &amp; args]
  (doseq [token (extract-tokens text)]
    (apply update-feature! token f args)))</pre></div><p>Using the <code class="literal">update-features!</code> function in the preceding code, we can train our spam classifier with a given e-mail. In order to keep track of the total number of spam and ham e-mails, we will have to send the <code class="literal">inc</code> function to the <code class="literal">total-spam</code> or <code class="literal">total-ham</code> agents depending on whether a given e-mail is spam or ham. We will do this with the help of the following code:</p><div><pre class="programlisting">(defn inc-total-count! [type]
  (send (case type
          :spam total-spam
          :ham total-ham)
        inc))

(defn train! [text type]
  (update-features! text inc-count type)
  (inc-total-count! type))</pre></div><p>The <code class="literal">inc-total-count!</code> function<a id="id517" class="indexterm"/> defined in the preceding code updates the total number of spam and ham e-mails in our feature database. The <code class="literal">train!</code> function simply calls the <code class="literal">update-features!</code> and <code class="literal">inc-total-count!</code> functions to train our spam classifier with a given e-mail and its type. Note that we pass the <code class="literal">inc-count</code> function to the <code class="literal">update-features!</code> function<a id="id518" class="indexterm"/>. Now, in order to classify a new e-mail as spam or ham, we must first define how to extract the known features from a given e-mail using our trained feature database. We will do this with the help of the following code:</p><div><pre class="programlisting">(defn extract-features
  "Extracts all known tokens from text"
  [text]
  (keep identity (map #(@feature-db %1) (extract-tokens text))))</pre></div><p>The <code class="literal">extract-features</code> function<a id="id519" class="indexterm"/> defined in the preceding code looks up all known features in a given e-mail by dereferencing the map stored in <code class="literal">feature-db</code> and applying it as a function to all the values returned by the <code class="literal">extract-tokens</code> function. As mapping the closure <code class="literal">#(@feature-db %1)</code> can return <code class="literal">()</code> or <code class="literal">nil</code> for all tokens that are not present in a <code class="literal">feature-db</code> agent, we will need to remove all empty values from the list of extracted features. To do this, <a id="id520" class="indexterm"/>we will use the <code class="literal">keep</code> function, which expects a function to apply to the non-nil values in a collection and the collection from which all nil values must be filtered out. Since we do not intend to transform the known features from the e-mail, we will pass the <code class="literal">identity</code> function, which returns its argument itself as the first parameter to the <code class="literal">keep</code> function.</p><p>Now that we have extracted all known features from a given e-mail, we must calculate all the probabilities of these features occurring in a spam e-mail. We must then combine these probabilities using Fisher's method we described earlier to determine the spam score of a new e-mail. Let's define the following functions to implement the Bayesian probability and Fisher's method:</p><div><pre class="programlisting">(defn spam-probability [feature]
  (let [s (/ (:spam feature) (max 1 @total-spam))
        h (/ (:ham feature) (max 1 @total-ham))]
      (/ s (+ s h))))

(defn bayesian-spam-probability
  "Calculates probability a feature is spam on a prior
  probability assumed-probability for each feature,
  and weight is the weight to be given to the prior
  assumed (i.e. the number of data points)."
  [feature &amp; {:keys [assumed-probability weight]
              :or   {assumed-probability 1/2 weight 1}}]
  (let [basic-prob (spam-probability feature)
        total-count (+ (:spam feature) (:ham feature))]
    (/ (+ (* weight assumed-probability)
          (* total-count basic-prob))
       (+ weight total-count))))</pre></div><p>The <code class="literal">spam-probability</code> function defined in the preceding code calculates the probability of occurrence of a given word feature in a spam e-mail using the number of occurrences of the word in spam and ham e-mails and the total number of spam and ham e-mails processed by the classifier. To avoid division-by-zero errors, we ensure that the value of the number of spam and ham e-mails is at least 1 before performing division. The <code class="literal">bayesian-spam-probability</code> function<a id="id521" class="indexterm"/> uses this probability returned by the <code class="literal">spam-probability</code> function<a id="id522" class="indexterm"/> to calculate a weighted average with the initial probability of 0.5 or <em>1/2</em>. </p><p>We will now implement Fisher's method of combining the probabilities returned by the <code class="literal">bayesian-spam-probability</code> function for all the known features found in an e-mail. We will do this with the help of the following code:</p><div><pre class="programlisting">(defn fisher
  "Combines several probabilities with Fisher's method."
  [probs]
  (- 1 (cdf-chisq
         (* -2 (reduce + (map #(Math/log %1) probs)))
         :df (* 2 (count probs)))))</pre></div><p>The <code class="literal">fisher</code> function<a id="id523" class="indexterm"/> defined in the preceding code uses the <code class="literal">cdf-chisq</code> function from the <code class="literal">Incanter</code> library to calculate the CDF of the several probabilities transformed by the expression <img src="img/4351OS_05_31.jpg" alt="Building a spam classifier"/>. We specify the number of degrees of freedom to this function using the <code class="literal">:df</code> optional parameter. We now need to apply the <code class="literal">fisher</code> function to the <a id="id524" class="indexterm"/>combined Bayesian probabilities of an e-mail being spam or ham, and combine these values into a final spam score. These two probabilities must be combined such that only a high number of occurrences of high probabilities indicate a strong probability of spam or ham. It has been shown that the simplest way to do this is to average the probability of a spam e-mail and the negative probability of a ham e-mail (or 1 minus the probability of a ham e-mail). We will do this with the help of the following code:</p><div><pre class="programlisting">(defn score [features]
  (let [spam-probs (map bayesian-spam-probability features)
        ham-probs (map #(- 1 %1) spam-probs)
        h (- 1 (fisher spam-probs))
        s (- 1 (fisher ham-probs))]
     (/ (+ (- 1 h) s) 2)))</pre></div><p>Hence, the <code class="literal">score</code> function will return the final spam score of a given e-mail. Let's define a function to extract the known word features from a given e-mail, combine the probabilities of occurrences of these features to produce the e-mail's spam score, and finally classify this spam score as a ham or spam e-mail, represented by the keywords <code class="literal">:ham</code> and <code class="literal">:spam</code> respectively, as shown in the following code:</p><div><pre class="programlisting">(defn classify
  "Returns a vector of the form [classification score]"
  [text]
   (-&gt; text
       extract-features
       score
       classify-score))</pre></div><p>So far, we have implemented how we train our spam classifier and use it to classify a new e-mail. Now, let's define some functions to load the sample data from the project's <code class="literal">corpus/</code> folder and use this data to train and cross-validate our classifier, as follows:</p><div><pre class="programlisting">(defn populate-emails
  "Returns a sequence of vectors of the form [filename type]"
  []
  (letfn [(get-email-files [type]
            (map (fn [f] [(.toString f) (keyword type)])
                 (rest (file-seq (file (str "corpus/" type))))))]
    (mapcat get-email-files ["ham" "spam"])))</pre></div><p>The <code class="literal">populate-emails</code> function<a id="id525" class="indexterm"/> defined in the preceding code returns a sequence of vectors to represent all the ham e-mails from the <code class="literal">ham/</code> folder and the spam e-mails from the <code class="literal">spam/</code> folder in our sample data.<a id="id526" class="indexterm"/> Each vector in this returned sequence has two elements. The first element in this vector is a given e-mail's relative file path and the second element is either <code class="literal">:spam</code> or <code class="literal">:ham</code> depending on whether the e-mail is spam or ham. Note the use of the <code class="literal">file-seq</code> function to read the files in a directory as a sequence.</p><p>We will now use the <code class="literal">train!</code> function to feed the content of all e-mails into our spam classifier. To do this, we can use the <code class="literal">slurp</code> function to read the content of a file as a string. For cross-validation, we will classify each e-mail in the supplied cross-validation data using the <code class="literal">classify</code> function and return a list of maps representing the test result of the cross-validation. We will do this with the help of the following code:</p><div><pre class="programlisting">(defn train-from-corpus! [corpus]
  (doseq [v corpus]
    (let [[filename type] v]
      (train! (slurp filename) type))))

(defn cv-from-corpus [corpus]
  (for [v corpus]
    (let [[filename type] v
          [classification score] (classify (slurp filename))]
      {:filename filename
       :type type
       :classification classification
       :score score})))</pre></div><p>The <code class="literal">train-from-corpus!</code> function<a id="id527" class="indexterm"/> defined in the preceding code will train our spam classifier with all e-mails found in the <code class="literal">corpus/</code> folder. The <code class="literal">cv-from-corpus</code> function classifies the supplied e-mails as spam or ham using the trained classifier and returns a sequence of maps indicating the results of the cross-validation process. Each map in the sequence returned by the <code class="literal">cv-from-corpus</code> function<a id="id528" class="indexterm"/> contains the file of the e-mail, the actual type (spam or ham) of the e-mail, the predicted type of the e-mail, and the spam score of the e-mail. Now, we need to call these two functions on two appropriately partitioned subsets of the sample data as follows:</p><div><pre class="programlisting">(defn test-classifier! [corpus cv-fraction]
  "Trains and cross-validates the classifier with the sample
  data in corpus, using cv-fraction for cross-validation.
  Returns a sequence of maps representing the results
  of the cross-validation."
    (clear-db)
    (let [shuffled (shuffle corpus)
          size (count corpus)
          training-num (* size (- 1 cv-fraction))
          training-set (take training-num shuffled)
          cv-set (nthrest shuffled training-num)]
      (train-from-corpus! training-set)
      (await feature-db)
      (cv-from-corpus cv-set)))</pre></div><p>The <code class="literal">test-classifier!</code> function<a id="id529" class="indexterm"/> defined in the preceding code will randomly shuffle the sample data and select a specified<a id="id530" class="indexterm"/> fraction of this randomized data as the cross-validation set for our classifier. The <code class="literal">test-classifier!</code> function then calls the <code class="literal">train-from-corpus!</code> and <code class="literal">cv-from-corpus</code> functions to train and cross-validate the data. Note that the use of the <code class="literal">await</code> function<a id="id531" class="indexterm"/> is to wait until the <code class="literal">feature-db</code> agent has finished applying all functions that have been sent to it via the <code class="literal">send</code> function.</p><p>Now we need to analyze the results of cross-validation. We must first determine the number of incorrectly classified and missed e-mails from the actual and expected class of a given e-mail as returned by the <code class="literal">cv-from-corpus</code> function. We will do this with the help of the following code:</p><div><pre class="programlisting">(defn result-type [{:keys [filename type classification score]}]
  (case type
    :ham  (case classification
            :ham :correct
            :spam :false-positive
            :unsure :missed-ham)
    :spam (case classification
            :spam :correct
            :ham :false-negative
            :unsure :missed-spam)))</pre></div><p>The <code class="literal">result-type</code> function will determine the number of incorrectly classified and missed e-mails in the cross-validation process. We can now apply the <code class="literal">result-type</code> function to all the maps in the results returned by the <code class="literal">cv-from-corpus</code> function and print a summary of the cross-validation results with the help of the following code:</p><div><pre class="programlisting">(defn analyze-results [results]
  (reduce (fn [map result]
            (let [type (result-type result)]
              (update-in map [type] inc)))
          {:total (count results) :correct 0 :false-positive 0
           :false-negative 0 :missed-ham 0 :missed-spam 0}
          results))

(defn print-result [result]
  (let [total (:total result)]
    (doseq [[key num] result]
      (printf "%15s : %-6d%6.2f %%%n"
              (name key) num (float (* 100 (/ num total)))))))</pre></div><p>The <code class="literal">analyze-results</code> function<a id="id532" class="indexterm"/> defined in the preceding code simply applies the <code class="literal">result-type</code> function<a id="id533" class="indexterm"/> to all the map values in the sequence returned by the <code class="literal">cv-from-corpus</code> function<a id="id534" class="indexterm"/>, while maintaining the total number of incorrectly classified and missed e-mails. The <code class="literal">print-result</code> function simply prints the analyzed result as a string. Finally, let's define a function to<a id="id535" class="indexterm"/> load all the e-mails using the <code class="literal">populate-emails</code> function and then use this data to train and cross-validate our spam classifier. Since the <code class="literal">populate-emails</code> function<a id="id536" class="indexterm"/> will return an empty list, or <code class="literal">nil</code> when there are no e-mails, we will check this condition to avoid failing at a later stage in our program:</p><div><pre class="programlisting">(defn train-and-cv-classifier [cv-frac]
  (if-let [emails (seq (populate-emails))]
    (-&gt; emails
        (test-classifier! cv-frac)
        analyze-results
        print-result)
    (throw (Error. "No mails found!"))))</pre></div><p>In the <code class="literal">train-and-cv-classifier</code> function<a id="id537" class="indexterm"/> shown in the preceding code, we first call the <code class="literal">populate-emails</code> function and convert the result to a sequence using the <code class="literal">seq</code> function. If the sequence has any elements, we train and cross-validate the classifier. If there are no e-mails found, we simply throw an error. Note that the <code class="literal">if-let</code> function<a id="id538" class="indexterm"/> is used to check whether the sequence returned by the <code class="literal">seq</code> function has any elements.</p><p>We have all the parts needed to create and train a spam classifier. Initially, as the classifier hasn't seen any e-mails, the probability of any e-mail or text being spam is 0.5. This can be verified by using the <code class="literal">classify</code> function<a id="id539" class="indexterm"/>, as shown in the following code, which initially classifies any text as the <code class="literal">:unsure</code> type:</p><div><pre class="programlisting">user&gt; (classify "Make money fast")
[:unsure 0.5]
user&gt; (classify "Job interview today! Programmer job position for GNU project")
[:unsure 0.5]</pre></div><p>We now train the classifier and cross-validate it using the <code class="literal">train-and-cv-classifier</code> function<a id="id540" class="indexterm"/>. We will use one-fifth of all the available sample data as our cross-validation set. This is shown in the following code:</p><div><pre class="programlisting">user&gt; (train-and-cv-classifier 1/5)
          total : 600   100.00 %
        correct : 585    97.50 %
 false-positive : 1       0.17 %
 false-negative : 1       0.17 %
     missed-ham : 9       1.50 %
    missed-spam : 4       0.67 %
nil</pre></div><p>Cross-validating<a id="id541" class="indexterm"/> our spam classifier asserts that it's appropriately classifying e-mails. Of course, there is still a <a id="id542" class="indexterm"/>small amount of error, which can be corrected by using more training data. Now, let's try to classify some text using our trained spam classifier, as follows:</p><div><pre class="programlisting">user&gt; (classify "Make money fast")
[:spam 0.9720416490829515]
user&gt; (classify "Job interview today! Programmer job position for GNU project")
[:ham 0.19095646757667556]</pre></div><p>Interestingly, the text <code class="literal">"Make money fast"</code> is classified as spam and the text <code class="literal">"Job interview … GNU project"</code> is classified as ham, as shown in the preceding code. Let's have a look at how the trained classifier extracts features from some text using the <code class="literal">extract-features</code> function<a id="id543" class="indexterm"/>. Since the classifier will initially have read no tokens, this function will obviously return an empty list or <code class="literal">nil</code> when the classifier is untrained, as follows:</p><div><pre class="programlisting">user&gt; (extract-features "some text to extract")
(#clj_ml5.spam.TokenFeature{:token "some", :spam 91, :ham 837}
 #clj_ml5.spam.TokenFeature{:token "text", :spam 907, :ham 1975}
 #clj_ml5.spam.TokenFeature{:token "extract", :spam 3, :ham 5})</pre></div><p>As shown in the preceding code, each <code class="literal">TokenFeature</code> record will contain the number of times a given word is seen in spam and ham e-mails. Also, the word <code class="literal">"to"</code> is not recognized as a feature since we only consider words comprising of three or more characters.</p><p>Now, let's check how sensitive to spam e-mail our spam classifier actually is. We'll first have to select some text or a particular term that is classified as neither spam nor ham. For the training data selected for this example, the word <code class="literal">"Job"</code> fits this requirement, as shown in the following code. Let's train the classifier with the word <code class="literal">"Job"</code> while specifying the type of the text as ham. We can do this using the <code class="literal">train!</code> function, as follows:</p><div><pre class="programlisting">user&gt; (classify "Job")
[:unsure 0.6871002132196162]
user&gt; (train! "Job" :ham)
#&lt;Agent@1f7817e: 1993&gt;
user&gt; (classify "Job")
[:unsure 0.6592140921409213]</pre></div><p>After training the classifier with<a id="id544" class="indexterm"/> the given text as ham, the probability of the term being spam is observed to decrease by a small amount. If the term <code class="literal">"Job"</code> occurred in several more e-mails that were ham, the classifier would eventually classify this word as ham. Thus, the classifier doesn't show much of a reaction to a new ham e-mail. On the contrary, the classifier is observed to be very sensitive to spam e-mails, as shown in the following code:</p><div><pre class="programlisting">user&gt; (train! "Job" :spam)
#&lt;Agent@1f7817e: 1994&gt;
user&gt; (classify "Job")
[:spam 0.7445135045480734]</pre></div><p>An occurrence of a particular word in a single spam e-mail is observed to greatly increase a classifier's predicted probability <a id="id545" class="indexterm"/>of the given term belonging to a spam e-mail. The term <code class="literal">"Job"</code> will subsequently be classified as spam by our classifier, at least until it's seen to appear in a sufficiently large number of ham e-mails. This is due to the nature of the chi-squared distribution that we are modeling.</p><p>We can also improve the overall error<a id="id546" class="indexterm"/> of our spam classifier by supplying it with more training data. To demonstrate this, let's cross-validate the classifier with only one-tenth of the sample data. Thus, the classifier would be effectively trained with nine-tenths of the available data, as follows:</p><div><pre class="programlisting">user&gt; (train-and-cv-classifier 1/10)
          total : 300   100.00 %
        correct : 294    98.00 %
 false-positive : 0       0.00 %
 false-negative : 1       0.33 %
     missed-ham : 3       1.00 %
    missed-spam : 2       0.67 %
nil</pre></div><p>As shown in the preceding code, the number of misses and wrongly classified e-mails is observed to reduce when we use more training data. Of course, this is only shown as an example, and we should instead collect more e-mails to feed into the classifier as training data. Using a significant amount of the sample data for cross-validation is a good practice.</p><p>In summary, we have effectively<a id="id547" class="indexterm"/> built a spam classifier that is trained using Fisher's method. We have also implemented a cross-validation diagnostic, which serves as a kind of unit test for our classifier.</p><div><div><h3 class="title"><a id="note29"/>Note</h3><p>Note that the exact values produced by the <code class="literal">train-and-cv-classifier</code> function will vary depending on the spam and ham emails used as training data.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec41"/>Summary</h1></div></div></div><p>In this chapter, we have explored techniques that can be used to diagnose and improve a given machine learning model. The following are some of the other points that we have covered:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We have revisited the problems of underfitting and overfitting of sample data and also discussed how we can evaluate a formulated model to diagnose whether it's underfit or overfit.</li><li class="listitem" style="list-style-type: disc">We have explored cross-validation and how it can be used to determine how well a formulated model will respond to previously unseen data. We have also seen that we can use cross-validation to select the features and the regularization parameter of a model. We also studied a few kinds of cross-validation that we can implement for a given model.</li><li class="listitem" style="list-style-type: disc">We briefly explored learning curves and how they can be used to diagnose the underfit and overfit models.</li><li class="listitem" style="list-style-type: disc">We've explored the tools provided by the <code class="literal">clj-ml</code> library to cross-validate a given classifier.</li><li class="listitem" style="list-style-type: disc">Lastly, we've built an operational spam classifier that incorporates cross-validation to determine whether the classifier is appropriately classifying e-mails as spam.</li></ul></div><p>In the following chapters, we will continue exploring more machine learning models, and we'll also study <strong>Support Vector Machines</strong> (<strong>SVMs</strong>) in detail.</p></div></body></html>