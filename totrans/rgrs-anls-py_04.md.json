["```py\nIn:\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=100, n_features=2,\n n_informative=2, n_redundant=0,\n n_clusters_per_class=1,\n class_sep = 2.0, random_state=101)\n\nplt.scatter(X[:, 0], X[:, 1], marker='o', c=y,\n linewidth=0, edgecolor=None)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\nOut:\n\n```", "```py\nIn:\ny_orig = [0,0,0,0,0,0,1,1,1,1]\ny_pred = [0,0,0,0,1,1,1,1,1,0]\n\n```", "```py\nIn:\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_orig, y_pred)\nOut:\narray([[4, 2],\n [1, 3]])\n\n```", "```py\nIn:\nplt.matshow(confusion_matrix(y_orig, y_pred))\nplt.title('Confusion matrix')\nplt.colorbar()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\nOut:\n\n```", "```py\nIn:\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_orig, y_pred)\nOut:\n0.69999999999999996\n\n```", "```py\nIn:\nfrom sklearn.metrics import precision_score\nprecision_score(y_orig, y_pred)\nOut:\n0.59999999999999998\n\n```", "```py\nIn:\nfrom sklearn.metrics import recall_score\nrecall_score(y_orig, y_pred)\nOut:\n0.75\n\n```", "```py\nIn:\nfrom sklearn.metrics import f1_score\nf1_score(y_orig, y_pred)\nOut:\n0.66666666666666652\n\n```", "```py\nIn:\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_orig, y_pred))\nOut:\n\n```", "```py\nIn:\nfrom sklearn.cross_validation import train_test_split\n\nX_train, X_test, y_train, \\y_test = train_test_split(X, y.astype(float),\\test_size=0.33, random_state=101)\n\nIn:\ny_test.dtype\nOut:\ndtype('float64')\nIn:\ny_test\nOut:\n\n```", "```py\nIn:\nfrom sklearn.linear_model import LinearRegression\nregr = LinearRegression()\nregr.fit(X_train, y_train)\nregr.predict(X_test)\nOut:\n\n```", "```py\nIn:\nimport numpy as np\n\ndef model(x):\n return 1 / (1 + np.exp(-x))\n\nX_vals = np.linspace(-10, 10, 1000)\nplt.plot(X_vals, model(X_vals), color='blue', linewidth=3)\nplt.ylabel('sigma(t)')\nplt.xlabel('t')\n\nplt.show()\nOut:\n\n```", "```py\nIn:\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train.astype(int))\ny_clf = clf.predict(X_test)\n\nprint(classification_report(y_test, y_clf))\nOut:\n\n```", "```py\nIn:\n# Example based on:\n# Code source: Gaël Varoquaux, Modified for documentation by Jaques Grobler, License: BSD 3 clause\n\nh = .02  # step size in the mesh\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.autumn)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', linewidth=0, cmap=plt.cm.Paired)\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\nOut:\n\n```", "```py\nIn:\nZ = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\nZ = Z.reshape(xx.shape)\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.autumn)\n\nax = plt.axes()\nax.arrow(0, 0, clf.coef_[0][0], clf.coef_[0][1], head_width=0.5, \nhead_length=0.5, fc='k', ec='k')\nplt.scatter(0, 0, marker='o', c='k')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.show()\nOut:\n\n```", "```py\nIn:\n%timeit clf.fit(X, y)\nOut:\n1000 loops, best of 3: 291 µs per loop\nIn:\n%timeit clf.predict(X)\nOut:\n10000 loops, best of 3: 45.5 µs per loop\nIn:\n%timeit clf.predict_proba(X)\nOut:\n10000 loops, best of 3: 74.4 µs per loop\n\n```", "```py\nIn:\n%reset -f\nIn:\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=200, n_features=2,\n n_classes=3, n_informative=2,\n n_redundant=0, n_clusters_per_class=1,\n class_sep = 2.0, random_state=101)\n\nplt.scatter(X[:, 0], X[:, 1], marker='o', c=y, linewidth=0, edgecolor=None)\nplt.show()\nOut:\n\n```", "```py\nIn:\nfrom sklearn.cross_validation import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y.astype(float),\n test_size=0.33, random_state=101)\nIn:\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train.astype(int))\ny_clf = clf.predict(X_test)\nIn:\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_clf))\nOut:\n\n```", "```py\nIn:\nimport numpy as np\n\nh = .02  # step size in the mesh\n\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.autumn)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\nOut:\n\n```", "```py\nIn:\nprint(X_test[0])\nprint(y_test[0])\nprint(y_clf[0])\nOut:\n[ 0.73255032  1.19639333]\n0.0\n0\n\n```", "```py\nIn:\nclf.predict_proba(X_test[0])\nOut:\narray([[ 0.72797056,  0.06275109,  0.20927835]])\n\n```", "```py\nIn:\nX, y = make_classification(n_samples=10000, n_features=10,\n n_informative=10, n_redundant=0,\n random_state=101)\n\n```", "```py\nIn:\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nIn:\nXc = sm.add_constant(X)\nlogistic_regression = sm.Logit(y,Xc)\nfitted_model = logistic_regression.fit()\nOut:\nOptimization terminated successfully.\n Current function value: 0.438685\n Iterations 7\n\n```", "```py\nIn:\nfitted_model.summary()\nOut:\n\n```", "```py\nIn:\nimport pandas as pd\n\nXd = pd.DataFrame(X)\nXd.columns = ['VAR'+str(i+1) for i in range(10)]\nXd['response'] = y\n\nlogistic_regression = smf.logit(formula = \n 'response ~ VAR1+ VAR2 + VAR3 + VAR4 + \\\n VAR5 + VAR6 + VAR7 + VAR8 + VAR9 + VAR10', data=Xd)\n\nfitted_model = logistic_regression.fit()\nfitted_model.summary()\nOut:\n[same output as above]\n\n```", "```py\nIn:\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nobservations = len(X)\nvariables = ['VAR'+str(i+1) for i in range(10)]\nIn:\nimport random\n\ndef random_w( p ):\n return np.array([np.random.normal() for j in range(p)])\n\ndef sigmoid(X,w):\n return 1./(1.+np.exp(-np.dot(X,w)))\n\ndef hypothesis(X,w):\n return np.dot(X,w)\n\ndef loss(X,w,y):\n return hypothesis(X,w) - y\n\ndef logit_loss(X,w,y):\n return sigmoid(X,w) - y\n\ndef squared_loss(X,w,y):\n return loss(X,w,y)**2\n\ndef gradient(X,w,y,loss_type=squared_loss):\n gradients = list()\n n = float(len( y ))\n for j in range(len(w)):\n gradients.append(np.sum(loss_type(X,w,y) * X[:,j]) / n)\n return gradients\n\ndef update(X,w,y, alpha=0.01, loss_type=squared_loss):\n return [t - alpha*g for t, g in zip(w, gradient(X,w,y,loss_type))]\n\ndef optimize(X,y, alpha=0.01, eta = 10**-12, loss_type=squared_loss, iterations = 1000):\n standardization = StandardScaler()\n Xst = standardization.fit_transform(X)\n original_means, originanal_stds = standardization.mean_, standardization.std_\n Xst = np.column_stack((Xst,np.ones(observations)))\n w = random_w(Xst.shape[1])\n path = list()\n for k in range(iterations):\n SSL = np.sum(squared_loss(Xst,w,y))\n new_w = update(Xst,w,y, alpha=alpha, loss_type=logit_loss)\n new_SSL = np.sum(squared_loss(Xst,new_w,y))\n w = new_w\n if k>=5 and (new_SSL - SSL <= eta and new_SSL - SSL >= -eta):\n path.append(new_SSL)\n break\n if k % (iterations / 20) == 0:\n path.append(new_SSL)\n unstandardized_betas = w[:-1] / originanal_stds\n unstandardized_bias = w[-1]-np.sum((original_means / \noriginanal_stds) * w[:-1])\nreturn np.insert(unstandardized_betas, 0, unstandardized_bias), \npath,k\n\nalpha = 0.5\nw, path, iterations = optimize(X, y, alpha, eta = 10**-5, loss_type=logit_loss, iterations = 100000)\nprint (\"These are our final standardized coefficients: %s\" % w)\nprint (\"Reached after %i iterations\" % (iterations+1))\nOut:\nThese are our final standardized coefficients: [ 0.42991407  0.0670771  -0.78279578  0.12208733  0.28410285  0.14689341\n -0.34143436  0.05031078 -0.1393206   0.11267402 -0.47916908]\nReached after 868 iterations \n\n```", "```py\nIn:\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(C=1E4, tol=1E-25, random_state=101)\nclf.fit(X,y)\nOut:\nLogisticRegression(C=10000.0, class_weight=None, dual=False,\n fit_intercept=True, intercept_scaling=1, max_iter=100,\n multi_class='ovr', penalty='l2', random_state=101,\n solver='liblinear', tol=1e-25, verbose=0)\nIn:\ncoeffs = [clf.intercept_[0]]\ncoeffs.extend(clf.coef_[0])\ncoeffs\nOut:\n\n```", "```py\nIn:\nfrom sklearn.linear_model import SGDClassifier\n\nclf = SGDClassifier(loss='log', alpha=1E-4, n_iter=1E2, random_state=101)\nclf.fit(X,y)\nOut:\nSGDClassifier(alpha=0.0001, average=False, class_weight=None,\n epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n learning_rate='optimal', loss='log', n_iter=100.0,\n n_jobs=1, penalty='l2', power_t=0.5, random_state=101,\n shuffle=True, verbose=0, warm_start=False)\nIn:\ncoeffs = [clf.intercept_[0]]\ncoeffs.extend(clf.coef_[0])\ncoeffs\nOut:\n\n```"]