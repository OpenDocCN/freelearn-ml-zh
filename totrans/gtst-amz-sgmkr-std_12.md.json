["```py\n    import smdistributed.dataparallel.tensorflow as sdp\n    ```", "```py\n    sdp.init()\n    ```", "```py\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        local_gpu = gpus[sdp.local_rank()]\n        tf.config.experimental.set_visible_devices(local_gpu, 'GPU')\n    ```", "```py\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n    ```", "```py\n    args.learning_rate = args.learning_rate * sdp.size()\n    ```", "```py\n    model = get_model(args)\n    loss = tf.losses.BinaryCrossentropy(name = 'binary_crossentropy')\n    acc = tf.metrics.BinaryAccuracy(name = 'accuracy')\n    optimizer = tf.optimizers.Adam(learning_rate = args.learning_rate)\n    with tf.GradientTape() as tape:\n        probs = model(x_train, training=True)\n        loss_value = loss(y_train, probs)\n        acc_value = acc(y_train, probs)\n    ```", "```py\ntape = sdp.DistributedGradientTape(tape, sparse_as_dense = True)\n```", "```py\n    if first_batch:\n        sdp.broadcast_variables(model.variables, root_rank=0)\n        sdp.broadcast_variables(optimizer.variables(), root_rank=0)\n    ```", "```py\n    loss_value = sdp.oob_allreduce(loss_value)\n    acc_value = sdp.oob_allreduce(acc_value)\n    ```", "```py\n    train_dataset.take(len(train_dataset)//sdp.size())\n    ```", "```py\n    if sdp.rank() == 0:\n        model.save(os.path.join(args.model_dir, '1'))\n    ```", "```py\n    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    ```", "```py\n    distribution = {'smdistributed': {'dataparallel': {'enabled': True}}}\n    ```", "```py\ntrain_instance_type = 'ml.p3.16xlarge'\nestimator = TensorFlow(source_dir='code',\n     entry_point='smdp_tensorflow_sentiment.py',\n     ...\n     distribution=distribution)\n```", "```py\n    import smdistributed.modelparallel.torch as smp\n    ```", "```py\n    smp.init()\n    ```", "```py\n    torch.cuda.set_device(smp.local_rank())\n    device = torch.device('cuda')\n    ```", "```py\n    if smp.local_rank() == 0:\n        dataset1 = datasets.MNIST('../data', train=True, \n                      download=True, transform=transform)        \n    smp.barrier() # Wait for all processes to be ready\n    ```", "```py\n    model = smp.DistributedModel(model)\n    optimizer = smp.DistributedOptimizer(optimizer)\n    ```", "```py\n    @smp.step\n    def train_step(model, data, target):\n        output = model(data)\n        loss = F.nll_loss(output, target, reduction='mean')\n        model.backward(loss)\n        return output, loss\n    ```", "```py\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        _, loss_mb = train_step(model, data, target)\n        # Average the loss across microbatches.\n        loss = loss_mb.reduce_mean()\n        optimizer.step()\n```", "```py\n    if smp.dp_rank() == 0:\n        model_dict = model.local_state_dict()\n        opt_dict = optimizer.local_state_dict()\n        model = {'model_state_dict': model_dict, 'optimizer_state_dict': opt_dict}\n        model_output_path = f'{args.model_dir}/pt_mnist_checkpoint.pt'\n        smp.save(model, model_output_path, partial=True)\n    ```", "```py\n    distribution = {'smdistributed': {\n                        'modelparallel': {\n                            'enabled': True,\n                            'parameters': {\n                                'partitions': 2,\n                                 'optimize': 'speed',\n                                'microbatches': 4,\n                                'pipeline': 'interleaved',\n                                'ddp': False\n                            }\n                        }\n                    },\n                    'mpi': {\n                        'enabled': True,\n                        'processes_per_host': 2\n                    }\n                }\n    ```", "```py\n    if not os.listdir(args.checkpoint_dir):\n        model = get_model(args)\n        initial_epoch_number = 0\n    else:    \n        model, initial_epoch_number = load_model_from_checkpoints(args.checkpoint_dir)\n    ```", "```py\nuse_spot_instances = True\nmax_run = 3600\nmax_wait = 3600\ncheckpoint_suffix = str(uuid.uuid4())[:8]\ncheckpoint_s3_uri = f's3://{bucket}/{prefix}/checkpoint-{checkpoint_suffix}'\nestimator = TensorFlow(use_spot_instances=use_spot_instances,\n                       checkpoint_s3_uri=checkpoint_s3_uri,\n                       max_run=max_run,\n                       max_wait=max_wait,\n                       ...)\n```"]