<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Neural Networks</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we discussed neural networks and their basic operation. Specifically, we discussed the fully connected feedforward neural network, which is just one simple topology out of many possible ANN topologies. In this chapter, we're going to focus on two advanced topologies: the <strong>Convolutional Neural Network</strong> (<strong>CNN</strong>) and one form of <strong>recurrent neural network</strong> (<strong>RNN</strong>), called the <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) network. CNNs are used most often for image processing tasks, such as object detection and image classification. LSTM networks are often used in NLP or language-modeling problems.</p>
<p>These exotic ANN topologies are considered to be <strong>deep neural networks</strong> (<strong>DNNs</strong>). While the term is not well-defined, DNNs are typically understood to be ANNs with multiple hidden layers between the input and output layers. Convolutional network architectures can become quite deep, with ten or more layers in the network. Recurrent architectures can be deep as well, however, much of their depth comes from the fact that information can flow either forward or backward through the network.</p>
<p>In this chapter, we're going to take a look at TensorFlow's capabilities in terms of CNN and RNN architectures. We will discuss TensorFlow's own examples of these topologies and take a look at how they are used in practice. In particular, we will discuss the following topics:</p>
<ul>
<li>CNNs</li>
<li>Simple RNNs</li>
<li>Gated recurrent unit networks</li>
<li>LSTM networks</li>
<li>CNN-LSTM networks for advanced applications</li>
</ul>
<p>Let's get started by taking a look at a classic <strong>machine learning</strong> (<strong>ML</strong>) problem: identifying handwritten digits from images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p>To make the case for CNNs, let's first imagine how we might approach an image classification task using a standard feedforward, fully connected ANN. We start with an image that's 600 x 600 pixels in size with three color channels. There are 1,080,000 pieces of information encoded in such an image (600 x 600 x 3), and therefore our input layer would require 1,080,000 neurons. If the next layer in the network contains 1,000 neurons, we'd need to maintain one billion weights between the first two layers alone. Clearly, the problem is already becoming untenable.</p>
<p>Assuming the ANN in this example can be trained, we'd also run into problems with scale and position invariance. If your task is to identify whether or not an image contains street signs, the network may have difficulty understanding that street signs can be located in any position in the image. The network may also have issues with color; if most street signs are green, it may have difficulty identifying a blue sign. Such a network would require many training examples to get around issues of scale, color, and position variance.</p>
<p>In the past, before CNNs became popular, many researchers viewed this problem as a dimensionality reduction problem. One common tactic was to convert all images to grayscale, reducing the amount of data by a factor of three. Another tactic is to downscale images to something more manageable, such as 100 x 100 pixels, or even smaller, depending on the type of processing required. Converting our 600 x 600 image to grayscale and to 100 x 100 would reduce the number of input neurons by a factor of 100, from one million to 10,000, and further reduce the number of weights between the input layer and a 1,000-neuron hidden layer down from 1 billion to only 10 million.</p>
<p>Even after employing these dimensionality reduction techniques, we would still require a very large network with tens of millions of weights. Converting images to grayscale before processing avoids issues with color detection, but still does not solve scale and position variance problems. We are also still solving a very complex problem, since shadows, gradients, and the overall variance of images would require us to use a very large training set.</p>
<p>Another common preprocessing tactic employed was to perform various operations on images, such as noise reduction, edge detection, and smoothing. By reducing shadows and emphasizing edges, the ANN gets clearer signals to learn from. The problem with this approach is that preprocessing tasks are typically unintelligent; the same edge detection algorithm gets applied to every image in the set, whether or not that specific edge detection algorithm is actually effective on a particular image.</p>
<p>The challenge, then, is to incorporate the image-preprocessing tasks directly in the ANN. If the ANN itself manages the preprocessing tasks, the network can learn the best and most efficient ways to preprocess the images in order to optimize the network's accuracy. Recall from <span><span><a href="69151615-d71a-4e8f-86c5-90801ffa5393.xhtml" target="_blank">Chapter 8</a>, <em>Artificial Neural Network Algorithms</em></span></span> that we can use <em>any </em>activation function in a neuron, as long as we can differentiate the activation function and employ its gradient in the backpropagation algorithm.</p>
<p>In short, a CNN is an ANN with multiple—perhaps many—preprocessing layers that perform transformations on the image before ultimately reaching a final fully connected layer or two that performs the actual classification. By incorporating the preprocessing tasks into the network, the backpropagation algorithm can tune the preprocessing tasks as part of the network training. The network will not only learn how to classify images, it will also learn how to preprocess the images for your task.</p>
<p>Convolutional networks contain several distinct layer types in addition to the standard ANN layer types. Both types of network contain an input layer, an output layer, and one or more fully connected layers. A CNN, however, also incorporates convolution layers, ReLU layers, and pooling layers. Let's take a look at each in turn.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutions and convolution layers</h1>
                </header>
            
            <article>
                
<p>Convolutions are a mathematical tool that combine two functions into a new function; specifically, the new function represents the area under the curve created by the pointwise multiplication of one function as another function is swept over it. If this is difficult to visualize, don't worry; it's easiest to visualize as an animation, which unfortunately we can't print in a book. The mathematical details of convolutions will not be important in this chapter, but I do encourage you to do some additional reading on the topic.</p>
<p>Most image filters—such as blur, sharpen, edge detect, and emboss—can be accomplished with convolution operations. In an image context, convolutions are represented by a <em>convolution matrix, </em>which is typically a small matrix (3 x 3, 5 x 5, or something similar). The convolution matrix is much smaller than the image to be processed, and the convolution matrix is swept across the image so the output of the convolution applied to the entire image builds a new image with the effect applied.</p>
<p>Consider the following image of Van Gogh's <em>Water Lilies</em>. Here is the original:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-206 image-border" src="assets/b5059b31-b768-4646-8435-813766e67f9e.jpeg" style="width:29.00em;height:19.08em;"/></div>
<p>I can use my image editor's <em>convolution matrix </em>filter to create a sharpening effect. This has the same effect as the image editor's <em>sharpen </em>filter, except that I'm writing the convolution matrix manually:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-207 image-border" src="assets/6ed62c7b-5cdd-480a-be07-16c049351766.png" style="width:26.58em;height:18.42em;"/></div>
<p>The result is a sharpened version of the original image:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-208 image-border" src="assets/3b3c3dff-d96b-4f66-b78f-0fe9bf40be49.jpeg" style="width:29.42em;height:19.33em;"/></div>
<p>I can also write a convolution matrix that blurs the image:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-209 image-border" src="assets/315d900d-e70a-4f56-ae1d-eefed0cd51ea.png" style="width:27.75em;height:19.50em;"/></div>
<p>It results in the following image. The effect is subtle, as the oil panting itself is a little blurry, but the effect is there:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-210 image-border" src="assets/fa8ff56a-c6cc-47f3-b4f6-d9d0d2f89376.jpeg" style="width:30.17em;height:19.83em;"/></div>
<p>Convolutions can also be used to emboss or detect edges:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-211 image-border" src="assets/eb7c1e45-a7da-491f-84d1-9c7d4c000132.png" style="width:29.17em;height:20.08em;"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">The preceding matrix results in the following:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-212 image-border" src="assets/66dd6ff7-6701-4afd-a654-613af40cfb90.jpeg" style="width:26.75em;height:17.58em;"/></div>
<p>A CNN uses multiple convolving layers, each with multiple convolution filters, to build a model of the image. The convolving layers and the convolution filters themselves are trained by the backpropagation algorithm, and the network will eventually discover the correct filters to use in order to enhance the features that the network is trying to identify. As with all learning problems, the types of filters the CNN develops may not necessarily be readily understood or interpretable by a human, but in many cases, you will find that your network develops a number of convolution filters that perform blur, edge detection, color isolation, and gradient detection.</p>
<p>In addition to extracting useful features from images, the convolution operations in effect provide for spatial and positional independence of features. The convolving layers are not fully connected, and therefore are able to inspect specific areas of the image. This reduces the dimensionality required of the weights in between layers and also helps us avoid reliance on the spatial positioning of features.</p>
<p>There is still a lot of data involved in these operations, so convolving layers are typically immediately followed by pooling layers, which essentially downsample an image. Most often you will employ something such as <em>2 x 2 max pooling</em>, which means that for every 2 x 2 area of pixels in the source feature, the pooling layer will downsample the 2 x 2 area to a single pixel that has the value of the maximum pixel in the source 2 x 2 area. A 2 x 2 pooling layer therefore reduces the image size by a factor of four; because the convolution operation (which may also reduce dimensionality) has already occurred, this downsampling will typically reduce the computation required without the loss of too much information.</p>
<p>In some cases, a CNN will employ simple ReLU activation functions immediately following the convolution operations and immediately preceding pooling; these ReLU functions help avoid oversaturation of the image or the feature maps that result from the convolution operations.</p>
<p>A typical architecture for a simple CNN would look like this:</p>
<ul>
<li>Input layer, with width x height x color depth neurons</li>
<li>Convolving layer, with N convolution filters of an M x M size</li>
<li>Max pooling layer</li>
<li>Second convolving layer</li>
<li>Second max pooling layer</li>
<li>Fully connected output layer</li>
</ul>
<p>More complex architectures for CNNs typically include several more groups of convolving and pooling layers, and may also involve two convolving layers in a row before reaching a pooling layer.</p>
<p>Each successive convolving layer in the network operates at a higher level than the convolving layers before it. The first convolving layer will only be able to perform simple convolutions, such as edge detection, smoothing, and blurring. The next convolving layer, however, is able to combine the results from previous convolutions into higher level features, such as basic shapes or color patterns. A third convolving layer can further combine information from previous layers to detect complex features, such as wheels, street signs, and handbags. The final fully connected layer, or layers, acts much like a standard feedforward ANN, and performs the actual classification of the image based on the high-level features that the convolving layers have isolated.</p>
<p>Let's now attempt to employ this technique in practice using <kbd>TensorFlow.js</kbd> on the MNIST handwritten digit dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example – MNIST handwritten digits</h1>
                </header>
            
            <article>
                
<p>Rather than building an example from first principles, let's instead walk through an excellent <kbd>TensorFlow.js</kbd> MNIST example. The goal of this example is to train a CNN to classify images of handwritten digits. More specifically, the goal of this example is to achieve a high accuracy in classifications made against the MNIST handwritten digit dataset. In this section, we will aim to get an understanding of the code and the algorithm by performing experiments on the code and observing their results.</p>
<p>The current version of this example may be found on <kbd>TensorFlow.js</kbd>'s GitHub: <a href="https://github.com/tensorflow/tfjs-examples/tree/master/mnist">https://github.com/tensorflow/tfjs-examples/tree/master/mnist</a>. However, as the repository may be updated after this writing, I have also added the version that I am using as a Git submodule in this book's example repository. If you are using this book's repository and haven't already done so, please run <kbd>git submodule init</kbd>; <kbd>git submodule update</kbd> from the command line in the repository directory.</p>
<p>In the terminal, navigate to <kbd>Ch5-CNN</kbd>. <span>This path is a symbolic link, so if it doesn't work on your system, you may alternately navigate to </span><kbd>tfjs-examples/mnist</kbd>.</p>
<p>Next, issue <kbd>yarn</kbd> from the command line to build the code, and finally issue <kbd>yarn watch</kbd>, which will start a local server and launch your browser to <kbd>http://localhost:1234</kbd>. If you have any other programs using that port, you will have to terminate them first.</p>
<p>The page will start by downloading MNIST images from Google's servers. It will then train a CNN for 150 epochs, periodically updating two graphs that show the loss and the accuracy. Recall that the loss is typically a metric, such as <strong>mean square error</strong> (<strong>MSE</strong>), while accuracy is the percentage of correct predictions. Finally, the page will display a few example predictions, highlighting correct versus incorrect predictions.</p>
<p>My test run of this page yielded a CNN with an accuracy of around 92%:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-216 image-border" src="assets/b8f0de54-12f5-4011-8352-151ec75d87d2.png" style="width:49.92em;height:20.08em;"/></div>
<p>Often, the incorrect predictions are understandable. In this example, the digit 1 does seem to be shaped a bit like a 2. It is unlikely a human would have made this particular error, though I have encountered examples where I would have gotten the prediction wrong as well:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-215 image-border" src="assets/496b160d-dafe-439a-8353-aa708bcb7765.png" style="width:15.83em;height:19.08em;"/></div>
<p>Opening <kbd>index.js</kbd>, we can see the topology of the network toward the top of the file:</p>
<pre>model.add(tf.layers.conv2d({<br/>  inputShape: [<span>28</span>, <span>28</span>, <span>1</span>],<br/>  kernelSize: <span>5</span>,<br/>  filters: <span>8</span>,<br/>  strides: <span>1</span>,<br/>  activation: <span>'relu'</span>,<br/>  kernelInitializer: <span>'varianceScaling'<br/></span>}));<br/>model.add(tf.layers.maxPooling2d({poolSize: [<span>2</span>, <span>2</span>], strides: [<span>2</span>, <span>2</span>]}));<br/>model.add(tf.layers.conv2d({<br/>  kernelSize: <span>5</span>,<br/>  filters: <span>16</span>,<br/>  strides: <span>1</span>,<br/>  activation: <span>'relu'</span>,<br/>  kernelInitializer: <span>'varianceScaling'<br/></span>}));<br/>model.add(tf.layers.maxPooling2d({poolSize: [<span>2</span>, <span>2</span>], strides: [<span>2</span>, <span>2</span>]}));<br/>model.add(tf.layers.flatten());<br/>model.add(tf.layers.dense(<br/>    {units: <span>10</span>, kernelInitializer: <span>'varianceScaling'</span>, activation: <span>'softmax'</span>}));</pre>
<p>This network has two convolving layers with a pooling layer after each, and then a single fully connected layer that makes a prediction. Both the convolving layers use a <kbd>kernelSize</kbd> of <kbd>5</kbd>, which means that the convolution filter is a 5 x 5 matrix. The first convolving layer uses eight filters, while the second uses 16. This means that the first layer will create and use eight different convolution filters, therefore identifying eight separate graphical features of the image. These features may be abstract, but in the first layer it is common to see features that represent edge detection, blurring or sharpening, or gradient identification.</p>
<p>The second convolving layer uses 16 features, which likely will be of a higher level than the first layer's features. This layer may try to identify straight lines, circles, curves, swoops, and so on. There are more high-level features than there are low-level features, so it makes sense that the first layer uses fewer filters than the second layer.</p>
<p>The final dense layer is a fully connected layer of 10 neurons, each representing a digit. The softmax activation function ensures that the output is normalized to 1. The input to this final layer is a flattened version of the second pooling layer. The data needs flattening because convolving and pooling layers are typically multidimensional. Convolving and pooling layers use matrices representing height, width, and color depth, which themselves are in turn stacked atop one another as the result of the convolution filters used. The output of the first convolving layer, for example, will be a volume that is [28 x 28 x 1] x 8 in size. The bracketed portion is the result of a single convolution operation (that is, a filtered image), and eight of them have been generated. When connecting this data to a vector layer, such as the standard dense or fully connected layer, it must also be flattened into a vector.</p>
<p>The data entering the final dense layer is much smaller than the data coming out of the first layer. The max-pooling layers serve to downscale the image. The<span><span> </span></span><kbd>poolSize </kbd>parameter of <kbd>[2, 2]</kbd> means that a 2 x 2 window of pixels will be reduced to a single value; since we are using max-pooling, this will be the largest value (the lightest pixel) in the set. The <kbd>strides</kbd> parameter means that the pooling window will move in steps of two pixels at a time. This pooling will reduce both the height and width of the image by half, meaning that the image and the data is reduced in area by a factor of four. After the first pooling operation, images are reduced to 14 x 14, and after the second they are 7 x 7. Because there are 16 filters in the second convolving layer, this means that the flattened layer will have <em>7 * 7 * 16 = 784</em> neurons.</p>
<p>Let's see whether we can squeeze some more accuracy out of this model by adding another fully connected layer before the output. In the best case scenario, adding another layer will give us an improved ability to interpret the interplay of the 16 features that the convolutions generate.</p>
<p>However, adding another layer will increase the required training time, and it also may not improve results. It's perfectly possible that there is no more information to be discovered by adding another layer. Always remember that ANNs simply build and navigate a mathematical landscape, looking for shapes in the data. If the data isn't highly dimensional, adding another dimension to our capabilities may simply be unnecessary.</p>
<p>Add the following line, before the final dense layer in the code:</p>
<pre>model.add(tf.layers.dense(<br/>    {units: <span>100</span>, kernelInitializer: <span>'varianceScaling'</span>, activation: <span>'sigmoid'</span>}));</pre>
<p>In context, the code should now look like this, with the new line highlighted:</p>
<pre>model.add(tf.layers.maxPooling2d({poolSize: [<span>2</span>, <span>2</span>], strides: [<span>2</span>, <span>2</span>]}));<br/>model.add(tf.layers.flatten());<br/><strong>model.add(tf.layers.dense(</strong><br/><strong>    {units: <span>100</span>, kernelInitializer: <span>'varianceScaling'</span>, activation: <span>'sigmoid'</span>}));</strong><br/>model.add(tf.layers.dense(<br/>    {units: <span>10</span>, kernelInitializer: <span>'varianceScaling'</span>, activation: <span>'softmax'</span>}));<br/><br/><span>const </span>LEARNING_RATE = <span>0.15</span>;</pre>
<p>Since you have issued <kbd>yarn watch</kbd> from the command line, the code should automatically rebuild. Refresh the page and observe the results:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-217 image-border" src="assets/fd60951b-94b1-4cfc-bc02-298925b711f3.png" style="width:50.42em;height:17.83em;"/></div>
<p>The algorithm is learning at a slower rate than the original version, which is expected because we have added a new layer and therefore more complexity to the model. Let's increase the training limit a little bit.</p>
<p>Find the <kbd>TRAIN_BATCHES</kbd> variable and update it to <kbd>300</kbd>. The line should now look like this:</p>
<pre><span>const </span>TRAIN_BATCHES = <span>300</span>;</pre>
<p>Save the file to trigger the rebuild and reload the page. Let's see whether we can beat the baseline:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-218 image-border" src="assets/8b9bdac3-9b99-4b7f-b002-059567d6e6d8.png" style="width:52.33em;height:18.17em;"/></div>
<p>It does seem that we have indeed beaten the baseline score of 92%, however I would caution against too much optimism. It is possible we have overtrained and overfit the model, and there is a chance it will not perform so well in real life. Additionally, because training and validation are stochastic, it is possible that the true accuracy of this network is comparable to the baseline's. Indeed, 92% is already an excellent result and I would not expect much better from any model. However this is still an encouraging result, as the new layer was not too much of a burden to add.</p>
<p>At this point, revert your changes so that you are working with the original copy of the file. Let's run a different experiment. It would be interesting to see how small we can make the network without losing too much accuracy.</p>
<p>First, let's reduce the number of convolution filters the second convolving layer uses. My reasoning is that numerals use pretty simple shapes: circles, lines, and curves. Perhaps we don't need to capture 16 different features. Maybe eight will do. In the second convolving layer, change <kbd>filters: 8</kbd> to <kbd>filters: 2</kbd>. Your code should now read:</p>
<pre>...<br/>model.add(tf.layers.maxPooling2d({poolSize: [<span>2</span>, <span>2</span>], strides: [<span>2</span>, <span>2</span>]}));<br/>model.add(tf.layers.conv2d({<br/>  kernelSize: <span>5</span>,<br/>  <strong>filters: 2,</strong><br/>  strides: <span>1</span>,<br/>  activation: <span>'relu'</span>,<br/>  kernelInitializer: <span>'varianceScaling'<br/></span>}));<br/>model.add(tf.layers.maxPooling2d({poolSize: [<span>2</span>, <span>2</span>], strides: [<span>2</span>, <span>2</span>]}));<br/>...</pre>
<p>Rerunning the code, we see that we still get decent accuracy, though the variance is a little higher than the baseline:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-219 image-border" src="assets/c9c3f174-2ebb-4800-9601-360c3e9e80c6.png" style="width:52.08em;height:18.33em;"/></div>
<p>This supports the overall idea that the shapes and features used are relatively few. However, when we look at the test examples, we also find that the mistakes are less <em>understandable</em> than before. Perhaps we have not lost much accuracy, but our model has become more abstract:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-220 image-border" src="assets/a085a4ec-111b-42ce-983d-fa02d3c053f0.png" style="width:14.00em;height:16.92em;"/></div>
<p>I encourage you to continue exploring and experimenting with this example, as there are many things you can learn by reading the code. One aspect of this example I would like to point out in particular is the <kbd>data.js</kbd> file, which manages the handling of the MNIST dataset. In your real-world applications, you will likely need to employ an approach similar to this, as your training data will not always be on the local machine. This file handles downloading data from a remote source, splitting it into testing and validation sets, and maintaining batches to be requested by the training algorithm. It is a good, lightweight approach to follow if you require training data from a remote source. We will discuss this topic in-depth in <a href="8bb0fe0d-84df-41b9-a955-69a84eb2d8ea.xhtml" target="_blank">Chapter 11</a>, <em>Using Machine Learning in Real-Time Applications</em>.</p>
<p>Here are some ideas for experiments you can try:</p>
<ul>
<li>Make the network as small as possible while maintaining 90%+ accuracy.</li>
<li>Make the network as small as possible while maintaining 85%+ accuracy.</li>
<li>Train the model to 90%+ accuracy in fewer than 50 epochs.</li>
<li>Discover the fewest number of training examples required to achieve 90%+ accuracy (reduce the value of <kbd>NUM_TRAIN_ELEMENTS </kbd>in <kbd>data.js </kbd>to use fewer training examples)</li>
</ul>
<p>In the next section, we will explore series prediction with recurrent neural networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent neural networks</h1>
                </header>
            
            <article>
                
<p>There are many cases where memory is required of neural networks. For instance, when modeling natural language context is important, that is, the meaning of a word late in a sentence is affected by the meaning of words earlier in the sentence. Compare this to the approach used by Naive Bayes classifiers, where only the bag of words is considered but not their order. Similarly, time series data may require some memory in order to make accurate predictions, as a future value may be related to current or past values.</p>
<p>RNN are a family of ANN topologies in which the information does not necessarily flow in only one direction. In contrast to feedforward neural networks, RNNs allow the output of neurons to be fed backward into their input, creating a feedback loop. Recurrent networks are almost always time-dependent. The concept of time is flexible, however; ordered words in a sentence can be considered time-dependent, as one word must follow another. It is not necessary for the time-dependence of RNNs to be related to the actual passage of time on a clock.</p>
<p>In the simplest case, all that is required of an RNN is for the output value of a neuron to be connected - typically with a weight or decay factor—not just to neurons in the next layer, but also back to its own input. If you are familiar with <strong>finite impulse response</strong> (<strong>FIR</strong>) filters in digital signal processing, this style of neuron can be considered a variant of an FIR filter. This type of feedback results in a sort of memory, as the previous activation value is partially preserved and used as an input to the neuron's next cycle. You can visualize this as an echo created by the neuron, becoming more and more faint until the echo is no longer audible. Networks designed in this manner will therefore have a finite memory, as ultimately the echo will fade away to nothing.</p>
<p>Another style of RNN is fully recurrent RNNs, in which every neuron is connected to every other neuron, whether in the forward or backward direction. In this case, it is not just a single neuron that can hear its own echo; every neuron can hear the echoes of every other neuron in the network.</p>
<p>While these types of networks are powerful, in many cases a network will need memory that persists longer than an echo will last. A very powerful, exotic topology, called <strong>LSTM</strong>, was invented to solve the problem of long-term memory. The LSTM topology uses an exotic form of neuron called an LSTM unit, which is capable of storing all previous input and activation values and recalling them when calculating future activation values. When the LSTM network was first introduced, it broke an impressive number of records, particularly in speech recognition, language modeling, and video processing.</p>
<p>In the next section, we will briefly discuss three different types of RNN topologies provided by TensorFlow.js: the SimpleRNN (or fully recurrent RNN), the <strong>gated recurrent unit</strong> (<strong>GRU</strong>) network, and the LSTM network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SimpleRNN</h1>
                </header>
            
            <article>
                
<p>The first RNN layer provided out-of-the-box by <kbd>TensorFlow.js</kbd> is the SimpleRNN layer type, which is a layer composed of a SimpleRNNCell neuron. This is an exotic neuron that can feed its output back to its input. The input to such a neuron is a vector of time-dependent values; the activation output of each input value is fed back into the input of the next value, and so on. A <em>dropout </em>factor between 0 and 1 may be specified; this value represents the strength of each echo. A neuron designed in this manner is similar in many ways to an FIR filter.</p>
<p>In fact, this type of RNN architecture is made possible by earlier work in digital signal processing concerning FIR filters. The advantage of this architecture is that the mathematics are well-understood. It is possible to <em>unroll</em> an RNN, meaning that it is possible to create a feedforward ANN of many layers that generates the same results as an RNN with fewer layers. This is because the echoes of the neurons' feedback are finite. If a neuron is known to echo 20 times, then that neuron can be modeled as 21 feedforward neurons (including the source neuron). Initial efforts in training these networks were inspired by work on FIR filters, as the analysis is much the same.</p>
<p>Consider the following image, created by François Deloche (own work, CC BY-SA 4.0), which illustrates the unrolling of a recurrent neuron:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-307 image-border" src="assets/89d834d6-7df8-407e-9ec9-cf2a41d56ce5.png" style="width:39.92em;height:11.50em;"/></div>
<p>The loop labeled <strong>V</strong> represents the feedback operation of the neuron. As future input values (<strong>X</strong>) are given to the neuron, the output from the previous activation reaches the input and becomes an input factor. As the graphic illustrates, this can be modeled as a linear series of simple neurons.</p>
<p>From TensorFlow's perspective, the operation of recurrent layers are abstracted away by the TensorFlow layers API. Let's look at another of TensorFlow.js's examples that illustrates the interchangeability of various RNN architectures.</p>
<p>From this book's GitHub repository, navigate to the <kbd>Ch9-RNN</kbd> directory, which is once again a symbolic link to the <kbd>tfjs-examples/addition-rnn</kbd> directory. (If you still have the previous RNN example running, you will need to stop it by pressing <em>Ctrl </em>+ <em>C</em> in the terminal that is running the yarn watch command.) First, issue the <kbd>yarn</kbd> command to build the code, then run <kbd>yarn watch</kbd> to once again start a local server and navigate to <kbd>http://localhost:1234</kbd>.</p>
<p>This particular example is meant to teach an RNN integer addition by example. The training data will be a list of questions, such as <kbd>24 + 22</kbd> or <kbd>14 + 54</kbd>, represented in string form, and the network will need to be able to decode the string, represent it numerically, learn the answers, and be able to extend the knowledge to new examples.</p>
<p>When the page loads, you'll see the following form. Keep the defaults and click the <strong>Train Model </strong>button:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-308 image-border" src="assets/df4355c3-4336-476c-beed-99d8bf83e2cb.png" style="width:25.50em;height:18.83em;"/></div>
<p>You'll see loss and accuracy graphs similar to the following, which show that after 100 epochs of training, the accuracy for this model was 93.8%:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-224 image-border" src="assets/7970408c-f5d0-498f-80be-4a276443ff7d.png" style="width:54.67em;height:13.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The loss and similarity graph</div>
<p>You'll also see test results from a random test input that the model validates against:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-309 image-border" src="assets/00c97fbd-a14a-4506-b313-66a262fc8e87.png" style="width:7.17em;height:29.92em;"/></div>
<p>Let's take a closer look at how this is working under the hood. Open the <kbd>index.js </kbd>file and find the <kbd>createAndCompileModel </kbd>function. I will assume that you selected the SimpleRNN network type for this example, and omit the switch/case statements that handle GRU and LSTM topologies:</p>
<pre><span>function </span><span>createAndCompileModel</span>(<br/>    layers, hiddenSize, rnnType, digits, vocabularySize) {<br/>    <span>const </span><span>maxLen </span>= digits + <span>1 </span>+ digits;<br/><br/>    <span>const </span><span>model </span>= <span>tf</span>.<span>sequential</span>();<br/>    <span>model</span>.<span>add</span>(<span>tf</span>.<span>layers</span>.<span>simpleRNN</span>({<br/>        <span>units</span>: hiddenSize,<br/>        <span>recurrentInitializer</span>: <span>'glorotNormal'</span>,<br/>        <span>inputShape</span>: [<span>maxLen</span>, vocabularySize]<br/>    }));<br/>    <span>model</span>.<span>add</span>(<span>tf</span>.<span>layers</span>.<span>repeatVector</span>({<span>n</span>: digits + <span>1</span>}));<br/>    <span>model</span>.<span>add</span>(<span>tf</span>.<span>layers</span>.<span>simpleRNN</span>({<br/>        <span>units</span>: hiddenSize,<br/>        <span>recurrentInitializer</span>: <span>'glorotNormal'</span>,<br/>        <span>returnSequences</span>: <span>true<br/></span><span>    </span>}));<br/>    <span>model</span>.<span>add</span>(<span>tf</span>.<span>layers</span>.<span>timeDistributed</span>(<br/>        {<span>layer</span>: <span>tf</span>.<span>layers</span>.<span>dense</span>({<span>units</span>: vocabularySize})}));<br/>    <span>model</span>.<span>add</span>(<span>tf</span>.<span>layers</span>.<span>activation</span>({<span>activation</span>: <span>'softmax'</span>}));<br/>    <span>model</span>.<span>compile</span>({<br/>        <span>loss</span>: <span>'categoricalCrossentropy'</span>,<br/>        <span>optimizer</span>: <span>'adam'</span>,<br/>        <span>metrics</span>: [<span>'accuracy'</span>]<br/>    });<br/>    <span>return </span><span>model</span>;<br/>}</pre>
<p>This code builds a model with two recurrent layers, a time-distributed, fully connected layer and an output layer. The <kbd>vocabularySize </kbd>parameter represents the total number of unique characters involved, which are the numerals 0-9, the plus sign, and the space character. The <kbd>maxLen</kbd> parameter represents the maximum length an input string could be; for two-digit addition problems, <kbd>maxLen </kbd>will be five characters, because the plus sign must be included.</p>
<p>Of particular note in this example is the <kbd>timeDistributed</kbd> layer type. This is a layer wrapper in TensorFlow's API, meant to create a volume of neurons in the layer where each slice represents one slice of time. This is similar in spirit to the volumes used by CNNs in the previous example, where the depth of the volume represented an individual convolution operation. In this example, however, the depth of the volume represents a time slice.</p>
<p>The <kbd>timeDistributed </kbd>wrapper allows each time slice to be handled by an individual dense or fully connected layer, rather than attempting to interpret the time-dependent data with only a single vector of neurons, in which case the temporal data may be lost. The <kbd>timeDistributed </kbd>wrapper is required because the previous <em>simpleRNN</em> layer uses the <kbd>returnSequences: true</kbd> parameter, which causes the layer to output not only the current time step, but all time steps encountered in the layer's history.</p>
<p>Next, let's take a look at the GRU topology.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gated recurrent units</h1>
                </header>
            
            <article>
                
<p>The GRU topology comprises specialized, exotic neurons that use several internal mechanisms to control the memory and feedback of the neuron. The GRU is a recent invention, being developed only in 2014 as a simplified version of the LSTM neuron. While the GRU is newer than the LSTM, I present it first as it is slightly simpler.</p>
<p>In both GRU and LSTM neurons, the input signal is sent to multiple activation functions. Each internal activation function can be considered a standard ANN neuron; these internal neurons are combined in order to give the overall neuron its memory capabilities. From the outside, GRU and LSTM neurons both look like neurons capable of receiving time-dependent inputs. On the inside, these exotic neurons use simpler neurons to control how much of the feedback from a previous activation is attenuated or amplified, as well as how much of the current signal is stored into memory.</p>
<p>GRU and LSTM neurons have two major advantages over simple RNN neurons. First, the memories of these neurons do not decay over time like the echoes of a simple RNN neuron do. Second, the memory is configurable and self-learning, in the sense that the neuron can learn through training how important specific memories are to the current activation.</p>
<p>Consider the following illustration, also by François Deloche (own work, CC BY-SA 4.0):</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-310 image-border" src="assets/733ab961-704c-4714-993f-eed6d557c398.png" style="width:30.50em;height:14.25em;"/></div>
<p>The flowchart may be a little difficult to interpret at first. The <strong>Z<sub>t</sub></strong> signal is a vector that controls how much of the activation gets stored into memory and passed to future values, while the <strong>R<sub>t</sub></strong> signal controls how much of the prior values should be forgotten from memory. Each of these signals are attached to standard activation functions, which in turn have their own weights. In a sense, the GRU is itself a tiny neural network.</p>
<p>At this point, it might be tempting to ask why the memory of neurons can't simply be programmed, for example, with a key/value store that the neuron can make lookups against. The reason these architectures are used is due to the fact that the backpropagation algorithm requires mathematical differentiability. Even exotic topologies like RNNs are still trained using mathematical methods such as gradient descent, so the entire system must be mathematically representable. For this reason, researchers need to use the preceding techniques in order to create a network whose every component is mathematically analyzable and differentiable.</p>
<p>On the test page at <kbd>http://localhost:1234</kbd>, change the <em>RNN Type </em>parameter to GRU while keeping all other parameters the same, and click <strong>Train Model </strong>again. The graphs will update and you should see something like the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-227 image-border" src="assets/d85622d8-f6d0-4491-9cf4-a6cc44349762.png" style="width:48.25em;height:11.25em;"/></div>
<p>In this case, the training process has taken longer, but the accuracy has improved from 92% to 95% over the SimpleRNN type. The increased training time is not surprising, as the GRU architecture essentially triples the number of activation functions employed by the network.</p>
<p>While many factors affect the accuracy of the network, two obvious ones stand out. First, the GRU topology has long-term memory, unlike the SimpleRNN that will eventually forget previous values as their echoes decay. Second, the GRU has more precise control over how much of an activation signal is fed into future activations as well as how much of the information is retained. These parameters of the network are trained by the backpropagation algorithm, so the forgetfulness of neurons itself is optimized by the training.</p>
<p>Next, let's take a look at the topology that inspired the GRU and opened up entire new fields of research: the LSTM.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Long Short-Term Memory</h1>
                </header>
            
            <article>
                
<p>The LSTM was introduced in 1997 and made waves throughout the academic ANN community due to its impressive accuracy at solving historically difficult problems. In particular, the LSTM excelled at many natural language-processing tasks, handwriting recognition, and speech recognition. In many cases, LSTM networks beat the previous accuracy records by a wide margin. Many systems at the forefront of speech recognition and language modeling use LSTM networks. Most likely, systems such as Apple's Siri and Google's Assistant, use LSTM networks both in their speech recognition and language- parsing models.</p>
<p>The LSTM network gets its name due to the fact that it can retain short-term memory (for example, memory of a word used earlier in a sentence) for long periods of time. When training, this avoids a problem known as the <strong>disappearing gradient</strong>, which is what simple RNNs suffer from as the echoes of previous activations fade away.</p>
<p>Like the GRU, an LSTM neuron is an exotic neuron cell with sophisticated inner workings. Specifically, the LSTM neuron has three <em>gates</em> it uses internally: an <em>input gate</em>, which controls how much of a value is allowed to enter the neuron, a <em>forget gate</em>, which manages the memory of the neuron, and an <em>output gate</em>, which controls how much of a signal is allowed in the output of the neuron. The combination of gates, along with the fact that the neurons are all connected to each other, gives the LSTM very fine-grained control over which signals a neuron remembers and how they are used. Like the gates in a GRU, the gates in an LSTM can also be thought of as individual standard neurons, each with their own weights.</p>
<p>Consider the following graphic <span>by François Deloche (own work, CC BY-SA 4.0):</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-311 image-border" src="assets/f2dd99f3-47e2-47ce-bbcc-b84d6de2b23a.png" style="width:36.33em;height:16.92em;"/></div>
<p>The <strong>I<sub>t </sub></strong>signal controls the proportion of the input signal that is allowed into the cell. The <strong>O<sub>t</sub></strong> signal controls how much of the output is allowed out of the cell, and the <strong>F<sub>t </sub></strong>signal controls how much of the previous value is retained by the cell. Keep in mind that these are all vector quantities, so that the input, output, and memory can be controlled on a per-element basis.</p>
<p>The LSTM excels at tasks that require memory and knowledge of prior values, though the sophisticated inner workings of the cell (there are five distinct activation functions involved) lead to much longer training times. Returning to the test page in your browser, switch the <strong>RNN Type </strong>to LSTM and click <strong>Train Model:</strong></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-312 image-border" src="assets/2791cf8b-a8b2-4d58-a58b-50e8cb9fa041.png" style="width:58.08em;height:13.67em;"/></div>
<p>The LSTM has achieved an accuracy of nearly 98%, exceeding both the SimpleRNN and GRU RNN topologies. Of course, this network took longer to train than either of the others, due to the simple fact that there are more neurons (internal to the LSTM cells) that need to be trained.</p>
<p>There are many state-of-the-art uses for LSTM networks. They are very popular in audio analysis, such as speech recognition, as audio is heavily time-dependent. A single audio sample on its own is meaningless; it is only when many thousands of audio samples are taken together in context that an audio clip starts to make sense. An LSTM trained to recognize speech would first be trained to decode short audio clips (on the order of 0.1-0.25 seconds) into <em>phonemes</em>, or textual representations of phonetic sounds. Another LSTM layer would then be trained to connect sequences of phonemes together in order to determine the most likely phrase that was uttered. The first LSTM layer relies on time dependence in order to interpret the raw audio signal. The second LSTM layer relies on time dependence to bring context to natural language—for instance, using context and grammar to figure out whether the word <em>where</em> or <em>we're</em> was spoken.</p>
<p>Another state-of-the-art use case for LSTM is the CNN-LSTM. This network topology combines a CNN with an LSTM; a typical application would be action detection in a video clip. The CNN portion of the model analyzes individual video frames (as if they were independent images) to identify an object and its position or state. The LSTM portion of the model brings the individual frames together and generates a time-dependent context around them. Without an LSTM portion, a model would not be able to tell whether a baseball is stationary or in motion, for instance. It is the memory of the previous states of the object detected by the CNN that provides the context for determining the action occurring in the video. The CNN portion of the model identifies a baseball, and then the LSTM portion is what understands that the ball is moving and likely has been thrown or hit.</p>
<p>Another variation of the CNN-LSTM is used for the automated description of images. One can present a CNN-LSTM with an image of a woman standing on a pier by a lake. The CNN portion of the model individually identifies the woman, the pier, and the lake as objects in the image. The LSTM portion can then generate a natural language description of the image based on the information gathered by the CNN; it is the LSTM portion that grammatically compiles the description, <em>woman on a pier by a lake</em>. Remember that natural language descriptions are time-dependent, as the order of words matters.</p>
<p>One final note about LSTM networks relates to the <em>gates</em> used in the LSTM cell. While the input, forget, and output gates are usually standard activation neurons, it is also possible to use entire neural networks as gates themselves. In this manner, an LSTM can use <em>other</em> models as part of their knowledge and memory. A typical use case for this approach would be automated language translation. Individual LSTMs can be used to model the English and French languages, for instance, while an overall LSTM can manage translations between the two.</p>
<p>It is my personal belief that LSTM networks, or some variation thereof, such as the GRU topology, will be a key player in the road toward AGI. Having a robust memory is essentially a requirement when attempting to emulate general human intelligence, and LSTM fits the use case very nicely. These network topologies are at the forefront of ANN research, so expect to see major advances over the next couple of years.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed two advanced neural network topologies: the CNN and the RNN. We discussed the CNN in the context of image recognition, specifically the problem of handwritten digit identification. While exploring the CNN, we also discussed the convolution operation itself in the context of image filtering.</p>
<p>We also discussed how neural networks can be made to retain memory through the RNN architecture. We learned that RNNs have many applications, ranging from time-series analysis to natural language modeling. We discussed several RNN architecture types, such as the simple fully recurrent network and the GRU network. Finally, we discussed the state-of-the-art LSTM topology, and how it can be used for language modeling and other advanced problems, such as image captioning or video annotation.</p>
<p>In the next chapter, we'll take a look at some practical approaches to natural language processing, particularly the techniques that are most commonly used in conjunction with ML algorithms.</p>


            </article>

            
        </section>
    </body></html>