["```py\nIn:\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\n%matplotlib inline\n\nIn:\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets.samples_generator import make_regression\nimport numpy as np\n\ndef generate_dataset(n_train, n_test, n_features, noise=0.1):\n    X, y = make_regression(n_samples=int(n_train + n_test),\n                           n_features=int(n_features), noise=noise, \n                           random_state=101)\n\n    X_train = X[:n_train]\n    X_test = X[n_train:]\n\n    y_train = y[:n_train]\n    y_test = y[n_train:]\n\n    X_scaler = StandardScaler()\n    X_train = X_scaler.fit_transform(X_train)\n    X_test = X_scaler.transform(X_test)\n\n    y_scaler = StandardScaler()\n    y_train = y_scaler.fit_transform(y_train)\n    y_test = y_scaler.transform(y_test)\n\n    return X_train, X_test, y_train, y_test\n```", "```py\nIn:\nfrom sklearn.linear_model import LinearRegression, SGDRegressor\nimport time\n\nIn:\n\nn_test = 1000\n\nn_train_v = (1000, 10000, 100000)\nn_features_v = (10, 50, 100, 500, 1000)\nregr_v = {'LR': LinearRegression(), 'SGD': SGDRegressor(random_state=101)}\nresults = {}\n\nfor regr_name, regr in regr_v.items():\n\n    results[regr_name] = {}\n\n    for n_train in n_train_v:\n        for n_features in n_features_v:\n\n            results[regr_name][(n_train, n_features)] = {'train': [], 'pred': []}\n\n            for n_repetition in range(5):\n\n                X_train, X_test, y_train, y_test = \\\n                generate_dataset(n_train, n_test, n_features)\n\n                tick = time.time()\n                regr.fit(X_train, y_train)\n                train_time = time.time() - tick\n\n                pred = regr.predict(X_test)\n                predict_time = time.time() - tick - train_time\n\n                results[regr_name][(n_train, n_features)]['train'].append(train_time)\n                results[regr_name][(n_train, n_features)]['pred'].append(predict_time)\n```", "```py\nIn:\npylab.rcParams['figure.figsize'] = 12, 6\nplt.subplot(1, 2, 1)\n\nfor n_train in n_train_v:\n    X = n_features_v\n    y = [np.mean(results['LR'][(n_train, n_features)]['train']) \n         for n_features in n_features_v]\n    plt.plot(X, y, label=str(n_train) + \" train points\")\n\nplt.title('Training time VS num. features')\nplt.xlabel('Num features')\nplt.ylabel('Training time [s]')\nplt.legend(loc=0)\n\nplt.subplot(1, 2, 2)\n\nfor n_features in n_features_v:\n    X = np.log10(n_train_v)\n    y = [np.mean(results['LR'][(n_train, n_features)]['train']) \n         for n_train in n_train_v]\n    plt.plot(X, y, label=str(n_features) + \" features\")\n\nplt.title('Training time VS num. training points')\nplt.xlabel('Num training points [log10]')\nplt.ylabel('Training time [s]')\nplt.legend(loc=0)\nplt.show()\n\nOut:\n```", "```py\nIn:\nplt.subplot(1, 2, 1)\n\nfor n_train in n_train_v:\n    X = n_features_v\n    y = [np.mean(results['LR'][(n_train, n_features)]['pred']) \n         for n_features in n_features_v]\n\n    plt.plot(X, y, label=str(n_train) + \" train points\")\n\nplt.title('Prediction time VS num. features')\nplt.xlabel('Num features')\nplt.ylabel('Prediction time [s]')\nplt.legend(loc=0)\n\nplt.subplot(1, 2, 2)\n\nfor n_features in n_features_v:\n    X = np.log10(n_train_v)\n    y = [np.mean(results['LR'][(n_train, n_features)]['pred']) \n         for n_train in n_train_v]\n\n    plt.plot(X, y, label=str(n_features) + \" features\")\n\nplt.title('Prediction time VS num. training points')\nplt.xlabel('Num training points [log10]')\nplt.ylabel('Prediction time [s]')\nplt.legend(loc=0)\nplt.show()\nOut:\n```", "```py\nIn:\nplt.subplot(1, 2, 1)\n\nfor n_train in n_train_v:\n    X = n_features_v\n    y = [np.mean(results['SGD'][(n_train, n_features)]['train']) \n         for n_features in n_features_v]\n    plt.plot(X, y, label=str(n_train) + \" train points\")\n\nplt.title('Training time VS num. features')\nplt.xlabel('Num features')\nplt.ylabel('Training time [s]')\nplt.legend(loc=0)\n\nplt.subplot(1, 2, 2)\n\nfor n_features in n_features_v:\n    X = np.log10(n_train_v)\n    y = [np.mean(results['SGD'][(n_train, n_features)]['train']) \n         for n_train in n_train_v]\n    plt.plot(X, y, label=str(n_features) + \" features\")\n\nplt.title('Training time VS num. training points')\nplt.xlabel('Num training points [log10]')\nplt.ylabel('Training time [s]')\nplt.legend(loc=0)\nplt.show()\nOut:\n```", "```py\nIn:\nplt.subplot(1, 2, 1)\n\nfor n_train in n_train_v:\n    X = n_features_v\n    y = [np.mean(results['SGD'][(n_train, n_features)]['pred']) \n         for n_features in n_features_v]\n\n    plt.plot(X, y, label=str(n_train) + \" train points\")\n\nplt.title('Prediction time VS num. features')\nplt.xlabel('Num features')\nplt.ylabel('Prediction time [s]')\nplt.legend(loc=0)\n\nplt.subplot(1, 2, 2)\n\nfor n_features in n_features_v:\n    X = np.log10(n_train_v)\n    y = [np.mean(results['SGD'][(n_train, n_features)]['pred']) \n         for n_train in n_train_v]\n\n    plt.plot(X, y, label=str(n_features) + \" features\")\n\nplt.title('Prediction time VS num. training points')\nplt.xlabel('Num training points [log10]')\nplt.ylabel('Prediction time [s]')\nplt.legend(loc=0)\nplt.show()\nOut:\n```", "```py\nIn:\n# Let's generate a 1M dataset\nX_train, X_test, y_train, y_test = generate_dataset(2000000, 10000, 100, 10.0)\nprint(\"Size of X_train is [GB]:\", X_train.size * X_train[0,0].itemsize/1E9)\n\nOut:\nSize of X_train is [GB]: 1.6\n```", "```py\nIn:\nfrom sklearn.metrics import mean_absolute_error\n\nregr = SGDRegressor(random_state=101)\ntick = time.time()\nregr.fit(X_train, y_train)\nprint(\"With SGD, after\", time.time() - tick ,\"seconds\")\npred = regr.predict(X_test)\nprint(\"the MAE is [log10]:\", np.log10(mean_absolute_error(y_test, pred)))\n\nOut:\nWith SGD, after 5.958770098299116 seconds\nthe MAE is [log10]: -1.2422451189257\n```", "```py\nIn:\ndef get_minibatch(X, y, batch_size):\n    # We will shuffle consistently the training observations\n    from sklearn.utils import resample\n    X, y = resample(X, y, replace=False, random_state=101)\n    n_cols = y.shape[0]\n    for i in range(int(n_cols/batch_size)):\n        yield (X[i*batch_size:(i+1)*batch_size, :], y[i*batch_size:(i+1)*batch_size])\n\n    if n_cols % batch_size > 0:\n        res_rows = n_cols % batch_size\n        yield (X[-res_rows:, :], y[-res_rows:])\n\nplot_x = []\nplot_y = []\nplot_labels = []\n\nfor batch_size in (1000, 10000, 100000):\n    regr = SGDRegressor(random_state=101)\n    training_time = 0.0\n\n    X = []\n    y = []\n\n    for dataset in get_minibatch(X_train, y_train, batch_size):\n        tick = time.time()\n        regr.partial_fit(dataset[0], dataset[1])\n        training_time += (time.time() - tick)\n        pred = regr.predict(X_test)\n        X.append(training_time)\n        y.append(np.log10(mean_absolute_error(y_test, pred)))\n\n    print(\"Report: Mini-batch size\", batch_size)\n    print(\"First output after [s]:\", X[0])\n    print(\"First model MAE [log10]:\", y[0])\n    print(\"Total training time [s]:\", X[-1])\n    print(\"Final MAE [log10]: \", y[-1])\n    print()\n\n    plot_x.append(X)\n    plot_y.append(y)\n    plot_labels.append(\"Batch size: \"+str(batch_size))\n\nOut:\n```", "```py\nIn:\nplt.subplot(1,2,1)\nfor i in range(len(plot_x)):\n    plt.plot(plot_x[i], plot_y[i], label=plot_labels[i])\nplt.title('Mini-batch learning')\nplt.xlabel('Training time [s]')\nplt.ylabel('MAE')\nplt.legend(loc=0)\n\nplt.subplot(1,2,2)\nfor i in range(len(plot_x)):\n    plt.plot(plot_x[i], plot_y[i], label=plot_labels[i])\nplt.title('Mini-batch learning: ZOOM 0-0.15s')\nplt.xlabel('Training time [s]')\nplt.ylabel('MAE')\nplt.xlim([0, 0.15])\nplt.legend(loc=0)\n\nplt.show()\nOut:\n```", "```py\nIn:\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\nto_remove = ('headers', 'footers', 'quotes')\n\ndata_train = fetch_20newsgroups(subset='train', random_state=101,\n                                remove=to_remove)\n\ndata_test = fetch_20newsgroups(subset='test', random_state=101,\n                                remove=to_remove)\n\nlabels = data_train.target_names\ntargets = np.unique(data_train.target)\n```", "```py\nIn:\ndef get_minibatch_docs(docs, targets, batch_size):\n    n_docs = len(docs)\n    for i in range(int(n_docs/batch_size)):\n       yield (docs[i*batch_size:(i+1)*batch_size], \n              targets[i*batch_size:(i+1)*batch_size])\n\n    if n_docs % batch_size > 0:\n        res_rows = n_docs % batch_size\n        yield (docs[-res_rows:], targets[-res_rows:])\n```", "```py\nIn: \nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\nimport sys\n\nminibatch_size = 1000\nvalues_to_plot = {}\n\nfor hash_table_size in (1000, 5000, 10000, 50000, 100000):\n\n    values_to_plot[hash_table_size] = {'time': [], 'score': []}\n\n    vectorizer = HashingVectorizer(stop_words='english',\n   non_negative=True, n_features=hash_table_size,\n   ngram_range=(1, 1))\n\n    X_test = vectorizer.transform(data_test.data)\n    y_test = data_test.target\n\n    clf = SGDClassifier(loss='log')\n    timings = []\n\n    for minibatch in get_minibatch_docs(data_train.data, data_train.target, minibatch_size):\n        y_train = minibatch[1]\n\n        tick = time.time()\n        X_train = vectorizer.transform(minibatch[0])\n        clf.partial_fit(X_train, y_train, targets)\n\n        timings.append(time.time() - tick)\n\n        pred = clf.predict(X_test)\n\n        values_to_plot[hash_table_size]['score'].append(accuracy_score(y_test, pred))\n\n    values_to_plot[hash_table_size]['time'] = np.cumsum(timings)\n```", "```py\nIn:\nfor k,v in sorted(values_to_plot.items()):\n    plt.plot(v['time'], v['score'], 'x-', label='Hashsize size '+str(k))\nplt.title('Mini-batch learning: 20newsgroups')\nplt.xlabel('Training time [s]')\nplt.ylabel('Accuracy')\nplt.legend(loc=0)\n\nplt.show()\nOut:\n```"]