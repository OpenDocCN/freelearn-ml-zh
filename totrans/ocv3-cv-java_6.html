<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Detecting Foreground and Background Regions and Depth with a Kinect Device"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Detecting Foreground and Background Regions and Depth with a Kinect Device</h1></div></div></div><p>In the field of video security applications, one often needs to notice the differences between frames because that's where the action happens. In other fields, it is also very important to isolate the objects from the background. This chapter shows several techniques to achieve this goal, comparing their strengths and weaknesses. Another completely different approach for detecting foreground or background regions is using a depth device <a id="id199" class="indexterm"/>like a <span class="strong"><strong>Kinect</strong></span>. This chapter also deals with how to accomplish this goal with this device.</p><p>In this chapter, we will be covering:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Background subtraction</li><li class="listitem" style="list-style-type: disc">Frame differencing</li><li class="listitem" style="list-style-type: disc">Averaging background method</li><li class="listitem" style="list-style-type: disc">Mixture of Gaussian's method</li><li class="listitem" style="list-style-type: disc">Contour finding</li><li class="listitem" style="list-style-type: disc">Kinect depth maps</li></ul></div><p>By the end of this chapter, you will have several approaches solving the problem of finding foreground/background regions, either through direct image processing or using a depth-compatible device such as a Kinect.</p><div class="section" title="Background subtraction"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec46"/>Background subtraction</h1></div></div></div><p>When working with surveillance cameras, it's easy to see that most of the frame keeps still, while the moving objects, the ones we are interested in, are the areas that vary most over time. Background subtraction is defined as the approach used to detect moving objects from static cameras, also <a id="id200" class="indexterm"/>known as <span class="strong"><strong>foreground detection</strong></span>, since we're mostly<a id="id201" class="indexterm"/> interested in the foreground objects.</p><p>In order to perform some valuable background subtraction, it is important to account for varying luminance conditions, taking care always to update our background model. Although some techniques extend the idea of background subtraction beyond its literal meaning, such as the mixture of Gaussian approach, they are still named like this.</p><p>In order to compare all the solutions in the following sections, we will come up with a useful interface, which is <a id="id202" class="indexterm"/>called <span class="strong"><strong>VideoProcessor</strong></span>. This interface is made of a simple method called <a id="id203" class="indexterm"/>
<span class="strong"><strong>process</strong></span>. The whole interface is given in the following piece of code:</p><div class="informalexample"><pre class="programlisting">public interface VideoProcessor {
  public Mat process(Mat inputImage);
}</pre></div><p>Note that we will implement this interface in the following background processors so that we can easily change them and compare their results. In this context, <code class="literal">Mat inputImage</code> refers to the current frame in the video sequence being processed.</p><p>All the code related to background subtraction can be found in the <code class="literal">background</code> project, available in the <code class="literal">chapter6</code> reference code.</p><p>Our main application consists of two windows. One of them simply plays back the input video or the webcam stream, while the other one shows the output of applying a background subtractor that implements the <code class="literal">VideoProcessor</code> interface. This way, our main loop looks pretty much like the following code:</p><div class="informalexample"><pre class="programlisting">while (true){  
  capture.read(currentImage);  
  if( !currentImage.empty() ){
    foregroundImage = videoProcessor.process(currentImage);
    ... update Graphical User Interfaces ...
    Thread.sleep(10);
  }
}</pre></div><p>Note that upon successful image retrieval, we pass it to our <code class="literal">VideoProcessor</code> and update our windows. We also sleep for 10 ms so that the video playback will not look like a fast <a id="id204" class="indexterm"/>forward. This 10 ms delay is not the recorded frame delay and it is used because the focus here is not to play back at the same speed as the original file. In order to try the different subtraction approaches, we simply change the instantiation of our <code class="literal">VideoProcessor</code> class.</p></div></div>
<div class="section" title="Frame differencing"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec47"/>Frame differencing</h1></div></div></div><p>It should be straightforward to think of a simple background subtraction in order to retrieve<a id="id205" class="indexterm"/> foreground objects. A simple solution could look similar to the following line of code:</p><div class="informalexample"><pre class="programlisting">Core.absdiff(backgroundImage,inputImage , foregroundImage);</pre></div><p>This function simply subtracts each pixel of <code class="literal">backgroundImage</code> from <code class="literal">inputImage</code> and writes its absolute value in <code class="literal">foregroundImage</code>. As long as we have initialized the background to <code class="literal">backgroundImage</code> and we have that clear from objects, this could work as a simple solution.</p><p>Here follows the background subtraction video processor code:</p><div class="informalexample"><pre class="programlisting">public class AbsDifferenceBackground implements VideoProcessor {
  private Mat backgroundImage;

  public AbsDifferenceBackground(Mat backgroundImage) {
    this.backgroundImage = backgroundImage;
  }

  public Mat process(Mat inputImage) {
    Mat foregroundImage = new Mat();
    Core.absdiff(backgroundImage,inputImage , foregroundImage);
    return foregroundImage;
  }

}</pre></div><p>The main method, <code class="literal">process</code>, is really simple. It only applies the absolute difference method. The only detail to remember is to initialize the background image in the constructor, which should correspond to the whole background being free from the foreground objects.</p><p>We can see the output of applying ordinary background subtraction in the following image; it is important to check that the moving leaves in the background are not correctly <a id="id206" class="indexterm"/>removed since this is a weak background modeling. Also, remember to move the <span class="strong"><strong>Video Playback Example</strong></span> window as it might be covering the <span class="strong"><strong>Background Removal Example</strong></span> window:</p><div class="mediaobject"><img src="graphics/3972OS_06_01.jpg" alt="Frame differencing"/></div></div>
<div class="section" title="Averaging a background method"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec48"/>Averaging a background method</h1></div></div></div><p>The problem with the background subtractor from the previous section is that the background <a id="id207" class="indexterm"/>will generally change due to illumination and other effects. Another fact is that the background may not be readily available, or the concept of background can change, for instance, when someone leaves a luggage in a video surveillance application. The luggage might be a foreground object for the first frames, but afterwards, it should be forgotten.</p><p>An interesting algorithm to deal with these problems uses the running average concept. Instead of always using the first frame as a clear background, it will update it constantly by calculating a moving average of it. Consider the following equation, which will be executed, updating each pixel from the old average and considering each pixel from the recently acquired image:</p><div class="mediaobject"><img src="graphics/3972OS_06_11.jpg" alt="Averaging a background method"/></div><p>Note that <span class="inlinemediaobject"><img src="graphics/3972OS_06_12.jpg" alt="Averaging a background method"/></span> is the new pixel value; <span class="inlinemediaobject"><img src="graphics/3972OS_06_13.jpg" alt="Averaging a background method"/></span> is the value of the average background at time <code class="literal">t-1</code>, which would be the last frame; <span class="inlinemediaobject"><img src="graphics/3972OS_06_14.jpg" alt="Averaging a background method"/></span> is the new value for the background; and <span class="inlinemediaobject"><img src="graphics/3972OS_06_15.jpg" alt="Averaging a background method"/></span> is the learning rate.</p><p>Fortunately, OpenCV already has the <code class="literal">accumulateWeighted</code> function, which performs the last <a id="id208" class="indexterm"/>equation for us. Now let's see how the average background process is implemented in the <code class="literal">RunningAverageBackground</code> class as we check its <code class="literal">process</code> method as follows:</p><div class="informalexample"><pre class="programlisting">public Mat process(Mat inputImage) {
  Mat foregroundThresh = new Mat();
  // Firstly, convert to gray-level image, yields good results with performance
  Imgproc.cvtColor(inputImage, inputGray, Imgproc.COLOR_BGR2GRAY);
  // initialize background to 1st frame, convert to floating type
  if (accumulatedBackground.empty())
    inputGray.convertTo(accumulatedBackground, CvType.CV_32F);
  
  // convert background to 8U, for differencing with input image
  accumulatedBackground.convertTo(backImage,CvType.CV_8U);
  // compute difference between image and background
  Core.absdiff(backImage,inputGray,foreground);
  
  // apply threshold to foreground image
  Imgproc.threshold(foreground,foregroundThresh, threshold,255, Imgproc.THRESH_BINARY_INV);
  
  // accumulate background
  Mat inputFloating = new Mat();
  inputGray.convertTo(inputFloating, CvType.CV_32F);
  Imgproc.accumulateWeighted(inputFloating, accumulatedBackground,learningRate, foregroundThresh);
  
  return negative(foregroundThresh);
}

private Mat negative(Mat foregroundThresh) {
  Mat result = new Mat();
  Mat white = foregroundThresh.clone();
  white.setTo(new Scalar(255.0));
  Core.subtract(white, foregroundThresh,  result);
  return result;
}</pre></div><p>First, we convert the input image to gray level since we will store the average background like this, although we could make it with three channels. Then, if the accumulated background hasn't been started, we will have to set it to the first input image in the floating point format. Then we subtract the recently acquired frame from the accumulated background, which yields our foreground image, which we later threshold in order to remove small illumination or noisy changes.</p><p>Note that this time <a id="id209" class="indexterm"/>we use <code class="literal">Imgproc.THRESH_BINARY_INV</code>, which turns every pixel above the given threshold black, yielding black pixels for the foreground objects and white pixels for the background.</p><p>This way, we can use this image as a mask for updating only background pixels when using the <code class="literal">acccumulateWeighted</code> method later. On the following line, we only convert <code class="literal">inputImage</code> to <code class="literal">inputFloating</code> so that we can have it in the floating point format. We then use <code class="literal">accumulateWeighted</code> to apply our commented equation for the running average. Finally, we invert the image and return our foreground objects as white pixels.</p><p>We can see a better modeling of the moving leaves on the background in the following image. Although thresholding makes it harder to compare these results with simple background subtraction, it is clear that lots of moving leaves have been removed. Besides, a good part of the hand has also been swept away. A careful tuning of the threshold parameter can be used for better results as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/3972OS_06_02.jpg" alt="Averaging a background method"/></div></div>
<div class="section" title="The mixture of Gaussians method"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec49"/>The mixture of Gaussians method</h1></div></div></div><p>Although we can get very good results with the previous idea, some more advanced methods have been proposed in literature. A great approach, proposed by Grimson in 1999, is to use not just one running average, but more averages so that if a pixel fluctuates between the two orbit points, these two running averages are calculated. If it does not fit any of them, it is considered foreground.</p><p>Besides, Grimson's approach also keeps the variance of the pixels, which is a measure of how far a set of numbers is spread out, taken from statistics. With a mean and a variance, a<a id="id210" class="indexterm"/> Gaussian model can be calculated and a probability can be measured to be taken into consideration, yielding a <span class="strong"><strong>Mixture of Gaussians model</strong></span> (<span class="strong"><strong>MOG</strong></span>). This can be very useful when branches and leaves are moving in the background.</p><p>Unfortunately, Grimson's method suffers from slow learning in the beginning and it can not distinguish between the moving shadows and moving objects. Therefore, an improved technique has been published by KaewTraKulPong and Bowden to tackle these problems. This one is implemented in OpenCV and it is quite straightforward to use it by means of the <code class="literal">BackgroundSubtractorMOG2</code> class.</p><p>In order to show how effective is the mixture of Gaussians approach, we have implemented a <code class="literal">BackgroundSubtractorMOG2</code>-based <code class="literal">VideoProcessor</code>. Its entire code is as follows:</p><div class="informalexample"><pre class="programlisting">public class MixtureOfGaussianBackground implements VideoProcessor {
  privateBackgroundSubtractorMOG2 mog=  org.opencv.video.Video. createBackgroundSubtractorMOG2();
  private Mat foreground = new Mat();
  private double learningRate = 0.01;

  public Mat process(Mat inputImage) {
    mog.apply(inputImage, foreground, learningRate);
    return foreground;
  }
}</pre></div><p>Note that we only need to instantiate the <code class="literal">BackgroundSubtractorMOG2</code> class and use the <code class="literal">apply</code> method, passing the input frame, the output image, and a learning rate that will tell how fast it should learn the new background. Besides the factory method without parameters, another one exists with the following signature:</p><div class="informalexample"><pre class="programlisting">Video.createBackgroundSubtractorMOG2 (int history, double varThreshold, boolean detectShadows)</pre></div><p>Here, <code class="literal">history</code> is the length of the <a id="id211" class="indexterm"/>history, <code class="literal">varThreshold</code> is the threshold on the squared Mahalanobis distance between the pixel and the model to decide whether a pixel is well described by the background model, and if <code class="literal">detectShadows</code> is <code class="literal">true</code>, the algorithm will detect and mark the shadows. If we do not set parameters by using the empty constructor, the following values are used by default:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">defaultHistory = 500;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">varThreshold = 16;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">detectShadows = true;</code></li></ul></div><p>Try playing with these values in order to look for better results when making background subtraction.</p><div class="mediaobject"><img src="graphics/3972OS_06_03.jpg" alt="The mixture of Gaussians method"/></div><p>In the preceding screenshot, we can clearly see a great background removal result with very little customization. Although some leaves still account for noise in the removed background result, we can see a good amount of the hand being correctly identified as foreground. A simple open morphological operator can be applied to remove some of the noise, as seen in the following screenshot:</p><div class="mediaobject"><img src="graphics/3972OS_06_04.jpg" alt="The mixture of Gaussians method"/></div></div>
<div class="section" title="Contour finding"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec50"/>Contour finding</h1></div></div></div><p>When dealing with the binary images removed from the background, it is important to<a id="id212" class="indexterm"/> transform pixels into useful information, such as by grouping them into an object or making it very clear for the user to see. In this context, it is important to know the concept of connected components, which are a set of connected pixels in a binary image, and OpenCV's function used to find its contours.</p><p>In this section, we will examine the <code class="literal">findContours</code> function, which extracts contours of connected components in an image as well as a helper function that will draw contours in an image, which is <code class="literal">drawContours</code>. The <code class="literal">findContours</code> function is generally applied over an image that has gone through a threshold procedure as well as some canny image transformation. In our example, a threshold is used.</p><p>The <code class="literal">findContours</code> function has the following signature:</p><div class="informalexample"><pre class="programlisting">public static void findContours(Mat image,
                java.util.List&lt;MatOfPoint&gt; contours,
                Mat hierarchy,
                int mode,
                int method)</pre></div><p>It is implemented using Suzuki's algorithm described in his paper <span class="emphasis"><em>Topological structural analysis of digitized binary images by border following</em></span>. The first parameter is the input image. Make sure you work on a copy of your target image since this function alters the image. Also, beware that the 1 pixel border of the image is not considered. The contours that are found are stored in the list of <code class="literal">MatOfPoints</code>. This is simply a structure that stores points in a matrix.</p><p><code class="literal">Mat hierarchy</code> is an optional output vector that is set for each contour found. They represent 0-based indices of the next and previous contours at the same hierarchical level, the first child contour, and the parent contour, represented in the <code class="literal">hierarcy[i][0]</code>, <code class="literal">hierarcy[i][1]</code>, <code class="literal">hierarcy[i][2]</code>, and <code class="literal">hierarcy[i][3]</code> elements, respectively for a given <code class="literal">i</code> contour. If there aren't contours corresponding to those values, they will be negative.</p><p>The <code class="literal">mode</code> parameter deals with how the hierarchical relationships are established. If this is not interesting to you, you can set it as <code class="literal">Imgproc.RETR_LIST</code>. When retrieving the <a id="id213" class="indexterm"/>contours, the <code class="literal">method</code> parameter controls how they are approximated. If <code class="literal">Imgproc.CHAIN_APPROX_NONE</code> is set, all the contour points are stored. On the other hand, when using <code class="literal">Imgproc.CHAIN_APPROX_SIMPLE</code> for this value, horizontal, vertical, and diagonal lines are compressed by using only their endpoints. Other approximations are available as well.</p><p>In order to draw the obtained contours outline or fill them, Imgproc's <code class="literal">drawContours</code> is used. This function has the following signature:</p><div class="informalexample"><pre class="programlisting">public static void drawContours(Mat image,
                java.util.List&lt;MatOfPoint&gt; contours,
                int contourIdx,
                Scalar color)</pre></div><p><code class="literal">Mat image</code> is simply the destination image, while the list of <code class="literal">MatOfPoint</code> contours is the one obtained while calling <code class="literal">findContours</code>. The <code class="literal">contourIdx</code> property is the one intended to be drawn, while <code class="literal">color</code> is the desired color for drawing. Overloaded functions are also available in which the user can choose the thickness, line type, hierarchy max level, and an offset.</p><p>When deciding on which contours are interesting, a useful function to help in that decision is to find the contour area. OpenCV implements this function through <code class="literal">Imgproc.contourArea</code>. This function can be found in the <code class="literal">chapter6</code> source code's sample <code class="literal">connected</code> project. This application takes an image as input, runs a threshold over it and then uses it for finding the contours. Several options are available for testing the functions discussed in this section, such as whether it is filling the contour or painting the contour according to the area found. The following is a screenshot of this application:</p><div class="mediaobject"><img src="graphics/3972OS_06_05.jpg" alt="Contour finding"/></div><p>When dealing <a id="id214" class="indexterm"/>with contours, it is also important to draw shapes around them in order to make measures or highlight what is found. The sample application also offers some code with instructions on how to draw a bounding box, circle, or convex hull around the contour. Let's take a look at the main <code class="literal">drawContours()</code> function, which is called upon pressing the button:</p><div class="informalexample"><pre class="programlisting">protected void drawContours() {
  Mat contourMat = binary.clone();
  List&lt;MatOfPoint&gt; contours = new ArrayList&lt;MatOfPoint&gt;();
  int thickness = (fillFlag.equals(onFillString))?-1:2;
  
  Imgproc.findContours(contourMat, contours, new Mat(),
  Imgproc.CHAIN_APPROX_NONE,Imgproc.CHAIN_APPROX_SIMPLE);
  for(int i=0;i&lt;contours.size();i++){
    MatOfPoint currentContour = contours.get(i);
    double currentArea = Imgproc.contourArea(currentContour);
    
    if(currentArea &gt; areaThreshold){
      Imgproc.drawContours(image, contours, i, new Scalar(0,255,0), thickness);
      if(boundingBoxString.equals(enclosingType)){
        drawBoundingBox(currentContour);
      }
      else if (circleString.equals(enclosingType)){
      drawEnclosingCircle(currentContour);
      }
      else if (convexHullString.equals(enclosingType)){
        drawConvexHull(currentContour);
      }
    }
    else{
      Imgproc.drawContours(image, contours, i, new Scalar(0,0,255), thickness);
    }
  }
  updateView();
}</pre></div><p>We firstly clone our <a id="id215" class="indexterm"/>target binary image, so we won't change it. Then, we initialize the <code class="literal">MatOfPoint</code> structure and define the thickness flag. We then run <code class="literal">findContours</code>, ignoring the output hierarchy matrix. It is time to iterate the contours in the <code class="literal">for</code> loop. We use the <code class="literal">Imgproc.contourArea</code> helper function for an area estimate. Based on that, if it is the previous <code class="literal">areaThreshold</code> defined by the slider, it is drawn as green using the <code class="literal">drawContours</code> function or else it is drawn as red. An interesting part of the code are the shape drawing functions, which are described as follows:</p><div class="informalexample"><pre class="programlisting">private void drawBoundingBox(MatOfPoint currentContour) {
  Rect rectangle = Imgproc.boundingRect(currentContour);
  Imgproc.rectangle(image, rectangle.tl(), rectangle.br(), new Scalar(255,0,0),1);
}

private void drawEnclosingCircle(MatOfPoint currentContour) {
  float[] radius = new float[1];
  Point center = new Point();
  
  MatOfPoint2f currentContour2f = new MatOfPoint2f();
  currentContour.convertTo(currentContour2f, CvType.CV_32FC2);
  Imgproc.minEnclosingCircle(currentContour2f, center, radius);
  Imgproc.circle(image, center, (int) radius[0], new Scalar(255,0,0));
}

private void drawConvexHull(MatOfPoint currentContour) {
  MatOfInt hull = new MatOfInt();
  Imgproc.convexHull(currentContour, hull);
  
  List&lt;MatOfPoint&gt; hullContours = new ArrayList&lt;MatOfPoint&gt;();
  MatOfPoint hullMat = new MatOfPoint();
  hullMat.create((int)hull.size().height,1,CvType.CV_32SC2);
  
  for(int j = 0; j &lt; hull.size().height ; j++){
    int index = (int)hull.get(j, 0)[0];
    double[] point = new double[] {
      currentContour.get(index, 0)[0], currentContour.get(index, 0)[1]
    };
    hullMat.put(j, 0, point);
  }
  hullContours.add(hullMat);
  Imgproc.drawContours(image, hullContours, 0, new Scalar(128,0,0), 2);
}</pre></div><p>Drawing a bounding box is simple; it is just a matter of calling <code class="literal">Imgproc.boundingRect()</code> in order to identify the shape's surrounding rectangle. Then, the Imgproc's <code class="literal">rectangle</code> function method is called to draw the rectangle itself.</p><p>Drawing the<a id="id216" class="indexterm"/> enclosing circle is also easy due to the existence of the <code class="literal">minEnclosingCircle</code> function. The only caveat is converting <code class="literal">MatOfPoint</code> to <code class="literal">MatOfPoint2f</code>, which is accomplished by calling Contour's <code class="literal">convertTo</code>. The Imgproc's <code class="literal">circle</code> function deals with drawing it.</p><p>Finding the convex hull is a rather important problem from  a computational geometry perspective. It can be seen as putting an elastic band around a set of points and checking the final shape it takes. Fortunately, OpenCV also deals with this problem through the Imgproc's <code class="literal">convexHull</code> function. Note that in the first and the second line of <code class="literal">drawConvexHull</code> in the preceding code, <code class="literal">MatOfInt</code> is created, and <code class="literal">convexHull</code> is called, passing the current contour and this matrix as parameters. This function will return convex hull indexes in <code class="literal">MatOfInt</code>. We can draw lines ourselves, based on the coordinates of these indexes from the original contour. Another idea is to use the OpenCV's <code class="literal">drawContour</code> function. In order to do this, you need to build a new contour. This is done in the following lines in the code until <code class="literal">drawContour</code> is called.</p></div>
<div class="section" title="Kinect depth maps"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec51"/>Kinect depth maps</h1></div></div></div><p>From the beginning of this chapter until now, we have focused on the background subtraction <a id="id217" class="indexterm"/>approaches that try to model the background of the scene using ordinary cameras and then on applying frame differencing.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>Although the Kinect is reported to work with Linux and OSX, this section deals only with Windows setup on OpenCV 2.4.7 version.</p></div></div><p>In this section, we will take a different approach. We will set how far we want our objects to be considered foreground and background, which means removing the background by selecting a depth parameter. Unfortunately, this can not be done using a single ordinary camera in a single shot, so we will need a sensor that tells us the depth of objects or try to determine depth from stereo, which is not in the scope of this chapter. Thanks to both gamers and several efforts from all around the world, this device has become a commodity and it is called a <span class="strong"><strong>Kinect</strong></span>. Some attempts can be made to use two cameras and try to get depth from stereo, but the results might not be as great as the ones with the Kinect sensor. Here is how it looks:</p><div class="mediaobject"><img src="graphics/3972OS_06_06.jpg" alt="Kinect depth maps"/></div><p>What makes the Kinect really different from an average camera is that it includes an infrared emitter and an infrared sensor that are able to project and sense a structured light pattern. It also <a id="id218" class="indexterm"/>contains an ordinary VGA camera so that the depth data can be merged into it. The idea behind the structured light is that when projecting a known pattern of pixels on to the objects, the deformation of this pattern allows the computer vision systems to calculate the depth and surface information from them. If a camera capable of registering infrared is used to record the emitted Kinect pattern, an image similar to the following can be seen:</p><div class="mediaobject"><img src="graphics/3972OS_06_07.jpg" alt="Kinect depth maps"/></div><p>Although it might look like a random set of points, they are actually pseudo-random patterns that have been previously generated. These patterns can be identified and a disparity to depth relationship can be calculated, inferring the depth. More information can be acquired when studying structured light concepts if it is required.</p><p>One should be aware of the implications this method has. As it relies on active infrared projection, some outdoor effects, such as direct sunlight will confuse the sensors, so outdoor use is not recommended. Users should also be aware that the depth range is from 0.8 meters to 4.0 meters (roughly from 2.6 feet to 13.1 feet). Some shadows related to the IR projection can also make the results not look as great as they should, and cause some noise in the images. Despite all these issues, it is one of the best results available for the near field background removal.</p><div class="section" title="The Kinect setup"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec15"/>The Kinect setup</h2></div></div></div><p>Using a Kinect should be straightforward, but we need to consider two important aspects. First we<a id="id219" class="indexterm"/> need to be sure that all the device driver softwares are correctly installed for using them. Then we need to check whether OpenCV has been compiled with Kinect support. Unfortunately, if you have downloaded precompiled binaries of version 2.4.7 from <a class="ulink" href="http://sourceforge.net/projects/opencvlibrary/files/">http://sourceforge.net/projects/opencvlibrary/files/</a>, as described in the beginning of <a class="link" href="ch01.html" title="Chapter 1. Setting Up OpenCV for Java">Chapter 1</a>, <span class="emphasis"><em>Setting Up OpenCV for Java</em></span> the out-of-the-box support is not included. We will briefly describe the setup instructions in the upcoming sections.</p><p>It is important to note that not only the Xbox 360 Kinect device is commercialized, but also the Kinect for Windows. Currently, if you are creating commercial applications with the Kinect, you should go with the Kinect for Windows, although the Xbox 360 Kinect works with the provided drivers.</p><div class="section" title="The driver setup"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec01"/>The driver setup</h3></div></div></div><p>OpenCV Kinect support relies on OpenNI and PrimeSensor Module for OpenNI. An OpenNI framework is <a id="id220" class="indexterm"/>an open source SDK used for the development of 3D sensing middleware libraries and applications. Unfortunately, <a class="ulink" href="http://OpenNI.org">OpenNI.org</a> site was available only until April 23rd, 2014, but the OpenNI source code is available on Github at <a class="ulink" href="https://github.com/OpenNI/OpenNI">https://github.com/OpenNI/OpenNI</a> and <a class="ulink" href="https://github.com/OpenNI/OpenNI2">https://github.com/OpenNI/OpenNI2</a>. We will focus on using version 1.5.7.10 in this section.</p><p>Although instructions for <a id="id221" class="indexterm"/>building the binaries are readily available, we can use installers provided in the code repository of this book.</p><p>After installing the OpenNI library, we will need to install the Kinect drivers. These are available<a id="id222" class="indexterm"/> at <a class="ulink" href="https://github.com/avin2/SensorKinect/">https://github.com/avin2/SensorKinect/</a>, and installers are specifically at <a class="ulink" href="https://github.com/avin2/SensorKinect/tree/unstable/Bin">https://github.com/avin2/SensorKinect/tree/unstable/Bin</a>.</p><p>When plugging your Xbox 360 Kinect device into Windows, you should see the following screenshot in your Device Manager:</p><div class="mediaobject"><img src="graphics/3972OS_06_08.jpg" alt="The driver setup"/></div><p>Make sure all <a id="id223" class="indexterm"/>of the three Kinect devices—<span class="strong"><strong>Audio</strong></span>, <span class="strong"><strong>Camera</strong></span>, and <span class="strong"><strong>Motor</strong></span>—show appropriately.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>One caveat that can happen is that if users forget to plug the power supply for the XBox 360 Kinect device, only <a id="id224" class="indexterm"/>
<span class="strong"><strong>Kinect Motor</strong></span> might show up since there isn't enough energy for the all three of them. Also, you won't be able to retrieve frames in your OpenCV application. Remember to plugin your power supply, and you should be fine.</p></div></div></div><div class="section" title="The OpenCV Kinect support"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec02"/>The OpenCV Kinect support</h3></div></div></div><p>After ensuring that the OpenNI and Kinect drivers have been correctly installed, you need to check<a id="id225" class="indexterm"/> for the OpenCV Kinect support. Fortunately, OpenCV offers quite a useful function to <a id="id226" class="indexterm"/>check that. It is called <code class="literal">Core.getBuildInformation()</code>. This function shows important information about which options have been enabled during the OpenCV compilation. In order to check for Kinect support, simply output the result of calling this function to the console by using <code class="literal">System.out.println(Core.getBuildInformation());</code> and look for the video I/O section which looks like the following:</p><div class="mediaobject"><img src="graphics/3972OS_06_output.jpg" alt="The OpenCV Kinect support"/></div><p>It means OpenNI and Kinect support has not been enabled.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Now, according to <a class="link" href="ch01.html" title="Chapter 1. Setting Up OpenCV for Java">Chapter 1</a>, <span class="emphasis"><em>Setting Up OpenCV for Java</em></span>, instead of typing:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cmake -DBUILD_SHARED_LIBS=OFF ..</strong></span>
</pre></div><p>Remember to add the <code class="literal">WITH_OPENNI</code> flag, as given in the following line of code:</p><div class="informalexample"><pre class="programlisting">cmake -DBUILD_SHARED_LIBS=OFF .. -D WITH_OPENNI</pre></div><p>Instead of the preceding code, make sure you tick this option when using CMake's GUI. Check<a id="id227" class="indexterm"/> for an <a id="id228" class="indexterm"/>output similar to the following screenshot:</p><div class="mediaobject"><img src="graphics/3972OS_06_09.jpg" alt="The OpenCV Kinect support"/></div><p>Make sure you point the OPENNI paths to your OpenNI correct installation folder. Rebuild the library, and now your <code class="literal">opencv_java247.dll</code> will be built with Kinect support.</p></li><li class="listitem">Now try checking your <code class="literal">Core.getBuildInformation()</code> again. The availability of OpenNI will be demonstrated in your Java console, as given in the following lines:<div class="informalexample"><pre class="programlisting">  Video I/O:
    Video for Windows:           YES
    DC1394 1.x:                  NO
    DC1394 2.x:                  NO
    FFMPEG:                      YES (prebuilt binaries)
      codec:                     YES (ver 55.18.102)
      format:                    YES (ver 55.12.100)
      util:                      YES (ver 52.38.100)
      swscale:                   YES (ver 2.3.100)
      gentoo-style:              YES
<span class="strong"><strong>    OpenNI:                      YES (ver 1.5.7, build 10)</strong></span>
<span class="strong"><strong>    OpenNI PrimeSensor Modules:  YES (C:/Program Files (x86)/PrimeSense/Sensor/Bin/XnCore.dll)</strong></span>
    PvAPI:                       NO
    GigEVisionSDK:               NO
    DirectShow:                  YES
    Media Foundation:            NO
    XIMEA:                       NO</pre></div></li></ol></div><p>An alternative <a id="id229" class="indexterm"/>approach is <a id="id230" class="indexterm"/>using our configured Maven repository. We have added a runtime dependency to the book Maven repository, only available for Windows x86, which is very easy to configure. Simply follow the Java OpenCV Maven configuration section from <a class="link" href="ch01.html" title="Chapter 1. Setting Up OpenCV for Java">Chapter 1</a>, <span class="emphasis"><em>Setting Up OpenCV for Java</em></span>, and then, instead of adding the ordinary OpenCV dependency, <code class="literal">opencvjar-runtime</code>, use the following dependency:</p><div class="informalexample"><pre class="programlisting">&lt;dependency&gt;
  &lt;groupId&gt;org.javaopencvbook&lt;/groupId&gt;
  &lt;artifactId&gt;opencvjar-kinect-runtime&lt;/artifactId&gt;
  &lt;version&gt;2.4.7&lt;/version&gt;
  &lt;classifier&gt;natives-windows-x86&lt;/classifier&gt;
&lt;/dependency&gt;</pre></div><p>The complete POM file can be accessed in this chapter's Kinect project source code.</p><p>Be sure you check for some caveats, such as not mixing 32 bit and 64 bit drivers and libraries as well as Java runtime. If this is the case, you might receive <span class="strong"><strong>Can't load IA 32-bit .dll on a AMD 64-bit platform</strong></span>, for instance. Another source of problems is forgetting to plugin the power supply for Kinect XBox 360, which will cause it to load only Kinect Motor.</p><p>Now that we are sure that the OpenNI and Kinect Drivers have been correctly installed as well as <a id="id231" class="indexterm"/>the OpenCV's <a id="id232" class="indexterm"/>OpenNI support, we are ready to move on to the next section.</p></div></div><div class="section" title="The Kinect depth application"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec16"/>The Kinect depth application</h2></div></div></div><p>The application focuses<a id="id233" class="indexterm"/> on the depth-sensing information from the Kinect as well as on the OpenCV API for OpenNI depth sensor, which means it won't cover some well-known Kinect features such as skeletal tracking (which puts nodes in important body parts like head, heap center, shoulder, wrists, hands, knees, feet, and others), gesture tracking, microphone recording, or tilting the device. Although we will just cover depth sensing, it is one of the most fantastic features of the Kinect.</p><p>The basic idea behind this application is to segment an image from its depth information and combine it with a background image. We will capture an RGB frame from the Kinect device and retrieve its depth map. From a slider, you can choose how much depth you want for the segmentation. Based on that, a mask is generated through simple thresholding. The combined RGB frame and depth are now used to overlay a background image, resulting in an effect similar to chroma key compositing, but without the need for a green screen background, of course. This process can be seen in the following screenshot:</p><div class="mediaobject"><img src="graphics/3972OS_06_10.jpg" alt="The Kinect depth application"/></div><p>We should notice that in OpenCV version 2.4.7, the Java API does not support the Kinect depth sensing, but this is built on top of <code class="literal">VideoCapture</code>, so just some minor modifications related to constants will be required. For the sake of simplicity, these constants are in the main <code class="literal">App</code> class, but they should be refactored to a class that only deals with the OpenNI constants. Please look for the project <code class="literal">kinect</code> from this chapter in order to check for source code.</p><p>In order to work with depth-sensing images, we will need to follow these simple guidelines:</p><div class="informalexample"><pre class="programlisting">VideoCapture capture = new VideoCapture(CV_CAP_OPENNI);
capture.grab();
capture.retrieve( depthMap,  CV_CAP_OPENNI_DEPTH_MAP);
capture.retrieve(colorImage, CV_CAP_OPENNI_BGR_IMAGE);</pre></div><p>We will use the <a id="id234" class="indexterm"/>same <code class="literal">VideoCapture</code> class as the one used in <a class="link" href="ch02.html" title="Chapter 2. Handling Matrices, Files, Cameras, and GUIs">Chapter 2</a>, <span class="emphasis"><em>Handling Matrices, Files, Cameras, and GUIs</em></span>, for webcam input, with the same interface, passing the constant <code class="literal">CV_CAP_OPENNI</code> for telling it to retrieve frames from the Kinect. The difference here is that instead of using the <code class="literal">read</code> method,we will break this step in grabbing the frame and then retrieving either the depth image or the captured frame. Note that this is done by firstly calling the <code class="literal">grab</code> method and then the <code class="literal">retrieve</code> method, passing <code class="literal">CV_CAP_OPENNI_DEPTH_MAP</code> and <code class="literal">CV_CAP_OPENNI_BGR_IMAGE</code> as parameters. Make sure you send it to different matrices. Note that all these constants are extracted from the <code class="literal">highgui_c.h</code> file, which is located in the <code class="literal">opencv\modules\highgui\include\opencv2\highgui</code> path from OpenCV's source code tree. We will only work with the disparity map and RGB images from the Kinect, but one can also use the <code class="literal">CV_CAP_OPENNI_DEPTH_MAP</code> constant for receiving the depth values in mm as a <code class="literal">CV_16UC1</code> matrix, or <code class="literal">CV_CAP_OPENNI_POINT_CLOUD_MAP</code> for a point cloud map in a <code class="literal">CV_32FC3</code> matrix in which the values are XYZ coordinates in meters.</p><p>Our main loop consists of the following code:</p><div class="informalexample"><pre class="programlisting">while(true){
  capture.grab();
  capture.retrieve( depthMap, CV_CAP_OPENNI_DISPARITY_MAP);
  disparityImage = depthMap.clone();
  capture.retrieve(colorImage, CV_CAP_OPENNI_BGR_IMAGE);
  workingBackground = resizedBackground.clone();
  Imgproc.threshold(disparityImage, disparityThreshold, gui.getLevel(), 255.0f, Imgproc.THRESH_BINARY);
  maskedImage.setTo(new Scalar(0,0,0));
  colorImage.copyTo(maskedImage,disparityThreshold);
  maskedImage.copyTo(workingBackground,maskedImage);
  renderOutputAccordingToMode(disparityImage, disparityThreshold,
  colorImage, resizedBackground, workingBackground, gui);
}</pre></div><p>First, we invoke the <code class="literal">grab</code> method to get the next frame from the Kinect. Then, we retrieve depth map <a id="id235" class="indexterm"/>and color images. As we have previously loaded our background in <code class="literal">resizedBackground</code>, we just clone it to <code class="literal">workingBackground</code>. Following this, we threshold our disparity image according to our slider level. This will make pixels farther away from our desired depth go black, while the ones we still want become white. It is time to clear our mask and combine it with the colored image.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec52"/>Summary</h1></div></div></div><p>This chapter has really covered several areas that deal with background removal as well as some details that arise from this problem, such as the need to use connected components to find their contours. Firstly, the problem of background removal itself was established. Then, a simple algorithm such as frame differencing was analyzed. After that, more interesting algorithms, such as averaging background and <span class="strong"><strong>mixture of Gaussian</strong></span> (<span class="strong"><strong>MOG</strong></span>) were covered.</p><p>After using algorithms to deal with background removal problems, an insight about connected components was explored. Core OpenCV algorithms such as <code class="literal">findContours</code> and <code class="literal">drawContours</code> were explained. Some properties of contours were also analyzed, such as their area as well as convex hulls.</p><p>The chapter finished with complete explanations of how to use the Kinect's depth sensor device as a background removal tool, for OpenCV 2.4.7. After depth instructions on the device setup, a complete application was developed, making it clear to deal with the depth sensing sensors API.</p><p>Well, now it's time to jump from desktop applications to web apps in the next chapter. There, we'll cover details on how to set up an OpenCV-based web application, deal with image uploads, and create a nice augmented reality application based on the Tomcat web server. It is going to be fun, just watch out for Einstein's screenshots.</p></div></body></html>