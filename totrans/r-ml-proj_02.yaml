- en: Predicting Employee Attrition Using Ensemble Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集成模型预测员工流失
- en: If you reviewed the recent machine learning competitions, one key observation
    I am sure you would make is that the recipes of all three winning entries in most
    of the competitions include very good feature engineering, along with well-tuned
    ensemble models. One conclusion I derive from this observation is that good feature
    engineering and building well-performing models are two areas that should be given
    equal emphasis in order to deliver successful machine learning solutions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回顾了最近的机器学习竞赛，我相信你一定会注意到的一个关键观察结果是，大多数竞赛中所有三个获胜者的方案都包括非常好的特征工程，以及调优良好的集成模型。从这个观察结果中，我得出的一个结论是，好的特征工程和构建表现良好的模型是两个应该给予同等重视的领域，以便提供成功的机器学习解决方案。
- en: 'While feature engineering most times is something that is dependent on the
    creativity and domain expertise of the person building the model, building a well-performing
    model is something that can be achieved through a philosophy called **ensembling**.
    Machine learning practitioners often use ensembling techniques to beat the performance
    benchmarks yielded by even the best performing individual ML algorithm. In this
    chapter, we will learn about the following topics of this exciting area of ML:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然特征工程大多数时候是依赖于构建模型的人的创造力和领域专业知识，但构建一个表现良好的模型可以通过一种称为**集成学习**的哲学来实现。机器学习从业者经常使用集成技术来超越甚至最佳性能的个体机器学习算法产生的性能基准。在本章中，我们将学习这个激动人心的机器学习领域的以下主题：
- en: Philosophy behind ensembling
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成学习的哲学
- en: Understanding the attrition problem and the dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解员工流失问题和数据集
- en: K-nearest neighbors model for benchmarking the performance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K最近邻模型进行性能基准测试
- en: Bagging
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging
- en: Randomization with random forests
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林中的随机化
- en: Boosting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boosting
- en: Stacking
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stacking
- en: Philosophy behind ensembling
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习的哲学
- en: Ensembling, which is super-famous among ML practitioners, can be well-understood
    through a simple real-world, non-ML example.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习，在机器学习从业者中非常著名，可以通过一个简单的现实世界、非机器学习示例来很好地理解。
- en: Assume that you have applied for a job in a very reputable corporate organization
    and you have been called for an interview. It is unlikely you will be selected
    for a job just based on one interview with an interviewer. In most cases, you
    will go through multiple rounds of interviews with several interviewers or with
    a panel of interviewers. The expectation from the organization is that each of
    the interviewers is an expert on a particular area and that the interviewer has
    evaluated your fitness for the job based on your experience in the interviewers'
    area of expertise. Your selection for the job, of course, depends on consolidated
    feedback from all of the interviewers that talked to you. The organization deems
    that you will be more successful in the job as your selection is based on a consolidated
    decision made by multiple experts and not just based on one expert's decision,
    which may be prone to certain biases.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经申请了一家非常有声望的企业的职位，并且你被邀请参加面试。仅凭一次与面试官的面试，你不太可能被选中工作。在大多数情况下，你将经历多轮面试，与几个面试官或面试官小组进行面试。组织对面试官的期望是，每位面试官都是特定领域的专家，并且面试官已经根据你在面试官领域专业知识中的经验评估了你的工作适应性。当然，你被选中工作取决于所有与你交谈的面试官的综合反馈。组织认为，由于你的选择是基于多个专家做出的综合决策，而不是仅基于一个专家的决策，这可能会存在某些偏见，因此你将更有可能在工作中取得成功。
- en: 'Now, when we talk about the consolidation of feedback from all the interviewers,
    the consolidation can happen through several methods:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们谈论所有面试官反馈的整合时，整合可以通过几种方法发生：
- en: '**Averaging**: Assume that your candidature for the job is based on you clearing
    a cut-off score in the interviews. Assume that you have met ten interviewers and
    each one of them have rated you on a maximum score of 10 which represents your
    experience as perceived by interviewers in his area of expertise. Now, your consolidated
    score is made by simply averaging all your scores given by all the interviewers.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均**：假设你的工作候选人资格是基于你在面试中通过一个截止分数。假设你已经见过十个面试官，每个面试官都对你进行了最高10分的评分，这代表面试官在他领域专业知识中对你经验的感知。现在，你的综合评分是通过简单地平均所有面试官给你的分数来计算的。'
- en: '**Majority vote**: In this case, there is no actual score out of 10 which is
    provided by each of the interviewers. However, of the 10 interviewers, eight of
    them confirmed that you are a good fit for the position. Two interviewers said
    no to your candidature. You are selected for the job as the majority of the interviewers
    are happy with your interview performance.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多数投票**：在这种情况下，每位面试官并没有给出10分中的实际分数。然而，在10位面试官中，有8位确认您适合该职位。两位面试官表示不认可您的候选人资格。由于大多数面试官对您的面试表现感到满意，您被选中担任该职位。'
- en: '**Weighted average**: Let''s consider that four of the interviewers are experts
    in some minor skills that are good to have for the job you applied for. These
    are not mandatory skills needed for the position. You are interviewed by all 10
    interviewers and each one of them have given you a score out of 10\. Similar to
    the averaging method, in the weighted averaging method as well, your interviews
    final score is obtained by averaging the scores given by all interviewers.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权平均**：假设有四位面试官在您申请的职位中具备一些有益的次要技能，这些技能对于该职位并非必需。您接受了所有10位面试官的面试，每位面试官都给您打出了10分中的分数。与平均方法类似，在加权平均方法中，您的最终面试分数是通过平均所有面试官给出的分数来获得的。'
- en: However, not all scores are treated equally to compute the final score. Each
    interview score is multiplied with a weight and a product is obtained. All the
    products thus obtained thereby are summed to obtain the final score. The weight
    for each interview is a function of the importance of the skill it tested in the
    candidate and the importance of that skill to do the job. It is obvious that a
    *good to have* skill for the job carries a lower weight when compared to a *must
    have* skill. The final score now inherently represents the proportion of mandatory
    skills that the candidate possesses and this has more influence on your selection.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有分数在计算最终分数时都同等重要。每个面试分数都会乘以一个权重，得到一个乘积。所有这些乘积相加，从而得到最终分数。每个面试的权重是测试候选人技能的重要性以及该技能对完成工作的重要性函数。显然，对于工作来说，“有益的”技能与“必需的”技能相比，权重较低。最终分数现在本质上代表了候选人拥有的必需技能的比例，这对您的选择有更大的影响。
- en: Similar to the interviews analogy, ensembling in ML also produces models based
    on consolidated learning. The term **consolidated learning** essentially represents
    learning obtained through applying several ML algorithms or it is learning obtained
    from several data subsets that are part of a large dataset. Analogous to interviews,
    multiple models are learned from the application of ensembling technique. However,
    a final consolidation is arrived at regarding the prediction by means of applying
    one of the averaging, majority voting, or weighted averaging techniques on individual
    predictions made by each of the individual models. The models created from the
    application of an ensembling technique along with the prediction consolidation
    technique is typically termed as an **ensemble**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与面试的类比类似，机器学习中的集成也基于综合学习产生模型。术语“综合学习”本质上代表通过应用多个机器学习算法或从属于大型数据集的多个数据子集中获得的学习。类似于面试，通过应用集成技术，从多个模型中学习多个模型。然而，通过应用平均、多数投票或加权平均技术对每个单独模型做出的预测进行综合，从而得出关于预测的最终结论。应用集成技术和预测综合技术的模型通常被称为**集成**。
- en: Each ML algorithm is special and has a unique way to model the underlying training
    data. For example, a k-nearest neighbors algorithm learns by computing distances
    between the elements in dataset; naive Bayes learns by computing the probabilities
    of each attribute in the data belonging to a particular class. Multiple models
    may be created using different ML algorithms and predictions can be done by combining
    predictions of several ML algorithms. Similarly, when a dataset is partitioned
    to create subsets and if multiple models are trained using an algorithm each focusing
    on one dataset, each model is very focused and it is specialized in learning the
    characteristics of the subset of data it is trained on. In both cases, with models
    based on multiple algorithms and multiple subsets of data, when we combine the
    predictions of multiple models through consolidation, we get better predictions
    as we leverage multiple strengths that each model in an ensemble carry. This,
    otherwise, is not obtained when using a single model for predictions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习算法都是特殊的，并且有自己独特的方式来建模底层的训练数据。例如，k-最近邻算法通过计算数据集中元素之间的距离来学习；朴素贝叶斯通过计算数据中每个属性属于特定类的概率来学习。可以使用不同的机器学习算法创建多个模型，并通过结合几个机器学习算法的预测来进行预测。同样，当数据集被划分为子集，并且使用专注于每个数据集的算法训练多个模型时，每个模型都非常专注，并且专门学习它所训练的数据子集的特性。在这两种情况下，通过整合多个算法和多个数据子集的模型，当我们通过结合多个模型的优势来综合预测时，我们会得到更好的预测。否则，使用单个模型进行预测是无法获得这种效果的。
- en: The crux of ensembling is that, better predictions are obtained when we combine
    the predictions of multiple models than just relying on one model for prediction.
    This is no different from the management philosophy that together we do better,
    which is otherwise termed as **synergy**!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习的核心在于，当我们结合多个模型的预测而不是仅仅依赖单个模型进行预测时，我们可以获得更好的预测。这与“团结就是力量”的管理哲学没有不同，这通常被称为**协同效应**！
- en: Now that we understand the core philosophy behind ensembling, we are now ready
    to explore the different types of ensembling techniques. However, we will learn
    the ensembling techniques by implementing them in a project to predict the attrition
    of employees. As we already know, prior to building any ML project, it is very
    important to have a deep understanding of the problem and the data. Therefore,
    in the next section, we first focus on understanding the attrition problem at
    hand, then we study the dataset associated with the problem, and lastly we understand
    the properties of the dataset through exploratory data analysis (EDA). The key
    insights we obtain in this section come from a one-time exercise and will hold
    good for all the ensembling techniques we will apply in the later sections.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了集成学习的核心哲学，我们现在可以探索不同的集成技术类型了。然而，我们将通过在一个项目中实现它们来学习集成技术，该项目旨在预测员工的流失。正如我们已知的，在构建任何机器学习项目之前，对问题和数据有深入的理解非常重要。因此，在下一节中，我们首先关注理解当前面临的员工流失问题，然后研究与该问题相关联的数据集，最后通过探索性数据分析（EDA）理解数据集的特性。本节中我们获得的关键见解来自于一次性的练习，并将适用于我们在后续章节中应用的所有集成技术。
- en: Getting started
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始学习
- en: To get started with this section, you will have to download the `WA_Fn-UseC_-HR-Employee-Attrition.csv`
    dataset from the GitHub link for the code in this chapter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始本节的学习，您需要从本章代码的GitHub链接下载`WA_Fn-UseC_-HR-Employee-Attrition.csv`数据集。
- en: Understanding the attrition problem and the dataset
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解员工流失问题和数据集
- en: HR analytics helps with interpreting organizational data. It finds out the people-related
    trends in the data and helps the HR department take the appropriate steps to keep
    the organization running smoothly and profitably. Attrition in a corporate setup
    is one of the complex challenges that the people managers and HR personnel have
    to deal with. Interestingly, machine learning models can be deployed to predict
    potential attrition cases, thereby helping the appropriate HR personnel or people
    managers take the necessary steps to retain the employee.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 人力资源分析有助于解释组织数据。它发现数据中与人员相关的发展趋势，并帮助人力资源部门采取适当的步骤，使组织运行顺畅并盈利。在企业环境中，员工流失是管理人员和人力资源人员必须应对的复杂挑战之一。有趣的是，可以部署机器学习模型来预测潜在的员工流失案例，从而帮助适当的人力资源人员或管理人员采取必要的步骤来留住员工。
- en: In this chapter, we are going to build ML ensembles that will predict such potential
    cases of attrition. The job attrition dataset used for the project is a fictional
    dataset created by data scientists at IBM. The `rsample` library incorporates
    this dataset and we can make use of this dataset directly from the library.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建机器学习集成，以预测潜在的离职案例。用于项目的职位离职数据集是由IBM的数据科学家创建的虚构数据集。`rsample`库包含了这个数据集，我们可以直接从库中使用这个数据集。
- en: 'It is a small dataset that has 1,470 records of 31 attributes. The description
    of the dataset can be obtained with the following code:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含31个属性、1,470条记录的小数据集。可以通过以下代码获取数据集的描述：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will result in the following output:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To view the `Attrition` target variable in the dataset run the following code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看数据集中的`Attrition`目标变量，请运行以下代码：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will result in the following output:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Out of the 1,470 observations in the dataset, we have 1,233 samples (83.87%) that
    are non-attrition cases and 237 attrition cases (16.12%). Clearly, we are dealing
    with a *class imbalance* dataset.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中的1,470个观测值中，我们有1,233个样本（83.87%）是非离职案例，237个离职案例（16.12%）。显然，我们正在处理一个*类别不平衡*的数据集。
- en: 'We will now visualize the highly correlated variables in the data through the
    `corrplot` library using the following code:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将通过`corrplot`库使用以下代码可视化数据中的高度相关变量：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will result in the following output:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/00693069-856f-49b3-b946-e3c7e53b1ac1.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00693069-856f-49b3-b946-e3c7e53b1ac1.png)'
- en: In the preceding screenshot, it may be observed that darker and larger blues
    dot in the cells indicate the existence of a strong correlation between the variables
    in the corresponding rows and columns that form the cell. High correlation between
    the independent variables indicates the existence of redundant features in the
    data. The problem of the existence of highly correlated features in the data is
    termed as **multicollinearity**. If we were to fit a regression model, then it
    is required that we treat the highly correlated variables from the data through
    some techniques such as removing the redundant features or by applying principal
    component analysis or partial least squares regression, which intuitively cuts
    down the redundant features.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，可能观察到单元格中的较深和较大的蓝色点表示单元格中对应行和列的变量之间存在强相关性。独立变量之间的高度相关性表明数据中存在冗余特征。数据中高度相关特征的存在问题被称为**多重共线性**。如果我们想要拟合一个回归模型，那么我们需要通过一些技术来处理数据中的高度相关变量，例如删除冗余特征或应用主成分分析或偏最小二乘回归，这些技术直观地减少了冗余特征。
- en: 'We infer from the output that the following variables are highly correlated
    and the person building the model needs to take care of these variables if we
    are to build a regression-based model:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以推断出以下变量高度相关，如果我们要构建一个基于回归的模型，构建模型的人需要通过一些技术来处理这些变量，例如删除冗余特征或应用主成分分析或偏最小二乘回归，这些技术直观地减少了冗余特征。
- en: '`JobLevel`-`MonthlyIncome`; `JobLevel`-`TotalWorkingYears`; `MonthlyIncome`-`TotalWorkingYears`; `PercentSalaryHike`-`PerformanceRating`; `YearsAtCompany`-`YearsInCurrentRole`; `YearsAtCompany`-`YearsWithCurrManager`; `YearsWithCurrManager`-`YearsInCurrentRole`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`JobLevel`-`MonthlyIncome`；`JobLevel`-`TotalWorkingYears`；`MonthlyIncome`-`TotalWorkingYears`；`PercentSalaryHike`-`PerformanceRating`；`YearsAtCompany`-`YearsInCurrentRole`；`YearsAtCompany`-`YearsWithCurrManager`；`YearsWithCurrManager`-`YearsInCurrentRole`'
- en: 'Now, plot the various independent variables with the dependent `Attrition` variable
    in order to understand the influence of the independent variable on the target:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制各种独立变量与依赖的`Attrition`变量之间的关系图，以了解独立变量对目标的影响：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s run the following command to get a graph view:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行以下命令以获取图形视图：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding command generates the following output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令生成了以下输出：
- en: '![](img/bf9f27c2-82ba-4b9c-b71e-067888466d2b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf9f27c2-82ba-4b9c-b71e-067888466d2b.png)'
- en: 'In the preceding output, it can be observed that employees that work overtime
    are more prone to attrition when compared to the ones that do not work overtime:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，可以观察到加班的员工与未加班的员工相比，更容易出现离职现象：
- en: 'Let''s calculate the attrition of the employees by executing the following
    commands:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过执行以下命令来计算员工的离职率：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s run the following command to get a graph view:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行以下命令以获取图形视图：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding command generates the following output:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令生成了以下输出：
- en: '![](img/b1940a98-684a-401c-b1dc-2f2c8abece69.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1940a98-684a-401c-b1dc-2f2c8abece69.png)'
- en: 'In the preceding output, it can be observed that employees that are single
    have more attrition:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，可以观察到单身员工有更高的流失率：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Execute the following command to get a graphical representation for the same:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下命令以获取相同情况的图形表示：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Take a look at the following output generated by running the preceding command:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 看看运行前面命令生成的以下输出：
- en: '![](img/8469586f-dac2-4e9f-a5d3-229b91cb749c.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8469586f-dac2-4e9f-a5d3-229b91cb749c.png)'
- en: In the preceding output, it can be observed that the lab technicians, sales
    representatives, and employees working in human resources job roles have more
    attrition than other organizational roles.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，可以观察到实验室技术人员、销售代表和从事人力资源工作的员工比其他组织角色的员工流失率更高。
- en: 'Let''s execute the following commands to check with the impact of the gender
    of an employee over attribution:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下命令来检查员工性别对流失率的影响：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Run the following command to get a graphical representation for the same:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令以获取相同情况的图形表示：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will result in the following output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/8f15307f-e66a-43cf-bd52-de8ad0dd65ae.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8f15307f-e66a-43cf-bd52-de8ad0dd65ae.png)'
- en: In the preceding output, you can see that the gender of an employee does not
    have any impact on attrition, in other words attrition is observed to be the same
    across all genders.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，你可以看到员工的性别对流失率没有影响，换句话说，流失率在所有性别中观察到是相同的。
- en: 'Let''s calculate the attribute of the employees from various fields by executing
    the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过执行以下命令来计算来自不同领域的员工的属性：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s execute the following command to get a graphical representation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下命令以获取图形表示：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will result in the following output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/84dd7b0e-ee6a-49fe-aea9-c1e7b1d26f00.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/84dd7b0e-ee6a-49fe-aea9-c1e7b1d26f00.png)'
- en: 'Looking at the preceding graph, we can conclude that employees with a technical
    degree or a degree in human resources are observed to have more attrition. Take
    a look at the following code:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 观察前面的图表，我们可以得出结论，拥有技术学位或人力资源学位的员工观察到有更高的流失率。看看下面的代码：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s execute the following command to check with the attribution of various
    departments:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下命令来检查不同部门的流失率：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will result in the following output:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/5bd5d2df-9dd3-4e4c-aaae-0a027f4d7718.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5bd5d2df-9dd3-4e4c-aaae-0a027f4d7718.png)'
- en: 'Looking at the preceding graph, we can conclude that the R and D department
    has less attrition compared to the sales and HR departments. Take a look at the
    following code:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 观察前面的图表，我们可以得出结论，研发部门与销售和人力资源部门的流失率相比较低。看看下面的代码：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Execute the following command to get a graphical representation for the same:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下命令以获取相同情况的图形表示：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will result in the following output:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/1283f61e-dd01-4435-98da-9c8ca191a61a.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1283f61e-dd01-4435-98da-9c8ca191a61a.png)'
- en: Looking at the preceding graph, we can conclude that employees with frequent
    travels are prone to more attrition compared to employees with a non-travel status
    or the ones that rarely travel.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 观察前面的图表，我们可以得出结论，经常出差的员工与没有出差状态或很少出差的员工相比，更容易出现流失。
- en: 'Let''s calculate the overtime of the employees by executing the following commands:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过执行以下命令来计算员工的加班时间：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will result in the following output:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/ee279cb3-4192-40e4-b608-81e10487a03b.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ee279cb3-4192-40e4-b608-81e10487a03b.png)'
- en: 'Looking at the preceding graph, we can conclude that it can be observed that
    employees that are young (age < 35 ) and are single, but work overtime, are more
    prone to attrition:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 观察前面的图表，我们可以得出结论，年轻（年龄 < 35）且单身但加班的员工更容易出现流失：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will result in the following output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/5d910b21-22e9-4301-a211-b4ae423cff62.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5d910b21-22e9-4301-a211-b4ae423cff62.png)'
- en: Looking at the preceding graph, we can conclude that attrition is higher in
    employees that are young (age < 30) and most attrition is observed with employees
    that earn less than $7,500.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 观察前面的图表，我们可以得出结论，年轻（年龄 < 30）的员工流失率较高，并且观察到大多数流失率发生在收入低于7500美元的员工中。
- en: Although we have learned a number of important details about the data at hand,
    there is actually so much more to explore and learn. However, so as to move to
    the next step, we stop here at this EDA step. It should be noted that, in a real-world
    situation, data would not be so very clean as we see in this attrition dataset.
    For example, we would have missing values in the data; in which case, we would
    do missing values imputation. Fortunately, we have an impeccable dataset that
    is ready for us to create models without having to do any data cleansing or additional
    preprocessing.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经了解了关于当前数据的许多重要细节，但实际上还有更多值得探索和学习的内容。然而，为了进入下一步，我们在这一 EDA 步骤处停止。需要注意的是，在现实世界中，数据可能不会像我们在这一磨损数据集中看到的那样非常干净。例如，数据中可能会有缺失值；在这种情况下，我们会进行缺失值插补。幸运的是，我们有一个完美的数据集，可以用来创建模型，而无需进行任何数据清洗或额外的预处理。
- en: K-nearest neighbors model for benchmarking the performance
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于性能基准的 K-最近邻模型
- en: 'In this section, we will implement the **k-nearest neighbors** (**KNN**) algorithm
    to build a model on our IBM attrition dataset. Of course, we are already aware
    from EDA that we have a class imbalance problem in the dataset at hand. However,
    we will not be treating the dataset for class imbalance for now as this is an
    entire area on its own and several techniques are available in this area and therefore
    out of scope for the ML ensembling topic covered in this chapter. We will, for
    now, consider the dataset as is and build ML models. Also, for class imbalance
    datasets, Kappa or precision and recall or the area under the curve of the receiver
    operating characteristic (AUROC) are the appropriate metrics to use. However,
    for simplicity, we will use *accuracy* as a performance metric. We will adapt
    10-fold cross validation repeated 10 times to avail the model performance measurement.
    Let''s now build our attrition prediction model with the KNN algorithm as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现 **k-最近邻**（**KNN**）算法，并在我们的 IBM 离职数据集上构建模型。当然，我们已经从 EDA 中了解到，我们手头的数据集中存在类别不平衡问题。然而，我们现在不会对数据集进行类别不平衡处理，因为这是一个独立的整个领域，并且该领域有几种技术可用，因此超出了本章中涵盖的机器学习集成主题的范围。我们将暂时将数据集视为现状，并构建机器学习模型。此外，对于类别不平衡数据集，Kappa
    或精确率和召回率或接收者操作特征（ROC）曲线下的面积（AUROC）是合适的指标。然而，为了简化，我们将使用 *准确率* 作为性能指标。我们将采用 10 折交叉验证重复
    10 次来评估模型性能。现在，让我们使用 KNN 算法构建我们的离职预测模型，如下所示：
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will result in the following output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can see from the model output that the best performing model is when `k`
    `= 11` and we obtained an accuracy of 84% with this `k` value. In the rest of
    the chapter, while experimenting with several ensembling techniques, we will check
    if this 84% accuracy obtained from KNN will get beaten at all.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型输出中我们可以看到，当 `k` `= 11` 时，表现最好的模型，我们使用这个 `k` 值获得了 84% 的准确率。在本章的其余部分，当我们实验几种集成技术时，我们将检查这个
    KNN 获得的 84% 准确率是否会被击败。
- en: In a realistic project-building situation, just identifying the best hyperparameters
    is not enough. A model needs to be trained on a full dataset with the best hyperparameters
    and the model needs to be saved for future use. We will review these steps in
    the rest of this section.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个实际的项目构建情况下，仅仅确定最佳超参数是不够的。模型需要在包含最佳超参数的完整数据集上训练，并且模型需要保存以供将来使用。我们将在本节的其余部分回顾这些步骤。
- en: 'In this case, the `caretmodel` object already has the trained model with `k
    = 11`, therefore we do not attempt to retrain the model with the best hyperparameter.
    To check the final model, you can query the model object with the code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`caretmodel` 对象已经包含了训练好的模型，其中 `k = 11`，因此我们不会尝试使用最佳超参数重新训练模型。要检查最终模型，可以使用以下代码查询模型对象：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This will result in the following output:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The next step is to save your best models to a file so that we can load them
    up later and make predictions on unseen data. A model can be saved to a local
    directory using the `saveRDS` R command:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将您最好的模型保存到文件中，以便我们稍后可以加载它们并预测未见数据。可以使用 `saveRDS` R 命令将模型保存到本地目录：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In this case, the `caretmodel` is saved as `production_model.rds` in the working
    directory. The model is now serialized as a file that can be loaded anytime and
    it can be used to score unseen data. Loading and scoring can be achieved through
    the following R code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`caretmodel` 模型被保存在工作目录中的 `production_model.rds` 文件中。现在该模型以文件形式序列化，可以随时加载，并可用于评估未见数据。加载和评估可以通过以下
    R 代码实现：
- en: '[PRE26]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Please note that `unseen_data` needs to be read prior to scoring through the
    `predict` command.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`unseen_data`需要在通过`predict`命令评分之前读取。
- en: The part of the code where the final model is trained on the entire dataset,
    saving the model, reloading it from the file whenever required and scoring the
    unseen data collectively, is termed as building an ML productionalization pipeline.
    This pipeline remains the same for all ML models irrespective of the fact that
    the model is built using one single algorithm or using an ensembling technique.
    Therefore, in the later sections when we implement the various ensembling techniques,
    we will not cover the productionalization pipeline but just stop at obtaining
    the performance measurement through 10-fold cross validation repeated 10 times.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的一部分，其中最终模型在全部数据集上训练，保存模型，在需要时从文件中重新加载它，并集体评分未看到的资料，被称为构建机器学习生产化管道。这个管道对所有机器学习模型都是相同的，无论模型是使用单个算法还是使用集成技术构建。因此，在后面的章节中，当我们实现各种集成技术时，我们不会涵盖生产化管道，而只是停止在通过10折交叉验证重复10次获得性能度量。
- en: Bagging
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging
- en: Bootstrap aggregation or **bagging** is the earliest ensemble technique adopted
    widely by the ML-practicing community. Bagging involves creating multiple different
    models from a single dataset. It is important to understand an important statistical
    technique called bootstrapping in order to get an understanding of bagging.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Bootstrap aggregation或**Bagging**是机器学习实践社区最早广泛采用的集成技术。Bagging涉及从单个数据集中创建多个不同的模型。为了理解Bagging，了解一个重要的统计技术——bootstrap，是很有必要的。
- en: Bootstrapping involves multiple random subsets of a dataset being created. It
    is possible that the same data sample gets picked up in multiple subsets and this
    is termed as **bootstrapping with replacement**. The advantage with this approach
    is that the standard error in estimating a quantity that occurs due to the use
    of whole dataset. This technique can be better explained with an example.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Bootstrapping涉及创建数据集的多个随机子集。有可能同一个数据样本被多个子集选中，这被称为**带有替换的bootstrap**。这种方法的优点是，它减少了由于使用整个数据集而导致的估计量的标准误差。这个技术可以通过一个例子来更好地解释。
- en: 'Assume you have a small dataset of 1,000 samples. Based on the samples, you
    are asked to compute the average of the population that the sample represents.
    Now, a direct way of doing it is through the following formula:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个包含1,000个样本的小数据集。根据样本，你需要计算代表该样本的总体平均。现在，直接做这件事的方法是以下公式：
- en: '![](img/1ffab3de-df4f-489c-9a25-be38913f6d33.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1ffab3de-df4f-489c-9a25-be38913f6d33.png)'
- en: 'As this is a small sample, we may have an error in estimating the population
    average. This error can be reduced by adapting bootstrap sampling with replacement.
    In the technique, we create 10 subsets of the dataset where each dataset has 100
    items in it. A data item may be randomly represented multiple times in a subset
    and there is no restriction on the number of times an item can be represented
    within a data subset as well as across the subsets. Now, we take the average of
    samples in each data subset, therefore, we end up with 10 different averages.
    Using all these collected averages, we estimate the average of the population
    with the following formula:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个小样本，我们在估计总体平均时可能会出现误差。通过采用带有替换的bootstrap采样可以减少这种误差。在这种技术中，我们创建了10个子集，每个数据集包含100个项目。一个数据项可以在子集中随机表示多次，并且对数据子集中以及跨子集的数据项表示的次数没有限制。现在，我们取每个数据子集中样本的平均值，因此，我们最终得到10个不同的平均值。使用所有这些收集到的平均值，我们使用以下公式估计总体的平均：
- en: '![](img/ab180bc6-78d6-497a-ae47-abeabca5e347.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ab180bc6-78d6-497a-ae47-abeabca5e347.png)'
- en: Now, we have a better estimate of the average as we have extrapolated the small
    sample to randomly generate multiple samples that are representative of the original population.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了更好的平均估计，因为我们已经将小样本外推以随机生成多个样本，这些样本代表了原始总体。
- en: In bagging, the actual training dataset is split into multiple bags through
    bootstrap sampling with replacement. Assuming that we ended up with *n* bags,
    when an ML algorithm is applied on each of these bags, we obtain *n* different
    models. Each model is focused on one bag. When it comes to making predictions
    on new unseen data, each of these *n* models makes independent predictions on
    the data. A final prediction for an observation is arrived at by combining the
    predictions of the observation of all the *n* models. In case of classification,
    voting is adopted and the majority is considered as the final prediction. For
    regression, the average of predictions from all models is considered as the final
    prediction.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在袋装中，实际训练数据集通过带有替换的 bootstrap 抽样分成多个袋。假设我们最终得到 *n* 个袋，当机器学习算法应用于这些袋中的每一个时，我们获得
    *n* 个不同的模型。每个模型都专注于一个袋。当需要对新未见数据做出预测时，这些 *n* 个模型中的每一个都会独立地对数据进行预测。通过结合所有 *n* 个模型的预测，得出一个观察值的最终预测。在分类的情况下，采用投票，并将多数视为最终预测。对于回归，考虑所有模型的预测平均值作为最终预测。
- en: Decision-tree-based algorithms, such as **classification and regression trees**
    (**CART**), are unstable learners. The reason is that a small change in the training
    dataset heavily impacts the model created. Model change essentially means that
    the predictions also change. Bagging is a very effective technique to handle the
    high sensitivity to data changes. As we can build multiple decision tree models
    on subsets of a dataset and then arrive at a final prediction based on predictions
    from each of the models, the effect of changes in data gets nullified or not experienced
    very significantly.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 基于决策树的算法，如**分类和回归树**（**CART**），是不稳定的学习者。原因是训练数据集的微小变化会严重影响创建的模型。模型变化本质上意味着预测也会变化。袋装是一种非常有效的技术，可以处理对数据变化的极高敏感性。因为我们可以在数据集的子集上构建多个决策树模型，然后根据每个模型的预测得出最终预测，因此数据变化的影响被消除或不太明显。
- en: One intuitive problem experienced with building multiple models on subsets of
    data is **overfitting**. However, this is overcome by growing deep trees without
    applying any pruning on the nodes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据子集上构建多个模型时，会遇到一个直观的问题，那就是**过拟合**。然而，通过在不应用任何剪枝的情况下生长深度树，可以克服这个问题。
- en: A downside with bagging is that it takes longer to build the models when compared
    to building a model with a stand-alone ML algorithm. This is obvious because multiple
    models gets built in bagging, as opposed to one single model, and it takes time
    to build these multiple models.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用袋装法的一个缺点是，与使用独立机器学习算法构建模型相比，构建模型所需的时间更长。这是显而易见的，因为在袋装法中会构建多个模型，而不是一个单独的模型，构建这些多个模型需要时间。
- en: Now, let's implement the R code to achieve a bagging ensemble and compare the
    performance obtained with that of the performance obtained from KNN. We will then
    explore the working mechanics of bagging methodology.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写 R 代码来实现袋装集成，并比较获得的性能与 KNN 算法获得的性能。然后，我们将探讨袋装方法的运作机制。
- en: The `caret` library provides a framework to implement bagging with any stand-alone
    ML algorithm. `ldaBag`, `plsBag`, `nbBag`, `treeBag`, `ctreeBag`, `svmBag`, and
    `nnetBag` are some of the example methods provided in caret. In this section, we
    will implement bagging with three different `caret` methods such as `treebag`,
    `svmbag`, and `nbbag`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`caret` 库提供了一个框架，可以与任何独立的机器学习算法实现袋装。`ldaBag`、`plsBag`、`nbBag`、`treeBag`、`ctreeBag`、`svmBag`
    和 `nnetBag` 是 `caret` 中提供的一些示例方法。在本节中，我们将使用三种不同的 `caret` 方法实现袋装，例如 `treebag`、`svmbag`
    和 `nbbag`。'
- en: Bagged classification and regression trees (treeBag) implementation
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 袋装分类和回归树（treeBag）实现
- en: 'To begin, load the essential libraries and register the number of cores for
    parallel processing:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载必要的库并注册用于并行处理的核心数：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This will result in the following output:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We can see that we achieved a better accuracy of 85.4% compared to 84% accuracy
    that was obtained with the KNN algorithm.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们实现了85.4%的更好准确率，而使用 KNN 算法获得的准确率是84%。
- en: Support vector machine bagging (SVMBag) implementation
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机袋装（SVMBag）实现
- en: 'The steps of loading the libraries, registering multiprocessing, setting a
    working directory, reading data from a working directory, removing nondiscriminatory
    features from data, and setting up cross-validation parameters remain the same
    in the SVMBag and NBBag implementations as well. So, we do not repeat these steps
    in the SVMBag or NBBag code. Rather, we will focus on discussing the SVMBag or
    NBBag specific code:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在SVMBag和NBBag实现中，加载库、注册多进程、设置工作目录、从工作目录读取数据、从数据中移除非判别性特征以及设置交叉验证参数的步骤保持不变。因此，我们不在SVMBag或NBBag代码中重复这些步骤。相反，我们将专注于讨论SVMBag或NBBag特定的代码：
- en: '[PRE30]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will result in the following output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE31]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You will see that we achieved an accuracy of 87.7%, which is much higher than
    the KNN model's 84% accuracy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到我们达到了87.7%的准确率，这比KNN模型的84%准确率要高得多。
- en: Naive Bayes (nbBag) bagging implementation
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单贝叶斯（nbBag）袋装化实现
- en: 'We will now do the `nbBag` implementation by executing the following code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将通过执行以下代码来实现`nbBag`实现：
- en: '[PRE32]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This will result in the following output:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE33]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We see that in this case, we achieved only 83.89% accuracy, which is slightly
    inferior to the KNN model's performance of 84%.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在这种情况下，我们只达到了83.89%的准确率，略低于KNN模型的84%性能。
- en: 'Although we have shown only three examples of the `caret` methods for bagging,
    the code remains the same to implement the other methods. The only change that
    is needed in the code is to replace the `fit`, `predict`, and `aggregate` parameters
    in `bagControl`. For example, to implement bagging with a neural network algorithm,
    we need to define `bagControl` as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们只展示了`caret`方法中用于袋装化的三个示例，但代码在实现其他方法时保持不变。在代码中需要做的唯一更改是在`bagControl`中替换`fit`、`predict`和`aggregate`参数。例如，要实现使用神经网络算法的袋装化，我们需要将`bagControl`定义为以下内容：
- en: '[PRE34]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: It may be noted that an appropriate library needs to be available in R for `caret`
    to run the methods, otherwise it results in error. For example, `nbBag` requires
    the `klaR` library to be installed on the system prior to executing the code.
    Similarly, the `ctreebag` function needs the `party` package to be installed.
    Users need to check the availability of an appropriate library on the system prior
    to including it for use with the `caret` bagging.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要注意，R中需要有一个适当的库来运行`caret`方法，否则将导致错误。例如，`nbBag`需要在执行代码之前在系统上安装`klaR`库。同样，`ctreebag`函数需要安装`party`包。用户在使用`caret`袋装化之前需要检查系统上是否存在适当的库。
- en: We now have an understanding of implementing a project through bagging technique.
    The next subsection covers the underlying working mechanism of bagging. This will
    help get clarity in terms of what bagging did internally with our dataset so as
    to produce better performance measurements than that of stand-alone model performance.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经了解了通过袋装化技术实施项目的实现方法。下一小节将介绍袋装化的底层工作机制。这将有助于我们了解袋装化在内部如何处理我们的数据集，以便产生比独立模型性能更好的性能测量结果。
- en: Randomization with random forests
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林中的随机化
- en: As we've seen in bagging, we create a number of bags on which each model is
    trained. Each of the bags consists of subsets of the actual dataset, however the
    number of features or variables remain the same in each of the bags. In other
    words, what we performed in bagging is subsetting the dataset rows.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在袋装化中看到的，我们创建了多个袋装，每个模型都在这些袋装上进行训练。每个袋装都由实际数据集的子集组成，然而每个袋装中的特征或变量数量保持不变。换句话说，我们在袋装化中所做的是对数据集行进行子集化。
- en: In random forests, while we create bags from the dataset through subsetting
    the rows, we also subset the features (columns) that need to be included in each
    of the bags.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林中，当我们通过子集化行从数据集中创建袋装时，我们还子集化了需要包含在每个袋装中的特征（列）。
- en: Assume that you have 1,000 observations with 20 features in your dataset. We
    can create 20 bags where each one of the bags has 100 observations (this is possible
    because of bootstrapping with replacement) and five features. Now 20 models are
    trained where each model gets to see only the bag it is assigned with. The final
    prediction is arrived at by voting or averaging based on the fact of whether the
    problem is a regression problem or a classification problem.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的数据集中有1,000个观测值和20个特征。我们可以创建20个袋装，其中每个袋装有100个观测值（这是由于有放回的重新抽样而成为可能），并且每个袋装有五个特征。现在，训练了20个模型，每个模型只能看到分配给它的袋装。最终的预测是通过投票或基于问题是否为回归问题或分类问题进行平均得出的。
- en: Another key difference between bagging and random forests is the ML algorithm
    that is used to build the model. In bagging, any ML algorithm may be used to create
    a model however random forest models are built specifically using CART.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与袋装相比，随机森林的另一个关键区别是用于构建模型的机器学习算法。在袋装中，可以使用任何机器学习算法来创建模型，但是随机森林模型是专门使用CART构建的。
- en: Random forest modeling is yet another very popular machine learning algorithm.
    It is one of the algorithms that has proved itself multiple times as the best
    performing of algorithms, despite applying it on noisy datasets. For a person
    that has understood bootstrapping, understanding random forests is a cakewalk.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林建模是另一种非常流行的机器学习算法。它是那些多次证明自己是最优算法之一的算法之一，尽管它在噪声数据集上应用。对于一个已经理解了自助法的个人来说，理解随机森林就像小菜一碟。
- en: Implementing an attrition prediction model with random forests
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林实现流失预测模型
- en: 'Let''s get our attrition model through random forest modeling by executing
    the following code:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过执行以下代码，通过随机森林建模来获取我们的流失模型：
- en: '[PRE35]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This will result in the following output:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE36]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We see the best random forest model achieved a better accuracy of 86% compared
    to KNN's 84%.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，最佳的随机森林模型实现了86%的更好准确率，而KNN的准确率是84%。
- en: Boosting
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升
- en: A weak learner is an algorithm that performs relatively poorly—generally, the
    accuracy obtained with the weak learners is just above chance. It is often, if
    not always, observed that weak learners are computationally simple. Decision stumps
    or 1R algorithms are some examples of weak learners. Boosting converts weak learners
    into strong learners. This essentially means that boosting is not an algorithm
    that does the predictions, but it works with an underlying weak ML algorithm to
    get better performance.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 弱学习器是一种表现相对较差的算法——通常，弱学习器获得的准确率仅略高于随机水平。通常，如果不是总是观察到，弱学习器在计算上很简单。决策树桩或1R算法是弱学习器的例子。提升将弱学习器转换为强学习器。这本质上意味着提升不是一个进行预测的算法，而是与一个底层的弱机器学习算法一起工作以获得更好的性能。
- en: A boosting model is a sequence of models learned on subsets of data similar
    to that of the bagging ensembling technique. The difference is in the creation
    of the subsets of data. Unlike bagging, all the subsets of data used for model
    training are not created prior to the start of the training. Rather, boosting
    builds a first model with an ML algorithm that does predictions on the entire
    dataset. Now, there are some misclassified instances that are subsets and used
    by the second model. The second model only learns from this misclassified set
    of data curated from the first model's output.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 提升模型是一系列在数据子集上学习的模型，这些子集与袋装集成技术类似。不同之处在于数据子集的创建。与袋装不同，用于模型训练的所有数据子集并不是在训练开始之前就创建好的。相反，提升通过一个机器学习算法构建第一个模型，该算法在整个数据集上进行预测。现在，有一些被错误分类的实例，这些实例是子集，并被第二个模型使用。第二个模型只从第一个模型输出的错误分类数据集中学习。
- en: The second model's misclassified instances become input to the third model.
    The process of building models is repeated until the stopping criteria is met.
    The final prediction for an observation in the unseen dataset is arrived by averaging
    or voting the predictions from all the models for that specific, unseen observation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个模型的错误分类实例成为第三个模型的输入。构建模型的过程会重复进行，直到满足停止标准。对未见数据集中的观察值的最终预测是通过平均或投票所有模型对该特定、未见观察值的预测来得到的。
- en: There are subtle differences between the various and numerous algorithms in
    the boosting algorithms family, however we are not going to discuss them in detail
    as the intent of this chapter is to get a generalized understanding of ML ensembles
    and not to gain in-depth knowledge of various boosting algorithms.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在提升算法家族中，各种算法之间存在着细微的差别，然而我们不会详细讨论它们，因为本章的目的是获得对机器学习集成的一般理解，而不是深入了解各种提升算法。
- en: While obtaining better performance, measurement is the biggest advantage with
    the boosting ensemble; difficulty with model interpretability, higher computational
    times, and model overfitting are some of the issues encountered with boosting.
    Of course, these problems can be overruled through the use of specialized techniques.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得更好的性能的同时，度量是提升集成最大的优势；模型可解释性困难、更高的计算时间、模型过拟合是使用提升时遇到的一些问题。当然，这些问题可以通过使用专门的技术来解决。
- en: Boosting algorithms are undoubtedly super-popular and are observed to be used
    by winners in many Kaggle and similar competitions. There are a number of boosting
    algorithms available such as **gradient boosting machines** (**GBMs**), **adaptive
    boosting** (**AdaBoost**) , gradient tree boosting, **extreme gradient boosting**
    (**XGBoost**), and **light gradient boosting machine** (**LightGBM**). In this
    section, we will learn the theory and implementation of two of the most popular
    boosting algorithms such as GBMs and XGBoost. Prior to learning the theoretical
    concept of boosting and its pros and cons, let's first start focusing on implementing
    the attrition prediction models with GBMs and XGBoost.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法无疑是超级流行的，并且观察到在许多Kaggle和类似比赛中获胜者都在使用。有几种提升算法可供选择，例如**梯度提升机**（**GBMs**）、**自适应提升**（**AdaBoost**）、梯度树提升、**极端梯度提升**（**XGBoost**）和**轻梯度提升机**（**LightGBM**）。在本节中，我们将学习两种最受欢迎的提升算法的理论和实现，即GBMs和XGBoost。在学习提升的理论概念及其优缺点之前，让我们首先开始关注使用GBMs和XGBoost实现员工流失预测模型。
- en: The GBM implementation
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GBM实现
- en: 'Let''s implement the attrition prediction model with GBMs:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现使用GBM的员工流失预测模型：
- en: '[PRE37]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This will result in the following output:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE38]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: You will see that with the GBM model, we have achieved accuracy above 87%, which
    is better accuracy compared to the 84% achieved with KNN.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到，使用GBM模型，我们实现了超过87%的准确率，这比使用KNN实现的84%的准确率要好。
- en: Building attrition prediction model with XGBoost
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用XGBoost构建员工流失预测模型
- en: 'Now, let''s implement the attrition prediction model with XGBoost:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用XGBoost实现员工流失预测模型：
- en: '[PRE39]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This will result in the following output:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/d37882b4-f89b-42d7-a2d2-69f6c208a1fc.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d37882b4-f89b-42d7-a2d2-69f6c208a1fc.png)'
- en: '[PRE40]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Again, we observed that with XGBoost model, we have achieved an accuracy above
    87%, which is a better accuracy compared to the 84% achieved with KNN.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们观察到，使用XGBoost模型，我们实现了超过87%的准确率，这比使用KNN实现的84%的准确率要好。
- en: Stacking
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠
- en: In all the ensembles we have learned about so far, we have manipulated the dataset
    in certain ways and exposed subsets of the data for model building. However, in
    stacking, we are not going to do anything with the dataset; instead we are going
    to apply a different technique that involves using multiple ML algorithms instead. In
    stacking, we build multiple models with various ML algorithms. Each algorithm
    possesses a unique way of learning the characteristics of data and the final stacked
    model indirectly incorporates all those unique ways of learning. Stacking gets
    the combined power of several ML algorithms through getting the final prediction
    by means of voting or averaging as we do in other types of ensembles.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迄今为止所学习的所有集成中，我们都以某种方式操纵了数据集，并暴露了数据集的子集以进行模型构建。然而，在堆叠中，我们不会对数据集做任何事情；相反，我们将应用一种不同的技术，该技术涉及使用多个机器学习算法。在堆叠中，我们使用各种机器学习算法构建多个模型。每个算法都有一种独特的学习数据特征的方式，最终的堆叠模型间接地结合了所有这些独特的学习方式。堆叠通过通过投票或平均（正如我们在其他类型的集成中所做的那样）获得最终预测，从而获得了几个机器学习算法的联合力量。
- en: Building attrition prediction model with stacking
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用堆叠构建员工流失预测模型
- en: 'Let''s build an attrition prediction model with stacking:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个使用堆叠的员工流失预测模型：
- en: '[PRE41]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This will result in the following output:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE42]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This will result in the following output:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/6659bd76-cb08-42b4-9b50-2fb804feba66.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6659bd76-cb08-42b4-9b50-2fb804feba66.png)'
- en: 'We can see from the correlation table results that none of the individual ML
    algorithm predictions are highly correlated. Very highly correlated results mean
    that the algorithms have produced very similar predictions. Combining the very
    similar predictions may not really yield significant benefit compared with what
    one would avail from accepting the individual predictions. In this specific case,
    we can observe that none of the algorithm predictions are highly correlated so
    we can straightforwardly move to the next step of stacking the predictions:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 从相关性表的结果中我们可以看出，没有任何一个单独的机器学习算法的预测结果高度相关。高度相关的结果意味着算法产生了非常相似的预测。与接受单个预测相比，结合这些非常相似的预测可能并不会真正带来显著的好处。在这个特定的情况下，我们可以观察到没有任何算法的预测是高度相关的，因此我们可以直接进入下一步，即堆叠预测：
- en: '[PRE43]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This will result in the following output:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE44]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'With GLM-based stacking, we have 88% accuracy. Let''s now examine the effect
    of using random forest modeling instead of GLM to stack the individual predictions
    from each of the five ML algorithms on the observations:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于GLM的stacking，我们达到了88%的准确率。现在，让我们来检查使用随机森林建模而不是GLM来堆叠来自五个机器学习算法的每个算法的预测对观察结果的影响：
- en: '[PRE45]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This will result in the following output:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE46]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We see that without much effort, we were able to achieve an accuracy of 91%
    by stacking the predictions. Now, let's explore the working principle of stacking.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，只需付出很少的努力，我们就能够通过堆叠预测实现91%的准确率。现在，让我们来探讨堆叠的工作原理。
- en: At last, we have discovered the various ensembling techniques that can provide
    us with better performing models. However, before ending the chapter, there are
    a couple of things we need to take a note of.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们发现了各种集成技术，这些技术可以为我们提供性能更好的模型。然而，在结束本章之前，还有一些事情我们需要注意。
- en: There is not just one way to implement ML models in R. For example, bagging
    can be implemented using functions available in the `ipred` library and not by
    using `caret` as we did in this chapter. We should be aware that hyperparameter
    tuning forms an important part of model building to avail the best performing
    model. The number of hyperparameters and the acceptable values for those hyperparameters
    vary depending on the library that we intend to use. This is the reason why we
    paid less attention to hyperparameter tuning in the models we built in this chapter.
    Nevertheless, it is very important to read up the library documentation to understand
    the hyperparameters that can be tuned with a library function. In most cases,
    incorporating hyperparameter tuning in models significantly improves the model's
    performance.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中实现机器学习模型的方式不止一种。例如，可以使用`ipred`库中的函数来实现bagging，而不是像本章中所做的那样使用`caret`。我们应该意识到，超参数调整是模型构建的重要组成部分，以便获得最佳性能的模型。超参数的数量以及这些超参数的可接受值取决于我们打算使用的库。这就是为什么我们在本章构建的模型中对超参数调整的关注较少。尽管如此，阅读库文档以了解可以使用库函数调整的超参数非常重要。在大多数情况下，将超参数调整纳入模型可以显著提高模型性能。
- en: Summary
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: To recollect, we were using a class-imbalanced dataset to build the attrition
    model. Using techniques to resolve the class imbalance prior to model building
    is another key aspect of getting better model performance measurements. We used
    bagging, randomization, boosting, and stacking to implement and predict the attrition
    model. We were able to accomplish 91% accuracy just by using the features that
    were readily available in the models. Feature engineering is a crucial aspect
    whose role cannot be ignored in ML models. This may be one other path to explore
    to improve model performance further.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，我们使用的是不平衡的数据集来构建流失模型。在模型构建之前使用技术来解决类别不平衡是获得更好的模型性能测量值的关键方面之一。我们使用了bagging、随机化、boosting和stacking来实现和预测流失模型。我们仅通过使用模型中现成的特征就实现了91%的准确率。特征工程是一个关键方面，其作用在机器学习模型中不容忽视。这可能又是探索进一步提高模型性能的另一条途径。
- en: In the next chapter, we will explore the secret recipe of recommending products
    or content through building a personalized recommendation engines. I am all set
    to implement a project to recommend jokes. Turn to the next chapter to continue
    the journey of learning.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨通过构建个性化推荐引擎来推荐产品或内容的秘密配方。我已经准备好实施一个推荐笑话的项目。翻到下一章继续学习之旅。
