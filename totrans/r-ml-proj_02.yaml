- en: Predicting Employee Attrition Using Ensemble Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you reviewed the recent machine learning competitions, one key observation
    I am sure you would make is that the recipes of all three winning entries in most
    of the competitions include very good feature engineering, along with well-tuned
    ensemble models. One conclusion I derive from this observation is that good feature
    engineering and building well-performing models are two areas that should be given
    equal emphasis in order to deliver successful machine learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'While feature engineering most times is something that is dependent on the
    creativity and domain expertise of the person building the model, building a well-performing
    model is something that can be achieved through a philosophy called **ensembling**.
    Machine learning practitioners often use ensembling techniques to beat the performance
    benchmarks yielded by even the best performing individual ML algorithm. In this
    chapter, we will learn about the following topics of this exciting area of ML:'
  prefs: []
  type: TYPE_NORMAL
- en: Philosophy behind ensembling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the attrition problem and the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-nearest neighbors model for benchmarking the performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomization with random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Philosophy behind ensembling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensembling, which is super-famous among ML practitioners, can be well-understood
    through a simple real-world, non-ML example.
  prefs: []
  type: TYPE_NORMAL
- en: Assume that you have applied for a job in a very reputable corporate organization
    and you have been called for an interview. It is unlikely you will be selected
    for a job just based on one interview with an interviewer. In most cases, you
    will go through multiple rounds of interviews with several interviewers or with
    a panel of interviewers. The expectation from the organization is that each of
    the interviewers is an expert on a particular area and that the interviewer has
    evaluated your fitness for the job based on your experience in the interviewers'
    area of expertise. Your selection for the job, of course, depends on consolidated
    feedback from all of the interviewers that talked to you. The organization deems
    that you will be more successful in the job as your selection is based on a consolidated
    decision made by multiple experts and not just based on one expert's decision,
    which may be prone to certain biases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, when we talk about the consolidation of feedback from all the interviewers,
    the consolidation can happen through several methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Averaging**: Assume that your candidature for the job is based on you clearing
    a cut-off score in the interviews. Assume that you have met ten interviewers and
    each one of them have rated you on a maximum score of 10 which represents your
    experience as perceived by interviewers in his area of expertise. Now, your consolidated
    score is made by simply averaging all your scores given by all the interviewers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Majority vote**: In this case, there is no actual score out of 10 which is
    provided by each of the interviewers. However, of the 10 interviewers, eight of
    them confirmed that you are a good fit for the position. Two interviewers said
    no to your candidature. You are selected for the job as the majority of the interviewers
    are happy with your interview performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted average**: Let''s consider that four of the interviewers are experts
    in some minor skills that are good to have for the job you applied for. These
    are not mandatory skills needed for the position. You are interviewed by all 10
    interviewers and each one of them have given you a score out of 10\. Similar to
    the averaging method, in the weighted averaging method as well, your interviews
    final score is obtained by averaging the scores given by all interviewers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, not all scores are treated equally to compute the final score. Each
    interview score is multiplied with a weight and a product is obtained. All the
    products thus obtained thereby are summed to obtain the final score. The weight
    for each interview is a function of the importance of the skill it tested in the
    candidate and the importance of that skill to do the job. It is obvious that a
    *good to have* skill for the job carries a lower weight when compared to a *must
    have* skill. The final score now inherently represents the proportion of mandatory
    skills that the candidate possesses and this has more influence on your selection.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the interviews analogy, ensembling in ML also produces models based
    on consolidated learning. The term **consolidated learning** essentially represents
    learning obtained through applying several ML algorithms or it is learning obtained
    from several data subsets that are part of a large dataset. Analogous to interviews,
    multiple models are learned from the application of ensembling technique. However,
    a final consolidation is arrived at regarding the prediction by means of applying
    one of the averaging, majority voting, or weighted averaging techniques on individual
    predictions made by each of the individual models. The models created from the
    application of an ensembling technique along with the prediction consolidation
    technique is typically termed as an **ensemble**.
  prefs: []
  type: TYPE_NORMAL
- en: Each ML algorithm is special and has a unique way to model the underlying training
    data. For example, a k-nearest neighbors algorithm learns by computing distances
    between the elements in dataset; naive Bayes learns by computing the probabilities
    of each attribute in the data belonging to a particular class. Multiple models
    may be created using different ML algorithms and predictions can be done by combining
    predictions of several ML algorithms. Similarly, when a dataset is partitioned
    to create subsets and if multiple models are trained using an algorithm each focusing
    on one dataset, each model is very focused and it is specialized in learning the
    characteristics of the subset of data it is trained on. In both cases, with models
    based on multiple algorithms and multiple subsets of data, when we combine the
    predictions of multiple models through consolidation, we get better predictions
    as we leverage multiple strengths that each model in an ensemble carry. This,
    otherwise, is not obtained when using a single model for predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The crux of ensembling is that, better predictions are obtained when we combine
    the predictions of multiple models than just relying on one model for prediction.
    This is no different from the management philosophy that together we do better,
    which is otherwise termed as **synergy**!
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the core philosophy behind ensembling, we are now ready
    to explore the different types of ensembling techniques. However, we will learn
    the ensembling techniques by implementing them in a project to predict the attrition
    of employees. As we already know, prior to building any ML project, it is very
    important to have a deep understanding of the problem and the data. Therefore,
    in the next section, we first focus on understanding the attrition problem at
    hand, then we study the dataset associated with the problem, and lastly we understand
    the properties of the dataset through exploratory data analysis (EDA). The key
    insights we obtain in this section come from a one-time exercise and will hold
    good for all the ensembling techniques we will apply in the later sections.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get started with this section, you will have to download the `WA_Fn-UseC_-HR-Employee-Attrition.csv`
    dataset from the GitHub link for the code in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the attrition problem and the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HR analytics helps with interpreting organizational data. It finds out the people-related
    trends in the data and helps the HR department take the appropriate steps to keep
    the organization running smoothly and profitably. Attrition in a corporate setup
    is one of the complex challenges that the people managers and HR personnel have
    to deal with. Interestingly, machine learning models can be deployed to predict
    potential attrition cases, thereby helping the appropriate HR personnel or people
    managers take the necessary steps to retain the employee.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to build ML ensembles that will predict such potential
    cases of attrition. The job attrition dataset used for the project is a fictional
    dataset created by data scientists at IBM. The `rsample` library incorporates
    this dataset and we can make use of this dataset directly from the library.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a small dataset that has 1,470 records of 31 attributes. The description
    of the dataset can be obtained with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the `Attrition` target variable in the dataset run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Out of the 1,470 observations in the dataset, we have 1,233 samples (83.87%) that
    are non-attrition cases and 237 attrition cases (16.12%). Clearly, we are dealing
    with a *class imbalance* dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now visualize the highly correlated variables in the data through the
    `corrplot` library using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00693069-856f-49b3-b946-e3c7e53b1ac1.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, it may be observed that darker and larger blues
    dot in the cells indicate the existence of a strong correlation between the variables
    in the corresponding rows and columns that form the cell. High correlation between
    the independent variables indicates the existence of redundant features in the
    data. The problem of the existence of highly correlated features in the data is
    termed as **multicollinearity**. If we were to fit a regression model, then it
    is required that we treat the highly correlated variables from the data through
    some techniques such as removing the redundant features or by applying principal
    component analysis or partial least squares regression, which intuitively cuts
    down the redundant features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We infer from the output that the following variables are highly correlated
    and the person building the model needs to take care of these variables if we
    are to build a regression-based model:'
  prefs: []
  type: TYPE_NORMAL
- en: '`JobLevel`-`MonthlyIncome`; `JobLevel`-`TotalWorkingYears`; `MonthlyIncome`-`TotalWorkingYears`; `PercentSalaryHike`-`PerformanceRating`; `YearsAtCompany`-`YearsInCurrentRole`; `YearsAtCompany`-`YearsWithCurrManager`; `YearsWithCurrManager`-`YearsInCurrentRole`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, plot the various independent variables with the dependent `Attrition` variable
    in order to understand the influence of the independent variable on the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run the following command to get a graph view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf9f27c2-82ba-4b9c-b71e-067888466d2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding output, it can be observed that employees that work overtime
    are more prone to attrition when compared to the ones that do not work overtime:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s calculate the attrition of the employees by executing the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run the following command to get a graph view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1940a98-684a-401c-b1dc-2f2c8abece69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding output, it can be observed that employees that are single
    have more attrition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following command to get a graphical representation for the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output generated by running the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8469586f-dac2-4e9f-a5d3-229b91cb749c.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding output, it can be observed that the lab technicians, sales
    representatives, and employees working in human resources job roles have more
    attrition than other organizational roles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s execute the following commands to check with the impact of the gender
    of an employee over attribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to get a graphical representation for the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f15307f-e66a-43cf-bd52-de8ad0dd65ae.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding output, you can see that the gender of an employee does not
    have any impact on attrition, in other words attrition is observed to be the same
    across all genders.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s calculate the attribute of the employees from various fields by executing
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s execute the following command to get a graphical representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84dd7b0e-ee6a-49fe-aea9-c1e7b1d26f00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at the preceding graph, we can conclude that employees with a technical
    degree or a degree in human resources are observed to have more attrition. Take
    a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s execute the following command to check with the attribution of various
    departments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bd5d2df-9dd3-4e4c-aaae-0a027f4d7718.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at the preceding graph, we can conclude that the R and D department
    has less attrition compared to the sales and HR departments. Take a look at the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following command to get a graphical representation for the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1283f61e-dd01-4435-98da-9c8ca191a61a.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the preceding graph, we can conclude that employees with frequent
    travels are prone to more attrition compared to employees with a non-travel status
    or the ones that rarely travel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s calculate the overtime of the employees by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee279cb3-4192-40e4-b608-81e10487a03b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at the preceding graph, we can conclude that it can be observed that
    employees that are young (age < 35 ) and are single, but work overtime, are more
    prone to attrition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d910b21-22e9-4301-a211-b4ae423cff62.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the preceding graph, we can conclude that attrition is higher in
    employees that are young (age < 30) and most attrition is observed with employees
    that earn less than $7,500.
  prefs: []
  type: TYPE_NORMAL
- en: Although we have learned a number of important details about the data at hand,
    there is actually so much more to explore and learn. However, so as to move to
    the next step, we stop here at this EDA step. It should be noted that, in a real-world
    situation, data would not be so very clean as we see in this attrition dataset.
    For example, we would have missing values in the data; in which case, we would
    do missing values imputation. Fortunately, we have an impeccable dataset that
    is ready for us to create models without having to do any data cleansing or additional
    preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: K-nearest neighbors model for benchmarking the performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will implement the **k-nearest neighbors** (**KNN**) algorithm
    to build a model on our IBM attrition dataset. Of course, we are already aware
    from EDA that we have a class imbalance problem in the dataset at hand. However,
    we will not be treating the dataset for class imbalance for now as this is an
    entire area on its own and several techniques are available in this area and therefore
    out of scope for the ML ensembling topic covered in this chapter. We will, for
    now, consider the dataset as is and build ML models. Also, for class imbalance
    datasets, Kappa or precision and recall or the area under the curve of the receiver
    operating characteristic (AUROC) are the appropriate metrics to use. However,
    for simplicity, we will use *accuracy* as a performance metric. We will adapt
    10-fold cross validation repeated 10 times to avail the model performance measurement.
    Let''s now build our attrition prediction model with the KNN algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can see from the model output that the best performing model is when `k`
    `= 11` and we obtained an accuracy of 84% with this `k` value. In the rest of
    the chapter, while experimenting with several ensembling techniques, we will check
    if this 84% accuracy obtained from KNN will get beaten at all.
  prefs: []
  type: TYPE_NORMAL
- en: In a realistic project-building situation, just identifying the best hyperparameters
    is not enough. A model needs to be trained on a full dataset with the best hyperparameters
    and the model needs to be saved for future use. We will review these steps in
    the rest of this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the `caretmodel` object already has the trained model with `k
    = 11`, therefore we do not attempt to retrain the model with the best hyperparameter.
    To check the final model, you can query the model object with the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to save your best models to a file so that we can load them
    up later and make predictions on unseen data. A model can be saved to a local
    directory using the `saveRDS` R command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the `caretmodel` is saved as `production_model.rds` in the working
    directory. The model is now serialized as a file that can be loaded anytime and
    it can be used to score unseen data. Loading and scoring can be achieved through
    the following R code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Please note that `unseen_data` needs to be read prior to scoring through the
    `predict` command.
  prefs: []
  type: TYPE_NORMAL
- en: The part of the code where the final model is trained on the entire dataset,
    saving the model, reloading it from the file whenever required and scoring the
    unseen data collectively, is termed as building an ML productionalization pipeline.
    This pipeline remains the same for all ML models irrespective of the fact that
    the model is built using one single algorithm or using an ensembling technique.
    Therefore, in the later sections when we implement the various ensembling techniques,
    we will not cover the productionalization pipeline but just stop at obtaining
    the performance measurement through 10-fold cross validation repeated 10 times.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bootstrap aggregation or **bagging** is the earliest ensemble technique adopted
    widely by the ML-practicing community. Bagging involves creating multiple different
    models from a single dataset. It is important to understand an important statistical
    technique called bootstrapping in order to get an understanding of bagging.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping involves multiple random subsets of a dataset being created. It
    is possible that the same data sample gets picked up in multiple subsets and this
    is termed as **bootstrapping with replacement**. The advantage with this approach
    is that the standard error in estimating a quantity that occurs due to the use
    of whole dataset. This technique can be better explained with an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume you have a small dataset of 1,000 samples. Based on the samples, you
    are asked to compute the average of the population that the sample represents.
    Now, a direct way of doing it is through the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ffab3de-df4f-489c-9a25-be38913f6d33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As this is a small sample, we may have an error in estimating the population
    average. This error can be reduced by adapting bootstrap sampling with replacement.
    In the technique, we create 10 subsets of the dataset where each dataset has 100
    items in it. A data item may be randomly represented multiple times in a subset
    and there is no restriction on the number of times an item can be represented
    within a data subset as well as across the subsets. Now, we take the average of
    samples in each data subset, therefore, we end up with 10 different averages.
    Using all these collected averages, we estimate the average of the population
    with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab180bc6-78d6-497a-ae47-abeabca5e347.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we have a better estimate of the average as we have extrapolated the small
    sample to randomly generate multiple samples that are representative of the original population.
  prefs: []
  type: TYPE_NORMAL
- en: In bagging, the actual training dataset is split into multiple bags through
    bootstrap sampling with replacement. Assuming that we ended up with *n* bags,
    when an ML algorithm is applied on each of these bags, we obtain *n* different
    models. Each model is focused on one bag. When it comes to making predictions
    on new unseen data, each of these *n* models makes independent predictions on
    the data. A final prediction for an observation is arrived at by combining the
    predictions of the observation of all the *n* models. In case of classification,
    voting is adopted and the majority is considered as the final prediction. For
    regression, the average of predictions from all models is considered as the final
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Decision-tree-based algorithms, such as **classification and regression trees**
    (**CART**), are unstable learners. The reason is that a small change in the training
    dataset heavily impacts the model created. Model change essentially means that
    the predictions also change. Bagging is a very effective technique to handle the
    high sensitivity to data changes. As we can build multiple decision tree models
    on subsets of a dataset and then arrive at a final prediction based on predictions
    from each of the models, the effect of changes in data gets nullified or not experienced
    very significantly.
  prefs: []
  type: TYPE_NORMAL
- en: One intuitive problem experienced with building multiple models on subsets of
    data is **overfitting**. However, this is overcome by growing deep trees without
    applying any pruning on the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: A downside with bagging is that it takes longer to build the models when compared
    to building a model with a stand-alone ML algorithm. This is obvious because multiple
    models gets built in bagging, as opposed to one single model, and it takes time
    to build these multiple models.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's implement the R code to achieve a bagging ensemble and compare the
    performance obtained with that of the performance obtained from KNN. We will then
    explore the working mechanics of bagging methodology.
  prefs: []
  type: TYPE_NORMAL
- en: The `caret` library provides a framework to implement bagging with any stand-alone
    ML algorithm. `ldaBag`, `plsBag`, `nbBag`, `treeBag`, `ctreeBag`, `svmBag`, and
    `nnetBag` are some of the example methods provided in caret. In this section, we
    will implement bagging with three different `caret` methods such as `treebag`,
    `svmbag`, and `nbbag`.
  prefs: []
  type: TYPE_NORMAL
- en: Bagged classification and regression trees (treeBag) implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin, load the essential libraries and register the number of cores for
    parallel processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We can see that we achieved a better accuracy of 85.4% compared to 84% accuracy
    that was obtained with the KNN algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machine bagging (SVMBag) implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps of loading the libraries, registering multiprocessing, setting a
    working directory, reading data from a working directory, removing nondiscriminatory
    features from data, and setting up cross-validation parameters remain the same
    in the SVMBag and NBBag implementations as well. So, we do not repeat these steps
    in the SVMBag or NBBag code. Rather, we will focus on discussing the SVMBag or
    NBBag specific code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: You will see that we achieved an accuracy of 87.7%, which is much higher than
    the KNN model's 84% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes (nbBag) bagging implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now do the `nbBag` implementation by executing the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We see that in this case, we achieved only 83.89% accuracy, which is slightly
    inferior to the KNN model's performance of 84%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we have shown only three examples of the `caret` methods for bagging,
    the code remains the same to implement the other methods. The only change that
    is needed in the code is to replace the `fit`, `predict`, and `aggregate` parameters
    in `bagControl`. For example, to implement bagging with a neural network algorithm,
    we need to define `bagControl` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: It may be noted that an appropriate library needs to be available in R for `caret`
    to run the methods, otherwise it results in error. For example, `nbBag` requires
    the `klaR` library to be installed on the system prior to executing the code.
    Similarly, the `ctreebag` function needs the `party` package to be installed.
    Users need to check the availability of an appropriate library on the system prior
    to including it for use with the `caret` bagging.
  prefs: []
  type: TYPE_NORMAL
- en: We now have an understanding of implementing a project through bagging technique.
    The next subsection covers the underlying working mechanism of bagging. This will
    help get clarity in terms of what bagging did internally with our dataset so as
    to produce better performance measurements than that of stand-alone model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Randomization with random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we've seen in bagging, we create a number of bags on which each model is
    trained. Each of the bags consists of subsets of the actual dataset, however the
    number of features or variables remain the same in each of the bags. In other
    words, what we performed in bagging is subsetting the dataset rows.
  prefs: []
  type: TYPE_NORMAL
- en: In random forests, while we create bags from the dataset through subsetting
    the rows, we also subset the features (columns) that need to be included in each
    of the bags.
  prefs: []
  type: TYPE_NORMAL
- en: Assume that you have 1,000 observations with 20 features in your dataset. We
    can create 20 bags where each one of the bags has 100 observations (this is possible
    because of bootstrapping with replacement) and five features. Now 20 models are
    trained where each model gets to see only the bag it is assigned with. The final
    prediction is arrived at by voting or averaging based on the fact of whether the
    problem is a regression problem or a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Another key difference between bagging and random forests is the ML algorithm
    that is used to build the model. In bagging, any ML algorithm may be used to create
    a model however random forest models are built specifically using CART.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest modeling is yet another very popular machine learning algorithm.
    It is one of the algorithms that has proved itself multiple times as the best
    performing of algorithms, despite applying it on noisy datasets. For a person
    that has understood bootstrapping, understanding random forests is a cakewalk.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an attrition prediction model with random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s get our attrition model through random forest modeling by executing
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We see the best random forest model achieved a better accuracy of 86% compared
    to KNN's 84%.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A weak learner is an algorithm that performs relatively poorly—generally, the
    accuracy obtained with the weak learners is just above chance. It is often, if
    not always, observed that weak learners are computationally simple. Decision stumps
    or 1R algorithms are some examples of weak learners. Boosting converts weak learners
    into strong learners. This essentially means that boosting is not an algorithm
    that does the predictions, but it works with an underlying weak ML algorithm to
    get better performance.
  prefs: []
  type: TYPE_NORMAL
- en: A boosting model is a sequence of models learned on subsets of data similar
    to that of the bagging ensembling technique. The difference is in the creation
    of the subsets of data. Unlike bagging, all the subsets of data used for model
    training are not created prior to the start of the training. Rather, boosting
    builds a first model with an ML algorithm that does predictions on the entire
    dataset. Now, there are some misclassified instances that are subsets and used
    by the second model. The second model only learns from this misclassified set
    of data curated from the first model's output.
  prefs: []
  type: TYPE_NORMAL
- en: The second model's misclassified instances become input to the third model.
    The process of building models is repeated until the stopping criteria is met.
    The final prediction for an observation in the unseen dataset is arrived by averaging
    or voting the predictions from all the models for that specific, unseen observation.
  prefs: []
  type: TYPE_NORMAL
- en: There are subtle differences between the various and numerous algorithms in
    the boosting algorithms family, however we are not going to discuss them in detail
    as the intent of this chapter is to get a generalized understanding of ML ensembles
    and not to gain in-depth knowledge of various boosting algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: While obtaining better performance, measurement is the biggest advantage with
    the boosting ensemble; difficulty with model interpretability, higher computational
    times, and model overfitting are some of the issues encountered with boosting.
    Of course, these problems can be overruled through the use of specialized techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting algorithms are undoubtedly super-popular and are observed to be used
    by winners in many Kaggle and similar competitions. There are a number of boosting
    algorithms available such as **gradient boosting machines** (**GBMs**), **adaptive
    boosting** (**AdaBoost**) , gradient tree boosting, **extreme gradient boosting**
    (**XGBoost**), and **light gradient boosting machine** (**LightGBM**). In this
    section, we will learn the theory and implementation of two of the most popular
    boosting algorithms such as GBMs and XGBoost. Prior to learning the theoretical
    concept of boosting and its pros and cons, let's first start focusing on implementing
    the attrition prediction models with GBMs and XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: The GBM implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s implement the attrition prediction model with GBMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: You will see that with the GBM model, we have achieved accuracy above 87%, which
    is better accuracy compared to the 84% achieved with KNN.
  prefs: []
  type: TYPE_NORMAL
- en: Building attrition prediction model with XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s implement the attrition prediction model with XGBoost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d37882b4-f89b-42d7-a2d2-69f6c208a1fc.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Again, we observed that with XGBoost model, we have achieved an accuracy above
    87%, which is a better accuracy compared to the 84% achieved with KNN.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In all the ensembles we have learned about so far, we have manipulated the dataset
    in certain ways and exposed subsets of the data for model building. However, in
    stacking, we are not going to do anything with the dataset; instead we are going
    to apply a different technique that involves using multiple ML algorithms instead. In
    stacking, we build multiple models with various ML algorithms. Each algorithm
    possesses a unique way of learning the characteristics of data and the final stacked
    model indirectly incorporates all those unique ways of learning. Stacking gets
    the combined power of several ML algorithms through getting the final prediction
    by means of voting or averaging as we do in other types of ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: Building attrition prediction model with stacking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s build an attrition prediction model with stacking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6659bd76-cb08-42b4-9b50-2fb804feba66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see from the correlation table results that none of the individual ML
    algorithm predictions are highly correlated. Very highly correlated results mean
    that the algorithms have produced very similar predictions. Combining the very
    similar predictions may not really yield significant benefit compared with what
    one would avail from accepting the individual predictions. In this specific case,
    we can observe that none of the algorithm predictions are highly correlated so
    we can straightforwardly move to the next step of stacking the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'With GLM-based stacking, we have 88% accuracy. Let''s now examine the effect
    of using random forest modeling instead of GLM to stack the individual predictions
    from each of the five ML algorithms on the observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We see that without much effort, we were able to achieve an accuracy of 91%
    by stacking the predictions. Now, let's explore the working principle of stacking.
  prefs: []
  type: TYPE_NORMAL
- en: At last, we have discovered the various ensembling techniques that can provide
    us with better performing models. However, before ending the chapter, there are
    a couple of things we need to take a note of.
  prefs: []
  type: TYPE_NORMAL
- en: There is not just one way to implement ML models in R. For example, bagging
    can be implemented using functions available in the `ipred` library and not by
    using `caret` as we did in this chapter. We should be aware that hyperparameter
    tuning forms an important part of model building to avail the best performing
    model. The number of hyperparameters and the acceptable values for those hyperparameters
    vary depending on the library that we intend to use. This is the reason why we
    paid less attention to hyperparameter tuning in the models we built in this chapter.
    Nevertheless, it is very important to read up the library documentation to understand
    the hyperparameters that can be tuned with a library function. In most cases,
    incorporating hyperparameter tuning in models significantly improves the model's
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To recollect, we were using a class-imbalanced dataset to build the attrition
    model. Using techniques to resolve the class imbalance prior to model building
    is another key aspect of getting better model performance measurements. We used
    bagging, randomization, boosting, and stacking to implement and predict the attrition
    model. We were able to accomplish 91% accuracy just by using the features that
    were readily available in the models. Feature engineering is a crucial aspect
    whose role cannot be ignored in ML models. This may be one other path to explore
    to improve model performance further.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the secret recipe of recommending products
    or content through building a personalized recommendation engines. I am all set
    to implement a project to recommend jokes. Turn to the next chapter to continue
    the journey of learning.
  prefs: []
  type: TYPE_NORMAL
