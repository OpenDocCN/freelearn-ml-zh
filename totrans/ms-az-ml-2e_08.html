<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer178">
			<h1 id="_idParaDest-103"><em class="italic"><a id="_idTextAnchor102"/>Chapter 6</em>: Feature Engineering and Labeling</h1>
			<p>In the previous chapter, we learned how to clean our data and do basic statistical analysis. In this chapter, we will delve into two more types of actions we must perform before we can start our ML training. These two steps are the most important of all besides efficiently cleaning your dataset, and to be good at them, you will require a high amount of experience. This chapter will give you a basis to build upon.</p>
			<p>In the first section, we will learn about feature engineering. We will understand the process, how to select predictive features from our dataset, and what methods exist to transform features from our dataset to make them usable for our ML algorithm.</p>
			<p>In the second section, we will look at data labeling. Most ML algorithms fall into the category of supervised learning, which means they require labeled training data. We will look at some typical scenarios that require labels and learn how Azure Machine Learning can help with this tedious task.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Understanding and applying feature engineering</li>
				<li>Handling data labeling</li>
			</ul>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor103"/>Technical requirements</h1>
			<p>In this chapter, we will use the following Python libraries and versions to perform feature engineering on different datasets. </p>
			<ul>
				<li><strong class="source-inline">azureml-sdk 1.34.0 </strong></li>
				<li><strong class="source-inline">azureml-widgets 1.34.0 </strong></li>
				<li><strong class="source-inline">azureml-dataprep 2.20.0 </strong></li>
				<li><strong class="source-inline">pandas 1.3.2 </strong></li>
				<li><strong class="source-inline">numpy 1.19.5 </strong></li>
				<li><strong class="source-inline">scikit-learn 0.24.2 </strong></li>
				<li><strong class="source-inline">seaborn 0.11.2 </strong></li>
				<li><strong class="source-inline">plotly 5.3.1 </strong></li>
				<li><strong class="source-inline">umap_learn 0.5.1 </strong></li>
				<li><strong class="source-inline">statsmodels 0.13.0 </strong></li>
				<li><strong class="source-inline">missingno 0.5.0</strong> </li>
			</ul>
			<p>Similar to previous chapters, you can execute this code using either a local Python interpreter or a notebook environment hosted in Azure Machine Learning. </p>
			<p>All code examples in this chapter can be found in the GitHub repository for this book: <a href="https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter06">https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter06</a>. </p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor104"/>Understanding and applying feature engineering</h1>
			<p><strong class="bold">Feature engineering</strong> is the general term that describes the process of transforming existing features in our <a id="_idIndexMarker802"/>dataset, creating missing features, and eventually selecting the most predictive features from our dataset to start the ML training process with a given ML algorithm. These cannot just be seen as some mathematical functions we must apply to our data. This is an art form and doing it well makes the difference between a mediocre and highly performing predictive model. If you want to understand where you should invest your time, feature engineering is the step where you can have the most impact on the quality of your final ML model. To create this impact and be efficient, we must consider the following:</p>
			<ul>
				<li><strong class="bold">ML algorithm requirements</strong>: Do the<a id="_idIndexMarker803"/> features have to be in a specific format or range? How do I best avoid overfitting and underfitting the model?</li>
				<li><strong class="bold">Domain knowledge</strong>: Are the given<a id="_idIndexMarker804"/> features sufficient for our model? Can we create additional features or derive features that contain more predictive information?</li>
			</ul>
			<p>In this section, we'll define the different classes of feature engineering techniques and then look at some of the most prominent methods to apply to different types of datasets.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Keep in mind that the usefulness of a specific feature engineering method depends on the utilized type of features (categorical, continuous, text, image, audio) and the chosen ML algorithm.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor105"/>Classifying feature engineering techniques</h2>
			<p>Broadly speaking, feature<a id="_idIndexMarker805"/> engineering methods can be grouped into the following categories:</p>
			<ul>
				<li><strong class="bold">Feature creation</strong>: Create<a id="_idIndexMarker806"/> new features from the given set of features or additional information sources.</li>
				<li><strong class="bold">Feature transformation</strong>: Transform <a id="_idIndexMarker807"/>single features to make them useful and stable for the utilized ML algorithm.</li>
				<li><strong class="bold">Feature extraction</strong>: Create derived features <a id="_idIndexMarker808"/>from the original data.</li>
				<li><strong class="bold">Feature selection</strong>: Choose the most<a id="_idIndexMarker809"/> prominent and predictive features.</li>
			</ul>
			<p>Let's look at each of these <a id="_idIndexMarker810"/>categories and what they entail.</p>
			<h3>Feature creation</h3>
			<p>The first step to take in<a id="_idIndexMarker811"/> feature engineering is finding all the features that should be included in the model. To be good at this, you must have an intimate understanding of the <a id="_idIndexMarker812"/>relevant domain or know someone who is a <strong class="bold">subject matter expert</strong> (<strong class="bold">SME</strong>) in the domain. In the end, we want to be sure that <a id="_idIndexMarker813"/>we consider any type of data point that is predictive and that is feasible to acquire in a reasonable amount of time.</p>
			<p>In turn, we must understand all the methods that can help us create new features in our dataset, either taken from additional sources or the initial dataset. Typically, these methods can be classified as follows:</p>
			<ul>
				<li><strong class="bold">Adding missing predictive features</strong>: We add external information that is missing to achieve a more predictive model.</li>
				<li><strong class="bold">Combining the available features</strong>: We create new features by combining already available features in our dataset.</li>
			</ul>
			<p>Why do we have to change already existing features in our dataset?</p>
			<p>The reason for this is that a lot of connections between features and labels, that we understand, may not be clear to the utilized ML algorithm. Therefore, it is a good idea to think about what features or representations of the available features we would assume are necessary to make it easier for the ML algorithm to grasp the intrinsic connections.</p>
			<p>Let's look at some examples to understand this better. </p>
			<p>Imagine that you have a dataset for predicting house prices, like the one we examined in <a href="B17928_05_ePub.xhtml#_idTextAnchor085"><em class="italic">Chapter 5</em></a>, <em class="italic">Performing Data Analysis and Visualization</em>. Furthermore, imagine that the features we have are the <strong class="bold">length</strong> and <strong class="bold">width</strong> of the house or apartment. In this case, it is probably<a id="_idIndexMarker814"/> useful to combine these two features to create a new one called the <strong class="bold">surface area</strong>. In addition, if the <strong class="bold">type</strong> of building is missing (house, flat, condo, and so on), we may want to <a id="_idIndexMarker815"/>add this from other sources since <a id="_idIndexMarker816"/>we know the type has an impact on the price of a property.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">If you create new features from existing ones, it is typically wise to only stick with the newly created feature by dropping those initial features from the dataset.</p>
			<p>Now, imagine the amount of money a person spends throughout their life. Being young, this might be very little. When they grow older, they may have mortgages and children and eventually, their spending may drop when their children move out of the house, and they are nearing retirement. As this would form something of a parabolic relationship between <strong class="bold">age</strong> and <strong class="bold">cost of living</strong>, it may not be easy for an ML algorithm to grasp this. Therefore, one possible option is to square the values of the <strong class="bold">cost of living</strong> feature to emphasize higher costs and deemphasize lower costs.</p>
			<p>In the previous two examples, we used our domain knowledge to create new features. But what if we do not have this at our disposal? </p>
			<p>There is a way to create new features<a id="_idIndexMarker817"/> mathematically using the so-called <strong class="bold">polynomial extension</strong>. The idea is to create new features by raising the value of a feature to a certain power and multiplying it by one or multiple other features. Here, we define the <strong class="bold">degree</strong> as the maximum power a single feature can be raised to, and we define the <strong class="bold">order</strong> as the number of features we allow to be multiplied by each other. The following diagram shows all the possible combinations for a degree of 2 and order of 2 on the left-hand side, and a degree of 3 and order of 3 on the right-hand side:</p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B17928_06_01.jpg" alt="Figure 6.1 – Possible combinations for polynomial extension  (degree=2, order=2 on the left/degree=2, order=3 on the right) " width="1298" height="447"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – Possible combinations for polynomial extension (degree=2, order=2 on the left/degree=2, order=3 on the right)</p>
			<p>You should only consider a<a id="_idIndexMarker818"/> maximum order of 3 because, as shown in the preceding diagram, even with a degree of 2, this operation already <a id="_idIndexMarker819"/>creates too many combinations. Still, this automatic process may lead to much better predictive features than the originating ones. </p>
			<p>To try this method, you can use the <strong class="source-inline">PolynomialFeatures</strong> class from the <strong class="source-inline">sklearn</strong> library (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html</a>). </p>
			<p>With all these methods in mind, we can create new features in our dataset that might be easier for our ML algorithm to handle and contain more precise, predictive information.</p>
			<p>Next, let's look at some methods that let us change a single feature by transforming its values or its representation.</p>
			<h3>Feature transformation</h3>
			<p><strong class="bold">Feature transformation</strong> is about<a id="_idIndexMarker820"/> manipulating a <a id="_idIndexMarker821"/>feature to change its value or create a new representation of the same. The following list covers the types of transformations we can perform on single features:</p>
			<ul>
				<li><strong class="bold">Discretization</strong>: Divide feature<a id="_idIndexMarker822"/> values into different groups or intervals to reduce complexity. This can be done on numerical or categorical features.</li>
				<li><strong class="bold">Splitting</strong>: Split a feature into <a id="_idIndexMarker823"/>multiple elements. This is typically done on datetime and string values.</li>
				<li><strong class="bold">Categorical encoding</strong>: Represent <a id="_idIndexMarker824"/>a categorical feature numerically, by creating new numerical features while following specific methods.</li>
				<li><strong class="bold">Scaling</strong>: Transform a <a id="_idIndexMarker825"/>continuous feature so that it fits into a specified range of values.</li>
				<li><strong class="bold">Standardization</strong>: Transform a <a id="_idIndexMarker826"/>continuous feature so that it represents a normal distribution with a mean of 0 and a standard deviation of 1.</li>
				<li><strong class="bold">Normalization</strong>: Transform <a id="_idIndexMarker827"/>a vector (row) of multiple continuous features individually into a so-called unit norm (unit magnitude).</li>
				<li><strong class="bold">Mathematical transformation</strong>: Transform <a id="_idIndexMarker828"/>a continuous feature by applying a specific mathematical function to it (<strong class="source-inline">square</strong>, <strong class="source-inline">square root</strong>, <strong class="source-inline">exp</strong>, <strong class="source-inline">log</strong>, and so on).</li>
			</ul>
			<p>In <a href="B17928_05_ePub.xhtml#_idTextAnchor085"><em class="italic">Chapter 5</em></a>, <em class="italic">Performing Data Analysis and Visualization</em>, we used the <strong class="source-inline">log</strong> function to calculate the logarithm of all house price values. We did this to reduce the impact that a handful of outliers would have on our ML training. Therefore, the main reason to transform features is to <a id="_idIndexMarker829"/>adapt the feature to the possible mathematical requirements of the given ML algorithm. Often, you may run into the following requirements of the ML algorithm:</p>
			<ul>
				<li><strong class="bold">Numerical format</strong>: The algorithm<a id="_idIndexMarker830"/> requires all the features to be numerical.</li>
				<li><strong class="bold">Same scale</strong>: The algorithm requires all <a id="_idIndexMarker831"/>the predictive features to be on the same scale, maybe even with a mean of 0 and a standard deviation of 1.</li>
				<li><strong class="bold">Mathematical theory</strong>: The domain itself <a id="_idIndexMarker832"/>may require certain transformations based on mathematical theory. For example, a price feature for predictions concerning economic theory should nearly always be transformed with the natural logarithm.</li>
				<li><strong class="bold">Computational limits</strong>: The algorithm <a id="_idIndexMarker833"/>may require each feature value to have a small scale. Such algorithms often require values to be in an interval of <strong class="source-inline">[-1,1]</strong>.</li>
				<li><strong class="bold">Complexity</strong>: Most algorithms<a id="_idIndexMarker834"/> require very precise features. Therefore, reducing the complexity of the possible values a feature can take is often worthwhile.</li>
			</ul>
			<p>An example would be discretizing <a id="_idIndexMarker835"/>features. One such <a id="_idIndexMarker836"/>method is called <strong class="bold">binning</strong>, which transforms numerical continuous values into a handful of discrete values. We will see this in action on text data in <a href="B17928_07_ePub.xhtml#_idTextAnchor112"><em class="italic">Chapter 7</em></a>, <em class="italic">Advanced Feature Extraction with NLP</em>. </p>
			<p>Another example would be splitting datetime features. Imagine that we want to predict the amount of traffic on a certain road at specific times of the day. Let's assume that we got a feature denoting the <strong class="bold">date and time</strong> of our recording and the <strong class="bold">number of cars</strong> we tracked at that point. To make a better prediction, one idea would be to create three new features, denoting whether it is a <em class="italic">workday</em>, <em class="italic">weekend</em>, or <em class="italic">holiday</em>. There will be less traffic on a Sunday at 7 A.M. compared to a workday morning at 7 A.M. </p>
			<p>Let's learn how to perform this transformation. The following screenshot shows our initial small dataset and the first transformation adding <strong class="source-inline">day of the week</strong>:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B17928_06_02.jpg" alt="Figure 6.2 – Dataset with a new weekday feature   " width="579" height="219"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Dataset with a new weekday feature  </p>
			<p>In the next step, we must enrich the data by adding a new categorical feature called <strong class="source-inline">daytype</strong>, which denotes whether a <a id="_idIndexMarker837"/>day is either a weekday, a weekend, or a holiday:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B17928_06_03.jpg" alt="Figure 6.3 – Dataset enrichment  " width="597" height="217"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – Dataset enrichment </p>
			<p>Theoretically, we are done. But our ML algorithm may beg to differ here. Our ML model may make up a natural order for our categorical data that does not exist or it simply cannot handle categorical data. In this case, it is prudent to <strong class="bold">encode</strong> our categorical data with numerical values. One<a id="_idIndexMarker838"/> such method is called <strong class="bold">one-hot encoding</strong>, which transforms a categorical feature into multiple numerical features by creating a new feature with two valid values (0 or 1) for every existing category. The following screenshot shows this encoding for our example:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B17928_06_04.jpg" alt=" Figure 6.4 – One-hot encoding the new feature " width="704" height="214"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 6.4 – One-hot encoding the new feature</p>
			<p>Here, we created three new features named <strong class="source-inline">holiday</strong>, <strong class="source-inline">weekday</strong>, and <strong class="source-inline">weekend</strong>, each representing our initial categories. Where a sample had this initial category, the value of that feature is set to <strong class="source-inline">1</strong>; otherwise, it is set to <strong class="source-inline">0</strong>.</p>
			<p>What have we done in this<a id="_idIndexMarker839"/> example? We transformed a very unintuitive datetime feature into something with more predictive power by splitting the<a id="_idIndexMarker840"/> feature into components, adding external knowledge through feature creation, and performing categorical encoding on the created feature. </p>
			<p>Now that we have a good grasp of feature transformation, let's look at what falls under feature extraction.</p>
			<h3>Feature extraction</h3>
			<p>With <strong class="bold">feature extraction</strong>, we group all the<a id="_idIndexMarker841"/> methods that do not manipulate features by simple means but extract useful information from a high-dimensional dataset. This is typically done by using complex mathematical algorithms or ML algorithms. </p>
			<p>Extraction is often required when the underlying dataset is too complex to be processed, so it needs to be brought into a simplified form while keeping its predictive value.</p>
			<p>The following are some typical extraction types for different scenarios:</p>
			<ul>
				<li><strong class="bold">High-dimensional reduction</strong>: Create representative<a id="_idIndexMarker842"/> features based on an <em class="italic">n</em>-dimensional dataset.</li>
				<li><strong class="bold">Feature detection</strong>: Find points of interest in <a id="_idIndexMarker843"/>every image in an image dataset.</li>
				<li><strong class="bold">Word embeddings</strong>: Create numeric <a id="_idIndexMarker844"/>encodings for words in a text dataset.</li>
				<li><strong class="bold">Signal processing</strong>: Extract the<a id="_idIndexMarker845"/> characteristics of sound waves from an audio dataset.</li>
			</ul>
			<p>We discussed high-dimensional reduction methods in <a href="B17928_05_ePub.xhtml#_idTextAnchor085"><em class="italic">Chapter 5</em></a>, <em class="italic">Performing Data Analysis and Visualization</em>, when we looked at visualizing high-dimensional datasets. In a process like <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>), the dataset is projected onto a two- or three-dimensional space by <a id="_idIndexMarker846"/>creating principal component vectors. Instead of only using this method for visualization, we could use these calculated vectors as derived and less complex features that represent our dataset.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">High-dimensional reduction techniques can be used for feature extraction, but keep in mind that we lose our intrinsic understanding of the features. Instead of features called suburbs or rooms, we end up with features called Principal Component 1 and Principal Component 2.</p>
			<p>Looking at the other scenarios, it seems that extraction typically happens when we are working with complex datasets made up of text, image, or audio data. In all these cases, there are specific methods to consider when extracting information from the raw data. </p>
			<p>In the case of an image dataset, we might be interested in key areas or points of interest, including finding edges and objects. In <a href="B17928_10_ePub.xhtml#_idTextAnchor165"><em class="italic">Chapter 10</em></a>, <em class="italic">Training Deep Neural Networks on Azure</em>, you will see that such image extraction steps are done automatically by <strong class="bold">deep neural networks</strong>, removing the <a id="_idIndexMarker847"/>need to perform manual feature extraction on images in a lot of cases.</p>
			<p>In the case of text data, we can use <a id="_idIndexMarker848"/>extraction methods such as <strong class="bold">bag of words</strong> and <strong class="bold">TF-IDF</strong>, both of which help create numerical representations of text, capturing meaning and semantic relationships. We will have an in-depth look at these methods in <a href="B17928_07_ePub.xhtml#_idTextAnchor112"><em class="italic">Chapter 7</em></a>, <em class="italic">Advanced Feature Extraction with NLP</em>.</p>
			<p>In the case of audio data, we can use signal processing to extract information and new features from the source. In this scenario, there are also two domains – the time domain and the frequency domain – that we can pull information from. From the time domain, we would typically extract <a id="_idIndexMarker849"/>something like the <strong class="bold">amplitude envelope</strong>, which is the maximum amplitude of the signal per frame, the <strong class="bold">root mean square energy</strong>, which hints at the<a id="_idIndexMarker850"/> loudness of the signal, and the <strong class="bold">zero-crossing rate</strong>, which is <a id="_idIndexMarker851"/>the number of times the wave is crossing the horizontal time axis. If you must work with data from this domain, make yourself comfortable with such processing techniques.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">A lot of feature extraction and feature transformation techniques are already embedded in common ML frameworks and algorithms, removing the need for you to manually touch features. Have a good understanding of what the algorithm does by itself and what you need to do manually when you're preprocessing.</p>
			<p>So far, we've learned how to create new features, transform features, and extract features from our dataset. Now, let's look at some methods that can help us select the most predictive feature from our feature set.</p>
			<h3>Feature selection</h3>
			<p>With <strong class="bold">feature selection</strong>, we define all the <a id="_idIndexMarker852"/>methods that help us understand how valuable and predictive a feature is for the target so that we can choose a useful subset of our feature variables for training. The reasons to reduce complexity are two-fold. On the one hand, we want the simplicity to make the model <strong class="bold">explainable</strong> while on the other, we want to avoid <strong class="bold">overfitting</strong> the model. With too much input information, we will end up with a model that, in most cases, will perfectly fit our training data and nothing else but will perform poorly on unseen data.</p>
			<p>Generally, there are three different types of feature selection methods, as follows:</p>
			<ul>
				<li><strong class="bold">Filter-based methods</strong>: These define a derived metric, that is not the target error rate, to measure the quality of a <a id="_idIndexMarker853"/>subset of features.</li>
				<li><strong class="bold">Wrapper-based methods</strong>: These use <a id="_idIndexMarker854"/>greedy search algorithms to run a prediction model on different combinations of feature subsets.</li>
				<li><strong class="bold">Embedded methods</strong>: These are<a id="_idIndexMarker855"/> specific selection methods that are already embedded into our final ML model.</li>
			</ul>
			<p>Filter-based methods can be very <a id="_idIndexMarker856"/>efficient in terms of computational resources but are only evaluated against a simpler filter. Typically, statistical measures such as correlation, mutual information, and entropy are used as metrics in these approaches. </p>
			<p>On the other hand, wrapper-based methods are computationally intense. At the same time, they can find a great performing feature set since the same error function or metric is being used for the selection of the features as the one that's being used in the actual model training. The downside of this approach is that without an independent metric, the selected subset is only <a id="_idIndexMarker857"/>useful for the chosen ML training algorithm. Typically, this is done by performing one of the following processes:</p>
			<ul>
				<li><strong class="bold">Step forward feature selection</strong>: Features are added one by one based on the training results of each feature until the model does not improve its performance.</li>
				<li><strong class="bold">Step backward feature selection</strong>: The model is evaluated with the full set of features. These features are subsequently removed until a predefined number of features is reached. This removal is done in a round-robin fashion.</li>
				<li><strong class="bold">Exhaustive feature selection</strong>: All the feature subsets are evaluated, which is the most expensive method. </li>
			</ul>
			<p>Finally, a selection method is called an embedded method when the selection step is part of the model learning algorithm itself. Embedded methods often combine the qualities of filter and wrapper methods through the fact that the learning algorithm takes advantage of its selection process and performs selection and training at the same time. Typical <a id="_idIndexMarker858"/>examples of embedded <a id="_idIndexMarker859"/>methods are ensemble models, <strong class="bold">Lasso</strong>, and <strong class="bold">Ridge</strong>.</p>
			<p>You may have realized this by now, but we used such methods in <a href="B17928_05_ePub.xhtml#_idTextAnchor085"><em class="italic">Chapter 5</em></a>, <em class="italic">Performing Data Analysis and Visualization</em>. The <strong class="bold">Pearson correlation coefficient</strong> we used for<a id="_idIndexMarker860"/> generating a correlation matrix is a derived metric, so it falls under the filter-based selection methods. In addition, we used an <strong class="bold">ensemble decision tree model</strong> to <a id="_idIndexMarker861"/>calculate feature importance for our dataset. This helped us get a clear understanding of which features may have more influence on the target than others. This ensemble<a id="_idIndexMarker862"/> method utilizes the <strong class="bold">random forest</strong> approach. A random forest not <a id="_idIndexMarker863"/>only implements the so-called <strong class="bold">bagging</strong> technique to<a id="_idIndexMarker864"/> randomly select a subset of samples to train on but also takes a random selection of features rather than using all the features to grow each tree. Therefore, for feature selection, random forests fall into the embedded category.</p>
			<p>We will have a more detailed look at the tree-based ensemble classifier, as well as bagging and boosting, in <a href="B17928_09_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 9</em></a>, <em class="italic">Building ML Models Using Azure Machine Learning</em>.</p>
			<p>Besides all these mathematical approaches to feature selection, sometimes, a more manual approach might be far superior. For example, when we removed the postal code from our <strong class="bold">Melbourne housing dataset</strong> in <a href="B17928_05_ePub.xhtml#_idTextAnchor085"><em class="italic">Chapter 5</em></a>, <em class="italic">Performing Data Analysis and Visualization</em>, we did so because we understood that the postal code and the suburbs contain the same information, which made them redundant. We did this because we have domain knowledge and understand the relationship between postal codes and suburbs. Note that this additional knowledge lessens the burden for the model to learn these connections by itself. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">For feature engineering, the more outside knowledge about the data or the domain, the simpler a lot of these preprocessing steps can get, or they become avoidable altogether.</p>
			<p>We will iterate this notion throughout this book as it needs to be ingrained into everything you do so that you get more efficient and better at working with data.</p>
			<p>We now have a general understanding of the general types of feature engineering we can perform. In the next section, we will provide an overview of the most prominent methods and drill deeper into some of them.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor106"/>Discovering feature transformation and extraction methods</h2>
			<p>Now that we have a <a id="_idIndexMarker865"/>good grasp of the types of feature engineering action we can<a id="_idIndexMarker866"/> apply to our feature, let's look at some of the most prominent feature engineering techniques and their names. The following table provides a good overview of most of the well-known methods in the different categories we have learned about:</p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B17928_06_05.jpg" alt="Figure 6.5 – Overview of different feature engineering methods  " width="1069" height="367"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Overview of different feature engineering methods </p>
			<p>Keep in mind that this list is far from exhaustive and as we mentioned previously, some of these methods are already implemented as part of specific ML algorithms. </p>
			<p>In the following sections, we will look at some of these. Feel free to download the <strong class="source-inline">01_feateng_examples.ipynb</strong> file in the GitHub repository for this chapter, which contains the code for the upcoming examples. If you would like to learn more about some of the feature extraction methods we will cover, we will come back to them in the upcoming chapters. For the methods we won't cover, feel free to research them.</p>
			<h3>Scaling, standardization, and normalization</h3>
			<p>Since all the scaling and <a id="_idIndexMarker867"/>normalization methods are very similar to each other, we will discuss all of them in detail here.</p>
			<p>Let's begin with the so-called <strong class="bold">StandardScaler</strong>. This <a id="_idIndexMarker868"/>scaling transforms our feature values<a id="_idIndexMarker869"/> so that the resulting value distribution has a mean (µ) of 0 and a standard deviation (s) of 1. The formula to apply to each value looks like this:</p>
			<p><img src="image/Formula_06_01.png" alt="" width="215" height="90"/></p>
			<p>Here, µ is the mean value of the given distribution and s is the standard deviation of the given distribution. With this, we can convert every value, <img src="image/Formula_06_02.png" alt="" width="38" height="35"/>, into a new scaled value, <img src="image/Formula_06_03.png" alt="" width="35" height="35"/>. </p>
			<p>The following diagram shows how this scaler changes the shape of multiple distributions:</p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B17928_06_06.jpg" alt="Figure 6.6 – StandardScaler distribution (left: before scaling, right: after scaling) " width="1259" height="356"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – StandardScaler distribution (left: before scaling, right: after scaling)</p>
			<p>You should only use this scaler if the underlying distribution is <em class="italic">normally distributed</em>, as this is the requirement.</p>
			<p>Next, we will <a id="_idIndexMarker870"/>look at the <strong class="bold">MinMaxScaler</strong>. This scaling<a id="_idIndexMarker871"/> method is very similar to standardization, except that we are not working with the mean or standard deviation of the value distribution; instead, we are scaling the values to a range of [0,1] or [-1,1] (if negative values exist). Scaling a feature<a id="_idIndexMarker872"/> like this will often increase the<a id="_idIndexMarker873"/> performance of ML algorithms as they are typically better at handling small-scale values. </p>
			<p>Mathematically, this scaling is defined as follows:</p>
			<p><img src="image/Formula_06_04.png" alt="" width="340" height="104"/></p>
			<p>Here, <img src="image/Formula_06_05.png" alt="" width="90" height="34"/> defines the minimum value and <img src="image/Formula_06_06.png" alt="" width="97" height="34"/> defines the maximum value in our initial distribution. </p>
			<p>The MinMaxScaler is a good choice if the minimum and maximum values are well-defined – think about the color intensity in an RGB picture. Furthermore, we can change the formula to influence the resulting range of values.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The StandardScaler and the MinMaxScaler are both very susceptible to outliers in a distribution, which, in turn, can skew certain ML algorithms.</p>
			<p>A lot of ML algorithms pay more attention to large values, so they have a problem with outliers. A scaler<a id="_idIndexMarker874"/> fittingly named <strong class="bold">RobustScaler</strong> was defined to tackle this behavior. This <a id="_idIndexMarker875"/>scaler uses the <strong class="bold">interquartile range</strong> (<strong class="bold">IQR</strong>) instead of the standard deviation as a measure of dispersion and uses the <strong class="bold">median</strong> value<a id="_idIndexMarker876"/> instead of the mean value of the distribution as a measure of central tendency. The interquartile range denotes the middle 50% of the distribution, which means it is the difference between the 75<span class="superscript">th</span> percentile and the 25<span class="superscript">th</span> percentile.</p>
			<p>Therefore, the mathematical scaling function looks like this:</p>
			<p><img src="image/Formula_06_07.png" alt="" width="377" height="107"/></p>
			<p>Here, <img src="image/Formula_06_08.png" alt="" width="160" height="38"/> denotes the median of the distribution, <img src="image/Formula_06_09.png" alt="" width="107" height="55"/> denotes the value where the first quartile starts, and <img src="image/Formula_06_10.png" alt="" width="104" height="43"/> denotes the value where the third quartile starts.</p>
			<p>Why does this scaler work better with outliers? </p>
			<p>In the previous formulas, the<a id="_idIndexMarker877"/> biggest outlier would still be<a id="_idIndexMarker878"/> falling into the predefined interval because the maximum outlier would be <img src="image/Formula_06_11.png" alt="" width="97" height="34"/>. Therefore, the further the outlier is from the bulk of the data points, the more the center values would be scaled toward 0. On the other hand, with the RobustScaler, all the data points in the middle 50% would be scaled into the unit distance, and everything above or below this would be scaled to the appropriate values outside of the main interval while keeping the relative distance between the values in the middle of the distribution intact.</p>
			<p>Simply put, the median and the interquartile range are not influenced greatly by outliers, so this scaler is not influenced greatly by outliers. </p>
			<p>Let's look at all these scalars on a sample distribution. For this, we will take the <strong class="source-inline">Price</strong> column of the <strong class="bold">Melbourne Housing dataset</strong> we used in <a href="B17928_05_ePub.xhtml#_idTextAnchor085"><em class="italic">Chapter 5</em></a>, <em class="italic">Performing Data Analysis and Visualization</em>. The following table shows the statistical distribution for the <strong class="source-inline">Price</strong> column and the distribution resulting from applying each scaling method we've discussed:</p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B17928_06_07.jpg" alt="Figure 6.7 – Distribution scaled using multiple scaling methods " width="726" height="138"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – Distribution scaled using multiple scaling methods</p>
			<p>As we can see, <strong class="bold">StandardScaler</strong> creates a distribution with a mean of 0 and a standard deviation of 1, <strong class="bold">MinMaxScaler</strong> scales the values between 0 and 1, and <strong class="bold">RobustScaler</strong> sets the mean to 0. Looking <a id="_idIndexMarker879"/>at the box plots in <em class="italic">Figure 6.8</em> and <em class="italic">Figure 6.9</em>, we can see the differences in their distributions. Please note the<a id="_idIndexMarker880"/> scale of the <em class="italic">y</em> axis as well:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B17928_06_08.jpg" alt="Figure 6.8 – Box plot for StandardScaler and RobustScaler " width="708" height="471"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – Box plot for StandardScaler and RobustScaler</p>
			<p>Comparing the following box plot to <em class="italic">Figure 6.8</em>, we can see the difference in their distribution:</p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B17928_06_09.jpg" alt="Figure 6.9 – Box plot for MinMaxScaler " width="733" height="473"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.9 – Box plot for MinMaxScaler</p>
			<p>Now that we have some idea of how to scale a feature, let's talk about normalization.</p>
			<p><strong class="bold">Normalization</strong> is the process of<a id="_idIndexMarker881"/> taking a vector (row) of feature values and scaling them to a <strong class="bold">unit magnitude</strong>, typically to<a id="_idIndexMarker882"/> simplify mathematical <a id="_idIndexMarker883"/>processes such as <strong class="bold">cosine similarity</strong>. </p>
			<p>Let's start by understanding a process where this normalization step can be of help. The cosine similarity describes how similar two different vectors are to each other. In an n-dimensional room, are they pointing in the same direction, are they perpendicular to each other, or are they facing in the opposite direction? </p>
			<p>Such calculations can, for example, help us understand how similar text documents are to each other, by taking a vector of word counts or similar information and comparing them with each other.</p>
			<p>Therefore, to understand document similarity, we must calculate a cosine between vectors using the following formula:</p>
			<p><img src="image/Formula_06_12.png" alt="" width="311" height="104"/></p>
			<p>As you can see, to make this calculation, we must calculate the magnitude of each vector – for example, <img src="image/Formula_06_13.png" alt="" width="74" height="46"/>. This magnitude is defined as follows:</p>
			<p><img src="image/Formula_06_14.png" alt="" width="430" height="108"/></p>
			<p>This single vector magnitude calculation is quite expensive to perform. Now, imagine that we have a dataset that contains hundreds of thousands of documents. We would have to calculate this every time for every combination of vectors (samples) in our dataset. Wouldn't it be easier to have all these vector magnitudes equal to 1? This would greatly simplify the calculation of the cosine.</p>
			<p>Therefore, the idea is to normalize all the samples in our dataset to achieve a unit magnitude by scaling them appropriately, as follows:</p>
			<p><img src="image/Formula_06_15.png" alt="" width="605" height="132"/></p>
			<p>In this equation, <img src="image/Formula_06_16.png" alt="" width="85" height="44"/> denotes our initial vector, <img src="image/Formula_06_17.png" alt="" width="63" height="40"/> denotes the magnitude of the initial vector, and <img src="image/Formula_06_18.png" alt="" width="100" height="45"/> denotes our scaled vector with the unit magnitude.</p>
			<p>This normalization is called <strong class="bold">L2 Norm</strong> and is <a id="_idIndexMarker884"/>one of three typical normalization methods. Let's look at how the magnitude of a vector is calculated in this and all the other metrics:</p>
			<ul>
				<li><strong class="bold">L1 Norm</strong>: This calculates the <a id="_idIndexMarker885"/>magnitude as the sum of the absolute values of the vector components.</li>
				<li><strong class="bold">L2 Norm</strong>: This calculates the<a id="_idIndexMarker886"/> traditional vector magnitude (as described).</li>
				<li><strong class="bold">Max Norm</strong>: This calculates the magnitude as the<a id="_idIndexMarker887"/> absolute value of the elements of the vector.</li>
			</ul>
			<p>The L1 Norm and the Max Norm cannot be used for cosine similarity as they do not calculate the mathematically defined vector magnitude. So, let's look at how those two are calculated.</p>
			<p>The L1 Norm is mathematically defined as follows:</p>
			<p><img src="image/Formula_06_19.png" alt="" width="647" height="118"/></p>
			<p>The L1 Norm is often used to regularize the values in the dataset when you're fitting an ML algorithm. It keeps the coefficient small, which makes the model training process less complex.</p>
			<p>The Max Norm is mathematically defined as follows:</p>
			<p><img src="image/Formula_06_20.png" alt="" width="780" height="120"/></p>
			<p>The Max Norm is also used for regularization, typically in <strong class="bold">neural networks</strong> to keep the weights low at the<a id="_idIndexMarker888"/> connections between neurons. It also helps with performing less extreme backpropagation runs to stabilize the ML algorithm's learning.</p>
			<p>At this point, you should have a good grasp of the usefulness of scaling and normalization. Next, we'll look at some methods we can use to transform categorical values into numerical representations.</p>
			<h3>Categorical encoding</h3>
			<p>When we<a id="_idIndexMarker889"/> looked at feature transformation as a <a id="_idIndexMarker890"/>concept, we looked at an example where we applied <strong class="bold">one-hot encoding</strong>. This method creates new features with two possible values (0,1) for <em class="italic">every</em> available category in the initial categorical feature. This can be<a id="_idIndexMarker891"/> helpful, but a categorical feature of high cardinality would blow up the feature space dramatically. Therefore, when using this method, we must figure out if every single category is predictive or not. </p>
			<p>In our previous example, instead of using a category with the days of the week (Monday through Saturday), we opted for only three categories, namely weekday, weekend, and holiday. In such a scenario, one-hot encoding is quite helpful.</p>
			<p>Besides this method, there are other ways to encode<a id="_idIndexMarker892"/> categorical features. The most basic of them would be <strong class="bold">label encoding</strong>. In label encoding, we replace every category with a numeric label (0,..,<em class="italic">n</em>), thus making it a numeric feature. Through this, we did not add any additional information to this feature.</p>
			<p>The next idea would be to add some intrinsic information from the whole dataset and ingrain it into the values we must encode. Some options for this idea are as follows:</p>
			<ul>
				<li><strong class="bold">Count encoding</strong>: Replace each<a id="_idIndexMarker893"/> category with the absolute number of observations of this category in the whole dataset. </li>
				<li><strong class="bold">Frequency encoding</strong>: Replace each <a id="_idIndexMarker894"/>category with the relative number (the percentage) of observations of this category in the whole dataset.</li>
				<li><strong class="bold">Target encoding</strong>: Replace each<a id="_idIndexMarker895"/> category with the mean value of the target that's been calculated from each entry of this category throughout the whole dataset.</li>
			</ul>
			<p>To understand these methods, let's assume that we have a dataset that contains the favorite snack item of 25 people as one of the features and their likelihood of buying a new snack product a company produces as the target. The following table shows the original values and all three encodings we have discussed:</p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/B17928_06_10.jpg" alt="Figure 6.10 – Count, frequency, and target encoding example " width="788" height="702"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.10 – Count, frequency, and target encoding example</p>
			<p>With these methods, we can ingrain additional information into the feature, making it easier for an ML algorithm to understand relationships.</p>
			<p>Finally, let's talk about <strong class="bold">rare label encoding</strong>. This technique is used to replace every rare category in a<a id="_idIndexMarker896"/> categorical feature with a single label called <strong class="source-inline">Rare</strong>, thus grouping them into one category. This helps reduce the overall complexity and should especially be done if the <strong class="source-inline">Rare</strong> category will still be a small part of the overall category distribution. You can compare this to grouping small parties under the <em class="italic">Others</em> label in an election graph, while primarily showing the major parties.</p>
			<p>At this point, you should have a good understanding of different encoding techniques. In the next section, we will discuss how we can try out these techniques on a real dataset.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor107"/>Testing feature engineering techniques on a tabular dataset</h2>
			<p>In <a href="B17928_05_ePub.xhtml#_idTextAnchor085"><em class="italic">Chapter 5</em></a>, <em class="italic">Performing Data Analysis and Visualization</em>, we did some cleaning and statistical analysis on the <strong class="bold">Melbourne Housing dataset</strong>. After looking through a set of possible feature <a id="_idIndexMarker897"/>engineering methods in the previous section, you may have realized that we used some of these methods when we were working with our dataset.</p>
			<p>As an exercise, think about where we left off and, keeping the feature engineering options in mind, what we could do now to create new useful features, transform the given features, and eventually select the most prominent and predictive features in our dataset.</p>
			<p>For inspiration, have a look at the <strong class="source-inline">02_fe_melbhousing.ipynb</strong> file in the GitHub repository for this chapter.</p>
			<p>In the final section of this chapter, we will leave the feature space behind and concentrate on the target or label for our ML training – to be more precise, on the cases where we are missing the labels.</p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor108"/>Handling data labeling</h1>
			<p>In this section, we will look at one<a id="_idIndexMarker898"/> of the most time-consuming and important tasks when it comes to preprocessing our dataset for ML training: <strong class="bold">data labeling</strong>. As we learned while looking at high-dimensional reduction and other ML techniques in <a href="B17928_05_ePub.xhtml#_idTextAnchor085"><em class="italic">Chapter 5</em></a>, <em class="italic">Performing Data Analysis and Visualization</em>, for most scenarios, it is vitally important to have labels attached to our samples. As we discussed in <a href="B17928_01_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Understanding the End-to-End Machine Learning Process</em>, there are only a few scenarios where unsupervised learning models are sufficient, such as a model that clusters emails as spam or not spam. In most cases, we want to use a supervised model, which means we will require labels.</p>
			<p>In the following sections, we will discuss what scenarios require us to do manual labeling and how Azure Machine Learning can help us be as efficient as possible to perform this monotonous task.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor109"/>Analyzing scenarios that require labels</h2>
			<p>We will start by<a id="_idIndexMarker899"/> looking at the types of datasets we have discussed so far and in which scenarios we will need to perform manual labeling.</p>
			<h3>Numerical and categorical data</h3>
			<p>As we saw when we worked with the <strong class="bold">Melbourne Housing dataset</strong>, for tabular datasets, we may often have a column that can be used as the label. In our case, it was the price column that <a id="_idIndexMarker900"/>we could use as a label since our goal for ML was to predict house prices based on specific feature inputs.</p>
			<p>But even if this column was missing, we could have incorporated other datasets, such as one that shows the mean price for houses in different suburbs of Melbourne, to calculate a reasonable value for each of our dataset samples.</p>
			<p>Therefore, the main advantage over any of the other scenarios we will discuss next is that in a dataset made up of numerical and categorical features with clear meaning (not the pixel values of an image), we can use logic and mathematical functions to create a numerical label, or we can classify samples into a categorical label in an automated fashion. This means we do not have to look at every sample manually to define its label. </p>
			<h3>Natural language processing </h3>
			<p>Let's start by looking at text data. You <a id="_idIndexMarker901"/>may think that a categorical entry would also be text in a sense, but typically, categorical data can also be exchanged with mathematical values without you losing much.</p>
			<p>Text data, on the other hand, denote blocks of words, such as those in this book, so they are much more complicated. Look at the following two sentences or utterances:</p>
			<p><em class="italic">I would like to book a plane ticket for December 23rd, 2020 from Dubai to Paris.</em></p>
			<p><em class="italic">The room wasn't cleaned, and the heating wouldn't work.</em></p>
			<p>How would we label these utterances? Once again, this very much depends on our goal for training. Maybe we just want to put these utterances into groups, such as order, greeting, or statement. In that scenario, every utterance would receive one label. On the other hand, we may want to drill down into the meaning of the words in the sentence. For our first utterance, we may want to understand the meaning of the order to offer an answer by showing possible flight options. For the second utterance, we may want to understand the sentiment since it is a statement about the quality of a hotel room.</p>
			<p>Therefore, we need to start<a id="_idIndexMarker902"/> labeling single words or phrases in the utterance itself, while looking for the semantic meaning.</p>
			<p>We will come back to this topic in <a href="B17928_07_ePub.xhtml#_idTextAnchor112"><em class="italic">Chapter 7</em></a>, <em class="italic">Advanced Feature Extraction with NLP</em>.</p>
			<h3>Computer vision</h3>
			<p>When we talk about ML<a id="_idIndexMarker903"/> modeling for images, we are typically trying to understand and learn about one of the following:</p>
			<ul>
				<li><strong class="bold">Image classification</strong>: Classify an<a id="_idIndexMarker904"/> image into one or more classes. Typical use cases include image searches, library management, and sentiment analysis of a person.</li>
				<li><strong class="bold">Object detection</strong>: Localize specific objects in <a id="_idIndexMarker905"/>an image. Typical use cases include pedestrian detection, traffic flow analysis, and object counting.</li>
				<li><strong class="bold">Image segmentation</strong>: Assign each <a id="_idIndexMarker906"/>pixel of an image to a specific segment. Typical use cases include precise environment analysis for self-driving cars and pixel-precise anomaly detection in an X-ray or MRI picture.</li>
			</ul>
			<p>The following figure shows an example of these three types:</p>
			<div>
				<div id="_idContainer167" class="IMG---Figure">
					<img src="image/B17928_06_11.jpg" alt="Figure 6.11 – Different image processing methods " width="809" height="286"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.11 – Different image processing methods</p>
			<p>For these methods, the <a id="_idIndexMarker907"/>process of labeling them becomes more complicated, the further we go down the list. For classification, we can just put one or more labels on an image. For object detection, we start drawing so-called bounding boxes or polygons on the image. Finally, image segmentation becomes very complicated as we must assign labels for each pixel of the image. For this, highly specialized tooling is required. </p>
			<p>As we will see shortly, we can use the data labeling tool from Azure Machine Learning Studio to do classification, object detection, and, to some degree, segmentation for image labeling tasks.</p>
			<h3>Audio annotation </h3>
			<p>Finally, let's talk about annotating <a id="_idIndexMarker908"/>audio data. When it comes to ML modeling for audio data, the following scenarios are possible:</p>
			<ul>
				<li><strong class="bold">Speech-to-text</strong>: Run real-time<a id="_idIndexMarker909"/> transcription, voice assistants, pronunciation assessments, and similar solutions.</li>
				<li><strong class="bold">Speech translation</strong>: Translate<a id="_idIndexMarker910"/> speech to trigger actions in an application or device.</li>
				<li><strong class="bold">Speaker recognition</strong>: Verify and<a id="_idIndexMarker911"/> identify speakers by their voice characteristics. </li>
			</ul>
			<p>Therefore, annotating audio data means that we must take out snippets from an audio file and label these snippets accordingly. The following diagram shows a simple example of this:</p>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="image/B17928_06_12.jpg" alt="Figure 6.12 – Audio labeling process " width="1603" height="745"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.12 – Audio labeling process</p>
			<p>As you can imagine, this labeling task is also not very straightforward and requires specialized <a id="_idIndexMarker912"/>tooling.</p>
			<p>We have seen a lot of scenarios so far, where labeling is of utmost importance. Now, let's try to label some images ourselves.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor110"/>Performing data labeling for image classification using the Azure Machine Learning labeling service</h2>
			<p>In this section, we will be<a id="_idIndexMarker913"/> using the data labeling <a id="_idIndexMarker914"/>service in Azure Machine Learning Studio to label some assets. As we learned in <a href="B17928_03_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 3</em></a>, <em class="italic">Preparing the Azure Machine Learning Workspace</em>, navigate to the Azure Machine Learning Studio and click on <strong class="bold">Data Labeling</strong> at the lower end of the menu, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/B17928_06_13.jpg" alt="Figure 6.13 – Azure Machine Learning Studio " width="827" height="590"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.13 – Azure Machine Learning Studio</p>
			<p>On the<a id="_idIndexMarker915"/> following screen, click <strong class="bold">Add Project</strong>, which will take<a id="_idIndexMarker916"/> you to the following view:</p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="image/B17928_06_14.jpg" alt="Figure 6.14 – Creation wizard for a labeling project " width="1202" height="654"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.14 – Creation wizard for a labeling project</p>
			<p>Before we start the exercise, let's look at what kind of labeling tasks we can perform with the service. As<a id="_idIndexMarker917"/> shown in the preceding screenshot, we can work with image and text data as our data source. Switching between the <strong class="bold">Image</strong> and <strong class="bold">Text</strong> options on-screen, we have the following<a id="_idIndexMarker918"/> choices:</p>
			<ul>
				<li><strong class="bold">Image Classification Multi-class</strong>: Attach a single label to each image.</li>
				<li><strong class="bold">Image Classification Multi-label</strong>: Attach multiple labels to each image.</li>
				<li><strong class="bold">Object Detection (Bounding Box)</strong>: Draw one or multiple boxes around an object on an image.</li>
				<li><strong class="bold">Instance Segmentation (Polygon)</strong>: Draw complex polygons around an object on an image.</li>
				<li><strong class="bold">Text Classification Multi-class</strong>: Attach a single label to a piece of text.</li>
				<li><strong class="bold">Text Classification Multi-label</strong>: Attach one or multiple labels to a piece of text.</li>
			</ul>
			<p>As we can see, there are a lot of helpful options when it comes to image data. We can even highlight and tag very specific pieces in <a id="_idIndexMarker919"/>an image by using a <strong class="bold">bounding box</strong> or a <strong class="bold">polygon</strong>. Using polygons, you <a id="_idIndexMarker920"/>are technically able to do a complete <strong class="bold">image segmentation</strong>, but it is quite hard to assign each pixel to a class with this tool.</p>
			<p>For text data, however, there are<a id="_idIndexMarker921"/> some limitations. We do not have the option to label specific words or phrases in a piece of text, as we discussed in the previous section. At the time of writing, the only option is to single- or multi-label a text block.</p>
			<p>Therefore, we will be working with images. To not make using this tool for the first time too complex, we will start by attaching a single label to images in an image dataset. In the following steps, we will create an image dataset and a corresponding labeling project:</p>
			<ol>
				<li>Before going<a id="_idIndexMarker922"/> through the wizard, let's look for a suitable image dataset to use. We will be using the <strong class="bold">STL-10 dataset</strong> (<a href="https://cs.stanford.edu/~acoates/stl10/">https://cs.stanford.edu/~acoates/stl10/</a>). This dataset contains a huge amount of small 96x96 images that can be divided into 10 classes (<strong class="bold">airplane</strong>, <strong class="bold">bird</strong>, <strong class="bold">car</strong>, <strong class="bold">cat</strong>, <strong class="bold">deer</strong>, <strong class="bold">dog</strong>, <strong class="bold">horse</strong>, <strong class="bold">monkey</strong>, <strong class="bold">ship</strong>, and <strong class="bold">truck</strong>). These 10 classes will be our labels. As the original page only offers us the images in binary format, we need to<a id="_idIndexMarker923"/> find a different source. On <strong class="bold">Kaggle</strong>, you often find these types of datasets prepared in different formats. </li>
				<li>Go to <a href="https://www.kaggle.com/jessicali9530/stl10">https://www.kaggle.com/jessicali9530/stl10</a> and download <strong class="source-inline">test_images</strong>, which is a set of 8,000 files in <strong class="source-inline">png</strong> format. Normally, we would use the <strong class="source-inline">unlabeled_images</strong> set, but since there are 100,000 of them, we will leave them be for now.</li>
				<li>If you haven't done so already, download the files for this chapter to your device and create a new folder called <strong class="source-inline">images</strong> under the <strong class="source-inline">chapter06</strong> folder.</li>
				<li>Extract all 8,000 images to the <strong class="source-inline">images</strong> folder. After that, open the <strong class="source-inline">03_reg_unlabeled_data.ipynb</strong> file. In this file, you will find the code we have been using so far to connect to our workspace and datastore. Please replace <strong class="source-inline">datastore_name</strong> with the one you have been given in your ML workspace. The last code snippet of the first cell reads as follows:<p class="source-code">file_ds = Dataset.File.<strong class="bold">upload_directory</strong>(</p><p class="source-code">                   src_dir='./images',</p><p class="source-code">                   target=DataPath (datastore,</p><p class="source-code">                           'mldata/STL10_unlabelled'),</p><p class="source-code">                   show_progress=True)</p></li>
			</ol>
			<p>The <strong class="source-inline">upload_directory</strong> method will, with one call, upload all the files from the <strong class="source-inline">images</strong> folder to the datastore location you defined in the target and will create a file dataset object called <strong class="source-inline">file_ds</strong>. Once the upload is complete, we can register our new dataset with the following code:</p>
			<p class="source-code">file_ds = file_ds.<strong class="bold">register</strong>(workspace=ws,</p>
			<p class="source-code">                           name='STL10_unlabeled',</p>
			<p class="source-code">                           description='8000 unlabeled </p>
			<p class="source-code">                           STL-10 images')</p>
			<p>If you navigate to the <strong class="bold">Datasets</strong> tab in Azure Machine Learning Studio, you will see our newly registered<a id="_idIndexMarker924"/> dataset. Under the <strong class="bold">Explore</strong> tab, you will see a subset of the images, including image metadata<a id="_idIndexMarker925"/> and a preview of the images.</p>
			<ol>
				<li value="5">Now that we have registered our dataset, we can set up our labeling project. Go back to the wizard, as shown in <em class="italic">Figure 6.14</em>, enter <strong class="source-inline">STL10_Labeling</strong> as the project name, and choose <strong class="bold">Image Classification Multi-class</strong> as the type. Click <strong class="bold">Next</strong>.</li>
				<li>On the next screen, Microsoft will give you the option to hire a workforce from the <strong class="bold">Azure Marketplace</strong> to perform your labeling work. This can be a helpful tool, as you will soon learn how tedious this task can be. For now, we do not require additional help. Click <strong class="bold">Next</strong>.</li>
				<li>Now, we can choose the dataset to work on. Select our newly create dataset, named <strong class="source-inline">STL10_unlabeled</strong>, and click <strong class="bold">Next</strong>.</li>
				<li>We will see an option called <strong class="bold">Incremental Refresh</strong>. This feature updates the project once a day if new images have been added to the underlying dataset. We are not planning on doing this here, so leave it as-is and click <strong class="bold">Next</strong>.</li>
				<li>The following screen asks us to define our labels. <strong class="bold">STL10 dataset</strong> contains 10 classes of images, which we will now define as labels. Enter <strong class="source-inline">airplane</strong>, <strong class="source-inline">bird</strong>, <strong class="source-inline">car</strong>, <strong class="source-inline">cat</strong>, <strong class="source-inline">deer</strong>, <strong class="source-inline">dog</strong>, <strong class="source-inline">horse</strong>, <strong class="source-inline">monkey</strong>, <strong class="source-inline">ship</strong>, and <strong class="source-inline">truck</strong> as labels. Then, click <strong class="bold">Next</strong>.</li>
				<li>The second to last screen allows us to enter <strong class="bold">Labeling instructions</strong>. These are useful if we are not working alone on the project or we have ordered a workforce to do<a id="_idIndexMarker926"/> the job. Here, we can give them instructions. For us, as we are working alone, this is unnecessary. So, click <strong class="bold">Next</strong>.</li>
				<li>Finally, we have the option to use <strong class="bold">ML-assisted labeling</strong>. If we do not activate this option, we<a id="_idIndexMarker927"/> would have to label all 8,000 images by ourselves without help. Please be aware that activating this option requires a GPU compute cluster that runs for a couple of minutes every time the assisting ML model is retrained. We will choose the <strong class="bold">Use default</strong> option, which will create an appropriate cluster for us. Click <strong class="bold">Create project</strong>. This will bring us back to the overview. When the cluster has been created, click on the project's name to get to the overview page.</li>
			</ol>
			<p>You will see a dashboard similar to the following: </p>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<img src="image/B17928_06_15.jpg" alt="Figure 6.15 – The dashboard for the labeling project " width="1088" height="641"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.15 – The dashboard for the labeling project</p>
			<p>The dashboard is divided into the following views:</p>
			<ul>
				<li><strong class="bold">Progress</strong>: This shows the <a id="_idIndexMarker928"/>number of assets being labeled. In our case, we are working with 8,000 images. It also shows the status for each asset (<strong class="bold">Completed</strong>, <strong class="bold">Skipped</strong>, <strong class="bold">Needs review</strong>, and <strong class="bold">Incomplete</strong>).</li>
				<li><strong class="bold">Label class distribution</strong>: This view will show a bar chart of which label has been used and how many times to classify an image.</li>
				<li><strong class="bold">Labeler performance</strong>: This <a id="_idIndexMarker929"/>view shows how many assets each labeler has processed. In our case, only our name will be shown there.</li>
				<li><strong class="bold">Task queue</strong>: This view shows what tasks are in the pipeline. At the moment, we need to label 150 images manually before the next training phase or the next check occurs.</li>
				<li><strong class="bold">ML-assisted labeling experiment</strong>: This view shows the running or already run training experiments for the assisting ML model.</li>
			</ul>
			<p>If you switch the view to the <strong class="bold">Data</strong> tab, you will see some previews for images and you can review the already labeled images. This is helpful when you're working in a team, where a couple of people are working on labeling the images and some are reviewing their labeling efforts.</p>
			<p>Finally, if you look at the <strong class="bold">Details</strong> tab, you will find the settings for this project. Here, we can see and change certain settings we chose during creation. If you click on <strong class="bold">ML-0assisted labeling</strong>, you can see the name of the training and inference cluster that was created for us. Let's look <a id="_idIndexMarker930"/>at that cluster. Switch the main menu of Azure Machine Learning Studio to <strong class="bold">Compute and Compute Cluster</strong> and click<a id="_idIndexMarker931"/> on the cluster you saw previously, probably named <strong class="source-inline">DefLabelNC6</strong>.</p>
			<p>The following screenshot shows the overview page of this cluster:</p>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<img src="image/B17928_06_16.jpg" alt="Figure 6.16 – Labeling cluster dashboard " width="769" height="619"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.16 – Labeling cluster dashboard</p>
			<p>As you can see, the machines that are being used for the nodes sport 6 cores, 56 GB of RAM, and a Tesla K80 GPU. Always check the pricing page (<a href="https://azure.microsoft.com/en-us/pricing/details/virtual-machines/ml-server-ubuntu/">https://azure.microsoft.com/en-us/pricing/details/virtual-machines/ml-server-ubuntu/</a>) when you're creating any type of compute instance on Azure. As shown on that page, the node we are using is called <strong class="bold">NC6</strong> and costs around $3 per hour. The cluster node shows that the cluster is <strong class="bold">Idle</strong>, so there are no costs. Later, you can check the <strong class="bold">Runs</strong> tabs for the <a id="_idIndexMarker932"/>duration of the training runs to understand the pricing implications. At the moment, a good, educated guess would be that we will need 2 to 4 hours for the ML-assisted support in our labeling project.</p>
			<p>So, before we start <a id="_idIndexMarker933"/>labeling the images, let's understand what ML-assisted labeling does. When you switch back to the dashboard of our labeling project, you will see three options under <strong class="bold">Task queue</strong>, as follows:</p>
			<ul>
				<li><strong class="bold">Manual</strong>: This denotes the assets we must handle without support at any given point. </li>
				<li><strong class="bold">Clustered</strong>: This denotes the assets where a clustering model was being used on the already labeled assets. When you work on these assets, they will be shown to you in groups of images that the model thinks belong to the same class.</li>
				<li><strong class="bold">Prelabeled</strong>: This denotes the assets where a classification model was trained on the already labeled assets. In this case, it predicted labels for unlabeled assets. When you're working on those images, you will be shown the suggested labels and have to check if the model was correct.</li>
			</ul>
			<p>Now, let's start labeling. When you click <strong class="bold">Label data</strong>, you will see the following view:</p>
			<div>
				<div id="_idContainer173" class="IMG---Figure">
					<img src="image/B17928_06_17.jpg" alt="Figure 6.17 – Labeling task view " width="888" height="662"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.17 – Labeling task view</p>
			<p>From this view, you <a id="_idIndexMarker934"/>can see the asset in the middle. With the controls up top, you can <strong class="bold">Zoom in</strong> and change the <strong class="bold">Brightness</strong> and <strong class="bold">Contrast</strong> properties of the image. If you are unsure about these options, you can select <strong class="bold">Skip</strong> for now. On the right, you can choose the appropriate label. If you are happy with your choice, you can click <strong class="bold">Submit</strong>.</p>
			<p>Do this for a couple of images<a id="_idIndexMarker935"/> to get a grip on things. After that, look at the controls at the top right. Here, we can change how many assets are shown to us at the same time (1, 4, 6, or 9). I would suggest displaying 6 assets at the same time. In addition, to label pictures, you can multi-select them and use the keyboard numbers 1 to 9 (as shown on the right of the preceding screenshot) to label faster.</p>
			<p>Now, to see the ML-assisted labeling being triggered, you will need to manually label around 400 to 600 images. You can decide if this is a good use of your time, but it is a good exercise to do as it gives you a perspective of how tedious this task is.</p>
			<p>Eventually, the training will be triggered, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer174" class="IMG---Figure">
					<img src="image/B17928_06_18.jpg" alt="Figure 6.18 – Triggered training run for labeling " width="1017" height="737"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.18 – Triggered training run for labeling</p>
			<p>I had to label 616 assets <a id="_idIndexMarker936"/>manually before the first labeling training would be triggered. As we can see, the<a id="_idIndexMarker937"/> tool shows the distribution of label classes that were encountered during the labeling process at that point. As with any other training, this creates an experiment with runs. You can find these under <strong class="source-inline">Experiments</strong> in the ML workspace, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="image/B17928_06_19.jpg" alt="Figure 6.19 – Experiment run for ML-assisted labeling " width="1060" height="428"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.19 – Experiment run for ML-assisted labeling</p>
			<p>At this point, just <a id="_idIndexMarker938"/>continue to label assets. Eventually, you<a id="_idIndexMarker939"/> will either be shown clustered images, defined by <strong class="bold">Tasks clustered</strong> at the top of the page (see <em class="italic">Figure 6.20</em>):</p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="image/B17928_06_20.jpg" alt="Figure 6.20 – Data labeling showing clustered images " width="1642" height="1063"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.20 – Data labeling showing clustered images</p>
			<p>Or you'll be shown prelabeled images, defined by <strong class="bold">Tasks prelabeled</strong> at the top of the page (see <em class="italic">Figure 6.21</em>):</p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="image/B17928_06_21.jpg" alt="Figure 6.21 – Data labeling showing prelabeled images " width="1646" height="1205"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.21 – Data labeling showing prelabeled images</p>
			<p>With that, you've seen how <a id="_idIndexMarker940"/>you can utilize ML modeling to label your assets and how Azure Machine Learning Studio makes this process easier. As you should understand by now, this is a time-consuming task, but it <a id="_idIndexMarker941"/>needs to be done if you wish to achieve much better results in your ML training down the line. </p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor111"/>Summary</h1>
			<p>In this chapter, we looked at how to prepare our features through feature engineering and how to prepare our labels through labeling. </p>
			<p>In the first section, we learned that feature engineering includes creating new and missing features, transforming existing features, extracting features from a high-dimensional dataset, and using methods to select the most predictive feature for ML training.</p>
			<p>In the second section, we learned that labeling is essential and tedious. Therefore, tooling such as Azure Machine Learning data labeling can be a blessing to alleviate this time-consuming task.</p>
			<p>The key takeaway from this chapter is that creating, transforming, and selecting predictive features has the biggest impact on the quality of the ML model. No other step in the ML pipeline will have more influence on its outcome.</p>
			<p>To pull off quality feature engineering, you must have intimate knowledge of the domain (or you must know someone with that knowledge) and a clear grasp of how the chosen ML algorithm works internally. This includes understanding the mathematical theory, the required data structure the algorithm expects as input, and the feature engineering methods that are applied automatically when you're fitting the model.</p>
			<p>In the next chapter, we will see feature engineering in action. We will look at how to perform feature extraction on text data for natural language processing.</p>
		</div>
	</div>
</div>
</body></html>