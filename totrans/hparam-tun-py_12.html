<html><head></head><body>
		<div id="_idContainer319">
			<h1 id="_idParaDest-87"><em class="italic"><a id="_idTextAnchor092"/>Chapter 10</em><span class="superscript">: Advanced Hyperparameter Tuning with DEAP and Microsoft NNI</span></h1>
			<p><strong class="bold">DEAP</strong> and <strong class="bold">Microsoft NNI</strong> are Python packages that provide various hyperparameter tuning methods that are not implemented in other packages that we have discussed in <em class="italic">Chapters 7 – 9</em>. For example, Genetic Algorithm, Particle Swarm Optimization, Metis, Population-Based Training, and many more. </p>
			<p>In this chapter, we’ll learn how to perform hyperparameter tuning using both DEAP and Microsoft NNI packages, starting from getting ourselves familiar with the packages, along with the important modules and parameters we need to be aware of. We’ll learn not only how to utilize both DEAP and Microsoft NNI to perform hyperparameter tuning with their default configurations but also discuss other available configurations along with their usage. Moreover, we’ll also discuss how the implementation of the hyperparameter tuning methods is related to the theory that we have learned in previous chapters, since there may be some minor differences or adjustments made in the implementation.</p>
			<p>By the end of this chapter, you will be able to understand all of the important things you need to know about DEAP and Microsoft NNI and be able to implement various hyperparameter tuning methods available in these packages. You’ll also be able to understand each of the important parameters of the classes and how they are related to the theory that we have learned in the previous chapters. Finally, equipped with the knowledge from previous chapters, you will also be able to understand what’s happening if there are errors or unexpected results and understand how to set up the method configuration to match your specific problem.</p>
			<p>The following are the main topics that will be discussed in this chapter:</p>
			<ul>
				<li>Introducing DEAP</li>
				<li>Implementing the Genetic Algorithm</li>
				<li>Implementing Particle Swarm Optimization</li>
				<li>Introducing Microsoft NNI</li>
				<li>Implementing Grid Search</li>
				<li>Implementing Random Search</li>
				<li>Implementing Tree-structured Parzen Estimators</li>
				<li>Implementing Sequential Model Algorithm Configuration</li>
				<li>Implementing Bayesian Optimization Gaussian Process</li>
				<li>Implementing Metis</li>
				<li>Implementing Simulated Annealing</li>
				<li>Implementing Hyper Band</li>
				<li>Implementing Bayesian Optimization Hyper Band</li>
				<li>Implementing Population-Based Training</li>
			</ul>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor093"/>Technical requirements</h1>
			<p>We will learn how to implement various hyperparameter tuning methods with DEAP and Microsoft NNI. To ensure that you are able to reproduce the code examples in this chapter, you will require the following:</p>
			<ul>
				<li>Python 3 (version 3.7 or above)</li>
				<li>Installed <strong class="source-inline">pandas</strong> package (version 1.3.4 or above)</li>
				<li>Installed <strong class="source-inline">NumPy</strong> package (version 1.21.2 or above)</li>
				<li>Installed <strong class="source-inline">SciPy</strong> package (version 1.7.3 or above)</li>
				<li>Installed <strong class="source-inline">Matplotlib</strong> package (version 3.5.0 or above)</li>
				<li>Installed <strong class="source-inline">scikit-learn</strong> package (version 1.0.1 or above)</li>
				<li>Installed <strong class="source-inline">DEAP</strong> package (version 1.3)</li>
				<li>Installed <strong class="source-inline">Hyperopt</strong> package (version 0.1.2)</li>
				<li>Installed <strong class="source-inline">NNI</strong> package (version 2.7)</li>
				<li>Installed <strong class="source-inline">PyTorch</strong> package (version 1.10.0)</li>
			</ul>
			<p>All of the code examples for this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/blob/main/10_Advanced_Hyperparameter-Tuning-via-DEAP-and-NNI.ipynb">https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/blob/main/10_Advanced_Hyperparameter-Tuning-via-DEAP-and-NNI.ipynb</a>.</p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor094"/>Introducing DEAP</h1>
			<p><strong class="bold">Distributed Evolutionary Algorithms in Python</strong> (<strong class="bold">DEAP</strong>) is a Python package that allows you to implement various<a id="_idIndexMarker442"/> evolutionary algorithms <a id="_idIndexMarker443"/>including (but not limited to) the <strong class="bold">Genetic Algorithm</strong> (<strong class="bold">GA</strong>) and <strong class="bold">Particle Swarm Optimization</strong> (<strong class="bold">PSO</strong>). To install DEAP, you can simply<a id="_idIndexMarker444"/> call the <strong class="source-inline">pip install deap</strong> command.</p>
			<p>DEAP allows you to craft your evolutionary algorithm optimization steps in a very flexible manner. The following steps show how to utilize DEAP to perform any hyperparameter tuning methods. More detailed steps, including the code implementation, will be given through various examples in the upcoming sections:</p>
			<ol>
				<li>Define the <em class="italic">type</em> classes through the <strong class="source-inline">creator.create()</strong> module. These classes are responsible for defining the type of objects that will be used in the optimization steps.</li>
				<li>Define the <em class="italic">initializers</em> along with the hyperparameter space and register them in the <strong class="source-inline">base.Toolbox()</strong> container. The initializers are responsible for setting the initial value of the objects that will be used in the optimization steps.</li>
				<li>Define the <em class="italic">operators</em> and register them in the <strong class="source-inline">base.Toolbox()</strong> container. The operators refer to the evolutionary<a id="_idIndexMarker445"/> tools or <strong class="bold">genetic operator</strong> (see <a href="B18753_05_ePub.xhtml#_idTextAnchor047"><em class="italic">Chapter 5</em></a>) that need to be defined as part of the optimization algorithm. For example, the selection, crossover, and mutation operators in the Genetic Algorithm.</li>
				<li>Define the objective function and register it in the <strong class="source-inline">base.Toolbox()</strong> container.</li>
				<li>Define your own hyperparameter tuning algorithm function.</li>
				<li>Perform hyperparameter tuning by calling the defined function in <em class="italic">step 5</em>.</li>
				<li>Train the model on full training data using the best set of hyperparameters found.</li>
				<li>Test the final trained model on the test data.</li>
			</ol>
			<p>The type classes refer to the type of objects used in the optimization steps. These type classes are inherited from the base classes implemented in DEAP. For example, we can define the type of our fitness function as the following:</p>
			<pre class="source-code">from deap import base, creator</pre>
			<pre class="source-code"><strong class="bold">creator.create</strong>("FitnessMax", <strong class="bold">base.Fitness</strong>, <strong class="bold">weights</strong>=(1.0,))</pre>
			<p>The <strong class="source-inline">base.Fitness</strong> class is a base abstract class implemented in DEAP that can be utilized to define our own fitness function type. It expects a <strong class="source-inline">weights</strong> parameter to understand the type of optimization <a id="_idIndexMarker446"/>problem we are working on. If it’s a maximization problem, then we have to put a positive weight and the other way around for a minimization problem. Notice that it expects a tuple data structure instead of a float. This is because DEAP also allows us to work with a <strong class="bold">multi-objective optimization problem</strong>. So, if we pass <strong class="source-inline">(1.0, -1.0)</strong> to the <strong class="source-inline">weights</strong> parameter, it means we have two objective functions where we want to maximize the first one and minimize the second one with equal weight.</p>
			<p>The <strong class="source-inline">creator.create()</strong> function is responsible for creating a new class based on the base class. In the preceding code, we created the type class for our objective function with the name “<strong class="source-inline">FitnessMax</strong>”. This <strong class="source-inline">creator.create()</strong> function expects at least two parameters: specifically, the name of the newly created class and the base class itself. The rest of the parameters passed to this function will be treated as the attributes for this newly created class. Besides defining the type of the objective function, we can also define the type of individuals in the evolutionary algorithm that will be performed. The following code shows how to create the type of individuals inherited from the built-in <strong class="source-inline">list</strong> data structure in Python that has <strong class="source-inline">fitness</strong> as its attribute: </p>
			<pre class="source-code">creator.create("Individual", <strong class="bold">list</strong>, fitness=creator.FitnessMax)</pre>
			<p>Note that the <strong class="source-inline">fitness</strong> attribute has a type of <strong class="source-inline">creator.FitnessMax</strong>, which is the type that we just created in the preceding code. </p>
			<p class="callout-heading">Types Definition in DEAP</p>
			<p class="callout">There are a lot of ways<a id="_idIndexMarker447"/> to define type classes in DEAP. While we have discussed the most straightforward and, arguably, most used type class, you may find other cases that need other definitions of type class. For more information on how to define other types in DEAP, please refer<a id="_idIndexMarker448"/> to the official documentation (<a href="https://deap.readthedocs.io/en/master/tutorials/basic/part1.html">https://deap.readthedocs.io/en/master/tutorials/basic/part1.html</a>).</p>
			<p>Once we have finished <a id="_idIndexMarker449"/>defining the type of objects that will be used in the optimization steps, we now need to initiate the value of those objects using the initializers and register them in the <strong class="source-inline">base.Toolbox()</strong> container. You can think of this module as a box or container of initializers and other tools that will be utilized during the optimization steps. The following code shows how we can set the random initial values for individuals:</p>
			<pre class="source-code">import random</pre>
			<pre class="source-code">from deap import <strong class="bold">tools</strong></pre>
			<pre class="source-code">toolbox = base.Toolbox()</pre>
			<pre class="source-code"><strong class="bold">toolbox.register</strong>("individual",<strong class="bold">tools.initRepeat</strong>,creator.Individual,</pre>
			<pre class="source-code">                 random.random, <strong class="bold">n=10</strong>)</pre>
			<p>The preceding code shows an example of how to register the <strong class="source-inline">"individual"</strong> object in the <strong class="source-inline">base.Toolbox()</strong> container, where each individual has a size of <strong class="source-inline">10</strong>. The individual is generated by repeatedly calling the <strong class="source-inline">random.random</strong> method 10 times. Note that, in the hyperparameter tuning setup, the size of <strong class="source-inline">10</strong> of each individual actually refers to the number of the hyperparameters we have in the space. The following shows the output of calling the registered individual via the <strong class="source-inline">toolbox.individual()</strong> method:</p>
			<pre class="source-code">[0.30752039354315985,0.2491982746819209,0.8423374678316783,0.3401579175109981,0.7699302429041264,0.046433183902334974,0.5287019598616896,0.28081693679292696,0.9562244184741888,0.0008450701833065954]</pre>
			<p>As you can see, the output of <strong class="source-inline">toolbox.individual()</strong> is just a list of 10 random values since we’ve defined <strong class="source-inline">creator.Individual</strong> to inherit from the built-in <strong class="source-inline">list</strong> data structure in Python. Furthermore, we also called <strong class="source-inline">tools.initRepeat</strong> when registering the individual with the <strong class="source-inline">random.random</strong> method by 10 times. </p>
			<p>You may now wonder, how <a id="_idIndexMarker450"/>do you define the actual hyperparameter space using this <strong class="source-inline">toolbox.register()</strong> method? Initiating a bunch of random values definitely doesn’t make any sense. We need to know the way to define the hyperparameter space that will be equipped for each individual. To do that, we can actually utilize another tool provided by DEAP, <strong class="source-inline">tools.InitCycle</strong>. </p>
			<p>Where <strong class="source-inline">tools.initRepeat</strong> will just call the provided function <strong class="source-inline">n</strong> times, in our previous example, the provided function is <strong class="source-inline">random.random</strong>. Here, <strong class="source-inline">tools.InitCycle</strong> expects a list of functions and will call those functions for <strong class="source-inline">n</strong> cycles. The following code shows an example to define the hyperparameter space that will be equipped for each individual:</p>
			<ol>
				<li value="1">We need to first register each of the hyperparameters that we have in the space along with their distribution. Note that we can pass all of the required parameters to the sampling distribution function to <strong class="source-inline">toolbox.register()</strong> as well. For example, here, we pass the <strong class="source-inline">a=0,b=0.5,loc=0.005,scale=0.01</strong> parameters of the <strong class="source-inline">truncnorm.rvs()</strong> method:<p class="source-code">from scipy.stats import randint,truncnorm,uniform</p><p class="source-code">toolbox.register(“<strong class="bold">param_1</strong>”, randint.rvs, 5, 200)</p><p class="source-code">toolbox.register(“<strong class="bold">param_2</strong>”, truncnorm.rvs, 0, 0.5, 0.005, 0.01)</p><p class="source-code">toolbox.register(“<strong class="bold">param_3</strong>”, uniform.rvs, 0, 1)</p></li>
				<li>Once we have registered each hyperparameter we have, we can register the individual by utilizing <strong class="source-inline">tools.initCycle</strong> with only one cycle of repetition:<p class="source-code">toolbox.register(“individual”,<strong class="bold">tools.initCycle</strong>,creator.Individual,</p><p class="source-code">    (</p><p class="source-code">        toolbox.<strong class="bold">param_1</strong>,</p><p class="source-code">        toolbox.<strong class="bold">param_2</strong>,</p><p class="source-code">        toolbox.<strong class="bold">param_3</strong></p><p class="source-code">    ),</p><p class="source-code">    <strong class="bold">n=1</strong>,</p><p class="source-code">)</p></li>
			</ol>
			<p>The following shows the output of calling the registered individual via the <strong class="source-inline">toolbox.individual()</strong> method:</p>
			<p class="source-code">[172, 0.005840196235159121, 0.37250162585120816]</p>
			<ol>
				<li value="3">Once we have registered the individual in our toolbox, registering a population is very simple. We just <a id="_idIndexMarker451"/>need to utilize the <strong class="source-inline">tools.initRepeat</strong> module and pass the defined <strong class="source-inline">toolbox.individual</strong> as the argument. The following code shows how to register a population in general. Note that, here, the population is just a list of five individuals defined previously:<p class="source-code">toolbox.register(“population”, <strong class="bold">tools.initRepeat</strong>, list, <strong class="bold">toolbox.individual</strong>, <strong class="bold">n=5</strong>)</p></li>
			</ol>
			<p>The following shows the output when calling the <strong class="source-inline">toolbox.population()</strong> method:</p>
			<p class="source-code">[[168, 0.009384417146554462, 0.4732188841620628],</p>
			<p class="source-code">[7, 0.009356636359759574, 0.6722125618177741],</p>
			<p class="source-code">[126, 0.00927973696427319, 0.7417964302134438],</p>
			<p class="source-code">[88, 0.008112369078803545, 0.4917555243983919],</p>
			<p class="source-code">[34, 0.008615337472475908, 0.9164442190622125]]</p>
			<p>As mentioned previously, the <strong class="source-inline">base.Toolbox()</strong> container is responsible for storing not only initializers but also other tools that will be utilized during the optimization steps. Another important <a id="_idIndexMarker452"/>building block for an evolutionary algorithm, such as the GA, is the genetic operator. Fortunately, DEAP already implemented various genetic operators that we can utilize via the <strong class="source-inline">tools</strong> module. The following code shows an example of how to register the selection, crossover, and mutation operators for the GA (see <a href="B18753_05_ePub.xhtml#_idTextAnchor047"><em class="italic">Chapter 5</em></a>):</p>
			<pre class="source-code"># selection strategy</pre>
			<pre class="source-code">toolbox.register("select", <strong class="bold">tools.selTournament</strong>, tournsize=3)</pre>
			<pre class="source-code"># crossover strategy</pre>
			<pre class="source-code">toolbox.register("mate", <strong class="bold">tools.cxBlend</strong>, alpha=0.5)</pre>
			<pre class="source-code"># mutation strategy</pre>
			<pre class="source-code">toolbox.register("mutate", <strong class="bold">tools.mutPolynomialBounded</strong>, eta = 0.1, low=-2, up=2, indpb=0.15)</pre>
			<p>The <strong class="source-inline">tools.selTournament</strong> selection strategy works by selecting the best individuals among <strong class="source-inline">tournsize</strong> randomly chosen individuals, <em class="italic">NPOP</em> times, where <strong class="source-inline">tournsize</strong> is the number of individuals participating in the tournament and <em class="italic">NPOP</em> is the number of individuals in the population. The <strong class="source-inline">tools.cxBlend</strong> crossover strategy works by performing a linear combination between two continuous individual genes, where the weight for the linear combination is governed by the <strong class="source-inline">alpha</strong> hyperparameter. The <strong class="source-inline">tools.mutPolynomialBounded</strong> mutation strategy works by passing continuous individual genes to a pre-defined polynomial mapping.</p>
			<p class="callout-heading">Evolutionary Tools in DEAP</p>
			<p class="callout">There are various built-in<a id="_idIndexMarker453"/> evolutionary tools implemented in DEAP that we can utilize for our own needs, starting from initializers, crossover, mutation, selection, and migration tools. For more information regarding the implemented tools, please refer to the official documentation (<a href="https://deap.readthedocs.io/en/master/api/tools.html">https://deap.readthedocs.io/en/master/api/tools.html</a>).</p>
			<p>To register the pre-defined objective function to the toolbox, we can just simply call the same <strong class="source-inline">toolbox.register()</strong> method and pass the objective function, as the following code shows:</p>
			<pre class="source-code">toolbox.register("<strong class="bold">evaluate</strong>", obj_func)</pre>
			<p>Here, <strong class="source-inline">obj_func</strong> is a Python function that expects to receive the <strong class="source-inline">individual</strong> object defined previously. We will see<a id="_idIndexMarker454"/> how to create such an objective function and how to define our own hyperparameter tuning algorithm function in the upcoming sections when we discuss how to implement the GA and PSO in DEAP.</p>
			<p>DEAP also allows us to utilize our parallel computing resources when calling the objective function. To do that, we can simply need to register the <strong class="source-inline">multiprocessing</strong> module to the toolbox as the following:</p>
			<pre class="source-code">import multiprocessing</pre>
			<pre class="source-code">pool = multiprocessing.Pool()</pre>
			<pre class="source-code">toolbox.register("<strong class="bold">map</strong>", pool.map)</pre>
			<p>Once we have registered the <strong class="source-inline">multiprocessing</strong> module, we can simply apply this when calling the objective function, as shown in the following code:</p>
			<pre class="source-code">fitnesses = toolbox.map(toolbox.evaluate, individual)</pre>
			<p>In this section, we have discussed the DEAP package and its building blocks. You may wonder how to construct a real hyperparameter tuning method using all of the building blocks provided by DEAP. Worry no more; in the upcoming two sections, we will learn how to utilize all of the discussed building blocks to perform hyperparameter tuning with the GA and PSO methods.</p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor095"/>Implementing the Genetic Algorithm</h1>
			<p>GA is one of the variants <a id="_idIndexMarker455"/>of the Heuristic Search hyperparameter tuning group (see <a href="B18753_05_ePub.xhtml#_idTextAnchor047"><em class="italic">Chapter 5</em></a>) that can be implemented by the DEAP package. To show you how we can implement GA with the DEAP package, let’s use the Random Forest classifier model and the same data as in the examples in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a>. The dataset used in this example is the <em class="italic">Banking Dataset – Marketing Targets</em> dataset provided on Kaggle (<a href="https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets">https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets</a>). </p>
			<p>The target variable consists of two classes, <strong class="source-inline">yes</strong> or <strong class="source-inline">no</strong>, indicating whether the client of the bank has subscribed to a term deposit or not. Hence, the goal of training an ML model on this dataset is to identify whether a customer is potentially wanting to subscribe to the term deposit or not. Out of the 16 features provided in the data, there are seven numerical features and nine categorical features. As for the target class distribution, 12% of them <a id="_idIndexMarker456"/>are <em class="italic">yes</em> and 88% of them are <em class="italic">no</em>, for both train and test datasets. For more detailed information about the data, please refer to <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a>.</p>
			<p>Before performing the GA, let’s see how the Random Forest classifier with default hyperparameters values works. As shown in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a>, we get around <strong class="source-inline">0.436</strong> in the F1-score when evaluating the Random Forest classifier with default hyperparameter values on the test set. Note that we’re still using the same scikit-learn pipeline definition to train and evaluate the Random Forest classifier, as explained in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a>. </p>
			<p>The following code shows how to implement the GA with the DEAP package. You can find the more detailed code in the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section:</p>
			<ol>
				<li value="1">Define the GA parameters and type classes through the <strong class="source-inline">creator.create()</strong> module:<p class="source-code"># GA Parameters</p><p class="source-code">NPOP = 50 #population size</p><p class="source-code">NGEN = 15 #number of trials</p><p class="source-code">CXPB = 0.5 #cross-over probability</p><p class="source-code">MUTPB = 0.2 #mutation probability</p></li>
			</ol>
			<p>Fix the seed for reproducibility:</p>
			<p class="source-code">import random</p>
			<p class="source-code">random.seed(1)</p>
			<p>Define the type of our fitness function. Here, we are working with a maximization problem and a single objective function, that’s why we set <strong class="source-inline">weights=(1.0,)</strong>:</p>
			<p class="source-code">from deap import creator, base</p>
			<p class="source-code">creator.create(“FitnessMax”, base.Fitness, weights=(1.0,))</p>
			<p>Define the type of individuals inherited from the built-in <strong class="source-inline">list</strong> data structure in Python that has <strong class="source-inline">fitness</strong> as its attribute:</p>
			<p class="source-code">creator.create(“Individual”, list, fitness=creator.FitnessMax)</p>
			<ol>
				<li value="2">Define the initializers<a id="_idIndexMarker457"/> along with the hyperparameter space and register them in the <strong class="source-inline">base.Toolbox()</strong> container. </li>
			</ol>
			<p>Initialize the toolbox:</p>
			<p class="source-code">toolbox = base.Toolbox()</p>
			<p>Define the naming of the hyperparameters:</p>
			<p class="source-code">PARAM_NAMES = [“model__n_estimators”,”model__criterion”,</p>
			<p class="source-code">             “model__class_weight”,”model__min_samples_split”</p>
			<p>Register each of the hyperparameters that we have in the space along with their distribution:</p>
			<p class="source-code">from scipy.stats import randint,truncnorm</p>
			<p class="source-code">toolbox.register(“model__n_estimators”, randint.rvs, 5, 200)</p>
			<p class="source-code">toolbox.register(“model__criterion”, random.choice, [“gini”, “entropy”])</p>
			<p class="source-code">toolbox.register(“model__class_weight”, random.choice, [“balanced”,”balanced_subsample”])</p>
			<p class="source-code">toolbox.register(“model__min_samples_split”, truncnorm.rvs, 0, 0.5, 0.005, 0.01)</p>
			<p>Register the individual by<a id="_idIndexMarker458"/> utilizing <strong class="source-inline">tools.initCycle</strong> with only one cycle of repetition:</p>
			<p class="source-code">from deap import tools</p>
			<p class="source-code">toolbox.register(</p>
			<p class="source-code">    “individual”,</p>
			<p class="source-code">    tools.initCycle,</p>
			<p class="source-code">    creator.Individual,</p>
			<p class="source-code">    (</p>
			<p class="source-code">        toolbox.model__n_estimators,</p>
			<p class="source-code">        toolbox.model__criterion,</p>
			<p class="source-code">        toolbox.model__class_weight,</p>
			<p class="source-code">        toolbox.model__min_samples_split,</p>
			<p class="source-code">    ),</p>
			<p class="source-code">)</p>
			<p>Register the population:</p>
			<p class="source-code">toolbox.register(“population”, tools.initRepeat, list, toolbox.individual)</p>
			<ol>
				<li value="3">Define the operators and register them in the <strong class="source-inline">base.Toolbox()</strong> container.</li>
			</ol>
			<p>Register the selection strategy:</p>
			<p class="source-code">toolbox.register(“select”, tools.selTournament, tournsize=3)</p>
			<p>Register the cross-over strategy:</p>
			<p class="source-code">toolbox.register(“mate”, tools.cxUniform, indpb=CXPB)</p>
			<p>Define a custom mutation strategy. Note that all of the implemented mutation strategies in DEAP are not really suitable for hyperparameter tuning purposes since they can only be utilized for floating or binary values, while most of the time, our hyperparameter<a id="_idIndexMarker459"/> space will be a combination of real and discrete hyperparameters. The following function shows how to implement such a custom mutation strategy. You can follow the same structure to suit your own need:</p>
			<p class="source-code">def <strong class="bold">mutPolynomialBoundedMix</strong>(individual, eta, low, up, is_int, indpb, discrete_params):</p>
			<p class="source-code">    for i in range(len(individual)):</p>
			<p class="source-code">        if discrete_params[i]:</p>
			<p class="source-code">            if random.random() &lt; indpb:</p>
			<p class="source-code">                individual[i] = random.choice(discrete_params[i])</p>
			<p class="source-code">        else:</p>
			<p class="source-code">            individual[i] = tools.mutPolynomialBounded([individual[i]], </p>
			<p class="source-code">                                                          eta[i], low[i], up[i], indpb)[0][0]</p>
			<p class="source-code">        </p>
			<p class="source-code">        if is_int[i]:</p>
			<p class="source-code">            individual[i] = int(individual[i])</p>
			<p class="source-code"> </p>
			<p class="source-code">    return individual,</p>
			<p>Register the custom mutation strategy:</p>
			<p class="source-code">toolbox.register(“mutate”, <strong class="bold">mutPolynomialBoundedMix</strong>, </p>
			<p class="source-code">                 eta = [0.1,None,None,0.1], </p>
			<p class="source-code">                 low = [5,None,None,0], </p>
			<p class="source-code">                 up = [200,None,None,1],</p>
			<p class="source-code">                 is_int = [True,False,False,False],</p>
			<p class="source-code">                 indpb=MUTPB,</p>
			<p class="source-code">                 discrete_params=[[],[“gini”, “entropy”],[“balanced”,”balanced_subsample”],[]]</p>
			<p class="source-code">                )</p>
			<ol>
				<li value="4">Define the objective<a id="_idIndexMarker460"/> function and register it in the <strong class="source-inline">base.Toolbox()</strong> container:<p class="source-code">def <strong class="bold">evaluate</strong>(individual):</p><p class="source-code">    # convert list of parameter values into dictionary of kwargs</p><p class="source-code">    strategy_params = {k: v for k, v in zip(PARAM_NAMES, individual)}</p><p class="source-code">    </p><p class="source-code">    if strategy_params['model__min_samples_split'] &gt; 1 or strategy_params['model__min_samples_split'] &lt;= 0:</p><p class="source-code">        return [-np.inf]</p><p class="source-code">    </p><p class="source-code">    tuned_pipe = clone(pipe).set_params(**strategy_params)</p><p class="source-code">    return [np.mean(cross_val_score(tuned_pipe,X_train_full, y_train, cv=5, scoring='f1',))]</p></li>
			</ol>
			<p>Register the objective function: </p>
			<p class="source-code">toolbox.register(“evaluate”, <strong class="bold">evaluate</strong>)</p>
			<ol>
				<li value="5">Define the Genetic Algorithm with parallel processing:<p class="source-code">import multiprocessing</p><p class="source-code">import numpy as np</p></li>
			</ol>
			<p>Register the <strong class="source-inline">multiprocessing</strong> module:</p>
			<p class="source-code">pool = multiprocessing.Pool(16)</p>
			<p class="source-code">toolbox.register(“map”, pool.map)</p>
			<p>Define empty arrays to store the best and average values of objective function scores in each trial:</p>
			<p class="source-code">mean = np.ndarray(NGEN)</p>
			<p class="source-code">best = np.ndarray(NGEN)</p>
			<p>Define a <strong class="source-inline">HallOfFame</strong> class that is responsible for storing the latest best individual (set of <a id="_idIndexMarker461"/>hyperparameters) in the population:</p>
			<p class="source-code">hall_of_fame = tools.HallOfFame(maxsize=3)</p>
			<p>Define the initial population:</p>
			<p class="source-code">pop = toolbox.population(n=NPOP)</p>
			<p>Start the GA iterations:</p>
			<p class="source-code">for g in range(NGEN):</p>
			<p>Select the next generation individuals/children/offspring.</p>
			<p class="source-code">    offspring = toolbox.select(pop, len(pop))</p>
			<p>Clone the selected individuals.</p>
			<p class="source-code">    offspring = list(map(toolbox.clone, offspring))</p>
			<p>Apply crossover on the offspring.</p>
			<p class="source-code">    for child1, child2 in zip(offspring[::2], offspring[1::2]):</p>
			<p class="source-code">        if random.random() &lt; CXPB:</p>
			<p class="source-code">            toolbox.mate(child1, child2)</p>
			<p class="source-code">            del child1.fitness.values</p>
			<p class="source-code">            del child2.fitness.values</p>
			<p>Apply mutation on the offspring.</p>
			<p class="source-code">    for mutant in offspring:</p>
			<p class="source-code">        if random.random() &lt; MUTPB:</p>
			<p class="source-code">            toolbox.mutate(mutant)</p>
			<p class="source-code">            del mutant.fitness.values</p>
			<p>Evaluate the individuals with an invalid fitness.</p>
			<p class="source-code">    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]</p>
			<p class="source-code">    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)</p>
			<p class="source-code">    for ind, fit in zip(invalid_ind, fitnesses):</p>
			<p class="source-code">        ind.fitness.values = fit</p>
			<p>The population is <a id="_idIndexMarker462"/>entirely replaced by the offspring.</p>
			<p class="source-code">    pop[:] = offspring</p>
			<p class="source-code">    hall_of_fame.update(pop)</p>
			<p class="source-code">    fitnesses = [</p>
			<p class="source-code">        ind.fitness.values[0] for ind in pop if not np.isinf(ind.fitness.values[0])</p>
			<p class="source-code">    ]</p>
			<p class="source-code">    mean[g] = np.mean(fitnesses)</p>
			<p class="source-code">    best[g] = np.max(fitnesses)</p>
			<ol>
				<li value="6">Perform hyperparameter tuning by running the defined algorithm in <em class="italic">step 5</em>. After running the GA, we can get the best set of hyperparameters based on the following code:<p class="source-code">params = {}</p><p class="source-code">for idx_hof, param_name in enumerate(PARAM_NAMES):</p><p class="source-code">    params[param_name] = hall_of_fame[0][idx_hof]</p><p class="source-code">print(params)</p></li>
			</ol>
			<p>Based on the preceding<a id="_idIndexMarker463"/> code, we get the following results:</p>
			<p class="source-code">{'model__n_estimators': 101,</p>
			<p class="source-code">'model__criterion': 'entropy',</p>
			<p class="source-code">'model__class_weight': 'balanced',</p>
			<p class="source-code">'model__min_samples_split': 0.0007106340458649385}</p>
			<p>We can also plot the trial history or the convergence plot based on the following code:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">import seaborn as sns</p>
			<p class="source-code">sns.set()</p>
			<p class="source-code">fig, ax = plt.subplots(sharex=True, figsize=(8, 6))</p>
			<p class="source-code">sns.lineplot(x=range(NGEN), y=mean, ax=ax, label=”Average Fitness Score”)</p>
			<p class="source-code">sns.lineplot(x=range(NGEN), y=best, ax=ax, label=”Best Fitness Score”)</p>
			<p class="source-code">ax.set_title(“Fitness Score”,size=20)</p>
			<p class="source-code">ax.set_xticks(range(NGEN))</p>
			<p class="source-code">ax.set_xlabel(“Iteration”)</p>
			<p class="source-code">plt.tight_layout()</p>
			<p class="source-code">plt.show()</p>
			<p>Based on the preceding code, the following figure is generated. As you can see, the objective function <a id="_idIndexMarker464"/>score or the fitness score is increasing throughout the number of trials since the population is updated with the improved individuals:</p>
			<div>
				<div id="_idContainer318" class="IMG---Figure">
					<img src="image/B18753_10_001.jpg" alt="Figure 10.1 – Genetic Algorithm convergence plot&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Genetic Algorithm convergence plot</p>
			<ol>
				<li value="7">Train the model on full training data using the best set of hyperparameters found:<p class="source-code">from sklearn.base import clone</p><p class="source-code">tuned_pipe = clone(pipe).set_params(**params)</p><p class="source-code">tuned_pipe.fit(X_train_full,y_train)</p></li>
				<li>Test the final trained model on the test data:<p class="source-code">y_pred = tuned_pipe.predict(X_test_full)</p><p class="source-code">print(f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.608</strong> in the F1-score when testing our final trained Random Forest model with the best set of hyperparameters on the test set.</p>
			<p>In this section, we have learned how to implement the GA with the DEAP package, starting from defining the <a id="_idIndexMarker465"/>necessary objects and defining the GA procedures with parallel processing and custom mutation strategy, until plotting the history of the trials and testing the best set of hyperparameters in the test set. In the next section, we will learn how to implement the PSO hyperparameter tuning method with the DEAP package.</p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor096"/>Implementing Particle Swarm Optimization</h1>
			<p>PSO is also one of the<a id="_idIndexMarker466"/> variants of the Heuristic Search hyperparameter tuning group (see <a href="B18753_05_ePub.xhtml#_idTextAnchor047"><em class="italic">Chapter 5</em></a>) that can be implemented by the DEAP package. We’ll still use the same example as in the previous section to see how we can implement PSO using the DEAP package.</p>
			<p>The following code shows how to implement PSO with the DEAP package. You can find the more detailed code in the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section:</p>
			<ol>
				<li value="1">Define the PSO parameters and type classes through the <strong class="source-inline">creator.create()</strong> module:<p class="source-code">N = 50 #swarm size</p><p class="source-code">w = 0.5 #inertia weight coefficient</p><p class="source-code">c1 = 0.3 #cognitive coefficient</p><p class="source-code">c2 = 0.5 #social coefficient</p><p class="source-code">num_trials = 15 #number of trials</p></li>
			</ol>
			<p>Fix the seed for reproducibility:</p>
			<p class="source-code">import random</p>
			<p class="source-code">random.seed(1)</p>
			<p>Define the type of our fitness function. Here, we are working with a maximization problem and a single objective function, which is why we set <strong class="source-inline">weights=(1.0,)</strong>:</p>
			<p class="source-code">from deap import creator, base</p>
			<p class="source-code">creator.create(“FitnessMax”, base.Fitness, weights=(1.0,))</p>
			<p>Define the type of <a id="_idIndexMarker467"/>particles inherited from the built-in <strong class="source-inline">list</strong> data structure in Python that has <strong class="source-inline">fitness</strong>, <strong class="source-inline">speed</strong>, <strong class="source-inline">smin</strong>, <strong class="source-inline">smax</strong>, and <strong class="source-inline">best</strong> as its attribute. These attributes will be utilized later on when updating each particle’s position (see <a href="B18753_05_ePub.xhtml#_idTextAnchor047"><em class="italic">Chapter 5</em></a>):</p>
			<p class="source-code">creator.create(“Particle”, list, fitness=creator.FitnessMax,</p>
			<p class="source-code">               speed=list, smin=list, smax=list, best=None)</p>
			<ol>
				<li value="2">Define the initializers along with the hyperparameter space and register them in the <strong class="source-inline">base.Toolbox()</strong> container. </li>
			</ol>
			<p>Initialize the toolbox:</p>
			<p class="source-code">toolbox = base.Toolbox()</p>
			<p>Define the naming of the hyperparameters:</p>
			<p class="source-code">PARAM_NAMES = [“model__n_estimators”,”model__criterion”,</p>
			<p class="source-code">             “model__class_weight”,”model__min_samples_split”</p>
			<p>Register each of the hyperparameters that we have in the space along with their distribution. Remember <a id="_idIndexMarker468"/>that PSO only works with the numerical type hyperparameters. That’s why we encode the <strong class="source-inline">"model__criterion"</strong> and <strong class="source-inline">"model__class_weight"</strong> hyperparameters to integers:</p>
			<p class="source-code">from scipy.stats import randint,truncnorm</p>
			<p class="source-code">toolbox.register(“model__n_estimators”, randint.rvs, 5, 200)</p>
			<p class="source-code">toolbox.register(“model__criterion”, random.choice, [0,1])</p>
			<p class="source-code">toolbox.register(“model__class_weight”, random.choice, [0,1])</p>
			<p class="source-code">toolbox.register(“model__min_samples_split”, truncnorm.rvs, 0, 0.5, 0.005, 0.01)</p>
			<p>Register the individual by utilizing <strong class="source-inline">tools.initCycle</strong> with only one cycle of repetition. Note that we need to also assign the <strong class="source-inline">speed</strong>, <strong class="source-inline">smin</strong>, and <strong class="source-inline">smax</strong> values to each individual. To do that, let’s just define a function called <strong class="source-inline">generate</strong>:</p>
			<p class="source-code">from deap import tools</p>
			<p class="source-code">def <strong class="bold">generate</strong>(speed_bound):</p>
			<p class="source-code">    part = tools.initCycle(creator.Particle,</p>
			<p class="source-code">                           [toolbox.model__n_estimators,</p>
			<p class="source-code">                            toolbox.model__criterion,</p>
			<p class="source-code">                            toolbox.model__class_weight,</p>
			<p class="source-code">                            toolbox.model__min_samples_split,</p>
			<p class="source-code">                           ]</p>
			<p class="source-code">                          )</p>
			<p class="source-code">    part.speed = [random.uniform(speed_bound[i]['smin'], speed_bound[i]['smax']) for i in range(len(part))]</p>
			<p class="source-code">    part.smin = [speed_bound[i]['smin'] for i in range(len(part))]</p>
			<p class="source-code">    part.smax = [speed_bound[i]['smax'] for i in range(len(part))]</p>
			<p class="source-code">    return part</p>
			<p>Register the individual:</p>
			<p class="source-code">toolbox.register(“particle”, <strong class="bold">generate</strong>, </p>
			<p class="source-code">                 speed_bound=[{'smin': -2.5,'smax': 2.5},</p>
			<p class="source-code">                              {'smin': -1,'smax': 1},</p>
			<p class="source-code">                              {'smin': -1,'smax': 1},</p>
			<p class="source-code">                              {'smin': -0.001,'smax': 0.001}])</p>
			<p>Register the population:</p>
			<p class="source-code">toolbox.register(“population”, tools.initRepeat, list, toolbox.particle)</p>
			<ol>
				<li value="3">Define the operators <a id="_idIndexMarker469"/>and register them in the <strong class="source-inline">base.Toolbox()</strong> container. The main operator in PSO is the particle’s position update operator, which is defined in the <strong class="source-inline">updateParticle</strong> function as follows:<p class="source-code">import operator</p><p class="source-code">import math</p><p class="source-code">def <strong class="bold">updateParticle</strong>(part, best, c1, c2, w, is_int):</p><p class="source-code">    w = [w for _ in range(len(part))]</p><p class="source-code">    u1 = (random.uniform(0, 1)*c1 for _ in range(len(part)))</p><p class="source-code">    u2 = (random.uniform(0, 1)*c2 for _ in range(len(part)))</p><p class="source-code">    v_u1 = map(operator.mul, u1, map(operator.sub, part.best, part))</p><p class="source-code">    v_u2 = map(operator.mul, u2, map(operator.sub, best, part))</p><p class="source-code">    part.speed = list(map(operator.add, map(operator.mul, w, part.speed), map(operator.add, v_u1, v_u2)))</p><p class="source-code">    for i, speed in enumerate(part.speed):</p><p class="source-code">        if abs(speed) &lt; part.smin[i]:</p><p class="source-code">            part.speed[i] = math.copysign(part.smin[i], speed)</p><p class="source-code">        elif abs(speed) &gt; part.smax[i]:</p><p class="source-code">            part.speed[i] = math.copysign(part.smax[i], speed)</p><p class="source-code">    part[:] = list(map(operator.add, part, part.speed))</p><p class="source-code">    </p><p class="source-code">    for i, pos in enumerate(part):</p><p class="source-code">        if <strong class="bold">is_int[i]</strong>:</p><p class="source-code">            part[i] = <strong class="bold">int(pos)</strong></p></li>
			</ol>
			<p>Register the operator. Note that the <strong class="source-inline">is_int</strong> attribute is responsible for marking which <a id="_idIndexMarker470"/>hyperparameter has an integer type of value:</p>
			<p class="source-code">toolbox.register(“update”, <strong class="bold">updateParticle</strong>, c1=c1, c2=c2, w=w,</p>
			<p class="source-code">                <strong class="bold">is_int</strong>=[True,True,True,False]</p>
			<p class="source-code">                )</p>
			<ol>
				<li value="4">Define the objective function and register it in the <strong class="source-inline">base.Toolbox()</strong> container. Note that we also decode the <strong class="source-inline">"model__criterion"</strong> and <strong class="source-inline">"model__class_weight"</strong> hyperparameters within the objective function:<p class="source-code">def <strong class="bold">evaluate</strong>(particle):</p><p class="source-code">    # convert list of parameter values into dictionary of kwargs</p><p class="source-code">    strategy_params = {k: v for k, v in zip(PARAM_NAMES, particle)}</p><p class="source-code">    strategy_params[“model__criterion”] = “gini” if strategy_params[“model__criterion”]==0 else “entropy”</p><p class="source-code">    strategy_params[“model__class_weight”] = “balanced” if strategy_params[“model__class_weight”]==0 else “balanced_subsample”</p><p class="source-code">    </p><p class="source-code">    if strategy_params['model__min_samples_split'] &gt; 1 or strategy_params['model__min_samples_split'] &lt;= 0:</p><p class="source-code">        return [-np.inf]</p><p class="source-code">    </p><p class="source-code">    tuned_pipe = clone(pipe).set_params(**strategy_params)</p><p class="source-code"> </p><p class="source-code">    return [np.mean(cross_val_score(tuned_pipe,X_train_full, y_train, cv=5, scoring='f1',))]</p></li>
			</ol>
			<p>Register the objective <a id="_idIndexMarker471"/>function:</p>
			<p class="source-code">toolbox.register(“evaluate”, <strong class="bold">evaluate</strong>)</p>
			<ol>
				<li value="5">Define PSO with parallel processing:<p class="source-code">import multiprocessing</p><p class="source-code">import numpy as np</p></li>
			</ol>
			<p>Register the <strong class="source-inline">multiprocessing</strong> module:</p>
			<p class="source-code">pool = multiprocessing.Pool(16)</p>
			<p class="source-code">toolbox.register(“map”, pool.map)</p>
			<p>Define empty arrays to store the best and average values of objective function scores in each trial:</p>
			<p class="source-code">mean_arr = np.ndarray(num_trials)</p>
			<p class="source-code">best_arr = np.ndarray(num_trials)</p>
			<p>Define a <strong class="source-inline">HallOfFame</strong> class that is responsible for storing the latest best individual (set of hyperparameters) in the population:</p>
			<p class="source-code">hall_of_fame = tools.HallOfFame(maxsize=3)</p>
			<p>Define the initial <a id="_idIndexMarker472"/>population:</p>
			<p class="source-code">pop = toolbox.population(n=NPOP)</p>
			<p>Start the PSO iterations:</p>
			<p class="source-code">best = None</p>
			<p class="source-code">for g in range(num_trials):</p>
			<p class="source-code">    fitnesses = toolbox.map(toolbox.evaluate, pop)</p>
			<p class="source-code">    for part, fit in zip(pop, fitnesses):</p>
			<p class="source-code">        part.fitness.values = fit</p>
			<p class="source-code">        </p>
			<p class="source-code">        if not part.best or part.fitness.values &gt; part.best.fitness.values:</p>
			<p class="source-code">            part.best = creator.Particle(part)</p>
			<p class="source-code">            part.best.fitness.values = part.fitness.values</p>
			<p class="source-code">        if not best or part.fitness.values &gt; best.fitness.values:</p>
			<p class="source-code">            best = creator.Particle(part)</p>
			<p class="source-code">            best.fitness.values = part.fitness.values</p>
			<p class="source-code">    for part in pop:</p>
			<p class="source-code">        toolbox.update(part, best)</p>
			<p class="source-code">        </p>
			<p class="source-code">    hall_of_fame.update(pop)    </p>
			<p class="source-code">    fitnesses = [</p>
			<p class="source-code">        ind.fitness.values[0] for ind in pop if not np.isinf(ind.fitness.values[0])</p>
			<p class="source-code">    ]</p>
			<p class="source-code">    mean_arr[g] = np.mean(fitnesses)</p>
			<p class="source-code">    best_arr[g] = np.max(fitnesses)</p>
			<ol>
				<li value="6">Perform hyperparameter<a id="_idIndexMarker473"/> tuning by running the algorithm defined in <em class="italic">step 5</em>. After running PSO, we can get the best set of hyperparameters based on the following code. Note that we need to decode the <strong class="source-inline">"model__criterion"</strong> and <strong class="source-inline">"model__class_weight"</strong> hyperparameters before passing them to the final model:<p class="source-code">params = {}</p><p class="source-code">for idx_hof, param_name in enumerate(PARAM_NAMES):</p><p class="source-code">    if param_name == “model__criterion”:</p><p class="source-code">        params[param_name] = “gini” if hall_of_fame[0][idx_hof]==0 else “entropy”</p><p class="source-code">    elif param_name == “model__class_weight”:</p><p class="source-code">        params[param_name] = “balanced” if hall_of_fame[0][idx_hof]==0 else “balanced_subsample”</p><p class="source-code">    else:</p><p class="source-code">        params[param_name] = hall_of_fame[0][idx_hof]   </p><p class="source-code">print(params)</p></li>
			</ol>
			<p>Based on the preceding <a id="_idIndexMarker474"/>code, we get the following results:</p>
			<p class="source-code">{'model__n_estimators': 75,</p>
			<p class="source-code">'model__criterion': 'entropy',</p>
			<p class="source-code">'model__class_weight': 'balanced',</p>
			<p class="source-code">'model__min_samples_split': 0.0037241038302412493}</p>
			<ol>
				<li value="7">Train the model on full training data using the best set of hyperparameters found:<p class="source-code">from sklearn.base import clone </p><p class="source-code">tuned_pipe = clone(pipe).set_params(**params) </p><p class="source-code">tuned_pipe.fit(X_train_full,y_train)</p></li>
				<li>Test the final trained model on the test data:<p class="source-code">y_pred = tuned_pipe.predict(X_test_full)</p><p class="source-code">print(f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.569</strong> in the F1-score when testing our final trained Random Forest model with the best set of hyperparameters on the test set.</p>
			<p>In this section, we have learned how to implement PSO with the DEAP package, starting from defining the necessary objects, encoding the categorical hyperparameter to integers, and defining<a id="_idIndexMarker475"/> the optimization procedures with parallel processing, until testing the best set of hyperparameters in the test set. In the next section, we will start learning about another hyperparameter tuning package called NNI, which is developed by Microsoft.</p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor097"/>Introducing Microsoft NNI</h1>
			<p><strong class="bold">Neural Network Intelligence</strong> (<strong class="bold">NNI</strong>) is a package that is developed by Microsoft and can be utilized not only for hyperparameter tuning tasks but also for neural architecture search, model compression, and <a id="_idIndexMarker476"/>feature engineering. In this section, we will discuss how to utilize NNI specifically for the hyperparameter tuning task. To install NNI, you can simply call the <strong class="source-inline">pip install nni</strong> command. </p>
			<p>Although NNI refers to <em class="italic">Neural Network Intelligence</em>, it actually supports numerous ML frameworks including (but not limited to) scikit-learn, XGBoost, LightGBM, PyTorch, TensorFlow, Caffe2, and MXNet.</p>
			<p>There are numerous hyperparameter tuning methods implemented in NNI; some of them are built-in and others are wrapped from other packages such as <strong class="source-inline">Hyperopt</strong> (see <a href="B18753_08_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 8</em></a>) and <strong class="source-inline">SMAC3</strong>. Here, in NNI, the hyperparameter tuning methods are referred to as <strong class="bold">tuners</strong>. We will not <a id="_idIndexMarker477"/>discuss all of the tuners implemented in NNI since there are too many of them. We will only discuss the tuners that have been discussed in <em class="italic">Chapters 3 – 6</em>. Apart from tuners, some of the hyperparameter tuning methods, such as Hyper Band and BOHB, are<a id="_idIndexMarker478"/> treated as <strong class="bold">advisors</strong> in NNI.</p>
			<p class="callout-heading">Available Tuners in NNI</p>
			<p class="callout">To see all of the available tuners in NNI, please<a id="_idIndexMarker479"/> refer to the official documentation page (<a href="https://nni.readthedocs.io/en/stable/hpo/tuners.html">https://nni.readthedocs.io/en/stable/hpo/tuners.html</a>).</p>
			<p>Unlike other hyperparameter tuning packages that we have discussed so far, in NNI, we have to prepare a Python script containing the model definition before being able to run the hyperparameter tuning process from the notebook. Furthermore, NNI also allows us to run the hyperparameter tuning experiment from the command-line tool where we need to define several other additional files to store the hyperparameter space information and other configurations.</p>
			<p>The following steps show how we can <a id="_idIndexMarker480"/>perform any hyperparameter tuning procedure with NNI with pure Python code: </p>
			<ol>
				<li value="1">Prepare the model to be tuned in a script, for example, <strong class="source-inline">model.py</strong>. This script should include the model architecture definition, dataset loading function, training function, and testing function. It also has to include three NNI API calls, as follows:<ul><li><strong class="source-inline">nni.get_next_parameter()</strong> is responsible for collecting the hyperparameters to be evaluated in a particular trial.</li><li><strong class="source-inline">nni.report_intermediate_result()</strong> is responsible for reporting the evaluation metric within each training iteration (epoch or steps). Note that this API call is not mandatory; if you can’t get the intermediate evaluation metric from your ML framework, then this API call is not required.</li><li><strong class="source-inline">nni.report_final_result()</strong> is responsible for reporting the final evaluation metric score after the training process is finished. </li></ul></li>
				<li>Define the hyperparameter space. NNI expects the hyperparameter space is in the form of a Python dictionary, where the first-level keys store the names of the hyperparameters. The second-level keys store the types of the sampling distribution and the hyperparameter values range. The following shows an example of how to define the hyperparameter space in the expected format:<p class="source-code">hyperparameter_space = {</p><p class="source-code">    ' n_estimators ': {'_type': 'randint', '_value': [5, 200]},</p><p class="source-code">    ' criterion ': {'_type': 'choice', '_value': ['gini', 'entropy']},</p><p class="source-code">    ' min_samples_split ': {'_type': 'uniform', '_value': [0, 0.1]},</p><p class="source-code">} </p></li>
			</ol>
			<p class="callout-heading">More Information on NNI</p>
			<p class="callout">For more information<a id="_idIndexMarker481"/> regarding the supported sampling distributions in NNI, please refer to the official documentation (<a href="https://nni.readthedocs.io/en/latest/hpo/search_space.html">https://nni.readthedocs.io/en/latest/hpo/search_space.html</a>). </p>
			<ol>
				<li value="3">Next, we need to<a id="_idIndexMarker482"/> set up the experiment configurations via the <strong class="source-inline">Experiment</strong> class. The following shows steps to set up several configurations before we can run the hyperparameter tuning process.</li>
			</ol>
			<p>Load the <strong class="source-inline">Experiment</strong> class. Here, we are using the <strong class="source-inline">'local'</strong> experiment mode, which means all the training and hyperparameter tuning processes will be done only on our local computer. NNI allows us to run the training procedures in various <a id="_idIndexMarker483"/>platforms, including (but not limited to) <strong class="bold">Azure Machine Learning</strong> (<strong class="bold">AML</strong>), Kubeflow, and OpenAPI. For more information, please refer to the official documentation (<a href="https://nni.readthedocs.io/en/latest/reference/experiment_config.html">https://nni.readthedocs.io/en/latest/reference/experiment_config.html</a>): </p>
			<p class="source-code">from nni.experiment import Experiment</p>
			<p class="source-code">experiment = Experiment('local')</p>
			<p>Set up the trial code configuration. Here, we need to specify the command to run the defined script in <em class="italic">step 1</em> and the relative path to the script. The following shows an example of how to set up the trial code configuration:</p>
			<p class="source-code">experiment.config.trial_command = 'python model.py'</p>
			<p class="source-code">experiment.config.trial_code_directory = '.'</p>
			<p>Set up the hyperparameter space configuration. To set up the hyperparameter space configuration, we simply need to pass the defined hyperparameter space in <em class="italic">step 2</em>. The following code shows how to do that:</p>
			<p class="source-code">experiment.config.search_space = hyperparameter_space</p>
			<p>Set up the hyperparameter tuning algorithm to be utilized. The following shows an example of how to use TPE as the hyperparameter tuning algorithm on a maximization problem:</p>
			<p class="source-code">experiment.config.tuner.name = 'TPE'</p>
			<p class="source-code">experiment.config.tuner.class_args['optimize_mode'] = 'maximize'</p>
			<p>Set up the number of trials and concurrent processes. NNI allows us to set how many numbers of <a id="_idIndexMarker484"/>hyperparameter sets are to be evaluated concurrently at a single time. The following code shows an example of how to set the number of trials to 50, where five sets will be evaluated concurrently at a particular time:</p>
			<p class="source-code">experiment.config.max_trial_number = 50</p>
			<p class="source-code">experiment.config.trial_concurrency = 5</p>
			<p>It is worth noting that NNI also allows you to define the stopping criterion based on the time duration instead of the number of trials. The following code shows how you can set the limit of the experiment time to 1 hour: </p>
			<p class="source-code">experiment.config.max_experiment_duration = '1h'</p>
			<p>If you don’t provide both <strong class="source-inline">max_trial_number</strong> and <strong class="source-inline">max_experiment_duration</strong>, then the experiment will run forever until you forcefully stop it via the <em class="italic">Ctrl + C</em> command.</p>
			<ol>
				<li value="4">Run the hyperparameter tuning experiment. To run the experiment, we can simply call the <strong class="source-inline">run</strong> method on the <strong class="source-inline">Experiment</strong> class. Here, we have to also choose what port to be used. We can see the experiment status and various interesting stats via the launched web portal. The following code shows how to run the experiment on port <strong class="source-inline">8080</strong> in <strong class="source-inline">local</strong>, meaning you can open the web portal on <strong class="source-inline">http://localhost:8080</strong>:<p class="source-code">experiment.run(8080) </p></li>
			</ol>
			<p>There are two available Boolean parameters for the <strong class="source-inline">run</strong> method, namely <strong class="source-inline">wait_completion</strong> and <strong class="source-inline">debug</strong>. When we set <strong class="source-inline">wait_completion=True</strong>, we can’t run other cells in the notebook until the experiment is done or some errors are found. The <strong class="source-inline">debug</strong> parameter enables us to choose whether we want to start the experiment in debug mode or not. </p>
			<ol>
				<li value="5">Train the model on<a id="_idIndexMarker485"/> full training data using the best set of hyperparameters found. </li>
				<li>Test the final trained model on the test data.</li>
			</ol>
			<p class="callout-heading">NNI Web Portal</p>
			<p class="callout">For more information<a id="_idIndexMarker486"/> regarding features available in the web portal, please refer to the official documentation (<a href="https://nni.readthedocs.io/en/stable/experiment/web_portal/web_portal.html">https://nni.readthedocs.io/en/stable/experiment/web_portal/web_portal.html</a>). Note that we will discuss the web portal more in <a href="B18753_13_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 13</em></a>, <em class="italic">Tracking Hyperparameter Tuning Experiments</em>.</p>
			<p>If you prefer to work with the command-line tool, the following steps show how to perform any hyperparameter tuning procedure with NNI with the command-line tool, JSON, and YAML config files:</p>
			<ol>
				<li value="1">Prepare the model to be tuned in a script. This step is exactly the same as the previous procedure to perform hyperparameter tuning with NNI with pure Python code.</li>
				<li>Define the hyperparameter space. The expected format of the hyperparameter space is exactly the same as in the procedure on how to perform any hyperparameter tuning procedure with NNI with pure Python code. However, here, we need to store the Python dictionary within a JSON file, for example, <strong class="source-inline">hyperparameter_space.json</strong>. </li>
				<li>Set up the experiment configurations via the <strong class="source-inline">config.yaml</strong> file. The configurations that need to be set up are basically the same as in the procedure with NNI with<a id="_idIndexMarker487"/> pure Python code. However, instead of configuring the experiment via a Python class, here, we store all of the configuration details in a single YAML file. The following shows an example of what the YAML file will look like:<p class="source-code">searchSpaceFile: hyperparameter_space.json</p><p class="source-code">trial_command: python model.py</p><p class="source-code">trial_code_directory: .</p><p class="source-code"> </p><p class="source-code">trial_concurrency: 5</p><p class="source-code">max_trial_number: 50</p><p class="source-code"> </p><p class="source-code">tuner:</p><p class="source-code">  name: TPE</p><p class="source-code">  class_args:</p><p class="source-code">    optimize_mode: maximize</p><p class="source-code"> </p><p class="source-code">training_service:</p><p class="source-code">  platform: local</p></li>
				<li>Run the hyperparameter tuning experiment. To run the experiment, we can simply call the <strong class="source-inline">nnictl create</strong> command. The following code shows how to use the command to run the experiment on port <strong class="source-inline">8080</strong> in <strong class="source-inline">local</strong>:<p class="source-code">nnictl create --config config.yaml --port 8080</p></li>
			</ol>
			<p>When the experiment is done, you can easily stop the process via the <strong class="source-inline">nnictl stop</strong> command.</p>
			<ol>
				<li value="5">Train the model on full training data using the best set of hyperparameters found. </li>
				<li>Test the final trained <a id="_idIndexMarker488"/>model on the test data.</li>
			</ol>
			<p class="callout-heading">Examples for Various ML Frameworks</p>
			<p class="callout">You can find all of the <a id="_idIndexMarker489"/>examples to perform hyperparameter tuning via NNI using your favorite ML frameworks in the official documentation (<a href="https://github.com/microsoft/nni/tree/master/examples/trials">https://github.com/microsoft/nni/tree/master/examples/trials</a>). </p>
			<p class="callout-heading">scikit-nni</p>
			<p class="callout">There is also a<a id="_idIndexMarker490"/> package called <strong class="source-inline">scikit-nni</strong>, which will automatically generate the required <strong class="source-inline">config.yml</strong> and <strong class="source-inline">search-space.json</strong> and build the <strong class="source-inline">scikit-learn</strong> pipelines based on your own custom needs. Please refer to the official repository for further information about this package (<a href="https://github.com/ksachdeva/scikit-nni">https://github.com/ksachdeva/scikit-nni</a>). </p>
			<p>Besides tuners or hyperparameter tuning<a id="_idIndexMarker491"/> algorithms, NNI also provides <strong class="bold">assessors</strong> that can be utilized. Assessors are basically early stopping modules that can be used to control the hyperparameter tuning experiment when there’s a sign that we may not need to finish the whole experiment trials. Assessors can only be utilized when we provide the intermediate results to NNI via the <strong class="source-inline">nni.report_intermediate_result()</strong> API call. There are only two built-in assessors in NNI: <em class="italic">median stop</em> and <em class="italic">curve fitting</em>. The first assessor will stop the experiment whenever a hyperparameter set performs worse than the median at any step. The latter assessor will stop the experiment if the learning curve is likely to converge to a suboptimal result. </p>
			<p>Setting up an assessor in NNI is very straightforward. You can simply add the configuration on the <strong class="source-inline">Experiment</strong> class or within the <strong class="source-inline">config.yaml</strong> file. The following code shows how to configure the median stop assessor on the <strong class="source-inline">Experiment</strong> class:</p>
			<p class="source-code">experiment.config.assessor.name = 'Medianstop'</p>
			<p class="callout-heading">Custom Algorithms in NNI</p>
			<p class="callout">NNI also allows us to define our own custom tuners and assessors. To do that, you need to inherit the base <strong class="source-inline">Tuner</strong> or <strong class="source-inline">Assessor</strong> class, write several required functions, and add more details on the <strong class="source-inline">Experiment</strong> class or in the <strong class="source-inline">config.yaml</strong> file. For more information regarding how to define your own custom tuners and assessors, please refer to the official documentation (<a href="https://nni.readthedocs.io/en/stable/hpo/custom_algorithm.html">https://nni.readthedocs.io/en/stable/hpo/custom_algorithm.html</a>).</p>
			<p>In this section, we have <a id="_idIndexMarker492"/>discussed the NNI package and how to perform hyperparameter tuning experiments in general. In the upcoming sections, we will learn how to implement various hyperparameter tuning algorithms using NNI.</p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor098"/>Implementing Grid Search</h1>
			<p>Grid Search is one of the variants <a id="_idIndexMarker493"/>of the Exhaustive Search hyperparameter tuning group (see <a href="B18753_03_ePub.xhtml#_idTextAnchor031"><em class="italic">Chapter 3</em></a>) that the NNI package can implement. To show you how we can implement Grid Search with the NNI package, let’s use the same data and pipeline as in the examples in the previous section. However, here, we’ll define a new hyperparameter space since NNI supports only limited types of sampling distribution.</p>
			<p>The following code shows how to implement Grid Search with the NNI package. Here, we’ll use the NNI command-line tool (<strong class="bold">nnictl</strong>) instead of using pure Python code. You can find the more detailed code in the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section:</p>
			<ol>
				<li value="1">Prepare the model to be tuned in a script. Here, we’ll name the script <strong class="source-inline">model.py</strong>. There are several functions defined within this script, including <strong class="source-inline">load_data</strong>, <strong class="source-inline">get_default_parameters</strong>, <strong class="source-inline">get_model</strong>, and <strong class="source-inline">run</strong>. </li>
			</ol>
			<p>The <strong class="source-inline">load_data</strong> function<a id="_idIndexMarker494"/> loads the original data and splits it into train and test data. Furthermore, it’s also responsible for returning the lists of numerical and categorical column names:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">from pathlib import Path</p>
			<p class="source-code">def <strong class="bold">load_data</strong>():</p>
			<p class="source-code">    df = pd.read_csv(f”{Path(__file__).parent.parent}/train.csv”,sep=”;”)</p>
			<p class="source-code"> </p>
			<p class="source-code">    #Convert the target variable to integer</p>
			<p class="source-code">    df['y'] = df['y'].map({'yes':1,'no':0})</p>
			<p class="source-code"> </p>
			<p class="source-code">    #Split full data into train and test data</p>
			<p class="source-code">    df_train, df_test = train_test_split(df, test_size=0.1, random_state=0) </p>
			<p class="source-code"> </p>
			<p class="source-code">    #Get list of categorical and numerical features</p>
			<p class="source-code">    numerical_feats = list(df_train.drop(columns='y').select_dtypes(include=np.number).columns)</p>
			<p class="source-code">    categorical_feats = list(df_train.drop(columns='y').select_dtypes(exclude=np.number).columns)</p>
			<p class="source-code"> </p>
			<p class="source-code">    X_train = df_train.drop(columns=['y'])</p>
			<p class="source-code">    y_train = df_train['y']</p>
			<p class="source-code">    X_test = df_test.drop(columns=['y'])</p>
			<p class="source-code">    y_test = df_test['y']</p>
			<p class="source-code"> </p>
			<p class="source-code">    return X_train, X_test, y_train, y_test, numerical_feats, categorical_feats</p>
			<p>The <strong class="source-inline">get_default_parameters</strong> function returns the default hyperparameter values<a id="_idIndexMarker495"/> used in the experiment:</p>
			<p class="source-code">def <strong class="bold">get_default_parameters</strong>():</p>
			<p class="source-code">    params = {</p>
			<p class="source-code">        'model__n_estimators': 5,</p>
			<p class="source-code">        'model__criterion': 'gini',</p>
			<p class="source-code">        'model__class_weight': 'balanced',</p>
			<p class="source-code">        'model__min_samples_split': 0.01,</p>
			<p class="source-code">    }</p>
			<p class="source-code"> </p>
			<p class="source-code">    return params</p>
			<p>The <strong class="source-inline">get_model</strong> function defines the <strong class="source-inline">sklearn</strong> pipeline used in this example:</p>
			<p class="source-code">from sklearn.compose import ColumnTransformer</p>
			<p class="source-code">from sklearn.preprocessing import StandardScaler, OneHotEncoder</p>
			<p class="source-code">from sklearn.pipeline import Pipeline</p>
			<p class="source-code">from sklearn.ensemble import RandomForestClassifier</p>
			<p class="source-code">def <strong class="bold">get_model</strong>(PARAMS, numerical_feats, categorical_feats): </p>
			<p>Initiate the Normalization Pre-processing for Numerical Features.</p>
			<p class="source-code">    numeric_preprocessor = StandardScaler()</p>
			<p>Initiate the One-Hot-Encoding <a id="_idIndexMarker496"/>Pre-processing for Categorical Features.</p>
			<p class="source-code">    categorical_preprocessor = OneHotEncoder(handle_unknown=”ignore”)</p>
			<p>Create the ColumnTransformer Class to delegate each preprocessor to the corresponding features.</p>
			<p class="source-code">    preprocessor = ColumnTransformer(</p>
			<p class="source-code">        transformers=[</p>
			<p class="source-code">            (“num”, numeric_preprocessor, numerical_feats),</p>
			<p class="source-code">            (“cat”, categorical_preprocessor, categorical_feats),</p>
			<p class="source-code">        ]</p>
			<p class="source-code">    )</p>
			<p>Create a Pipeline of preprocessor and model.</p>
			<p class="source-code">    pipe = Pipeline(</p>
			<p class="source-code">        steps=[(“preprocessor”, preprocessor), </p>
			<p class="source-code">               (“model”, RandomForestClassifier(random_state=0))]</p>
			<p class="source-code">    )</p>
			<p>Set hyperparmeter values.</p>
			<p class="source-code">    pipe = pipe.set_params(**PARAMS)</p>
			<p class="source-code"> </p>
			<p class="source-code">    return pipe</p>
			<p>The <strong class="source-inline">run</strong> function<a id="_idIndexMarker497"/> is responsible for training the model and getting the cross-validation score:</p>
			<p class="source-code">import nni</p>
			<p class="source-code">import logging</p>
			<p class="source-code">from sklearn.model_selection import cross_val_score</p>
			<p class="source-code">LOG = logging.getLogger('nni_sklearn')</p>
			<p class="source-code">def <strong class="bold">run</strong>(X_train, y_train, model):</p>
			<p class="source-code">    model.fit(X_train, y_train)</p>
			<p class="source-code">    score = np.mean(cross_val_score(model,X_train, y_train, </p>
			<p class="source-code">                    cv=5, scoring='f1')</p>
			<p class="source-code">            )</p>
			<p class="source-code">    LOG.debug('score: %s', score)</p>
			<p class="source-code">    nni.report_final_result(score)</p>
			<p>Finally, we can call those functions in the same script:</p>
			<p class="source-code"><strong class="bold">if __name__ == '__main__':</strong></p>
			<p class="source-code">    X_train, _, y_train, _, numerical_feats, categorical_feats = load_data()</p>
			<p class="source-code">    try:</p>
			<p class="source-code">        # get parameters from tuner</p>
			<p class="source-code">        RECEIVED_PARAMS = nni.get_next_parameter()</p>
			<p class="source-code">        LOG.debug(RECEIVED_PARAMS)</p>
			<p class="source-code">        PARAMS = get_default_parameters()</p>
			<p class="source-code">        PARAMS.update(RECEIVED_PARAMS)</p>
			<p class="source-code">        LOG.debug(PARAMS)</p>
			<p class="source-code">        model = get_model(PARAMS, numerical_feats, categorical_feats)</p>
			<p class="source-code">        run(X_train, y_train, model)</p>
			<p class="source-code">    except Exception as exception:</p>
			<p class="source-code">        LOG.exception(exception)</p>
			<p class="source-code">        raise</p>
			<ol>
				<li value="2">Define the<a id="_idIndexMarker498"/> hyperparameter space in a JSON file called <strong class="source-inline">hyperparameter_space.json</strong>:<p class="source-code">{“model__n_estimators”: {“_type”: “randint”, “_value”: [5, 200]}, “model__criterion”: {“_type”: “choice”, “_value”: [“gini”, “entropy”]}, “model__class_weight”: {“_type”: “choice”, “_value”: [“balanced”,”balanced_subsample”]}, “model__min_samples_split”: {“_type”: “uniform”, “_value”: [0, 0.1]}}</p></li>
				<li>Set up the experiment configurations via the <strong class="source-inline">config.yaml</strong> file:<p class="source-code">searchSpaceFile: hyperparameter_space.json</p><p class="source-code">experimentName: nni_sklearn</p><p class="source-code">trial_command: python '/mnt/c/Users/Louis\ Owen/Desktop/Packt/Hyperparameter-Tuning-with-Python/nni/model.py'</p><p class="source-code">trial_code_directory: .</p><p class="source-code">trial_concurrency: 10</p><p class="source-code">max_trial_number: 100 </p><p class="source-code">maxExperimentDuration: 1h</p><p class="source-code">tuner: </p><p class="source-code">  name: <strong class="bold">GridSearch</strong></p><p class="source-code">training_service:</p><p class="source-code">  platform: local</p></li>
				<li>Run the hyperparameter tuning experiment. We can see the experiment status and various interesting stats via the launched web portal. The following code shows how to run the experiment on port <strong class="source-inline">8080</strong> in <strong class="source-inline">local</strong>, meaning you can open the web portal on <strong class="source-inline">http://localhost:8080</strong>: <p class="source-code">nnictl create --config config.yaml --port 8080</p></li>
				<li>Train the model on full training data using the best set of hyperparameters found. To get<a id="_idIndexMarker499"/> the best set of hyperparameters, you can go to the web portal and see them from the <strong class="bold">Overview</strong> tab.</li>
			</ol>
			<p>Based on the experiment results shown in the web portal within the <em class="italic">Top trials</em> tab, the following are the best hyperparameter values found from the experiment. Note that we will discuss the web portal more in <a href="B18753_13_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 13</em></a>, <em class="italic">Tracking Hyperparameter Tuning Experiments</em>:</p>
			<p class="source-code">best_parameters = {</p>
			<p class="source-code">    “model__n_estimators”: 27,</p>
			<p class="source-code">    “model__criterion”: “entropy”,</p>
			<p class="source-code">    “model__class_weight”: “balanced_subsample”,</p>
			<p class="source-code">    “model__min_samples_split”: 0.05</p>
			<p class="source-code">}</p>
			<p>We can now train the model on full training data:</p>
			<p class="source-code">from sklearn.base import clone</p>
			<p class="source-code">tuned_pipe = clone(pipe).set_params(**best_parameters)</p>
			<p class="source-code"># Fit the pipeline on train data </p>
			<p class="source-code">tuned_pipe.fit(X_train_full,y_train)</p>
			<ol>
				<li value="6">Test the final trained model on the test data:<p class="source-code">y_pred = tuned_pipe.predict(X_test_full)</p><p class="source-code">print(f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.517</strong> in the F1-score when testing our final trained Random Forest model with the best set of hyperparameters on the test set.</p>
			<p>In this section, we have<a id="_idIndexMarker500"/> learned how to implement Grid Search with the NNI package via <strong class="source-inline">nnictl</strong>. In the next section, we will learn how to implement Random Search with NNI via pure Python code.</p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor099"/>Implementing Random Search</h1>
			<p>Random Search is one of the <a id="_idIndexMarker501"/>variants of the Exhaustive Search hyperparameter tuning group (see <a href="B18753_03_ePub.xhtml#_idTextAnchor031"><em class="italic">Chapter 3</em></a>) that the NNI package can implement. Let’s use the same data, pipeline, and hyperparameter space as in the example in the previous section to show you how to implement Random Search with NNI using pure Python code.</p>
			<p>The following code shows how to implement Random Search with the NNI package. Here, we’ll use pure Python code instead of using <strong class="source-inline">nnictl</strong> as in the previous section. You can find the more detailed code in the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section:</p>
			<ol>
				<li value="1">Prepare the model to be tuned in a script. We’ll use the same <strong class="source-inline">model.py</strong> script as in the previous section.</li>
				<li> Define the hyperparameter space in the form of a Python dictionary:<p class="source-code">hyperparameter_space = { </p><p class="source-code">    'model__n_estimators': {'_type': 'randint', '_value': [5, 200]}, </p><p class="source-code">    'model__criterion': {'_type': 'choice', '_value': ['gini', 'entropy']}, </p><p class="source-code">    'model__class_weight': {'_type': 'choice', '_value': [“balanced”,”balanced_subsample”]}, </p><p class="source-code">    'model__min_samples_split': {'_type': 'uniform', '_value': [0, 0.1]}, </p><p class="source-code">}  </p></li>
				<li>Set up the experiment configurations via the <strong class="source-inline">Experiment</strong> class. Note that there is only one parameter for the Random Search tuner, namely the random <strong class="source-inline">seed</strong> parameter:<p class="source-code">experiment = Experiment('local')</p><p class="source-code"> </p><p class="source-code">experiment.config.experiment_name = 'nni_sklearn_random_search'</p><p class="source-code">experiment.config.tuner.name = <strong class="bold">'Random'</strong></p><p class="source-code">experiment.config.tuner.<strong class="bold">class_args['seed']</strong> = 0</p><p class="source-code"> </p><p class="source-code"># Boilerplate code</p><p class="source-code">experiment.config.trial_command = “python '/mnt/c/Users/Louis\ Owen/Desktop/Packt/Hyperparameter-Tuning-with-Python/nni/model.py'”</p><p class="source-code">experiment.config.trial_code_directory = '.'</p><p class="source-code">experiment.config.search_space = hyperparameter_space</p><p class="source-code">experiment.config.max_trial_number = 100</p><p class="source-code">experiment.config.trial_concurrency = 10</p><p class="source-code">experiment.config.max_experiment_duration = '1h'</p></li>
				<li>Run the hyperparameter tuning experiment:<p class="source-code">experiment.run(8080, wait_completion = True, debug = False)</p></li>
				<li>Train the model on full training data using the best set of hyperparameters found. </li>
			</ol>
			<p>Get the best set of hyperparameters:</p>
			<p class="source-code">best_trial = sorted(experiment.export_data(),key = lambda x: x.value, reverse = True)[0]</p>
			<p class="source-code">print(best_trial.parameter)</p>
			<ol>
				<li value="6">Based on the <a id="_idIndexMarker502"/>preceding code, we get the following results:<p class="source-code">{'model__n_estimators': 194, 'model__criterion': 'entropy', 'model__class_weight': 'balanced_subsample', 'model__min_samples_split': 0.0014706304965369289}</p></li>
			</ol>
			<p>We can now train the model on full training data:</p>
			<p class="source-code">from sklearn.base import clone</p>
			<p class="source-code">tuned_pipe = clone(pipe).set_params(**best_trial.parameter)</p>
			<p class="source-code"># Fit the pipeline on train data </p>
			<p class="source-code">tuned_pipe.fit(X_train_full,y_train)</p>
			<ol>
				<li value="7">Test the final trained model on the test data:<p class="source-code">y_pred = tuned_pipe.predict(X_test_full)</p><p class="source-code">print(f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.597</strong> in the F1-score when testing our final trained Random Forest model with the best set of hyperparameters on the test set.</p>
			<p>In this section, we have learned how to implement Random Search using NNI with pure Python code. In the next section, we will learn how to implement Tree-structured Parzen Estimators with NNI via pure Python code.</p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor100"/>Implementing Tree-structured Parzen Estimators</h1>
			<p><strong class="bold">Tree-structured Parzen Estimators</strong> (<strong class="bold">TPEs</strong>) are one of the variants of the Bayesian Optimization hyperparameter tuning group (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>) that the NNI package can implement. Let’s use the <a id="_idIndexMarker503"/>same data, pipeline, and hyperparameter space as in the example in the previous section to implement TPE with NNI using pure Python code.</p>
			<p>The following code shows how to implement TPE with the NNI package using pure Python code. You can find the more detailed code in the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section:</p>
			<ol>
				<li value="1">Prepare the model to be tuned in a script. We’ll use the same <strong class="source-inline">model.py</strong> script as in the previous section.</li>
				<li>Define the hyperparameter space in the form of a Python dictionary. We’ll use the same hyperparameter space as in the previous section.</li>
				<li>Set up the experiment configurations via the <strong class="source-inline">Experiment</strong> class. Note that there are three parameters for the TPE tuner: <strong class="source-inline">optimize_mode</strong>, <strong class="source-inline">seed</strong>, and <strong class="source-inline">tpe_args</strong>. Please refer to the official documentation page for more information regarding <a id="_idIndexMarker504"/>the TPE tuner parameters (<a href="https://nni.readthedocs.io/en/stable/reference/hpo.html#tpe-tuner">https://nni.readthedocs.io/en/stable/reference/hpo.html#tpe-tuner</a>):<p class="source-code">experiment = Experiment('local')</p><p class="source-code">experiment.config.experiment_name = 'nni_sklearn_tpe'</p><p class="source-code">experiment.config.tuner.name = <strong class="bold">'TPE'</strong></p><p class="source-code">experiment.config.tuner.<strong class="bold">class_args</strong> = {<strong class="bold">'optimize_mode'</strong>: 'maximize', <strong class="bold">'seed'</strong>: 0}</p><p class="source-code"> </p><p class="source-code"># Boilerplate code</p><p class="source-code"># same with previous section</p></li>
				<li>Run the hyperparameter tuning experiment:<p class="source-code">experiment.run(8080, wait_completion = True, debug = False)</p></li>
				<li>Train the model on full training data using the best set of hyperparameters found. </li>
			</ol>
			<p>Get the best set <a id="_idIndexMarker505"/>of hyperparameters:</p>
			<p class="source-code">best_trial = sorted(experiment.export_data(),key = lambda x: x.value, reverse = True)[0]</p>
			<p class="source-code">print(best_trial.parameter)</p>
			<p>Based on the preceding code, we get the following results:</p>
			<p class="source-code">{'model__n_estimators': 195, 'model__criterion': 'entropy', 'model__class_weight': 'balanced_subsample', 'model__min_samples_split': 0.0006636374717157983}</p>
			<p>We can now <a id="_idIndexMarker506"/>train the model on full training data:</p>
			<p class="source-code">from sklearn.base import clone</p>
			<p class="source-code">tuned_pipe = clone(pipe).set_params(**best_trial.parameter)</p>
			<p>Fit the pipeline on train data.</p>
			<p class="source-code">tuned_pipe.fit(X_train_full,y_train)</p>
			<ol>
				<li value="6">Test the final trained model on the test data:<p class="source-code">y_pred = tuned_pipe.predict(X_test_full)</p><p class="source-code">print(f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.618</strong> in the F1-score when testing our final trained Random Forest model with the best set of hyperparameters on the test set.</p>
			<p>In this section, we have learned how to implement TPE using NNI with pure Python code. In the next section, we will learn how to implement Sequential Model Algorithm Configuration with<a id="_idIndexMarker507"/> NNI via pure Python code.</p>
			<h1 id="_idParaDest-96">Implementing Sequential Model Algorit<a id="_idTextAnchor101"/>hm Configuration</h1>
			<p><strong class="bold">Sequential Model Algorithm Configuration</strong> (<strong class="bold">SMAC</strong>) is one of the variants of the Bayesian Optimization <a id="_idIndexMarker508"/>hyperparameter tuning group (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>) that the NNI package can implement. Note that to use SMAC in NNI, we need to install additional dependencies using the following command: <strong class="source-inline">pip install "nni[SMAC]"</strong>. Let’s use the same data, pipeline, and hyperparameter space as in the example in the previous section to implement SMAC with NNI using pure Python code.</p>
			<p>The following code shows how to implement SMAC with the NNI package using pure Python code. You can find the more detailed code in the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section:</p>
			<ol>
				<li value="1">Prepare the model to be tuned in a script. We’ll use the same <strong class="source-inline">model.py</strong> script as in the previous section.</li>
				<li>Define the hyperparameter space in the form of a Python dictionary. We’ll use the same hyperparameter space as in the previous section.</li>
				<li>Set up the experiment configurations via the <strong class="source-inline">Experiment</strong> class. Note that there are two parameters for the SMAC tuner: <strong class="source-inline">optimize_mode</strong>, and <strong class="source-inline">config_dedup</strong>. Please refer to the <a id="_idIndexMarker509"/>official documentation page for more information regarding the SMAC tuner parameters (<a href="https://nni.readthedocs.io/en/stable/reference/hpo.html#smac-tuner">https://nni.readthedocs.io/en/stable/reference/hpo.html#smac-tuner</a>):<p class="source-code">experiment = Experiment('local')</p><p class="source-code"> </p><p class="source-code">experiment.config.experiment_name = 'nni_sklearn_smac'</p><p class="source-code">experiment.config.tuner.name = <strong class="bold">'SMAC'</strong></p><p class="source-code">experiment.config.tuner.<strong class="bold">class_args['optimize_mode']</strong> = 'maximize'</p><p class="source-code"># Boilerplate code</p><p class="source-code"># same with previous section</p></li>
				<li>Run the <a id="_idIndexMarker510"/>hyperparameter tuning experiment:<p class="source-code">experiment.run(8080, wait_completion = True, debug = False)</p></li>
				<li>Train the model on full training data using the best set of hyperparameters found. </li>
			</ol>
			<p>Get the best set of hyperparameters:</p>
			<p class="source-code">best_trial = sorted(experiment.export_data(),key = lambda x: x.value, reverse = True)[0]</p>
			<p class="source-code">print(best_trial.parameter)</p>
			<p>Based on the preceding code, we get the following results:</p>
			<p class="source-code">{'model__class_weight': 'balanced', 'model__criterion': 'entropy', 'model__min_samples_split': 0.0005502416428725066, 'model__n_estimators': 199}</p>
			<p>We can now train the model on full training data:</p>
			<p class="source-code">from sklearn.base import clone</p>
			<p class="source-code">tuned_pipe = clone(pipe).set_params(**best_trial.parameter)</p>
			<p class="source-code"># Fit the pipeline on train data </p>
			<p class="source-code">tuned_pipe.fit(X_train_full,y_train)</p>
			<ol>
				<li value="6">Test the final trained model on the test data:<p class="source-code">y_pred = tuned_pipe.predict(X_test_full)</p><p class="source-code">print(f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>Based on<a id="_idIndexMarker511"/> the preceding code, we get around <strong class="source-inline">0.619</strong> in the F1-score when testing our final trained Random Forest model with the best set of hyperparameters on the test set.</p>
			<p>In this section, we have learned how to implement SMAC using NNI with pure Python code. In the next section, we will learn how to implement Bayesian Optimization Gaussian Process with NNI via pure Python code.</p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor102"/>Implementing Bayesian Optimization Gaussian Process</h1>
			<p><strong class="bold">Bayesian Optimization Gaussian Process</strong> (<strong class="bold">BOGP</strong>) is one of the variants of the Bayesian Optimization <a id="_idIndexMarker512"/>hyperparameter tuning group (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>) that the NNI package can implement. Let’s use the same data, pipeline, and hyperparameter space as in the example in the previous section to implement BOGP with NNI using pure Python code.</p>
			<p>The following code shows how to implement BOGP with the NNI package using pure Python code. You can find the more detailed code in the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section:</p>
			<ol>
				<li value="1">Prepare the model to be tuned in a script. Here, we’ll use a new script called <strong class="source-inline">model_numeric.py</strong>. In this script, we add a mapping for non-numeric hyperparameters since BOGP can only work with numerical hyperparameters:<p class="source-code">non_numeric_mapping = params = {</p><p class="source-code">   'model__criterion': ['gini','entropy'],</p><p class="source-code">   'model__class_weight': ['balanced','balanced_subsample'],</p><p class="source-code">    }</p></li>
				<li>Define the hyperparameter space in the form of a Python dictionary. We’ll use a similar hyperparameter<a id="_idIndexMarker513"/> space as in the previous section with the only difference on the non-numeric hyperparameters. Here, all of the non-numeric hyperparameters are encoded into integer types of values:<p class="source-code">hyperparameter_space_numeric = { </p><p class="source-code">    'model__n_estimators': {'_type': 'randint', '_value': [5, 200]}, </p><p class="source-code">    <strong class="bold">'model__criterion'</strong>: {'_type': 'choice', '_value': <strong class="bold">[0, 1]</strong>}, </p><p class="source-code">    <strong class="bold">'model__class_weight'</strong>: {'_type': 'choice', '_value': <strong class="bold">[0, 1]</strong>}, </p><p class="source-code">    'model__min_samples_split': {'_type': 'uniform', '_value': [0, 0.1]}, </p><p class="source-code">}  </p></li>
				<li>Set up the experiment configurations via the <strong class="source-inline">Experiment</strong> class. Note that there are nine parameters for the BOGP tuner: <strong class="source-inline">optimize_mode</strong>, <strong class="source-inline">utility</strong>, <strong class="source-inline">kappa</strong>, <strong class="source-inline">xi</strong>, <strong class="source-inline">nu</strong>, <strong class="source-inline">alpha</strong>, <strong class="source-inline">cold_start_num</strong>, <strong class="source-inline">selection_num_warm_up</strong>, and <strong class="source-inline">selection_num_starting_points</strong>. Please refer to the official documentation page for more information<a id="_idIndexMarker514"/> regarding the BOGP tuner parameters (<a href="https://nni.readthedocs.io/en/stable/reference/hpo.html#gp-tuner">https://nni.readthedocs.io/en/stable/reference/hpo.html#gp-tuner</a>):<p class="source-code">experiment = Experiment('local')</p><p class="source-code"> </p><p class="source-code">experiment.config.experiment_name = 'nni_sklearn_bogp'</p><p class="source-code">experiment.config.tuner.name = <strong class="bold">'GPTuner'</strong></p><p class="source-code">experiment.config.tuner.<strong class="bold">class_args</strong> = {</p><p class="source-code"><strong class="bold">'optimize_mode'</strong>: 'maximize', <strong class="bold">'utility'</strong>: 'ei',<strong class="bold">'xi'</strong>: 0.01}</p><p class="source-code"># Boilerplate code</p><p class="source-code">experiment.config.trial_command = “python '/mnt/c/Users/Louis\ Owen/Desktop/Packt/Hyperparameter-Tuning-with-Python/nni/<strong class="bold">model_numeric.py</strong>'”</p><p class="source-code">experiment.config.trial_code_directory = '.'</p><p class="source-code">experiment.config.search_space = <strong class="bold">hyperparameter_space_numeric</strong></p><p class="source-code">experiment.config.max_trial_number = 100</p><p class="source-code">experiment.config.trial_concurrency = 10</p><p class="source-code">experiment.config.max_experiment_duration = '1h'</p></li>
				<li>Run the <a id="_idIndexMarker515"/>hyperparameter tuning experiment:<p class="source-code">experiment.run(8080, wait_completion = True, debug = False)</p></li>
				<li>Train the model on full training data using the best set of hyperparameters found. </li>
			</ol>
			<p>Get the best set of hyperparameters:</p>
			<p class="source-code"><strong class="bold">non_numeric_mapping</strong> = params = {</p>
			<p class="source-code">'model__criterion': ['gini','entropy'],</p>
			<p class="source-code">'model__class_weight': ['balanced','balanced_subsample'],</p>
			<p class="source-code">    }</p>
			<p class="source-code">best_trial = sorted(experiment.export_data(),key = lambda x: x.value, reverse = True)[0]</p>
			<p class="source-code">for key in non_numeric_mapping:</p>
			<p class="source-code">    best_trial.parameter[key] = non_numeric_mapping[key][best_trial.parameter[key]]</p>
			<p class="source-code">print(best_trial.parameter)</p>
			<p>Based on<a id="_idIndexMarker516"/> the preceding code, we get the following results:</p>
			<p class="source-code">{'model__class_weight': 'balanced_subsample', 'model__criterion': 'entropy', 'model__min_samples_split': 0.00055461211818435, 'model__n_estimators': 159}</p>
			<p>We can now train the model on full training data:</p>
			<p class="source-code">from sklearn.base import clone</p>
			<p class="source-code">tuned_pipe = clone(pipe).set_params(**best_trial.parameter)</p>
			<p>Fit the pipeline on train data.</p>
			<p class="source-code">tuned_pipe.fit(X_train_full,y_train)</p>
			<ol>
				<li value="6">Test the final trained model on the test data:<p class="source-code">y_pred = tuned_pipe.predict(X_test_full)</p><p class="source-code">print(f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.619</strong> in the F1-score when testing our final trained Random Forest model with the best set of hyperparameters on the test set.</p>
			<p>In this section, we have learned how to implement BOGP using NNI with pure Python code. In the next section, we will learn how to implement Metis with NNI via pure Python code.</p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor103"/>Implementing Metis</h1>
			<p><strong class="bold">Metis</strong> is one of the variants of the <a id="_idIndexMarker517"/>Bayesian Optimization hyperparameter tuning group (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>) that the NNI package can implement. Let’s use the same data, pipeline, and hyperparameter space as in the example in the previous section to implement Metis with NNI using pure Python code.</p>
			<p>The following code shows how to implement Metis with the NNI package using pure Python code. You can find the more detailed code in the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section:</p>
			<ol>
				<li value="1">Prepare the model to be tuned in a script. Here, we’ll use the same script as in the previous section, <strong class="source-inline">model_numeric.py</strong>, since Metis can only work with numerical hyperparameters.</li>
				<li>Define the hyperparameter space in the form of a Python dictionary. We’ll use the same hyperparameter space as in the previous section.</li>
				<li>Set up the experiment configurations via the <strong class="source-inline">Experiment</strong> class. Note that there are six parameters for the Metis tuner: <strong class="source-inline">optimize_mode</strong>, <strong class="source-inline">no_resampling</strong>, <strong class="source-inline">no_candidates</strong>, <strong class="source-inline">selection_num_starting_points</strong>, <strong class="source-inline">cold_start_num</strong>, and <strong class="source-inline">exploration_probability</strong>. Please refer to the official documentation page for more <a id="_idIndexMarker518"/>information regarding the Metis tuner parameters (<a href="https://nni.readthedocs.io/en/stable/reference/hpo.html#metis-tuner">https://nni.readthedocs.io/en/stable/reference/hpo.html#metis-tuner</a>):<p class="source-code">experiment = Experiment('local')</p><p class="source-code"> </p><p class="source-code">experiment.config.experiment_name = 'nni_sklearn_metis'</p><p class="source-code">experiment.config.tuner.name = <strong class="bold">'MetisTuner'</strong></p><p class="source-code">experiment.config.tuner.<strong class="bold">class_args['optimize_mode']</strong> = 'maximize'</p><p class="source-code"># Boilerplate code </p><p class="source-code"># same as previous section</p></li>
				<li>Run the hyperparameter tuning experiment:<p class="source-code">experiment.run(8080, wait_completion = True, debug = False)</p></li>
				<li>Train the <a id="_idIndexMarker519"/>model on full training data using the best set of hyperparameters found. </li>
			</ol>
			<p>Get the best set of hyperparameters:</p>
			<p class="source-code">non_numeric_mapping = params = {</p>
			<p class="source-code">'model__criterion': ['gini','entropy'],</p>
			<p class="source-code">'model__class_weight': ['balanced','balanced_subsample'],</p>
			<p class="source-code">    }</p>
			<p class="source-code">best_trial = sorted(experiment.export_data(),key = lambda x: x.value, reverse = True)[0]</p>
			<p class="source-code">for key in non_numeric_mapping:</p>
			<p class="source-code">    best_trial.parameter[key] = non_numeric_mapping[key][best_trial.parameter[key]]</p>
			<p class="source-code">print(best_trial.parameter)</p>
			<p>Based on the preceding code, we get the following results:</p>
			<p class="source-code">{'model__n_estimators': 122, 'model__criterion': 'gini', 'model__class_weight': 'balanced', 'model__min_samples_split': 0.00173059072806428}</p>
			<p>We can now train the model on full training data:</p>
			<p class="source-code">from sklearn.base import clone</p>
			<p class="source-code">tuned_pipe = clone(pipe).set_params(**best_trial.parameter)</p>
			<p class="source-code"># Fit the pipeline on train data </p>
			<p class="source-code">tuned_pipe.fit(X_train_full,y_train)</p>
			<ol>
				<li value="6">Test the final trained model on the test data:<p class="source-code">y_pred = tuned_pipe.predict(X_test_full)</p><p class="source-code">print(f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.590</strong> in the F1-score when testing our final trained<a id="_idIndexMarker520"/> Random Forest model with the best set of hyperparameters on the test set.</p>
			<p>In this section, we have learned how to implement Metis using NNI with pure Python code. In the next section, we will learn how to implement Simulated Annealing with NNI via pure Python code.</p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor104"/>Implementing Simulated Annealing</h1>
			<p>Simulated Annealing is one<a id="_idIndexMarker521"/> of the variants of the Heuristic Search hyperparameter tuning group (see <a href="B18753_05_ePub.xhtml#_idTextAnchor047"><em class="italic">Chapter 5</em></a>) that the NNI package can implement. Let’s use the same data, pipeline, and hyperparameter space as in the example in the previous section, to implement Simulated Annealing with NNI using pure Python code.</p>
			<p>The following code shows how to implement Simulated Annealing with the NNI package using pure Python code. You can find the more detailed code in the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section:</p>
			<ol>
				<li value="1">Prepare the model to be tuned in a script. We’ll use the same <strong class="source-inline">model.py</strong> script as in the <em class="italic">Implementing Grid Search</em> section.</li>
				<li>Define the hyperparameter space in the form of a Python dictionary. We’ll use the same hyperparameter space as in the <em class="italic">Implementing Grid Search</em> section.</li>
				<li>Set up the experiment configurations via the <strong class="source-inline">Experiment</strong> class. Note that there is one parameter for the Simulated Annealing tuner, namely <strong class="source-inline">optimize_mode</strong>: <p class="source-code">experiment = Experiment('local')</p><p class="source-code"> </p><p class="source-code">experiment.config.experiment_name = 'nni_sklearn_anneal'</p><p class="source-code">experiment.config.tuner.name = 'Anneal'</p><p class="source-code">experiment.config.tuner.class_args['optimize_mode'] = 'maximize'</p><p class="source-code"># Boilerplate code</p><p class="source-code">experiment.config.trial_command = “python '/mnt/c/Users/Louis\ Owen/Desktop/Packt/Hyperparameter-Tuning-with-Python/nni/model.py'”</p><p class="source-code">experiment.config.trial_code_directory = '.'</p><p class="source-code">experiment.config.search_space = hyperparameter_space</p><p class="source-code">experiment.config.max_trial_number = 100</p><p class="source-code">experiment.config.trial_concurrency = 10</p><p class="source-code">experiment.config.max_experiment_duration = '1h'</p></li>
				<li>Run the hyperparameter tuning experiment:<p class="source-code">experiment.run(8080, wait_completion = True, debug = False)</p></li>
				<li>Train the<a id="_idIndexMarker522"/> model on full training data using the best set of hyperparameters found. </li>
			</ol>
			<p>Get the best set of hyperparameters:</p>
			<p class="source-code">best_trial = sorted(experiment.export_data(),key = lambda x: x.value, reverse = True)[0]</p>
			<p class="source-code">print(best_trial.parameter)</p>
			<p>Based on the preceding code, we get the following results:</p>
			<p class="source-code">{'model__n_estimators': 103, 'model__criterion': 'gini', 'model__class_weight': 'balanced_subsample', 'model__min_samples_split': 0.0010101249953063539}</p>
			<p>We can now<a id="_idIndexMarker523"/> train the model on full training data:</p>
			<p class="source-code">from sklearn.base import clone</p>
			<p class="source-code">tuned_pipe = clone(pipe).set_params(**best_trial.parameter)</p>
			<p class="source-code"># Fit the pipeline on train data </p>
			<p class="source-code">tuned_pipe.fit(X_train_full,y_train)</p>
			<ol>
				<li value="6">Test the final trained model on the test data:<p class="source-code">y_pred = tuned_pipe.predict(X_test_full)</p><p class="source-code">print(f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.600</strong> in the F1-score when testing our final trained Random Forest model with the best set of hyperparameters on the test set.</p>
			<p>In this section, we have learned how to implement Simulated Annealing using NNI with pure Python code. In the next section, we will learn how to implement Hyper Band with NNI via pure Python code.</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor105"/>Implementing Hyper Band</h1>
			<p>Hyper Band<a id="_idIndexMarker524"/> is one of the variants of the Multi-Fidelity Optimization hyperparameter tuning group (see <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a>) that the NNI package can implement. Let’s use the same data, pipeline, and hyperparameter space as in the example in the previous section to implement Hyper Band with NNI using pure Python code.</p>
			<p>The following code shows how to implement Hyper Band with the NNI package using pure Python<a id="_idIndexMarker525"/> code. You can find the more detailed code in the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section:</p>
			<ol>
				<li value="1">Prepare the model to be tuned in a script. Here, we’ll use a new script called <strong class="source-inline">model_advisor.py</strong>. In this script, we utilize the <strong class="source-inline">TRIAL_BUDGET</strong> value from the output of <strong class="source-inline">nni.get_next_parameter()</strong> to update the <strong class="source-inline">'model__n_estimators'</strong> hyperparameter. </li>
				<li>Define the hyperparameter space in the form of a Python dictionary. We’ll use a similar hyperparameter space to the <em class="italic">Implementing Grid Search</em> section but we will remove the <strong class="source-inline">'model__n_estimators'</strong> hyperparameter since it will become the budget definition for Hyper Band:<p class="source-code">hyperparameter_space_advisor = { </p><p class="source-code">    'model__criterion': {'_type': 'choice', '_value': ['gini', 'entropy']}, </p><p class="source-code">    'model__class_weight': {'_type': 'choice', '_value': [“balanced”,”balanced_subsample”]}, </p><p class="source-code">    'model__min_samples_split': {'_type': 'uniform', '_value': [0, 0.1]}, </p><p class="source-code">}  </p></li>
				<li>Set up the experiment configurations via the <strong class="source-inline">Experiment</strong> class. Note that there are four parameters for the Hyper Band advisor: <strong class="source-inline">optimize_mode</strong>, <strong class="source-inline">R</strong>, <strong class="source-inline">eta</strong>, and <strong class="source-inline">exec_mode</strong>. Please refer to the official documentation page for more information regarding <a id="_idIndexMarker526"/>the Hyper Band advisor parameters (<a href="https://nni.readthedocs.io/en/latest/reference/hpo.html#hyperband-tuner">https://nni.readthedocs.io/en/latest/reference/hpo.html</a>#hyperband-tuner):<p class="source-code">experiment = Experiment('local')</p><p class="source-code"> </p><p class="source-code">experiment.config.experiment_name = 'nni_sklearn_hyper_band'</p><p class="source-code">experiment.config.<strong class="bold">advisor</strong>.name = <strong class="bold">'Hyperband'</strong></p><p class="source-code">experiment.config.advisor.class_args[<strong class="bold">'optimize_mode'</strong>] = 'maximize'</p><p class="source-code">experiment.config.advisor.class_args[<strong class="bold">'R'</strong>] = 200</p><p class="source-code">experiment.config.advisor.class_args[<strong class="bold">'eta'</strong>] = 3</p><p class="source-code">experiment.config.advisor.class_args[<strong class="bold">'exec_mode'</strong>] = 'parallelism'</p><p class="source-code"> </p><p class="source-code"># Boilerplate code</p><p class="source-code">experiment.config.trial_command = “python '/mnt/c/Users/Louis\ Owen/Desktop/Packt/Hyperparameter-Tuning-with-Python/nni/<strong class="bold">model_advisor</strong>.py'”</p><p class="source-code">experiment.config.trial_code_directory = '.'</p><p class="source-code">experiment.config.search_space = <strong class="bold">hyperparameter_space_advisor</strong></p><p class="source-code">experiment.config.max_trial_number = 100</p><p class="source-code">experiment.config.trial_concurrency = 10</p><p class="source-code">experiment.config.max_experiment_duration = '1h'</p></li>
				<li>Run the <a id="_idIndexMarker527"/>hyperparameter tuning experiment:<p class="source-code">experiment.run(8080, wait_completion = True, debug = False)</p></li>
				<li>Train the model on full training data using the best set of hyperparameters found. </li>
			</ol>
			<p>Get the best set of hyperparameters:</p>
			<p class="source-code">best_trial = sorted(experiment.export_data(),key = lambda x: x.value, reverse = True)[0]</p>
			<p class="source-code">best_trial.parameter['model__n_estimators'] = best_trial.parameter['TRIAL_BUDGET'] * 50</p>
			<p class="source-code">del best_trial.parameter['TRIAL_BUDGET']</p>
			<p class="source-code">print(best_trial.parameter)</p>
			<p>Based on the<a id="_idIndexMarker528"/> preceding code, we get the following results:</p>
			<p class="source-code">{'model__criterion': 'gini', 'model__class_weight': 'balanced_subsample', 'model__min_samples_split': 0.001676130360763284, 'model__n_estimators': 100}</p>
			<p>We can now train the model on full training data:</p>
			<p class="source-code">from sklearn.base import clone</p>
			<p class="source-code">tuned_pipe = clone(pipe).set_params(**best_trial.parameter)</p>
			<p>Fit the pipeline on train data.</p>
			<p class="source-code">tuned_pipe.fit(X_train_full,y_train)</p>
			<ol>
				<li value="6">Test the final trained model on the test data:<p class="source-code">y_pred = tuned_pipe.predict(X_test_full)</p><p class="source-code">print(f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.593</strong> in the F1-score when testing our final trained Random Forest model with the best set of hyperparameters on the test set.</p>
			<p>In this section, we have learned how to implement Hyper Band using NNI with pure Python code. In the next section, we will learn how to implement Bayesian Optimization Hyper Band with NNI via pure Python code.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor106"/>Implementing Bayesian Optimization Hyper Band</h1>
			<p><strong class="bold">Bayesian Optimization Hyper Band</strong> (<strong class="bold">BOHB</strong>) is one of the variants of the Multi-Fidelity Optimization<a id="_idIndexMarker529"/> hyperparameter tuning group (see <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a>) that the NNI package can implement. Note that to use BOHB in NNI, we need to install additional dependencies using the following command: </p>
			<p class="source-code">pip install "nni[BOHB]"</p>
			<p>Let’s use the same data, pipeline, and hyperparameter space as in the example in the previous section to implement BOHB with NNI using pure Python code.</p>
			<p>The following code<a id="_idIndexMarker530"/> shows how to implement Hyper Band with the NNI package using pure Python code. You can find the more detailed code in the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section:</p>
			<ol>
				<li value="1">Prepare the model to be tuned in a script. We’ll use the same <strong class="source-inline">model_advisor.py</strong> script as in the previous section.</li>
				<li>Define the hyperparameter space in the form of a Python dictionary. We’ll use the same hyperparameter space as in the previous section.</li>
				<li>Set up the experiment configurations via the <strong class="source-inline">Experiment</strong> class. Note that there are 11 parameters for the BOHB advisor: <strong class="source-inline">optimize_mode</strong>, <strong class="source-inline">min_budget</strong>, <strong class="source-inline">max_budget</strong>, <strong class="source-inline">eta</strong>, <strong class="source-inline">min_points_in_model</strong>, <strong class="source-inline">top_n_percent</strong>, <strong class="source-inline">num_samples</strong>, <strong class="source-inline">random_fraction</strong>, <strong class="source-inline">bandwidth_factor</strong>, <strong class="source-inline">min_bandwidth</strong>, and <strong class="source-inline">config_space</strong>. Please refer to the official <a id="_idIndexMarker531"/>documentation page for more information regarding the Hyper Band advisor parameters (<a href="https://nni.readthedocs.io/en/latest/reference/hpo.html#bohb-tuner">https://nni.readthedocs.io/en/latest/reference/hpo.html#bohb-tuner</a>):<p class="source-code">experiment = Experiment('local')</p><p class="source-code"> </p><p class="source-code">experiment.config.experiment_name = 'nni_sklearn_bohb'</p><p class="source-code">experiment.config.<strong class="bold">advisor</strong>.name = <strong class="bold">'BOHB'</strong></p><p class="source-code">experiment.config.advisor.class_args['optimize_mode'] = 'maximize'</p><p class="source-code">experiment.config.advisor.class_args['max_budget'] = 200</p><p class="source-code">experiment.config.advisor.class_args['min_budget'] = 5</p><p class="source-code">experiment.config.advisor.class_args['eta'] = 3</p><p class="source-code"># Boilerplate code  </p><p class="source-code"># same as previous section</p></li>
				<li>Run the hyperparameter tuning experiment:<p class="source-code">experiment.run(8080, wait_completion = True, debug = False)</p></li>
				<li>Train the model on full training data using the best set of hyperparameters found. </li>
			</ol>
			<p>Get the best set <a id="_idIndexMarker532"/>of hyperparameters:</p>
			<p class="source-code">best_trial = sorted(experiment.export_data(),key = lambda x: x.value, reverse = True)[0]</p>
			<p class="source-code">best_trial.parameter['model__n_estimators'] = best_trial.parameter['TRIAL_BUDGET'] * 50</p>
			<p class="source-code">del best_trial.parameter['TRIAL_BUDGET']</p>
			<p class="source-code">print(best_trial.parameter)</p>
			<p>Based on the preceding code, we get the following results:</p>
			<p class="source-code">{'model__class_weight': 'balanced', 'model__criterion': 'gini', 'model__min_samples_split': 0.000396569883631686, 'model__n_estimators': 1100}</p>
			<p>We can now train the model on full training data:</p>
			<p class="source-code">from sklearn.base import clone</p>
			<p class="source-code">tuned_pipe = clone(pipe).set_params(**best_trial.parameter)</p>
			<p class="source-code"># Fit the pipeline on train data </p>
			<p class="source-code">tuned_pipe.fit(X_train_full,y_train)</p>
			<ol>
				<li value="6">Test the final trained model on the test data:<p class="source-code">y_pred = tuned_pipe.predict(X_test_full)</p><p class="source-code">print(f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>Based on<a id="_idIndexMarker533"/> the preceding code, we get around <strong class="source-inline">0.617</strong> in the F1-score when testing our final trained Random Forest model with the best set of hyperparameters on the test set.</p>
			<p>In this section, we have learned how to implement Bayesian Optimization Hyper Band using NNI with pure Python code. In the next section, we will learn how to implement Population-Based Training with NNI via <strong class="source-inline">nnictl</strong>.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor107"/>Implementing Population-Based Training</h1>
			<p><strong class="bold">Population-Based Training</strong> (<strong class="bold">PBT</strong>) is one of the variants of the Heuristic Search hyperparameter tuning group (see <a href="B18753_05_ePub.xhtml#_idTextAnchor047"><em class="italic">Chapter 5</em></a>) that the NNI package can implement. To show you how to implement PBT with NNI using pure Python code, let’s use the same example provided by the<a id="_idIndexMarker534"/> NNI package. Here, the MNIST dataset and a convolutional neural network model are utilized. We’ll use PyTorch to implement the neural network model. For details of the code example provided by NNI, please refer to the NNI GitHub repository (<a href="https://github.com/microsoft/nni/tree/1546962f83397710fe095538d052dc74bd981707/examples/trials/mnist-pbt-tuner-pytorch">https://github.com/microsoft/nni/tree/1546962f83397710fe095538d052dc74bd981707/examples/trials/mnist-pbt-tuner-pytorch</a>).</p>
			<p class="callout-heading">MNIST Dataset</p>
			<p class="callout">MNIST is a dataset of handwritten <a id="_idIndexMarker535"/>digits that have been size-normalized and centered in a fixed-size image. Here, we’ll use the MNIST dataset provided directly by the PyTorch package (<a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST">https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST</a>).</p>
			<p>The following code shows how to implement PBT with the NNI package. Here, we’ll use <strong class="source-inline">nnictl</strong> instead of using pure Python code. You can find the more detailed code in the GitHub repository<a id="_idIndexMarker536"/> mentioned in the <em class="italic">Technical requirements</em> section:</p>
			<ol>
				<li value="1">Prepare the model to be tuned in a script. Here, we’ll use the same <strong class="source-inline">mnist.py</strong> script from the NNI GitHub repository. Note that we save the script with a new name: <strong class="source-inline">model_pbt.py</strong>.</li>
				<li>Define the hyperparameter space in a JSON file called <strong class="source-inline">hyperparameter_space_pbt.json</strong>. Here, we’ll use the same <strong class="source-inline">search_space.json</strong> file from the NNI GitHub repository.</li>
				<li>Set up the experiment configurations via the <strong class="source-inline">config_pbt.yaml</strong> file. Note that there are six parameters for the PBT tuner: <strong class="source-inline">optimize_mode</strong>, <strong class="source-inline">all_checkpoint_dir</strong>, <strong class="source-inline">population_size</strong>, <strong class="source-inline">factor</strong>, <strong class="source-inline">resample_probability</strong>, and <strong class="source-inline">fraction</strong>. Please refer to the official documentation page for more information <a id="_idIndexMarker537"/>regarding the PBT tuner parameters (<a href="https://nni.readthedocs.io/en/latest/reference/hpo.html#pbt-tuner">https://nni.readthedocs.io/en/latest/reference/hpo.html#pbt-tuner</a>):<p class="source-code">searchSpaceFile: <strong class="bold">hyperparameter_space_pbt.json</strong></p><p class="source-code">trialCommand: python '/mnt/c/Users/Louis\ Owen/Desktop/Packt/Hyperparameter-Tuning-with-Python/nni/<strong class="bold">model_pbt.py</strong>'</p><p class="source-code">trialGpuNumber: 1</p><p class="source-code">trialConcurrency: 10</p><p class="source-code">maxTrialNumber: 100</p><p class="source-code">maxExperimentDuration: 1h</p><p class="source-code">tuner:</p><p class="source-code">  name: PBTTuner</p><p class="source-code">  classArgs:</p><p class="source-code">    optimize_mode: maximize</p><p class="source-code">trainingService:</p><p class="source-code">  platform: local</p><p class="source-code">  useActiveGpu: false</p></li>
				<li>Run the <a id="_idIndexMarker538"/>hyperparameter tuning experiment. We can see the experiment status and various interesting stats via the launched web portal. The following code shows how to run the experiment on port <strong class="source-inline">8080</strong> in <strong class="source-inline">local</strong>, meaning you can open the web portal on <strong class="source-inline">http://localhost:8080</strong>:<p class="source-code">nnictl create --config config_pbt.yaml --port 8080</p></li>
			</ol>
			<p>In this section, we have learned how to implement Population-Based Training with NNI via <strong class="source-inline">nnictl</strong> using the same example as provided in the official documentation of NNI.</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor108"/>Summary</h1>
			<p>In this chapter, we have learned all the important things about the DEAP and Microsoft NNI packages. We also have learned how to implement various hyperparameter tuning methods with the help of these packages, along with understanding each of the important parameters of the classes and how are they related to the theory that we have learned in the previous chapters. From now on, you should be able to utilize these packages to implement your chosen hyperparameter tuning method, and ultimately, boost the performance of your ML model. Equipped with the knowledge from <em class="italic">Chapters 3 – 6</em>, you will also be able to debug your code if there are errors or unexpected results, and be able to craft your own experiment configuration to match your specific problem.</p>
			<p>In the next chapter, we’ll learn about hyperparameters for several popular algorithms. There will be a wide explanation for each of the algorithms, including (but not limited to) the definition of each hyperparameter, what will be impacted when the value of each hyperparameter is changed, and the priority list of hyperparameters based on the impact.</p>
		</div>
		<div>
			<div id="_idContainer320">
			</div>
		</div>
	</body></html>