["```py\nIn: import numpy as np\n import pandas as pd\n import matplotlib.pyplot as plt\n from sklearn.datasets import load_boston\n from sklearn import linear_model\n %matplotlib inline\n #To set float output to 5 decimals and to suppress printing of \\small floating point values using scientific notation\n np.set_printoptions(precision=5, suppress=True)\n\n```", "```py\nIn:\tboston = load_boston()\n dataset = pd.DataFrame(boston.data, \\columns=boston.feature_names)\n dataset['target'] = boston.target\n observations = len(dataset)\n variables = dataset.columns[:-1]\n X = dataset.ix[:,:-1]\n y = dataset['target'].values\n\n```", "```py\nIn: yq = np.array(y>25, dtype=int)\n\n```", "```py\nIn: from sklearn.preprocessing import StandardScaler\n from sklearn.preprocessing import MinMaxScaler\n linear_regression = linear_model.LinearRegression(normalize=False,\\fit_intercept=True)\n linear_regression.fit(X,y)\n print (\"coefficients: %s\\nintercept: %0.3f\" % \\(linear_regression.coef_,linear_regression.intercept_))\n\nOut:\n\n```", "```py\nIn: dataset.min()\n\nOut:\n\n```", "```py\nIn: centering = StandardScaler(with_mean=True, with_std=False)\n linear_regression.fit(centering.fit_transform(X),y)\n print (\"coefficients: %s\\nintercept: %s\" % \\(linear_regression.coef_,linear_regression.intercept_))\n\nOut:\n\n```", "```py\nIn: print ('mean: %0.3f' % np.mean(y))\n\nOut:mean: 22.533\n\n```", "```py\nIn: standardization = StandardScaler(with_mean=True, with_std=True)\n linear_regression.fit(standardization.fit_transform(X),y)\n print (\"coefficients: %s\\nintercept: %0.3f\" % \\(linear_regression.coef_,linear_regression.intercept_))\n\nOut:\n\n```", "```py\nIn: scaling  = MinMaxScaler(feature_range=(0, 1))\n linear_regression.fit(scaling.fit_transform(X),y)\n print (\"coefficients: %s\\nintercept: %0.3f\" % \\(linear_regression.coef_,linear_regression.intercept_))\n\nOut:\n\n```", "```py\nIn: import statsmodels.api as sm\n Xq = sm.add_constant(standardization.fit_transform(X))\n logit = sm.Logit(yq, Xq)\n result = logit.fit()\n print (result.summary())\n\nOut:\n\n```", "```py\nIn: print ('odd ratios of coefficients: %s' % np.exp(result.params))\n\nOut: odd ratios of coefficients: [  0.04717   0.90948   1.2896    0.46908   1.2779    0.45277   3.75996   1.10314   0.28966  15.9012    0.16158   0.46602   0.81363   0.07275]\n\n```", "```py\nIn: def sigmoid(p):\n return 1 / (1 + np.exp(-p))\n\n print ('intercept: %0.3f' % result.params[0])\n print ('probability of value above 25 when all predictors are \\average: %0.3f' % \tsigmoid(result.params[0]))\n\nOut: intercept: -3.054\n probability of value above 25 when all predictors\n are average: 0.045\n\n```", "```py\nIn: print ('average likelihood of positive response: %0.3f' % \n (sum(yq) /float(len(yq))))\n\nOut: average likelihood of positive response: 0.245\n\n```", "```py\nIn: C = np.ones(len(X))\n logit = sm.Logit(yq, C)\n result = logit.fit()\n print (result.summary())\n print ('\\nprobability of value above 25 using just a constant: %0.3f' % \tsigmoid(result.params[0]))\n\nOut:\n\n```", "```py\nIn: outlook   = ['sunny', 'overcast', 'rainy']\n temperature = ['hot', 'mild', 'cool']\n humidity    = ['high', 'normal']\n windy       = ['TRUE', 'FALSE']\n\n weather_dataset = list()\n\n for o in outlook:\n for t in temperature:\n for h in humidity:\n for w in windy:\n weather_dataset.append([o,t,h,w])\n\n play = [0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n\n```", "```py\nIn: import pandas as pd\n df = pd.DataFrame(weather_dataset, columns=['outlook', \\'temperature', 'humidity', 'windy'])\n\n```", "```py\nIn: print (pd.get_dummies(df.humidity).ix[:5,:])\nOut:    high  normal\n 0     1       0\n 1     1       0\n 2     0       1\n 3     0       1\n 4     1       0\n 5     1       0\n\n```", "```py\nIn: dummy_encoding = pd.get_dummies(df)\n\n```", "```py\nIn: import statsmodels.api as sm\n X = sm.add_constant(dummy_encoding)\n logit = sm.Logit(play, X)\n result = logit.fit()\n print (result.summary())\n\nOut: \n\n```", "```py\nIn: X.drop(['outlook_sunny', 'temperature_mild', 'humidity_normal', 'windy_FALSE'], inplace=True, axis=1)\n logit = sm.Logit(play, X)\n result = logit.fit()\n print (result.summary())\n\nOut:\n\n```", "```py\nIn: from sklearn.feature_extraction import DictVectorizer\n vectorizer = DictVectorizer(sparse = False)\n dict_representation = [{varname:var for var, varname in \\zip(row,['outlook', 'temperature', 'humidity', 'windy'])}\n for row in weather_dataset]\n print (dict_representation[0])\n print (vectorizer.fit_transform(dict_representation))\n\nOut: {'windy': 'TRUE', 'humidity': 'high', 'temperature': 'hot', 'outlook': 'sunny'}\n [[ 1\\.  0\\.  0\\.  0\\.  1\\.  0\\.  1\\.  0\\.  0\\.  1.]\n [ 1\\.  0\\.  0\\.  0\\.  1\\.  0\\.  1\\.  0\\.  1\\.  0.]\n [ 0\\.  1\\.  0\\.  0\\.  1\\.  0\\.  1\\.  0\\.  0\\.  1.]\n...\n\n```", "```py\nIn: print (vectorizer.feature_names_)\n\nOut: ['humidity=high', 'humidity=normal', 'outlook=overcast', \\\n      'outlook=rainy', 'outlook=sunny', 'temperature=cool', \\\n      'temperature=hot', 'temperature=mild', 'windy=FALSE', \\\n      'windy=TRUE']\n\n```", "```py\nIn: from sklearn.preprocessing import LabelEncoder, LabelBinarizer\nlabel_encoder = LabelEncoder()\nprint (label_encoder.fit_transform(df.outlook))\n\nOut: [2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n\n```", "```py\nIn: label_encoder.inverse_transform([0,1,2])\n\nOut: array(['overcast', 'rainy', 'sunny'], dtype=object)\n\n```", "```py\nIn: print (label_encoder.classes_)\n\nOut: ['overcast' 'rainy' 'sunny']\n\n```", "```py\nIn: label_binarizer = LabelBinarizer(neg_label=0, pos_label=1, \\sparse_output=False)\n print (label_binarizer.fit_transform( \\label_encoder.fit_transform(df.outlook)))\n\nOut: [[0 0 1]\n [0 0 1]\n [0 0 1]\n...\n\n```", "```py\nIn: your_text = 'Nomina sunt consequentia rerum'\nmapping_words_in_text = {word:position for position, word in enumerate(set(your_text.lower().split(' ')))}\nprint (mapping_words_in_text)\n\nOut: {'rerum': 0, 'sunt': 1, 'consequentia': 2, 'nomina': 3}\n\n```", "```py\nIn: corpus = ['The quick fox jumped over the lazy dog', 'I sought a dog wondering around with a bird', 'My dog is named Fido']\n\n```", "```py\nIn: from sklearn.feature_extraction.text import CountVectorizer\ntextual_one_hot_encoder = CountVectorizer(binary=True)\ntextual_one_hot_encoder.fit(corpus)\nvectorized_text = textual_one_hot_encoder.transform(corpus)\nprint(vectorized_text.todense())\n\nOut: [[0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0]\n [1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1]\n [0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0]]\n\n```", "```py\nIn: print (textual_one_hot_encoder.get_feature_names())\n\nOut: ['around', 'bird', 'dog', 'fido', 'fox', 'is', 'jumped', 'lazy', 'my', 'named', 'over', 'quick', 'sought', 'the', 'with', 'wondering']\n\n```", "```py\nIn: print (textual_one_hot_encoder.transform(['John went home today']).todense())\n\nOut: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n\n```", "```py\nIn: from sklearn.feature_extraction.text import HashingVectorizer\n hashing_trick = HashingVectorizer(n_features=11, binary=True, \\norm=None, non_negative=True)\n M = hashing_trick.transform(corpus)\n print (M.todense())\n\nOut: [[ 1\\.  0\\.  0\\.  1\\.  1\\.  0\\.  0\\.  1\\.  0\\.  0\\.  0.]\n [ 0\\.  0\\.  0\\.  1\\.  0\\.  1\\.  0\\.  1\\.  1\\.  0\\.  0.]\n [ 0\\.  0\\.  0\\.  1\\.  0\\.  0\\.  0\\.  1\\.  1\\.  0\\.  0.]]\n\n```", "```py\nIn: print (hashing_trick.transform(['John is the owner of that dog']).todense())\n\nOut: [[1\\.  1\\.  1\\.  0\\.  0\\.  0\\.  0\\.  0\\.  0\\.  0\\.  0.]]\n\n```", "```py\nIn: import numpy as np\n boston = load_boston()\n labels = boston.feature_names\n X = boston.data\n y = boston.target\n print (boston.feature_names)\n\nOut: ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' \\'RAD' 'TAX' 'PTRATIO' 'B' 'LSTAT']\n\n```", "```py\nIn: linear_regression = \\linear_model.LinearRegression(fit_intercept=True)\n linear_regression.fit(X, y)\n\n from sklearn.metrics import r2_score\n print (\"R-squared: %0.3f\" % r2_score(y, \\linear_regression.predict(X)))\n\nOut: R-squared: 0.741\n\n```", "```py\nIn: residuals = y - linear_regression.predict(X)\n print (\"Head of residual %s\" % residuals[:5])\n print (\"Mean of residuals: %0.3f\" % np.mean(residuals))\n print (\"Standard deviation of residuals: %0.3f\" \\% np.std(residuals))\n\nOut: Head of residual [-6.00821 -3.42986  4.12977  4.79186  8.25712]\n Mean of residuals: 0.000\n Standard deviation of residuals: 4.680\n\n```", "```py\nIn: var = 7 # the variable in position 7 is DIS\n partial_residual = residuals + X[:,var] * \\linear_regression.coef_[var]\n plt.plot(X[:,var], partial_residual, 'wo')\n plt.xlabel(boston.feature_names[var])\n plt.ylabel('partial residuals')\n plt.show()\nOut: \n\n```", "```py\nIn: X_t = X.copy()\n X_t[:,var] = 1./np.sqrt(X_t[:,var])\n linear_regression.fit(X_t, y)\n partial_residual = residuals + X_t[:,var] * \\linear_regression.coef_[var]\n plt.plot(X_t[:,var], partial_residual, 'wo')\n plt.xlabel(boston.feature_names[var])\n plt.ylabel('partial residuals')\n plt.show()\n print (\"R-squared: %0.3f\" % r2_score(y, \\linear_regression.predict(X_t)))\n\nOut: R-squared: 0.769\n\n```", "```py\nIn: import numpy as np\n from sklearn.preprocessing import LabelBinarizer\n LB = LabelBinarizer()\n X_t = X.copy()\n edges = np.histogram(X_t[:,var], bins=20)[1]\n binning = np.digitize(X_t[:,var], edges)\n X_t = np.column_stack((np.delete(X_t, var, \\axis=1),LB.fit_transform(binning)))\n linear_regression.fit(X_t, y)\n print (\"R-squared: %0.3f\" % r2_score(y, \\linear_regression.predict(X_t)))\n\nOut: R-squared: 0.768\n\n```", "```py\nIn: import Numpy as np\n example = np.array([1,2,np.nan,4,5])\n print (example)\n\nOut: [  1\\.   2\\.  nan   4\\.   5.]\n\n```", "```py\nIn: print (np.isnan(example))\n\nOut: [False False  True False False]\n\n```", "```py\nIn: print (np.nan_to_num(example))\n\nOut: [ 1\\.  2\\.  0\\.  4\\.  5.]\n\n```", "```py\nIn: missing = np.isnan(example)\n replacing_value = np.mean(example[~missing])\n example[missing] = replacing_value\n print (example)\n\nOut: [ 1\\.  2\\.  3\\.  4\\.  5.]\n\n```", "```py\nIn: from random import sample, seed\n import numpy as np\n seed(19)\n Xm = X.copy()\n missing = sample(range(len(y)), len(y)//4)\n Xm[missing,5] = np.nan\n print (\"Header of Xm[:,5] : %s\" % Xm[:10,5])\n\nOut: Header of Xm[:,5] : [ 6.575    nan  7.185    nan  7.147  6.43   6.012  6.172    nan  6.004]\n\n```", "```py\nIn: from sklearn.preprocessing import Imputer\n impute = Imputer(missing_values = 'NaN', strategy='mean', axis=1)\n print (\"Header of imputed Xm[:,5] : %s\" % \\impute.fit_transform(Xm[:,5])[0][:10])\n\nOut: Header of imputed Xm[:,5] : [ 6.575    6.25446  7.185    6.25446  7.147    6.43     6.012    6.172  6.25446  6.004  ]\n\n```", "```py\nIn: missing_indicator = np.isnan(Xm[:,5]).astype(int)\n print (\"Header of missing indicator : %s\" \\% missing_indicator[:10])\n\nOut: Header of missing indicator : [0 1 1 0 0 0 0 0 1 1]\n\n```", "```py\nIn: boston = load_boston()\n dataset = pd.DataFrame(boston.data, columns=boston.feature_names)\n labels = boston.feature_names\n X = dataset\n y = boston.target\n\n```", "```py\nIn: plt.boxplot(y,labels=('y'))\nplt.show()\n\n```", "```py\nIn: scatter = plt.plot(linear_regression.predict(X), \\standardized_residuals, 'wo')\n plt.plot([-10,50],[0,0], \"r-\")\n plt.plot([-10,50],[3,3], \"r--\")\n plt.plot([-10,50],[-3,-3], \"r--\")\n plt.xlabel('fitted values')\n plt.ylabel('standardized residuals')\n plt.show()\n\n```", "```py\nIn: standardization = StandardScaler(with_mean=True, with_std=True)\n Xs = standardization.fit_transform(X)\n boxplot = plt.boxplot(Xs[:,0:7],labels=labels[0:7])\n\n```", "```py\nIn: boxplot = plt.boxplot(Xs[:,7:13],labels=labels[7:13])\n\n```", "```py\nIn: from sklearn.decomposition import PCA\n pca = PCA()\n pca.fit(Xs)\n C = pca.transform(Xs)\n print (pca.explained_variance_ratio_)\n\nOut: [ 0.47097  0.11016  0.09547  0.06598  0.0642   0.05074 \\0.04146  0.0305\t0.02134  0.01694  0.01432  0.01301  0.00489]\n\nIn: import numpy as np\n import matplotlib.pyplot as plt\n explained_variance = pca.explained_variance_ratio_\n plt.title('Portion of explained variance by component')\n range_ = [r+1 for r in range(len(explained_variance))]\n plt.bar(range_,explained_variance, color=\"b\", alpha=0.4, \\align=\"center\")\n plt.plot(range_,explained_variance,'ro-')\n for pos, pct in enumerate(explained_variance):\n plt.annotate(str(round(pct,2)), (pos+1,pct+0.007))\n plt.xticks(range_)\n plt.show()\n\nOut:\n\n```", "```py\nIn: scatter = plt.scatter(C[:,0],C[:,1], facecolors='none', \\edgecolors='black')\n plt.xlabel('Dimension 1')\n plt.ylabel('Dimension 2')\n\n```", "```py\nIn: scatter = plt.scatter(C[:,0],C[:,2], facecolors='none', \\edgecolors='black')\n plt.xlabel('Dimension 1')\n plt.ylabel('Dimension 3')\nOut:\n\n```"]