<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Classification Algorithms</h1>
                </header>
            
            <article>
                
<p>Classification problems involve detecting patterns in data and using those patterns to assign a data point to a group of similar data points. If that's too abstract, here are some examples of classification problems: analyzing an email to determine whether it's spam; detecting the language of a piece of text; reading an article and categorizing it as finance, sports, politics, opinion pieces, or crime; and determining whether a review of your product <span>posted on Twitter </span>is positive or negative (this last example is commonly called <strong>sentiment analysis</strong>).</p>
<p>Classification algorithms are tools that solve classification problems. By definition, they are supervised learning algorithms, as they'll always need a labeled training set to build a model from. There are lots of classification algorithms, each designed with a specific principle in mind or for a particular type of input data.</p>
<p>In this chapter, we'll discuss four classifiers: <strong>k-Nearest Neighbors</strong> (<strong>KNN</strong>), Naive Bayes, <strong>Support Vector Machines</strong> (<strong>SVMs</strong>), and random forest. Here's a brief introduction to each of the algorithms:</p>
<ul>
<li>The KNN algorithm is one of the simplest classifiers, and works well when your dataset has numerical features and clustered patterns. It is similar in nature to the k-means clustering algorithm, in that it relies on plotting data points and measuring distances from point to point.</li>
<li>The Naive Bayes classifier is an effective and versatile classifier based on Bayesian probability. While it can be used for numerical data, it's most commonly used in text classification problems, such as spam detection and sentiment analysis. Naive Bayes classifiers, when implemented properly, can be both fast and highly accurate for narrow domains. The Naive Bayes classifier is one of my go-to algorithms for classification.</li>
</ul>
<ul>
<li>SVMs are, in spirit, a very advanced form of the KNN algorithm. The SVM graphs your data and attempts to find dividing lines between the categories you've labeled. Using some non-trivial mathematics, the SVM can linearize non-linear patterns, so this tool can be effective for both linear and non linear data.</li>
<li>Random forests are a relatively recent development in classification algorithms, but they are effective and versatile and therefore a go-to classifier for many researchers, myself included. Random forests build an ensemble of decision trees (another type of classifier we'll discuss later), each with a random subset of the data's features. Decision trees can handle both numerical and categorical data, they can perform both regression and classification tasks, and they also assist in feature selection, so they are becoming many researchers' first tool to grab when facing new problems.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">k-Nearest Neighbor</h1>
                </header>
            
            <article>
                
<p>The KNN is a simple, fast, and straightforward classification algorithm. It is very useful for categorized numerical datasets where the data is naturally clustered. It will feel similar in some ways to the k-means clustering algorithm, with the major distinction being that k-means is an unsupervised algorithm while KNN is a supervised learning algorithm.</p>
<p>If you were to perform a KNN analysis <span><span>manually</span></span>, here's how it would go: first, plot all your training data on a graph, and label each point with its category or label. When you wish to classify a new, unknown point, put it on the graph and find the <em>k</em> closest points to it (the <em>nearest neighbors</em>). The number <em>k</em> should be an odd number in order to avoid ties; three is a good starting point, but some applications will need more and some can get away with one. Report whatever the majority of the <em>k</em> nearest neighbors are classified as, and that will be the result of the algorithm.</p>
<p>Finding the <em>k</em> nearest neighbors to a test point is straightforward, but can use some optimizations if your training data is very large. Typically, when evaluating a new point, you would calculate the Euclidean distance (the typical, high school geometry distance measure we introduced in <a href="84fd2c4d-41b4-46c4-82e5-4d8e55bb0066.xhtml" target="_blank">Chapter 4</a>, <em>Grouping with Clustering Algorithms</em>) between your test point and every other training point, and sort them by distance. This algorithm is quite fast because the training data is generally not more than 10,000 points or so.</p>
<p>If you have many training examples (in the order of millions) or you really need the algorithm to be lightning-fast, there are two optimizations you can make. The first is to skip the square root operation in the distance measure, and use the squared distance instead. While modern CPUs are very fast, the square root operation is still much slower than multiplication and addition, so you can save a few milliseconds by avoiding the square root. The second optimization is to only consider points within some bounding rectangle of distance to your test point; for instance, only consider points within +/- 5 units in each dimension from the test point's location. If your training data is dense, this optimization will not affect results but will speed up the algorithm because it will avoid calculating distances for many points.</p>
<p>The following is the KNN algorithm as a high-level description:</p>
<ol>
<li>Record all training data and their labels</li>
<li>Given a new point to evaluate, generate a list of its distances to all training points</li>
<li>Sort the list of distances in order of closest to farthest</li>
<li>Throw out all but the <em>k</em> nearest distances</li>
<li>Determine which label represents the majority of your <em>k</em> nearest neighbors; this is the result of the algorithm</li>
</ol>
<p>A more efficient version avoids maintaining a large list of distances that need to be sorted by limiting the list of distances to <em>k</em> items. Let's now write our own implementation of the KNN algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the KNN algorithm</h1>
                </header>
            
            <article>
                
<p>Since the KNN algorithm is quite simple, we'll build our own implementation:</p>
<ol>
<li>Create a new folder and name it <kbd>Ch5-knn</kbd>.</li>
<li>To the folder, add the following <kbd>package.json</kbd> file. Note that this file is a little different from previous examples because we have added a dependency for the <kbd>jimp</kbd> library, which is an image processing library that we'll use in the second example:</li>
</ol>
<pre style="padding-left: 60px">{<br/>  <span>"name"</span>: <span>"Ch5-knn"</span>,<br/>  <span>"version"</span>: <span>"1.0.0"</span>,<br/>  <span>"description"</span>: <span>"ML in JS Example for Chapter 5 - k-nearest-neighbor"</span>,<br/>  <span>"main"</span>: <span>"src/index.js"</span>,<br/>  <span>"author"</span>: <span>"Burak Kanber"</span>,<br/>  <span>"license"</span>: <span>"MIT"</span>,<br/>  <span>"scripts"</span>: {<br/>    <span>"build-web"</span>: <span>"browserify src/index.js -o dist/index.js -t [ babelify --presets [ env ] ]"</span>,<br/>    <span>"build-cli"</span>: <span>"browserify src/index.js --node -o dist/index.js -t [ babelify --presets [ env ] ]"</span>,<br/>    <span>"start"</span>: <span>"yarn build-cli &amp;&amp; node dist/index.js"<br/></span><span>  </span>},<br/>  <span>"dependencies"</span>: {<br/>    <span>"babel-core"</span>: <span>"^6.26.0"</span>,<br/>    <span>"babel-plugin-transform-object-rest-spread"</span>: <span>"^6.26.0"</span>,<br/>    <span>"babel-preset-env"</span>: <span>"^1.6.1"</span>,<br/>    <span>"babelify"</span>: <span>"^8.0.0"</span>,<br/>    <span>"browserify"</span>: <span>"^15.1.0"</span>,<br/>    <span>"jimp"</span>: <span>"^0.2.28"<br/></span><span>  </span>}<br/>}</pre>
<ol start="3">
<li>Run the <kbd>yarn install</kbd> command to download and install all the dependencies, and then create subfolders called <kbd>src</kbd>, <kbd>dist</kbd>, and <kbd>files</kbd>.</li>
<li>Inside the <kbd>src</kbd> folder, create an <kbd>index.js</kbd> file and an <kbd>knn.js</kbd> file.</li>
</ol>
<p>You will also need a <kbd>data.js</kbd> file. For these examples, I'm using a larger dataset than can be printed in this book, so you should take a minute to download the <kbd>Ch5-knn/src/data.js</kbd> file from this book's GitHub account.</p>
<p>Let's start with the <kbd>knn.js</kbd> file. Like the k-means example in the previous chapter, we will need a distance-measuring function. Let's use the one from <a href="84fd2c4d-41b4-46c4-82e5-4d8e55bb0066.xhtml" target="_blank">Chapter 4</a>, <em>Grouping with Clustering Algorithms</em>; add the following to the beginning of <kbd>knn.js</kbd>:</p>
<pre><span>/**<br/></span><span> * Calculate the distance between two points.<br/></span><span> * Points must be given as arrays or objects with equivalent keys.<br/></span><span> * </span><span>@param </span><span>{Array.&lt;number&gt;} a<br/></span><span> * </span><span>@param </span><span>{Array.&lt;number&gt;} b<br/></span><span> * </span><span>@return </span><span>{number}<br/></span><span> */<br/></span><span>const </span><span>distance </span>= (a, b) =&gt; <span>Math</span>.<span>sqrt</span>(<br/>    a.<span>map</span>((aPoint, i) =&gt; b[i] - aPoint)<br/>        .<span>reduce</span>((sumOfSquares, diff) =&gt; sumOfSquares + (diff*diff), <span>0</span>)<br/>);</pre>
<p>If you really need a performance optimization for your KNN implementation, this is where you might omit the <kbd>Math.sqrt</kbd> operation and return just the squared distance. I reiterate, however, that because this is such a fast algorithm by nature, you should only need to do this if you're working on an extreme problem with a lot of data or with very strict speed requirements.</p>
<p>Next, let's add the stub of our KNN class. Add the following to <kbd>knn.js</kbd>, beneath the distance function:</p>
<pre><span>class </span>KNN {<br/><br/>    <span>constructor</span>(k = <span>1</span>, data, labels) {<br/>        <span>this</span>.<span>k </span>= k;<br/>        <span>this</span>.<span>data </span>= data;<br/>        <span>this</span>.<span>labels </span>= labels;<br/>    }<br/><br/>}<br/><br/><span>export default </span>KNN;</pre>
<p>The constructor accepts three arguments: the <kbd>k</kbd><em>,</em> or the number of neighbors to consider when classifying your new point; the training data split up into the data points alone; and a corresponding array of their labels.</p>
<p>Next, we need to add an internal method that considers a test point and calculates a sorted list of distances from the test point to the training points. We'll call this a <strong>distance map</strong>. Add the following to the body of the KNN class:</p>
<pre>generateDistanceMap(point) {<br/><br/>    <span>const </span>map = [];<br/>    <span>let </span>maxDistanceInMap;<br/><br/>    <span>for </span>(<span>let </span>index = <span>0</span>, len = <span>this</span>.data.length; index &lt; len; index++) {<br/><br/>        <span>const </span>otherPoint = <span>this</span>.data[index];<br/>        <span>const </span>otherPointLabel = <span>this</span>.labels[index];<br/>        <span>const </span>thisDistance = distance(point, otherPoint);<br/><br/>        <span>/**<br/></span><span>         * Keep at most k items in the map. <br/></span><span>         * Much more efficient for large sets, because this <br/></span><span>         * avoids storing and then sorting a million-item map.<br/></span><span>         * This adds many more sort operations, but hopefully k is small.<br/></span><span>         */<br/></span><span>        </span><span>if </span>(!maxDistanceInMap || thisDistance &lt; maxDistanceInMap) {<br/><br/>            // Only add an item if it's closer than the farthest of the candidates<br/>            map.push({<br/>                index,<br/>                distance: thisDistance,<br/>                label: otherPointLabel<br/>            });<br/><br/>            // Sort the map so the closest is first<br/>            map.sort((a, b) =&gt; a.distance &lt; b.distance ? -<span>1 </span>: <span>1</span>);<br/><br/>            // If the map became too long, drop the farthest item<br/>            <span>if </span>(map.length &gt; <span>this</span>.k) {<br/>                map.pop();<br/>            }<br/><br/>            // Update this value for the next comparison<br/>            maxDistanceInMap = map[map.length - <span>1</span>].distance;<br/><br/>        }<br/>    }<br/><br/><br/>    <span>return </span>map;<br/>}</pre>
<p>This method could be easier to read, but the simpler version is not efficient for very large training sets. What we're doing here is maintaining a list of points that might be the KNNs and storing them in <kbd>map</kbd>. By maintaining a variable called <kbd>maxDistanceInMap</kbd>, we can loop over every training point and make a simple comparison to see whether the point should be added to our candidates list. If the point we're iterating over is closer than the farthest of our candidates, we can add the point to the list, re-sort the list, remove the farthest point to keep the list small, and then update <kbd>mapDistanceInMap</kbd>.</p>
<p>If that sounds like a lot of work, a simpler version might loop over all points, add each one with its distance measurement to the map, sort the map, and then return the first <em>k</em> items. The downside of that implementation is that for a dataset of a million points, you'd need to build a distance map of a million points and then sort that giant list in memory. In our version, you only ever hold <em>k</em> items as candidates, so you never need to store a separate million-point map. Our version does require a call to <kbd>Array.sort</kbd> whenever an item is added to the map. This is inefficient in its own way, as the sort function is called for each addition to the map. Fortunately, the sort operation is only for <em>k</em> items, where <em>k</em> might be something like 3 or 5. The computational complexity of the sorting algorithm is most likely <kbd>O(n log n)</kbd> (for a quicksort or mergesort implementation), so it only takes about 30 data points for the more sophisticated version to be more efficient than the simple version when <em>k = 3</em>, and for <em>k = 5</em>, that happens at around 3,000 data points. However, both versions are so fast that for a dataset smaller than 3,000 points, you won't notice the difference.</p>
<p>Finally, we tie the algorithm together with a <kbd>predict</kbd> method. The <kbd>predict</kbd> method must accept a test point, and at the very least return the determined label for the <kbd>point</kbd>. We will also add some additional output to the method, and report the labels of the <em>k</em> nearest neighbors as well as the number of votes each label contributed.</p>
<p>Add the following to the body of the KNN class:</p>
<pre><span>predict</span>(point) {<br/><br/>    <span>const </span><span>map </span>= <span>this</span>.<span>generateDistanceMap</span>(point);<br/>    <span>const </span><span>votes </span>= <span>map</span>.<span>slice</span>(<span>0</span>, <span>this</span>.<span>k</span>);<br/>    <span>const </span><span>voteCounts </span>= <span>votes<br/></span><span>        </span><span>// Reduces into an object like {label: voteCount}<br/></span><span>        </span>.<span>reduce</span>((obj, vote) =&gt; Object.<span>assign</span>({}, obj, {[vote.<span>label</span>]: (obj[vote.<span>label</span>] || <span>0</span>) + <span>1</span>}), {})<br/>    ;<br/>    <span>const </span><span>sortedVotes </span>= Object.<span>keys</span>(<span>voteCounts</span>)<br/>        .<span>map</span>(label =&gt; ({label, <span>count</span>: <span>voteCounts</span>[label]}))<br/>        .<span>sort</span>((a, b) =&gt; a.<span>count </span>&gt; b.<span>count </span>? -<span>1 </span>: <span>1</span>)<br/>    ;<br/><br/>    <span>return </span>{<br/>        <span>label</span>: <span>sortedVotes</span>[<span>0</span>].<span>label</span>,<br/>        <span>voteCounts</span>,<br/>        <span>votes<br/></span><span>    </span>};<br/><br/>}</pre>
<p>This method requires a little bit of datatype juggling in JavaScript, but is simple in concept. First, we generate our distance map using the method we just implemented. Then, we remove all data except for the <em>k</em> nearest points and store that in a <kbd>votes</kbd> variable. If you're using 3 as <em>k</em>, then <kbd>votes</kbd> will be an array of length three.</p>
<p>Now that we have our <em>k</em> nearest neighbors, we need to figure out which label represents the majority of the neighbors. We'll do this by reducing our votes array into an object called <kbd>voteCounts</kbd>. To get a picture of what we want <kbd>voteCounts</kbd> to look like, imagine that we're looking for the three nearest neighbors and the possible categories are <kbd>Male</kbd> or <kbd>Female</kbd>. The <kbd>voteCounts</kbd> variable might look like this: <kbd>{"Female": 2, "Male": 1}</kbd>.</p>
<p>Our job is still not done, however—after reducing our votes into a vote-count object, we still need to sort that and determine the majority label. We do this by mapping the vote counts object back into an array and then sorting the array based on vote counts.</p>
<p>There are other ways to approach this problem of tallying votes; any method you can think of will work, as long as you can return the majority vote at the end of the day. I like thinking about data in terms of structure and the transformations necessary to get from one structure to the next, but as long as you can report the top vote, the algorithm will work.</p>
<p>That's all we need to do in the <kbd>knn.js</kbd> file. The algorithm is complete, requiring fewer than 70 lines of code.</p>
<p>Let's set up our <kbd>index.js</kbd> file and get ready to run some examples. Remember that you need to download the <kbd>data.js</kbd> file first—see Packt's GitHub account or my personal GitHub account at <a href="https://github.com/bkanber/MLinJSBook">https://github.com/bkanber/MLinJSBook</a>.</p>
<p>Add the following to the top of <kbd>index.js</kbd>:</p>
<pre><span>import</span> KNN <span>from</span> <span>'./knn.js'</span>;<br/><span>import</span> {<span>weight_height</span>} <span>from</span> <span>'./data.js'</span>;</pre>
<p>Let's try our algorithm out on a few simple examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 1 – Height, weight, and gender</h1>
                </header>
            
            <article>
                
<p>KNN, like k-means, can work on high-dimensional data—but, like k-means, we can only graph example data in a two-dimensional plane so we'll keep our examples simple. The first question we'll tackle is: can we predict a person's biological sex given only their height and weight?</p>
<p>I've downloaded some data for this example from a national longitudinal survey on people's perception of their weight. Included in the data are the respondents' height, weight, and gender. This is what the data looks like, when graphed:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-84 image-border" src="assets/39a20c3b-71d0-40d6-b896-6a43a084a953.png" style="width:53.08em;height:32.67em;"/></div>
<p>Just by looking at the preceding charted data, you can get a sense as to why KNN is so effective at evaluating clustered data. It's true that there's no neat boundary between male and female, but if you were to evaluate a new data point of a 200 pound, 72 inches-tall person, it's clear that all the training data around that point is male and it's likely your new point is male, too. Conversely, a new respondent at 125 pounds and a height of 62 inches is well into the female area of the graph, though there are a couple of males with those characteristics as well. The middle of the graph, around 145 pounds and 65 inches tall, is the most ambiguous, with an even split of male and female training points. I would expect the algorithm to be uncertain about new points in that area. Because there is no clear dividing line in this dataset, we would need more features or more dimensions to get a better resolution of the boundaries.</p>
<p>In any case, let's try out a few examples. We'll pick five points that we expect to be definitely male, definitely female, probably male, probably female, and indeterminable. Add the following code to <kbd>index.js</kbd>, beneath the two import lines:</p>
<pre><span>console</span>.<span>log</span>(<span>"</span><span>Testing height and weight with k=5"</span>);<br/><span>console</span>.<span>log</span>(<span>"=========================="</span>);<br/> <br/> <span>const</span> <span>solver1</span> = <span>new</span> KNN(<span>5</span>, <span>weight_height</span>.<span>data</span>, <span>weight_height</span>.<span>labels</span>);<br/> <br/> <span>console</span>.<span>log</span>(<span>"Testing a 'definitely male' point:"</span>);<br/> <span>console</span>.<span>log</span>(<span>solver1</span>.<span>predict</span>([<span>200</span>, <span>75</span>]));<br/> <span>console</span>.<span>log</span>(<span>"</span><span>\n</span><span>Testing a 'probably male' point:"</span>);<br/> <span>console</span>.<span>log</span>(<span>solver1</span>.<span>predict</span>([<span>170</span>, <span>70</span>]));<br/> <span>console</span>.<span>log</span>(<span>"</span><span>\n</span><span>Testing a 'totally uncertain' point:"</span>);<br/> <span>console</span>.<span>log</span>(<span>solver1</span>.<span>predict</span>([<span>140</span>, <span>64</span>]));<br/> <span>console</span>.<span>log</span>(<span>"</span><span>\n</span><span>Testing a 'probably female' point:"</span>);<br/> <span>console</span>.<span>log</span>(<span>solver1</span>.<span>predict</span>([<span>130</span>, <span>63</span>]));<br/> <span>console</span>.<span>log</span>(<span>"</span><span>\n</span><span>Testing a 'definitely female' point:"</span>);<br/> <span>console</span>.<span>log</span>(<span>solver1</span>.<span>predict</span>([<span>120</span>, <span>60</span>]));</pre>
<p>Run <kbd>yarn start</kbd> from the command line and you should see the following output. Since the KNN is not stochastic, meaning it does not use any random conditions in its evaluation, you should see exactly the same output as I do<span>—</span>with the possible exception of the ordering of votes and their indexes, if two votes have the same distance.</p>
<p>If you get an error when you run <kbd>yarn start</kbd>, make sure your <kbd>data.js</kbd> file has been correctly downloaded and installed.</p>
<p>Here's the output from the preceding code:</p>
<pre><strong>Testing <span>height</span> <span>and</span> weight <span>with</span> <span>k</span>=<span>5<br/></span>======================================================================</strong><br/> <br/><strong> Testing <span>a</span> <span>'definitely male'</span> point:</strong><br/><strong> { label: <span>'Male'</span>,</strong><br/><strong> voteCounts: { <span>Male</span>: <span>5</span> },</strong><br/><strong> <span>votes</span>:</strong><br/><strong> [ { <span>index</span>: <span>372</span>, <span>distance</span>: <span>0</span>, <span>label</span>: <span>'Male'</span> },</strong><br/><strong> { <span>index</span>: <span>256</span>, <span>distance</span>: <span>1</span>, <span>label</span>: <span>'Male'</span> },</strong><br/><strong> { <span>index</span>: <span>291</span>, <span>distance</span>: <span>1</span>, <span>label</span>: <span>'Male'</span> },</strong><br/><strong> { <span>index</span>: <span>236</span>, <span>distance</span>: <span>2.8284271247461903</span>, <span>label</span>: <span>'Male'</span> },</strong><br/><strong> { <span>index</span>: <span>310</span>, <span>distance</span>: <span>3</span>, <span>label</span>: <span>'Male'</span> } ] }</strong><br/> <br/><strong> Testing <span>a</span> <span>'probably male'</span> point:</strong><br/><strong> { label: <span>'Male'</span>,</strong><br/><strong> voteCounts: { <span>Male</span>: <span>5</span> },</strong><br/><strong> <span>votes</span>:</strong><br/><strong> [ { <span>index</span>: <span>463</span>, <span>distance</span>: <span>0</span>, <span>label</span>: <span>'Male'</span> },</strong><br/><strong> { <span>index</span>: <span>311</span>, <span>distance</span>: <span>0</span>, <span>label</span>: <span>'Male'</span> },</strong><br/><strong> { <span>index</span>: <span>247</span>, <span>distance</span>: <span>1</span>, <span>label</span>: <span>'Male'</span> },</strong><br/><strong> { <span>index</span>: <span>437</span>, <span>distance</span>: <span>1</span>, <span>label</span>: <span>'Male'</span> },</strong><br/><strong> { <span>index</span>: <span>435</span>, <span>distance</span>: <span>1</span>, <span>label</span>: <span>'Male'</span> } ] }</strong><br/> <br/><strong> Testing <span>a</span> <span>'totally uncertain'</span> point:</strong><br/><strong> { label: <span>'Male'</span>,</strong><br/><strong> voteCounts: { Male: <span>3</span>, Female: <span>2</span> },</strong><br/><strong> <span>votes</span>:</strong><br/><strong> [ { <span>index</span>: <span>329</span>, <span>distance</span>: <span>0</span>, <span>label</span>: <span>'Male'</span> },</strong><br/><strong> { <span>index</span>: <span>465</span>, <span>distance</span>: <span>0</span>, <span>label</span>: <span>'Male'</span> },</strong><br/><strong> { <span>index</span>: <span>386</span>, <span>distance</span>: <span>0</span>, <span>label</span>: <span>'Male'</span> },</strong><br/><strong> { <span>index</span>: <span>126</span>, <span>distance</span>: <span>0</span>, <span>label</span>: <span>'Female'</span> },</strong><br/><strong> { <span>index</span>: <span>174</span>, <span>distance</span>: <span>1</span>, <span>label</span>: <span>'Female'</span> } ] }</strong><br/> <br/><strong> Testing <span>a</span> <span>'probably female'</span> point:</strong><br/><strong> { label: <span>'Female'</span>,</strong><br/><strong> voteCounts: { Female: <span>4</span>, Male: <span>1</span> },</strong><br/><strong> <span>votes</span>:</strong><br/><strong> [ { <span>index</span>: <span>186</span>, <span>distance</span>: <span>0</span>, <span>label</span>: <span>'Female'</span> },</strong><br/><strong> { <span>index</span>: <span>90</span>, <span>distance</span>: <span>0</span>, <span>label</span>: <span>'Female'</span> },</strong><br/><strong> { <span>index</span>: <span>330</span>, <span>distance</span>: <span>0</span>, <span>label</span>: <span>'Male'</span> },</strong><br/><strong> { <span>index</span>: <span>51</span>, <span>distance</span>: <span>1</span>, <span>label</span>: <span>'Female'</span> },</strong><br/><strong> { <span>index</span>: <span>96</span>, <span>distance</span>: <span>1</span>, <span>label</span>: <span>'Female'</span> } ] }</strong><br/> <br/><strong> Testing <span>a</span> <span>'definitely female'</span> point:</strong><br/><strong> { label: <span>'Female'</span>,</strong><br/><strong> voteCounts: { <span>Female</span>: <span>5</span> },</strong><br/><strong> <span>votes</span>:</strong><br/><strong> [ { <span>index</span>: <span>200</span>, <span>distance</span>: <span>0</span>, <span>label</span>: <span>'Female'</span> },</strong><br/><strong> { <span>index</span>: <span>150</span>, <span>distance</span>: <span>0</span>, <span>label</span>: <span>'Female'</span> },</strong><br/><strong> { <span>index</span>: <span>198</span>, <span>distance</span>: <span>1</span>, <span>label</span>: <span>'Female'</span> },</strong><br/><strong> { <span>index</span>: <span>147</span>, <span>distance</span>: <span>1</span>, <span>label</span>: <span>'Female'</span> },</strong><br/><strong> { <span>index</span>: <span>157</span>, <span>distance</span>: <span>1</span>, <span>label</span>: <span>'Female'</span> } ] }</strong></pre>
<p>The algorithm has determined genders just as we would have done, visually, by looking at the chart. Feel free to play with this example more and experiment with different values of <kbd>k</kbd> to see how results might differ for any given test point.</p>
<p>Let's now look at a second example of KNN in action. This time, we'll choose a problem where <kbd>k = 1</kbd> really shines.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 2 – Decolorizing a photo</h1>
                </header>
            
            <article>
                
<p>The KNN algorithm is very susceptible to local noise and isn't very useful when there is a lot of overlap between classes expected. It is typically not very useful for more advanced tasks, such as psychographic, demographic, or behavioral analysis. But it's a very useful tool to keep handy in your toolbox, because it can assist with lower-level tasks very easily.</p>
<p>In this example, we'll use our KNN class to de colorize a photo. Specifically, we're going to take colorful input photos and restrict them to a color scheme of only 16 colors. We'll use KNN here to select the appropriate replacement color for a pixel, given that pixel's original color.</p>
<p>Our workflow will look like this:</p>
<ol>
<li>
<p>Use the <kbd>jimp</kbd> library to read an input image</p>
</li>
<li>
<p>Loop over each pixel in the image and:</p>
<ol>
<li>
<p>Find the most similar color in our 16-color scheme</p>
</li>
<li>
<p>Replace that pixel with the new color</p>
</li>
</ol>
</li>
<li>
<p>Write a new output file, based on the 16-color scheme</p>
</li>
</ol>
<p>Before we start, <em>verify</em> that the following exists in your <kbd>data.js</kbd> file. If you downloaded the <kbd>data.js</kbd> file from the GitHub for this book, then this should already be in there. However, if you sourced your gender survey data from a different place, you will need the following in the <kbd>data.js</kbd> file:</p>
<pre><span>export const</span> <span>colors_16</span> = {<br/> <span>data</span>: [<span><br/></span> [<span>0</span>, <span>0</span>, <span>0</span>], <span>// black<br/></span> [<span>128</span>, <span>128</span>, <span>128</span>], <span>// gray<br/></span> [<span>128</span>, <span>0</span>, <span>0</span>], <span>//maroon<br/></span> [<span>255</span>, <span>0</span>, <span>0</span>], <span>// red<br/></span> [<span>0</span>, <span>128</span>, <span>0</span>], <span>// green<br/></span> [<span>0</span>, <span>255</span>, <span>0</span>], <span>// lime<br/></span> [<span>128</span>, <span>128</span>, <span>0</span>], <span>// olive<br/></span> [<span>255</span>, <span>255</span>, <span>0</span>], <span>// yellow<br/></span> [<span>0</span>, <span>0</span>, <span>128</span>], <span>// navy<br/></span> [<span>0</span>, <span>0</span>, <span>255</span>], <span>// blue<br/></span> [<span>128</span>, <span>0</span>, <span>128</span>], <span>// purple<br/></span> [<span>255</span>, <span>0</span>, <span>255</span>], <span>// fuchsia<br/></span> [<span>0</span>, <span>128</span>, <span>128</span>], <span>// teal<br/></span> [<span>0</span>, <span>255</span>, <span>255</span>], <span>// aqua<br/></span> [<span>192</span>, <span>192</span>, <span>192</span>], <span>// silver<br/></span> [<span>255</span>, <span>255</span>, <span>255</span>], <span>// white</span><span><br/></span> ],<br/> <br/> <span>labels</span>: [<br/> <span>'Black'</span>,<br/> <span>'Gray'</span>,<br/> <span>'Maroon'</span>,<br/> <span>'Red'</span>,<br/> <span>'Green'</span>,<br/> <span>'Lime'</span>,<br/> <span>'Olive'</span>,<br/> <span>'Yellow'</span>,<br/> <span>'Navy'</span>,<br/> <span>'Blue'</span>,<br/> <span>'Purple'</span>,<br/> <span>'Fuchsia'</span>,<br/> <span>'Teal'</span>,<br/> <span>'Aqua'</span>,<br/> <span>'Silver'</span>,<br/> <span>'White'</span>,<br/> ]<br/> };</pre>
<p>The preceding color definitions represent a common color scheme of 16 colors. You can also experiment with color schemes on your own; you can use this approach to colorize to shades of blue, or to warm colors, or to sepia tones, and so on. You can also allow for far more than 16 colors by increasing the training data size.</p>
<p>Let's start by writing a couple of helper functions. Create a new file, in the <kbd>src</kbd> folder, called <kbd>decolorize.js</kbd>. Make sure you added <kbd>jimp</kbd> to your <kbd>package.json</kbd>—if you're unsure, run <kbd>yarn add jimp</kbd> from the command line. Add the following imports to the top of the file:</p>
<pre><span>import</span> KNN <span>from</span> <span>'./knn.js'</span>;<br/><span>import</span> {<span>colors_16</span>} <span>from</span> <span>'./data.js'</span>;<br/><span>import</span> jimp <span>from</span> <span>'jimp'</span></pre>
<p>Then, create and export a function that accepts an image filename and writes a new file with the decolorized image. I've left some gentle comments in the code snippet that describe the workflow; most of the code is just juggling data formats. In general, our approach is to open and read the input file, iterate over all pixels, use a KNN to find a substitute color for that pixel, write the new color to the pixel, and then finally write a new output file using the modified colors:</p>
<pre><span>const</span> <span>decolorize</span> = filename =&gt; {<br/> <br/>  <span>return</span> jimp.<span>read</span>(filename)<br/>    .<span>then</span>(image =&gt; {<br/> <br/>      <span>// Create a KNN instance with our color scheme as training data<br/></span>      <span>// We use k=1 to find the single closest color<br/></span>      <span>// k &gt; 1 wouldn't work, because we only have 1 label per training point<br/></span>      <span>const</span> <span>mapper</span> = <span>new</span> KNN(<span>1</span>, <span>colors_16</span>.<span>data</span>, <span>colors_16</span>.<span>labels</span>);<br/>      <span>const</span> {<span>width</span>, <span>height</span>} = image.bitmap;<br/> <br/>      <span>// For every pixel in the image...<br/></span>      <span>for</span> (<span>let</span> <span>x</span> = <span>0</span>; <span>x</span> &lt; <span>width</span>; <span>x</span>++) {<br/>      <span>for</span> (<span>let</span> <span>y</span> = <span>0</span>; <span>y</span> &lt; <span>height</span>; <span>y</span>++) {<br/> <br/>      <span>// Do some work to get the RGB value as an array: [R,G,B]<br/></span>      <span>const</span> <span>originalColorHex</span> = image.<span>getPixelColor</span>(<span>x</span>, <span>y</span>);<br/>      <span>const</span> <span>originalColorRgb</span> = jimp.intToRGBA(<span>originalColorHex</span>);<br/>      <span>const</span> <span>pixelPoint</span> = [<span>originalColorRgb</span>.<span>r</span>, <span>originalColorRgb</span>.<span>g</span>, <span>originalColorRgb</span>.<span>b</span>];<br/> <br/>      <span>// Ask the KNN instance what the closest color from the scheme is<br/></span>      <span>const</span> <span>closestColor</span> = <span>mapper</span>.<span>predict</span>(<span>pixelPoint</span>);<br/> <br/>      <span>// Then get that color in hex format, and set the pixel to the new color<br/></span>      <span>const</span> <span>newColor</span> = <span>colors_16</span>.<span>data</span>[<span>colors_16</span>.<span>labels</span>.<span>indexOf</span>(<span>closestColor</span>.<span>label</span>)];<br/>      <span>const</span> <span>newColorHex</span> = jimp.<span>rgbaToInt</span>(<span>newColor</span>[<span>0</span>], <span>newColor</span>[<span>1</span>], <span>newColor</span>[<span>2</span>], <span>255</span>);<br/>      image.<span>setPixelColor</span>(<span>newColorHex</span>, <span>x</span>, <span>y</span>);<br/> <br/>    }<br/>  }<br/> <br/>  <span>const</span> <span>ext</span> = image.<span>getExtension</span>();<br/>  image.<span>write</span>(filename.<span>replace</span>(<span>'.'</span>+<span>ext</span>, <span>''</span>) + <span>'_16.'</span> + <span>ext</span>);<br/> <br/>  })<br/>  .<span>catch</span>(err =&gt; {<br/>    <span>console</span>.<span>log</span>(<span>"Error reading image:"</span>);<br/>    <span>console</span>.<span>log</span>(err);<br/>  })<br/>};<br/> <br/><span>export default</span> decolorize</pre>
<p>We now have a function that will accept a filename and create a new de-colorized photo. If you haven't already, create a folder called <kbd>files</kbd> in the <kbd>Ch5-knn</kbd> directory. Find a few of your favorite pictures and add them to the <kbd>files</kbd> folder. Or, you can use the image examples from the book's GitHub, which are <kbd>landscape.jpeg</kbd>, <kbd>lily.jpeg</kbd>, and <kbd>waterlilies.jpeg</kbd>.</p>
<p>Finally, open up <kbd>index.js</kbd> and add the following to the bottom of the file:</p>
<pre>[<strong>'landscape.jpeg',</strong><strong> 'lily.jpeg', 'waterlilies.jpeg'</strong>].<span>forEach</span>(filename =&gt; {<br/>  <span>console</span>.<span>log</span>(<span>"Decolorizing "</span> + filename + <span>'...'</span>);<br/>  <span>decolorize</span>(<span>'./files/'</span> + filename)<br/>    .<span>then</span>(() =&gt; <span>console</span>.<span>log</span>(filename + <span>" decolorized"</span>));<br/>});</pre>
<p>If you are using your own example files, make sure to update the filenames shown in bold in the preceding code.</p>
<p>Run the code with <kbd>yarn start</kbd> and you should see output like the following (you may have the results from the other KNN experiment in your output, as well):</p>
<pre><strong> Decolorizing images</strong><br/><strong> =======================================================</strong><br/><strong> Decolorizing landscape.jpeg...</strong><br/><strong> Decolorizing lily.jpeg...</strong><br/><strong> Decolorizing waterlilies.jpeg...</strong><br/><strong> lily.jpeg decolorized</strong><br/><strong> waterlilies.jpeg decolorized</strong><br/><strong> landscape.jpeg decolorized</strong></pre>
<p>If there are any errors with filenames or permissions, resolve them. Look in the <kbd>files</kbd> folder for your new photos. I don't know which format you're reading this book in and how these images will look to you, but the following is my <kbd>landscape.jpeg</kbd> file, original and processed.</p>
<p>The original:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-85 image-border" src="assets/7e0e320d-9170-48f2-8979-7553f8e10b02.jpeg" style="width:39.17em;height:24.50em;"/></div>
<p>And the de-colorized version:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-86 image-border" src="assets/be92c1f9-c64a-47f7-ba1c-31d0acb46818.jpeg" style="width:39.17em;height:24.50em;"/></div>
<p>I think it did a very good job on the foreground and the scenery, however, the limited color palette definitely affects the sky, water, and mountains in the background. Try adding another 8 or 16 colors to the training data to see what happens.</p>
<p>I like this project as a KNN example because it shows you that <strong>machine learning</strong> (<strong>ML</strong>) algorithms don't always have to be used for sophisticated analyses. Many of them can be used as part of your everyday toolbox, trained with smaller models to help you with simpler data-processing tasks.</p>
<p>I should also make a note here about measuring the distance between colors. The approach we have taken, using the Euclidean distance formula to measure distances between RGB values, is not perceptually accurate. The RGB space is slightly warped when it comes to human visual perception, so our Euclidean distance measurements are not totally accurate. For our purposes, they are close enough because we are downgrading to a very low resolution. If you need perceptually-accurate image processing, you will either need to transform all RGB values into a more accurate color space, such as <em>Lab</em>,<em><strong> </strong></em>or update your distance function to measure perceptual distance rather than just the geometric distance between points. </p>
<p>Let's move on from KNN and look at a more sophisticated way to classify objects, based on centuries-old probability theory that is still powerful today: Bayesian classification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive Bayes classifier</h1>
                </header>
            
            <article>
                
<p>A Naive Bayes classifier is a type of probabilistic classifier, or an algorithm that assigns a probability distribution to the potential outcomes. As opposed to a binary classification, such as <kbd>Male</kbd> or <kbd>Female</kbd>, the probabilistic classifier tells you there is an 87% chance this data point is <kbd>Male</kbd> and a 13% chance it is <kbd>Female</kbd>.</p>
<p>Not all probabilistic classifiers are Bayesian, nor are they all necessarily naive. The term <em>Naive</em>, in this context, is not a veiled insult to the classifier—it's a mathematical term that has a meaning in probability theory, which we'll discuss further later. The term <em>Bayes</em> or <em>Bayesian</em> means that the principles used in the classifier were first published by Reverend Thomas Bayes, an 18th century mathematician, popular for his <em>Bayes theorem </em>in probability theory.</p>
<p>Let's first have a probability refresher. First, you should know that probability can work with both <em>continuous distributions</em> and <em>discrete distributions</em>. Continuous distributions are those where your variable is a number and can have any value. Discrete distributions have only a fixed number of possible states, even if that number is large. Continuous values are things such as <em>activity of 54.21 minutes per week; $23.34 per share; 18 total logins</em>. Discrete values are <em>true</em>/<em>false</em>; <em>Hollywood</em>, <em>gossip</em>, <em>politics</em>, <em>sports</em>, <em>local events</em>, or <em>world news</em>, or even the frequency of individual words in an article. Most of the theorems in probability can be used both for continuous and discrete distributions, though the implementation details between the two will differ.</p>
<p>In discrete probability, which we will use for our example, you work with the probability of various <em>events</em> occurring. An event is a set of possible outcomes from an experiment. The classical illustrative example of this involves a pack of playing cards; imagine you draw a card at random from a shuffled deck. What's the probability that the card you pulled is a heart? When we ask this question, we're asking about the probability of a certain event, specifically <em>that the card is a heart</em>. We can give our event a label, such as <kbd>H</kbd> for heart, and then we can shorten the phrase <em>probability that the card is a heart</em> to simply, <kbd>P(H)</kbd>. The answer is 13/52, or 1/4, or 0.25, so you could also say that <kbd>P(H) = 0.25</kbd>. There are many other possible events in our scenario. What's the chance the card is the five of diamonds? What's the chance the card is black? What's the chance the card is a face card? What's the chance the value is less than five? All of those are types of events, and each one has its own probability.</p>
<p>Not all events are independent. For instance, let's say the experiment is <em>did you drink a soda yesterday?</em>, and we are surveying Americans. We can define the event <kbd>S</kbd> as <em>drank a soda yesterday</em>. By surveying everyone in America (or at least a representative sample), we find that nearly 50% of respondents said yes! (It is actually 48%, according to Yale University.) So we can say that the probability of <kbd>S</kbd> is 50%, or <kbd>P(S) = 0.5</kbd>. We can also define an event as <kbd>S',</kbd>, which is the probability of the event <em>did not drink a soda yesterday</em>, or the inverse.</p>
<p>We want to develop more insight into citizens' eating habits, so we add another question to the survey: did you eat at a fast food restaurant yesterday? We'll name this event <kbd>M</kbd>, for McDonald's, and we find that <kbd>P(M) = 0.25</kbd>, or a quarter of the nation.</p>
<p>We can now ask more sophisticated questions, such as: does eating fast food affect whether people drink soda? We can ask about the probability that someone drank a soda given that they ate fast food yesterday. This is called the <strong>conditional probability</strong> of <kbd>S</kbd> events given <kbd>M</kbd>, or <kbd>P(S|M)</kbd>.</p>
<p>If we asked the questions about drinking a soda and eating fast food in the same survey, then we can calculate <kbd>P(S|M)</kbd> by finding the probability of a respondent doing both events (this is written <kbd>P(S ∩ M)</kbd>, pronounced <em>probability of S intersect M</em>), and dividing by <kbd>P(M)</kbd>. The full formula is <kbd>P(S|M) = P(S ∩ M) / P(M)</kbd>.</p>
<p>Let's say that 20% of respondents both drank a soda and ate fast food. We can now calculate that <kbd>P(S|M) = 0.2 / 0.25 = 0.8</kbd>. The probability of having drunk a soda yesterday is 80%, given that you ate fast food yesterday.</p>
<div class="packt_infobox">Note that this is <em>not</em> the probability that you drank a soda <em>while</em> eating fast food. To answer that question, you'd have to go to a fast food restaurant and survey the people there. Our version is less committal in terms of causation.</div>
<p>Now you want to ask the reverse question: what's the probability of someone having eaten fast food given that they drank a soda yesterday? This is asking about <kbd>P(M|S)</kbd>. We could just reverse the preceding formula, but let's say that we lost the original survey data and can no longer determine <kbd>P(S ∩ M)</kbd>.</p>
<p>We can use the Bayes theorem to correctly reverse our probability:</p>
<div class="CDPAlignCenter CDPAlign"><em>P(M|S) = P(S|M) * P(M) / P(S)</em></div>
<p>Fortunately, we remember those three values and find that:</p>
<div class="CDPAlignCenter CDPAlign"><em>P(M|S) = 0.8 * 0.25 / 0.5  = 0.4</em></div>
<p>The probability that someone ate fast food yesterday, knowing that they drank a soda, is 40%. That's up from the baseline of 25% for anyone eating fast food.</p>
<p>How does this apply to naive Bayes classifiers? We use the preceding conditional probability theorems to relate features to their respective classes. In a spam filter, we ask the question: what's the probability that this document is spam given that it has the word <em>credit </em>in it<em>?</em> And what's the probability this document is spam given that it has the word <em>transfer</em> in it<em>?</em> We ask that question for every word in the document, and then we combine those probabilities to get the overall probability that the document is spam. The naive Bayes classifier is naive because it assumes that the events are all independent. Truthfully, this is a bad assumption. Emails with the word <em>credit </em>are more likely to also have the word <em>transfer</em> in them, but in practice it turns out that these classifiers are still very accurate despite the incorrect assumption.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tokenization</h1>
                </header>
            
            <article>
                
<p>We also must briefly discuss the concept of <em>tokenization</em>. We will discuss tokenization in depth in Chapter 10, <em>Natural Language Processing in Practice</em> when we discuss natural language programming, but we do need a short introduction to it now. Tokenization is the act of breaking up a document into individual <em>tokens</em>. You can think of a token as a word, but not all words are necessarily tokens and not all tokens are necessarily words.</p>
<p>The simplest tokenizer would be to split a document up by spaces. The result would be an array of words, including their capitalization and their punctuation. A slightly more advanced tokenizer might convert everything to lowercase and remove any non-alphanumeric characters. Now the tokens are all lowercase words, numbers, and words with numbers in them. Your tokenizer can remove common words, such as <em>and</em> and <em>the</em><span>—</span>this is called <strong>stopword filtering</strong><em>.</em> You can also <em>stem</em> as part of your tokenizer, which is to remove extraneous endings from a word. For instance <em>parties</em>, <em>partied</em>, and <em>party</em> might all become <em>parti</em>. This is a great dimensionality reduction technique, and helps your classifier focus on the meaning of words rather than the particular tense or usage. You can take it even farther by <em>lemmatizing</em>, which is similar to stemming but actually grammatically transforms words to their root form, so that <em>running</em>, <em>runs</em>, and <em>ran</em> would all become <em>run</em>.</p>
<p>Tokenization can take more advanced forms. A token does not need to be a single word; it can be pairs or trios of words. These are called <strong>bigrams</strong> and <strong>trigrams</strong>, respectively. Tokens can also be generated from metadata. Email spam filters, in particular, do very well when some information from the message's headers are included as tokens: whether the email passed or failed its SPF check, whether it has a valid DKIM key, the sender's domain, and so on. Tokenizers can also modify tokens from certain fields; for instance, it was found that prefixing tokens from email subject lines (as opposed to body content) improved spam filtering performance. Rather than tokenizing <em>buy pharmaceuticals now</em> as <em>buy</em>, <em>pharmaceuticals</em>, <em>now</em>, you can tokenize those as <em>SUBJ_buy</em>, <em>SUBJ_pharmaceuticals</em>, <em>SUBJ_now</em>. The effect of this prefixing is to allow the classifier to consider subject and body words separately, which may increase performance.</p>
<p>Do not underestimate the importance of the tokenizer. Often, you can get significant accuracy improvements by being thoughtful about your tokenizer algorithm. In this example, we'll use a simple, intuitive one that is still quite effective.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the algorithm</h1>
                </header>
            
            <article>
                
<p>Let's now build the naive Bayes classifier. These are the steps that are to be followed to build the algorithm :</p>
<ol>
<li>Create a new folder for the project called <kbd>Ch5-Bayes</kbd>. As usual, create <kbd>src</kbd> and <kbd>data</kbd> and <kbd>dist</kbd> folders, and add the following <kbd>package.json</kbd> file:</li>
</ol>
<pre style="padding-left: 60px">{<br/> <span>"name"</span>: <span>"Ch5-Bayes"</span>,<br/> <span>"version"</span>: <span>"1.0.0"</span>,<br/> <span>"description"</span>: <span>"ML in JS Example for Chapter 5 - Bayes"</span>,<br/> <span>"main"</span>: <span>"src/index.js"</span>,<br/> <span>"author"</span>: <span>"Burak Kanber"</span>,<br/> <span>"license"</span>: <span>"MIT"</span>,<br/> <span>"scripts"</span>: {<br/> <span>"build-web"</span>: <span>"browserify src/index.js -o dist/index.js -t [ babelify --presets [ env ] ]"</span>,<br/> <span>"build-cli"</span>: <span>"browserify src/index.js --node -o dist/index.js -t [ babelify --presets [ env ] ]"</span>,<br/> <span>"start"</span>: <span>"yarn build-cli &amp;&amp; node dist/index.js"<br/></span> },<br/> <span>"dependencies"</span>: {<br/> <span>"babel-core"</span>: <span>"^6.26.0"</span>,<br/> <span>"babel-plugin-transform-object-rest-spread"</span>: <span>"^6.26.0"</span>,<br/> <span>"babel-preset-env"</span>: <span>"^1.6.1"</span>,<br/> <span>"babelify"</span>: <span>"^8.0.0"</span>,<br/> <span>"browserify"</span>: <span>"^15.1.0"<br/></span> }<br/> }</pre>
<ol start="2">
<li>Once you've added the <kbd>package.json</kbd> file, run <kbd>yarn install</kbd> from the command line to install all the project dependencies.</li>
<li>Navigate to the book's GitHub account, and download the four files in the <kbd>data</kbd> folder. They should be called <kbd>train_negative.txt</kbd>, <kbd>train_positive.txt</kbd>, <kbd>test_negative.txt</kbd>, and <kbd>test_positive.txt</kbd>. These files contain reviews of movies from <a href="https://www.imdb.com/" target="_blank">https://www.imdb.com/</a>, and are pre-sorted into positive reviews and negative reviews, using IMDB's star-rating system. We will use this data to train and later validate an algorithm to detect movie review sentiment.</li>
</ol>
<ol start="4">
<li>Create a <kbd>bayes.js</kbd> file in the <kbd>src</kbd> folder. Add the following tokenizer function to the top of the file:</li>
</ol>
<pre style="padding-left: 60px"><span>export const</span> <span>simpleTokenizer</span> = string =&gt; string<br/> .toLowerCase()<br/> .<span>replace</span>(<span>/[^\w\d]/g</span>, <span>' '</span>)<br/> .<span>split</span>(<span>' '</span>)<br/> .<span>filter</span>(word =&gt; word.<span>length</span> &gt; <span>3</span>)<span><br/></span> .<span>filter</span>((word, index, arr) =&gt; arr.<span>indexOf</span>(word, index+<span>1</span>) === -<span>1</span>);</pre>
<p>This function accepts a string as an input, and returns an array of tokens as the output. The string is first converted to lowercase, because our analysis is case-sensitive. Then any characters that are not word or number characters are removed and replaced with spaces. We split the string up by spaces to get an array of tokens. Next, we filter out any tokens that are three characters or shorter (so the words <em>the</em> and <em>was</em> would be removed, while words like <em>this</em> and <em>that</em> are preserved). The last line of the tokenizer filters out non-unique tokens; we will consider only the existence of words in documents, not the number of times those words are used.</p>
<div class="packt_tip">Note that the <kbd>filter</kbd> function in the tokenizer does not preserve word order. To preserve word order, you would need to add <kbd>.reverse()</kbd> before and after the final filter line. However, our algorithm doesn't consider word order, so preserving it is not necessary.</div>
<ol start="5">
<li>Create the <kbd>BayesClassifier</kbd> class and export it from <kbd>bayes.js</kbd>. Add the following to the file:</li>
</ol>
<pre style="padding-left: 60px"><span>class</span> BayesClassifier {<br/> <br/> <span>constructor</span>(tokenizer = <span>null</span>) {<br/> <span>this</span>.<span>database</span> = {<br/> <span>labels</span>: {},<br/> <span>tokens</span>: {}<br/> };<br/> <br/> <span>this</span>.<span>tokenizer</span> = (tokenizer !== <span>null</span>) ? tokenizer : <span>simpleTokenizer</span>;<br/> }<br/> }<br/> <br/> <span>export default</span> BayesClassifier;</pre>
<p>The constructor for the classifier accepts only a <kbd>tokenizer</kbd> function, however, it defaults to the simple preceding tokenizer we created. Making the tokenizer configurable like this will allow you to experiment with better tokenizers that fit your particular dataset.</p>
<p>Training a Naive Bayes classifier is a straightforward process. First, simply count the number of documents in each category that you have seen. If your training set has 600 positive movie reviews and 400 negative movie reviews, then you should have 600 and 400 as your document counts, respectively. Next, tokenize the document to be trained. You must always make sure to use the same tokenizer during training as you do during evaluation. For each token in the training document, record how many times you've seen that token amongst all documents in the category. For example, if your training data has 600 positive movie reviews and the word <em>beautiful</em> appears in 100 of them, you would need to maintain a count of 100 for the token <em>beautiful</em> in the <em>positive</em> category. If the token <em>beautiful</em> only appears three times in your negative review training data, then you must maintain that count separately.</p>
<p>Let's translate this into code. It's a very simple operation, but we are also dividing up the work between many small count and incrementing functions; we will use these counting functions in our evaluation stage as well:</p>
<pre><span>/**<br/></span> <span>* Trains a given document for a label.<br/></span> <span>*</span> <span>@param</span> <span>label<br/></span> <span>*</span> <span>@param</span> <span>text<br/></span> <span>*/<br/></span><span>train</span>(label, text) {<br/>  <span>this</span>.<span>incrementLabelDocumentCount</span>(label);<br/>  <span>this</span>.<span>tokenizer</span>(text).<span>forEach</span>(token =&gt; <span>this</span>.<span>incrementTokenCount</span>(token, label));<br/>}<br/> <br/> <span>/**<br/></span> <span>* Increments the count of documents in a given category/label<br/></span> <span>*</span> <span>@param</span> <span>label<br/></span> <span>*/<br/></span><span>incrementLabelDocumentCount</span>(label) {<br/>  <span>this</span>.<span>database</span>.<span>labels</span>[label] = <span>this</span>.<span>getLabelDocumentCount</span>(label) + <span>1</span>;<br/>}<br/> <br/> <span>/**<br/></span> <span>* Returns the number of documents seen for a given category/label.<br/></span> <span>* If null is passed as the label, return the total number of training documents seen.<br/></span> <span>*</span> <span>@param</span> <span>label<br/></span> <span>*/<br/></span><span>getLabelDocumentCount</span>(label = <span>null</span>) {<br/>  <span>if</span> (label) {<br/>    <span>return this</span>.<span>database</span>.<span>labels</span>[label] || <span>0</span>;<br/>  } <span>else</span> {<br/>    <span>return</span> Object.<span>values</span>(<span>this</span>.<span>database</span>.<span>labels</span>)<br/>      .<span>reduce</span>((sum, count) =&gt; sum + count, <span>0</span>);<br/>  }<br/>}<br/> <br/> <span>/**<br/></span> <span>* Increment the count of a token observed with a given label.<br/></span> <span>*</span> <span>@param</span> <span>token<br/></span> <span>*</span> <span>@param</span> <span>label<br/></span> <span>*/<br/></span><span>incrementTokenCount</span>(token, label) {<br/>  <span>if</span> (<span>typeof this</span>.<span>database</span>.<span>tokens</span>[token] === <span>'undefined'</span>) {<br/>    <span>this</span>.<span>database</span>.<span>tokens</span>[token] = {};<br/>  }<br/> <br/>  <span>this</span>.<span>database</span>.<span>tokens</span>[token][label] = <span>this</span>.<span>getTokenCount</span>(token, label) + <span>1</span>;<br/>}<br/> <br/> <span>/**<br/></span> <span>* Get the number of times a token was seen with a given category/label.<br/></span> <span>* If no label is given, returns the total number of times the token was seen<br/></span> <span>* across all training examples.<br/></span> <span>*</span> <span>@param</span> <span>token<br/></span> <span>*</span> <span>@param</span> <span>label<br/></span> <span>*</span> <span>@returns</span> <span>{*}<br/></span> <span>*/<br/></span><span>getTokenCount</span>(token, label = <span>null</span>) {<br/>  <span>if</span> (label) {<br/>    <span>return</span> (<span>this</span>.<span>database</span>.<span>tokens</span>[token] || {})[label] || <span>0</span>;<br/>  } <span>else</span> {<br/>    <span>return</span> Object.<span>values</span>(<span>this</span>.<span>database</span>.<span>tokens</span>[token] || {})<br/>      .<span>reduce</span>((sum, count) =&gt; sum + count, <span>0</span>);<br/>  }<br/>}</pre>
<p>As you can see, the <kbd>train()</kbd> method is quite simple: increment the document count for the given label (for example, <kbd>spam</kbd> or <kbd>not spam</kbd>, <kbd>positive sentiment</kbd> or <kbd>negative sentiment</kbd>); then, for each token in the document, increment the token count for the given label (for example, <em>beautiful</em> was seen 100 times in positive-sentiment documents, and was seen three times in negative-sentiment documents). These counts are maintained in an instance variable called <kbd>this.database</kbd> in the <kbd>BayesClassifier</kbd> class.</p>
<p>In order to make a prediction on a new document, we'll need to consider each of the labels we encountered during training separately, calculate a probability for that label, and return the most probable label. Let's work backwards in terms of implementing the prediction; we'll start by adding the <kbd>predict</kbd> method and then work backwards, filling in all the other methods we'll need.</p>
<p>First, add the following <kbd>predict</kbd> method to the <kbd>BayesClassifier</kbd> class:</p>
<pre><span>/**<br/></span> <span>* Given a document, predict its category or label.<br/></span> <span>*</span> <span>@param</span> <span>text<br/></span> <span>*</span> <span>@returns</span> <span>{{label: string, probability: number, probabilities: array}}<br/></span> <span>*/<br/></span><span>predict</span>(text) {<br/>  <span>const</span> <span>probabilities</span> = <span>this</span>.<span>calculateAllLabelProbabilities</span>(text);<br/>  <span>const</span> <span>best</span> = <span>probabilities</span>[<span>0</span>];<br/> <br/>  <span>return</span> {<br/>    <span>label</span>: <span>best</span>.<span>label</span>,<br/>    <span>probability</span>: <span>best</span>.<span>probability</span>,<br/>    <span>probabilities<br/></span>  };<br/> <br/>}</pre>
<p>This method accepts an input string or document, and returns a <kbd>result</kbd> object with the most likely label or category, the probability of that label or category, and an array of all the probabilities for all labels encountered during training.</p>
<p>Next, add the method that <kbd>predict</kbd> relies upon to calculate the probability of each label on the input document:</p>
<pre><span>/**<br/></span> <span>* Given a document, determine its probability for all labels/categories encountered in the training set.<br/></span> <span>* The first element in the return array (element 0) is the label/category with the best match.<br/></span> <span>*</span> <span>@param</span> <span>text<br/></span> <span>*</span> <span>@returns</span> <span>{Array.&lt;Object&gt;}<br/></span> <span>*/<br/></span><span>calculateAllLabelProbabilities</span>(text) {<br/>  <span>const</span> <span>tokens</span> = <span>this</span>.<span>tokenizer</span>(text);<br/>  <span>return this</span>.<span>getAllLabels</span>()<br/>    .<span>map</span>(label =&gt; ({<br/>      label,<br/>      <span>probability</span>: <span>this</span>.<span>calculateLabelProbability</span>(label, <span>tokens</span>)<br/>    }))<br/>    .<span>sort</span>((a, b) =&gt; a.<span>probability</span> &gt; b.<span>probability</span> ? -<span>1</span> : <span>1</span>);<br/>}</pre>
<p>This method tokenizes the input text, and then generates an array of all labels and their probabilities, sorted in order of most probable to least probable. You will now need to add these two methods to the class—first, the simple <kbd>getAllLabels()</kbd> method:</p>
<pre><span>/**<br/></span> <span>* Get all labels encountered during training.<br/></span> <span>*</span> <span>@returns</span> <span>{Array}<br/></span> <span>*/<br/></span><span>getAllLabels</span>() {<br/>  <span>return</span> Object.<span>keys</span>(<span>this</span>.<span>database</span>.<span>labels</span>);<br/>}</pre>
<p>And then add the more complex <kbd>calculateLabelProbability</kbd>, which is responsible for calculating the probability of an individual label fitting a document:</p>
<pre><span>/**<br/></span> <span>* Given a token stream (ie a tokenized document), calculate the probability that<br/></span> <span>* this document has a given label.<br/></span> <span>*</span> <span>@param</span> <span>label<br/></span> <span>*</span> <span>@param</span> <span>tokens<br/></span> <span>*</span> <span>@returns</span> <span>{number}<br/></span> <span>*/<br/></span><span>calculateLabelProbability</span>(label, tokens) {<br/> <br/>  <span>// We assume that the a-priori probability of all labels are equal.<br/></span>  <span>// You could alternatively calculate the probability based on label frequencies.<br/></span>  <span>const</span> <span>probLabel</span> = <span>1</span> / <span>this</span>.<span>getAllLabels</span>().<span>length</span>;<br/> <br/>  <span>// How significant each token must be in order to be considered;<br/></span>  <span>// Their score must be greater than epsilon from the default token score<br/></span>  <span>// This basically filters out uninteresting tokens from consideration.<br/></span>  <span>// Responsible for 78% =&gt; 87.8% accuracy bump (e=.17) overall.<br/></span>  <span>const</span> <span>epsilon</span> = <span>0.15</span>;<br/> <br/>  <span>// For each token, we have to calculate a "token score", which is the probability of this document<br/></span>  <span>// belonging to a category given the token appears in it.<br/></span>  <span>const</span> <span>tokenScores</span> = tokens<br/>    .<span>map</span>(token =&gt; <span>this</span>.<span>calculateTokenScore</span>(token, label))<br/>    .<span>filter</span>(score =&gt; <span>Math</span>.<span>abs</span>(<span>probLabel</span> - score) &gt; <span>epsilon</span>);<br/> <br/> <span>// To avoid floating point underflow when working with really small numbers,<br/></span> <span>// we add combine the token probabilities in log space instead.<br/></span> <span>// This is only used because of floating point math and should not affect the algorithm overall.<br/></span>  <span>const</span> <span>logSum</span> = <span>tokenScores</span>.<span>reduce</span>((sum, score) =&gt; sum + (<span>Math</span>.<span>log</span>(<span>1</span>-score) - <span>Math</span>.<span>log</span>(score)), <span>0</span>);<br/>  <span>const</span> <span>probability</span> = <span>1</span> / (<span>1</span> + <span>Math</span>.<span>exp</span>(<span>logSum</span>));<br/> <br/>  <span>return</span> <span>probability</span>;<br/>}</pre>
<p>The inline comments in the <kbd>calculateLabelProbability</kbd> method illuminate the specifics of how the method works, but the basic goal of this step is to calculate a probability for each token in the document, and then to combine the individual token probabilities into one overall probability for the label.</p>
<p>For instance, if a movie review states <em>beautiful [but] awful garbage</em>, this method is responsible for looking at all of the tokens (<em>but</em> is omitted by the tokenizer) and determining how well they fit a given label (for example, <em>positive</em> or <em>negative</em>).</p>
<p>Let's imagine we're running this method for the <em>positive </em>category label. The word <em>beautiful</em> would get a strong score, maybe 90%, but the tokens <em>awful</em> and <em>garbage</em> would both get weak scores, for instance 5%. This method would then report that the probability of the <em>positive</em> label is low for this document. On the other hand, when this method is run for the <em>negative</em> category label, the <em>beautiful</em> token gets a low score but both <em>awful</em> and <em>garbage</em> get high scores, so the method will return a high probability of the document being negative.</p>
<p>This method involves a couple of tricks. The first one is an accuracy enhancement. If a token is ambiguous (a word such as <em>that</em> or <em>movie</em>, something that applies equally to all categories), it is removed from consideration. We do this by filtering out token scores that are close to 50%; specifically, we ignore all tokens that have a score between 35-65%. This is a very effective technique and increases accuracy by about 10%. The reason it works so well is that it filters out noise in those marginal tokens. If the word <em>movie</em> has a positive score of 55% but is generally seen in both positive and negative documents, it'll skew all documents toward the positive category. Our approach is to instead only consider the most impactful tokens.</p>
<p>The second trick is our log sum approach. Normally, the way to combine individual words or token probabilities into an overall probability looks like this—assuming you already have an array variable called <kbd>tokenScores</kbd>:</p>
<pre><span>const</span> <span>multiplyArray</span> = <span>arr</span> =&gt; arr.<span>reduce</span>((product, current) =&gt; current * product, <span>1</span>);<br/><span>const</span> <span>tokenScores</span> = []; <span>// array of scores, defined elsewhere<br/></span><span>const</span> <span>inverseTokenScores</span> = <span>tokenScores</span>.<span>map</span>(<span>score</span> =&gt; <span>1</span> - score);<br/><span>const</span> <span>combinedProbability</span> = <span>multiplyArray</span>(<span>tokenScores</span>) / (<span>multiplyArray</span>(<span>tokenScores</span>) + <span>multiplyArray</span>(<span>inverseTokenScores</span>));</pre>
<p>Put another way, assume you have probabilities for individual tokens called <kbd>p1</kbd>, <kbd>p2</kbd>, <kbd>p3</kbd>, ... <kbd>pN</kbd>; the way to get the combined probability of all those tokens would be:</p>
<pre>p = (p1 * p2 * p3 * ... pN) / ( (p1 * p2 * p3 * ... pN) + (1-p1 * 1-p2 * 1-p3 * ... 1-pN) )</pre>
<p>This approach has some issues when dealing with small, floating-point numbers. If you start multiplying small, floating-point numbers by each other, you risk creating numbers so small that floating-point math can't deal with it, and you get <em>floating-point underflow</em>, or NaN in JavaScript. The solution is to convert this calculation to log space, and manage the whole calculation by adding natural log values of each of the probabilities and removing the log at the end.</p>
<p>The final piece of the puzzle is to generate the probabilities of each individual token given a label. This is where the Bayes theorem truly comes into play. What we're looking for is a probability like <kbd>P(L|W)</kbd>, or the probability that the document has a <strong>label</strong> given a <strong>word</strong>. We need this probability for each token in the document, and for each label that we're considering. However, we don't have the <kbd>P(L|W)</kbd> value on hand, so we can use Bayes' theorem to get an equivalent expression:</p>
<div class="CDPAlignCenter CDPAlign"><em>P(L|W) = P(W|L)P(L) / P(W|L)P(L) + P(W|L')P(L')</em></div>
<p>This may look complicated, but it's not bad. We are transforming the <kbd>P(L|W)</kbd> goal into much easier probabilities, such as <kbd>P(W|L)</kbd> (the probability the word appears given a label, or its frequency in that label) and <kbd>P(L)</kbd> (the probability of any given label). The denominator also uses the inverse probabilities, <kbd>P(W|L')</kbd> (the probability the word appears in any other label) and <kbd>P(L')</kbd> (the probability of any other label).</p>
<p>We make this transformation because we can get the word frequencies just by counting tokens and labels when we see them during training; we do not need to record which tokens appear in which documents, and we can keep our database simple and fast.</p>
<p>The preceding expression is what we've been calling the <em>token score</em>, or the probability that a document has a label, given that the document has a word in it. Making things a little more concrete, we can ask the question <kbd>P("positive review" | "beautiful")</kbd>, or the probability that a document is a positive movie review, given that the word beautiful is in it.</p>
<p>If there is a 50/50 chance of reviews being positive or negative, and we see the word <em>beautiful</em> in 10% of positive reviews and only 1% of negative reviews, then our <kbd>P(L|W)</kbd> probability is around 91%. (The calculation for this was <kbd>(0.1 * 0.5) / ( (0.1 * 0.5) + (0.01 * 0.5) )</kbd>, using the preceding formula.) You can interpret this 91% figure as the <em>positivity</em> of the word <em>beautiful</em>. By analyzing all words in a document in this manner, we can combine their positivity scores to get an overall probability that a document is positive. The same holds for any type of classification, whether it's positive/negative movie reviews, spam/ham emails, or English/French/Spanish language detection.</p>
<p>There is one other thing we need to consider when calculating token scores. What do we do if we've never seen a token before? Or if we've only seen it once or twice? The best approach for us is to adjust the token score that we calculate by a weighted average; we want to weight the average so that rare words are pulled toward a 50/50 score.</p>
<p>Let's implement all of the preceding logic. This method is long, but as you can see, much of the work is simply grabbing the correct counts for the various variables we need to calculate. We also define a <em>strength</em> for our rare word weighting; we define the strength as three so that we must see the token in question three times for it to have an equivalent weight as the default 50/50 weighting:</p>
<pre> <span>/**<br/></span> <span>* Given a token and a label, calculate the probability that<br/></span> <span>* the document has the label given that the token is in the document.<br/></span> <span>* We do this by calculating the much easier to find Bayesian equivalent:<br/></span> <span>* the probability that the token appears, given the label (the word frequency in that category).<br/></span> <span>* This method also adjusts for rare tokens.<br/></span> <span>*</span> <span>@param</span> <span>token<br/></span> <span>*</span> <span>@param</span> <span>label<br/></span> <span>*</span> <span>@returns</span> <span>{number}<br/></span> <span>*/<br/></span><span>calculateTokenScore</span>(token, label) {<br/>  <span>const</span> <span>rareTokenWeight</span> = <span>3</span>;<br/> <br/>  <span>const</span> <span>totalDocumentCount</span> = <span>this</span>.<span>getLabelDocumentCount</span>();<br/>  <span>const</span> <span>labelDocumentCount</span> = <span>this</span>.<span>getLabelDocumentCount</span>(label);<br/>  <span>const</span> <span>notLabelDocumentCount</span> = <span>totalDocumentCount</span> - <span>labelDocumentCount</span>;<br/> <br/>  <span>// Assuming equal probabilities gave us 1% accuracy bump over using the frequencies of each label<br/></span>  <span>const</span> <span>probLabel</span> = <span>1</span> / <span>this</span>.<span>getAllLabels</span>().<span>length</span>;<br/>  <span>const</span> <span>probNotLabel</span> = <span>1</span> - <span>probLabel</span>;<br/> <br/>  <span>const</span> <span>tokenLabelCount</span> = <span>this</span>.<span>getTokenCount</span>(token, label);<br/>  <span>const</span> <span>tokenTotalCount</span> = <span>this</span>.<span>getTokenCount</span>(token);<br/>  <span>const</span> <span>tokenNotLabelCount</span> = <span>tokenTotalCount</span> - <span>tokenLabelCount</span>;<br/> <br/>  <span>const</span> <span>probTokenGivenLabel</span> = <span>tokenLabelCount</span> / <span>labelDocumentCount</span>;<br/>  <span>const</span> <span>probTokenGivenNotLabel</span> = <span>tokenNotLabelCount</span> / <span>notLabelDocumentCount</span>;<br/>  <span>const</span> <span>probTokenLabelSupport</span> = <span>probTokenGivenLabel</span> * <span>probLabel</span>;<br/>  <span>const</span> <span>probTokenNotLabelSupport</span> = <span>probTokenGivenNotLabel</span> * <span>probNotLabel</span>;<br/> <br/>  <span>const</span> <span>rawWordScore</span> =<br/>    (<span>probTokenLabelSupport</span>)<br/>    /<br/>    (<span>probTokenLabelSupport</span> + <span>probTokenNotLabelSupport</span>);<br/> <br/>  <span>// Adjust for rare tokens -- essentially weighted average<br/></span>  <span>// We're going to shorthand some variables to make reading easier.<br/></span>  <span>// s is the "strength" or the "weight"<br/></span>  <span>// n is the number of times we've seen the token total<br/></span>  <span>const</span> <span>s</span> = <span>rareTokenWeight</span>;<br/>  <span>const</span> <span>n</span> = <span>tokenTotalCount</span>;<br/>  <span>const</span> <span>adjustedTokenScore</span> =<br/>    ( (<span>s</span> * <span>probLabel</span>) + (<span>n</span> * (<span>rawWordScore</span> || <span>probLabel</span>)) )<br/>    /<br/>    ( <span>s</span> + <span>n</span> );<br/> <br/>  <span>return</span> <span>adjustedTokenScore</span>;<br/>}</pre>
<p>To review the way this algorithm works, here is a brief summary:</p>
<p>To train:</p>
<ol>
<li>Accept an input document and known label or category</li>
<li>Tokenize the input document into an array of tokens</li>
<li>Record the total number of documents you've seen for this specific label</li>
<li>For each token, record the number of times you've seen this token <em>with this specific label</em></li>
</ol>
<p>To predict:</p>
<ol>
<li>Accept an input document and tokenize it</li>
<li class="mce-root">For each possible label (all the labels you encountered during training), and for each token in the document, calculate the <em>token score</em> for that token (mathematically, the probability of a document having that label, given that specific token)</li>
</ol>
<ol start="3">
<li class="mce-root">You may need to filter token scores for significance</li>
<li class="mce-root">You may need to adjust token scores for rare words</li>
<li class="mce-root">For each possible label, combine the token scores into a single, overall label probability (for example, the probability that the document is in this category or label)</li>
<li class="mce-root">Report the label with the highest overall probability</li>
</ol>
<p>With all of our code added, we're ready to train and test our Naive Bayes classifier. We'll train it on IMDB movie reviews, and try to guess the sentiment of never-before-seen reviews.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 3 – Movie review sentiment</h1>
                </header>
            
            <article>
                
<p>We're going to use our Naive Bayes classifier to tackle the <em>sentiment analysis</em> problem, or the problem of inspecting a piece of text and determining whether it has an overall positive or negative sentiment. This is a common analysis done in advertising, marketing, and public relations; most brand managers want to know whether people on Twitter have good things or bad things to say about their brand or product.</p>
<p>The training data for this example will come from <a href="https://www.imdb.com/" target="_blank">https://www.imdb.com/</a>. We'll train our classifier on positive and negative movie reviews, and then use our classifier to check untrained (but pre-labeled) reviews to see how many it gets right.</p>
<p>If you haven't done so yet, download the data files from the <kbd>data</kbd> directory from this project's GitHub page. You will need all four text files: <kbd>train_positive.txt</kbd>, <kbd>train_negative.txt</kbd>, <kbd>test_positive.txt</kbd>, and <kbd>test_negative.txt</kbd>. We will use the two training files for training, and the two test files for validation.</p>
<p>Next, create an <kbd>index.js</kbd> file in the <kbd>src</kbd> folder. Add the following code to the top of the file:</p>
<pre><span>import</span> readline <span>from</span> <span>'readline'</span>;<br/><span>import</span> fs <span>from</span> <span>'fs'</span>;<br/><span>import</span> BayesClassifier, {<span>simpleTokenizer</span>} <span>from</span> <span>"./bayes"</span>;<br/> <br/><span>const</span> <span>classifier</span> = <span>new</span> BayesClassifier(<span>simpleTokenizer</span>);</pre>
<p>We import the <kbd>readline</kbd> and <kbd>fs</kbd> libraries to help us process the training files. Next, create a <kbd>utility</kbd> function to help us train the classifier:</p>
<pre><span>const</span> <span>trainer</span> = (filename, label, classifier) =&gt; {<br/> <br/>  <span>return new</span> Promise((resolve) =&gt; {<br/>    <span>console</span>.<span>log</span>(<span>"Training "</span> + label + <span>" examples..."</span>);<br/>    readline.createInterface({<br/>      <span>input</span>: fs.createReadStream(filename)<br/>    })<br/>      .<span>on</span>(<span>'line'</span>, line =&gt; classifier.<span>train</span>(label, line))<br/>      .<span>on</span>(<span>'close'</span>, () =&gt; {<br/>        <span>console</span>.<span>log</span>(<span>"Finished training "</span> + label + <span>" examples."</span>);<br/>        resolve();<br/>      });<br/>  });<br/>}</pre>
<p>This <kbd>helper</kbd> function accepts a filename, a label, and an instance of a <kbd>BayesClassifier</kbd> class. It reads an input file line by line, and trains the classifier on each line for the given label. All the logic is wrapped up in a promise so that we can externally detect when the trainer has completed.</p>
<p>Next, add a helper utility to test the classifier. In order to test the classifier, it must be trained first. The testing function will open up a test file with a known label, and test each line in the file using the classifier's <kbd>predict</kbd> method. The utility will count how many examples the classifier got right and how many it got wrong, and report back:</p>
<pre><span>const</span> <span>tester</span> = (filename, label, classifier) =&gt; {<br/> <br/>  <span>return new</span> Promise((resolve) =&gt; {<br/>    <span>let</span> <span>total</span> = <span>0</span>;<br/>    <span>let</span> <span>correct</span> = <span>0</span>;<br/>    <span>console</span>.<span>log</span>(<span>"Testing "</span> + label + <span>" examples..."</span>);<br/>    readline.createInterface({ <span>input</span>: fs.createReadStream(filename) })<br/>      .<span>on</span>(<span>'line'</span>, line =&gt; {<br/>        <span>const</span> <span>prediction</span> = classifier.<span>predict</span>(line);<br/>        <span>total</span>++;<br/>        <span>if</span> (<span>prediction</span>.<span>label</span> === label) {<br/>          <span>correct</span>++;<br/>        }<br/>      })<br/>      .<span>on</span>(<span>'close'</span>, () =&gt; {<br/>        <span>console</span>.<span>log</span>(<span>"Finished testing "</span> + label + <span>" examples."</span>);<br/>        <span>const</span> <span>results</span> = {<span>total</span>, <span>correct</span>};<br/>        <span>console</span>.<span>log</span>(<span>results</span>);<br/>        resolve(<span>results</span>);<br/>      });<br/>  }); <br/>}</pre>
<p>We also wrap this in a promise, and make sure to deliver the results as part of the resolution of the promise, so we can inspect the results from without.</p>
<p>Finally, add some bootstrap code. This code will train the classifier on the two training files, wait for training to complete, and then test the classifier on the two test files, reporting the overall results when finished:</p>
<pre>Promise.<span>all</span>([<br/>  <span>trainer</span>(<span>'./data/train_positive.txt'</span>, <span>'positive'</span>, <span>classifier</span>),<br/>  <span>trainer</span>(<span>'./data/train_negative.txt'</span>, <span>'negative'</span>, <span>classifier</span>)<br/>])<br/>  .<span>then</span>(() =&gt; {<br/>    <span>console</span>.<span>log</span>(<span>"Finished training. Now testing."</span>);<br/> <br/>    Promise.<span>all</span>([<br/>      <span>tester</span>(<span>'./data/test_negative.txt'</span>, <span>'negative'</span>, <span>classifier</span>),<br/>      <span>tester</span>(<span>'./data/test_positive.txt'</span>, <span>'positive'</span>, <span>classifier</span>)<br/>    ])<br/>      .<span>then</span>(results =&gt; results.<span>reduce</span>(<br/>        (obj, item) =&gt; ({<span>total</span>: obj.<span>total</span> + item.<span>total</span>, <span>correct</span>: obj.<span>correct</span> + item.<span>correct</span>}), {<span>total</span>: <span>0</span>, <span>correct</span>: <span>0</span>}<br/>      ))<br/>      .<span>then</span>(results =&gt; {<br/>        <span>const</span> <span>pct</span> = (<span>100</span> * results.<span>correct</span> / results.<span>total</span>).<span>toFixed</span>(<span>2</span>) + <span>'%'</span>;<br/>        <span>console</span>.<span>log</span>(results);<br/>        <span>console</span>.<span>log</span>(<span>"Test results: "</span> + <span>pct</span>);<br/>      });<br/> })</pre>
<p>Once this code is added, you can run the program by issuing <kbd>yarn start</kbd> from the command line. You should see output like the following:</p>
<pre><strong>Training positive examples...</strong><br/><strong>Training negative examples...</strong><br/><strong>Finished training positive examples.</strong><br/><strong>Finished training negative examples.</strong><br/><strong>Finished training. Now testing.</strong><br/><strong>Testing negative examples...</strong><br/><strong>Testing positive examples...</strong><br/><strong>Finished testing positive examples.</strong><br/><strong>{ total: <span>4999</span>, correct: <span>4402</span> }</strong><br/><strong>Finished testing negative examples.</strong><br/><strong>{ total: <span>5022</span>, correct: <span>4738</span> }</strong><br/><strong>{ total: <span>10021</span>, correct: <span>9140</span> }</strong><br/><strong>Test results: <span>91.21</span>%</strong></pre>
<p>This simple, probabilistic classifier has an accuracy of over 91%! A 9% error rate may not seem impressive, but in the ML world this is actually a very good result, especially considering the ease of implementation and speed of operation of the classifier. These results are why the Naive Bayes classifier is so popular when classifying text. With more thoughtful tokenization, especially in narrow fields, such as spam detection, you can get the accuracy of a Naive Bayes classifier over 95%.</p>
<p>Let's see what an individual example looks like. You can add the following code to the <kbd>index.js</kbd> file if you would like to test out some documents on your own:</p>
<pre>Promise.<span>all</span>([<br/>  <span>trainer</span>(<span>'./data/train_positive.txt'</span>, <span>'positive'</span>, <span>classifier</span>),<br/>  <span>trainer</span>(<span>'./data/train_negative.txt'</span>, <span>'negative'</span>, <span>classifier</span>)<br/>])<br/>  .<span>then</span>(() =&gt; {<br/> <br/>    <span>const</span> <span>tests</span> = [<br/>      <span>"i really hated this awful movie, it was so bad I didn't even know what to do with myself"</span>,<br/>      <span>"this was the best movie i've ever seen. it was so exciting, i was on the edge of my seat every minute"</span>,<br/>      <span>"i am indifferent about this"<br/></span>    ];<br/> <br/>    <span>tests</span>.<span>forEach</span>(test =&gt; {<br/>      <span>console</span>.<span>log</span>(<span>"Testing: "</span> + test);<br/>      <span>const</span> <span>result</span> = <span>classifier</span>.<span>predict</span>(test);<br/>      <span>console</span>.<span>log</span>(<span>result</span>);<br/>    });<br/>  });</pre>
<p>Running the preceding code results in the following code:</p>
<pre><strong>Training positive examples...</strong><br/><strong>Training negative examples...</strong><br/><strong>Finished training positive examples.</strong><br/><strong>Finished training negative examples.</strong><br/> <br/><strong>Testing: <span>i</span> really hated <span>this</span> awful movie, it was so bad I didn<span>'t even know what to do with myself<br/></span>{ label: <span>'negative'</span>,</strong><br/><strong> probability: <span>0.9727173302897202</span>,</strong><br/><strong> probabilities:</strong><br/><strong> [ { <span>label</span>: <span>'negative'</span>, <span>probability</span>: <span>0.9727173302897202</span> },</strong><br/><strong> { <span>label</span>: <span>'positive'</span>, <span>probability</span>: <span>0.027282669710279664</span> } ] }</strong><br/> <br/><strong>Testing: <span>this</span> was the best movie <span>i</span><span>'ve ever seen. it was so exciting, i was on the edge of my seat every minute<br/></span>{ label: <span>'positive'</span>,</strong><br/><strong> probability: <span>0.8636681390743286</span>,</strong><br/><strong> probabilities:</strong><br/><strong> [ { <span>label</span>: <span>'positive'</span>, <span>probability</span>: <span>0.8636681390743286</span> },</strong><br/><strong> { <span>label</span>: <span>'negative'</span>, <span>probability</span>: <span>0.13633186092567148</span> } ] }</strong><br/> <br/><strong>Testing: <span>i</span> <span>am</span> indifferent about <span>this<br/></span>{ label: <span>'negative'</span>,</strong><br/><strong> probability: <span>0.5</span>,</strong><br/><strong> probabilities:</strong><br/><strong> [ { <span>label</span>: <span>'negative'</span>, <span>probability</span>: <span>0.5</span> },</strong><br/><strong> { <span>label</span>: <span>'positive'</span>, <span>probability</span>: <span>0.5</span> } ] }</strong></pre>
<p>The classifier works as expected. Our strongly negative statement has a 97% probability of being negative. Our positive statement has an 86% probability of being positive. And our indifferent statement, even though it returns the negative label, also reports an even 50/50 probability split between positive and negative sentiments.</p>
<p>We did all this, and achieved great accuracy, by simply counting the number of times we saw words in documents and using centuries-old probability theory to interpret the data. We didn't need a neural network, an advanced framework, or a deep natural language programming knowledge to get these results; for these reasons, the Naive Bayes classifier should be one of the core algorithms you pay attention to when researching ML.</p>
<p>In the following sections, we'll take a look at two more classification algorithms that should not be ignored: the SVM and the random forest.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support Vector Machine</h1>
                </header>
            
            <article>
                
<p>An SVM is a numerical classifier that in some ways is similar to the KNN algorithm, although the SVM is far more mathematically advanced. Rather than comparing a test point to the points closest to it, an SVM attempts to draw boundary lines between the classes of data points, creating regions where all points inside that region will be considered a member of that class.</p>
<p>Consider this image (from Wikipedia's article on SVMs). The two categories of data points are separated by a straight line. The line that separates the classes is chosen as the line of <em>maximum margin</em>, meaning this dividing line has the most room on either side of it, as compared to any other separating line you can draw:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-303 image-border" src="assets/45f79df8-1acd-4185-971b-6e4054d2de29.png" style="width:27.83em;height:29.50em;"/></div>
<p>The SVM, exactly as implemented here, is useful in some limited situations, but is not a powerful tool, because it requires that the classes be <em>linearly separable</em>; that is, it requires that you can draw a straight line through the two classes. This SVM is also a <em>binary classifier</em>, meaning it only works with two categories or classes.</p>
<p>Consider the following data (this image and the one after are both courtesy of Shiyu Ji, licensed under Creative Commons CC BY-SA 4.0). While there are only two classes, they are not linearly separable; only a circle or ellipse can separate the two classes:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-304 image-border" src="assets/82c62bf3-1bbb-4414-83c3-d57062f504ab.png" style="width:27.92em;height:26.42em;"/></div>
<p>While the SVM has been around since the 1960s, it wasn't until 1992 that researchers figured out how to approach this problem. By using a technique called the <strong>kernel trick</strong>, you can transform the non-linearly-separable data into linearly-separable data in a higher number of dimensions. In this case, transforming the data through a kernel will add a third dimension, and it's that new third dimension that becomes linearly separable:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-305 image-border" src="assets/d7619f08-a3f9-402b-a36f-a1ad21502235.png" style="width:30.83em;height:31.42em;"/></div>
<p>After applying the kernel trick, the data has been mapped onto three dimensions. The class of red data points have been pulled downward in the third dimension, while the purple points have been pulled upward. It's now possible to draw a plane (the three-dimensional equivalent of a straight line in two dimensions) that separates the two categories.</p>
<p>Through appropriate selection of kernels and parameters, the support vector machine can work its way through all sorts of shapes of data. While the support vector machine will always draw a line, plane, or hyperplane (a higher-dimensional version of a plane) through the data—these are always <em>straight—</em>the algorithm first transforms the data into something that can be separated by straight lines.</p>
<p>There are many types of kernels that can be used with an SVM. Each kernel transforms the data in a different manner, and the appropriate selection of kernel will depend on the shape of your data. In our case, we will use the <em>radial basis function kernel</em>, which is a good general-purpose kernel to use for clustered data. The SVM itself has settings and parameters that you must tweak, such as the error cost parameter, but keep in mind that the kernel you select may also have its own configurable parameters. The radial basis function, for instance, uses a parameter called <strong>gamma</strong>, which controls the curvature of the kernel.</p>
<p>Because SVMs require a lot of math, we won't attempt to build our own. Instead, we'll use an off-the-shelf library with a popular, classical dataset. The dataset we'll use is called the <kbd>iris flower</kbd> dataset. This particular dataset was created around 1936 by Edgar Anderson (a botanist) and Ronald Fisher (a statistician and biologist). Anderson chose three species of iris flowers, specifically the <em>Iris setosa,</em> the <em>Iris versicolor,</em> and the <em>Iris virginica</em>. For each species, Anderson chose 50 samples and measured the petal length, petal width, sepal length, and sepal width, and recorded the measurements along with the species name (a <em>sepal</em> is the green leaf that protects the flower bud before it blooms).</p>
<p>The <kbd>Iris</kbd> dataset is a common toy or test dataset for many ML algorithms for a few reasons. It's a small dataset: there are only 150 samples, four dimensions or features, and three categories. The data is multidimensional, but with only four features, it is still easy to visualize and understand intuitively. The pattern in the data is also interesting and poses a non-trivial challenge for classifiers: one species (<em>Iris setosa</em>) is clearly separated from the other two, but <em>Iris versicolor</em> and <em>Iris virginica</em> are more intermingled.</p>
<p>Because the data is four-dimensional, it cannot be visualized directly, but we can plot each combination of two features separately into a grid. This image is courtesy of Wikipedian Nicoguaro, and is licensed CC BY 4.0:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-306 image-border" src="assets/a9554a32-a247-4bae-85a1-f90aac3b5203.png" style="width:42.67em;height:42.67em;"/></div>
<p>You can see why this dataset would be interesting to researchers. In several dimensions, such as sepal length versus sepal width, the <em>Iris versicolor</em> and <em>Iris virginica</em> overlap a great deal; in others, they look nearly linearly separable, for instance, in the petal length versus petal width plots.</p>
<p>Let's finally implement an SVM to solve this problem for us.</p>
<p>Create a new folder called <kbd>Ch5-SVM</kbd> and add the following <kbd>package.json</kbd> file:</p>
<pre>{<br/> <span>"name"</span>: <span>"Ch5-SVM"</span>,<br/> <span>"version"</span>: <span>"1.0.0"</span>,<br/> <span>"description"</span>: <span>"ML in JS Example for Chapter 5 - Support Vector Machine"</span>,<br/> <span>"main"</span>: <span>"src/index.js"</span>,<br/> <span>"author"</span>: <span>"Burak Kanber"</span>,<br/> <span>"license"</span>: <span>"MIT"</span>,<br/> <span>"scripts"</span>: {<br/> <span>"build-web"</span>: <span>"browserify src/index.js -o dist/index.js -t [ babelify --presets [ env ] ]"</span>,<br/> <span>"build-cli"</span>: <span>"browserify src/index.js --node -o dist/index.js -t [ babelify --presets [ env ] ]"</span>,<br/> <span>"start"</span>: <span>"yarn build-cli &amp;&amp; node dist/index.js"<br/></span> },<br/> <span>"dependencies"</span>: {<br/> <span>"babel-core"</span>: <span>"^6.26.0"</span>,<br/> <span>"babel-plugin-transform-object-rest-spread"</span>: <span>"^6.26.0"</span>,<br/> <span>"babel-preset-env"</span>: <span>"^1.6.1"</span>,<br/> <span>"babelify"</span>: <span>"^8.0.0"</span>,<br/> <span>"browserify"</span>: <span>"^15.1.0"</span>,<br/> <span>"libsvm-js"</span>: <span>"^0.1.3"</span>,<br/> <span>"ml-cross-validation"</span>: <span>"^1.2.0"</span>,<br/> <span>"ml-dataset-iris"</span>: <span>"^1.0.0"</span>,<br/> <span>"ml-random-forest"</span>: <span>"^1.0.2"<br/></span> }<br/> }</pre>
<p>Once the file is in place, run <kbd>yarn install</kbd> to install all the dependencies. Rather than using a <kbd>data.js</kbd> file, we will use the <kbd>Iris</kbd> dataset that comes with the <kbd>MLJS</kbd> library.</p>
<p>Next, create an <kbd>src</kbd> folder and an <kbd>index.js</kbd> file. At the top of <kbd>index.js</kbd>, import the following:</p>
<pre class="mce-root"><span>import</span> SVM from <span>'libsvm-js/asm'</span>;<br/><span>import</span> IrisDataset from <span>'ml-dataset-iris'</span>;</pre>
<p class="mce-root">Next, we need to extract the data from the <kbd>IrisDataset</kbd> library. This implementation of the SVM algorithm requires our labels to be integers (it doesn't support strings as labels), so we must simply map the species names from the dataset to integers:</p>
<pre class="mce-root"><span>const</span> data = IrisDataset.getNumbers();<br/><span>const</span> labels = IrisDataset.getClasses().map(<br/>  (elem) =&gt; IrisDataset.getDistinctClasses().indexOf(elem)<br/>);</pre>
<p>Let's also write a simple function that measures accuracy, or more specifically <em>los</em><em>s</em> (or error). This function must accept an array of expected values as well as an array of actual values, and return the proportion of incorrect guesses:</p>
<pre><span>const</span> loss = (expected, actual) =&gt; {<br/>  <span>let</span> incorrect = <span>0</span>,<br/>  len = expected.length;<br/>  <span>for</span> (<span>let</span> i <span>in</span> expected) {<br/>    <span>if</span> (expected[i] !== actual[i]) {<br/>      incorrect++;<br/>    }<br/>  }<br/>  <span>return</span> incorrect / len;<br/>};</pre>
<p>We're now ready to implement the SVM class. We will test our classifier in two ways: first, we'll train the classifier on the full dataset and then test it on the full dataset; this will test the algorithm's ability to fit data. Then we will use a cross-validation method to train the classifier on only subsets of the data and test it on unseen data; this will test the algorithm's ability to generalize its learning.</p>
<p>Add the following code to <kbd>index.js</kbd>:</p>
<pre>console.log(<span>"Support Vector Machine"</span>);<br/>console.log(<span>"======================"</span>);<br/> <br/><span>const</span> svm = <span>new</span> SVM({<br/>  kernel: SVM.KERNEL_TYPES.RBF,<br/>  type: SVM.SVM_TYPES.C_SVC,<br/>  gamma: <span>0.25</span>,<br/>  cost: <span>1</span>,<br/>  quiet: <span>true<br/></span>});<br/> <br/>svm.train(data, labels);<br/> <br/><span>const</span> svmPredictions = svm.predict(data);<br/><span>const</span> svmCvPredictions = svm.crossValidation(data, labels, <span>5</span>);<br/> <br/>console.log(<span>"Loss for predictions: "</span> + Math.round(loss(labels, svmPredictions) * <span>100</span>) + <span>"%"</span>);<br/>console.log(<span>"Loss for crossvalidated predictions: "</span> + Math.round(loss(labels, svmCvPredictions) * <span>100</span>) + <span>"%"</span>);</pre>
<p>We initialize the SVM with some reasonable parameters. We choose the radial basis function as our kernel, we choose a specific algorithm called <strong>CSVC</strong> for our SVM (this is the most common SVM algorithm), and we choose values of 1 for cost and 0.25 for gamma. Cost and gamma will both have similar effects on how the classifier draws boundaries around your classes: the larger the values, the tighter the curves and boundaries around the clusters will be.</p>
<p>The <kbd>svm.crossValidation</kbd> method accepts three arguments: the data, the labels, and the number of segments to divide the data into, reserving one segment for validation on each pass.</p>
<p>Run <kbd>yarn start</kbd> from the command line and you should see the following:</p>
<pre><strong> Support Vector Machine</strong><br/><strong> =============================================</strong><br/><strong> Loss for predictions: 1%</strong><br/><strong> Loss for crossvalidated predictions: 3%</strong></pre>
<p>This is a very strong result. The SVM was able to correctly recall 99% of training examples, meaning only a couple of data points were guessed incorrectly after being fully trained. When crossvalidating, we see a loss of only 3%; only perhaps five examples out of 150 were guessed incorrectly. The crossvalidation step is important because it more accurately represents what real-world performance would be; you should tune your algorithm's parameters so that the crossvalidated accuracy is maximized.</p>
<p>It is easy to get 100% accuracy for the fully-trained algorithm: we can simply overfit the data and memorize the category of each datapoint. Change the values of both gamma and cost to 50 and re-run the algorithm. You should see something like this:</p>
<pre><strong> Support Vector Machine</strong><br/> <strong>=============================================</strong><br/> <strong>Loss for predictions: 0%</strong><br/> <strong>Loss for crossvalidated predictions: 25%</strong></pre>
<p>By cranking up the cost and the gamma, we're drawing really tight boundaries around our existing data points. With a high enough value of cost and gamma, we might even be drawing individual circles around each and every data point! The result is a perfect score when testing the fully-trained classifier (for example, every training point has been memorized), but an awful score when cross-validating the dataset. Our cross-validation uses 80% of the data for training and reserves 20% for validation; in this case, we've overfit the training data so much that the classifier simply cannot categorize unseen data points. The classifier has memorized the data, but has not learned from it.</p>
<p>As a rule of thumb, a good starting point for the cost value is around 1. A higher cost will penalize training errors more harshly, meaning that your classification boundaries will try to more tightly wrap the training data. The cost parameter attempts to balance the simplicity of the boundaries with the recall of the training data: a lower cost will favor simpler, smoother boundaries, while a higher cost will favor higher training accuracy even if it means drawing more complex boundaries. This might lead to large sections of the sample space being misclassified with real-world data, especially if your dataset is highly dispersed. A higher cost value works better for very tightly clustered and neatly separated data; the more you trust your data, the higher you can make the cost. Values between 0.01 and 100 are most common for the cost parameter, though there are certainly cases where you may need a larger or smaller cost.</p>
<p>Similarly, the gamma value also controls the shape and curvature of the SVM's boundaries, however, this value influences the data preprocessing when applying the kernel trick to transform the data. The result is similar to that of the cost parameter, but arises from a completely different mechanism. The gamma parameter essentially controls the influence of a single training example. Lower values for gamma will result in smoother, broader boundaries around training points, while higher values will result in closer, tighter boundaries. One common rule of thumb for gamma is to set it to roughly 1/M, where M is the number of features in your data. In our case, we have four features or dimensions in our data, so we've set gamma to 1/4 or 0.25.</p>
<p>When training an SVM for the first time, you should always use cross-validation to tune your parameters. As with any ML algorithm, you'll have to tune the parameters to fit your data set and make sure that you're sufficiently generalizing the problem and not overfitting your data. Tune and test parameters methodically: for instance, choose five possible values for cost and five possible values for gamma, test all 25 combinations with cross-validation, and choose the parameters with the highest accuracy.</p>
<p>Next, we'll take a look at a modern workhorse of ML: the random forest.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forest</h1>
                </header>
            
            <article>
                
<p>The random forest algorithm is modern, versatile, robust, accurate, and is deserving of consideration for nearly any new classification task that you might encounter. It won't always be the best algorithm for a given problem domain, and it has issues with high dimensional and very large datasets. Give it more than 20-30 features or more than, say, 100,000 training points and it will certainly struggle in terms of resources and training time.</p>
<p>However, the random forest is virtuous in many ways. It can easily handle features of different types, meaning that some features can be numerical and others can be categorical; you can blend features such as <kbd>number_of_logins: 24</kbd> with features such as <kbd>account_type: guest</kbd>. A random forest is very robust to noise and therefore performs well with real-world data. Random forests are designed to avoid overfitting, and therefore are quite easy to train and implement, requiring less tweaking and tuning than other algorithms. Random forests also automatically evaluate the importance of each feature of your data, and therefore can help you reduce dimensionality or select better features <em>for free</em>, so to speak. And while random forests can be expensive for high dimensional data, in my experience, most real-world ML problems involve only about a dozen features and a few thousand training points, which random forests can handle. These virtues make the random forest a great go-to algorithm for general-purpose classification tasks.</p>
<p>I am therefore heartbroken to report that, at the time of writing, I have found no high-quality random forest classifiers in the JavaScript ecosystem. Regardless, I'm going to continue writing this section—and even show you one existing library that I believe may have some bugs or problems—in the hopes that by the time you read this everything will be fixed and high-quality random forests will be readily available in JavaScript.</p>
<p>Random forests are a type of <em>ensemble</em> classifier built on top of decision trees. An ensemble classifier comprises several or many individual classifiers that all vote on the prediction. In <a href="94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml" target="_blank">Chapter 2</a>, <em>Data Exploration</em>, we ran the k-means algorithm several times with different random initial conditions in order to avoid getting caught in local optima; that was a rudimentary example of ensemble classification.</p>
<p>A random forest is an ensemble of <em>decision trees</em>. You're probably already familiar with decision trees: in everyday life, decision trees are more commonly called <strong>flowcharts</strong>. In an ML context, decision trees are automatically trained and built by an algorithm, rather than drawn by hand.</p>
<p>First, let's discuss a single decision tree. Decision trees predate random forests, but have historically been of only moderate usefulness to ML. The concept behind a decision tree is the same as a hand-drawn flowchart. When a decision tree evaluates a data point, it'll check each feature in turn: <em>is petal length less than 1.5 centimeters? If so, check the sepal length; if not, check the petal width.</em> Eventually, the decision tree will come to a final leaf or node where no more decisions are possible, and the tree will predict the category of the data point.</p>
<p>Decision trees are automatically trained by using a couple of concepts from information theory, such as information gain, entropy, and a metric called <strong>Gini impurity</strong>. In essence, these techniques are used to determine what the most important branching decisions are. A decision tree wants to be as small and simple as possible, so these techniques are used to determine how best to split the dataset between decisions and when. Should the first branch in the tree check petal width or sepal length? If it checks sepal length, should it split at 2.0 centimeters or 1.5 centimeters? Which comparisons will result in the best splits for the whole dataset? This training is done recursively, and each feature and each training point is evaluated to determine its effect on the whole.</p>
<p>The result is a lightning-fast classifier that is also easy to understand and debug. Unlike a neural network, where the influence of each neuron is highly abstract, and unlike a Bayesian classifier, which requires skill in probability to understand, a decision tree can be rendered as a flowchart and interpreted directly by a researcher.</p>
<p>Unfortunately, a decision tree by itself is not very accurate, they are not robust to changes in training data or noise, they can get trapped in local optima, and there are certain categories of problems that a decision tree cannot handle well (like the classic XOR problem, which will result in a very complex tree).</p>
<p>In the mid-1990s, researchers figured out two new ensemble approaches to decision trees. First, the technique of <em>sample bagging</em> (or <em>bootstrap aggregating</em>) was developed. In this approach, you create a number of decision trees, each based on a totally random subset of the training data (with replacement), and you use the majority vote of all the trees when coming up with a prediction. Bagging works because the variance due to noise is high for a single tree, but for many uncorrelated trees the noise tends to cancel out. Think of concertgoers singing along to their favorite band in an arena—the crowd always sounds in tune because the people singing sharp get canceled out by the people singing flat.</p>
<p>The random forest builds on the idea of bagging by not only randomizing the samples that are given to each tree, but also the <em>features</em> that are given to each tree. As opposed to <em>sample bagging</em>, you could call this <em>feature bagging</em>. If you build a random forest of 50 trees for our <kbd>Iris</kbd> dataset (which has four features and 150 data points), you might expect each tree to have only 100 unique data points and only two of the four features. Like sample bagging, feature bagging serves to decouple each of the decision trees and reduce the overall variance of the ensemble. Feature bagging also serves to identify the most important features, and if you need to save resources you can always remove the least important features from the dataset. When you attempt to predict a data point, each of the 50 trees will submit its vote; some trees will be wildly incorrect, but the ensemble as a whole will make a very good prediction that is robust to noise.</p>
<p>Let's build a random forest and test our <kbd>Iris</kbd> data against it. You should already have the random forest and cross-validation libraries installed in your <kbd>package.json</kbd> file from the SVM section; if not, you should <kbd>yarn add</kbd> both <kbd>ml-cross-validation</kbd> and <kbd>ml-random-forest</kbd>.</p>
<p>At the top of the existing <kbd>index.js</kbd> file for the <kbd>Ch5-SVM</kbd> example, import the appropriate classes:</p>
<pre><span>import</span> {RandomForestClassifier} from <span>'ml-random-forest'</span>;<br/><span>import</span> crossValidation from <span>'ml-cross-validation'</span>;</pre>
<p>You should already have <kbd>labels</kbd> and <kbd>data</kbd> set up from the SVM section. Now, add the following to the bottom of the file, beneath the SVM example:</p>
<pre>console.log(<span>"======================"</span>);<br/>console.log(<span>"Random Forest"</span>);<br/>console.log(<span>"======================"</span>);<br/> <br/><span>const</span> rfOptions = {<br/>  maxFeatures: <span>3</span>,<br/>  replacement: <span>true</span>,<br/>  nEstimators: <span>100</span>,<br/>  useSampleBagging: <span>true<br/></span>};<br/> <br/><span>const</span> rf = <span>new</span> RandomForestClassifier(rfOptions);<br/>rf.train(data, labels);<br/><span>const</span> rfPredictions = rf.predict(data);<br/> <br/><span>const</span> confusionMatrix = crossValidation.kFold(RandomForestClassifier, data, labels, rfOptions, <span>10</span>);<br/><span>const</span> accuracy = confusionMatrix.getAccuracy();<br/> <br/>console.log(<span>"Predictions:"</span>);<br/>console.log(rfPredictions.join(<span>","</span>));<br/>console.log(<span>"</span><span>\n</span><span>Loss for predictions: "</span> + Math.round(loss(labels, rfPredictions) * <span>100</span>) + <span>"%"</span>);<br/>console.log(<span>"Loss for crossvalidated predictions: "</span> + Math.round( (<span>1</span> - accuracy) * <span>100</span>) + <span>"%</span><span>\n</span><span>"</span>);<br/>console.log(confusionMatrix);</pre>
<p>Similar to the SVM example, we're evaluating the random forest in two ways. We first train the forest on the full training data and evaluate its recall, then we use cross-validation to get an idea of its real-world performance. In this example, we're using MLJS's cross-validation and confusion matrix tools to evaluate the classifier's performance.</p>
<p>Run the code with <kbd>yarn start</kbd> and you should see something like the following:</p>
<pre><strong>Random Forest</strong><br/><strong>======================================================================</strong><br/><strong>Predictions:</strong><br/><strong>0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0, 0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,2,0,2,0,2,0,0,2,2,2,1,2,1,2,2,1, 2,2,2,2,2,2,2,2,2,0,1,1,2,2,0,2,2,2,1,1,1,2,2,0,1,0,0,2,0,0,2,2,2,2,2,<br/>2,0,2,2,2,2,2,2,0,2,2,2,2,2,2,2,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,<br/>2,2,2,2,2,2,2,2,2,2</strong><br/> <br/><strong>Loss for predictions: 31%</strong><br/><strong>Loss for crossvalidated predictions: 33%</strong><br/> <br/><strong>ConfusionMatrix {</strong><br/><strong>   labels: [ 0, 1, 2 ],</strong><br/><strong>   matrix: [ [ 43, 6, 1 ], [ 8, 11, 31 ], [ 1, 2, 47 ] ] }</strong></pre>
<p>Unfortunately, the accuracy of this algorithm is very poor. In fact, this performance is atypical of random forests, especially for the <kbd>Iris</kbd> dataset, which should be very easy for an algorithm to interpret.</p>
<p>I wanted to be certain that these poor results were an implementation problem rather than a conceptual one, so I ran the same exact Iris data through a familiar random forest library that I use daily, using the same options and parameters, and I got very different results: <strong>my random forest has a cross-validated loss of 2% only</strong>. Unfortunately, I must blame this poor accuracy not on the random forest itself, but this specific implementation of the algorithm. While I did spend some time looking into the matter, I was not able to quickly identify the issue with this implementation. There is a possibility I am misusing this tool, however, it is more likely that there's a minus sign where there should be a plus (or something similarly silly and disastrous) somewhere in the library. My personal prediction for the performance of a random forest on the <kbd>Iris</kbd> dataset was around 95% accuracy, my familiar random forest library resulted in 98% accuracy, yet this library resulted in only 70% accuracy.</p>
<p>Even worse is the fact that I was unable to find a single random forest library in JavaScript that works for the <kbd>Iris</kbd> dataset. There are a couple of random forest libraries out there, but none that are modern, maintained, and correct. Andrej Karpathy has an abandoned random forest library that seems to work, but it can only handle binary classifications (only 1 and -1 as labels), and a few other random forest libraries are limited in similar ways. The <kbd>MLJS</kbd> random forest library that we used previously is the closest thing to a working, maintained library that I've found, so I hope the issue—whatever it is—will be discovered and resolved by the time you read this.</p>
<p>I do not want you to be discouraged from using random forests. If you are working in languages other than JavaScript, there are many random forest libraries available to you. You should become familiar with them as they'll quickly become your go-to first choice for the majority of classification problems. In terms of JavaScript, while random forests are harder to build from scratch than Bayesian classifiers, they are still quite achievable. If you are able to correctly implement a decision tree, or port one from a different language, building a random forest becomes very easy—the trees do most of the work in the forest.</p>
<p>While JavaScript's ML toolset is advancing all the time, this random forest example perfectly highlights that there's still work to be done. You must proceed cautiously. I started writing this example with the expectation of 95% accuracy or greater, based on my prior experience with random forests. But what if I had no expectations or experience going into it? Would I have just accepted the 70% accuracy from this tool? Would I have convinced myself that a random forest is the wrong tool for the job? Would it have discouraged me from using random forests in the future? Maybe! The ML in the JavaScript ecosystem will have more land mines like this one; look out for them.</p>
<p>Before we finish this chapter, I would like to revisit the confusion matrix we just saw, as this may be a new concept for you. We discussed precision, recall, and accuracy in an earlier chapter. The confusion matrix is the raw data from which these values can be derived for any classification. Here's the confusion matrix from the random forest, again:</p>
<pre><strong>ConfusionMatrix {</strong><br/><strong>   labels: [ 0, 1, 2 ],</strong><br/><strong>   matrix: [ [ 43, 6, 1 ], [ 8, 11, 31 ], [ 1, 2, 47 ] ] }</strong></pre>
<p>If we organize this into a table, it might look like this:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td/>
<td>
<p><strong>Guessed <em>I. setosa</em></strong></p>
</td>
<td>
<p><strong>Guessed <em>I. versicolor</em></strong></p>
</td>
<td>
<p><strong>Guessed <em>I. virginica</em></strong></p>
</td>
</tr>
<tr>
<td>
<p>Actual <em>I. setosa</em></p>
</td>
<td>
<p>43</p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p>Actual <em>I. versicolor</em></p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>11</p>
</td>
<td>
<p>31</p>
</td>
</tr>
<tr>
<td>
<p>Actual <em>I. virginica</em></p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>47</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The confusion matrix is the matrix (or table) of guesses versus actual categories. In a perfect world, you want the confusion matrix to be all zeros except for the diagonal. The confusion matrix tells us that the random forest did a pretty good job of guessing <em>Iris setosa</em> and <em>Iris virginica</em>, but it got most <em>Iris versicolor</em> wrong and incorrectly labeled them as <em>Iris virginica</em>. This is not too surprising, considering the shape of the data; recall that the latter two species overlap quite a bit (however, a random forest still should have been able to resolve this).</p>
<p>The code we wrote for the random forest also printed out the individual predictions for each data point, which looked like this:</p>
<pre><strong>0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,2,0,2,0,2,0,0,2,2,2,1,2,1,2,2,1,2,2,2,2,2,2,2,2,2,0,1,1,2,2,0,2,2,2,1,1,1,2,2,0,1,0,0,2,0,0,2,2,2,2,2,2,0,2,2,2,2,2,2,0,2,2,2,2,2,2,2,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2</strong></pre>
<p>The numbers are not exactly the same as in the confusion matrix, because these predictions came from the fully-trained tree while the confusion matrix came from the cross-validation process; but you can see that they are still similar. The first 50 predictions should all be 0s (for <em>Iris setosa</em>), and they mostly are. The next 50 predictions should be all 1s, but they are primarily 2s; the confusion matrix tells us the same thing (that most <kbd>I. versicolor</kbd> were incorrectly labeled <kbd>I. virginica</kbd>). The last 50 predictions should be all 2s, and are for the most part correct. The confusion matrix is a more compact and intuitive way of looking at the difference between expected and actual guesses, and this is exactly the type of information you'll need when fine-tuning an algorithm.</p>
<p>In short, the random forest is an excellent classifier algorithm that currently has no convincing JavaScript implementation. I encourage you to be part of JavaScript's evolution and build your own random forest, or at least keep this algorithm in mind for the future.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Classification algorithms are a type of supervised learning algorithm whose purpose is to analyze data and assign unseen data points to a pre-existing category, label, or classification. Classification algorithms are a very popular subset of ML, and there are many classification algorithms to choose from.</p>
<p>Specifically, we discussed the simple and intuitive k-nearest-neighbor algorithm, which compares a data point to its neighbors on a graph. We discussed the excellent and very popular Naive Bayes classifier, which is a classic probability-based classifier that dominates the text classification and sentiment analysis problem spaces (though it can be used for many other types of problems). We also discussed the support vector machine, an advanced geometric classifier that works well for non-linearly-separable data. Finally, we discussed the random forest classifier, a robust and powerful ensemble technique that relies on decision trees but unfortunately has only a questionable implementation in JavaScript.</p>
<p>We also discussed cross-validation and the confusion matrix, two powerful techniques for evaluating the accuracy of your models.</p>
<p>In the next chapter, we'll look at association rules, which give us some more predictive power. If someone buys bread and butter from a store, are they more likely to also buy milk, or to buy deli meat? Association rules can help us model and interpret those relationships.</p>


            </article>

            
        </section>
    </body></html>