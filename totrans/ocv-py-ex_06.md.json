["```py\nimport sys\n\nimport cv2\nimport numpy as np\n\ndef draw_matches(img1, keypoints1, img2, keypoints2, matches):\n    rows1, cols1 = img1.shape[:2]\n    rows2, cols2 = img2.shape[:2]\n\n    # Create a new output image that concatenates the two images together\n    output_img = np.zeros((max([rows1,rows2]), cols1+cols2, 3), dtype='uint8')\n    output_img[:rows1, :cols1, :] = np.dstack([img1, img1, img1])\n    output_img[:rows2, cols1:cols1+cols2, :] = np.dstack([img2, img2, img2])\n\n    # Draw connecting lines between matching keypoints\n    for match in matches:\n        # Get the matching keypoints for each of the images\n        img1_idx = match.queryIdx\n        img2_idx = match.trainIdx\n\n        (x1, y1) = keypoints1[img1_idx].pt\n        (x2, y2) = keypoints2[img2_idx].pt\n\n        # Draw a small circle at both co-ordinates and then draw a line\n        radius = 4\n        colour = (0,255,0)   # green\n        thickness = 1\n        cv2.circle(output_img, (int(x1),int(y1)), radius, colour, thickness)\n        cv2.circle(output_img, (int(x2)+cols1,int(y2)), radius, colour, thickness)\n        cv2.line(output_img, (int(x1),int(y1)), (int(x2)+cols1,int(y2)), colour, thickness)\n\n    return output_img\n\nif __name__=='__main__':\n    img1 = cv2.imread(sys.argv[1], 0)   # query image (rotated subregion)\n    img2 = cv2.imread(sys.argv[2], 0)   # train image (full image)\n\n    # Initialize ORB detector\n    orb = cv2.ORB()\n\n    # Extract keypoints and descriptors\n    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n\n    # Create Brute Force matcher object\n    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n\n    # Match descriptors\n    matches = bf.match(descriptors1, descriptors2)\n\n    # Sort them in the order of their distance\n    matches = sorted(matches, key = lambda x:x.distance)\n\n    # Draw first 'n' matches\n    img3 = draw_matches(img1, keypoints1, img2, keypoints2, matches[:30])\n\n    cv2.imshow('Matched keypoints', img3)\n    cv2.waitKey()\n```", "```py\nbf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n```", "```py\nmatches = bf.match(descriptors1, descriptors2)\n```", "```py\noutput_img = np.zeros((max([rows1,rows2]), cols1+cols2, 3), dtype='uint8')\n```", "```py\n(x1, y1) = keypoints1[img1_idx].pt\n(x2, y2) = keypoints2[img2_idx].pt\n```", "```py\nimport sys\nimport argparse\n\nimport cv2\nimport numpy as np\n\ndef argument_parser():\n    parser = argparse.ArgumentParser(description='Stitch two images together')\n    parser.add_argument(\"--query-image\", dest=\"query_image\", required=True,\n            help=\"First image that needs to be stitched\")\n    parser.add_argument(\"--train-image\", dest=\"train_image\", required=True,\n            help=\"Second image that needs to be stitched\")\n    parser.add_argument(\"--min-match-count\", dest=\"min_match_count\", type=int,\n            required=False, default=10, help=\"Minimum number of matches required\")\n    return parser\n\n# Warp img2 to img1 using the homography matrix H\ndef warpImages(img1, img2, H):\n    rows1, cols1 = img1.shape[:2]\n    rows2, cols2 = img2.shape[:2]\n\n    list_of_points_1 = np.float32([[0,0], [0,rows1], [cols1,rows1], [cols1,0]]).reshape(-1,1,2)\n    temp_points = np.float32([[0,0], [0,rows2], [cols2,rows2], [cols2,0]]).reshape(-1,1,2)\n    list_of_points_2 = cv2.perspectiveTransform(temp_points, H)\n    list_of_points = np.concatenate((list_of_points_1, list_of_points_2), axis=0)\n\n    [x_min, y_min] = np.int32(list_of_points.min(axis=0).ravel() - 0.5)\n    [x_max, y_max] = np.int32(list_of_points.max(axis=0).ravel() + 0.5)\n    translation_dist = [-x_min,-y_min]\n    H_translation = np.array([[1, 0, translation_dist[0]], [0, 1, translation_dist[1]], [0,0,1]])\n\n    output_img = cv2.warpPerspective(img2, H_translation.dot(H), (x_max-x_min, y_max-y_min))\n    output_img[translation_dist[1]:rows1+translation_dist[1], translation_dist[0]:cols1+translation_dist[0]] = img1\n\n    return output_img\n\nif __name__=='__main__':\n    args = argument_parser().parse_args()\n    img1 = cv2.imread(args.query_image, 0)\n    img2 = cv2.imread(args.train_image, 0)\n    min_match_count = args.min_match_count\n\n    cv2.imshow('Query image', img1)\n    cv2.imshow('Train image', img2)\n\n    # Initialize the SIFT detector\n    sift = cv2.SIFT()\n\n    # Extract the keypoints and descriptors\n    keypoints1, descriptors1 = sift.detectAndCompute(img1, None)\n    keypoints2, descriptors2 = sift.detectAndCompute(img2, None)\n\n    # Initialize parameters for Flann based matcher\n    FLANN_INDEX_KDTREE = 0\n    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n    search_params = dict(checks = 50)\n\n    # Initialize the Flann based matcher object\n    flann = cv2.FlannBasedMatcher(index_params, search_params)\n\n    # Compute the matches\n    matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n\n    # Store all the good matches as per Lowe's ratio test\n    good_matches = []\n    for m1,m2 in matches:\n        if m1.distance < 0.7*m2.distance:\n            good_matches.append(m1)\n\n    if len(good_matches) > min_match_count:\n        src_pts = np.float32([ keypoints1[good_match.queryIdx].pt for good_match in good_matches ]).reshape(-1,1,2)\n        dst_pts = np.float32([ keypoints2[good_match.trainIdx].pt for good_match in good_matches ]).reshape(-1,1,2)\n\n        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n        result = warpImages(img2, img1, M)\n        cv2.imshow('Stitched output', result)\n\n        cv2.waitKey()\n\n    else:\n        print \"We don't have enough number of matches between the two images.\"\n        print \"Found only %d matches. We need at least %d matches.\" % (len(good_matches), min_match_count)\n```", "```py\nM, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\nresult = warpImages(img2, img1, M)\n```"]