- en: Blend It with Stacking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding stacked generalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing stacked generalization by combining the predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing stacked generalization for marketing campaign outcome prediction
    using H2O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The technical requirements for this chapter remain the same as those we detailed
    in earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Visit the GitHub repository to find the dataset and the code. The datasets and
    code files are arranged according to chapter numbers, and by the name of the topic.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding stacked generalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Stacked generalization is an ensemble of a diverse group of models that introduces
    the concept of a meta-learner. A meta-learner is a second-level machine learning
    algorithm that learns from an optimal combination of base learners:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Stacked generalization is a means of non-linearly combining generalizers to
    make a new generalizer, to try to optimally integrate what each of the original
    generalizers has to say about the learning set. The more each generalizer has
    to say (which isn''t duplicated in what the other generalizers have to say), the
    better the resultant stacked generalization."'
  prefs: []
  type: TYPE_NORMAL
- en: - Wolpert (1992)*,* Stacked Generalization
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps for stacking are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Split your dataset into a training set and a testing set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train several base learners on the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the base learners on the testing set to make predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the predictions as inputs and the actual responses as outputs to train a
    higher-level learner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because the predictions from the base learners are blended together, stacking
    is also referred to as blending.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram gives us a conceptual representation of stacking:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/addebcb4-b1e7-44d6-ad30-af0d0ddefebf.png)'
  prefs: []
  type: TYPE_IMG
- en: It's of significance for stack generalization that the predictions from the
    base learners are not correlated with each other. In order to get uncorrelated
    predictions from the base learners, algorithms that use different approaches internally
    may be used to train the base learners. Stacked generalization is used mainly
    for minimizing the generalization error of the base learners, and can be seen
    as a refined version of cross-validation. It uses a strategy that's more sophisticated
    than cross-validation's **winner-takes-all** approach for combining the predictions
    from the base learners.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing stacked generalization by combining predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll look at how to implement stacked generalization from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will carry out the following steps to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Build three base learners for stacking.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine the predictions from each of the base learners.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the meta-learner using another algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Getting ready...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we use a dataset from the UCI ML Repository on credit card
    defaults. This dataset contains information on default payments, demographic factors,
    credit data, history of payments, and bill statements of credit card clients.
    The data and the data descriptions are provided in the GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading the required libraries and reading our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We set our working folder as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now read our data. We will prefix the DataFrame name with `df_` so that
    we can understand it easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We drop the `ID` column, as this isn''t required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We check the shape of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We notice that the dataset now has 30,000 observations and 24 columns. Let's
    now move on to training our models.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We split our target and feature variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into training, validation, and testing subsets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the dimensions of each subset to ensure that our splits are correct:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the required libraries for the base learners and the meta-learner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Create instances of the base learners and fit the model on our training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the base learners on our validation subset to make predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We have three sets of prediction results from three base learners. We use them
    to create a stacked array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We convert the `final_train_stack` stacked array to a DataFrame and add column
    names to each of the columns. Verify the dimensions and take a look at the first
    few rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following image, we see that the stacked array now has 5,400 observations
    and 4 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56917bad-d1a0-46f2-9aef-12025ce62841.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Train the meta-learner using the stacked array that we created in *Step 8*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the stacked test set with the testing subset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the `final_test_stack` stacked array to a DataFrame and add column
    names to each of the columns. Verify the dimensions and take a look at the first
    few rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that the stacked array now has 3,000 observations and 3 columns in `stacked_test_dataframe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26e39687-b793-40a5-85a8-4b9d6989a2a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Check the accuracy of `base_learner` on our original test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We notice that the accuracy is as follows. Note that based on the sampling
    strategy and hyperparameters, the results may vary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/587145bc-00ad-429b-a493-115eb20bacc3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Use the meta-learner on the stacked test data and check the accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We see the following output returned by the meta-learner applied on the stacked
    test data. This accuracy is higher than the individual base learners:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23f76ead-80a5-407b-a4cd-ccf09120bfc5.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we split our dataset into target and feature sets. In *Step 2*,
    we created our training, validation, and testing subsets. We took a look at the
    dimensions of each of the subset in *Step 3* to verify that the splits were done
    correctly.
  prefs: []
  type: TYPE_NORMAL
- en: We then moved on to building our base learners and the meta-learner. In *Step
    4*, we imported the required libraries for the base learners and the meta-learner.
    For the base learners, we used Gaussian Naive Bayes, KNN, and a decision tree,
    while for the meta-learner we used logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, we fitted the base learners to our train dataset. Single models,
    including Gaussian Naive Bayes, KNN, and a decision tree, are established in the
    level 0 space. We then had three base models.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 6*, we used these three base models on our validation subset to predict
    the target variable. We then had three sets of predictions given by the respective
    base learners.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the base learners will be integrated by logistic regression in the level
    1 space via stacked generalization. In *Step 7*, we stacked the three sets of
    predicted values to create an array. We also stacked the actual target variable
    of our training dataset to the array. We then had four columns in our array: three
    columns from the three sets of predicted values of the base learners and a fourth
    column from the target variable of our training dataset. We called it `final_train_stack` known
    as `stacked_train_dataframe`, and we named the columns according to the algorithm
    used for the base learners. In our case, we used the names `NB_VAL`, `KNN_VAL`,
    and `DT_VAL` since we used Gaussian Naive Bayes, KNN, and a decision tree classifier,
    respectively. Because the base learners are fitted to our validation subset, we
    suffixed the column names with `_VAL` to make them easier to understand.'
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 9*, we built the meta-learner with logistic regression and fitted it
    to our stacked dataset, `stacked_train_dataframe`. Notice that we moved away from
    our original dataset to a stacked dataset, which contains the predicted values
    from our base learners.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 10*, we used the base models on our test subset to get the predicted
    results. We called it `final_test_stack`. In *Step 11*, we converted the `final_test_stack`
    array to a DataFrame called `stacked_test_dataframe`. Note that in our `stacked_test_dataframe`,
    we only had three columns, which held the predicted values returned by the base
    learners applied on our test subset. The three columns were named after the algorithm
    used, suffixed with `_TEST`, so we have `NB_TEST`, `KNN_TEST`, and `DT_TEST` as
    the three columns in `stacked_test_dataframe`.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 12*, we checked the accuracy of the base models on our original test
    subset. The Gaussian Naive Bayes, KNN, and decision tree classifier models gave
    us accuracy ratings of 0.39, 0.69, and 0.73, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 13*, we checked the accuracy that we get by applying the meta-learner
    model on our stacked test data. This gave us an accuracy of 0.77, which we can
    see is higher than the individual base learners. However, bear in mind that simply
    adding more base learners to your stacking algorithm doesn't guarantee that you'll
    get better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a stacking model can be tedious. The `mlxtend` library provides tools
    that simplify building the stacking model. It provides StackingClassifier, which
    is the ensemble-learning meta-classifier for stacking, and it also provides StackingCVClassifier,
    which uses cross-validation to prepare the input for the second level meta-learner
    to prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: You can download the library from [https://pypi.org/project/mlxtend/](https://pypi.org/project/mlxtend/)
    or use the `pip install mlxtend` command to install it. You can find some great
    examples of simple stacked classification and stacked classification with grid
    search at [http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can also take a look at the `ML-Ensemble` library. To find out more about `ML-Ensemble`,
    visit [http://ml-ensemble.com/](http://ml-ensemble.com/). A guide to using `ML-Ensemble`
    is available at [https://bit.ly/2GFsxJN](https://bit.ly/2GFsxJN).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing stacked generalization for campaign outcome prediction using H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: H2O is an open source platform for building machine learning and predictive
    analytics models. The algorithms are written on H2O's distributed map-reduce framework.
    With H2O, the data is distributed across nodes, read in parallel, and stored in
    the memory in a compressed manner. This makes H2O extremely fast.
  prefs: []
  type: TYPE_NORMAL
- en: H2O's stacked ensemble method is an ensemble machine learning algorithm for
    supervised problems that finds the optimal combination of a collection of predictive
    algorithms using stacking. H2O's stacked ensemble supports regression, binary
    classification, and multiclass classification.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we'll take a look at how to use H2O's stacked ensemble to build
    a stacking model. We'll use the bank marketing dataset which is available in the
    Github.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, import the `h2o` library and other modules from H2O:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the `h2o` instance using the `init()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we run the preceding code, the `h2o` instance gets initialized and we
    will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecafcf23-ff57-404b-8924-0eb395f3964f.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have instantiated an `H2O` instance, we move onto reading our dataset
    and building stacking models.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We read our data using the `h2o.import_file()` function. We pass the filename
    to the function as the parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We split our data into training and testing subsets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We check the dimensions of the training and testing subsets to verify that
    the splits are OK:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We take a look at the first few rows to ensure that data is loaded correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We separate the target and predictor column names, which are the `response`
    and `predictors`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We convert the `response` variable to a categorical type with the `asfactor()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We will train our base learners using cross-validation. We set the `nfolds` value
    to `5`.We also set a variable 'encoding' to 'OneHotExplicit'. We will use this
    variable to encode our categorical variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We start training our base learners. We choose the Gradient Boosting Machine
    algorithm to build our first base learner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'For our second base learner, we use a Random Forest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'For our third base learner, we implement a **Generalized Linear Model** (**GLM**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the best-performing base learner on the test set in terms of the `test
    AUC`. Compare this with the `test AUC` of the stacked ensemble model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We train a stacked ensemble using the base learners we built in the preceding
    steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we used the `h2o.import_file()` function to read our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The `h2o.import_file()` function returns an `H2OFrame` instance.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 2*, we split our `H2OFrame` into training and testing subsets. In *Step
    3*, we checked the dimensions of these subsets to verify that our split is adequate
    for our requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we took a look at the first few rows to check if the data is correctly
    loaded. In *Step* *5*, we separated out the column names of our response and predictor
    variables, and in *Step 6*, we converted the response variables into a categorical
    type with the `asfactor()` function.
  prefs: []
  type: TYPE_NORMAL
- en: We defined a variable called `nfolds` in *Step 7*, which we used for cross-validation.
    We have also defined a variable `encoding` which we used in the next steps to
    instruct H2O to use one-hot encoding for categorical variables. In *Step 8* to
    *Step 10*, we built our base learners.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 11*, we trained a Gradient Boosting Machine model. We passed some
    values to a few hyperparameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nfolds`: Number of folds for K-fold cross-validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fold_assignment`: This option specifies the scheme to use for cross-validation
    fold assignment. This option is only applicable if a value for `nfolds` is specified
    and a `fold_column` isn''t specified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distribution`: Specifies the distribution. In our case, since the response
    variable has two classes, we set `distribution` to `"bernoulli"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ntrees`: Number of trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: Denotes the maximum tree depth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_rows`: Fewest allowed observations in a leaf.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learn_rate`: Learning rate takes value from `0.0` to `1.0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that for all base learners, cross-validation folds must be the same and
    `keep_cross_validation_predictions` must be set to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 9*, we trained a random forest base learner using the following hyperparameters:
    `ntrees`, `nfolds`, `fold_assignment`.'
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 10*, we trained our algorithm with a GLM. Note that we have not encoded
    the categorical variables in GLM.
  prefs: []
  type: TYPE_NORMAL
- en: H2O recommends users to allow GLM handle categorical columns, as it can take
    advantage of the categorical column for better performance and efficient memory
    utilization.
  prefs: []
  type: TYPE_NORMAL
- en: 'From H2o.ai: "We strongly recommend avoiding one-hot encoding categorical columns
    with any levels into many binary columns, as this is very inefficient. This is
    especially true for Python users who are used to expanding their categorical variables
    manually for other frameworks".'
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 11*, we generated the test AUC values for each of the base learners
    and printed the best AUC.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 12*, we trained a stacked ensemble model by combining the output of
    the base learners using `H2OStackedEnsembleEstimator`. We used the trained ensemble
    model on our test subset. Note that by default GLM is used as the meta-learner
    for `H2OStackedEnsembleEstimator`. However, we have used deep learning as the
    meta-learner in our example.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have used default hyperparameters values for our meta-learner.
    We can specify the hyperparameter values with `metalearner_params`. The `metalearner_params` option
    allows you to pass in a dictionary/list of hyperparameters to use for the algorithm
    that is used as meta-learner.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning the hyperparameters can deliver better results.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may also assemble a list of models to stack together in different ways.
    In the preceding example, we trained individual models and put them in a list
    to ensemble them. We can also train a grid of models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We specify the random forest hyperparameters for the grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We train the grid using the hyperparameters defined in the preceding code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We train the ensemble using the random forest grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will give the best base-learner test AUC and test the AUC
    from the ensemble model. If the response variable is highly imbalanced, consider
    fine-tuning the following hyperparameters to control oversampling and under-sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '`balance_classes`: This option can be used to balance the class distribution. When
    enabled, H2O will either under-sample the majority classes or oversample the minority
    classes. If this option is enabled, you can also specify a value for the `class_sampling_factors` and `max_after_balance_size`
    options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class_sampling_factors`: By default, sampling factors will be automatically
    computed to obtain class balance during training. This behavior may be changed
    using the `class_sampling_factors` parameter. This option sets an over- or under-sampling
    ratio for each class and requires `balance_classes=true`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_after_balance_size`: In most cases, setting `balance_classes` to true will
    increase the size of the DataFrame. To reduce the DataFrame size, you can use
    the `max_after_balance_size` parameter. This specifies the maximum relative size
    of the training data after balancing the class counts and defaults to `5.0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Take a look at `StackNet`, which was developed by Marios Michailidis as part
    of his PhD. `StackNet` is available under the MIT licence. It's a scalable and
    analytical framework that resembles a feed-forward neural network, and uses Wolpert's
    stacked-generalization concept to improve accuracy in machine learning predictive
    tasks. It uses the notion of meta-learners, in that it uses the predictions of
    some algorithms as features for other algorithms. StackNet can also generalize
    stacking on multiple levels. It is, however, computationally intensive. It was originally
    developed in Java, but a lighter Python version of `StackNet`, named `pystacknet`,
    is now available as well.
  prefs: []
  type: TYPE_NORMAL
- en: Let's think about how StackNet works. In the case of a neural network, the output
    of one layer is inserted as an input to the next layer and an activation function,
    such as sigmoid, tanh, or relu, is applied. Similarly, in the case of StackNet,
    the activation functions can be replaced with any supervised machine learning
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The stacking element can be run on two modes: a normal stacking mode and a
    re-stacking mode. In the case of a normal stacking mode, each layer uses the predictions
    of the previous one. In the case of re-stacking mode, each layer uses the neurons
    and activations of the previous layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample code that uses StackNet would consist of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries (note that we have imported `StackNetClassifier`
    and `StackNetRegressor` from the `pystacknet` library):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We read the data, drop the `ID` column, and check the dimensions of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We separate our target and predictor variables. We also split the data into
    training and testing subsets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the models for the base learners and the meta-learner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We now use `StackNetClassifier` to build the stacking ensemble. However, note
    that we use `restacking=False`, which means that it uses the normal stacking mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: With `restacking=True`, `StackNetClassifier` would use the re-stacking mode
    to build the models.
  prefs: []
  type: TYPE_NORMAL
- en: There are various case studies of StackNet being used in winning competitions
    in Kaggle. An example of how `StackNet` can be used is available at [https://bit.ly/2T7339y](https://bit.ly/2T7339y).
  prefs: []
  type: TYPE_NORMAL
