- en: Blend It with Stacking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Understanding stacked generalization
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing stacked generalization by combining the predictions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing stacked generalization for marketing campaign outcome prediction
    using H2O
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The technical requirements for this chapter remain the same as those we detailed
    in earlier chapters.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Visit the GitHub repository to find the dataset and the code. The datasets and
    code files are arranged according to chapter numbers, and by the name of the topic.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Understanding stacked generalization
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Stacked generalization is an ensemble of a diverse group of models that introduces
    the concept of a meta-learner. A meta-learner is a second-level machine learning
    algorithm that learns from an optimal combination of base learners:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '"Stacked generalization is a means of non-linearly combining generalizers to
    make a new generalizer, to try to optimally integrate what each of the original
    generalizers has to say about the learning set. The more each generalizer has
    to say (which isn''t duplicated in what the other generalizers have to say), the
    better the resultant stacked generalization."'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: - Wolpert (1992)*,* Stacked Generalization
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps for stacking are as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Split your dataset into a training set and a testing set.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train several base learners on the training set.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the base learners on the testing set to make predictions.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the predictions as inputs and the actual responses as outputs to train a
    higher-level learner.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because the predictions from the base learners are blended together, stacking
    is also referred to as blending.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram gives us a conceptual representation of stacking:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/addebcb4-b1e7-44d6-ad30-af0d0ddefebf.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: It's of significance for stack generalization that the predictions from the
    base learners are not correlated with each other. In order to get uncorrelated
    predictions from the base learners, algorithms that use different approaches internally
    may be used to train the base learners. Stacked generalization is used mainly
    for minimizing the generalization error of the base learners, and can be seen
    as a refined version of cross-validation. It uses a strategy that's more sophisticated
    than cross-validation's **winner-takes-all** approach for combining the predictions
    from the base learners.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Implementing stacked generalization by combining predictions
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll look at how to implement stacked generalization from
    scratch.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'We will carry out the following steps to get started:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Build three base learners for stacking.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine the predictions from each of the base learners.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the meta-learner using another algorithm.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Getting ready...
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we use a dataset from the UCI ML Repository on credit card
    defaults. This dataset contains information on default payments, demographic factors,
    credit data, history of payments, and bill statements of credit card clients.
    The data and the data descriptions are provided in the GitHub.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading the required libraries and reading our dataset:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We set our working folder as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s now read our data. We will prefix the DataFrame name with `df_` so that
    we can understand it easily:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We drop the `ID` column, as this isn''t required:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We check the shape of the dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We notice that the dataset now has 30,000 observations and 24 columns. Let's
    now move on to training our models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We split our target and feature variables:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Split the data into training, validation, and testing subsets:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Check the dimensions of each subset to ensure that our splits are correct:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Import the required libraries for the base learners and the meta-learner:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create instances of the base learners and fit the model on our training data:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Use the base learners on our validation subset to make predictions:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We have three sets of prediction results from three base learners. We use them
    to create a stacked array:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We convert the `final_train_stack` stacked array to a DataFrame and add column
    names to each of the columns. Verify the dimensions and take a look at the first
    few rows:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the following image, we see that the stacked array now has 5,400 observations
    and 4 columns:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56917bad-d1a0-46f2-9aef-12025ce62841.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'Train the meta-learner using the stacked array that we created in *Step 8*:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create the stacked test set with the testing subset:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Convert the `final_test_stack` stacked array to a DataFrame and add column
    names to each of the columns. Verify the dimensions and take a look at the first
    few rows:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We see that the stacked array now has 3,000 observations and 3 columns in `stacked_test_dataframe`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26e39687-b793-40a5-85a8-4b9d6989a2a0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: 'Check the accuracy of `base_learner` on our original test data:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We notice that the accuracy is as follows. Note that based on the sampling
    strategy and hyperparameters, the results may vary:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/587145bc-00ad-429b-a493-115eb20bacc3.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: 'Use the meta-learner on the stacked test data and check the accuracy:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We see the following output returned by the meta-learner applied on the stacked
    test data. This accuracy is higher than the individual base learners:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23f76ead-80a5-407b-a4cd-ccf09120bfc5.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: How it works...
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we split our dataset into target and feature sets. In *Step 2*,
    we created our training, validation, and testing subsets. We took a look at the
    dimensions of each of the subset in *Step 3* to verify that the splits were done
    correctly.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: We then moved on to building our base learners and the meta-learner. In *Step
    4*, we imported the required libraries for the base learners and the meta-learner.
    For the base learners, we used Gaussian Naive Bayes, KNN, and a decision tree,
    while for the meta-learner we used logistic regression.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, we fitted the base learners to our train dataset. Single models,
    including Gaussian Naive Bayes, KNN, and a decision tree, are established in the
    level 0 space. We then had three base models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 6*, we used these three base models on our validation subset to predict
    the target variable. We then had three sets of predictions given by the respective
    base learners.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the base learners will be integrated by logistic regression in the level
    1 space via stacked generalization. In *Step 7*, we stacked the three sets of
    predicted values to create an array. We also stacked the actual target variable
    of our training dataset to the array. We then had four columns in our array: three
    columns from the three sets of predicted values of the base learners and a fourth
    column from the target variable of our training dataset. We called it `final_train_stack` known
    as `stacked_train_dataframe`, and we named the columns according to the algorithm
    used for the base learners. In our case, we used the names `NB_VAL`, `KNN_VAL`,
    and `DT_VAL` since we used Gaussian Naive Bayes, KNN, and a decision tree classifier,
    respectively. Because the base learners are fitted to our validation subset, we
    suffixed the column names with `_VAL` to make them easier to understand.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 9*, we built the meta-learner with logistic regression and fitted it
    to our stacked dataset, `stacked_train_dataframe`. Notice that we moved away from
    our original dataset to a stacked dataset, which contains the predicted values
    from our base learners.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 10*, we used the base models on our test subset to get the predicted
    results. We called it `final_test_stack`. In *Step 11*, we converted the `final_test_stack`
    array to a DataFrame called `stacked_test_dataframe`. Note that in our `stacked_test_dataframe`,
    we only had three columns, which held the predicted values returned by the base
    learners applied on our test subset. The three columns were named after the algorithm
    used, suffixed with `_TEST`, so we have `NB_TEST`, `KNN_TEST`, and `DT_TEST` as
    the three columns in `stacked_test_dataframe`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 12*, we checked the accuracy of the base models on our original test
    subset. The Gaussian Naive Bayes, KNN, and decision tree classifier models gave
    us accuracy ratings of 0.39, 0.69, and 0.73, respectively.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 13*, we checked the accuracy that we get by applying the meta-learner
    model on our stacked test data. This gave us an accuracy of 0.77, which we can
    see is higher than the individual base learners. However, bear in mind that simply
    adding more base learners to your stacking algorithm doesn't guarantee that you'll
    get better accuracy.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a stacking model can be tedious. The `mlxtend` library provides tools
    that simplify building the stacking model. It provides StackingClassifier, which
    is the ensemble-learning meta-classifier for stacking, and it also provides StackingCVClassifier,
    which uses cross-validation to prepare the input for the second level meta-learner
    to prevent overfitting.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 创建堆叠模型可能很繁琐。`mlxtend`库提供了简化堆叠模型构建的工具。它提供了`StackingClassifier`，这是堆叠的集成学习元分类器，并且它还提供了`StackingCVClassifier`，它使用交叉验证为第二级元学习器准备输入，以防止过拟合。
- en: You can download the library from [https://pypi.org/project/mlxtend/](https://pypi.org/project/mlxtend/)
    or use the `pip install mlxtend` command to install it. You can find some great
    examples of simple stacked classification and stacked classification with grid
    search at [http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[https://pypi.org/project/mlxtend/](https://pypi.org/project/mlxtend/)下载库，或者使用`pip
    install mlxtend`命令进行安装。您可以在[http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/)找到一些简单的堆叠分类和带有网格搜索的堆叠分类的优秀示例。
- en: See also
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关内容
- en: You can also take a look at the `ML-Ensemble` library. To find out more about `ML-Ensemble`,
    visit [http://ml-ensemble.com/](http://ml-ensemble.com/). A guide to using `ML-Ensemble`
    is available at [https://bit.ly/2GFsxJN](https://bit.ly/2GFsxJN).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以查看`ML-Ensemble`库。要了解更多关于`ML-Ensemble`的信息，请访问[http://ml-ensemble.com/](http://ml-ensemble.com/)。`ML-Ensemble`的使用指南可在[https://bit.ly/2GFsxJN](https://bit.ly/2GFsxJN)找到。
- en: Implementing stacked generalization for campaign outcome prediction using H2O
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用H2O实现针对活动结果预测的堆叠泛化
- en: H2O is an open source platform for building machine learning and predictive
    analytics models. The algorithms are written on H2O's distributed map-reduce framework.
    With H2O, the data is distributed across nodes, read in parallel, and stored in
    the memory in a compressed manner. This makes H2O extremely fast.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: H2O是一个开源平台，用于构建机器学习和预测分析模型。算法是写在H2O的分布式map-reduce框架上的。使用H2O，数据在节点间分布式，并行读取，并以压缩方式存储在内存中。这使得H2O非常快。
- en: H2O's stacked ensemble method is an ensemble machine learning algorithm for
    supervised problems that finds the optimal combination of a collection of predictive
    algorithms using stacking. H2O's stacked ensemble supports regression, binary
    classification, and multiclass classification.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: H2O的堆叠集成方法是一个用于监督问题的集成机器学习算法，它通过堆叠找到一组预测算法的最佳组合。H2O的堆叠集成支持回归、二分类和多分类。
- en: In this example, we'll take a look at how to use H2O's stacked ensemble to build
    a stacking model. We'll use the bank marketing dataset which is available in the
    Github.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将探讨如何使用H2O的堆叠集成构建堆叠模型。我们将使用可在GitHub上找到的银行营销数据集。
- en: Getting ready...
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备中...
- en: 'First, import the `h2o` library and other modules from H2O:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入`h2o`库和其他H2O模块：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Initialize the `h2o` instance using the `init()` function:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`init()`函数初始化`h2o`实例：
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once we run the preceding code, the `h2o` instance gets initialized and we
    will see the following output:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行前面的代码，`h2o`实例将被初始化，我们将看到以下输出：
- en: '![](img/ecafcf23-ff57-404b-8924-0eb395f3964f.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ecafcf23-ff57-404b-8924-0eb395f3964f.png)'
- en: Now that we have instantiated an `H2O` instance, we move onto reading our dataset
    and building stacking models.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实例化了`H2O`实例，我们将继续读取我们的数据集并构建堆叠模型。
- en: How to do it...
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We read our data using the `h2o.import_file()` function. We pass the filename
    to the function as the parameter:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`h2o.import_file()`函数读取数据。我们将文件名作为参数传递给函数：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We split our data into training and testing subsets:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据分为训练集和测试集：
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We check the dimensions of the training and testing subsets to verify that
    the splits are OK:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们检查训练集和测试集的维度，以验证分割是否正确：
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We take a look at the first few rows to ensure that data is loaded correctly:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们查看前几行以确保数据正确加载：
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We separate the target and predictor column names, which are the `response`
    and `predictors`, respectively:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将目标列和预测列的名称分别命名为`response`和`predictors`：
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We convert the `response` variable to a categorical type with the `asfactor()`
    function:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`asfactor()`函数将`response`变量转换为分类类型：
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We will train our base learners using cross-validation. We set the `nfolds` value
    to `5`.We also set a variable 'encoding' to 'OneHotExplicit'. We will use this
    variable to encode our categorical variables.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We start training our base learners. We choose the Gradient Boosting Machine
    algorithm to build our first base learner:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'For our second base learner, we use a Random Forest:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For our third base learner, we implement a **Generalized Linear Model** (**GLM**):'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Get the best-performing base learner on the test set in terms of the `test
    AUC`. Compare this with the `test AUC` of the stacked ensemble model:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We train a stacked ensemble using the base learners we built in the preceding
    steps:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: How it works...
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we used the `h2o.import_file()` function to read our dataset.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: The `h2o.import_file()` function returns an `H2OFrame` instance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 2*, we split our `H2OFrame` into training and testing subsets. In *Step
    3*, we checked the dimensions of these subsets to verify that our split is adequate
    for our requirements.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we took a look at the first few rows to check if the data is correctly
    loaded. In *Step* *5*, we separated out the column names of our response and predictor
    variables, and in *Step 6*, we converted the response variables into a categorical
    type with the `asfactor()` function.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: We defined a variable called `nfolds` in *Step 7*, which we used for cross-validation.
    We have also defined a variable `encoding` which we used in the next steps to
    instruct H2O to use one-hot encoding for categorical variables. In *Step 8* to
    *Step 10*, we built our base learners.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 11*, we trained a Gradient Boosting Machine model. We passed some
    values to a few hyperparameters as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '`nfolds`: Number of folds for K-fold cross-validation.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fold_assignment`: This option specifies the scheme to use for cross-validation
    fold assignment. This option is only applicable if a value for `nfolds` is specified
    and a `fold_column` isn''t specified.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distribution`: Specifies the distribution. In our case, since the response
    variable has two classes, we set `distribution` to `"bernoulli"`.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ntrees`: Number of trees.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: Denotes the maximum tree depth.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_rows`: Fewest allowed observations in a leaf.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learn_rate`: Learning rate takes value from `0.0` to `1.0`.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that for all base learners, cross-validation folds must be the same and
    `keep_cross_validation_predictions` must be set to `True`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 9*, we trained a random forest base learner using the following hyperparameters:
    `ntrees`, `nfolds`, `fold_assignment`.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 10*, we trained our algorithm with a GLM. Note that we have not encoded
    the categorical variables in GLM.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: H2O recommends users to allow GLM handle categorical columns, as it can take
    advantage of the categorical column for better performance and efficient memory
    utilization.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'From H2o.ai: "We strongly recommend avoiding one-hot encoding categorical columns
    with any levels into many binary columns, as this is very inefficient. This is
    especially true for Python users who are used to expanding their categorical variables
    manually for other frameworks".'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 来自H2o.ai的建议："我们强烈建议避免将具有任何级别的分类列进行one-hot编码成多个二进制列，因为这非常低效。这对于习惯手动扩展其分类变量以适应其他框架的Python用户来说尤其如此"。
- en: In *Step 11*, we generated the test AUC values for each of the base learners
    and printed the best AUC.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤11*中，我们为每个基础学习者生成了测试AUC值，并打印了最佳的AUC。
- en: In *Step 12*, we trained a stacked ensemble model by combining the output of
    the base learners using `H2OStackedEnsembleEstimator`. We used the trained ensemble
    model on our test subset. Note that by default GLM is used as the meta-learner
    for `H2OStackedEnsembleEstimator`. However, we have used deep learning as the
    meta-learner in our example.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤12*中，我们通过使用`H2OStackedEnsembleEstimator`结合基础学习者的输出，训练了一个堆叠集成模型。我们在测试子集上使用了训练好的集成模型。请注意，默认情况下，GLM被用作`H2OStackedEnsembleEstimator`的元学习器。然而，在我们的示例中，我们使用了深度学习作为元学习器。
- en: Note that we have used default hyperparameters values for our meta-learner.
    We can specify the hyperparameter values with `metalearner_params`. The `metalearner_params` option
    allows you to pass in a dictionary/list of hyperparameters to use for the algorithm
    that is used as meta-learner.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已经为我们的元学习器使用了默认的超参数值。我们可以使用`metalearner_params`指定超参数值。`metalearner_params`选项允许您传递一个字典/超参数列表，用于作为元学习器的算法。
- en: Fine-tuning the hyperparameters can deliver better results.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 微调超参数可以带来更好的结果。
- en: There's more...
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You may also assemble a list of models to stack together in different ways.
    In the preceding example, we trained individual models and put them in a list
    to ensemble them. We can also train a grid of models:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以以不同的方式组装一个模型列表以堆叠。在前面的示例中，我们训练了单个模型并将它们放入列表中进行集成。我们也可以训练一个模型网格：
- en: 'We specify the random forest hyperparameters for the grid:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定网格的随机森林超参数：
- en: '[PRE32]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We train the grid using the hyperparameters defined in the preceding code:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用前面代码中定义的超参数进行网格训练：
- en: '[PRE33]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We train the ensemble using the random forest grid:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用随机森林网格进行集成训练：
- en: '[PRE34]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding code will give the best base-learner test AUC and test the AUC
    from the ensemble model. If the response variable is highly imbalanced, consider
    fine-tuning the following hyperparameters to control oversampling and under-sampling:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将给出最佳基础学习器测试AUC，并测试集成模型的AUC。如果响应变量高度不平衡，考虑微调以下超参数以控制过采样和欠采样：
- en: '`balance_classes`: This option can be used to balance the class distribution. When
    enabled, H2O will either under-sample the majority classes or oversample the minority
    classes. If this option is enabled, you can also specify a value for the `class_sampling_factors` and `max_after_balance_size`
    options.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance_classes`：此选项可用于平衡类分布。当启用时，H2O将减少多数类的样本量或增加少数类的样本量。如果此选项被启用，您还可以指定`class_sampling_factors`和`max_after_balance_size`选项的值。'
- en: '`class_sampling_factors`: By default, sampling factors will be automatically
    computed to obtain class balance during training. This behavior may be changed
    using the `class_sampling_factors` parameter. This option sets an over- or under-sampling
    ratio for each class and requires `balance_classes=true`.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_sampling_factors`：默认情况下，采样因子将在训练期间自动计算以获得类平衡。此行为可以通过`class_sampling_factors`参数进行更改。此选项为每个类设置过采样或欠采样比率，并需要`balance_classes=true`。'
- en: '`max_after_balance_size`: In most cases, setting `balance_classes` to true will
    increase the size of the DataFrame. To reduce the DataFrame size, you can use
    the `max_after_balance_size` parameter. This specifies the maximum relative size
    of the training data after balancing the class counts and defaults to `5.0`.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_after_balance_size`：在大多数情况下，将`balance_classes`设置为true会增加DataFrame的大小。为了减少DataFrame的大小，您可以使用`max_after_balance_size`参数。此参数指定平衡类计数后的训练数据的最大相对大小，默认为`5.0`。'
- en: See also
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: Take a look at `StackNet`, which was developed by Marios Michailidis as part
    of his PhD. `StackNet` is available under the MIT licence. It's a scalable and
    analytical framework that resembles a feed-forward neural network, and uses Wolpert's
    stacked-generalization concept to improve accuracy in machine learning predictive
    tasks. It uses the notion of meta-learners, in that it uses the predictions of
    some algorithms as features for other algorithms. StackNet can also generalize
    stacking on multiple levels. It is, however, computationally intensive. It was originally
    developed in Java, but a lighter Python version of `StackNet`, named `pystacknet`,
    is now available as well.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 看一看由Marios Michailidis在其博士论文中开发的`StackNet`。`StackNet`在MIT许可下可用。它是一个可扩展的分析框架，类似于前馈神经网络，并使用Wolpert的堆叠泛化概念来提高机器学习预测任务的准确性。它使用元学习者的概念，即它使用某些算法的预测作为其他算法的特征。StackNet还可以在多个级别上进行泛化堆叠。然而，它计算密集。最初是用Java开发的，但现在也提供了一种较轻的Python版本，名为`pystacknet`。
- en: Let's think about how StackNet works. In the case of a neural network, the output
    of one layer is inserted as an input to the next layer and an activation function,
    such as sigmoid, tanh, or relu, is applied. Similarly, in the case of StackNet,
    the activation functions can be replaced with any supervised machine learning
    algorithm.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下StackNet是如何工作的。在神经网络的情况下，某一层的输出被插入到下一层作为输入，并应用激活函数，如sigmoid、tanh或relu。同样，在StackNet的情况下，激活函数可以被任何监督机器学习算法所替代。
- en: 'The stacking element can be run on two modes: a normal stacking mode and a
    re-stacking mode. In the case of a normal stacking mode, each layer uses the predictions
    of the previous one. In the case of re-stacking mode, each layer uses the neurons
    and activations of the previous layers.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠元素可以在两种模式下运行：正常堆叠模式和重新堆叠模式。在正常堆叠模式下，每一层使用前一层的结果进行预测。在重新堆叠模式下，每一层使用前几层的神经元和激活。
- en: 'Sample code that uses StackNet would consist of the following steps:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用StackNet的示例代码将包括以下步骤：
- en: 'Import the required libraries (note that we have imported `StackNetClassifier`
    and `StackNetRegressor` from the `pystacknet` library):'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库（请注意，我们已经从`pystacknet`库中导入了`StackNetClassifier`和`StackNetRegressor`）：
- en: '[PRE35]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We read the data, drop the `ID` column, and check the dimensions of the dataset:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取数据，删除`ID`列，并检查数据集的维度：
- en: '[PRE36]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We separate our target and predictor variables. We also split the data into
    training and testing subsets:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将目标变量和预测变量分开。同时，我们将数据分为训练集和测试集：
- en: '[PRE37]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We define the models for the base learners and the meta-learner:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了基学习器和元学习器的模型：
- en: '[PRE38]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We now use `StackNetClassifier` to build the stacking ensemble. However, note
    that we use `restacking=False`, which means that it uses the normal stacking mode:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用`StackNetClassifier`构建堆叠集成。但是请注意，我们使用`restacking=False`，这意味着它使用正常堆叠模式：
- en: '[PRE39]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: With `restacking=True`, `StackNetClassifier` would use the re-stacking mode
    to build the models.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`restacking=True`时，`StackNetClassifier`将使用重新堆叠模式来构建模型。
- en: There are various case studies of StackNet being used in winning competitions
    in Kaggle. An example of how `StackNet` can be used is available at [https://bit.ly/2T7339y](https://bit.ly/2T7339y).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle竞赛中，StackNet被用于赢得各种案例研究。关于如何使用`StackNet`的示例可以在[https://bit.ly/2T7339y](https://bit.ly/2T7339y)找到。
