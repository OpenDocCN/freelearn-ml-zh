["```py\nMat image, image_gray, image_hist;\nVideoCapture webcam(0);\nWebcam >> image;\nresize(image, image, Size(640,360));\ncvtColor(image, image_gray, COLOR_BGR2GRAY);\nequalizeHist(image_gray, image_hist);\n```", "```py\nCascadeClassifier cascade('path/to/face/model/');\nvector<Rect> faces;\ncascade.detectMultiScale(image_hist, faces, 1.1, 3);\n```", "```py\nfor(int i=0; i<faces.size(); i++){\n   Rect current_face = faces[i];\n   Mat face_region = image( current_face ).clone();\n   // do something with this image here\n}\n```", "```py\nCascadeClassifier eye('../haarcascades/haarcascade_eye.xml');\nvector<Rect> eyes_found;\neye.detectMultiScale(face_region, eyesfound, 1.1, 3);\n// Now let us assume only two eyes (both eyes and no FP) are found\ndouble angle = atan( double(eyes_found[0].y - eyes_found[1].y) / double(eyes_found[0].x - eyes_found[1].x) ) * 180 / CV_PI;\nPoint2f pt(image.cols/2, image.rows/2);\nMat rotation = getRotationMatrix2D(pt, angle, 1.0);\nMat rotated_face;\nwarpAffine(face_region, rotated_face, rotation, Size(face_region.cols, face_region.rows));\n```", "```py\nPtr<BasicFaceRecognizer> face_model = createEigenFaceRecognizer();\n```", "```py\n// train a face recognition model\nvector<Mat> faces;\nvector<int> labels;\nMat test_image = imread(\"/path/to/test/image.png\");\n// do not forget to fill the data before training\nface_model.train(faces, labels);\n// when you want to predict on a new image given, using the model\nint predict = modelàpredict(test_image);\n```", "```py\nint predict = -1; // a label that is unexisting for starters\ndouble confidence = 0.0;\nmodelàpredict(test_image, predict, confidence);\n```", "```py\n// Getting the actual eigenvalues (reprojection values on the eigenvectors for each sample)\nMat eigenvalues = face_model->getEigenValues();\n// Get the actual eigenvectors used for projection and dimensionality reduction\nMat eigenvectors = face_model->getEigenVectors();\n// Get the mean eigenface\nMat mean = face_model->getMean();\n```", "```py\n// Again make sure that the data is available\nvector<Mat> faces;\nvector<int> labels;\nMat test_image = imread(\"/path/to/test/image.png\");\n// Now train the model, again overload functions are available\nPtr<BasicFaceRecognizer> face_model = createFisherFaceRecognizer();\nface_modelàtrain(faces, labels);\n// Now predict the outcome of a sample test image\nint predict = face_modelàpredict(test_image);\n```", "```py\n// Get the eigenvectors or fishervectors and the mean face\nMat mean = face_modelàgetMean();\nMat vectors = face_modelàgetEigenValues();\n// Then apply the partial reconstruction\n// Do specify at which stage you want to look\nint component_index = 5;\nMat current_slice = vectors.col(component_index);\n// Images[0] is the first image and used for reshape properties\nMat projection = cv::LDA::subspaceProject(current_slice, mean, images[0].reshape(1,1));\nMat reconstruction = cv::LDA::subspaceReconstruct(current_slice, mean, projection);\n// Then normalize and reshape the result if you want to visualize, as explained on the web page which I referred to.\n```", "```py\n// Again make sure that the data is available\nvector<Mat> faces;\nvector<int> labels;\nMat test_image = imread(\"/path/to/test/image.png\");\n// Now train the model, again overload functions are available\nPtr<LBPHFaceRecognizer> face_model = createLBPHFaceRecognizer();\nface_modelàtrain(faces, labels);\n// Now predict the outcome of a sample test image\nint predict = face_modelàpredict(test_image);\n```", "```py\n// functionality createLBPHFaceRecognizer(radius, neighbors, grid_X, grid_Y, treshold)\ncv::createLBPHFaceRecognizer(1,8,8,8,123.0);\n// Getting the properties can be done using the getInt function.\nint radius = model->getRadius();\nint neighbors = model->getNeighbors();\nint grid_x = model->getGridX();\nint grid_y = model->getGridY();\ndouble threshold = model->getThreshold();\n```", "```py\n// Start by reading in an image\nMat input = imread(\"/data/fingerprints/image1.png\", IMREAD_GRAYSCALE);\n// Binarize the image, through local thresholding\nMat input_binary;\nthreshold(input, input_binary, 0, 255, THRESH_BINARY | THRESH_OTSU);\n```", "```py\n#include <opencv2/imgproc.hpp>\n#include <opencv2/highgui.hpp>\n\nusing namespace std;\nusing namespace cv;\n\n// Perform a single thinning iteration, which is repeated until the skeletization is finalized\nvoid thinningIteration(Mat& im, int iter)\n{\n    Mat marker = Mat::zeros(im.size(), CV_8UC1);\n    for (int i = 1; i < im.rows-1; i++)\n    {\n        for (int j = 1; j < im.cols-1; j++)\n        {\n            uchar p2 = im.at<uchar>(i-1, j);\n            uchar p3 = im.at<uchar>(i-1, j+1);\n            uchar p4 = im.at<uchar>(i, j+1);\n            uchar p5 = im.at<uchar>(i+1, j+1);\n            uchar p6 = im.at<uchar>(i+1, j);\n            uchar p7 = im.at<uchar>(i+1, j-1);\n            uchar p8 = im.at<uchar>(i, j-1);\n            uchar p9 = im.at<uchar>(i-1, j-1);\n\n            int A  = (p2 == 0 && p3 == 1) + (p3 == 0 && p4 == 1) + \n                     (p4 == 0 && p5 == 1) + (p5 == 0 && p6 == 1) + \n                     (p6 == 0 && p7 == 1) + (p7 == 0 && p8 == 1) +\n                     (p8 == 0 && p9 == 1) + (p9 == 0 && p2 == 1);\n            int B  = p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9;\n            int m1 = iter == 0 ? (p2 * p4 * p6) : (p2 * p4 * p8);\n            int m2 = iter == 0 ? (p4 * p6 * p8) : (p2 * p6 * p8);\n\n            if (A == 1 && (B >= 2 && B <= 6) && m1 == 0 && m2 == 0)\n                marker.at<uchar>(i,j) = 1;\n        }\n    }\n\n    im &= ~marker;\n}\n\n// Function for thinning any given binary image within the range of 0-255\\. If not you should first make sure that your image has this range preset and configured!\nvoid thinning(Mat& im)\n{\n    // Enforce the range to be in between 0 - 255  \n    im /= 255;\n\n    Mat prev = Mat::zeros(im.size(), CV_8UC1);\n    Mat diff;\n\n    do {\n        thinningIteration(im, 0);\n        thinningIteration(im, 1);\n        absdiff(im, prev, diff);\n        im.copyTo(prev);\n    } \n    while (countNonZero(diff) > 0);\n\n    im *= 255;\n}\n```", "```py\n// Apply thinning algorithm\nMat input_thinned = input_binary.clone();\nthinning(input_thinned);\n```", "```py\nMat harris_corners, harris_normalised;\nharris_corners = Mat::zeros(input_thinned.size(), CV_32FC1);\ncornerHarris(input_thinned, harris_corners, 2, 3, 0.04, BORDER_DEFAULT);\nnormalize(harris_corners, harris_normalised, 0, 255, NORM_MINMAX, CV_32FC1, Mat());\n```", "```py\nfloat threshold = 125.0;\nvector<KeyPoint> keypoints;\nMat rescaled;\nconvertScaleAbs(harris_normalised, rescaled);\nMat harris_c(rescaled.rows, rescaled.cols, CV_8UC3);\nMat in[] = { rescaled, rescaled, rescaled };\nint from_to[] = { 0,0, 1,1, 2,2 };\nmixChannels( in, 3, &harris_c, 1, from_to, 3 );\nfor(int x=0; x<harris_normalised.cols; x++){\n   for(int y=0; y<harris_normalised.rows; y++){\n          if ( (int)harris_normalised.at<float>(y, x) > threshold ){\n             // Draw or store the keypoint location here, just like\n             //you decide. In our case we will store the location of \n             // the keypoint\n             circle(harris_c, Point(x, y), 5, Scalar(0,255,0), 1);\n             circle(harris_c, Point(x, y), 1, Scalar(0,0,255), 1);\n             keypoints.push_back( KeyPoint (x, y, 1) );\n          }\n       }\n    }\n```", "```py\nPtr<Feature2D> orb_descriptor = ORB::create();\nMat descriptors;\norb_descriptor->compute(input_thinned, keypoints, descriptors);\n```", "```py\n// Imagine we have a vector of single entry descriptors as a database\n// We will still need to fill those once we compare everything, by using the code snippets above\nvector<Mat> database_descriptors;\nMat current_descriptors;\n// Create the matcher interface\nPtr<DescriptorMatcher> matcher = DescriptorMatcher::create(\"BruteForce-Hamming\");\n// Now loop over the database and start the matching\nvector< vector< DMatch > > all_matches;\nfor(int entry=0; i<database_descriptors.size();entry++){\n   vector< DMatch > matches;\n   matcheràmatch(database_descriptors[entry], current_descriptors, matches);\n   all_matches.push_back(matches);\n}\n```", "```py\n// Read in image and perform contour detection\nMat original = imread(\"path/to/eye/image.png\", IMREAD_GRAYSCALE);\nMat mask_pupil;\ninRange(original, Scalar(30,30,30), Scalar(80,80,80), mask_pupil);\nvector< vector<Point> > contours;\nfindContours(mask_pupil.clone(), contours, RETR_EXTERNAL, CHAIN_APPROX_NONE);\n// Calculate all the corresponding areas which are larger than\nvector< vector<Point> > filtered;\nfor(int i=0; i<contours.size(); i++){\n   double area = contourArea(contours[i]);\n   // Remove noisy regions\n   if(area > 50.0){\n      filtered.push_back(contours[i]);\n   }\n}\n// Now make a last check, if there are still multiple contours left, take the one that has a center closest to the image center\nvector<Point> final_contour=filtered[0];\nif(filtered.size() > 1){\n   double distance = 5000;\n   int index = -1;\n   Point2f orig_center(original.cols/2, original.rows/2); \n   for(int i=0; i<filtered.size(); i++){\n      Moments temp = moments(filtered[i]);\n      Point2f current_center((temp.m10/temp.m00), (temp.m01/temp.m00));\n      // Find the Euclidean distance between both positions\n      double dist = norm(Mat(orig_center), Mat(current_center));\n      if(dist < distance){\n         distance = dist;\n         index = i;\n      }   \n   }\n   final_contour = filtered[index];\n}\n// Now finally make the black contoured image;\nvector< vector<Point> > draw;\ndraw.push_back(final_contour);\nMat blacked_pupil = original.clone();\ndrawContours(blacked_pupil, draw, -1, Scalar(0,0,0), FILLED);\n```", "```py\nNoteSometimes, the Hough circle detection will not yield a single circle. This is not the case with the proposed database, but if you encounter this, then looking at other techniques like the Laplacian of Gaussians should help you to find and reconstruct the iris region.\n```", "```py\n// Make sure that the input image is gray, we took care of that while reading in the original image and making sure that the blacked pupil image is a clone of that.\n// Apply a canny edge filter to look for borders\n// Then clean it a bit by adding a smoothing filter, reducing noise\nMat preprocessed;\nCanny(blacked_pupil, blacked_pupil, 5, 70);\nGaussianBlur(blacked_pupil, preprocessed, Size(7,7));\n// Now run a set of HoughCircle detections with different parameters\n// We increase the second accumulator value until a single circle is left and take that one for granted\nint i = 80;\nvector<Point3f> found_circle;\nwhile (i < 151){\n   vector< vector<Point3f> > storage;\n   // If you use other data than the database provided, tweaking of these parameters will be necessary\n   HoughCircles(preprocessed, storage, HOUGH_GRADIENT, 2, 100.0, 30, i, 100, 140);\n   if(storage.size() == 1){\n      found_circle = storage[0];\n      break;\n   }\n   i++;\n}\n// Now draw the outer circle of the iris\nint radius = found_circle[2];\nMat mask = Mat::zeros(blacked_pupil.rows, blacked_pupil.cols, CV_8UC1);\n// The centroid value here must be the same as the one of the inner pupil so we reuse it back here\nMoments temp = Moments(final_contour);\nPoint2f centroid((temp.m10/temp.m00), (temp.m01/temp.m00));\nCircle(mask, centroid, radius, Scalar(255,255,255), FILLED);\nbitwise_not(mask, mask);\nMat final_result;\nsubtract(blacked_pupil, blacked_pupil.clone(), final_result, mask);\n// Visualize the final result\nimshow(\"final blacked iris region\", final_result);\n```", "```py\n// Lets first crop the final iris region from the image\nint x = int(centroid[0] - radius);\nint y = int(centroid[1] - radius);\nint w = int(radius * 2);\nint h = w;\nMat cropped_region = final_result( Rect(x,y,w,h) ).clone();\n// Now perform the unwrapping\n// This is done by the logpolar function who does Logpolar to Cartesian coordinates, so that it can get unwrapped properly\nMat unwrapped;\nint center = (float(cropped_region.cols/2.0), float(cropped_region.cols /2.0));\nLogPolar(image, unwrapped, c, 60.0, INTER_LINEAR +  WARP_FILL_OUTLIERS);\nimshow(\"unwrapped image\", unwrapped); waitKey(0);\n```", "```py\n// Try using the facerecognizer interface for these irises\n// Choice of using LBPH --> local and good for finding texture\nPtr<LBPHFaceRecognizer> iris_model = createLBPHFaceRecognizer();\n// Train the facerecognizer\niris_model->train(train_iris, train_labels);\n// Loop over test images and match their labels\nint total_correct = 0, total_wrong = 0;\nfor(int i=0; i<test_iris.size(); i ++){\n       int predict = iris_model->predict(test_iris[i]);\n       if(predict == test_labels[i]){\n            total_correct++;\n       }else{\n            total_wrong++;\n       }\n}\n```"]