- en: Chapter 11. Reconstructing 3D Scenes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Calibrating a camera
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recovering camera pose
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reconstructing a 3D scene from calibrated cameras
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing depth from stereo image
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned in the previous chapter how a camera captures a 3D scene by projecting
    light rays on a 2D sensor plane. The image produced is an accurate representation
    of what the scene looks like from a particular point of view, at the instant the
    image was captured. However, by its nature, the process of image formation eliminates
    all information concerning the depth of the represented scene elements. This chapter
    will teach how, under specific conditions, the 3D structure of the scene and the
    3D pose of the cameras that captured it, can be recovered. We will see how a good
    understanding of projective geometry concepts allows us to devise methods that
    enable 3D reconstruction. We will therefore revisit the principle of image formation
    introduced in the previous chapter; in particular, we will now take into consideration
    that our image is composed of pixels.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Digital image formation
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now redraw a new version of the figure shown in Chapter 10 , *Estimating
    Projective Relations in Images*, describing the pin-hole camera model. More specifically,
    we want to demonstrate the relation between a point in 3D at position `(X,Y,Z)`
    and its image `(x,y)` on a camera specified in pixel coordinates:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![Digital image formation](img/image_11_001.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: 'Notice the changes that have been made to the original figure. First, we added
    a reference frame that we positioned at the center of the projection. Second,
    we have the `Y`-axis pointing downward to get a coordinate system compatible with
    the usual convention that places the image origin in the upper-left corner of
    the image. Finally, we also identified a special point on the image plane: considering
    the line coming from the focal point that is orthogonal to the image plane, the
    point `(u0,v0)` is the pixel position at which this line pierces the image plane.
    This point is called the **principal point**. It could be logical to assume that
    this principal point is at the center of the image plane, but in practice, this
    one might be off by a few pixels, depending on the precision with which the camera
    has been manufactured.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we learned that the essential parameters of a camera
    in the pin-hole model are its focal length and the size of the image plane (which
    defines the field of view of the camera). In addition, since we are dealing with
    digital images, the number of pixels on the image plane (its resolution) is another
    important characteristic of a camera. We also learned previously that a 3D point
    `(X,Y,Z)` will be projected onto the image plane at `(fX/Z,fY/Z)`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we want to translate this coordinate into pixels, we need to divide
    the 2D image position by the pixel width (`px`) and height (`py`), respectively.
    We notice that by dividing the focal length given in world units (generally given
    in millimeters) by `px,` we obtain the focal length expressed in (horizontal)
    pixels. Let''s define this term, then, as `fx`. Similarly, `fy =f/py` is defined
    as the focal length expressed in vertical pixel units. The complete projective
    equation is therefore as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![Digital image formation](img/B05388_11_15.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: Recall that `(u0,v0)` is the principal point that is added to the result in
    order to move the origin to the upper-left corner of the image. Note also that
    the physical size of a pixel can be obtained by dividing the size of the image
    sensor (generally in millimeters) by the number of pixels (horizontally or vertically).
    In modern sensors, pixels are generally square, that is, they have the same horizontal
    and vertical size.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding equations can be rewritten in matrix form as we did in Chapter
    10 , *Estimating Projective Relations in Images*. Here is the complete projective
    equation in its most general form:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Digital image formation](img/B05388_11_16.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Calibrating a camera
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Camera calibration is the process by which the different camera parameters (that
    is, the ones appearing in the projective equation) are obtained. One can obviously
    use the specifications provided by the camera manufacturer, but for some tasks,
    such as 3D reconstruction, these specifications are not accurate enough. By undertaking
    an appropriate camera calibration step, accurate calibration information can be
    obtained.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: An active camera calibration procedure will proceed by showing known patterns
    to the camera and analyzing the obtained images. An optimization process will
    then determine the optimal parameter values that explain the observations. This
    is a complex process that has been made easy by the availability of OpenCV calibration
    functions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To calibrate a camera, the idea is to show it a set of scene points for which
    their 3D positions are known. Then, you need to observe where these points project
    on the image. With the knowledge of a sufficient number of 3D points and associated
    2D image points, the exact camera parameters can be inferred from the projective
    equation. Obviously, for accurate results, we need to observe as many points as
    possible. One way to achieve this would be to take one picture of a scene with
    many known 3D points, but in practice, this is rarely feasible. A more convenient
    way is to take several images of a set of some 3D points from different viewpoints.
    This approach is simpler but requires you to compute the position of each camera
    view in addition to the computation of the internal camera parameters, which,
    fortunately, is feasible.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV proposes that you use a chessboard pattern to generate the set of 3D
    scene points required for calibration. This pattern creates points at the corners
    of each square, and since this pattern is flat, we can freely assume that the
    board is located at `Z=0`, with the `X` and `Y` axes well-aligned with the grid.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the calibration process simply consists of showing the chessboard
    pattern to the camera from different viewpoints. Here is one example of a calibration
    pattern image made of `7x5` inner corners as captured during the calibration step:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/B05388_11_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: 'The good thing is that OpenCV has a function that automatically detects the
    corners of this chessboard pattern. You simply provide an image and the size of
    the chessboard used (the number of horizontal and vertical inner corner points).
    The function will return the position of these chessboard corners on the image.
    If the function fails to find the pattern, then it simply returns `false`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output parameter, `imageCorners`, will simply contain the pixel coordinates
    of the detected inner corners of the shown pattern. Note that this function accepts
    additional parameters if you need to tune the algorithm, which is not discussed
    here. There is also a special function that draws the detected corners on the chessboard
    image, with lines connecting them in a sequence:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following image is obtained:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/B05388_11_03.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: The lines that connect the points show the order in which the points are listed
    in the vector of detected image points. To perform a calibration, we now need
    to specify the corresponding 3D points. You can specify these points in the units
    of your choice (for example, in centimeters or in inches); however, the simplest
    thing to do is to assume that each square represents one unit. In that case, the
    coordinates of the first point would be `(0,0,0)` (assuming that the board is
    located at a depth of `Z=0`), the coordinates of the second point would be `(1,0,0),`
    and so on, the last point being located at `(6,4,0)`. There is a total of `35`
    points in this pattern, which is too small to obtain an accurate calibration.
    To get more points, you need to show more images of the same calibration pattern
    from various points of view. To do so, you can either move the pattern in front
    of the camera or move the camera around the board; from a mathematical point of
    view, this is completely equivalent. The OpenCV calibration function assumes that
    the reference frame is fixed on the calibration pattern and will calculate the
    rotation and translation of the camera with respect to the reference frame.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now encapsulate the calibration process in a `CameraCalibrator` class.
    The attributes of this class are as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that the input vectors of the scene and image points are in fact made
    of `std::vector` of point instances; each vector element is a vector of the points
    from one view. Here, we decided to add the calibration points by specifying a
    vector of the chessboard image filename as input; the method will take care of
    extracting the point coordinates from these images:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The first loop inputs the 3D coordinates of the chessboard, and the corresponding
    image points are the ones provided by the `cv::findChessboardCorners` function.
    This is done for all the available viewpoints. Moreover, in order to obtain a
    more accurate image point location, the `cv::cornerSubPix` function can be used;
    and as the name suggests, the image points will then be localized with subpixel
    accuracy. The termination criterion that is specified by the `cv::TermCriteria`
    object defines the maximum number of iterations and the minimum accuracy in subpixel
    coordinates. The first of these two conditions that is reached will stop the corner
    refinement process.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'When a set of chessboard corners have been successfully detected, these points
    are added to our vectors of image and scene points using our `addPoints` method.
    Once a sufficient number of chessboard images have been processed (and consequently,
    a large number of 3D scene point/2D image point correspondences are available),
    we can initiate the computation of the calibration parameters as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In practice, `10` to `20` chessboard images are sufficient, but these must be
    taken from different viewpoints at different depths. The two important outputs
    of this function are the camera matrix and the distortion parameters. These will
    be described in the next section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to explain the result of the calibration, we need to go back to the
    projective equation presented in the introduction of this chapter. This equation
    describes the transformation of a 3D point into a 2D point through the successive
    application of two matrices. The first matrix includes all of the camera parameters,
    which are called the intrinsic parameters of the camera. This `3x3` matrix is
    one of the output matrices returned by the `cv::calibrateCamera` function. There
    is also a function called `cv::calibrationMatrixValues` that explicitly returns
    the value of the intrinsic parameters given by a calibration matrix.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: The second matrix is there to have the input points expressed into camera-centric
    coordinates. It is composed of a rotation vector (a `3x3` matrix) and a translation
    vector (a `3x1` matrix). Remember that in our calibration example, the reference
    frame was placed on the chessboard. Therefore, there is a rigid transformation
    (made of a rotation component represented by the matrix entries `r1` to `r9` and
    a translation represented by `t1`, `t2`, and `t3`) that must be computed for each
    view. These are in the output parameter list of the `cv::calibrateCamera` function.
    The rotation and translation components are often called the **extrinsic parameters**
    of the calibration, and they are different for each view. The intrinsic parameters
    remain constant for a given camera/lens system.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: The calibration results provided by the `cv::calibrateCamera` are obtained through
    an optimization process. This process aims to find the intrinsic and extrinsic
    parameters that minimize the difference between the predicted image point position,
    as computed from the projection of the 3D scene points, and the actual image point
    position, as observed on the image. The sum of this difference for all the points
    specified during the calibration is called the **re-projection error**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: The intrinsic parameters of our test camera obtained from a calibration based
    on `27` chessboard images are `fx=409` pixels, `fy=408` pixels, `u0=237` pixels,
    and `v0=171`pixels. Our calibration images have a size of `536x356` pixels. From
    the calibration results, you can see that, as expected, the principal point is
    close to the center of the image, but yet off by few pixels. The calibration images
    were taken using a Nikon D500 camera with a `18mm` lens. Looking at the manufacturer
    specifications, we find that the sensor size of this camera is `23.5mm x 15.7mm`,
    which gives us a pixel size of `0.0438mm`. The estimated focal length is expressed
    in pixels, so multiplying the result by the pixel size gives us an estimated focal
    length of `17.8mm`, which is consistent with the actual lens we used.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now turn our attention to the distortion parameters. So far, we have
    mentioned that under the pin-hole camera model, we can neglect the effect of the
    lens. However, this is only possible if the lens that is used to capture an image
    does not introduce important optical distortions. Unfortunately, this is not the
    case with lower quality lenses or with lenses that have a very short focal length.
    Even the lens we used in this experiment introduced some distortion: the edges
    of the rectangular board are curved in the image. Note that this distortion becomes
    more important as we move away from the center of the image. This is a typical distortion observed
    with a fish-eye lens, and it is called **radial distortion**.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to compensate for these deformations by introducing an appropriate
    distortion model. The idea is to represent the distortions induced by a lens by
    a set of mathematical equations. Once established, these equations can then be
    reverted in order to undo the distortions visible on the image. Fortunately, the
    exact parameters of the transformation that will correct the distortions can be
    obtained together with the other camera parameters during the calibration phase.
    Once this is done, any image from the newly calibrated camera will be undistorted.
    Therefore, we have added an additional method to our calibration class:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Running this code on one of our calibration image results in the following
    undistorted image:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_11_006.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: To correct the distortion, OpenCV uses a polynomial function that is applied
    to the image points in order to move them to their undistorted position. By default,
    five coefficients are used; a model made of eight coefficients is also available.
    Once these coefficients are obtained, it is possible to compute two `cv::Mat`
    mapping functions (one for the `x` coordinate and one for the `y` coordinate)
    that will give the new undistorted position of an image point on a distorted image.
    This is computed by the `cv::initUndistortRectifyMap` function, and the `cv::remap`
    function remaps all the points of an input image to a new image. Note that because
    of the nonlinear transformation, some pixels of the input image now fall outside
    the boundary of the output image. You can expand the size of the output image
    to compensate for this loss of pixels, but you now obtain output pixels that have
    no values in the input image (they will then be displayed as black pixels).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More options are available when it comes to camera calibration.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Calibration with known intrinsic parameters
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a good estimate of the camera's intrinsic parameters is known, it could
    be advantageous to input them in the `cv::calibrateCamera` function. They will
    then be used as initial values in the optimization process. To do so, you just
    need to add the `cv::CALIB_USE_INTRINSIC_GUESS` flag and input these values in
    the calibration matrix parameter. It is also possible to impose a fixed value
    for the principal point (`cv::CALIB_FIX_PRINCIPAL_POINT`), which can often be
    assumed to be the central pixel. You can also impose a fixed ratio for the focal
    lengths `fx` and `fy` (`cv::CALIB_FIX_RATIO`), in which case, you assume that
    the pixels are square.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Using a grid of circles for calibration
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead of the usual chessboard pattern, OpenCV also offers the possibility
    to calibrate a camera by using a grid of circles. In this case, the centers of
    the circles are used as calibration points. The corresponding function is very
    similar to the function we used to locate the chessboard corners, for example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: See also
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *A flexible new technique for camera calibration* article by *Z. Zhang*
    in *IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22*,
    no 11, 2000, is a classic paper on the problem of camera calibration
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recovering camera pose
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a camera is calibrated, it becomes possible to relate the captured images
    with the outside world. We previously explained that if the 3D structure of an
    object is known, then one can predict how the object will be imaged on the sensor
    of the camera. The process of image formation is indeed completely described by
    the projective equation that was presented at the beginning of this chapter. When
    most of the terms of this equation are known, then it becomes possible to infer
    the value of the other elements (2D or 3D) through the observation of some images.
    In this recipe, we will look at the camera pose recovery problem when a known
    3D structure is observed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s consider a simple object, a bench in a park. We took an image of this
    one using the camera/lens system calibrated in the previous recipe. We also have
    manually identified eight distinct image points on the bench that we will use
    for our camera pose estimation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/B05388_11_05.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: 'Having access to this object, it is possible to make some physical measurements.
    The bench is composed of a seat that is `242.5cm x 53.5cm x 9cm` and a back that
    is `242.5cm x 24cm x 9cm` fixed `12cm` over the seat. Using this information,
    we can then easily derive the 3D coordinates of the eight identified points in
    some object-centric reference frame (here, we fixed the origin at the left extremity
    of the intersection between the two planes). We can then create a `cv::Point3f`
    vector containing these coordinates:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The question now is where the camera was with respect to these points when
    the shown picture was taken. Since the coordinates of the image of these known
    points on the 2D image plane are also known, then it becomes easy to answer this
    question by using the `cv::solvePnP` function. Here, the correspondence between
    the 3D and the 2D points has been established manually, but one should be able
    to come up with some methods that would allow you to obtain this information automatically:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This function in fact, computes the rigid transformation (rotation and translation)
    that brings the object coordinates in the camera-centric reference frame (that
    is, the one that has its origin at the focal point). It is also important to note
    that the rotation computed by this function is given in the form of a 3D vector.
    This is a compact representation in which the rotation to apply is described by
    a unit vector (an axis of rotation) around which the object is rotated by a certain
    angle. This axis-angle representation is also called the **Rodrigues' rotation
    formula**. In OpenCV, the angle of the rotation corresponds to the norm of the
    output rotation vector, the latter being aligned with the axis of rotation. This
    is why the `cv::Rodrigues` function is used to obtain the 3D matrix of rotation
    that appears in our projective equation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数实际上计算的是将物体坐标从相机中心参考系（即原点位于焦点处的参考系）转换过来的刚体变换（旋转和平移）。值得注意的是，这个函数计算出的旋转是以3D向量的形式给出的。这是一种紧凑的表示方式，其中要应用的旋转由一个单位向量（旋转轴）描述，物体围绕这个轴旋转一定角度。这种轴角表示法也称为**罗德里格斯旋转公式**。在OpenCV中，旋转角度对应于输出旋转向量的范数，后者与旋转轴对齐。这就是为什么我们使用`cv::Rodrigues`函数来获取出现在我们投影方程中的3D旋转矩阵。
- en: 'The pose recovery procedure described here is simple, but how do we know we
    obtained the right camera/object pose information? We can visually assess the
    quality of the results by using the `cv::viz` module that gives us the ability
    to visualize 3D information. The use of this module is explained in the last section
    of this recipe, but let''s display a simple 3D representation of our object and
    the camera that captured it:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述的姿态恢复过程很简单，但我们如何知道我们获得了正确的相机/物体姿态信息呢？我们可以通过使用`cv::viz`模块来评估结果的质量，该模块使我们能够可视化3D信息。这个模块的使用在食谱的最后部分有解释，但让我们显示一个简单的3D表示，包括我们的物体和捕获它的相机：
- en: '![How to do it...](img/image_11_008.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![如何做到这一点...](img/image_11_008.jpg)'
- en: It might be difficult to judge of the quality of the pose recovery just by looking
    at this image but if you test the example of this recipe on your computer, you
    will have the possibility to move this representation in 3D using your mouse,
    which should give you a better sense of the solution obtained.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过查看这张图片可能很难判断姿态恢复的质量，但如果你在你的电脑上测试这个食谱的例子，你将有机会使用鼠标在3D中移动这个表示，这应该会给你一个更好的解决方案感。
- en: How it works...
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we assumed that the 3D structure of the object was known, as
    well as the correspondence between sets of object points and image points. The
    camera's intrinsic parameters were also known through calibration. If you look
    at our projective equation presented at the end of the *Digital image formation*
    section of the introduction of this chapter, this means that we have points for
    which coordinates `(X,Y,Z)` and `(x,y)` are known. We also have the elements of
    the first matrix known (the intrinsic parameters). Only the second matrix is unknown;
    this is the one that contains the extrinsic parameters of the camera that is the
    camera/object pose information. Our objective is then to recover these unknown
    parameters from the observation of 3D scene points. This problem is known as the
    **Perspective-n-Point** (**PnP**) problem.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们假设物体的3D结构是已知的，以及物体点集和图像点集之间的对应关系。通过校准，我们也知道了相机的内在参数。如果你查看我们在本章引言中“数字图像形成”部分的投影方程，这意味着我们有坐标为`(X,Y,Z)`和`(x,y)`的点。我们还有第一个矩阵的元素已知（内在参数）。只有第二个矩阵是未知的；这是包含相机外参数的矩阵，也就是相机/物体姿态信息。我们的目标是从对3D场景点的观察中恢复这些未知参数。这个问题被称为**透视-n-点**（**PnP**）问题。
- en: Rotation has three degrees of freedom (for example, angle of rotation around
    the three axes) and translation also has three degrees of freedom. We therefore
    have a total of six unknowns. For each object point/image point correspondence,
    the projective equation gives us three algebraic equations, but since the projective
    equation is up to a scale factor, we only have two independent equations. A minimum
    of three points is therefore required to solve this system of equations. Obviously,
    more points provide a more reliable estimate.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: In practice, many different algorithms have been proposed to solve this problem
    and OpenCV proposes a number of different implementation in its `cv::solvePnP`
    function. The default method consists in optimizing what is called the reprojection
    error. Minimizing this type of error is considered to be the best strategy to
    get accurate 3D information from camera images. In our problem, it corresponds
    to finding the optimal camera position that minimizes the 2D distance between
    the projected 3D points (as obtained by applying the projective equation) and
    the observed image points given as input.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Note that OpenCV also has a `cv::solvePnPRansac` function. As the name suggests,
    this function uses the **RANSAC** algorithm in order to solve the PnP problem.
    This means that some of the object point/image point correspondences may be wrong
    and the function will return the ones that have been identified as outliers. This
    is very useful when these correspondences have been obtained through an automatic
    process that can fail for some points.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When working with 3D information, it often difficult to validate the solutions
    obtained. To this end, OpenCV offers a simple yet powerful visualization module
    that facilitates the development and debugging of 3D vision algorithms. It allows
    inserting points, lines, cameras and other objects in a virtual 3D environment
    that you can interactively visualize from various points of views.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: cv::Viz, a 3D Visualizer module
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`cv::Viz` is an extra module of the OpenCV library that is built on top of
    the **Visualization Toolkit** (**VTK**) open source library. This is a powerful
    framework used for 3D computer graphics. With `cv::viz`, you create a 3D virtual
    environment to which you can add a variety of objects. A visualization window
    is created that displays the environment from a given point of view. You saw in
    this recipe an example of what can be displayed in a `cv::viz` window. This window
    responds to mouse events that are used to navigate inside the environment (through
    rotations and translations). This section describes the basic use of the `cv::viz`
    module.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do is to create the visualization window. Here, we use a
    white background:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, you create your virtual objects and insert them into the scene. There
    is a variety of predefined objects. One of them is particularly useful for us;
    it is the one that creates a virtual pin-hole camera:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `cMatrix` variable is a `cv::Matx33d` (that is, a `cv::Matx<double,3,3>`)
    instance containing the intrinsic camera parameters as obtained from calibration.
    By default, this camera is inserted at the origin of the coordinate system. To
    represent the bench, we used two rectangular cuboid objects:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This virtual bench is also added at the origin; it then needs to be moved at
    its camera-centric position as found from our `cv::solvePnP` function. It is the
    responsibility of the `setWidgetPose` method to perform this operation. This one
    simply applies the rotation and translation components of the estimated motion:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The final step is to create a loop that keeps displaying the visualization
    window. The `1ms` pause is there to listen to mouse events:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This loop will stop when the visualization window is closed or when a key is
    pressed over an OpenCV image window. Try to apply inside this loop some motion
    on an object (using `setWidgetPose`); this is how animation can be created.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Model-based object pose in 25 lines of code* by *D. DeMenthon* and *L. S.
    Davis*, in the *European Conference on Computer Vision*, 1992, pp.335-343 is a
    famous method for recovering camera pose from scene points'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Matching images using random sample consensus* recipe in Chapter 10 , *Estimating
    Projective Relations in Images* describes the RANSAC algorithm
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Installing the OpenCV library* recipe in [Chapter 1](ch01.html "Chapter 1. Playing
    with Images") , *Playing with Images* explains how to install the RANSAC `cv::viz`
    extra module
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reconstructing a 3D scene from calibrated cameras
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw in the previous recipe that it is possible to recover the position of
    a camera observing a 3D scene, when this one is calibrated. The approach described
    took advantage of the fact that, sometimes, the coordinates of some 3D points
    visible in the scene might be known. We will now learn that if a scene is observed
    from more than one point of view, 3D pose and structure can be reconstructed even
    if no information about the 3D scene is available. This time, we will use correspondences
    between image points in the different views in order to infer 3D information.
    We will introduce a new mathematical entity encompassing the relation between
    two views of a calibrated camera, and we will discuss the principle of triangulation
    in order to reconstruct 3D points from 2D images.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's again use the camera we calibrated in the first recipe of this chapter
    and take two pictures of some scene. We can match feature points between these
    two views using, for example, the SIFT detector and descriptor presented in [Chapter
    8](ch08.html "Chapter 8. Detecting Interest Points") , *Detecting Interest Points*
    and [Chapter 9](ch09.html "Chapter 9. Describing and Matching Interest Points")
    , *Describing and Matching interest points*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'The fact that the calibration parameters of the camera are available, allows
    us to work in world coordinates; and therefore establish a physical constraint
    between the camera poses and the position of the corresponding points. Basically,
    we introduce a new mathematical entity called the **Essential matrix**, which
    is the calibrated version of the fundamental matrix introduced in the previous
    chapter. Therefore, there is a `cv::findEssentialMat` function that''s identical
    to the `cv::findFundametalMat` that was used in the *Computing the fundamental
    matrix of an image pair* recipe in Chapter 10 , *Estimating Projective Relations
    in Images*. We can call this function with the established point correspondences
    and through a RANSAC scheme, filter out the outlier points to retain only the
    matches that comply with the found geometry:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The resulting set of inliers matches is then as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_11_009.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: 'As it will be explained in the next section, the essential matrix encapsulates
    the rotation and translation components that separate the two views. It is therefore
    possible to recover the relative pose between our two views directly from this
    matrix. OpenCV has a function that performs this operation, it is the `cv::recoverPose`
    function. This one is used as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now that we have the relative pose between the two cameras, it becomes possible
    to estimate the location of points for which we have established correspondence
    between the two views. The following screenshot illustrates how this is possible.
    It shows the two cameras at their estimated position (the left one is placed at
    the origin). We also have selected a pair of corresponding points and, for these
    image points, we traced a ray that, according to the projective geometry model,
    corresponds to all possible locations of the associated 3D point:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_11_010.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Clearly, since these two image points have been generated by the same 3D point,
    the two rays must intersect at one location, the location of the 3D point. The
    method that consists of intersecting the lines of projection of two corresponding
    image points, when the relative position of two cameras is known, is called **triangulation**.
    This process first requires the two projection matrices and can be repeated for
    all matches. Remember, however, that these ones must be expressed in world coordinates;
    this is done here by using the `cv::undistortPoints` function.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we call our triangulate function, which computes the position of the
    triangulated point, and that will be described in the next section:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'A cloud of 3D points located on the surface of the scene elements is thus found:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_11_011.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Note that from this new point of view, we can see that the two rays we drew
    do not intersect as they were supposed to. This fact will be discussed in the
    next section.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The calibration matrix is the entity allowing us to transform pixel coordinates
    into world coordinates. We can then more easily relate image points to the 3D
    points that have produced them. This is demonstrated in the following figure,
    which we will now use to demonstrate a simple relationship between a world point
    and its images:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_11_012.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: 'The figure shows two cameras separated by a rotation `R` and a translation
    `T`. It is interesting to note that the translation vector `T` joins the centers
    of projection of the two cameras. We also have a vector `x` joining the first
    camera center to an image point and a vector `x''` joining the second camera center
    to the corresponding image point. Since we have the relative motion between the
    two cameras, we can express the orientation of `x` in terms of the second camera
    reference as `Rx`. Now, if you carefully observe the geometry of the image points
    shown, you will observe that vectors `T`, `Rx`, and `x''` are all coplanar. This
    fact can be expressed by the following mathematical relation:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/B05388_11_17.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: It was possible to reduce the first relation to a single `3x3` matrix `E` because
    a cross-product can also be expressed by a matrix operation. This matrix `E` is
    called the essential matrix and the associated equation is the calibrated equivalent
    of the epipolar constraint presented in the *Computing the fundamental matrix
    of an image pair* recipe in Chapter 10 , *Estimating Projective Relations in Images*.
    We can then estimate this one from image correspondences, as we did for the fundamental
    matrix, but this time expressing these ones in world coordinates. Also, as demonstrated,
    the essential matrix is built from the rotation and translation components of
    the motion between the two cameras. This means that once this one has been estimated,
    it can be decomposed to obtain the relative pose between the cameras. This is
    what we did by using the `cv::recoverPose` function. This function calls the `cv::decomposeEssentialMat`
    function, which produces four possible solutions for the relative pose. The right
    one is identified by looking at the set of provided matches to determine the solution
    that is physically possible.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the relative pose between the cameras has been obtained, the position
    of any point corresponding to a match pair is recovered through triangulation.
    Different methods have been proposed to solve the triangulation problem. Probably
    the simplest solution consists of considering the two projection matrices, `P`
    and `P''`. The seek 3D point in homogenous coordinates can be expressed as `X=[X,Y,Z,1]^T`,
    and we know that `x=PX` and `x''=P''X`. Each of these two homogenous equations
    brings two independent equations, which is sufficient to solve the three unknowns
    of the 3D point position. This over determined system of equation can be solved
    using a least-square approach, which can be accomplished using a convenient OpenCV
    utility function called `cv::solve`. The complete function is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We have noted in the previous section that very often, because of noise and
    digitization, the projection lines that should normally intersect do not intersect
    in practice. The least-square solution will therefore find a solution somewhere
    around the point of intersection. Also, this method will not work if you try to
    reconstruct a point at infinity. This is because, for such a point, the fourth
    element of the homogenous coordinates should be at `0` not at `1` as assumed.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is important to understand that this 3D reconstruction is done up
    to a scale factor only. If you need to make real measurements, you need to know
    at least one physical distance, for example, the real distance between the two
    cameras or the height of one of the visible objects.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The 3D reconstruction is a rich field of research in computer vision, and there
    is much more to explore in the OpenCV library on the subject.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Decomposing a homography
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We learned in this recipe that an essential matrix can be decomposed in order
    to recover the rotation and translation between two cameras. We also learned in
    the previous chapter that a homography exists between two views of a plane. In
    this case, this homography contains also the rotational and translational components.
    In addition, it contains information about the plane, namely its normal with respect
    to each camera. The function `cv::decomposeHomographyMat` can be used to decompose
    this matrix; the condition, however, is to have a calibrated camera.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Bundle adjustment
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, we first estimate the camera position from matches and then
    reconstruct the associated 3D points through triangulation. It is possible to
    generalize this process by using any number of views. For each of these views,
    feature points are detected and are matched with the other views. Using this information,
    it is possible to write equations that relate the rotations and translations between
    the views, the set of 3D points and the calibration information. All these unknowns
    can be optimized together through a large optimization process that aims at minimizing
    the reprojection errors of all points in each view where they are visible. This
    combined optimization procedure is called **bundle adjustment**. Have a look at
    the `cv::detail::BundleAdjusterReproj` class, which implements a camera parameters
    refinement algorithm that minimizes the sum of the reprojection error squares.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Triangulation* by *R. Hartley* and *P. Sturm* in *Computer Vision and Image
    Understanding vol. 68*, no. 2, 1997 presents a formal analysis of different triangulation
    methods'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Modeling the World from Internet Photo Collections* by *N. Snavely*, *S.M.
    Seitz*, and *R. Szeliski* in *International Journal of **Computer Vision,* vol.
    80, no 2, 2008 describes a large-scale application of 3D reconstruction through
    bundle adjustment'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing depth from stereo image
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Humans view the world in three dimensions using their two eyes. Robots can do
    the same when they are equipped with two cameras. This is called **stereovision**.
    A stereo rig is a pair of cameras mounted on a device, looking at the same scene
    and separated by a fixed baseline (distance between the two cameras). This recipe
    will show you how a depth map can be computed from two stereo images by computing
    dense correspondence between the two views.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A stereovision system is generally made of two side-by-side cameras looking
    at the same direction. The following figure illustrates such a stereo system in
    a perfectly aligned configuration:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/image_11_014.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'Under this ideal configuration the cameras are only separated by a horizontal
    translation and therefore all epipolar lines are horizontal. This means that corresponding
    points have the same `y` coordinates, which reduces the search for matches to
    a 1D line. The difference in their `x` coordinates depends on the depth of the
    points. Points at infinity have image points at the same `(x,y)` coordinates and
    the closer the points are to the stereo rig the greater will be the difference
    of their `x` coordinates. This fact can be demonstrated formally by looking at
    the projective equation. When cameras are separated by a pure horizontal translation,
    then the projective equation of the second camera (the one on the right) becomes
    this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B05388_11_18.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'Here, for simplicity, we assume square pixels and same calibration parameters
    for both cameras. Now if you compute the difference of `x-x''` (do not forget
    to divide by `s` to normalize the homogenous coordinates) and isolate the `z`
    coordinate, you obtain the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B05388_11_19.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: The term `(x-x')` is called the **disparity**. To compute the depth map of a
    stereovision system, the disparity of each pixel must be estimated. This recipe
    will show you how to do it.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The ideal configuration shown in the previous section is, in practice, very
    difficult to realize. Even if they are accurately positioned, the cameras of the
    stereo rig will unavoidably include some extra translational and rotational components.
    But, fortunately, the images can be rectified such to produce horizontal epilines.
    This can be achieved by computing the fundamental matrix of the stereo system
    using, for example, the robust matching algorithm of the previous chapter. This
    is what we did for the following stereo pair (with some epipolar lines drawn on
    it):'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_11_017.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: 'OpenCV offers a rectifying function that uses a homographic transformation
    to project the image plane of each camera onto perfectly aligned virtual plane.
    This transformation is computed from a set of matched points and a fundamental
    matrix. Once computed, these homographies are then used to wrap the images:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'For our example, the rectified image pair is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_11_018.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'Computing the disparity map can then be accomplished using methods that assume
    parallelism of the cameras (and consequently horizontal epipolar lines):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The obtained disparity map can then be displayed as an image. Bright values
    correspond to high disparities and, from what we learned earlier in this recipe,
    those high disparity values correspond to proximal objects:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_11_019.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: The quality of the computed disparity mainly depends on the appearance of the
    different objects that compose the scene. Highly-textured regions tend to produce
    more accurate disparity estimates since they can be non-ambiguously matched. Also,
    a larger baseline increases the range of detectable depth values. However, enlarging
    the baseline also makes disparity computation more complex and less reliable.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computing disparities is a pixel matching exercise. We already mentioned that
    when the images are properly rectified, the search space is conveniently aligned
    with the image rows. The difficulty, however, is that, in stereovision, we are
    generally seeking a dense disparity map, that is, we want to match every pixel
    of one image with the pixels of the other image.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be more challenging than selecting a few distinctive points in an
    image and finding their corresponding points in the other image. Disparity computation
    is therefore a complex process that is generally composed of four steps:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Matching cost calculation.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cost aggregation.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成本聚合。
- en: Disparity computation and optimization.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 差异计算和优化。
- en: Disparity refinement.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 差异细化。
- en: These steps are detailed in the next paragraph.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤在下一段中详细说明。
- en: Assigning a disparity to one pixel, is putting a pair of points in correspondence
    in a stereo set. Finding the best disparity map is often posed as an optimization
    problem. With this perspective, matching two points has a cost that must be computed
    following a defined metric. This can be, for example, a simple absolute or squared
    difference of intensities, colors or gradients. In the search for an optimal solution,
    the matching cost is generally aggregated over a region in order to cope with
    noise local ambiguity. The global disparity map can then be estimated by evaluating
    an energy function that includes terms to smooth the disparity map, take into
    account any possible occlusion, and enforce a uniqueness constraint. Finally,
    a post-processing step is often applied in order to refine the disparity estimates
    during which, for example, planar regions are detected or depth discontinuities
    are detected.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个像素分配给差异，是在立体集中将一对点对应起来。寻找最佳差异图通常被提出为一个优化问题。从这个角度来看，匹配两个点有一个必须按照定义的度量计算的代价。这可以是，例如，简单绝对或平方的强度、颜色或梯度的差异。在寻找最优解的过程中，匹配代价通常在一个区域内聚合，以应对噪声和局部模糊。然后可以通过评估一个包含平滑差异图、考虑任何可能的遮挡并强制唯一性约束的能量函数来估计全局差异图。最后，通常会应用后处理步骤来细化差异估计，在此期间，例如，检测平面区域或检测深度不连续性。
- en: OpenCV implements a number of disparity computation methods. Here, we used the
    `cv::StereoSGBM` approach. The simplest method is `cv::StereoBM` , which is based
    on block matching.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV实现了许多差异计算方法。在这里，我们使用了`cv::StereoSGBM`方法。最简单的方法是`cv::StereoBM`，它基于块匹配。
- en: Finally, it should be noted that a more accurate rectification can be performed
    if you are ready to undergo a full calibration process. The `cv::stereoCalibrate`
    and `cv::stereoRectify` functions are in this case used in conjunction with a
    calibration pattern. The rectification mapping then computes new projection matrices
    for the cameras instead of simple homographies.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，需要注意的是，如果你准备进行完整的校准过程，可以执行更精确的校正。在这种情况下，`cv::stereoCalibrate`和`cv::stereoRectify`函数与校准图案一起使用。校正映射随后计算相机的新的投影矩阵，而不是简单的单应性。
- en: See also
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The article *A Taxonomy and Evaluation of Dense two-Frame Stereo Correspondence
    Algorithms* by *D. Scharstein* and *R. Szeliski* in *International **Journal of
    Computer Vision,* vol. 47, 2002 is a classic reference on disparity computation
    methods
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D. Scharstein和R. Szeliski在2002年发表的《International Journal of Computer Vision》第47卷上的文章“A
    Taxonomy and Evaluation of Dense two-Frame Stereo Correspondence Algorithms”是关于差异计算方法的经典参考文献。
- en: The article *Stereo processing by semiglobal matching and mutual information*
    by *H. Hirschmuller* in *IEEE **Transactions on Pattern Analysis and Machine Intelligence,*
    vol. 30, no 2, pp. 328-341, 2008 describes the approach used for computing the
    disparity in this recipe
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: H. Hirschmuller在2008年发表的《IEEE Transactions on Pattern Analysis and Machine Intelligence》第30卷第2期上的文章“Stereo
    processing by semiglobal matching and mutual information”描述了在此配方中计算差异所使用的方法。
