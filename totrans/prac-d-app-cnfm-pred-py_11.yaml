- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Handling Imbalanced Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理不平衡数据
- en: This chapter delves into the intriguing world of imbalanced data and how conformal
    prediction can be a game-changer in handling such scenarios.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了不平衡数据的迷人世界，以及如何使符合预测成为处理此类场景的转折点。
- en: Imbalanced datasets are a common challenge in machine learning, often leading
    to biased predictions and underperforming models. This chapter will equip you
    with the knowledge and skills to tackle these issues head-on.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡数据集是机器学习中常见的挑战，往往导致预测偏差和模型表现不佳。本章将为您提供解决这些问题的知识和技能。
- en: We will be introduced to imbalanced data and learn why it poses a significant
    challenge in machine learning applications. We will then explore various methods
    traditionally used to address imbalanced data problems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将了解不平衡数据，并学习为什么它在机器学习应用中构成一个重大挑战。然后，我们将探讨传统上用来解决不平衡数据问题的各种方法。
- en: The highlight of the chapter is the application of conformal prediction to imbalanced
    data problems.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是符合预测在解决不平衡数据问题中的应用。
- en: 'This chapter will illustrate how conformal prediction can solve imbalanced
    data problems by covering the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将通过以下主题来展示如何通过符合预测解决不平衡数据问题：
- en: Introducing imbalanced data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍不平衡数据
- en: Why imbalanced data problems are complex to solve
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么不平衡数据问题难以解决
- en: Methods for solving imbalanced data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决不平衡数据的方法
- en: How conformal prediction can be applied to help solve imbalanced data problems
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将符合预测应用于帮助解决不平衡数据问题
- en: Join us on this enlightening journey as we unravel the complexities of imbalanced
    data and discover innovative solutions through conformal prediction.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们这次启发性的旅程，我们将揭示不平衡数据的复杂性，并通过符合预测发现创新解决方案。
- en: By the end of this chapter, you will have a solid understanding of how conformal
    prediction can be effectively applied to handle imbalanced data, thereby improving
    the performance and reliability of your machine learning models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将深入了解如何有效地应用符合预测来处理不平衡数据，从而提高机器学习模型的性能和可靠性。
- en: Introducing imbalanced data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍不平衡数据
- en: In machine learning, we often come across datasets that need to be more balanced.
    But what does it mean for a dataset to be imbalanced?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们经常遇到需要更加平衡的数据集。但一个数据集不平衡意味着什么？
- en: An imbalanced dataset is one where the distribution of samples across the different
    classes is not uniform. In other words, one type has significantly more samples
    than the other(s). This is a common scenario in many real-world applications.
    For instance, in a dataset for fraud detection, the number of non-fraudulent transactions
    (majority class) is typically much higher than the number of fraudulent ones (minority
    class).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不平衡的数据集是指不同类别样本分布不均匀的情况。换句话说，某一类别的样本数量显著多于其他类别。这在许多实际应用中很常见。例如，在用于欺诈检测的数据集中，非欺诈交易（多数类别）的数量通常远高于欺诈交易（少数类别）。
- en: Imagine a medical dataset recording instances of a rare disease. Most patients
    will be disease-free, resulting in a large class of healthy records, while only
    a tiny fraction will be affected by the disease. This disproportion in the distribution
    of categories is what we call imbalanced data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个记录罕见疾病实例的医疗数据集。大多数患者将无病，导致大量健康记录的大类别，而受疾病影响的患者比例极小。这种类别分布的不平衡就是我们所说的不平衡数据。
- en: Imbalanced data can lead to a significant challenge in predictive modeling.
    By their very nature, machine learning algorithms are designed to minimize errors
    and maximize accuracy. When trained on imbalanced data, they tend to be biased
    toward the majority class, often at the expense of the minority class prediction
    accuracy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡数据可能导致预测建模中的重大挑战。由于机器学习算法的本质是设计来最小化错误并最大化准确率，当在不平衡数据上训练时，它们往往偏向于多数类别，这通常是以牺牲少数类别的预测准确率为代价的。
- en: In our medical example, a naive model might predict that no one has the disease,
    achieving a high accuracy due to the sheer number of healthy records but failing
    to identify the few crucial cases that do. Such models, misled by the imbalance,
    could have dire real-world implications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的医学示例中，一个简单的模型可能会预测没有人患有疾病，由于健康记录的数量巨大，因此实现了高准确率，但未能识别出少数几个确实患有疾病的病例。这样的模型可能会因为不平衡而误导，导致在现实世界中产生严重的后果。
- en: The nature of imbalanced data is pervasive across industries. From fraud detection
    in finance, where fraudulent transactions are rare but crucial to detect, to natural
    disaster predictions in meteorology, where the event of interest (e.g., a tornado
    or earthquake) is infrequent but significant, imbalances pose challenges that
    professionals must be equipped to handle.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡数据的性质在各个行业中普遍存在。从金融中的欺诈检测，欺诈交易虽然罕见但至关重要，到气象学中的自然灾害预测，感兴趣的（例如，龙卷风或地震）事件虽然不常见但意义重大，不平衡带来了专业人士必须能够应对的挑战。
- en: Recognizing and understanding imbalanced data is the first step in effectively
    addressing their challenges. As we proceed, we’ll deep dive into why these problems
    are particularly tough to crack and explore methodologies to handle them, focusing
    on the potential of conformal prediction.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 认识和理解不平衡数据是有效解决其挑战的第一步。随着我们继续前进，我们将深入探讨为什么这些问题特别难以解决，并探讨处理它们的方法，重点关注符合预测的潜力。
- en: Why imbalanced data problems are complex to solve
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么不平衡数据问题难以解决
- en: Addressing imbalanced data is no walk in the park, and here’s why. At the core
    of the challenge is the nature of conventional machine learning algorithms. These
    algorithms minimize overall error and are designed with the assumption of balanced
    class distributions. This becomes problematic when faced with imbalanced datasets,
    leading to a pronounced bias toward the majority class.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 解决不平衡数据并非易事，原因如下。挑战的核心是传统机器学习算法的性质。这些算法最小化总体错误，并假设类别分布平衡。当面对不平衡数据集时，这就会成为问题，导致对多数类别的明显偏差。
- en: The gravity of this problem becomes evident when we realize that in many scenarios,
    it’s the minority class that carries more significance. Take fraud detection or
    medical diagnoses as cases in point. While fraudulent transactions or disease
    instances might be sparse, their correct identification is paramount. Yet, a model
    trained on skewed data might often lean toward predicting the majority class,
    achieving superficially high accuracy but failing its core objective.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们意识到在许多情况下，少数类别的信息更为重要时，这个问题的重要性就变得显而易见了。以欺诈检测或医疗诊断为例。虽然欺诈交易或疾病实例可能很少，但它们的正确识别至关重要。然而，在倾斜数据上训练的模型可能会经常倾向于预测多数类别，表面上达到很高的准确率，但未能实现其核心目标。
- en: 'To add to the challenge, conventional metrics, such as accuracy, are only sometimes
    our friends here. A dataset with just 2% fraudulent transactions can trick us
    into complacency: a naive model predicting every transaction as legitimate will
    boast a 98% accuracy, masking its utter failure in detecting fraud.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步，传统的指标，如准确率，在这里有时并不是我们的朋友。一个只有2%欺诈交易的数据库可能会让我们产生自满：一个简单地将每笔交易预测为合法的模型会吹嘘98%的准确率，掩盖了它在检测欺诈方面的彻底失败。
- en: The maze of academic literature on this topic makes things even more difficult.
    With many methods and theories, determining which ones genuinely work is akin
    to finding a needle in a haystack. Methods such as **Synthetic Minority Oversampling
    Technique** (**SMOTE**), which is frequently discussed, require discerning analysis
    to gauge their actual effectiveness.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个主题的学术文献迷宫使事情变得更加困难。由于有许多方法和理论，确定哪些真正有效就像在 haystack 中找针一样。经常讨论的方法，如**合成少数过采样技术**（**SMOTE**），需要经过仔细分析来评估其实际的有效性。
- en: 'A word of advice for those just starting in data science: approach the realm
    of imbalanced classification with a discerning eye. Not all that glitters is gold.
    While searching for a magic solution is tempting, sometimes it’s about reframing
    the problem. By shifting our perspective and focusing on more relevant metrics,
    we can find a way through the maze, making informed and effective decisions.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 给那些刚开始学习数据科学的人一些建议：用敏锐的眼光看待不平衡分类的领域。并非所有闪光的东西都是金子。虽然寻找一个神奇的解决方案很有吸引力，但有时它关乎重新定义问题。通过转变我们的视角，关注更相关的指标，我们可以找到一条出路，做出明智而有效的决策。
- en: We will now look into some common methods for dealing with imbalanced data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨一些处理不平衡数据的常见方法。
- en: Methods for solving imbalanced data
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决不平衡数据的方法
- en: Where should we turn when confronted with the challenge of imbalanced class
    distribution? While a significant portion of resources in the field suggest using
    resampling methods, including undersampling, oversampling, and techniques such
    as SMOTE, it’s crucial to note that these recommendations often sidestep foundational
    theory and practical application.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 面对不平衡类别分布的挑战时，我们应该转向何处？虽然该领域的大部分资源建议使用重采样方法，包括欠采样、过采样以及如SMOTE等技术，但重要的是要注意，这些建议通常回避了基础理论和实际应用。
- en: Before diving into solutions for imbalanced classes, it’s essential first to
    understand their underlying nature. The issue might be better approached in specific
    scenarios such as anomaly detection rather than in a traditional classification
    problem.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨不平衡类别的解决方案之前，首先理解其潜在性质是至关重要的。这个问题可能更适合在特定场景中解决，例如异常检测，而不是在传统的分类问题中。
- en: In specific scenarios, the class imbalance isn’t static. It can evolve or may
    be influenced by the need for adequate labels. For instance, consider a system
    monitoring network traffic for potential security threats. Initially, threats
    might be rare, leading to a class imbalance. However, as the system matures and
    more potential hazards are identified and labeled, the imbalance might shift,
    reducing or reversing the skew.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定场景中，类别不平衡不是静态的。它可能演变或可能受到对适当标签的需求的影响。例如，考虑一个监控系统网络流量以检测潜在安全威胁的系统。最初，威胁可能很少，导致类别不平衡。然而，随着系统的成熟和更多潜在危害被识别和标记，不平衡可能发生变化，减少或逆转偏差。
- en: Addressing such dynamic imbalances requires adaptive methods that can recalibrate
    as data characteristics change, ensuring the model remains effective throughout
    its life cycle.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这种动态不平衡需要自适应方法，这些方法可以在数据特征变化时重新校准，确保模型在其生命周期内保持有效。
- en: When these challenges are absent, it’s prudent to shift focus to the evaluation
    metrics. We’ve previously examined metrics such as log loss and Brier loss, which
    are instrumental in assessing model calibration. Notably, employing resampling
    techniques with these metrics might adversely impact the model’s calibration.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些挑战不存在时，明智的做法是将重点转向评估指标。我们之前已经检查了如对数损失和Brier损失等指标，这些指标在评估模型校准方面至关重要。值得注意的是，使用这些指标与重采样技术可能会对模型的校准产生不利影响。
- en: One frequently proposed remedy for imbalanced data is to modify the dataset
    through various resampling techniques.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不平衡数据，一个经常提出的补救措施是通过各种重采样技术修改数据集。
- en: 'Resampling methods are techniques used to balance the distribution of classes
    in an imbalanced dataset. These methods can be broadly categorized into two main
    types:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 重采样方法是用来平衡不平衡数据集中类别分布的技术。这些方法可以大致分为两大类：
- en: '**Oversampling**: This involves increasing the number of instances in the minority
    class. Methods include the following:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过采样**：这涉及到增加少数类别的实例数量。方法包括以下几种：'
- en: '**Random oversampling**: This involves duplicating random records from the
    minority class.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机过采样**：这涉及到复制少数类别的随机记录。'
- en: '**SMOTE**: SMOTE creates synthetic samples for the minority class in a feature
    space by following a specific algorithm. It starts by randomly selecting a minority
    class instance and finding its k-nearest minority class neighbors. SMOTE randomly
    picks one from these neighbors and calculates the difference between its features
    and the selected instance’s features. It then multiplies this difference by a
    random number between 0 and 1, adding the result to the original instance’s features.
    This procedure generates a new, synthetic data point that lies somewhere on the
    line segment, connecting the actual instance with its chosen neighbor, effectively
    creating plausible new instances that contribute to a more balanced dataset for
    the classifier to learn from.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SMOTE**：SMOTE通过遵循特定的算法在特征空间中为少数类别创建合成样本。它首先随机选择一个少数类别实例，并找到其k个最近的少数类别邻居。SMOTE从这些邻居中随机选择一个，并计算其特征与所选实例特征之间的差异。然后，它将这个差异乘以0到1之间的随机数，并将结果加到原始实例的特征上。这个过程生成一个新的、合成的数据点，它位于连接实际实例与其所选邻居的实际线段上，从而有效地创建出对构建更平衡数据集有贡献的合理新实例。'
- en: '**Adaptive synthetic (ADASYN) sampling**: Creating synthetic instances for
    the minority class by following their density distributions. Extra synthetic data
    is produced for minority samples that pose more significant learning challenges
    than those that are easier to learn.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应合成（ADASYN）采样**：通过遵循它们的密度分布为少数类创建合成实例。对于比容易学习的样本更具学习挑战性的少数样本，会产生额外的合成数据。'
- en: '**Undersampling**: This involves reducing the number of instances in the majority
    class. Methods include the following:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠采样**：这涉及到减少多数类的实例数量。方法包括以下几种：'
- en: '**Random undersampling**: This involves randomly eliminating majority class
    instances.'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机欠采样**：这涉及到随机消除多数类实例。'
- en: '**Tomek links**: This identifies pairs of instances from nearest neighbor classes
    and removes the majority instance from the pair.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tomek链接**：这识别了最近邻类中实例的对，并从这对中移除多数类实例。'
- en: '**Cluster centroids**: This method replaces a cluster of majority samples with
    the cluster centroid of a k-means algorithm.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类中心**：这种方法用k-means算法的聚类中心替换多数样本的聚类。'
- en: '**Neighborhood cleaning rule**: This combines undersampling and the **edited
    nearest neighbor** (**ENN**) method to remove majority class instances that are
    misclassified by the KNN classifier and the instances from the minority class
    that are misclassified.'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**邻域清洗规则**：这种方法结合了欠采样和**编辑最近邻**（**ENN**）方法，以去除被KNN分类器错误分类的多数类实例以及被错误分类的少数类实例。'
- en: '**Combining oversampling and undersampling**: Techniques can be used to both
    oversample the minority class and undersample the majority class to achieve a
    balance.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结合过采样和欠采样**：可以使用技术来同时过采样少数类和欠采样多数类以达到平衡。'
- en: '**Ensemble resampling**: This involves creating multiple balanced subsets through
    resampling and building an ensemble of models.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成重采样**：这涉及到通过重采样创建多个平衡子集，并通过构建模型集成。'
- en: While resampling methods can help balance the class distribution, they may not
    always improve model performance, especially in terms of calibration. Evaluating
    models on a separate, untouched validation set and considering other strategies
    such as choosing appropriate evaluation metrics is crucial.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然重采样方法可以帮助平衡类分布，但它们并不总是能提高模型性能，特别是在校准方面。在单独的、未受干扰的验证集上评估模型，并考虑其他策略，如选择适当的评估指标，是至关重要的。
- en: While resampling methods such as SMOTE have been accepted for many years as
    potential solutions, there is no evidence that such methods work across a wide
    range of datasets. For example, in Kaggle competitions, SMOTE was never successfully
    used as part of winning solutions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像SMOTE这样的重采样方法多年来一直被视为潜在解决方案，但并没有证据表明这些方法在广泛的数据库集上都能有效工作。例如，在Kaggle竞赛中，SMOTE从未成功作为获胜方案的一部分被使用。
- en: 'Over the years, resampling methods, notably SMOTE, have been championed as
    potential solutions to the challenge of imbalanced datasets. However, a deeper
    dive into their effectiveness paints a more nuanced picture. Despite their widespread
    mention in literature and tutorials, there’s a conspicuous absence of empirical
    evidence supporting their efficacy across diverse datasets. A testament to this
    is the world of Kaggle competitions, where precision, innovation, and effectiveness
    are paramount. Notably, SMOTE and similar strategies have rarely, if ever, been
    components of winning solutions. This isn’t just a statistical anomaly or coincidence.
    It underscores a profound observation: while these methods might offer superficial
    relief in some contexts, they aren’t universally applicable or reliably effective.
    Any practitioner aiming for cutting-edge performance would do well to approach
    resampling methods with a healthy dose of skepticism and thorough validation.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，重采样方法，尤其是SMOTE，一直被推崇为解决不平衡数据集挑战的潜在解决方案。然而，对这些方法有效性的深入研究描绘出一幅更为复杂的图景。尽管它们在文献和教程中被广泛提及，但支持它们在多样化数据集上有效性的实证证据却明显不足。这一点在Kaggle竞赛的世界中得到了证明，在那里精确度、创新和有效性至关重要。值得注意的是，SMOTE和类似策略很少，如果不是从未，成为获胜方案的一部分。这不仅仅是一个统计异常或巧合。它强调了深刻的观察：虽然这些方法可能在某些情况下提供表面上的缓解，但它们并不普遍适用或可靠有效。任何追求尖端性能的从业者都应该以健康程度的怀疑态度和彻底的验证来对待重采样方法。
- en: 'The study *The harm of class imbalance corrections for risk prediction models:
    illustration and simulation using logistic* *regression* by Ruben Van Den Goorbergh,
    Maarten van Smeden, Dirk Timmerman, Ben Van Calster, investigates the impact of
    class imbalance adjustments on the performance of logistic regression models.
    The research scrutinizes conventional and ridge-penalized versions of the model,
    assessing how these corrections influence their discrimination ability, calibration
    accuracy, and classification effectiveness.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Ruben Van Den Goorbergh、Maarten van Smeden、Dirk Timmerman、Ben Van Calster的研究论文《使用逻辑回归对类别不平衡纠正对风险预测模型的影响：说明和模拟》探讨了类别不平衡调整对逻辑回归模型性能的影响。研究仔细审查了模型的常规和岭回归惩罚版本，评估这些纠正如何影响它们的歧视能力、校准准确性和分类有效性。
- en: The paper analyzed techniques such as random undersampling and SMOTE, leveraging
    both Monte Carlo simulations and a real-world case study on ovarian cancer diagnosis.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 论文分析了随机欠采样和SMOTE等技术，利用蒙特卡洛模拟和针对卵巢癌诊断的真实世界案例研究。
- en: Interestingly, while these corrective methods consistently resulted in miscalibrated
    models (with a pronounced overestimation of the likelihood of falling into the
    minority class), they didn’t necessarily enhance discrimination as measured by
    the area under the receiver operating characteristic curve. However, they did
    improve classification metrics such as sensitivity and specificity. Similar classification
    outcomes could be achieved simply by adjusting the probability threshold.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，尽管这些纠正方法一致导致模型校准不准确（对落入少数类别的可能性高估明显），但它们并不一定增强了通过接收者操作特征曲线下的面积来衡量的歧视能力。然而，它们确实提高了分类指标，如敏感性和特异性。仅通过调整概率阈值就可以实现类似的分类结果。
- en: The paper argues that class imbalance correction techniques can harm the performance
    of prediction models, particularly in terms of calibration. The research determined
    that an imbalance in outcomes does not necessarily pose an issue and that attempts
    to correct this imbalance could degrade the model’s performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 论文认为，类别不平衡纠正技术可能会损害预测模型的性能，特别是在校准方面。研究确定，结果的不平衡并不一定构成问题，而试图纠正这种不平衡可能会降低模型的性能。
- en: The paper’s findings underscore that class imbalance, in isolation, isn’t inherently
    problematic and that efforts to rectify it might inadvertently degrade model performance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的研究结果表明，类别不平衡本身并不固有地存在问题，而纠正它的努力可能会无意中降低模型性能。
- en: In data science, distinguishing between prediction and classification is pivotal.
    Classification often mandates a premature decision, merging prediction with the
    decision-making process, potentially sidelining the actual decision-makers’ considerations.
    This is especially true when the cost of incorrect decisions shifts or data sampling
    criteria change. On the other hand, predictions remain neutral, serving as tools
    for any decision-maker.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，区分预测和分类至关重要。分类往往要求做出过早的决定，将预测与决策过程合并，可能会忽视实际决策者的考虑。这在决策成本转移或数据采样标准改变时尤其如此。另一方面，预测保持中立，作为任何决策者的工具。
- en: In his article *Classification vs. Prediction* ([https://www.fharrell.com/post/classification/](https://www.fharrell.com/post/classification/)),
    Frank Harell argues that classification can lead to hasty decisions, and its application
    in machine learning is sometimes misguided. On the other hand, probability modeling
    quantifies underlying patterns, typically aligning more closely with the core
    objectives of a project.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的文章《分类与预测》([https://www.fharrell.com/post/classification/](https://www.fharrell.com/post/classification/))中，Frank
    Harell 认为，分类可能导致草率的决策，其在机器学习中的应用有时是误导性的。另一方面，概率建模量化了潜在的模式，通常更紧密地与项目的核心目标相一致。
- en: Classification is most apt when outcomes are clear-cut, and predictors offer
    near-certain outcomes. However, many machine learning enthusiasts lean toward
    classifiers, neglecting the richness of probabilistic thinking, which is deeply
    rooted in statistics. An example of this is the frequent misclassification of
    logistic regression as a mere classification tool when, in essence, it offers
    rich probability estimates.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当结果明确时，分类最为合适，预测者提供几乎确定的预测结果。然而，许多机器学习爱好者倾向于使用分类器，忽视了深深植根于统计学中的概率思维的丰富性。一个例子是将逻辑回归频繁误分类为仅仅是一种分类工具，而实际上它提供了丰富的概率估计。
- en: It’s a misconception that binary decisions necessitate binary classifications.
    Often, the decision might be to gather more data or to take a phased approach.
    For instance, a physician might opt for progressive treatment based on evolving
    symptoms rather than making a binary decision upfront.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 认为二元决策需要二元分类是一种误解。通常，决策可能是收集更多数据或采取分阶段的方法。例如，医生可能会根据不断发展的症状选择渐进式治疗，而不是一开始就做出二元决策。
- en: Consider a high-clarity scenario such as optical character recognition. Here,
    the outcome is primarily deterministic, and machine learning classifiers excel.
    However, probability estimates become crucial when there’s inherent variability,
    such as in predicting disease outcomes. They inherently provide error margins,
    aiding decision-makers in understanding the associated risks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个高清晰度场景，例如光学字符识别。在这里，结果主要是确定性的，机器学习分类器表现卓越。然而，当存在内在变异性时，例如在预测疾病结果时，概率估计变得至关重要。它们本质上提供了误差范围，帮助决策者理解相关的风险。
- en: There’s also a challenge with classifiers in imbalanced scenarios. For instance,
    in a dataset with an overwhelming majority of non-diseased patients, a naive classifier
    might label everyone as non-diseased, achieving high accuracy but failing in actual
    detection. Addressing this imbalance often involves practices such as subsampling,
    which can lead to more issues. Logistic regression, in contrast, can gracefully
    handle such situations by recalibrating for different datasets or prevalences.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在不平衡场景中，分类器也存在挑战。例如，在一个绝大多数为非疾病患者的数据集中，一个简单的分类器可能会将所有人标记为非疾病，从而实现高准确率，但在实际检测中却失败了。解决这种不平衡通常涉及诸如子采样等实践，这可能导致更多问题。相比之下，逻辑回归可以通过为不同的数据集或流行度重新校准来优雅地处理这种情况。
- en: The choice of accuracy metrics is also fundamental. Opting for simplistic accuracy
    measures can lead to misleading models. The focus should instead be on more nuanced
    and statistically sound accuracy scoring rules.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性指标的选取也是至关重要的。选择简单的准确性度量可能会导致误导性的模型。相反，应该关注更细微且统计上可靠的准确性评分规则。
- en: In conclusion, while classifiers might be suitable for deterministic scenarios
    with high-clarity outcomes, for most real-world situations with inherent variability
    and nuances, probability-based models, such as logistic regression, are more apt,
    versatile, and insightful.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，虽然分类器可能适用于具有高清晰度结果的确定性场景，但对于大多数具有内在变异性及细微差别的现实世界情况，基于概率的模型，如逻辑回归，更为合适、灵活且具有洞察力。
- en: The issue with resampling methods is that they destroy calibration, which is
    critical for decision-making; the resampling techniques do not add any new information.
    The general acceptance of the SMOTE paper that has received over 25K citations
    is very unfortunate, especially considering that the paper is 20 years old, used
    only a few datasets, and performed experiments using weak classifiers such as
    C4.5 (decision tree classifier), Ripper (rule-based algorithm), and a naïve Bayes
    classifier.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 重采样方法的问题在于它们破坏了校准，这对于决策至关重要；重采样技术并没有增加任何新信息。SMOTE论文被广泛接受，并获得了超过25K次的引用，这是非常不幸的，特别是考虑到该论文已经20年历史，仅使用了几个数据集，并使用了一些弱分类器，如C4.5（决策树分类器）、Ripper（基于规则的算法）和朴素贝叶斯分类器。
- en: The paper also concentrated on inappropriate metrics, focusing solely on the
    **area under the curve** (**AUC**) and the ROC convex hull without considering
    metrics that measure classifier calibration. Consequently, the paper failed to
    report the adverse effects on calibration caused by SMOTE.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文还关注了不适当的指标，仅关注**曲线下面积**（**AUC**）和ROC凸包，而没有考虑测量分类器校准的指标。因此，该论文未能报告SMOTE对校准产生的负面影响。
- en: In the following section, we’ll examine effective strategies to address the
    challenges of imbalanced datasets in machine learning.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨有效应对机器学习中不平衡数据集挑战的策略。
- en: The methods for solving imbalanced data
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决不平衡数据的方法
- en: 'Addressing the challenge of imbalanced data isn’t just about achieving a balanced
    class distribution; it’s about understanding the nuances of the problem and adopting
    a holistic approach that encompasses all facets of model performance. Let us go
    through the methods for it:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 解决不平衡数据的问题不仅仅是实现平衡的类别分布；它涉及到理解问题的细微差别，并采取涵盖模型性能所有方面的整体方法。让我们来探讨这些方法：
- en: '**Understanding the problem**: The first step is a deep understanding of the
    problem. It’s essential to discern why the data is imbalanced. Is it because of
    the nature of the data or perhaps due to some external factors or biases in data
    collection? Recognizing the root cause can offer insights into the most effective
    strategies.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解问题**：第一步是对问题有深入的理解。重要的是要辨别数据不平衡的原因。是因为数据的性质，还是由于数据收集中的某些外部因素或偏差？识别根本原因可以为最有效的策略提供见解。'
- en: '**Prioritizing calibration**: One critical aspect that’s often overlooked is
    calibration. A model’s ability to provide probability estimates that reflect true
    likelihoods is paramount, especially when decisions are based on these probabilities.
    Ensuring the model is well calibrated is often more crucial than mere class separation.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优先考虑校准**：一个经常被忽视的关键方面是校准。模型提供反映真实可能性的概率估计的能力至关重要，尤其是在基于这些概率做出决策时。确保模型校准良好通常比单纯的类别分离更为关键。'
- en: '**Metrics beyond ROC AUC**: While the **receiver operating characteristic area
    under the curve** (**ROC AUC**) is a popular metric, relying solely on it can
    be misleading, especially in imbalanced datasets. It’s pivotal to incorporate
    metrics that capture the essence of calibration. Metrics such as **expected calibration
    error** (**ECE**), log loss, and Brier score, which we’ve looked into in previous
    chapters, provide a more comprehensive understanding of a model’s performance.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超越ROC AUC的指标**：虽然**接收者操作特征曲线下的面积**（ROC AUC）是一个流行的指标，但仅仅依赖它可能会误导，尤其是在数据不平衡的数据集中。纳入能够捕捉校准本质的指标至关重要。例如，**预期校准误差**（ECE）、对数损失和Brier分数等指标，我们在前面的章节中已经探讨过，它们提供了对模型性能的更全面理解。'
- en: '**Resampling techniques**: While techniques such as oversampling, undersampling,
    and SMOTE have been propagated as potential solutions, it’s crucial to understand
    their implications. While they might balance class distributions, they may not
    always improve or even maintain a model’s calibration. Therefore, any resampling
    should be performed cautiously, and the resulting models should be rigorously
    evaluated on untouched validation sets.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重采样技术**：尽管过采样、欠采样和SMOTE等技术被宣传为潜在解决方案，但理解它们的含义至关重要。虽然它们可能平衡类分布，但它们不一定总是改善或维持模型的校准。因此，任何重采样都应该谨慎进行，并且结果模型应该在未接触的验证集上严格评估。'
- en: '**Cost-sensitive learning**: Another approach is to assign different costs
    to misclassifications of the minority and majority classes. By doing so, the algorithm
    inherently gives more weight to the minority class during training, aiming to
    reduce the more costly errors.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本敏感学习**：另一种方法是给少数类和多数类的误分类分配不同的成本。通过这样做，算法在训练过程中内在地给予少数类更多权重，旨在减少更昂贵的错误。'
- en: '**Threshold tuning**: By adjusting the decision threshold away from the default
    (usually 0.5 for binary classification), one can perform better in the minority
    class. It’s about finding a balance between precision and recall, and this technique
    can be particularly effective when the real-world costs of false positives and
    false negatives are different.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阈值调整**：通过调整决策阈值（对于二分类通常为默认值0.5）来远离，可以在少数类中表现更好。这关乎于在精确度和召回率之间找到平衡，当现实世界中假阳性与假阴性的成本不同时，这种技术尤其有效。'
- en: Ultimately, the goal is to build effective models differentiating classes and
    offering calibrated reliable probability estimates. A multifaceted approach emphasizing
    understanding, calibration, and the right metrics is the way to tackle the imbalanced
    data problem.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最终目标是通过构建能够区分类别并提供校准可靠概率估计的有效模型。强调理解、校准和正确指标的多方面方法是解决数据不平衡问题的途径。
- en: Next, we’ll explore how conformal prediction can be applied to help solve imbalanced
    data problem and offer insights into its potential to enhance data analysis.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何将符合性预测应用于帮助解决数据不平衡问题，并探讨其增强数据分析的潜力。
- en: Solving imbalanced data problems by applying conformal prediction
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过应用符合性预测解决数据不平衡问题
- en: 'Conformal prediction is a technique that can be applied to handle imbalanced
    data problems. Here are a few ways it can be used:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 符合性预测是一种可以应用于处理数据不平衡问题的技术。以下是它可以用作的一些方式：
- en: '**Graceful handling of imbalanced datasets**: conformal prediction can gracefully
    handle large imbalanced datasets. It strictly defines the level of similarity
    needed, removing any ambiguity. It can handle severely imbalanced datasets with
    ratios of 1:100 to 1:1000 without oversampling or undersampling.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优雅地处理不平衡数据集**：一致性预测可以优雅地处理大型不平衡数据集。它严格定义了所需的相似度水平，消除了任何歧义。它可以处理严重不平衡的数据集，其比例为1:100到1:1000，而无需过采样或欠采样。'
- en: '**Local clustering conformal prediction** (**LCCP**): LCCP incorporates a dual-layer
    partitioning approach within the conformal prediction framework. Initially, it
    segments the imbalanced training dataset into subsets based on class taxonomy.
    Then, it further divides the examples from the majority class into subsets using
    clustering techniques. The goal of LCCP is to offer reliable confidence levels
    for its predictions while also enhancing the efficiency of the prediction process.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部聚类一致性预测**（**LCCP**）：LCCP在一致性预测框架内采用双层分区方法。最初，它根据类别分类将不平衡的训练数据集分割成子集。然后，它进一步使用聚类技术将多数类别的示例进一步分割成子集。LCCP的目标是在提高预测过程效率的同时，为其预测提供可靠的置信水平。'
- en: '**Mondrian conformal prediction** (**MCP**): This can deal with imbalanced
    datasets. It categorizes data based on their respective labels and assigns a distinct
    significance level to each class, ensuring that predictive validity is maintained
    across different classes.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蒙德里安一致性预测**（**MCP**）：这可以处理不平衡数据集。它根据各自标签对数据进行分类，并为每个类别分配一个独特的显著性水平，确保在不同类别之间保持预测的有效性。'
- en: '**Non-conformity scoring**: The core of conformal prediction is the non-conformity
    measure, which ranks new observations based on how “strange” they appear compared
    to the training data. This measure can be adapted for imbalanced datasets to give
    more weight to the minority class, ensuring that the model is more sensitive to
    the patterns associated with this class.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非一致性评分**：一致性预测的核心是非一致性度量，它根据新观察结果相对于训练数据“奇怪”的程度进行排名。这个度量可以适应不平衡数据集，为少数类赋予更多权重，确保模型对与该类相关的模式更加敏感。'
- en: '**Calibration with validity**: conformal prediction guarantees that if we claim
    a prediction interval with a 95% confidence level, it will contain the actual
    outcome 95% of the time in the long run. This built-in calibration, maintained
    even for imbalanced datasets, ensures that the prediction intervals or sets genuinely
    reflect the model’s uncertainty.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有有效性的校准**：一致性预测保证如果我们声称一个预测区间具有95%的置信水平，那么在长期来看，它将包含实际结果95%的时间。这种内置校准，即使在不平衡数据集中也能保持，确保预测区间或集合真正反映了模型的不确定性。'
- en: '**Flexibility with underlying models**: conformal prediction is not tied to
    a specific machine learning algorithm. This means that, even in the context of
    imbalanced data, practitioners can choose the best-performing base model (a tree-based
    method, neural network, or linear model) and then apply the conformal framework
    to obtain reliable predictions.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与底层模型的灵活性**：一致性预测并不局限于特定的机器学习算法。这意味着，即使在处理不平衡数据的情况下，从业者也可以选择表现最佳的基模型（基于树的算法、神经网络或线性模型），然后应用一致性框架以获得可靠的预测。'
- en: '**Transparency and interpretability**: The conformal prediction framework’s
    transparent nature allows straightforward interpretation. This transparency can
    be invaluable for imbalanced datasets, enabling stakeholders to understand why
    specific predictions are made and how certain the model is about them.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明性和可解释性**：一致性预测框架的透明性质允许简单的解释。这种透明性对于不平衡数据集来说非常有价值，使利益相关者能够理解为什么做出特定的预测，以及模型对这些预测的确定性。'
- en: '**Adaptive to changing distributions**: One of the challenges with imbalanced
    data is that the distribution of the minority class can change over time. With
    its emphasis on ranking new observations based on their non-conformity, conformal
    prediction can adapt to these changes, ensuring that predictions remain calibrated
    even as the underlying data distribution evolves.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适应变化的分布**：不平衡数据的一个挑战是少数类的分布可能会随时间变化。由于它强调根据新观察的非一致性进行排名，一致性预测可以适应这些变化，确保即使底层数据分布演变，预测仍然保持校准。'
- en: conformal prediction provides a framework that can be adapted to handle imbalanced
    datasets in various ways, offering potential solutions to this common problem
    in machine learning. While classification is now commonplace, the ultimate goal
    is enabling informed decisions, which requires reliable probability estimates
    even with skewed class data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性预测提供了一种框架，可以适应以各种方式处理不平衡数据集，为机器学习中这个常见问题提供潜在解决方案。虽然分类现在很常见，但最终目标是实现明智的决策，这需要即使在有偏的类别数据中也能提供可靠的概率估计。
- en: Addressing imbalanced data with Venn-Abers predictors
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Venn-Abers 预测器解决不平衡数据
- en: In the ever-evolving world of machine learning, addressing classification problems
    has become commonplace. From distinguishing between cats and dogs to more intricate
    challenges, the real aim of classification isn’t merely labeling; it’s also facilitating
    informed decision-making. For this purpose, more than just class labels are required.
    We need well-calibrated class probabilities.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习不断发展的世界中，解决分类问题已经变得很常见。从区分猫和狗到更复杂的挑战，分类的真正目的不仅仅是标记；它还在于促进明智的决策。为此，仅仅类别标签是不够的。我们需要良好的类别概率校准。
- en: Most data scientists, especially those in the early stages of their journey,
    tend to evaluate classification models using standard metrics such as accuracy,
    precision, and recall. While these metrics are insightful for more straightforward
    tasks, they can be misleading for more intricate, real-world problems. The true
    essence of classification lies in calibration, an aspect often overlooked in introductory
    courses.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据科学家，尤其是在他们职业生涯早期的人，倾向于使用标准指标如准确率、精确率和召回率来评估分类模型。虽然这些指标对于更直接的任务很有洞察力，但它们对于更复杂、更现实的问题可能会产生误导。分类的真正本质在于校准，这是入门课程中经常被忽视的一个方面。
- en: For professionals working on critical applications, from finance to healthcare,
    the calibration of classifiers is paramount. The heart of a classification problem
    is to make informed decisions. These decisions revolve around the probabilities
    of various scenarios, each with potential costs and benefits.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从事关键应用（从金融到医疗保健）的专业人士来说，分类器的校准至关重要。分类问题的关键在于做出明智的决策。这些决策围绕着各种场景的概率，每种场景都有潜在的成本和收益。
- en: Take the banking sector, for instance. If a model merely predicts that a potential
    customer won’t default on a loan, it needs to provide more depth for decision-making,
    especially when substantial amounts of money are at stake. What’s needed is a
    model that offers well-calibrated probabilities of various outcomes, allowing
    for a nuanced evaluation of risks and rewards.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以银行业为例。如果一个模型仅仅预测一个潜在客户不会违约，它需要为决策提供更多深度，尤其是在涉及大量资金的情况下。所需的是一个提供各种结果良好校准概率的模型，以便对风险和回报进行细致的评价。
- en: 'However, a significant challenge arises: many machine learning models don’t
    inherently produce class probabilities. Even if they do, these probabilities can
    often be miscalibrated, leading to erroneous decision-making. This is particularly
    concerning in critical sectors. For example, a self-driving car that misinterprets
    an obstacle due to miscalibrated probabilities could result in accidents.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，出现了一个重大挑战：许多机器学习模型本身并不产生类别概率。即使它们能产生，这些概率也常常被校准不当，导致错误的决策。这在关键领域尤其令人担忧。例如，由于概率校准不当而误解障碍物的自动驾驶汽车可能导致事故。
- en: So, what can be done to achieve better calibration? Classic methods, such as
    Platt’s scaling ([https://en.wikipedia.org/wiki/Platt_scaling](https://en.wikipedia.org/wiki/Platt_scaling))
    and isotonic regression ([https://en.wikipedia.org/wiki/Isotonic_regression](https://en.wikipedia.org/wiki/Isotonic_regression)),
    were early solutions. However, these methods have limitations, often rooted in
    restrictive assumptions that hamper their efficacy across diverse datasets.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何才能实现更好的校准呢？经典方法，如 Platt 的缩放法([https://en.wikipedia.org/wiki/Platt_scaling](https://en.wikipedia.org/wiki/Platt_scaling))和等调回归([https://en.wikipedia.org/wiki/Isotonic_regression](https://en.wikipedia.org/wiki/Isotonic_regression))，是早期的解决方案。然而，这些方法存在局限性，通常根源于限制性假设，这阻碍了它们在多样化数据集上的有效性。
- en: Enter **Venn-Abers predictors**, a beacon of hope in classifier calibration.
    Venn-Abers predictors, a subset of the conformal prediction framework, promise
    a more robust approach to calibration. Unlike traditional methods, they don’t
    hinge on overly simplistic assumptions and offer a more versatile calibration
    tool apt for today’s complex datasets.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 进入**Venn-Abers预测器**，这是分类器校准中的希望灯塔。Venn-Abers预测器，符合性预测框架的一个子集，承诺了一种更稳健的校准方法。与传统的不同，它们不依赖于过于简化的假设，并提供了适用于当今复杂数据集的更通用的校准工具。
- en: In essence, if you aim to harness the true potential of machine learning classifiers
    in 2022 and beyond, Venn-Abers and the broader conformal prediction framework
    are worth exploring. They might be the key to unlocking well-calibrated, reliable
    machine learning models.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，如果你想在2022年及以后充分利用机器学习分类器的潜力，Venn-Abers和更广泛的符合性预测框架值得探索。它们可能是解锁良好校准、可靠的机器学习模型的关键。
- en: Venn-Abers predictors stand out in machine learning, offering probability-driven
    predictions for test data labels. What sets them apart is their built-in assurance
    of calibration. This assurance is grounded in the typical premise that data observations
    are independently sourced from a consistent distribution.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Venn-Abers预测器在机器学习中脱颖而出，为测试数据标签提供概率驱动预测。使它们与众不同的地方在于它们内置的校准保证。这种保证建立在数据观察通常独立来源于一致分布的典型前提之上。
- en: At its core, the Venn-Abers approach is inspired by isotonic regression. It
    refines the probabilistic prediction calibration method pioneered by Zadrozny
    and Elkan. In contrast to techniques such as Platt’s scaler and isotonic regression,
    Venn-Abers predictors come equipped with inherent mathematical proofs, ensuring
    their unbiased validity.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Venn-Abers方法的核心灵感来源于等调回归。它精炼了Zadrozny和Elkan开创的概率预测校准方法。与Platt的缩放器、等调回归等技术相比，Venn-Abers预测器配备了固有的数学证明，确保其无偏有效性。
- en: 'An intriguing feature of Venn-Abers predictors is their ability to produce
    dual probability predictions for the *class 1* label. This dual output captures
    the range of prediction uncertainty. As a result, these predictors offer calibrated
    predictions and shed light on the inherent confidence associated with each prediction.
    This makes them invaluable tools for enhancing the calibration of probability-based
    predictions. Here’s how:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Venn-Abers预测器的一个有趣特性是它们能够为*类别1*标签生成双重概率预测。这种双重输出捕捉了预测不确定性的范围。因此，这些预测器提供了校准预测并揭示了与每个预测相关的内在置信度。这使得它们成为增强基于概率预测校准的无价工具。以下是方法：
- en: '**True-to-life probability intervals**: Venn-Abers predictors shine in delivering
    calibrated probability intervals. This ensures that the probabilities they produce
    genuinely represent the actual chances of an event, eliminating the pitfalls of
    overconfidence or underestimation.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真实生活的概率区间**：Venn-Abers预测器在提供校准概率区间方面表现出色。这确保了它们产生的概率真正代表了事件的实际可能性，消除了过度自信或低估的陷阱。'
- en: '**Versatility across models**: The beauty of Venn-Abers calibration is its
    adaptability. Whether you’re working with decision trees, random forests, or even
    XGBoost models, Venn-Abers can recalibrate them, fine-tuning overambitious and
    cautious models to enhance their accuracy.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨模型的通用性**：Venn-Abers校准的美丽之处在于其适应性。无论你是在处理决策树、随机森林，甚至是XGBoost模型，Venn-Abers都可以重新校准它们，微调过于雄心勃勃和过于谨慎的模型以提高其准确性。'
- en: '**Enhanced decision support with valid intervals**: The predictors don’t just
    stop at labels. For every prediction, especially from typically complex models
    such as random forests and XGBoost, Venn-Abers offers a probability interval.
    The span of this interval serves as a barometer of the prediction’s reliability'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有有效区间的增强决策支持**：预测器不仅停留在标签上。对于每一次预测，尤其是来自通常复杂的模型（如随机森林和XGBoost）的预测，Venn-Abers提供概率区间。这个区间的跨度是预测可靠性的晴雨表。'
- en: Venn-Abers predictors are a beacon for those navigating the choppy waters of
    imbalanced data issues. They refine the predictive accuracy of various machine
    learning models and arm users with credible probability intervals, making decision-making
    more informed and confident.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Venn-Abers预测器对于那些在处理不平衡数据问题中航行的人来说是一盏灯塔。它们精炼了各种机器学习模型的预测准确性，并为用户提供可信的概率区间，使决策更加明智和自信。
- en: 'To illustrate the various issues in imbalanced data problems, we will use the
    following notebook: `https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_11.ipynb`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明不平衡数据问题中的各种问题，我们将使用以下笔记本：`https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_11.ipynb`
- en: This notebook will look at various methods for handling an imbalanced class
    problem and apply conformal prediction to calibrate class probabilities.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本笔记本将探讨处理不平衡类别问题的各种方法，并将符合性预测应用于校准类别概率。
- en: 'We will use the Credit Card Fraud Detection dataset from Kaggle: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Kaggle的信用卡欺诈检测数据集：https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud
- en: The dataset contains data on credit card transactions in September 2013 by cardholders
    in Europe. The transactions occurred over two days, with 492 fraudulent transactions
    out of 284,807 transactions. The dataset is highly imbalanced, with the positive
    class (fraudulent transactions) accounting for 0.17% of all transactions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含2013年9月欧洲持卡人的信用卡交易数据。交易发生在两天内，共有492笔欺诈交易，占284,807笔交易中的0.17%。数据集高度不平衡，正类（欺诈交易）占所有交易的0.17%。
- en: The dataset contains numerical features that are the result of PCA transformation;
    the original features have been withheld due to confidentiality and privacy issues.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含由PCA变换得到的数值特征；由于保密性和隐私问题，原始特征已被保留。
- en: 'Features `V1`, `V2`, ... `V28` are the principal components obtained using
    PCA:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 特征`V1`、`V2`、... `V28`是使用PCA获得的主成分：
- en: The only original features are `Time` and `Amount`
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 唯一保留的原始特征是`Time`和`Amount`
- en: The feature `Time` contains the time (in seconds) for each transaction relative
    to the first transaction in the dataset
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征`Time`包含每个交易相对于数据集中第一个交易的时间（以秒为单位）
- en: The feature `Amount` is the transaction amount
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征`Amount`是交易金额
- en: The `label` Class is the dependent variable that needs to be predicted (fraudulent
    transactions labeled with 1)
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label`类是需要预测的因变量（标记为1的欺诈交易）'
- en: We will use various classifiers, including popular classifiers such as XGBoost,
    LightGBM, CatBoost, Random Forest, and logistic regression.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用各种分类器，包括XGBoost、LightGBM、CatBoost、随机森林和逻辑回归等流行分类器。
- en: Key insights from the Credit Card Fraud Detection notebook
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信用卡欺诈检测笔记本的关键见解
- en: 'In our exploration of the Credit Card Fraud Detection dataset, we unearthed
    several pivotal insights that can reshape our approach to imbalanced data:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对信用卡欺诈检测数据集的探索中，我们发现了几个关键见解，这些见解可以重塑我们对不平衡数据的处理方法：
- en: '**Embracing simplicity**: The most effective strategy is often to leave the
    data untouched. Contrary to the push for intricate resampling techniques, a minimalist
    approach can sometimes yield superior results.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拥抱简单性**：最有效的策略通常是保持数据不变。与复杂的重采样技术相比，简约的方法有时可以产生更优的结果。'
- en: '**Reframing imbalance**: Rather than viewing imbalanced data as a dilemma needing
    a direct fix, it’s crucial to understand that the imbalance isn’t always the root
    issue. The quest shouldn’t be to balance the scales but to derive meaningful insights
    from the data, irrespective of distribution.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新审视不平衡**：与其将不平衡数据视为需要直接解决的困境，不如理解不平衡并不总是根本问题。追求的目标不应该是平衡天平，而应该从数据中提取有意义的见解，无论分布如何。'
- en: '**The power of robust metrics**: The choice of metrics can make or break your
    analysis. By employing a comprehensive set of metrics, you can accurately define
    the problem and pave the way for practical solutions.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稳健性度量指标的力量**：度量指标的选择可以决定你的分析成败。通过采用一套全面的度量指标，你可以准确定义问题并为实际解决方案铺平道路。'
- en: '**Calibration’s central role**: Calibration is non-negotiable in real-world
    decision-making scenarios, especially in critical applications. Accurate probability
    estimations are vital, ensuring decisions are based on reliable data.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**校准的核心作用**：在校准现实世界的决策场景中，特别是在关键应用中，校准是不可协商的。准确的概率估计至关重要，确保决策基于可靠的数据。'
- en: '**The double-edged sword of resampling**: While resampling methods might seem
    promising, they often compromise the model’s calibration. Our analysis demonstrated
    that such techniques could deteriorate calibration metrics such as ECE, log loss,
    and Brier score.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重采样的双刃剑**：虽然重采样方法可能看起来很有希望，但它们通常会损害模型的校准。我们的分析表明，这些技术可能会降低校准指标，如ECE、对数损失和Brier分数。'
- en: '**conformal prediction** **as a beacon**: Amid the challenges posed by imbalanced
    data and the potential pitfalls of resampling, conformal prediction emerges as
    a silver lining. It offers a reliable method to recalibrate probabilities, ensuring
    that even post-resampling, the data remains conducive for sound decision-making.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性预测** **作为灯塔**：在处理不平衡数据带来的挑战和重采样的潜在陷阱时，一致性预测成为一线希望。它提供了一种可靠的方法来校准概率，确保即使在重采样之后，数据仍然有利于明智的决策。'
- en: By internalizing these insights, we can approach imbalanced datasets with a
    refined perspective, prioritizing meaningful analysis over superficial fixes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过内化这些见解，我们可以以更精细的视角来处理不平衡数据集，优先考虑有意义的分析而非表面的修复。
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The challenge of imbalanced datasets in machine learning often results in biased
    predictions and compromised model outcomes. This chapter delves deep into the
    complexities of such datasets and illuminates the path through conformal prediction,
    a groundbreaking approach to handling these scenarios.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，不平衡数据集的挑战往往导致预测偏差和模型结果受损。本章深入探讨了这类数据集的复杂性，并通过一致性预测，一种处理这些场景的突破性方法，照亮了前进的道路。
- en: Traditional methods, such as resampling techniques, and metrics, such as ROC
    AUC, often fail to address the imbalances effectively. Furthermore, they can sometimes
    lead to even more skewed results. On the other hand, conformal prediction emerges
    as a robust solution, offering calibrated and reliable probability estimates.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法，如重采样技术以及度量标准，如ROC AUC，往往无法有效解决不平衡问题。此外，它们有时甚至会导致结果更加偏斜。另一方面，一致性预测作为一种稳健的解决方案，提供了校准和可靠的概率估计。
- en: The practical implications of these methods are illustrated using the Credit
    Card Fraud Detection dataset from Kaggle, an inherently imbalanced dataset. The
    exploration underscores the significance of understanding the data, using robust
    metrics, and the transformative potential of conformal prediction.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的实际应用通过Kaggle的信用卡欺诈检测数据集进行说明，这是一个固有的不平衡数据集。这种探索强调了理解数据、使用稳健的度量标准以及一致性预测的变革潜力。
- en: In essence, while imbalanced data presents challenges, practitioners can navigate
    toward calibrated and insightful predictions with the right tools such as conformal
    prediction.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，尽管不平衡数据带来挑战，但从业者可以使用如一致性预测等正确工具，朝着校准和有洞察力的预测方向前进。
- en: In the next chapter of this book, we will dive deep into the fascinating world
    of multi-class conformal prediction. This chapter will introduce you to various
    conformal prediction methods that can be effectively applied to multi-class classification
    problems.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的下一章中，我们将深入探讨多类一致性预测的迷人世界。本章将向您介绍各种可以有效地应用于多类分类问题的一致性预测方法。
