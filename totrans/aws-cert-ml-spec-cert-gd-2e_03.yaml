- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS Services for Data Migration and Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, you learned about several ways of storing data in
    AWS. In this chapter, you will explore the techniques for using that data and
    gaining some insight from the data. There are use cases where you have to process
    your data or load the data to a hive data warehouse to query and analyze the data.
    If you are on AWS and your data is in S3, then you have to create a table in hive
    on AWS EMR to query the data in the hive table. To provide the same functionality
    as a managed service, AWS has a product called Athena, where you create a data
    catalog and query your data on S3\. If you need to transform the data, then AWS
    Glue is the best option to transform and restore it to S3\. Imagine a use case
    where you need to stream data and create analytical reports on that data. For
    this, you can opt for AWS Kinesis Data Streams to stream data and store it in
    S3\. Using Glue, the same data can be copied to Redshift for further analytical
    utilization. AWS **Database Migration Service** (**DMS**) provides seamless migration
    of heterogeneous and homogeneous databases. This chapter will cover the following
    topics that are required for the purpose of the certification:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Glue to design ETL jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying S3 data using Athena
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming data through AWS Kinesis Data Streams and storing it using Kinesis
    Firehose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting data from on-premises locations to AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Migrating data to AWS and extending on-premises data centers to AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing data on AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can download the data used in the examples from GitHub, available here:
    [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating ETL jobs o****n AWS Glue**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a modern data pipeline, there are multiple stages, such as generating data,
    collecting data, storing data, performing ETL, analyzing, and visualizing. In
    this section, you will cover each of these at a high level and understand the
    **extract, transform, load (ETL)** process in depth:'
  prefs: []
  type: TYPE_NORMAL
- en: Data can be generated from several devices, including mobile devices or IoT,
    weblogs, social media, transactional data, and online games.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This huge amount of generated data can be collected by using polling services,
    through API gateways integrated with AWS Lambda to collect the data, or via streams
    such as AWS Kinesis, AWS-managed Kafka, or Kinesis Firehose. If you have an on-premises
    database and you want to bring that data to AWS, then you would choose AWS DMS
    for that. You can sync your on-premises data to Amazon S3, Amazon EFS, or Amazon
    FSx via AWS DataSync. AWS Snowball is used to collect/transfer data into and out
    of AWS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step involves storing data. You learned about some of the services
    to do this in the previous chapter, such as S3, EBS, EFS, RDS, Redshift, and DynamoDB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you know your data storage requirements, an ETL job can be designed to
    extract-transform-load or extract-load-transform your structured or unstructured
    data into the format you desire for further analysis. For example, you can use
    AWS Lambda to transform the data on the fly and store the transformed data in
    S3, or you can run a Spark application on an EMR cluster to transform the data
    and store it in S3 or Redshift or RDS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many services available in AWS for performing an analysis on transformed
    data, for example, EMR, Athena, Redshift, Redshift Spectrum, and Kinesis Analytics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the data is analyzed, you can visualize it using AWS QuickSight to understand
    the patterns or trends. Data scientists or machine learning professionals would
    want to apply statistical analysis to understand data distribution in a better
    way. Business users use statistical analysis to prepare reports. You will learn
    and explore various ways to present and visualize data in [*Chapter 5*](B21197_05.xhtml#_idTextAnchor638)*,
    Data Understanding* *and Visualization.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What you understood from the traditional data pipeline is that ETL is all about
    coding and maintaining code on the servers so that everything runs smoothly. If
    the data format changes in any way, then the code needs to be changed, and that
    results in a change to the target schema. If the data source changes, then the
    code must be able to handle that too, and it’s an overhead. *Should you write
    code to recognize these changes in data sources? Do you need a system to adapt
    to the change and discover the data for you?* The answer to these questions is
    yes, and to do so, you can use **AWS Glue**. Now, you will learn why AWS Glue
    is so popular.
  prefs: []
  type: TYPE_NORMAL
- en: '**Features of AWS Glue**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AWS Glue is a completely managed serverless ETL service on AWS. It has the
    following features:'
  prefs: []
  type: TYPE_NORMAL
- en: It automatically discovers and categorizes your data by connecting to the data
    sources and generates a data catalog.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services such as Amazon Athena, Amazon Redshift, and Amazon EMR can use the
    data catalog to query the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Glue generates the ETL code, which is an extension to Spark in Python or
    Scala, which can be modified, too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It scales out automatically to match your Spark application requirements for
    running the ETL job and loading the data into the destination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AWS Glue has the **Data Catalog**, and that’s the secret to its success. It
    helps with discovering data from data sources and understanding a bit about it:'
  prefs: []
  type: TYPE_NORMAL
- en: The Data Catalog automatically discovers new data and extracts schema definitions.
    It detects schema changes and version tables. It detects Apache Hive-style partitions
    on Amazon S3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Data Catalog comes with built-in classifiers for popular data types. Custom
    classifiers can be written using **Grok expressions**. The classifiers help to
    detect the schema.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glue crawlers can be run ad hoc or in a scheduled fashion to update the metadata
    in the Glue Data Catalog. Glue crawlers must be associated with an IAM role with
    sufficient access to read the data sources, such as Amazon RDS, Redshift, and
    S3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you now have a brief idea of what AWS Glue is used for, move on to run the
    following example to get your hands dirty.
  prefs: []
  type: TYPE_NORMAL
- en: '**Getting hands-on with AWS Glue Data Catalog components**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, you will create a job to copy data from S3 to Redshift by
    using AWS Glue. All my components were created in the `us-east-1` region. Start
    by creating a bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the AWS S3 console and create a bucket. I have named the bucket
    `aws-glue-example-01`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on `input-data`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate inside the folder and click on the `sales-records.csv` dataset. The
    data is available in the following GitHub location: [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03/AWS-Glue-Demo/input-data](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03/AWS-Glue-Demo/input-data).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you have the data uploaded in the S3 bucket, now create a VPC in which you
    will create your Redshift cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to the VPC console by accessing the [https://console.aws.amazon.com/vpc/home?region=us-east-1#](https://console.aws.amazon.com/vpc/home?region=us-east-1#)
    URL and click on `AWS services`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`com.amazonaws.us-east-1.s3` (gateway type)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**VPC**: Select **the Default VPC** (use this default VPC in which your Redshift
    cluster will be created)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leave the other fields as is and click on **Create Endpoint**.*   Click on `redshift-self`,
    and choose the default VPC drop-down menu. Provide an appropriate description,
    such as `Redshift Security Group`. Click on **Create** **security group**.*   Click
    on the `All traffic`*   `Custom`*   In the search field, select the same security
    group (`redshift-self`)*   Click on **Save Rules**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, create your Redshift cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to the Amazon Redshift console. Click on **Create Cluster** and complete
    the highlighted fields, as shown in *Figure 3**.1*:![Figure 3.1 – A screenshot
    of Amazon Redshift’s Create cluster screen](img/B21197_03_01.jpg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 3.1 – A screenshot of Amazon Redshift’s Create cluster screen
  prefs: []
  type: TYPE_NORMAL
- en: Scroll down and fill in the highlighted fields shown in *Figure 3**.2* with
    your own values:![Figure 3.2 – A screenshot of an Amazon Redshift cluster’s Database
    configurations section](img/B21197_03_02.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 3.2 – A screenshot of an Amazon Redshift cluster’s Database configurations
    section
  prefs: []
  type: TYPE_NORMAL
- en: Scroll down and change the **Additional configurations** settings, as shown
    in *Figure 3**.3*:![Figure 3.3 – A screenshot of an Amazon Redshift cluster’s
    Additional configurations section](img/B21197_03_03.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 3.3 – A screenshot of an Amazon Redshift cluster’s Additional configurations
    section
  prefs: []
  type: TYPE_NORMAL
- en: Change the IAM permissions too, as shown in *Figure 3**.4*:![Figure 3.4 – A
    screenshot of an Amazon Redshift cluster’s Cluster permissions section](img/B21197_03_04.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 3.4 – A screenshot of an Amazon Redshift cluster’s Cluster permissions
    section
  prefs: []
  type: TYPE_NORMAL
- en: Scroll down and click on **Create Cluster**. It will take a minute or two to
    get the cluster in the available state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, you will create an IAM role.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to the AWS IAM console and select **Roles** in the **Access Management**
    section on the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Create role** button and choose **Glue** from the services.
    Click on the **Next: permissions** button to navigate to the next page.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Search for `AmazonS3FullAccess` and select it. Then, search for `AWSGlueServiceRole`
    and select it. As you are writing your data to Redshift as part of this example,
    select **AmazonRedshiftFullAccess**. Click on **Next: Tags**, followed by the
    **Next:** **Review** button.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a name, `Glue-IAM-Role`, and then click on the **Create role** button.
    The role appears as shown in *Figure 3**.5*:![Figure 3.5 – A screenshot of the
    IAM role](img/B21197_03_05.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 3.5 – A screenshot of the IAM role
  prefs: []
  type: TYPE_NORMAL
- en: Now, you have the input data source and the output data storage handy. The next
    step is to create the Glue crawler from the AWS Glue console.
  prefs: []
  type: TYPE_NORMAL
- en: Select `glue-redshift-connection`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Amazon Redshift`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on `redshift-glue-example`*   `glue-dev`*   `awsuser`*   `********` (enter
    the value chosen in *step 10*)*   Click on **Next** and then **Finish**. To verify
    that it’s working, click on **Test Connection**, select **Glue-IAM-Role** in the
    IAM role section, and then click **Test Connection**.*   Go to Crawler and select
    Add Crawler. Provide a name for the crawler, S3-glue-crawler, and then click Next.
    On the Specify crawler source type page, leave everything as their default settings
    and then click Next.*   On the`s3://aws-glue-example-01/input-data/sales-records.csv`.*   Click
    **Next**.*   Set`No`. Click **Next**.*   For `Glue-IAM-Role`. Then, click **Next**.*   Set
    **Frequency** to **Run on demand**. Click **Next**.*   No database has been created,
    so click on `s3-data`, click **Next**, and then click **Finish**.*   Select the
    crawler, `s3-data`, has been created, as mentioned in the previous step, and a
    table has been added. Click on **Tables** and select the newly created table,
    **sales_records_csv**. You can see that the schema has been discovered now. You
    can change the data type if the inferred data type does not meet your requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this hands-on section, you learned about database tables, database connections,
    crawlers in S3, and the creation of a Redshift cluster. In the next hands-on section,
    you will learn about creating ETL jobs using Glue.
  prefs: []
  type: TYPE_NORMAL
- en: '**Getting hands-on with AWS Glue ETL components**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will use the Data Catalog components created earlier to
    build a job. You will start by creating a job:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the AWS Glue console and click on **Jobs** under the **ETL** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the `s3-glue-redshift`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Spark`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Glue version**: **Spark 2.4, Python 3 with improved job start up times (Glue**
    **version 2.0)**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leave the other fields as they are and then click on **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **sales_records_csv** and click on **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Change Schema** by default and then click **Next** (at the time of
    writing this book, machine learning transformations are not supported for Glue
    2.0).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `glue-dev` as the database name (as created in the previous section)
    and then click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next comes the **Output Schema Definition** page, where you can choose the desired
    columns to be removed from the target schema. Scroll down and click on **Save
    job and** **edit script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can now see the pipeline being created on the left-hand side of the screen
    and the suggested code on the right-hand side, as shown in *Figure 3**.6*. You
    can modify the code based on your requirements. Click on the **Run job** button.
    A pop-up window appears, asking you to edit any details that you wish to change.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is optional. Then, click on the **Run** **job** button:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.6 – A screenshot of the AWS Glue ETL job](img/B21197_03_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.6 – A screenshot of the AWS Glue ETL job
  prefs: []
  type: TYPE_NORMAL
- en: Once the job is successful, navigate to Amazon Redshift and click on **Query
    editor**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the database name as `glue-dev` and then provide the username and password
    to create a connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the `public` schema, and now you can query the table to see the records,
    as shown in *Figure 3**.7*:![Figure 3.7 – A screenshot of Amazon Redshift’s Query
    editor](img/B21197_03_07.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 3.7 – A screenshot of Amazon Redshift’s Query editor
  prefs: []
  type: TYPE_NORMAL
- en: You now understand how to create an ETL job using AWS Glue to copy the data
    from an S3 bucket to Amazon Redshift. You also queried data in Amazon Redshift
    using the query editor from the UI console. It is recommended to delete the Redshift
    cluster and AWS Glue job once you have completed the steps successfully. AWS creates
    two buckets in your account to store AWS Glue scripts and temporary results from
    AWS Glue. Delete these as well to save costs. You will use the data catalog created
    on S3 data in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, you will learn about querying S3 data using Athena.
  prefs: []
  type: TYPE_NORMAL
- en: '**Querying S3 data us****ing Athena**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Athena is a serverless service designed for querying data stored in S3\. It
    is serverless because the client doesn’t manage the servers that are used for
    computation:'
  prefs: []
  type: TYPE_NORMAL
- en: Athena uses a schema to present the results against a query on the data stored
    in S3\. You define how (the way or the structure) you want your data to appear
    in the form of a schema and Athena reads the raw data from S3 to show the results
    as per the defined schema.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output can be used by other services for visualization, storage, or various
    analytics purposes. The source data in S3 can be in any of the following structured,
    semi-structured, or unstructured data formats: XML, JSON, CSV/TSV, AVRO, Parquet,
    or ORC (as well as others). CloudTrail, ELB logs, and VPC flow logs can also be
    stored in S3 and analyzed by Athena.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This follows the schema-on-read technique. Unlike traditional techniques, tables
    are defined in advance in a data catalog, and the data’s structure is validated
    against the table’s schema while reading the data from the tables. SQL-like queries
    can be carried out on data without transforming the source data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, to help you understand this, here’s an example, where you will use **AWSDataCatalog**
    created in AWS Glue on the S3 data and query them using Athena:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the AWS Athena console. Select `AWSDataCatalog` from `sampledb`
    database will be created with a table, `elb_logs`, in the AWS Glue Data Catalog).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `s3-data` as the database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Settings** in the top-right corner and fill in the details as shown
    in *Figure 3**.8* (I have used the same bucket as in the previous example and
    a different folder):![Figure 3.8 – A screenshot of Amazon Athena’s settings](img/B21197_03_08.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 3.8 – A screenshot of Amazon Athena’s settings
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to write your query in the query editor and execute it. Once
    your execution is complete, please delete your S3 buckets and AWS Glue data catalogs.
    This will save you money.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, you learned how to query S3 data using Amazon Athena through
    the AWS Glue Data Catalog. You also learned how to create a schema and query data
    from S3\. In the next section, you will learn about Amazon Kinesis Data Streams.
  prefs: []
  type: TYPE_NORMAL
- en: '**Processing real-time data using Kinesis Da****ta Streams**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kinesis is Amazon’s streaming service and can be scaled based on requirements.
    It has a level of persistence that retains data for 24 hours by default or optionally
    up to 365 days. Kinesis Data Streams is used for large-scale data ingestion, analytics,
    and monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: Kinesis streams can be ingested by multiple producers and multiple consumers
    can also read data from the streams. The following is an example to help you understand
    this. Suppose you have a producer ingesting data to a Kinesis stream and the default
    retention period is 24 hours, which means data ingested at 05:00:00 A.M. today
    will be available in the stream until 04:59:59 A.M. tomorrow. This data won’t
    be available beyond that point, and ideally, it should be consumed before it expires;
    otherwise, it can be stored somewhere if it’s critical. The retention period can
    be extended to a maximum of 365 days, at an extra cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kinesis can be used for real-time analytics or dashboard visualization. Producers
    can be imagined as a piece of code pushing data into the Kinesis stream, and it
    can be an EC2 instance, a Lambda function, an IoT device, on-premises servers,
    mobile applications or devices, and so on running the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, the consumer can also be a piece of code running on an EC2 instance,
    Lambda function, or on-premises servers that know how to connect to a Kinesis
    stream, read the data, and apply some action to the data. AWS provides triggers
    to invoke a Lambda consumer as soon as data arrives in the Kinesis stream.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kinesis is scalable due to its shard architecture, which is the fundamental
    throughput unit of a Kinesis stream. *What is a shard?* A shard is a logical structure
    that partitions the data based on a partition key. A shard supports a writing
    capacity of *1 MB/sec* and a reading capacity of *2 MB/sec*. 1,000 `PUT` records
    per second are supported by a single shard. If you have created a stream with
    *three shards*, then *3 MB/sec write throughput* and *6 MB/sec read throughput*
    can be achieved, and this allows 3,000 `PUT` records. So, with more shards, you
    have to pay an extra amount to get higher performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data in a shard is stored via a Kinesis data record and can be a maximum
    of 1 MB. Kinesis data records are stored across the shard based on the partition
    key. They also have a sequence number. A sequence number is assigned by Kinesis
    as soon as a `putRecord` or `putRecords` API operation is performed so as to uniquely
    identify a record. The partition key is specified by the producer while adding
    the data to the Kinesis data stream, and the partition key is responsible for
    segregating and routing the record to different shards in the stream to balance
    the load.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two ways to encrypt the data in a Kinesis stream: server-side encryption
    and client-side encryption. Client-side encryption makes it hard to implement
    and manage the keys because the client has to encrypt the data before putting
    it into the stream and decrypt the data after reading it from the stream. With
    server-side encryption enabled via **AWS KMS**, the data is automatically encrypted
    and decrypted as you put the data and get it from a stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kinesis shouldn’t be confused with Amazon SQS. Amazon SQS supports one
    production group and one consumption group. If your use case demands multiple
    users sending data and receiving data, then Kinesis is the solution.
  prefs: []
  type: TYPE_NORMAL
- en: For decoupling and asynchronous communications, SQS is the solution, because
    the sender and receiver do not need to be aware of one another.
  prefs: []
  type: TYPE_NORMAL
- en: In SQS, there is no concept of persistence. Once the message is read, the next
    step is deletion. There’s no concept of a retention time window for Amazon SQS.
    If your use case demands large-scale ingestion, then Kinesis should be used.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn about storing the streamed data for further
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '**Storing and transforming real-time data using Kinesis Dat****a Firehose**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a lot of use cases that require data to be streamed and stored for
    future analytics purposes. To overcome such problems, you can write a Kinesis
    consumer to read the Kinesis stream and store the data in S3\. This solution needs
    an instance or a machine to run the code with the required access to read from
    the stream and write to S3\. The other possible option would be to run a Lambda
    function that gets triggered on the `putRecord` or `putRecords` API made to the
    stream and reads the data from the stream to store in the S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: To make this easy, Amazon provides a separate service called Kinesis Data Firehose.
    This can easily be plugged into a Kinesis data stream and will require essential
    IAM roles to write data into S3\. It is a fully managed service to reduce the
    load of managing servers and code. It also supports loading the streamed data
    into Amazon Redshift, Elasticsearch, and Splunk. Kinesis Data Firehose scales
    automatically to match the throughput of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data can be transformed via an AWS Lambda function before storing or delivering
    it to the destination. If you want to build a raw data lake with the untransformed
    data, then by enabling source record backup, you can store it in another S3 bucket
    prior to the transformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the help of AWS KMS, data can be encrypted following delivery to the S3
    bucket. It has to be enabled while creating the delivery stream. Data can also
    be compressed in supported formats, such as gzip, ZIP, or Snappy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, you will learn about different AWS services used for ingesting
    data from on-premises servers to AWS.
  prefs: []
  type: TYPE_NORMAL
- en: '**Different ways of ingesting data from on****-premises** **into AWS**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the increasing demand for data-driven use cases, managing data on on-premises
    servers is pretty tough at the moment. Taking backups is not easy when you deal
    with a huge amount of data. This data in data lakes is used to build deep neural
    networks, create a data warehouse to extract meaningful information from it, run
    analytics, and generate reports.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you look at the available options to migrate data into AWS, this comes
    with various challenges too. For example, if you want to send data to S3, then
    you have to write a few lines of code to send your data to AWS. You will have
    to manage the code and servers to run the code. It has to be ensured that the
    data is commuting via the HTTPS network. You need to verify whether the data transfer
    was successful. This adds complexity as well as time and effort challenges to
    the process. To avoid such scenarios, AWS provides services to match or solve
    your use cases by designing a hybrid infrastructure that allows data sharing between
    the on-premises data centers and AWS. You will learn about these in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Storage Gateway**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Storage Gateway is a hybrid storage virtual appliance. It can run in three
    different modes – **File Gateway**, **Tape Gateway**, and **Volume Gateway**.
    It can be used for the extension, migration, and backups of an on-premises data
    center to AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: In Tape Gateway mode, Storage Gateway stores virtual tapes on S3, and when ejected
    and archived, the tapes are moved from S3 to Glacier. Active tapes are stored
    in S3 for storage and retrieval. Archived or exported tapes are stored in **Virtual
    Tape Shelf** (**VTS**) in Glacier. Virtual tapes can be created and can range
    in size from 100 GiB to 5 TiB. A total of 1 petabyte of storage can be configured
    locally and an unlimited number of tapes can be archived to Glacier. This is ideal
    for an existing backup system on tape and where there is a need to migrate backup
    data into AWS. You can decommission the physical tape hardware later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In File Gateway mode, Storage Gateway maps files onto S3 objects, which can
    be stored using one of the available storage classes. This helps you to extend
    the data center into AWS. You can load more files to your file gateway and these
    are stored as S3 objects. It can run on your on-premises virtual server, which
    connects to various devices using **Server Message Block (SMB)** or **Network
    File System (NFS)**. File Gateway connects to AWS using an HTTPS public endpoint
    to store the data on S3 objects. Life cycle policies can be applied to those S3
    objects. You can easily integrate your **Active** **Directory** (**AD**) with
    File Gateway to control access to the files on the file share.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Volume Gateway mode, the storage gateway presents block storage. There are
    two ways of using this; one is **Gateway Cached** and the other is **Gateway Stored**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gateway Stored** is a volume storage gateway running locally on-premises.
    It has local storage and an upload buffer. A total of 32 volumes can be created,
    and each volume can be up to 16 TB in size for a total capacity of 512 TB. Primary
    data is stored on-premises and backup data is asynchronously replicated to AWS
    in the background. Volumes are made available via **Internet Small Computer Systems
    Interface (iSCSI)** for network-based servers to access. It connects to a Storage
    Gateway endpoint via an HTTPS public endpoint and creates EBS snapshots from backup
    data. These snapshots can be used to create standard EBS volumes. This option
    is ideal for migration to AWS, disaster recovery, or business continuity. The
    local system will still use the local volume, but the EBS snapshots are in AWS,
    which can be used instead of backups. It’s not the best option for data center
    extensions because you require a huge amount of local storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gateway Cached** is a volume storage gateway running locally on-premises.
    It has cache storage and an upload buffer. The difference is that the data that
    is added to Storage Gateway is not local but uploaded to AWS. Primary data is
    stored in AWS. Frequently accessed data is cached locally. This is an ideal option
    for extending an on-premises data center to AWS. It connects to a Storage Gateway
    endpoint via an HTTPS public endpoint and creates S3-backed volume (AWS-managed
    bucket) snapshots that are stored as standard EBS snapshots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Snowball, Snowball Edge, and Snowmobile**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These belong to the same product category or family for the physical transfer
    of data between business operating locations and AWS. To move a large amount of
    data into and out of AWS, you can use any of the three:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Snowball**: This physical device can be ordered from AWS by logging a job.
    AWS delivers a device for you to load your data onto before sending it back. Data
    in Snowball is encrypted using KMS. It comes with two capacity ranges: 50 TB and
    80 TB. It is economical to order one or more Snowball devices for data between
    10 TB and 10 PB. The device can be sent to different premises. It does not have
    any compute capability; it only comes with storage capability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Snowball Edge**: This is like Snowball, but it comes with both storage and
    compute capability. It has a larger capacity than Snowball. It offers fastened
    networking, such as 10 Gbps over RJ45, 10/25 Gb over SFP28, and 40/100 Gb+ over
    QSFP+ copper. This is ideal for the secure and quick transfer of terabytes to
    petabytes of data into AWS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Snowmobile**: This is a portable data center within a shipping container
    on a truck. It allows you to move exabytes of data from on-premises to AWS. If
    your data size exceeds 10 PB, then Snowmobile is preferred. Essentially, upon
    requesting to use the Snowmobile service, a truck is driven to your location and
    you plug your data center into the truck and transfer the data. If you have multiple
    sites, choosing Snowmobile for data transfer is not an ideal option.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS DataSync**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AWS DataSync is designed to move data from on-premises storage to AWS, or vice
    versa:'
  prefs: []
  type: TYPE_NORMAL
- en: It is an ideal product from AWS for data processing transfers, archival or cost-effective
    storage, disaster recovery, business continuity, and data migrations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a special data validation feature that verifies the original data with
    the data in AWS, as soon as the data arrives in AWS. In other words, it checks
    the integrity of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To understand this product in depth, consider an example of an on-premises data
    center that has SAN/NAS storage. When you run the AWS DataSync agent on a VMWare
    platform, this agent is capable of communicating with the NAS/SAN storage via
    an NFS/SMB protocol. Once it is on, it communicates with the AWS DataSync endpoint,
    and from there, it can connect with several different types of locations, including
    various S3 storage classes or VPC-based resources, such as **Elastic** **File**
    **System** (**EFS**) and FSx for Windows Server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows you to schedule data transfers during specific periods. By configuring
    the built-in bandwidth throttle, you can limit the amount of network bandwidth
    that DataSync uses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Database Migration Service**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several situations when an organization might decide to migrate their
    databases from one to another, such as the need for better performance, enhanced
    security, or advanced features or to avoid licensing costs from vendors. If an
    organization wants to expand its business to a different geolocation, it will
    need to carry out database migration, disaster recovery improvements, and database
    sync in a cost-effective manner. AWS DMS allows you to leverage the benefits of
    scalability, flexibility, and cost-efficiency when migrating databases from an
    `on-premises/EC2 instance/Amazon RDS` to Amazon RDS or Amazon Aurora.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scenarios where multiple databases need to be consolidated into a single
    database or data needs to be integrated across multiple databases, AWS DMS can
    be a valuable tool. AWS DMS is designed to move data from a source to a target
    provided one of the endpoints is on AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: DMS supports both homogenous and heterogeneous database migrations, allowing
    you to migrate between different database engines, such as Oracle, MySQL, PostgreSQL,
    and Microsoft SQL Server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DMS simplifies the database migration process by handling schema conversion,
    data replication, and ongoing synchronization between the source and target databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DMS supports both full-load and ongoing **Change Data Capture** (**CDC**) replication
    methods. Full-load migration copies the entire source database to the target,
    while CDC captures and replicates only the changes made after the initial load.
    For example, for a database migration with minimal downtime, DMS performs an initial
    full-load migration, followed by CDC replication to keep the target database up
    to date with changes in the source database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DMS provides a user-friendly console and API for the easy configuration, monitoring,
    and management of database migrations. It offers detailed logging and error handling
    to help diagnose and resolve migration issues. For example, during a migration
    from Oracle to Amazon Aurora, DMS can automatically convert Oracle-specific data
    types and modify table structures to align with the Aurora database schema.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DMS supports continuous data replication, allowing you to keep the source and
    target databases in sync even after the initial migration. This is particularly
    useful in scenarios requiring ongoing data synchronization or database replication.
    An example is if a company maintains an active-active database setup for high
    availability, where DMS replicates data changes between multiple database instances
    located in different regions for real-time synchronization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DMS offers built-in validation and testing capabilities to ensure the integrity
    and consistency of the migrated data. It performs data validation checks and generates
    reports to verify the success of the migration process. For example, after migrating
    a large database from Microsoft SQL Server to Amazon RDS for PostgreSQL, DMS validates
    the migrated data by comparing row counts, data types, and other metrics to ensure
    data accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DMS supports both one-time migrations and continuous replication for database
    consolidation and integration scenarios. It enables organizations to consolidate
    data from multiple databases into a single target database or distribute data
    across multiple databases as needed. For example, say a company with several subsidiary
    databases wants to consolidate all the data into a centralized database for unified
    reporting and analysis. DMS facilitates the migration and ongoing synchronization
    of data from multiple sources to the target database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processing stored d****ata on AWS**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several services for processing the data stored in AWS. You will learn
    about AWS Batch and AWS `MapReduce` jobs and Spark applications in a managed way.
    AWS Batch is used for long-running, compute-heavy workloads.
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS EMR**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'EMR is a managed implementation of Apache Hadoop provided as a service by AWS.
    It includes other components of the Hadoop ecosystem, such as Spark, HBase, Flink,
    Presto, Hive, and Pig. You will not need to learn about these in detail for the
    certification exam, but here’s some information about EMR:'
  prefs: []
  type: TYPE_NORMAL
- en: EMR clusters can be launched from the AWS console or via the AWS CLI with a
    specific number of nodes. The cluster can be a long-term cluster or an ad hoc
    cluster. In a long-running traditional cluster, you have to configure the machines
    and manage them yourself. If you have jobs that need to be executed faster, then
    you need to manually add a cluster. In the case of EMR, these admin overheads
    disappear. You can request any number of nodes from EMR and it manages and launches
    the nodes for you. If you have autoscaling enabled on the cluster, EMR regulates
    nodes according to the requirement. That means, EMR launches new nodes in the
    cluster when the load is high and decommissions the nodes once the load is reduced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EMR uses EC2 instances in the background and runs in one Availability Zone in
    a VPC. This enables faster network speeds between the nodes. AWS Glue uses EMR
    clusters in the background, where users do not need to worry about having an operational
    understanding of AWS EMR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a use case standpoint, EMR can be used to process or transform the data
    stored in S3 and output data to be stored in S3\. EMR uses nodes (EC2 instances)
    as the computing units for data processing. EMR nodes come in different variants,
    including master nodes, core nodes, and task nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The EMR master node acts as a Hadoop NameNode and manages the cluster and its
    health. It is responsible for distributing the job workload among the other core
    nodes and task nodes. If you have SSH enabled, then you can connect to the master
    node instance and access the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An EMR cluster can have one or more core nodes. If you relate it to the Hadoop
    ecosystem, then core nodes are similar to Hadoop data nodes for HDFS and they
    are responsible for running the tasks within them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task nodes are optional and they don’t have HDFS storage. They are responsible
    for running tasks. If a task node fails for some reason, then this does not impact
    HDFS storage, but a core node failure causes HDFS storage interruptions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EMR has a filesystem called EMRFS. It is backed by S3, which makes it regionally
    resilient. If a core node fails, the data is still safe in S3\. HDFS is efficient
    in terms of I/O and faster than EMRFS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, you will learn about AWS Batch, which is a managed
    batch-processing compute service that can be used for long-running services.
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Batch**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is a managed batch-processing product. If you are using AWS Batch, then
    jobs can be run without end user interaction or can be scheduled to run:'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine an event-driven application that launches a Lambda function to process
    the data stored in S3\. If the processing time goes beyond 15 minutes, then Lambda
    stops the execution and fails. For such scenarios, AWS Batch is a better solution,
    where computation-heavy workloads can be scheduled or driven through API events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Batch is a good fit for use cases where a longer processing time is required
    or more computation resources are needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Batch jobs can be a script or an executable. One job can depend on another
    job. A job needs to be defined, such as who can run the job (with IAM permissions),
    where the job can be run (resources to be used), mount points, and other metadata.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jobs are submitted to queues, where they wait for compute environment capacity.
    These queues are associated with one or more compute environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute environments do the actual work of executing the jobs. These can be
    ECS or EC2 instances, or any computing resources. You can define their sizes and
    capacities too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environments receive jobs from the queues based on their priority and execute
    them. They can be managed or unmanaged compute environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Batch can store the metadata in DynamoDB for further use and can also store
    the output in an S3 bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: If you get a question in the exam on an event-style workload that requires flexible
    compute, a higher disk space, no time limit (more than 15 minutes), or an effective
    resource limit, then the answer is likely to be AWS Batch.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about different ways of processing data in AWS.
    You also learned the capabilities in terms of extending your data centers to AWS,
    migrating data to AWS, and the ingestion process. You learned about the various
    ways of using data to process it and make it ready for analysis. You understood
    the magic of using a data catalog, which helps you to query your data via AWS
    Glue and Athena.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about various machine learning algorithms
    and their usage.
  prefs: []
  type: TYPE_NORMAL
- en: Exam Readiness Drill – Chapter Review Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How To Access These Resources
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the link – [https://packt.link/MLSC01E2_CH03](https://packt.link/MLSC01E2_CH03).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, you can scan the following **QR code** (*Figure 3**.9*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.9 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – QR code that opens Chapter Review Questions for logged-in users
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 3**.10*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Chapter Review Questions for Chapter 3](img/B21197_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Chapter Review Questions for Chapter 3
  prefs: []
  type: TYPE_NORMAL
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exam Readiness Drill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first three attempts, don’t worry about the time limit.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  prefs: []
  type: TYPE_NORMAL
- en: Working On Timing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attempt** | **Score** | **Time Taken** |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  prefs: []
  type: TYPE_TB
- en: Table 3.1 – Sample timing practice drills on the online platform
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  prefs: []
  type: TYPE_NORMAL
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  prefs: []
  type: TYPE_NORMAL
