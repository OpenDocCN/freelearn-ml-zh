- en: Predicting User Behavior with Tree-Based Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will introduce decision trees, random forests, and gradient-boosted
    trees. Decision trees methodology is a popular technique used in data science
    that provides a visual representation of how the information in the training set
    can be represented as a hierarchy. Traversing the hierarchy based on an observation
    helps you to predict the probability of that event.  We will explore how to use
    these algorithms can be used to predict when a user may click on online advertisement
    based on existing advertising click records. Additionally, we will show how to
    use AWS **Elastic MapReduce** (**EMR**) with Apache Spark and the SageMaker XGBoost
    service to engineer models in the context of big data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding random forests algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding gradient boosting algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting clicks on log streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees graphically show the decisions to be made, the observed events
    that may occur, and the probabilities of the outcomes given a specific set of
    observable events occurring together. Decision trees are used as a popular machine
    learning algorithm, where, based on a dataset of observable events and the known
    outcomes, we can construct a decision tree that can represent the probability
    of an event occurring.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows a very simple example of how decision trees can be
    generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Car Make** | **Year** | **Price** |'
  prefs: []
  type: TYPE_TB
- en: '| BMW | 2015 | >$40K |'
  prefs: []
  type: TYPE_TB
- en: '| BMW | 2018 | >$40K |'
  prefs: []
  type: TYPE_TB
- en: '| Honda  | 2015 | <$40K |'
  prefs: []
  type: TYPE_TB
- en: '| Honda  | 2018 | >$40K |'
  prefs: []
  type: TYPE_TB
- en: '| Nissan | 2015 | <$40K |'
  prefs: []
  type: TYPE_TB
- en: '| Nissan | 2018 | >$40K |'
  prefs: []
  type: TYPE_TB
- en: 'This is a very simple dataset that is represented by the following decision
    tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6743663-fd91-403f-9a57-775f6fe90608.png)'
  prefs: []
  type: TYPE_IMG
- en: The aim of the machine learning algorithm is to generate decision trees that
    best represent the observations in the dataset. For a new observation, if we traverse
    the decision tree, the leaf nodes represent the class variable or event that is
    most likely to occur. In the preceding example, we have a dataset that has information
    regarding the make and the year of a used car. The class variable (also called
    the **feature label**) is the price of the car. We can observe in the dataset
    that, irrespective of the year variable value, the price of a BMW car is greater
    than $40,000\. However, if the make of the car is not BMW, the cost of the car
    is determined by the year the car was produced. The example is based on a very
    small amount of data. However, the decision tree represents the information in
    the dataset, and if we have to determine the cost of a new car where the make
    is BMW and year is 2015, then we can predict that the cost is greater than $40,000\.
    For more complex decision trees, the leaf nodes also have a probability associated
    with them that represents the probability of the class value occurring. In this
    chapter, we will study algorithms that can be used to generate such decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Recursive splitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees can be built by recursively splitting the dataset into subsets.
    During each split, we evaluate splits based on all the input attributes and use
    a cost function to determine which split has the lowest cost. The cost functions
    generally evaluate the loss of information when we split the dataset into two
    branches. This partitioning of the dataset into smaller subsets is also referred
    to as recursive partitioning. The cost of splitting the datasets into subsets
    is generally determined by how records with similar class variables are grouped
    together in each dataset. Hence, the most optimal split would be when observations
    in each subset will have the same class variable values.
  prefs: []
  type: TYPE_NORMAL
- en: Such recursive splitting of decision trees is a top-down approach in generating
    decision trees. This is also a greedy algorithm since we made the decision at
    each point on how to divide the dataset, without considering how it may affect
    the later splits.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we made the first split based on the make of the car.
    This is because one of our subsets, where the make is BMW, has a 100% probability
    of the price of the car being greater than $40,000\. Similarly, if we had made
    a split based on the year, we would also get a subset of the year equal to 2018
    that also has a 100% probability of the cost of the car is greater than $40,000\.
    Hence, for the same dataset, we can generate multiple decision trees that represent
    the dataset. There are various cost functions that we will look at that generate
    different decision trees based on the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Types of decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are following two main types of decision trees that most data scientists
    have to work with based on the class variables in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification trees: **Classification trees are decision trees that are
    used to predict discrete values. This means that the class variable of the dataset
    used to generate classification trees is a discrete value. The preceding example
    regarding car prices at the start of this section is a classification tree as
    it only has two values of the class variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression trees: **Regression trees are decision trees that are used to
    predict real numbers, such as the example in C*hapter 3**, Predicting House Value
    with Regression Algorithms*, where we were predicting the price of the house.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term **Classification and Regression Trees** (**CART**) is used to describe
    the algorithm for generating decision trees. CART is a popular algorithm for decision
    trees. Other popular decision tree algorithms include ID3 and C4.5\. These algorithms
    are different from each other in terms of the cost functions they use for splitting
    the dataset and the criteria used to determine when to stop splitting.
  prefs: []
  type: TYPE_NORMAL
- en: Cost functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the section on *Recursive splitting*, we need cost functions
    to determine whether splitting on a given input variable is better than other
    variables. The effectiveness of these cost functions is crucial for the quality
    of the decision trees being built. In this section, we'll discuss two popular
    cost functions for generating a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Gini Impurity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gini Impurity is defined as the measurement of the likelihood of incorrect
    classification of a random observation, given the random observation is classified
    based on the distribution of the class variables in the dataset. Consider a dataset
    with  ![](img/1cb79df3-f4d0-4635-a7eb-a36ca0e0d4e7.png) class variables, and  ![](img/2cfcf87b-3030-4cc6-ba1c-c60a62ddbc18.png) is
    the fraction of observations in the dataset labeled as ![](img/f7eea272-393c-47d6-9b01-81233bc5a30d.png).
    *Gini Impurity* can be calculated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55adff7c-2b06-42b2-8fb0-ad42229aa761.png)      .. 4.1'
  prefs: []
  type: TYPE_NORMAL
- en: Gini Impurity tells us the amount of noise present in the dataset, based on
    the distributions of various class variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the car price dataset presented at the start of *Understanding
    Decision Trees* section, we have two class variables: greater than 40,000 and
    less than 40,000\. If we had to calculate the Gini Impurity of the dataset, it
    could be calculated as shown in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31bfe354-0fab-4186-adb4-18ee95eaf77f.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, there is a lot of noise in the base dataset since each class variable
    has 50% of the observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when we create a branch where the make of the car, the Gini Impurity
    of that subset of the dataset is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3bab794-0e25-46aa-a736-3190545ed1fa.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/658abb2a-deee-4d28-ab16-80ec3f2aba2c.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/0ce42a6c-fe7d-4249-9883-a8d70c2ec65f.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the branch of for BMW only contains class values of *>40K*, there is no
    noise in the branch and the value of Gini Impurity is *0*. Note that when the
    subset of the data only has one class value, the Gini Impurity value is always
    *0*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gini Impurity is used to calculate the Gini Index for each attribute. The Gini
    Index is a weighted sum of all the values of an attribute on which we create branches.
    For an attribute, ![](img/f6e07e55-9112-4986-8e76-7b75afbfd099.png), that has
    ![](img/ff667824-2211-4440-a6bd-fba1eea27034.png) unique values, Gini Gain is
    calculated using formula below. ![](img/5e9ca5cf-4d11-4848-990b-c1b3ac2e3961.png) is the
    fraction of observations in the dataset where the value of the attribute, ![](img/ea9db92c-195e-4477-be7f-68149a481142.png),
    is ![](img/98d43c54-cf8a-464d-9376-503ccb926ee0.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43d86f3b-a08a-4d4e-b190-a83c7424690d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, in our preceding example, the Gini Index for the *Make* attribute that
    has three distinct values is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54b85ded-6b89-4176-b5ee-b331e651c0e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, we calculate the Gini Index for other attributes. In our example,
    the Gini Index for the *Year* attribute is 0.4422\. We encourage you to calculate
    this value on your own. Our aim is to pick the attribute that generates the lowest
    Gini Index score. For a perfect classification, where all the class values in
    each branch are the same, the Gini Index score will be 0.
  prefs: []
  type: TYPE_NORMAL
- en: Information gain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Information gain is based on the concept of entropy, which is commonly used
    in physics to represent the unpredictability in a random variable. For example,
    if we have an unbiased coin, the entropy of the coin is represented as *1*, as
    it has the highest unpredictability. However, if a coin is biased, and has a 100%
    chance of heads, the entropy of the coin is 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'This concept of entropy can also be used to determine the unpredictability
    of the class variable in a given branch. The entropy, denoted as *H*, of a branch
    is calculated using the formula below. ![](img/faf67a04-8f90-4c08-b99d-a1ae95804c69.png)
    represents the entropy of the attribute. ![](img/a03e73dd-2003-4c47-a55a-8b355a5ded39.png)
    is the number of class variables in the dataset. ![](img/5a31aed3-e450-45d4-9618-e8721da55b32.png)
    is the fraction of observations in the dataset that belong to the class, ![](img/fbd47654-3821-45aa-89dd-d73f69e93690.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d65e3d7-daba-4b80-b152-dade6bf59135.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 1**: In our example, for the entire dataset, we can calculate the entropy
    of the dataset as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01cf8bda-1f09-470f-aa69-9846c0257f32.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 2**: In our decision tree, we split the tree based on the make of the
    car. Hence, we also calculate the entropy of each branch of the tree, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db694e43-90fd-41a6-b5c3-1843aa950295.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/dbc40d20-a5fb-47a0-bf2b-68bde5a06fbd.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b9e63287-3d3e-41bc-be71-2384ef33fdc2.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 3**: Based on the entropy of the parent and the branches, we can evaluate
    the branch using a measure called **information gain**. For a parent branch, ![](img/16b7bb2a-eac9-4d30-8422-29ecf3315a9c.png),
    and attribute, ![](img/88e885c5-21d3-47e0-a455-15404f53baf9.png), information
    gain is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdea57aa-19c4-4d55-b646-4844a3415ba5.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/873963f4-165f-49bf-9406-1ab0817f9ce8.png) is the weighted sum of the
    entropy of the children. In our example, ![](img/b1162a12-fc43-4cda-ad99-31d329d8df34.png) of
    the *Make* attribute is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef8027cc-d825-411c-8cef-ccdf02baba9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, the information gain for the *Make* attribute is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85f98981-1681-43e7-b486-1da57abb3f24.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, we can calculate the information gain score for other attributes.
    The attribute with the highest information gain should be used to split the dataset
    for the highest quality of a decision tree. Information gain is used in the ID3
    and C4.5 algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Criteria to stop splitting trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As decision tree generation algorithms are recursive, we need a criterion that
    indicates when to stop splitting the trees. There are various criteria we can
    set to stop splitting the trees. Let us now look at the list of commonly used
    criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of observations in the node**:We can set criteria to stop the recursion
    in a branch if the number of observations is less than a pre-specified amount.
    A good rule of thumb is to stop the recursion when there is fewer than 5% of the
    total training data in a branch. If we over split the data, such that each node
    only has one data point, it leads to overfitting the decision tree to the training
    data. Any new observation that has not been previously seen will not be accurately
    classified in such trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Purity of the node**:In the *Gini Impurity* section, we learned to calculate
    the likelihood of error in classifying a random observation. We can also use the
    same methodology to calculate the purity of the dataset. If the purity of the
    subset in a branch is greater than a pre-specified threshold, we can stop splitting
    based on that branch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The depth of the tree**:We can also pre-specify the limit on the depth of
    the tree. If the depth of any branch exceeds the limit, we can stop splitting
    the branch further.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pruning trees**:Another strategy is to let the trees grow fully. This avoids
    the branch splitting being terminated prematurely, without looking ahead. However,
    after the full tree is built, it is likely that the tree is large and there may
    be overfitting in some branches. Hence, pruning strategies are applied to evaluate
    each branch of the tree; any branch that introduces less than the pre-specified
    amount of impurity in the parent branch is eliminated. There are various techniques
    to prune decision trees. We encourage our readers to explore this topic further
    in the libraries that they implement their decision trees in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding random forest algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two main disadvantages to using decision trees. First, the decision
    trees use algorithms that make a choice to split on an attribute based on a cost
    function. The decision tree algorithm is a greedy algorithm that optimizes toward
    a local optimum when making every decision regarding splitting the dataset into
    two subsets. However, it does not explore whether making a suboptimal decision
    while splitting over an attribute, would lead to a more optimal decision tree
    in the future. Hence, we do not get a globally optimum tree when running this
    algorithm. Second, decision trees tend to overfit to the training data. For example,
    a small sample of observations available in the dataset may lead to a branch that
    provides a very high probability of a certain class event occurring. This leads
    to the decision trees being really good at generating correct predictions for
    the dataset that was used for training. However, for observations that they have
    never seen before, decision trees may not be accurate due to overfitting to the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: To tackle these issues, the random forest algorithm can be used to improve the
    accuracy of the existing decision tree algorithms. In this approach, we divide
    the training data into random subsets and create a collection of decision trees,
    each based on a subset. This tackles the issue of overfitting, as we no longer
    rely on one tree to make the decision that has overfit to the entire training
    set. Secondly, this also helps with the issue of splitting on only one attribute
    based on a cost function. Different decision trees in random forests may make
    decisions on splitting based on different attributes, based on the random sample
    they are training on.
  prefs: []
  type: TYPE_NORMAL
- en: During the prediction phase, the random forest algorithm gets a probability
    of an event from each branch and uses a voting methodology to generate a prediction.
    This helps us suppress predictions from trees that may have overfitted or made
    sub-optimal decisions when generating the trees. Such an approach to divide the
    training set into random subsets and train multiple machine learning models is
    known as **Bagging**. The Bagging approach can also be applied to other machine
    learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding gradient boosting algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient boosting algorithms are also used to address the disadvantages of the
    decision tree algorithm. However, unlike the random forests algorithm, which trains
    multiple trees based on random subsets of training data, gradient-boosting algorithms
    train multiple trees sequentially by reducing the errors in the decision trees.
    Gradient boosting decision trees are based on a popular machine learning technique
    called **Adaptive Boosting**, where we learn why a machine learning model is making
    errors, and then train a new machine learning model that reduces the errors from
    the previous models.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting algorithms discover patterns in the data that are difficult
    to represent in the decision trees, and add a greater weight to the training examples,
    which can lead to correct predictions. Thus, similar to random forests, we generate
    multiple decision trees from subsets of the training data. However, during each
    step, the subset of training data is not selected randomly. Instead, we create
    a subset of training data, where the examples that would lead to fewer errors
    in decision trees are prioritized. We stop this process when we cannot observe
    patterns in errors that may lead to more optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of how random forest algorithms and gradient-boosting algorithms are
    implemented are provided in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting clicks on log streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will show you how to use tree-based methods to predict who
    will click on a mobile advertisement given a set of conditions, such as region,
    where the ad is shown, time of day, location of the banner, and the application
    delivering the advertisement.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we will use throughout the rest of the chapter is obtained from
    *Shioji, Enno, 2017, Adform click prediction dataset,* [https://doi.org/10.7910/DVN/TADBY7](https://doi.org/10.7910/DVN/TADBY7)*,
    Harvard Dataverse, V2*.
  prefs: []
  type: TYPE_NORMAL
- en: The main task is to build a classifier capable of predicting whether a user
    will click on an advertisement given the conditions. Having such a model is very
    useful for ad-tech platforms that select which ads to show to users and when.
    These platforms can use these models to only show ads to users who are likely
    to click on the ad being delivered.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is large enough (5 GB) to justify the use of technologies that span
    multiple machines to perform the training. We will first look at how to use AWS
    EMR to carry out this task with Apache Spark. We will also show how to do this
    with SageMaker services.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Elastic MapReduce (EMR)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: EMR is an AWS service that allows us to run and scale Apache Spark, Hadoop,
    HBase, Presto, Hive, and other big data frameworks. We will cover more EMR details
    in [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml), *Tuning Clusters
    for Machine Learning*. However, for now, let's think of EMR as a service that
    allows us to launch several interconnected machines with running software, such
    as Apache Spark, that coordinates distributed processing. EMR clusters have a
    master and several slaves. The master typically orchestrates the jobs, whereas
    the slaves process and combine the data to provide the master with a result. This
    result can range from a simple number (for example, a count of rows) to a machine
    learning model capable of making predictions. The Apache Spark Driver is the machine
    that coordinates the jobs necessary to complete the operation. The driver typically
    runs on the master node but it can also be configured to run on a slave node.
    The Spark executors (the demons that Spark uses to crunch the data) typically
    run on the EMR slaves.
  prefs: []
  type: TYPE_NORMAL
- en: 'EMR can also host notebook servers that connect to the cluster. This way, we
    can run our notebook paragraphs and this will trigger any distributed processing
    through Apache Spark. There are two ways to host notebooks on Apache Spark: EMR
    notebooks and JupyterHub EMR Application. We will use the first method in this
    chapter, and will cover JupyterHub in [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml),
    *Tuning Clusters for Machine Learning*.'
  prefs: []
  type: TYPE_NORMAL
- en: Through EMR notebooks, you can launch the cluster and the notebook at the same
    time through the **EMR notebooks** link on the console ([https://console.aws.amazon.com/elasticmapreduce/home](https://console.aws.amazon.com/elasticmapreduce/home)).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create the cluster and notebook simultaneously by clicking on the Create
    Notebook button, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de2ef949-ffb7-4b85-b029-e104dab52790.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you create the notebook, it will click on the Open button, as shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/804659f6-2720-4e02-bb01-6f62a27b27e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Clicking on the Open button opens the notebook for us to start coding. The
    notebook is a standard Jupyter Notebook as it can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9a106eb-25fa-4d0f-9af5-3bad1df69518.png)'
  prefs: []
  type: TYPE_IMG
- en: Alternatively, you can create the cluster separately and attach the notebook
    to the cluster. The advantage of doing so is that you have access to additional
    advanced options.
  prefs: []
  type: TYPE_NORMAL
- en: 'We recommend at least 10 machines (for instance, 10 m5.xlarge nodes) to run
    the code from this chapter in a timely fashion.  Additionally, we suggest you
    increase the Livy session timeout if your jobs take longer than an hour to complete.
    For such jobs, the notebook may get disconnected from the cluster. Livy is the
    software responsible for the communication between the notebook and the cluster.
    The following screenshot shows the create cluster options including a way to extend
    the Livy session timeout:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92fd675d-eee4-4138-ae7d-4179f17419a4.png)'
  prefs: []
  type: TYPE_IMG
- en: On [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml), *Tuning Clusters
    for Machine Learning, *we will cover more details regarding cluster configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Training with Apache Spark on EMR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now explore the training with Apache Spark on EMR.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step is to upload the data to EMR. You can do this straight from the
    notebook or download the dataset locally and then uploaded it to S3 using the
    command-line tools from AWS (awscli). In order to use the command-line tools from
    AWS, you need to create AWS access keys on the IAM console. Details on how to
    do that can be found here: [https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have your AWS access and secret keys, you can configure them by executing
    `aws configure` on the command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will get a portion of the dataset through the following `wget` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will unzip and upload the CSV dataset onto a `s3` bucket called `mastering-ml-aws` as
    shown by the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once the data is in S3, we can come back to our notebook and start coding to
    train the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The EMR notebooks, as opposed to the examples we ran locally in previous chapters
    *(*[Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml),* Classifying Twitter
    Feeds with Naive Bayes and* [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml),
    *Predicting House Value with Regression Algorithms)* have implicit variables to
    access the Spark context. In particular, the Spark session is named `spark`. The
    first paragraph run will always initialize the context and trigger the Spark driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see the spark application starting and
    a link to the Spark UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02ff36ff-e8d8-4321-9ae9-36c3f28ffe4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next step is to load our dataset and explore the different the first few
    rows by running the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the above show command is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e2044ea-ac04-4c4d-8a48-9b1fb485b0b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `spark.read.json`  method, the first command from the preceding code block, reads
    the JSON data into a dataset similar to what we''ve done before with CSV using
    `spark.read.csv`. We can observe our dataset has 10 features and an `l` column
    indicating the label which we''re trying to predict, that is, if the user clicked
    (1) or didn''t click (0) in the advertisement. You might realize that some features
    are multivalued (more than one value in a cell) and some are null. To simplify
    the code examples in this chapter we will just pick the first five features by
    constructing a new dataset and name these features `f0` through `f4` while also
    replacing null features with the value `0` and only taking the first value in
    the case of multivalued features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `selectExpr` command above allows us to use SQL-like operations. In this
    particular case we will use coalesce operation which transforms any null expressions
    into the value `0`. Also note that we're always just taking the first value for
    multivalued features.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, it's a bad idea to discard features as they might carry important
    predictive value. Likewise, replacing nulls for a fixed value can also be sub-optimal.
    We should consider common imputation techniques for missing values such as replacing
    with a point estimate (medians, modes, and means are commonly used). Alternatively,
    a model can be trained to fill in the missing value from the remaining features.
    In order to keep our focus on using trees in this chapter, we won't go deeper
    on the issue of missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `df` dataset now looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1172103f-359d-456d-84fb-10caa8390080.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we do something quite Spark specific, which is to reshuffle the different
    portions of the CSV into different machines and cache them in memory. The command
    to do such thing is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Since we will repeatedly iterate on processing the same dataset, by loading
    it in memory, it will significantly speed up any future operation made for `df` .
    The repartitioning helps to make sure the data is better distributed throughout
    the cluster, hence increasing the parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `describe()` method builds a dataframe with some basic stats (`min`, `max`,
    `mean`,  `count`) of the different fields in our dataset, as seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a789e41c-f793-4e40-b93e-490782298ade.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe that most features range from low negative values to very large
    integers, suggesting these are anonymized feature values for which a hash function
    was applied.  The field we're trying to predict is `click`, which is `1` when
    the user clicked on the advertisement and 0 when the user didn't click.  The mean
    value for the click column informs us that there is certain degree of label imbalance
    (as about 18% of the instances are clicks). Additionally, the `count` row tell
    us that there is a total of 12,000,000 rows on our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful inspection is to understand the cardinality of the categorical
    values.   The following screenshot from our notebooks shows the different number
    of unique values each feature gets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b320793-9f7f-49df-bf65-c31acbf00164.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the f4 feature is an example of a category that has many distinct
    values. These kinds of features often require special attention, as we will see
    later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees and most of Spark ML libraries require our features to be numerical
    only. It happens by chance that our features are already in numerical form, but
    these really represent categories which were hashed into numbers. In [Chapter
    2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying Twitter Feeds with
    Naive Bayes,* we learned that in order to train a classifier, we need to provide
    a vector of numbers. For this reason, we need to transform our categories into
    numbers in our dataset to include them in our vectors. This transformation is
    often called **feature encoding**. There are two popular ways to do this: through
    one-hot encoding or categorical encoding (also called **string indexing**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following generic examples, we assume that the `site_id` feature could
    only take up to three distinct values: `siteA`, `siteB`, and `siteC`.  These examples
    will also illustrate the case in which we have string features to encode into
    numbers (not integer hashes as in our dataset).'
  prefs: []
  type: TYPE_NORMAL
- en: Categorical encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Categorical encoding (or string indexing) is the simplest kind of encoding,
    in which we assign a number to each site value. Let''s look at an example in the
    following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `site_id` | `site_id_indexed` |'
  prefs: []
  type: TYPE_TB
- en: '| `siteA` | `1` |'
  prefs: []
  type: TYPE_TB
- en: '| `siteB` | `2` |'
  prefs: []
  type: TYPE_TB
- en: '| `siteC` | `3` |'
  prefs: []
  type: TYPE_TB
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this kind of encoding, we create new binary columns for each possible site
    value and set the value as `1` when the value is present, as shown in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `site_id` | `siteA` | `siteB` | `siteC` |'
  prefs: []
  type: TYPE_TB
- en: '| `siteA` | `1` | `0` | `0` |'
  prefs: []
  type: TYPE_TB
- en: '| `siteB` | `0` | `1` | `0` |'
  prefs: []
  type: TYPE_TB
- en: '| `siteC` | `0` | `0` | `1` |'
  prefs: []
  type: TYPE_TB
- en: Categorical encoding is simple; however, it may create an artificial ordering
    of the features, and some ML algorithms are sensitive to that. One-hot encoding
    has the additional benefit of supporting multi-valued features (for example, if
    a row has two sites, we can set a `1` in both columns). However, one-hot encoding
    adds more features to our dataset, which increases the dimensionality. Adding
    more dimensions to our dataset makes the training more complex and may reduce
    its predictive ability. This is known as the **curse of dimensionality**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how we would use categorical encoding on a sample of our dataset
    to transform the C1 feature (a categorical feature) into numerical values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code first instantiates a `StringIndexer` that will encode column
    `f0` into a new column `f0_index` upon fitting, goes through the dataset and finds
    distinct feature values that assign an index based on the popularity of such values.
    Then we can use the `transform()` method to get indices for each value. The output
    of the preceding final `show()` command is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d2c0b3a-c31c-4f11-bc7f-e7f75243b79e.png)'
  prefs: []
  type: TYPE_IMG
- en: In the above screenshot we can see the numerical value that each  raw (hashed)
    categorical value was assigned to.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform one-hot encoding on the values, we use the `OneHotEncoder` transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding commands generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0303965c-ce6c-422a-bbbb-91c92eb20b7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note how the different `f0` values get mapped to the corresponding boolean
    vector. We did the encoding for just one feature; however, for training, we need
    to go through the same process for several features. For this reason, we built
    a function that builds all the indexing and encoding stages necessary for our
    pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code builds a training pipeline, including the `DecisionTree`
    estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code,`VectorAssembler` constructs a vector with all features
    that require encoding as well as the numerical features ( `VectorAssembler` can
    take as input columns that can be vectors or scalars so you can use numerical
    features directly if existent in your dataset). Given the high number of one-hot-encoded
    values, the feature vector can be huge and make the trainer very slow or require
    massive amounts of memory. One way to mitigate that is to use a **chi-squared**
    feature selector. In our pipeline, we have selected the best 100 features. By
    best, we mean the features that have more predictive power—note how the chi-squared
    estimator takes both the features and the label to decide on the best features.
    Finally, we include the decision engine estimator stage, which is the one that
    will actually create the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: If we attempt to string index features with very large cardinality, the driver
    will collect all possible values (in order to keep a value-to-index dictionary
    for transformation). In such an attempt, the driver will most likely run out of
    memory as we're looking at millions of distinct values to keep. For these cases,
    you need other strategies, such as keeping only the features with the most predictive
    ability or considering only the most popular values. Check out our article, which
    includes a solution to this problem at [https://medium.com/dataxutech/how-to-write-a-custom-spark-classifier-categorical-naive-bayes-60f27843b4ad](https://medium.com/dataxutech/how-to-write-a-custom-spark-classifier-categorical-naive-bayes-60f27843b4ad).
  prefs: []
  type: TYPE_NORMAL
- en: Training a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our pipeline is now constructed, so we can proceed to split our dataset for
    testing and training and then we fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Once this is executed, the Spark Driver will figure out the best plan for distributing
    the processing necessary to train the model across many machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'By following the Spark UI link shown at the beginning of this section, we can
    see the status of the different jobs running on EMR:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ded0e39-d9eb-4dd5-9e98-a0a93532ff6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the model is trained, we can explore the decision tree behind it. We can
    do this by inspecting the last stage of the pipeline (that is, the decision tree
    model).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the result of outputting the decision tree
    in text format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how each decision is based on a feature that takes a value of `0` or `1`.
    This is because we have used one-hot encoding on our pipeline. If we had used
    the categorical encoding (string indexing), we would have seen a condition that
    involves several indexed values, such as the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating our model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contrary to our Twitter classification problem in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml),
    *Classifying Twitter Feeds with Naive Baye*s, the label in this dataset is very
    skewed. This is because there are only a few occasions where users decide to click
    on ads. The accuracy measurement we used in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml),
    *Classifying Twitter Feeds with Naive Bayes*, would not be suitable, as a model
    that never predicts a click would still have very high accuracy (all non-clicks
    would result in correct predictions). Two possible alternatives for this case
    could be to use metrics derived from the ROC or **precision-recall curves**, which
    can be seen in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Area Under ROC Curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Receiver Operating Characteristic** (**ROC**) is a representation of a
    trade-off between true-positive rates and false-positive rates. True-positive
    rates describe how good a model is at predicting a positive class when the actual
    class is positive. True-positive rates are calculated as the ratio of true positives
    predicted by a model, to the sum of true positives and false negatives. False-positive
    rates describe how often the model predicts the positive class, when the actual
    class is negative. False-positive rates are calculated as the ratio of false positives,
    to the sum of false positives and true negatives. ROC is a plot where the *x*
    axis is represented by the false-positive rate with a range of 0-1, while the
    *y* axis is represented as the true-positive rate. **Area Under Curve** (**AUC**)
    is the measure of the area under the ROC curve. AUC is a measure of predictiveness
    of a classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three examples of receiver operator curves are seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc95ba2b-6336-4f9b-96e7-dec2cbbcb0d4.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding plot, the dotted line represents an example of when `AUC` is
    `1`. Such AUCs occur when all the positive outcomes are classified correctly.
    The solid line represents the `AUC` that is `0.5`. For a binary classifier, the `AUC`
    is `0.5` when the predictions coming from the machine learning model are similar
    to randomly generating an outcome. This indicates that the machine learning model
    is no better than a random-number generator in predicting outcomes. The dashed
    line represents the `AUC` that is `0.66`. This happens when a machine learning
    model predicts some examples correctly, but not all. However, if the `AUC` is
    higher than `0.5` for the binary classifier, the model is better than just randomly
    guessing the outcome. However, if it is below 0.5, this means that the machine
    learning model is worse than a random-outcome generator. Thus, AUC is a good measure
    of comparing machine learning models and evaluating their effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Area under the precision-recall curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The precision-recall curve represents a tradeoff between precision and recall in
    a prediction model. **Precision** is defined as the ratio of true positives to
    the total number of positive predictions made by the model. **Recall** is defined
    as the ratio of positive predictions to the total number of actual positive predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the precision-recall curve does not model true negative values. This
    is useful in cases of the unbalanced dataset. ROC curves may provide a very optimistic
    view of a model if the model is good at classifying true negatives and generates
    a smaller number of false positives.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows an example of a precision-recall curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6012ad5-b4ee-4bdd-965c-db7d4adb5d20.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, the dashed line shows when the area under precision-recall
    curve is `0.5`. This indicates that the precision is always 0.5, which is similar
    to a random-number generator. The solid line represents the precision-recall curve
    that is better than random. The precision recall curve also can be used to evaluate
    a machine learning model, similar to the ROC area. However, the precision-recall
    curve should be used when the dataset is unbalanced, and the ROC should be used
    when the dataset is balanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, going back to our example, we can use Spark''s `BinaryClassificationEvaluator`
    to calculate the scores by providing the actual and predicted labels on our test
    dataset. First we will apply the model on our test dataset to get the predictions
    and scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'By applying the previous transformation `test_transformed` will have all columns
    included in `test_df` plus an additional one called `rawPrediction` which will
    have a score which can be used for evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The output of the preceding command is 0.43\. The fact that we got an ROC metric
    lower than 0.5 means that our classifier is even worse than random classifier
    and hence it is not a good model for predicting clicks! In the next section, we
    will show how to use ensemble models to improve our predictive ability.
  prefs: []
  type: TYPE_NORMAL
- en: Training tree ensembles on EMR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees can be useful for understanding the decisions made by our classifier,
    especially when decision trees are small and readable. However, decision trees
    tend to overfit the data (by learning the details of the training dataset and
    not being able to generalize on new data). For this reason, ML practitioners tend
    to use tree ensembles, such as random forests and gradient-boosted trees, which
    are explained in the previous sections in this chapter under *Understanding gradient
    boosting algorithms* and *Understanding random forest algorithms*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our code examples, to use random forests or gradient boosted trees, we just
    need to replace the last stage of our pipeline with the corresponding constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how we get a better ROC value with random forests on our sampled dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can see that now we get a ROC greater than 0.5 which means that our model
    has improved an is now better than random guessing. Similarly, you can train a
    gradient boosted tree with the `pyspark.mllib.tree.GradientBoostedTrees` class.
  prefs: []
  type: TYPE_NORMAL
- en: Training gradient-boosted trees with the SageMaker services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Training a model* and *Evaluating our model* sections, we learned how
    to build and evaluate a random forest classifier using Spark on EMR. In this section,
    we will see how to train a gradient boosted tree using the SageMaker services
    through the SageMaker notebooks. The XGBoost SageMaker service allows us to train
    gradient-boosted trees in a distributed fashion. Given that our clickthrough data
    is relatively large, it will be convenient to use such a service.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to use the SageMaker services, we will need to place our training and
    testing data in S3\. The documentation at [https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html)
    requires us to drop the data as CSV files where the first column indicates the
    training label (target feature) and the rest of the columns represent the training
    features (other formats are supported but we will use CSV in our example). For
    splitting and preparing the data in this way, EMR is still the best option as
    we want our data preparation to be distributed as well. Given our testing and
    training Spark datasets from the last *Preparing the data* section, we can apply
    the pipeline model, not for getting predictions in this case, but instead, for
    obtaining the selected encoded features for each row.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following snippet, for both `test_df` and `train_df` we apply the model
    transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the last three columns of the `test_transformed`
    dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04932beb-ce2f-4b4b-af48-02cff4bab6f9.png)'
  prefs: []
  type: TYPE_IMG
- en: The transformed datasets includes the feature vector column (named `selected_features`
    with a size of 100). We need to transform these two columns into a CSV with 101
    columns (the `click` and the `selected_features` vectors flattened out). A simple
    transformation in Spark allows us to do this. We define a `deconstruct_vector`
    function, which we will use to obtain a Spark dataframe with the label and each
    vector component as a distinct column. We then save that to S3 both for training
    and testing as a CSV without headers, as SageMaker requires.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we provide the `deconstruct_vector` function
    as well as the series of transformations needed to save the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In a similar fashion, we will save an additional CSV file that will not include
    the label (just the features) under the `s3://mastering-ml-aws/chapter4/test-trans-vec-csv-no-label` path.
    We will use this dataset to score the testing dataset through the SageMaker batch
    transform job in the next section, *Training with SageMaker XGBoost*.
  prefs: []
  type: TYPE_NORMAL
- en: Training with SageMaker XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that our datasets for training and testing are in S3 in the right format,
    we can launch our SageMaker notebook instance and start coding our trainer. Let''s
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the SageMaker session, container, and variables with the location
    of our datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a classifier by instantiating a SageMaker estimator and providing the
    basic parameters, such as the number and type of machines to use (details can
    be found in the AWS documentation at [https://sagemaker.readthedocs.io/en/stable/estimators.html](https://sagemaker.readthedocs.io/en/stable/estimators.html) ):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the hyperparameters of our trainer. The details can be found in the documentation
    (and we will cover it in more detail in [Chapter 14](7de65295-dd1f-4eb3-af00-3868ed7e2df9.xhtml),
    *Optimizing SageMaker and Spark Machine Learning Models*). The main parameter
    to look at here is the objective, which we have set for binary classification
    (using a logistic regression score, which is the standard way XGBoost performs
    classification). XGBoost can also be used for other problems, such as regressions
    or multi-class classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Before fitting the model, we need to specify the location and format of the
    input (there are a couple of formats accepted; we have chosen CSV for our example):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoking the `fit` function will train the model with the data provided (that
    is, the data we saved in S3 through our EMR/Spark preparation):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The logs will show the some details about the training and validation error
    being optimized by XGBoost, as well as the status of the job and training costs.
  prefs: []
  type: TYPE_NORMAL
- en: Applying and evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following steps will show you how to use `sagemaker` to create batch predictions
    so you can evaluate the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to obtain predictions, we can use a batch transform job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'For every file in the input `s3` directory, the batch transform job will produce
    a file with the scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then load this single-column CSV file into a `pandas` dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'These scores represent probabilities (derived via logistic regression). If
    we had set the objective to binary: hinge, we would get actual predictions instead.
    Choosing which kind to use depends on the type of application. In our case, it
    seems useful to gather probabilities, as any indication of a particular user being
    more likely to perform clicks would help to improve the marketing targeting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the advantages of SageMaker XGBoost is that it provides a serialization
    in S3 of a compatible XGBoost model with Python’s standard serialization library
    (pickle). As an example, we will take a portion of our test data in S3 and run
    the model to get scores. With this, we can compute the area under the ROC curve
    by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Locate the model tarball in `s3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy the model from S3 to our local directory and uncompress the tarball:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding command, showing the name of the file uncompressed
    from the tarball:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is locally downloaded and untared, we can load the model in
    memory via the `pickle` serialization library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the names of our columns (`f0` to `f99` for the features, and `click`
    as the label) and load the validation data from S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'To create predictions with `xgboost`, we need to assemble a matrix from our
    `pandas` dataframe. Select all columns except the first one (which is the label),
    and then construct a DMatrix. Call the predict method from the `xgboost` model
    to get the scores for every row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, the reader can see how the dataframe looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd2a3999-60a2-431b-966a-ff6fe1cf1c58.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Given the `click` column and the `score` column, we can construct the ROC AUC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: For our sample, we get a AUC value of 0.67, which is comparable to the value
    we got with Spark's random forests.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we did not focus on building the most optimal model for our
    dataset. Instead, we focused on providing simple and popular transformations and
    tree models you can use to classify large volumes of data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we covered the basic theoretical concepts for understanding
    tree ensembles and showed ways to train and evaluate these models in EMR, through
    Apache Spark, as well as through the SageMaker XGBoost service. Decision tree
    ensembles are one of the most popular classifiers, for many reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: They are able to find complex patterns in relatively short training time and
    with few resources. The XGBoost library is known as the most popular classifier
    among Kaggle competition winners (these are competitions held to find the best
    model for an open dataset).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's possible to understand why the classifier is predicting a given value.
    Following the decision tree paths or just looking at the feature importance are
    quick ways to understand the rationale behind the decisions made by tree ensembles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementations of distributed training are available through Apache Spark and
    XGBoost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, we will look into how to use machine learning to cluster
    customers based on their behavioral patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the main difference between random forests and gradient-boosted trees?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain why the Gini Impurity may be interpreted as the misclassification rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain why it is necessary to perform feature encoding for categorical features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this chapter, we provided two ways to do feature encoding. Find one other
    way to encode categorical features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain why the accuracy metric we used in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying
    Twitter Feeds with Naive Bayes,* is not suitable for predicting clicks on our
    dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find other objectives we can use for the XGBoost algorithm. When would you use
    each objective?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
