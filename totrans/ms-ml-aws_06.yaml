- en: Predicting User Behavior with Tree-Based Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于树的算法预测用户行为
- en: This chapter will introduce decision trees, random forests, and gradient-boosted
    trees. Decision trees methodology is a popular technique used in data science
    that provides a visual representation of how the information in the training set
    can be represented as a hierarchy. Traversing the hierarchy based on an observation
    helps you to predict the probability of that event.  We will explore how to use
    these algorithms can be used to predict when a user may click on online advertisement
    based on existing advertising click records. Additionally, we will show how to
    use AWS **Elastic MapReduce** (**EMR**) with Apache Spark and the SageMaker XGBoost
    service to engineer models in the context of big data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍决策树、随机森林和梯度提升树。决策树方法是一种在数据科学中广泛使用的流行技术，它以可视化的方式展示了训练集中的信息如何表示为一个层次结构。根据观察结果遍历层次结构可以帮助你预测该事件发生的概率。我们将探讨如何使用这些算法来预测用户可能会点击在线广告的时间，基于现有的广告点击记录。此外，我们还将展示如何使用AWS
    **弹性映射减少**（**EMR**）与Apache Spark以及SageMaker XGBoost服务在大数据环境中构建模型。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding decision trees
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解决策树
- en: Understanding random forests algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解随机森林算法
- en: Understanding gradient boosting algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解梯度提升算法
- en: Predicting clicks on log streams
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测日志流中的点击
- en: Understanding decision trees
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解决策树
- en: Decision trees graphically show the decisions to be made, the observed events
    that may occur, and the probabilities of the outcomes given a specific set of
    observable events occurring together. Decision trees are used as a popular machine
    learning algorithm, where, based on a dataset of observable events and the known
    outcomes, we can construct a decision tree that can represent the probability
    of an event occurring.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树图形化地展示了将要做出的决策、可能发生的观察事件以及给定一组特定的可观察事件同时发生时的结果概率。决策树作为一种流行的机器学习算法，基于一组可观察事件的数据集和已知的输出结果，我们可以构建一个决策树来表示事件发生的概率。
- en: 'The following table shows a very simple example of how decision trees can be
    generated:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了决策树如何生成的一个非常简单的例子：
- en: '| **Car Make** | **Year** | **Price** |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| **汽车品牌** | **年份** | **价格** |'
- en: '| BMW | 2015 | >$40K |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 宝马 | 2015 | >$40K |'
- en: '| BMW | 2018 | >$40K |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 宝马 | 2018 | >$40K |'
- en: '| Honda  | 2015 | <$40K |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 本田  | 2015 | <$40K |'
- en: '| Honda  | 2018 | >$40K |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 本田  | 2018 | >$40K |'
- en: '| Nissan | 2015 | <$40K |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 尼桑 | 2015 | <$40K |'
- en: '| Nissan | 2018 | >$40K |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 尼桑 | 2018 | >$40K |'
- en: 'This is a very simple dataset that is represented by the following decision
    tree:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的数据集，它由以下决策树表示：
- en: '![](img/d6743663-fd91-403f-9a57-775f6fe90608.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6743663-fd91-403f-9a57-775f6fe90608.png)'
- en: The aim of the machine learning algorithm is to generate decision trees that
    best represent the observations in the dataset. For a new observation, if we traverse
    the decision tree, the leaf nodes represent the class variable or event that is
    most likely to occur. In the preceding example, we have a dataset that has information
    regarding the make and the year of a used car. The class variable (also called
    the **feature label**) is the price of the car. We can observe in the dataset
    that, irrespective of the year variable value, the price of a BMW car is greater
    than $40,000\. However, if the make of the car is not BMW, the cost of the car
    is determined by the year the car was produced. The example is based on a very
    small amount of data. However, the decision tree represents the information in
    the dataset, and if we have to determine the cost of a new car where the make
    is BMW and year is 2015, then we can predict that the cost is greater than $40,000\.
    For more complex decision trees, the leaf nodes also have a probability associated
    with them that represents the probability of the class value occurring. In this
    chapter, we will study algorithms that can be used to generate such decision trees.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法的目标是生成最能代表数据集中观测值的决策树。对于一个新观测值，如果我们遍历决策树，叶节点代表最有可能发生的类变量或事件。在先前的例子中，我们有一个包含关于二手车品牌和年份的信息的数据集。类变量（也称为**特征标签**）是汽车的价格。我们可以在数据集中观察到，无论年份变量的值如何，宝马汽车的价格都超过40,000美元。然而，如果汽车的品牌不是宝马，汽车的成本将由汽车生产的年份决定。这个例子基于非常少量的数据。然而，决策树代表了数据集中的信息，如果我们必须确定品牌为宝马且年份为2015的新车的成本，那么我们可以预测其成本将超过40,000美元。对于更复杂的决策树，叶节点还与一个概率相关联，该概率代表类值发生的概率。在本章中，我们将研究可以用来生成此类决策树的算法。
- en: Recursive splitting
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归分割
- en: Decision trees can be built by recursively splitting the dataset into subsets.
    During each split, we evaluate splits based on all the input attributes and use
    a cost function to determine which split has the lowest cost. The cost functions
    generally evaluate the loss of information when we split the dataset into two
    branches. This partitioning of the dataset into smaller subsets is also referred
    to as recursive partitioning. The cost of splitting the datasets into subsets
    is generally determined by how records with similar class variables are grouped
    together in each dataset. Hence, the most optimal split would be when observations
    in each subset will have the same class variable values.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以通过递归地将数据集划分为子集来构建。在每次分割过程中，我们根据所有输入属性评估分割，并使用成本函数来确定哪个分割的成本最低。成本函数通常评估将数据集分割成两个分支时的信息损失。这种将数据集分割成更小子集的过程也被称为递归分割。分割数据集的成本通常由每个数据集中具有相似类变量的记录如何分组在一起来决定。因此，最优的分割是在每个子集中的观测值将具有相同的类变量值时。
- en: Such recursive splitting of decision trees is a top-down approach in generating
    decision trees. This is also a greedy algorithm since we made the decision at
    each point on how to divide the dataset, without considering how it may affect
    the later splits.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种决策树的递归分割是一种自上而下的生成方法。这同样是一种贪婪算法，因为我们在每个点上做出了如何划分数据集的决定，而没有考虑它可能对后续分割的影响。
- en: In the preceding example, we made the first split based on the make of the car.
    This is because one of our subsets, where the make is BMW, has a 100% probability
    of the price of the car being greater than $40,000\. Similarly, if we had made
    a split based on the year, we would also get a subset of the year equal to 2018
    that also has a 100% probability of the cost of the car is greater than $40,000\.
    Hence, for the same dataset, we can generate multiple decision trees that represent
    the dataset. There are various cost functions that we will look at that generate
    different decision trees based on the same dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的例子中，我们是根据汽车的品牌进行第一次分割的。这是因为我们的一个子集，其中品牌是宝马，有100%的概率汽车的价格将超过40,000美元。同样，如果我们根据年份进行分割，我们也会得到一个年份等于2018的子集，该子集也有100%的概率汽车的成本将超过40,000美元。因此，对于相同的数据集，我们可以生成多个代表数据集的决策树。我们将查看各种成本函数，它们基于相同的数据集生成不同的决策树。
- en: Types of decision trees
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树的类型
- en: 'There are following two main types of decision trees that most data scientists
    have to work with based on the class variables in the dataset:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据集中的类变量，大多数数据科学家必须处理以下两种主要的决策树类型：
- en: '**Classification trees: **Classification trees are decision trees that are
    used to predict discrete values. This means that the class variable of the dataset
    used to generate classification trees is a discrete value. The preceding example
    regarding car prices at the start of this section is a classification tree as
    it only has two values of the class variable.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression trees: **Regression trees are decision trees that are used to
    predict real numbers, such as the example in C*hapter 3**, Predicting House Value
    with Regression Algorithms*, where we were predicting the price of the house.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term **Classification and Regression Trees** (**CART**) is used to describe
    the algorithm for generating decision trees. CART is a popular algorithm for decision
    trees. Other popular decision tree algorithms include ID3 and C4.5\. These algorithms
    are different from each other in terms of the cost functions they use for splitting
    the dataset and the criteria used to determine when to stop splitting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Cost functions
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the section on *Recursive splitting*, we need cost functions
    to determine whether splitting on a given input variable is better than other
    variables. The effectiveness of these cost functions is crucial for the quality
    of the decision trees being built. In this section, we'll discuss two popular
    cost functions for generating a decision tree.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Gini Impurity
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gini Impurity is defined as the measurement of the likelihood of incorrect
    classification of a random observation, given the random observation is classified
    based on the distribution of the class variables in the dataset. Consider a dataset
    with  ![](img/1cb79df3-f4d0-4635-a7eb-a36ca0e0d4e7.png) class variables, and  ![](img/2cfcf87b-3030-4cc6-ba1c-c60a62ddbc18.png) is
    the fraction of observations in the dataset labeled as ![](img/f7eea272-393c-47d6-9b01-81233bc5a30d.png).
    *Gini Impurity* can be calculated using the following formula:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55adff7c-2b06-42b2-8fb0-ad42229aa761.png)      .. 4.1'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Gini Impurity tells us the amount of noise present in the dataset, based on
    the distributions of various class variables.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the car price dataset presented at the start of *Understanding
    Decision Trees* section, we have two class variables: greater than 40,000 and
    less than 40,000\. If we had to calculate the Gini Impurity of the dataset, it
    could be calculated as shown in the following formula:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31bfe354-0fab-4186-adb4-18ee95eaf77f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Hence, there is a lot of noise in the base dataset since each class variable
    has 50% of the observations.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when we create a branch where the make of the car, the Gini Impurity
    of that subset of the dataset is calculated as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3bab794-0e25-46aa-a736-3190545ed1fa.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: '![](img/658abb2a-deee-4d28-ab16-80ec3f2aba2c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: '![](img/0ce42a6c-fe7d-4249-9883-a8d70c2ec65f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: Since the branch of for BMW only contains class values of *>40K*, there is no
    noise in the branch and the value of Gini Impurity is *0*. Note that when the
    subset of the data only has one class value, the Gini Impurity value is always
    *0*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Gini Impurity is used to calculate the Gini Index for each attribute. The Gini
    Index is a weighted sum of all the values of an attribute on which we create branches.
    For an attribute, ![](img/f6e07e55-9112-4986-8e76-7b75afbfd099.png), that has
    ![](img/ff667824-2211-4440-a6bd-fba1eea27034.png) unique values, Gini Gain is
    calculated using formula below. ![](img/5e9ca5cf-4d11-4848-990b-c1b3ac2e3961.png) is the
    fraction of observations in the dataset where the value of the attribute, ![](img/ea9db92c-195e-4477-be7f-68149a481142.png),
    is ![](img/98d43c54-cf8a-464d-9376-503ccb926ee0.png):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43d86f3b-a08a-4d4e-b190-a83c7424690d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: 'Hence, in our preceding example, the Gini Index for the *Make* attribute that
    has three distinct values is calculated as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54b85ded-6b89-4176-b5ee-b331e651c0e6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Similarly, we calculate the Gini Index for other attributes. In our example,
    the Gini Index for the *Year* attribute is 0.4422\. We encourage you to calculate
    this value on your own. Our aim is to pick the attribute that generates the lowest
    Gini Index score. For a perfect classification, where all the class values in
    each branch are the same, the Gini Index score will be 0.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Information gain
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Information gain is based on the concept of entropy, which is commonly used
    in physics to represent the unpredictability in a random variable. For example,
    if we have an unbiased coin, the entropy of the coin is represented as *1*, as
    it has the highest unpredictability. However, if a coin is biased, and has a 100%
    chance of heads, the entropy of the coin is 0.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'This concept of entropy can also be used to determine the unpredictability
    of the class variable in a given branch. The entropy, denoted as *H*, of a branch
    is calculated using the formula below. ![](img/faf67a04-8f90-4c08-b99d-a1ae95804c69.png)
    represents the entropy of the attribute. ![](img/a03e73dd-2003-4c47-a55a-8b355a5ded39.png)
    is the number of class variables in the dataset. ![](img/5a31aed3-e450-45d4-9618-e8721da55b32.png)
    is the fraction of observations in the dataset that belong to the class, ![](img/fbd47654-3821-45aa-89dd-d73f69e93690.png):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d65e3d7-daba-4b80-b152-dade6bf59135.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: '**Step 1**: In our example, for the entire dataset, we can calculate the entropy
    of the dataset as follows.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01cf8bda-1f09-470f-aa69-9846c0257f32.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: '**Step 2**: In our decision tree, we split the tree based on the make of the
    car. Hence, we also calculate the entropy of each branch of the tree, as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db694e43-90fd-41a6-b5c3-1843aa950295.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: '![](img/dbc40d20-a5fb-47a0-bf2b-68bde5a06fbd.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: '![](img/b9e63287-3d3e-41bc-be71-2384ef33fdc2.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: '**Step 3**: Based on the entropy of the parent and the branches, we can evaluate
    the branch using a measure called **information gain**. For a parent branch, ![](img/16b7bb2a-eac9-4d30-8422-29ecf3315a9c.png),
    and attribute, ![](img/88e885c5-21d3-47e0-a455-15404f53baf9.png), information
    gain is represented as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3**：基于父节点和分支的熵，我们可以使用称为**信息增益**的度量来评估分支。对于父分支![图片](img/16b7bb2a-eac9-4d30-8422-29ecf3315a9c.png)和属性![图片](img/88e885c5-21d3-47e0-a455-15404f53baf9.png)，信息增益表示如下：'
- en: '![](img/bdea57aa-19c4-4d55-b646-4844a3415ba5.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bdea57aa-19c4-4d55-b646-4844a3415ba5.png)'
- en: '![](img/873963f4-165f-49bf-9406-1ab0817f9ce8.png) is the weighted sum of the
    entropy of the children. In our example, ![](img/b1162a12-fc43-4cda-ad99-31d329d8df34.png) of
    the *Make* attribute is calculated as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/873963f4-165f-49bf-9406-1ab0817f9ce8.png)是子节点熵的加权总和。在我们的例子中，*Make*属性的![图片](img/b1162a12-fc43-4cda-ad99-31d329d8df34.png)计算如下：'
- en: '![](img/ef8027cc-d825-411c-8cef-ccdf02baba9e.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ef8027cc-d825-411c-8cef-ccdf02baba9e.png)'
- en: 'Hence, the information gain for the *Make* attribute is calculated as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*Make*属性的信息增益计算如下：
- en: '![](img/85f98981-1681-43e7-b486-1da57abb3f24.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/85f98981-1681-43e7-b486-1da57abb3f24.png)'
- en: Similarly, we can calculate the information gain score for other attributes.
    The attribute with the highest information gain should be used to split the dataset
    for the highest quality of a decision tree. Information gain is used in the ID3
    and C4.5 algorithms.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以计算其他属性的信息增益分数。应该使用信息增益最高的属性来分割数据集，以获得最高质量的决策树。信息增益在ID3和C4.5算法中使用。
- en: Criteria to stop splitting trees
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停止分割树的准则
- en: 'As decision tree generation algorithms are recursive, we need a criterion that
    indicates when to stop splitting the trees. There are various criteria we can
    set to stop splitting the trees. Let us now look at the list of commonly used
    criteria:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于决策树生成算法是递归的，我们需要一个准则来指示何时停止分割树。我们可以设置各种准则来停止分割树。现在让我们看看常用准则的列表：
- en: '**Number of observations in the node**:We can set criteria to stop the recursion
    in a branch if the number of observations is less than a pre-specified amount.
    A good rule of thumb is to stop the recursion when there is fewer than 5% of the
    total training data in a branch. If we over split the data, such that each node
    only has one data point, it leads to overfitting the decision tree to the training
    data. Any new observation that has not been previously seen will not be accurately
    classified in such trees.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点中的观测数**：我们可以设置标准，如果分支中的观测数少于预先指定的数量，则停止分支的递归。一个很好的经验法则是当分支中有少于5%的总训练数据时停止递归。如果我们过度分割数据，以至于每个节点只有一个数据点，这会导致决策树过度拟合训练数据。任何之前未见过的新观察结果将无法在这些树中得到准确分类。'
- en: '**Purity of the node**:In the *Gini Impurity* section, we learned to calculate
    the likelihood of error in classifying a random observation. We can also use the
    same methodology to calculate the purity of the dataset. If the purity of the
    subset in a branch is greater than a pre-specified threshold, we can stop splitting
    based on that branch.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点的纯度**：在*基尼不纯度*部分，我们学习了如何计算对随机观察进行分类的错误概率。我们也可以使用相同的方法来计算数据集的纯度。如果一个分支中的子集纯度大于预先指定的阈值，我们可以停止基于该分支的分割。'
- en: '**The depth of the tree**:We can also pre-specify the limit on the depth of
    the tree. If the depth of any branch exceeds the limit, we can stop splitting
    the branch further.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树的深度**：我们也可以预先指定树的深度限制。如果任何分支的深度超过限制，我们可以停止进一步分割该分支。'
- en: '**Pruning trees**:Another strategy is to let the trees grow fully. This avoids
    the branch splitting being terminated prematurely, without looking ahead. However,
    after the full tree is built, it is likely that the tree is large and there may
    be overfitting in some branches. Hence, pruning strategies are applied to evaluate
    each branch of the tree; any branch that introduces less than the pre-specified
    amount of impurity in the parent branch is eliminated. There are various techniques
    to prune decision trees. We encourage our readers to explore this topic further
    in the libraries that they implement their decision trees in.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修剪树木**：另一种策略是让树木充分生长。这样做可以避免分支过早终止，而不考虑未来。然而，在完全构建了整棵树之后，树可能很大，某些分支可能存在过拟合。因此，应用修剪策略来评估树的每一分支；任何引入的杂质少于预先指定的父分支杂质量的分支将被消除。修剪决策树有多种技术。我们鼓励读者在实现决策树的库中进一步探索这个主题。'
- en: Understanding random forest algorithms
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解随机森林算法
- en: There are two main disadvantages to using decision trees. First, the decision
    trees use algorithms that make a choice to split on an attribute based on a cost
    function. The decision tree algorithm is a greedy algorithm that optimizes toward
    a local optimum when making every decision regarding splitting the dataset into
    two subsets. However, it does not explore whether making a suboptimal decision
    while splitting over an attribute, would lead to a more optimal decision tree
    in the future. Hence, we do not get a globally optimum tree when running this
    algorithm. Second, decision trees tend to overfit to the training data. For example,
    a small sample of observations available in the dataset may lead to a branch that
    provides a very high probability of a certain class event occurring. This leads
    to the decision trees being really good at generating correct predictions for
    the dataset that was used for training. However, for observations that they have
    never seen before, decision trees may not be accurate due to overfitting to the training
    data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树有两个主要缺点。首先，决策树使用基于成本函数选择在属性上分割的算法。决策树算法是一种贪婪算法，在做出关于将数据集分割成两个子集的每个决策时，都趋向于局部最优。然而，它并没有探索在属性上做出次优决策是否会导致未来有更优的决策树。因此，当我们运行此算法时，我们不会得到全局最优的树。其次，决策树倾向于过度拟合训练数据。例如，数据集中可用的少量观察结果可能导致一个分支，该分支提供了非常高的某个类事件发生的概率。这导致决策树在生成用于训练的数据集的正确预测方面表现得非常好。然而，对于他们以前从未见过的观察结果，由于过度拟合训练数据，决策树可能不准确。
- en: To tackle these issues, the random forest algorithm can be used to improve the
    accuracy of the existing decision tree algorithms. In this approach, we divide
    the training data into random subsets and create a collection of decision trees,
    each based on a subset. This tackles the issue of overfitting, as we no longer
    rely on one tree to make the decision that has overfit to the entire training
    set. Secondly, this also helps with the issue of splitting on only one attribute
    based on a cost function. Different decision trees in random forests may make
    decisions on splitting based on different attributes, based on the random sample
    they are training on.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，可以使用随机森林算法来提高现有决策树算法的准确性。在这种方法中，我们将训练数据划分为随机子集，并创建一个决策树集合，每个决策树基于一个子集。这解决了过拟合的问题，因为我们不再依赖于一棵树来做出对整个训练集过度拟合的决策。其次，这也帮助解决了仅基于成本函数在一个属性上分割的问题。随机森林中的不同决策树可能会基于它们训练的随机样本，基于不同的属性做出分割决策。
- en: During the prediction phase, the random forest algorithm gets a probability
    of an event from each branch and uses a voting methodology to generate a prediction.
    This helps us suppress predictions from trees that may have overfitted or made
    sub-optimal decisions when generating the trees. Such an approach to divide the
    training set into random subsets and train multiple machine learning models is
    known as **Bagging**. The Bagging approach can also be applied to other machine
    learning algorithms.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测阶段，随机森林算法从每个分支获取事件的概率，并使用投票方法生成预测。这有助于我们抑制可能过度拟合或做出次优决策的树的预测。将训练集划分为随机子集并训练多个机器学习模型的方法被称为**Bagging**。Bagging方法也可以应用于其他机器学习算法。
- en: Understanding gradient boosting algorithms
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解梯度提升算法
- en: Gradient boosting algorithms are also used to address the disadvantages of the
    decision tree algorithm. However, unlike the random forests algorithm, which trains
    multiple trees based on random subsets of training data, gradient-boosting algorithms
    train multiple trees sequentially by reducing the errors in the decision trees.
    Gradient boosting decision trees are based on a popular machine learning technique
    called **Adaptive Boosting**, where we learn why a machine learning model is making
    errors, and then train a new machine learning model that reduces the errors from
    the previous models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升算法也被用来解决决策树算法的缺点。然而，与基于训练数据随机子集训练多个树的随机森林算法不同，梯度提升算法通过减少决策树中的错误来顺序地训练多个树。梯度提升决策树基于一种流行的机器学习技术，称为**自适应提升**，其中我们学习为什么机器学习模型会出错，然后训练一个新的机器学习模型来减少先前模型的错误。
- en: Gradient boosting algorithms discover patterns in the data that are difficult
    to represent in the decision trees, and add a greater weight to the training examples,
    which can lead to correct predictions. Thus, similar to random forests, we generate
    multiple decision trees from subsets of the training data. However, during each
    step, the subset of training data is not selected randomly. Instead, we create
    a subset of training data, where the examples that would lead to fewer errors
    in decision trees are prioritized. We stop this process when we cannot observe
    patterns in errors that may lead to more optimizations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升算法在数据中发现了难以在决策树中表示的规律，并给训练示例添加了更大的权重，这可能导致正确的预测。因此，与随机森林类似，我们从训练数据的一个子集中生成多个决策树。然而，在每一步中，训练数据的子集并不是随机选择的。相反，我们创建一个训练数据子集，其中优先考虑那些会导致决策树中错误更少的示例。当我们无法观察到可能导致更多优化的错误模式时，我们停止这个过程。
- en: Examples of how random forest algorithms and gradient-boosting algorithms are
    implemented are provided in the next section.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分提供了随机森林算法和梯度提升算法实现的示例。
- en: Predicting clicks on log streams
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测日志流中的点击
- en: In this section, we will show you how to use tree-based methods to predict who
    will click on a mobile advertisement given a set of conditions, such as region,
    where the ad is shown, time of day, location of the banner, and the application
    delivering the advertisement.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您展示如何使用基于树的方法来预测在给定一系列条件（如地区、广告展示的位置、一天中的时间、横幅的位置以及提供广告的应用程序）的情况下，谁会点击移动广告。
- en: The dataset we will use throughout the rest of the chapter is obtained from
    *Shioji, Enno, 2017, Adform click prediction dataset,* [https://doi.org/10.7910/DVN/TADBY7](https://doi.org/10.7910/DVN/TADBY7)*,
    Harvard Dataverse, V2*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的其余部分使用的数据集来自 *Shioji, Enno, 2017, Adform点击预测数据集*，[https://doi.org/10.7910/DVN/TADBY7](https://doi.org/10.7910/DVN/TADBY7)，哈佛数据集，V2。
- en: The main task is to build a classifier capable of predicting whether a user
    will click on an advertisement given the conditions. Having such a model is very
    useful for ad-tech platforms that select which ads to show to users and when.
    These platforms can use these models to only show ads to users who are likely
    to click on the ad being delivered.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 主要任务是构建一个分类器，能够根据条件预测用户是否会点击广告。拥有这样的模型对于选择向用户展示哪些广告以及何时展示的广告技术平台非常有用。这些平台可以使用这些模型只为可能点击所提供广告的用户展示广告。
- en: The dataset is large enough (5 GB) to justify the use of technologies that span
    multiple machines to perform the training. We will first look at how to use AWS
    EMR to carry out this task with Apache Spark. We will also show how to do this
    with SageMaker services.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集足够大（5 GB），足以证明使用跨多台机器的技术来执行训练是合理的。我们首先将探讨如何使用AWS EMR结合Apache Spark来完成这项任务。我们还将展示如何使用SageMaker服务来完成这项任务。
- en: Introduction to Elastic MapReduce (EMR)
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性MapReduce (EMR) 简介
- en: EMR is an AWS service that allows us to run and scale Apache Spark, Hadoop,
    HBase, Presto, Hive, and other big data frameworks. We will cover more EMR details
    in [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml), *Tuning Clusters
    for Machine Learning*. However, for now, let's think of EMR as a service that
    allows us to launch several interconnected machines with running software, such
    as Apache Spark, that coordinates distributed processing. EMR clusters have a
    master and several slaves. The master typically orchestrates the jobs, whereas
    the slaves process and combine the data to provide the master with a result. This
    result can range from a simple number (for example, a count of rows) to a machine
    learning model capable of making predictions. The Apache Spark Driver is the machine
    that coordinates the jobs necessary to complete the operation. The driver typically
    runs on the master node but it can also be configured to run on a slave node.
    The Spark executors (the demons that Spark uses to crunch the data) typically
    run on the EMR slaves.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: EMR是AWS服务，允许我们运行和扩展Apache Spark、Hadoop、HBase、Presto、Hive和其他大数据框架。我们将在第15章中详细介绍EMR的更多细节，*调整集群以进行机器学习*。然而，现在让我们将EMR视为一种服务，允许我们启动几个运行软件（如Apache
    Spark）的相互连接的机器，这些软件协调分布式处理。EMR集群有一个主节点和几个从节点。主节点通常协调作业，而从节点处理和合并数据，为主节点提供结果。这个结果可以从一个简单的数字（例如，行数计数）到一个能够进行预测的机器学习模型不等。Apache
    Spark Driver是协调完成操作所需作业的机器。驱动程序通常在主节点上运行，但它也可以配置在从节点上运行。Spark executors（Spark用于处理数据的恶魔）通常在EMR从节点上运行。
- en: 'EMR can also host notebook servers that connect to the cluster. This way, we
    can run our notebook paragraphs and this will trigger any distributed processing
    through Apache Spark. There are two ways to host notebooks on Apache Spark: EMR
    notebooks and JupyterHub EMR Application. We will use the first method in this
    chapter, and will cover JupyterHub in [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml),
    *Tuning Clusters for Machine Learning*.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Through EMR notebooks, you can launch the cluster and the notebook at the same
    time through the **EMR notebooks** link on the console ([https://console.aws.amazon.com/elasticmapreduce/home](https://console.aws.amazon.com/elasticmapreduce/home)).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create the cluster and notebook simultaneously by clicking on the Create
    Notebook button, as seen in the following screenshot:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de2ef949-ffb7-4b85-b029-e104dab52790.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: 'Once you create the notebook, it will click on the Open button, as shown in
    the following screenshot:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/804659f6-2720-4e02-bb01-6f62a27b27e6.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: 'Clicking on the Open button opens the notebook for us to start coding. The
    notebook is a standard Jupyter Notebook as it can be seen in the following screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9a106eb-25fa-4d0f-9af5-3bad1df69518.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: Alternatively, you can create the cluster separately and attach the notebook
    to the cluster. The advantage of doing so is that you have access to additional
    advanced options.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'We recommend at least 10 machines (for instance, 10 m5.xlarge nodes) to run
    the code from this chapter in a timely fashion.  Additionally, we suggest you
    increase the Livy session timeout if your jobs take longer than an hour to complete.
    For such jobs, the notebook may get disconnected from the cluster. Livy is the
    software responsible for the communication between the notebook and the cluster.
    The following screenshot shows the create cluster options including a way to extend
    the Livy session timeout:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92fd675d-eee4-4138-ae7d-4179f17419a4.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: On [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml), *Tuning Clusters
    for Machine Learning, *we will cover more details regarding cluster configuration.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Training with Apache Spark on EMR
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now explore the training with Apache Spark on EMR.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step is to upload the data to EMR. You can do this straight from the
    notebook or download the dataset locally and then uploaded it to S3 using the
    command-line tools from AWS (awscli). In order to use the command-line tools from
    AWS, you need to create AWS access keys on the IAM console. Details on how to
    do that can be found here: [https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Once you have your AWS access and secret keys, you can configure them by executing
    `aws configure` on the command line.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will get a portion of the dataset through the following `wget` command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will unzip and upload the CSV dataset onto a `s3` bucket called `mastering-ml-aws` as
    shown by the following command:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once the data is in S3, we can come back to our notebook and start coding to
    train the classifier.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The EMR notebooks, as opposed to the examples we ran locally in previous chapters
    *(*[Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml),* Classifying Twitter
    Feeds with Naive Bayes and* [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml),
    *Predicting House Value with Regression Algorithms)* have implicit variables to
    access the Spark context. In particular, the Spark session is named `spark`. The
    first paragraph run will always initialize the context and trigger the Spark driver.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see the spark application starting and
    a link to the Spark UI:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02ff36ff-e8d8-4321-9ae9-36c3f28ffe4e.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: 'The next step is to load our dataset and explore the different the first few
    rows by running the following snippet:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the above show command is:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e2044ea-ac04-4c4d-8a48-9b1fb485b0b0.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: 'The `spark.read.json`  method, the first command from the preceding code block, reads
    the JSON data into a dataset similar to what we''ve done before with CSV using
    `spark.read.csv`. We can observe our dataset has 10 features and an `l` column
    indicating the label which we''re trying to predict, that is, if the user clicked
    (1) or didn''t click (0) in the advertisement. You might realize that some features
    are multivalued (more than one value in a cell) and some are null. To simplify
    the code examples in this chapter we will just pick the first five features by
    constructing a new dataset and name these features `f0` through `f4` while also
    replacing null features with the value `0` and only taking the first value in
    the case of multivalued features:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `selectExpr` command above allows us to use SQL-like operations. In this
    particular case we will use coalesce operation which transforms any null expressions
    into the value `0`. Also note that we're always just taking the first value for
    multivalued features.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Generally, it's a bad idea to discard features as they might carry important
    predictive value. Likewise, replacing nulls for a fixed value can also be sub-optimal.
    We should consider common imputation techniques for missing values such as replacing
    with a point estimate (medians, modes, and means are commonly used). Alternatively,
    a model can be trained to fill in the missing value from the remaining features.
    In order to keep our focus on using trees in this chapter, we won't go deeper
    on the issue of missing values.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `df` dataset now looks as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1172103f-359d-456d-84fb-10caa8390080.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: 'Now we do something quite Spark specific, which is to reshuffle the different
    portions of the CSV into different machines and cache them in memory. The command
    to do such thing is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Since we will repeatedly iterate on processing the same dataset, by loading
    it in memory, it will significantly speed up any future operation made for `df` .
    The repartitioning helps to make sure the data is better distributed throughout
    the cluster, hence increasing the parallelization.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'The `describe()` method builds a dataframe with some basic stats (`min`, `max`,
    `mean`,  `count`) of the different fields in our dataset, as seen in the following
    screenshot:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a789e41c-f793-4e40-b93e-490782298ade.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: We can observe that most features range from low negative values to very large
    integers, suggesting these are anonymized feature values for which a hash function
    was applied.  The field we're trying to predict is `click`, which is `1` when
    the user clicked on the advertisement and 0 when the user didn't click.  The mean
    value for the click column informs us that there is certain degree of label imbalance
    (as about 18% of the instances are clicks). Additionally, the `count` row tell
    us that there is a total of 12,000,000 rows on our dataset.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful inspection is to understand the cardinality of the categorical
    values.   The following screenshot from our notebooks shows the different number
    of unique values each feature gets:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b320793-9f7f-49df-bf65-c31acbf00164.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: As you can see, the f4 feature is an example of a category that has many distinct
    values. These kinds of features often require special attention, as we will see
    later in this section.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees and most of Spark ML libraries require our features to be numerical
    only. It happens by chance that our features are already in numerical form, but
    these really represent categories which were hashed into numbers. In [Chapter
    2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying Twitter Feeds with
    Naive Bayes,* we learned that in order to train a classifier, we need to provide
    a vector of numbers. For this reason, we need to transform our categories into
    numbers in our dataset to include them in our vectors. This transformation is
    often called **feature encoding**. There are two popular ways to do this: through
    one-hot encoding or categorical encoding (also called **string indexing**).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following generic examples, we assume that the `site_id` feature could
    only take up to three distinct values: `siteA`, `siteB`, and `siteC`.  These examples
    will also illustrate the case in which we have string features to encode into
    numbers (not integer hashes as in our dataset).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Categorical encoding
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Categorical encoding (or string indexing) is the simplest kind of encoding,
    in which we assign a number to each site value. Let''s look at an example in the
    following table:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '| `site_id` | `site_id_indexed` |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| `siteA` | `1` |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| `siteB` | `2` |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| `siteC` | `3` |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: One-hot encoding
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this kind of encoding, we create new binary columns for each possible site
    value and set the value as `1` when the value is present, as shown in the following
    table:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '| `site_id` | `siteA` | `siteB` | `siteC` |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| `siteA` | `1` | `0` | `0` |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| `siteB` | `0` | `1` | `0` |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| `siteC` | `0` | `0` | `1` |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: Categorical encoding is simple; however, it may create an artificial ordering
    of the features, and some ML algorithms are sensitive to that. One-hot encoding
    has the additional benefit of supporting multi-valued features (for example, if
    a row has two sites, we can set a `1` in both columns). However, one-hot encoding
    adds more features to our dataset, which increases the dimensionality. Adding
    more dimensions to our dataset makes the training more complex and may reduce
    its predictive ability. This is known as the **curse of dimensionality**.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how we would use categorical encoding on a sample of our dataset
    to transform the C1 feature (a categorical feature) into numerical values:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code first instantiates a `StringIndexer` that will encode column
    `f0` into a new column `f0_index` upon fitting, goes through the dataset and finds
    distinct feature values that assign an index based on the popularity of such values.
    Then we can use the `transform()` method to get indices for each value. The output
    of the preceding final `show()` command is shown in the following screenshot:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d2c0b3a-c31c-4f11-bc7f-e7f75243b79e.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: In the above screenshot we can see the numerical value that each  raw (hashed)
    categorical value was assigned to.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform one-hot encoding on the values, we use the `OneHotEncoder` transformer:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding commands generates the following output:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0303965c-ce6c-422a-bbbb-91c92eb20b7a.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'Note how the different `f0` values get mapped to the corresponding boolean
    vector. We did the encoding for just one feature; however, for training, we need
    to go through the same process for several features. For this reason, we built
    a function that builds all the indexing and encoding stages necessary for our
    pipeline:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following code builds a training pipeline, including the `DecisionTree`
    estimator:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding code,`VectorAssembler` constructs a vector with all features
    that require encoding as well as the numerical features ( `VectorAssembler` can
    take as input columns that can be vectors or scalars so you can use numerical
    features directly if existent in your dataset). Given the high number of one-hot-encoded
    values, the feature vector can be huge and make the trainer very slow or require
    massive amounts of memory. One way to mitigate that is to use a **chi-squared**
    feature selector. In our pipeline, we have selected the best 100 features. By
    best, we mean the features that have more predictive power—note how the chi-squared
    estimator takes both the features and the label to decide on the best features.
    Finally, we include the decision engine estimator stage, which is the one that
    will actually create the classifier.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: If we attempt to string index features with very large cardinality, the driver
    will collect all possible values (in order to keep a value-to-index dictionary
    for transformation). In such an attempt, the driver will most likely run out of
    memory as we're looking at millions of distinct values to keep. For these cases,
    you need other strategies, such as keeping only the features with the most predictive
    ability or considering only the most popular values. Check out our article, which
    includes a solution to this problem at [https://medium.com/dataxutech/how-to-write-a-custom-spark-classifier-categorical-naive-bayes-60f27843b4ad](https://medium.com/dataxutech/how-to-write-a-custom-spark-classifier-categorical-naive-bayes-60f27843b4ad).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Training a model
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our pipeline is now constructed, so we can proceed to split our dataset for
    testing and training and then we fit the model:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once this is executed, the Spark Driver will figure out the best plan for distributing
    the processing necessary to train the model across many machines.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'By following the Spark UI link shown at the beginning of this section, we can
    see the status of the different jobs running on EMR:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ded0e39-d9eb-4dd5-9e98-a0a93532ff6a.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Once the model is trained, we can explore the decision tree behind it. We can
    do this by inspecting the last stage of the pipeline (that is, the decision tree
    model).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the result of outputting the decision tree
    in text format:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note how each decision is based on a feature that takes a value of `0` or `1`.
    This is because we have used one-hot encoding on our pipeline. If we had used
    the categorical encoding (string indexing), we would have seen a condition that
    involves several indexed values, such as the following example:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Evaluating our model
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contrary to our Twitter classification problem in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml),
    *Classifying Twitter Feeds with Naive Baye*s, the label in this dataset is very
    skewed. This is because there are only a few occasions where users decide to click
    on ads. The accuracy measurement we used in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml),
    *Classifying Twitter Feeds with Naive Bayes*, would not be suitable, as a model
    that never predicts a click would still have very high accuracy (all non-clicks
    would result in correct predictions). Two possible alternatives for this case
    could be to use metrics derived from the ROC or **precision-recall curves**, which
    can be seen in the following section.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Area Under ROC Curve
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Receiver Operating Characteristic** (**ROC**) is a representation of a
    trade-off between true-positive rates and false-positive rates. True-positive
    rates describe how good a model is at predicting a positive class when the actual
    class is positive. True-positive rates are calculated as the ratio of true positives
    predicted by a model, to the sum of true positives and false negatives. False-positive
    rates describe how often the model predicts the positive class, when the actual
    class is negative. False-positive rates are calculated as the ratio of false positives,
    to the sum of false positives and true negatives. ROC is a plot where the *x*
    axis is represented by the false-positive rate with a range of 0-1, while the
    *y* axis is represented as the true-positive rate. **Area Under Curve** (**AUC**)
    is the measure of the area under the ROC curve. AUC is a measure of predictiveness
    of a classification model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Three examples of receiver operator curves are seen in the following screenshot:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc95ba2b-6336-4f9b-96e7-dec2cbbcb0d4.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: In the preceding plot, the dotted line represents an example of when `AUC` is
    `1`. Such AUCs occur when all the positive outcomes are classified correctly.
    The solid line represents the `AUC` that is `0.5`. For a binary classifier, the `AUC`
    is `0.5` when the predictions coming from the machine learning model are similar
    to randomly generating an outcome. This indicates that the machine learning model
    is no better than a random-number generator in predicting outcomes. The dashed
    line represents the `AUC` that is `0.66`. This happens when a machine learning
    model predicts some examples correctly, but not all. However, if the `AUC` is
    higher than `0.5` for the binary classifier, the model is better than just randomly
    guessing the outcome. However, if it is below 0.5, this means that the machine
    learning model is worse than a random-outcome generator. Thus, AUC is a good measure
    of comparing machine learning models and evaluating their effectiveness.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Area under the precision-recall curve
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The precision-recall curve represents a tradeoff between precision and recall in
    a prediction model. **Precision** is defined as the ratio of true positives to
    the total number of positive predictions made by the model. **Recall** is defined
    as the ratio of positive predictions to the total number of actual positive predictions.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Note that the precision-recall curve does not model true negative values. This
    is useful in cases of the unbalanced dataset. ROC curves may provide a very optimistic
    view of a model if the model is good at classifying true negatives and generates
    a smaller number of false positives.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows an example of a precision-recall curve:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6012ad5-b4ee-4bdd-965c-db7d4adb5d20.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, the dashed line shows when the area under precision-recall
    curve is `0.5`. This indicates that the precision is always 0.5, which is similar
    to a random-number generator. The solid line represents the precision-recall curve
    that is better than random. The precision recall curve also can be used to evaluate
    a machine learning model, similar to the ROC area. However, the precision-recall
    curve should be used when the dataset is unbalanced, and the ROC should be used
    when the dataset is balanced.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'So, going back to our example, we can use Spark''s `BinaryClassificationEvaluator`
    to calculate the scores by providing the actual and predicted labels on our test
    dataset. First we will apply the model on our test dataset to get the predictions
    and scores:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'By applying the previous transformation `test_transformed` will have all columns
    included in `test_df` plus an additional one called `rawPrediction` which will
    have a score which can be used for evaluation:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output of the preceding command is 0.43\. The fact that we got an ROC metric
    lower than 0.5 means that our classifier is even worse than random classifier
    and hence it is not a good model for predicting clicks! In the next section, we
    will show how to use ensemble models to improve our predictive ability.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Training tree ensembles on EMR
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees can be useful for understanding the decisions made by our classifier,
    especially when decision trees are small and readable. However, decision trees
    tend to overfit the data (by learning the details of the training dataset and
    not being able to generalize on new data). For this reason, ML practitioners tend
    to use tree ensembles, such as random forests and gradient-boosted trees, which
    are explained in the previous sections in this chapter under *Understanding gradient
    boosting algorithms* and *Understanding random forest algorithms*.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'In our code examples, to use random forests or gradient boosted trees, we just
    need to replace the last stage of our pipeline with the corresponding constructor:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note how we get a better ROC value with random forests on our sampled dataset:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can see that now we get a ROC greater than 0.5 which means that our model
    has improved an is now better than random guessing. Similarly, you can train a
    gradient boosted tree with the `pyspark.mllib.tree.GradientBoostedTrees` class.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Training gradient-boosted trees with the SageMaker services
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Training a model* and *Evaluating our model* sections, we learned how
    to build and evaluate a random forest classifier using Spark on EMR. In this section,
    we will see how to train a gradient boosted tree using the SageMaker services
    through the SageMaker notebooks. The XGBoost SageMaker service allows us to train
    gradient-boosted trees in a distributed fashion. Given that our clickthrough data
    is relatively large, it will be convenient to use such a service.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to use the SageMaker services, we will need to place our training and
    testing data in S3\. The documentation at [https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html)
    requires us to drop the data as CSV files where the first column indicates the
    training label (target feature) and the rest of the columns represent the training
    features (other formats are supported but we will use CSV in our example). For
    splitting and preparing the data in this way, EMR is still the best option as
    we want our data preparation to be distributed as well. Given our testing and
    training Spark datasets from the last *Preparing the data* section, we can apply
    the pipeline model, not for getting predictions in this case, but instead, for
    obtaining the selected encoded features for each row.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following snippet, for both `test_df` and `train_df` we apply the model
    transformation:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following screenshot shows the last three columns of the `test_transformed`
    dataframe:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04932beb-ce2f-4b4b-af48-02cff4bab6f9.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: The transformed datasets includes the feature vector column (named `selected_features`
    with a size of 100). We need to transform these two columns into a CSV with 101
    columns (the `click` and the `selected_features` vectors flattened out). A simple
    transformation in Spark allows us to do this. We define a `deconstruct_vector`
    function, which we will use to obtain a Spark dataframe with the label and each
    vector component as a distinct column. We then save that to S3 both for training
    and testing as a CSV without headers, as SageMaker requires.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we provide the `deconstruct_vector` function
    as well as the series of transformations needed to save the dataframe:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In a similar fashion, we will save an additional CSV file that will not include
    the label (just the features) under the `s3://mastering-ml-aws/chapter4/test-trans-vec-csv-no-label` path.
    We will use this dataset to score the testing dataset through the SageMaker batch
    transform job in the next section, *Training with SageMaker XGBoost*.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Training with SageMaker XGBoost
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that our datasets for training and testing are in S3 in the right format,
    we can launch our SageMaker notebook instance and start coding our trainer. Let''s
    perform the following steps:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the SageMaker session, container, and variables with the location
    of our datasets:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Create a classifier by instantiating a SageMaker estimator and providing the
    basic parameters, such as the number and type of machines to use (details can
    be found in the AWS documentation at [https://sagemaker.readthedocs.io/en/stable/estimators.html](https://sagemaker.readthedocs.io/en/stable/estimators.html) ):'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Set the hyperparameters of our trainer. The details can be found in the documentation
    (and we will cover it in more detail in [Chapter 14](7de65295-dd1f-4eb3-af00-3868ed7e2df9.xhtml),
    *Optimizing SageMaker and Spark Machine Learning Models*). The main parameter
    to look at here is the objective, which we have set for binary classification
    (using a logistic regression score, which is the standard way XGBoost performs
    classification). XGBoost can also be used for other problems, such as regressions
    or multi-class classification:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Before fitting the model, we need to specify the location and format of the
    input (there are a couple of formats accepted; we have chosen CSV for our example):'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Invoking the `fit` function will train the model with the data provided (that
    is, the data we saved in S3 through our EMR/Spark preparation):'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The logs will show the some details about the training and validation error
    being optimized by XGBoost, as well as the status of the job and training costs.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Applying and evaluating the model
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following steps will show you how to use `sagemaker` to create batch predictions
    so you can evaluate the model.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to obtain predictions, we can use a batch transform job:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For every file in the input `s3` directory, the batch transform job will produce
    a file with the scores:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The preceding command generates the following output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can then load this single-column CSV file into a `pandas` dataframe:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'These scores represent probabilities (derived via logistic regression). If
    we had set the objective to binary: hinge, we would get actual predictions instead.
    Choosing which kind to use depends on the type of application. In our case, it
    seems useful to gather probabilities, as any indication of a particular user being
    more likely to perform clicks would help to improve the marketing targeting.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the advantages of SageMaker XGBoost is that it provides a serialization
    in S3 of a compatible XGBoost model with Python’s standard serialization library
    (pickle). As an example, we will take a portion of our test data in S3 and run
    the model to get scores. With this, we can compute the area under the ROC curve
    by performing the following steps:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Locate the model tarball in `s3`:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output looks as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Copy the model from S3 to our local directory and uncompress the tarball:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here is the output of the preceding command, showing the name of the file uncompressed
    from the tarball:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once the model is locally downloaded and untared, we can load the model in
    memory via the `pickle` serialization library:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Define the names of our columns (`f0` to `f99` for the features, and `click`
    as the label) and load the validation data from S3:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To create predictions with `xgboost`, we need to assemble a matrix from our
    `pandas` dataframe. Select all columns except the first one (which is the label),
    and then construct a DMatrix. Call the predict method from the `xgboost` model
    to get the scores for every row:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In the following screenshot, the reader can see how the dataframe looks:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd2a3999-60a2-431b-966a-ff6fe1cf1c58.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: 'Given the `click` column and the `score` column, we can construct the ROC AUC:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: For our sample, we get a AUC value of 0.67, which is comparable to the value
    we got with Spark's random forests.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we did not focus on building the most optimal model for our
    dataset. Instead, we focused on providing simple and popular transformations and
    tree models you can use to classify large volumes of data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we covered the basic theoretical concepts for understanding
    tree ensembles and showed ways to train and evaluate these models in EMR, through
    Apache Spark, as well as through the SageMaker XGBoost service. Decision tree
    ensembles are one of the most popular classifiers, for many reasons:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: They are able to find complex patterns in relatively short training time and
    with few resources. The XGBoost library is known as the most popular classifier
    among Kaggle competition winners (these are competitions held to find the best
    model for an open dataset).
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's possible to understand why the classifier is predicting a given value.
    Following the decision tree paths or just looking at the feature importance are
    quick ways to understand the rationale behind the decisions made by tree ensembles.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementations of distributed training are available through Apache Spark and
    XGBoost.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, we will look into how to use machine learning to cluster
    customers based on their behavioral patterns.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the main difference between random forests and gradient-boosted trees?
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain why the Gini Impurity may be interpreted as the misclassification rate.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain why it is necessary to perform feature encoding for categorical features.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this chapter, we provided two ways to do feature encoding. Find one other
    way to encode categorical features.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain why the accuracy metric we used in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying
    Twitter Feeds with Naive Bayes,* is not suitable for predicting clicks on our
    dataset.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find other objectives we can use for the XGBoost algorithm. When would you use
    each objective?
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
