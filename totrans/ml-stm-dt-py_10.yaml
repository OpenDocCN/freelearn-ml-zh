- en: '*Chapter 8*: Reinforcement Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*：强化学习'
- en: The reinforcement learning paradigm is very different than standard machine
    learning and even the online machine learning methods that we have covered in
    earlier chapters. Although reinforcement learning will not always be a better
    choice than "regular" learning for many use cases, it is a powerful tool for tackling
    re-learning and the adaptation of models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习范式与标准机器学习以及我们在前面章节中介绍过的在线机器学习方法非常不同。尽管对于许多用例来说，强化学习并不总是比“常规”学习更好，但它是一个解决再学习和模型适应的强大工具。
- en: In reinforcement learning, we give the model a lot of decisive power to do its
    re-learning and to update the rules of its decision-making process. Rather than
    letting the model make a prediction and hardcode the action to take for this prediction,
    the model will directly decide on the action to take.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，我们给予模型很大的决策权来进行其再学习和更新决策过程的规则。而不是让模型做出预测并硬编码采取该预测的动作，模型将直接决定采取的动作。
- en: For automated machine learning pipelines in which actions are effectively automated,
    this can be a great choice. Of course, this must be complemented with different
    types of logging, monitoring, and more. For cases in which we need a prediction
    rather than an action, reinforcement learning will not be appropriate.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自动化机器学习管道，其中动作被有效自动化，这可以是一个很好的选择。当然，这必须辅以不同类型的日志记录、监控等。对于我们需要预测而不是动作的情况，强化学习可能不适用。
- en: Although very powerful in the right use case, reinforcement learning is currently
    not a standard choice with respect to regular machine learning. In the future,
    reinforcement learning may very well gain popularity for a larger number of use
    cases.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在合适的使用场景中非常强大，但强化学习目前在常规机器学习方面并不是一个标准的选择。在未来，强化学习可能会在更多的使用场景中变得非常流行。
- en: In this chapter, you will first be thoroughly introduced to the different concepts
    behind reinforcement learning. You will then see an implementation of reinforcement
    learning in Python.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将首先全面了解强化学习背后的不同概念。然后，你将看到Python中强化学习的实现。
- en: 'This chapter covers the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Defining reinforcement learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义强化学习
- en: The main steps of reinforcement learning models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习模型的主要步骤
- en: Exploring Q-learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索Q学习
- en: Deep Q-learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度Q学习
- en: Using reinforcement learning for streaming data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用强化学习处理流数据
- en: Use cases of reinforcement learning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的用例
- en: Implementing reinforcement learning in Python
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中实现强化学习
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find all the code for this book on GitHub at the following link: [https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python).
    If you are not yet familiar with Git and GitHub, the easiest way to download the
    notebooks and code samples is the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到本书的所有代码，链接如下：[https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python)。如果你还不熟悉Git和GitHub，下载笔记本和代码示例的最简单方法是以下：
- en: Go to the link of the repository.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往仓库的链接。
- en: Go to the green **Code** button.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击绿色的**代码**按钮。
- en: Select **Download zip**.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**下载zip**。
- en: When you download the ZIP file, you unzip it in your local environment, and
    you will be able to access the code through your preferred Python editor.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当你下载ZIP文件时，你需要在本地环境中解压缩它，然后你将能够通过你首选的Python编辑器访问代码。
- en: Python environment
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python环境
- en: To follow along with this book, you can download the code in the repository
    and execute it using your preferred Python editor.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本书的内容，你可以下载仓库中的代码，并使用你首选的Python编辑器执行它。
- en: If you are not yet familiar with Python environments, I would advise you to
    check out either Anaconda ([https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)),
    which comes with the Jupyter Notebook and JupyterLab, which are both great for
    executing notebooks. It also comes with Spyder and VSCode for editing scripts
    and programs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还不熟悉Python环境，我建议你检查Anaconda（[https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)），它自带Jupyter
    Notebook和JupyterLab，这两个都是执行笔记本的绝佳选择。它还包含了Spyder和VSCode，用于编辑脚本和程序。
- en: If you have difficulty installing Python or the associated programs on your
    machine, you can check out Google Colab ([https://colab.research.google.com/](https://colab.research.google.com/))
    or Kaggle Notebooks ([https://www.kaggle.com/code](https://www.kaggle.com/code)),
    which both allow you to run Python code in online notebooks for free, without
    any setup.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你安装 Python 或相关程序有困难，你可以查看 Google Colab ([https://colab.research.google.com/](https://colab.research.google.com/))
    或 Kaggle 笔记本 ([https://www.kaggle.com/code](https://www.kaggle.com/code))，这两个都允许你免费在线运行
    Python 代码，无需任何设置。
- en: Note
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The code in the book will generally use Colab and Kaggle notebooks with Python
    version 3.7.13 and you can set up your own environment to mimic this.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 书中的代码通常使用 Colab 和 Kaggle 笔记本，Python 版本为 3.7.13，你可以设置自己的环境来模拟这种情况。
- en: Defining reinforcement learning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义强化学习
- en: Reinforcement learning is a subdomain of machine learning that focuses on creating
    machine learning models that make decisions. Sometimes, the models are not referred
    to as models, but rather as intelligent agents.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习的一个子领域，专注于创建能够做出决策的机器学习模型。有时，这些模型并不被称为模型，而是被称为智能代理。
- en: When looking from a distance, you could argue that reinforcement learning is
    very close to machine learning. We could say that both of them are methods inside
    artificial intelligence that try to deliver intelligent black boxes, which are
    able to learn specific tasks just like a human would – often better.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从远处看，你可能会认为强化学习与机器学习非常接近。我们可以说，它们都是人工智能内部的方法，试图提供智能的黑盒，这些黑盒能够像人类一样学习特定的任务——通常表现得更好。
- en: If we look closer, however, we start to see important differences. In previous
    chapters, you have seen machine learning models such as anomaly detection, classification,
    and regression. All of them use a number of variables and are able to make real-time
    predictions on a target variable based on those.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们更仔细地观察，我们开始看到重要的差异。在前面的章节中，你已经看到了机器学习模型，如异常检测、分类和回归。所有这些模型都使用多个变量，并且能够根据这些变量对目标变量进行实时预测。
- en: You have seen a number of metrics that allow us data scientists to decide whether
    a model is any good. The online models are also able to adapt to changing data
    by relearning and continuously taking into account their own error metrics.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了许多指标，这些指标允许我们数据科学家决定一个模型是否有所作为。在线模型也能够通过重新学习和持续考虑自己的错误指标来适应变化的数据。
- en: Reinforcement learning goes further than that. RL models not only make predictions
    but also take action. You could say that offline models do not take any autonomy
    in relearning from their mistakes, online models do take into account mistakes
    right away, and reinforcement learning models are designed to make mistakes and
    learn from them.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习不仅限于这些。RL 模型不仅做出预测，还采取行动。你可以这样说，离线模型在重新学习时不会从错误中吸取任何自主性，在线模型会立即考虑到错误，而强化学习模型被设计成会犯错误并从中学习。
- en: Online models can adapt to their mistakes, just like reinforcement learning.
    However, when you build the first version of an online model, you do expect it
    to have acceptable performance in the beginning, and you would train it on some
    historical data. It can then adapt in the case of data drift or other changes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在线模型可以适应它们的错误，就像强化学习一样。然而，当你构建在线模型的第一版时，你确实期望它在开始时就有可接受的表现，并且你会在一些历史数据上对其进行训练。它可以在数据漂移或其他变化的情况下进行适应。
- en: The reinforcement learning model, on the other hand, starts out totally naïve
    and unknowing. It will try out actions, make some mistakes, and then by pure hazard
    at some point, it will make some good decisions as well. At this point, the reinforcement
    model will receive rewards and start to remember those.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，强化学习模型一开始是完全无知的。它会尝试采取行动，犯一些错误，然后纯粹出于偶然，在某个时刻，它也会做出一些好的决策。在这个时候，强化学习模型将获得奖励并开始记住这些。
- en: Comparing online and offline reinforcement learning
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较在线和离线强化学习
- en: 'Reinforcement learning is generally online learning: the intelligent agent
    learns through repeated action taking with rewards for good predictions. This
    can continue indefinitely, at least as long as feedback on the decision keeps
    getting fed into the model.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习通常是在线学习：智能代理通过重复采取行动并获得对良好预测的奖励来学习。这可以无限期地继续，至少在决策反馈持续输入模型的情况下。
- en: However, reinforcement learning can also be offline. In this case, the model
    would learn for a given period of time, and then at some point, the feedback loop
    is cut off so that the model (the decision rules) stays the same after that point.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，强化学习也可以是离线的。在这种情况下，模型会在一段时间内学习，然后在某个时刻，反馈循环被切断，这样模型（决策规则）在那之后保持不变。
- en: In general, when reinforcement learning is used, it is because we are interested
    in continuous relearning. So, the online variant is the most common.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当使用强化学习时，是因为我们对持续再学习感兴趣。因此，在线变体是最常见的。
- en: A more detailed overview of feedback loops in reinforcement learning
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习中反馈循环的更详细概述
- en: Now, let's go more into the details of reinforcement learning. To start, it
    is important to understand how the feedback loop of a general reinforcement learning
    model works. The following schema shows the logic of a model learning through
    a feedback loop.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更深入地探讨强化学习的细节。首先，了解一个通用强化学习模型的反馈循环是如何工作的非常重要。以下图示显示了模型通过反馈循环学习的逻辑。
- en: '![Figure 8.1 – Feedback loops in RL'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.1 – 强化学习中的反馈循环]'
- en: '](img/B18335_08_1.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18335_08_1.jpg]'
- en: Figure 8.1 – Feedback loops in RL
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 强化学习中的反馈循环
- en: 'In this schema, you observe the following elements:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图示中，你可以观察到以下元素：
- en: '**The RL agent**: Our model that is continuously learning and making decisions.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习代理**：我们这个持续学习和做决策的模型。'
- en: '**The environment**: A fixed environment in which the agent can make a specific
    set of decisions.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境**：一个固定的环境，代理可以在其中做出特定的决策集。'
- en: '**The action**: Every time the agent makes a decision, this will alter the
    environment.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：每当代理做出决策时，这将会改变环境。'
- en: '**The reward**: If the decision yields a good result, then a reward will be
    given to the agent.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：如果决策产生好的结果，那么将给予代理奖励。'
- en: '**The state**: The information about the environment that the agent needs to
    make its decisions.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：代理做出决策所需的环境信息。'
- en: As a simplified example, imagine that the agent is a human baby learning to
    walk. At each point in time, the baby is trying out stuff that could get them
    to walk. More specifically, they are activating several muscles in their body.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作为简化的例子，想象一下代理是一个学习行走的婴儿。在每一个时间点，婴儿都在尝试可能使他们行走的事情。更具体地说，他们正在激活身体中的几个肌肉。
- en: While doing this, the baby is observing that they are or are not walking. Also,
    their parents cheer them on when they are getting closer to walking correctly.
    This is a reward being sent to the baby that indicates to them that they are learning
    in the right way.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在做这件事的时候，宝宝正在观察他们是否在行走。同时，当他们的父母看到他们接近正确行走时，会为他们欢呼。这是一种奖励，传达给宝宝的信息是他们在以正确的方式学习。
- en: The baby will then again try to walk by using almost the same muscles, but with
    a little variation. If it's better, they'll see that as a positive thing and continue
    to move in that way.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，宝宝将再次尝试通过几乎使用相同的肌肉，但有一点变化来行走。如果效果更好，他们会将其视为积极的事情，并继续以这种方式移动。
- en: Let's now cover the remaining steps that are necessary for all of this to work.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来讨论所有这些步骤中剩余的必要步骤。
- en: The main steps of a reinforcement learning model
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习模型的主体步骤
- en: The actions of the agent are the decisions that it can make. This is a limited
    set of decisions. As you will understand, the agent is just a piece of code, so
    all its decisions will need to be programmed controls of its own behavior.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的动作是它可以做出的决策。这是一个有限的决策集。正如你将理解的，代理只是一段代码，所以它所有的决策都需要编程控制其自身的行为。
- en: If we think of it as a computer game, then you understand that the actions that
    you as a player can execute are limited by the buttons that you can press on your
    game console. All of the combinations together still allow for a very wide range
    of options, but they are limited in some way.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果把它比作一个电脑游戏，那么你就能理解作为玩家，你可以执行的动作是受限于你在游戏控制台上的按键。所有的组合加在一起仍然允许一个非常广泛的选项，但它们在某种程度上是有限的。
- en: The same is true for our human baby learning to walk. They only have control
    over their own body, so they would not be able to execute any actions beyond this.
    This gives a huge number of things that can be done by humans, but still, it is
    a fixed set of actions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们人类宝宝学习行走来说，也是如此。他们只能控制自己的身体，因此他们无法执行超出这个范围的动作。这为人类提供了大量的可能性，但仍然是一个固定的动作集。
- en: Making the decisions
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 做出决策
- en: Now, as your reinforcement agent is receiving information about its environment
    (the state), it will need to convert this information into a decision. This is
    the same idea as a machine learning model that needs to map independent variables
    into a target variable.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着您的强化智能体接收有关其环境（状态）的信息，它需要将此信息转换为决策。这与需要将独立变量映射到目标变量的机器学习模型中的相同概念。
- en: This decision mapping is generally called the policy in the case of reinforcement
    learning. The policy will generally decide on the best action by estimating the
    expected rewards and then executing the action with the highest expected reward.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种强化学习的情况下，这种决策映射通常被称为策略。策略通常会通过估计预期奖励来决定最佳行动，然后执行预期奖励最高的行动。
- en: Updating the decision rules
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新决策规则
- en: 'The last part of this big picture description of reinforcement learning is
    the update of the policy: basically, the learning itself. There are many models,
    and they all have their own specificities, but let''s try to get a general idea
    anyway.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习这一大图景描述的最后一部分是策略的更新：基本上，就是学习本身。有许多模型，它们都有自己的特定之处，但无论如何，让我们尽量获得一个一般性的概念。
- en: At this point, you have seen that an agent takes an action from a set of fixed
    actions. The agent has estimated which is most likely to maximize rewards. After
    the execution of this task, the model will receive a certain reward. This will
    be used to alter the policy, in a way that depends on the exact method of reinforcement
    learning that you use.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到智能体从一组固定的行动中选择一个行动。智能体已经估计出哪个最有可能最大化奖励。在执行此任务后，模型将收到一定的奖励。这将用于改变策略，具体取决于您使用的强化学习方法的精确方法。
- en: In the next section, you will see how this can be done in more detail by exploring
    the Q-learning algorithm.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将通过探索Q学习算法来更详细地了解这一点。
- en: Exploring Q-learning
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Q学习
- en: 'Although there are many variants of reinforcement learning, the previous explanation
    should have given you a good general overview of how most reinforcement models
    work. It is now time to move deeper into a specific model for reinforcement learning:
    Q-learning.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强化学习有许多变体，但前面的解释应该已经为您提供了一个关于大多数强化模型工作原理的良好概述。现在是时候深入探讨强化学习的一个特定模型：Q学习。
- en: 'Q-learning is a reinforcement learning algorithm that is, so-called, model
    free. Model-free reinforcement learning algorithms can be seen as pure trial-and-error
    algorithms: they have no prior notion of the environment, but merely just try
    out actions and learn whether their actions yield the correct outcome.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习是一种所谓的无模型强化学习算法。无模型强化学习算法可以被视为纯试错算法：它们对环境没有先验概念，只是尝试行动并学习其行动是否产生正确的结果。
- en: Model-based algorithms, on the other hand, use a different theoretical approach.
    Rather than just learning the outcome based on the actions, they try to understand
    their environment through some form of a model. Once the agent learns how the
    environment works, it can take actions that will optimize the reward according
    to this knowledge.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，基于模型的算法使用不同的理论方法。它们不仅基于行动学习结果，还试图通过某种形式的模型来理解其环境。一旦智能体学会了环境如何运作，它就可以采取根据这种知识优化奖励的行动。
- en: Although the model-based approach may seem more intuitively likely to perform,
    model-free approaches such as Q-learning are actually quite good.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于模型的方案可能看起来更直观，但无模型的方法，如Q学习，实际上相当不错。
- en: The goal of Q-learning
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q学习的目标
- en: The goal of the Q-learning algorithm is to find a policy that maximizes the
    expected reward obtained from a number of successive steps starting at the current
    state.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习算法的目标是找到一个策略，该策略从当前状态开始，通过一系列连续步骤获得最大化的预期奖励。
- en: In regular language, this means that Q-learning looks at the current state (the
    variables of its environment) and then uses this information to take the best
    steps in the future. The model does not look at past happenings, only the future.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 用常规语言来说，这意味着Q学习会查看当前状态（其环境的变量），然后利用这些信息在未来采取最佳步骤。该模型不会查看过去发生的事情，只关注未来。
- en: 'The model uses the Q-value as a calculation for the quality of a state-action
    combination: that is, for each state, there is a list of potential actions. Each
    combination of a potential state and a potential action is called a state-action
    combination. The Q-value indicates the quality of this action when the state is
    the given one.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 模型使用Q值作为状态-动作组合质量的计算：也就是说，对于每个状态，都有一个潜在动作的列表。每个潜在状态和潜在动作的组合称为状态-动作组合。Q值表示当状态是给定的时，这个动作的质量。
- en: At the beginning of the reinforcement learning process, the value of Q is initialized
    in some way (randomly or fixed) and then updates every time that a reward is received.
    The agent handles the model according to the Q-values, and when rewards (feedback
    on the actions) start to come in, those Q values change. The agent still continues
    to follow the Q-values, but as they update, the behavior of the agent changes.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习过程的开始，Q的值以某种方式初始化（随机或固定），然后在每次收到奖励时更新。智能体根据Q值处理模型，当奖励（对动作的反馈）开始到来时，那些Q值会改变。智能体仍然继续遵循Q值，但随着它们的更新，智能体的行为也会改变。
- en: 'The core of this algorithm is the Bellman equation: an update rule for Q-values
    that uses a weighted average of older and new Q-values. Therefore, old information
    is forgotten at some point, when a lot of learning has happened. This avoids getting
    "stuck" in previous behaviors.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的核心是Bellman方程：一个用于Q值的更新规则，它使用旧的和新的Q值的加权平均值。因此，在发生大量学习的情况下，旧信息会在某个时刻被遗忘。这避免了陷入以前的行为中。
- en: 'The formula of the Bellman equation is the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Bellman方程的公式如下：
- en: '![](img/Formula_08_001.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![公式_08_001](img/Formula_08_001.jpg)'
- en: Parameters of the Q-learning algorithm
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q学习算法的参数
- en: 'In this Bellman equation, there are a few important parameters that you can
    tune. Let''s briefly cover those:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个Bellman方程中，有几个重要的参数你可以调整。让我们简要地介绍一下：
- en: The learning rate is a very commonly used hyperparameter in machine learning
    algorithms. It generally defines the step size of an optimizer in which large
    steps may make you move around faster in the optimization space, but too large
    steps may also cause a problem to go into narrow optimums.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率是机器学习算法中一个非常常用的超参数。它通常定义了优化器的步长，大步长可能会让你在优化空间中移动得更快，但过大的步长也可能导致问题进入狭窄的最优解。
- en: The discount factor is a concept that is very often used in finance and economics.
    In reinforcement learning, it indicates at which rate the model needs to prioritize
    short-term or long-term rewards.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 折扣因子是一个在金融和经济中经常使用的概念。在强化学习中，它表示模型需要以多快的速度优先考虑短期或长期奖励。
- en: After this overview of Q-learning, the next section will introduce a more complex
    alternative version of this approach called Deep Q-learning.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在对Q学习的概述之后，下一节将介绍这种方法的更复杂版本，称为深度Q学习。
- en: Deep Q-learning
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q学习
- en: Now that you have seen the basics of reinforcement learning and the most basic
    reinforcement learning model, Q-learning, it is time to move on to a more performant
    and more commonly used model called Deep Q-learning.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了强化学习的基础以及最基本的强化学习模型Q学习，现在是时候转向一个性能更好、更常用的模型，即深度Q学习了。
- en: Deep Q-learning is a variant of Q-learning in which the Q-values are not just
    a list of expected Q-values for each combination of state and actions, updated
    by the Bellman equation. Rather, in Deep Q-learning, this estimation is done using
    a (deep) neural network.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q学习是Q学习的一个变体，其中Q值不仅仅是状态和动作组合的预期Q值列表，这些值由Bellman方程更新。相反，在深度Q学习中，这种估计是通过一个（深度）神经网络完成的。
- en: If you are not familiar, neural networks are a class of machine learning models
    that are amongst the state of the art in terms of performance. Neural networks
    are largely used for many use cases in artificial intelligence, machine learning,
    and data science in general. Deep neural networks are the technology that allows
    many data science use cases such as **Natural Language Processing** (**NLP**),
    computer vision, and much more.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不太熟悉，神经网络是一类机器学习模型，在性能方面处于最前沿。神经网络在人工智能、机器学习和数据科学等许多用例中得到了广泛应用。深度神经网络是允许许多数据科学用例的技术，例如**自然语言处理**（**NLP**）、计算机视觉等等。
- en: The idea behind the neural network is to pass an input data point through a
    network of nodes, called neurons, that each do a very simple operation. The fact
    that there are many such simple operations being done, and weights applied in
    between, means that the neural network is a powerful learning algorithm for mapping
    input data into a target variable.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络背后的想法是通过一个节点网络（称为神经元）传递输入数据点，每个神经元执行一个非常简单的操作。由于有许多这样的简单操作正在进行，并且之间应用了权重，这意味着神经网络是一种强大的学习算法，可以将输入数据映射到目标变量。
- en: The following example shows a standard depiction of a neural network. The models
    can be as simple or as complex as you want. You can go to huge numbers of hidden
    layers and add as many nodes per hidden layer as you want. Each arrow is a coefficient
    and needs to be estimated. So, it must be kept in mind that a large quantity of
    data will be necessary for estimating such models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例展示了神经网络的标准表示。模型可以像你想要的那样简单或复杂。你可以拥有大量的隐藏层，并且可以为每个隐藏层添加尽可能多的节点。每条箭头代表一个系数，需要被估计。因此，必须记住，估计此类模型需要大量的数据。
- en: 'The example schematic of a neural network is shown here:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了神经网络的示例示意图：
- en: '![Figure 8.2 – Neural network architecture'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.2 – Neural network architecture'
- en: '](img/B18335_08_2.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18335_08_2.jpg]'
- en: Figure 8.2 – Neural network architecture
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 神经网络架构
- en: For reinforcement learning, this has to be applied inside the Q-learning paradigm.
    In essence, the deep learning model is just a better way to estimate Q-values
    than the standard Q-learning approach (or at least that's what it aspires to be).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于强化学习，这必须在Q学习范式中应用。本质上，深度学习模型只是比标准Q学习方法更好地估计Q值的一种方式（或者至少这是它所追求的）。
- en: 'You could see the analogy as follows. In standard Q-learning, there is a relatively
    simple storage and update mechanism for new rewards coming in and updating the
    policy. You could see it as depicted as a table, as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将类比看作如下。在标准Q学习中，对于新奖励的存储和更新机制相对简单，你可以将其视为如下所示的表格：
- en: '![Figure 8.3 – Example table format'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.3 – Example table format'
- en: '](img/B18335_08_3.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18335_08_3.jpg]'
- en: Figure 8.3 – Example table format
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 示例表格格式
- en: In Deep Q-learning, the input and output processes are mostly the same, yet
    the state is transcribed as a number of variables that are input into a neural
    network. The neural network then outputs the estimated Q-values for each action.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度Q学习中，输入和输出过程大多相同，但状态被转录为一系列变量，这些变量被输入到神经网络中。然后，神经网络输出每个动作的估计Q值。
- en: The following graph shows how the state is added as input to the neural network.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了如何将状态作为输入添加到神经网络中。
- en: '![Figure 8.4 – Adding the state as input to the neural network'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.4 – Adding the state as input to the neural network'
- en: '](img/B18335_08_4.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18335_08_4.jpg]'
- en: Figure 8.4 – Adding the state as input to the neural network
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 将状态作为输入添加到神经网络
- en: Now that you understand the theory behind reinforcement learning, the next section
    will be more applied as it presents a number of example use cases for reinforcement
    learning on streaming data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了强化学习背后的理论，下一节将更加应用性，因为它将展示一些强化学习在流数据上的示例用例。
- en: Using reinforcement learning for streaming data
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用强化学习处理流数据
- en: As discussed throughout earlier chapters, the challenge of building models on
    streaming data is to find models that are able to learn incrementally and that
    are able to adapt in the case of model drift or data drift.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所讨论的，在流数据上构建模型的挑战在于找到能够增量学习并且能够在模型漂移或数据漂移的情况下适应的模型。
- en: Reinforcement learning is a potential candidate that could respond well to those
    two challenges. After all, reinforcement learning has a feedback loop that allows
    it to change policy when many mistakes are made. It is therefore able to adapt
    itself in the event of changes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是能够很好地应对这两个挑战的潜在候选者。毕竟，强化学习有一个反馈循环，允许它在犯了很多错误时改变策略。因此，它能够在变化的情况下自我适应。
- en: Reinforcement learning can be seen as a subcase of online learning. At the same
    time, the second specificity of reinforcement learning is its focus on learning
    actions, whereas regular online models are focused on making accurate predictions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习可以看作是在线学习的一个子案例。同时，强化学习的第二个特性是它专注于学习动作，而常规在线模型则专注于做出准确的预测。
- en: The split between the two fields is present in practice in the types of use
    cases and domains of application, but many streaming use cases have the potential
    to benefit from reinforcement learning and it is a great toolset to master.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个领域的分割在实际中体现在用例类型和应用领域上，但许多流用例有可能从强化学习中受益，并且它是一个值得掌握的优秀工具集。
- en: 'If you are looking for more depth and examples, you can look at the following
    insightful article: [https://www.researchgate.net/publication/337581742_Machine_learning_for_streaming_data_state_of_the_art_challenges_and_opportunities](https://www.researchgate.net/publication/337581742_Machine_learning_for_streaming_data_state_of_the_art_challenges_and_opportunities).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要更深入和更多的例子，你可以查看以下有洞察力的文章：[https://www.researchgate.net/publication/337581742_Machine_learning_for_streaming_data_state_of_the_art_challenges_and_opportunities](https://www.researchgate.net/publication/337581742_Machine_learning_for_streaming_data_state_of_the_art_challenges_and_opportunities)。
- en: In the next section, we will explore a few key use cases where reinforcement
    learning proves crucial.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨几个关键用例，在这些用例中，强化学习证明是至关重要的。
- en: Use cases of reinforcement learning
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的应用案例
- en: The use cases of reinforcement learning are almost as numerous as online learning.
    It is a less often used technology when compared to standard offline and online
    models, but with the changes in the machine learning domain over the last years,
    it is still a great candidate that could become huge in the coming years.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的应用案例几乎和在线学习一样多。与标准离线和在线模型相比，它是一种较少使用的科技，但随着过去几年机器学习领域的变化，它仍然是一个可能在未来几年变得巨大的优秀候选者。
- en: Let's look at some use cases to get a better feel of the types of use cases
    that can be suitable for reinforcement learning. Among the types of examples,
    there are some that are more traditional reinforcement learning use cases, and
    others that are more specific streaming data use cases.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些用例，以更好地了解哪些类型的用例可能适合强化学习。在示例类型中，有一些是更传统的强化学习用例，还有一些是更具体的流数据用例。
- en: Use case one – trading system
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用案例一 - 交易系统
- en: As a first use case of reinforcement learning, let's talk about stock market
    trading. The stock market use case was already discussed in the forecasting use
    case of the regression chapter. Reinforcement learning is an alternative solution
    to it.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 作为强化学习的一个第一个应用案例，让我们来谈谈股票市场交易。股票市场的用例已经在回归章节的预测用例中讨论过了。强化学习是它的一个替代解决方案。
- en: In regression, online models are used to build forecasting tools. Using these
    forecasting tools, a stock trader could predict the price developments of specific
    stocks in the near future and use those predictions to decide on buying or selling
    the stocks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归中，在线模型被用来构建预测工具。使用这些预测工具，股票交易者可以预测特定股票在不久的将来价格的发展趋势，并利用这些预测来决定买入或卖出股票。
- en: 'Using reinforcement learning, the use case would be developed slightly differently.
    The intelligent agent would learn how to make decisions rather than to predict
    prices. As an example, you could give the agent three actions: sell, buy, or hold
    (hold meaning do nothing/ignore).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用强化学习，这个用例的开发会有所不同。智能代理会学习如何做决策，而不是预测价格。例如，你可以给代理三个动作：卖出、买入或持有（持有意味着什么都不做/忽略）。
- en: The agent would receive information about its environment, which could include
    past stock prices, macroeconomic information, and much more. This information
    would be used together with a policy and this policy decides when to buy, sell,
    or hold.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 代理会接收到关于其环境的信息，这可能包括过去的股票价格、宏观经济信息等等。这些信息将与策略一起使用，这个策略决定何时买入、卖出或持有。
- en: By training this agent for a long period of time, and with a lot of data including
    all types of market scenarios, the agent could learn pretty well how to trade
    markets. You would then obtain a profitable "trading robot," making money without
    much intervention. If successful, this is clearly an advantage over regression
    models as they only predict price and do not take any action.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 通过长时间训练这个代理，并且使用包括所有类型市场场景的大量数据，代理可以很好地学习如何交易市场。这样你就可以获得一个盈利的“交易机器人”，无需太多干预就能赚钱。如果成功，这显然比回归模型有优势，因为它们只预测价格而不采取任何行动。
- en: 'For more information on this topic, you could start by checking out the following
    links:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个主题的更多信息，你可以从查看以下链接开始：
- en: '[https://arxiv.org/pdf/1911.10107.pdf](https://arxiv.org/pdf/1911.10107.pdf)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1911.10107.pdf](https://arxiv.org/pdf/1911.10107.pdf)'
- en: '[http://cslt.riit.tsinghua.edu.cn/mediawiki/images/a/aa/07407387.pdf](http://cslt.riit.tsinghua.edu.cn/mediawiki/images/a/aa/07407387.pdf)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://cslt.riit.tsinghua.edu.cn/mediawiki/images/a/aa/07407387.pdf](http://cslt.riit.tsinghua.edu.cn/mediawiki/images/a/aa/07407387.pdf)'
- en: Use case two – social network ranking system
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例二 - 社交网络排名系统
- en: A second use case for reinforcement learning is the ranking of posts on social
    networks. The general idea of what happens behind this is a number of posts being
    created and the most relevant has to be shown to each specific user, based on
    their preference.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的第二个用例是社交网络帖子排名。背后发生的一般想法是创建大量帖子，并根据用户的偏好向每个特定用户展示最相关的帖子。
- en: There are many machine learning approaches that could be leveraged for this,
    and reinforcement learning is one of them. Basically, the model would end up making
    decisions on the posts to show to the user, so in this way, it is a real action
    that is taken.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多机器学习方法可以用于此，强化学习就是其中之一。基本上，模型将最终对要向用户展示的帖子做出决策，因此这种方式实际上是在采取行动。
- en: This action also generates feedback. If the user likes, comments, shares, clicks,
    pauses, or interacts in other ways with the post, the agent will be rewarded and
    learns that this type of post does interest the user.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作还会生成反馈。如果用户喜欢、评论、分享、点击、暂停或以其他方式与帖子互动，代理将获得奖励，并学会这种类型的帖子确实对用户感兴趣。
- en: By trial and error, the agent can publish different types of posts to each user
    and learn which decisions are good and which are bad.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过试错，代理可以向每个用户发布不同类型的帖子，并学习哪些决策是好的，哪些是不好的。
- en: Real-time response is very important here, as well as learning rapidly from
    mistakes. If a user receives a number of irrelevant posts, this would be detrimental
    to their user experience and the model should learn as soon as possible that its
    predictions are not correct. Online learning or reinforcement learning is therefore
    great for this use case.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 实时响应在这里非常重要，以及从错误中快速学习。如果用户收到大量不相关的帖子，这将损害他们的用户体验，模型应尽快学会其预测是不正确的。因此，在线学习或强化学习非常适合此类用例。
- en: 'For more information about such use cases, you can find some materials here:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此类用例的更多信息，您可以在以下链接中找到一些资料：
- en: '[https://arxiv.org/abs/1601.00667](https://arxiv.org/abs/1601.00667)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1601.00667](https://arxiv.org/abs/1601.00667)'
- en: '[https://rbcdsai.iitm.ac.in/blogs/finding-influencers-in-social-networks-reinforcement-learning-shows-the-way/](https://rbcdsai.iitm.ac.in/blogs/finding-influencers-in-social-networks-reinforcement-learning-shows-the-way/)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://rbcdsai.iitm.ac.in/blogs/finding-influencers-in-social-networks-reinforcement-learning-shows-the-way/](https://rbcdsai.iitm.ac.in/blogs/finding-influencers-in-social-networks-reinforcement-learning-shows-the-way/)'
- en: Use case three – a self-driving car
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例三 - 自动驾驶汽车
- en: Reinforcement learning has also been proposed for the use case of self-driving
    cars. As you probably know, self-driving cars have been increasingly gaining attention
    over the last few years. The goal is to make machine learning or artificial intelligence
    models that can replace the behavior of human drivers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习还被提议用于自动驾驶汽车的用例。正如您可能知道的，自动驾驶汽车在过去几年中越来越受到关注。目标是创建可以取代人类驾驶员行为的机器学习或人工智能模型。
- en: 'It is easy to understand that the essential part of this model will be to take
    actions: accelerate, slow down, brake, turn, and so on. If a good enough reinforcement
    learning model could be built to obtain all those skills, it would be a great
    candidate for building self-driving cars.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易理解，该模型的关键部分将是采取行动：加速、减速、刹车、转弯等等。如果能够构建一个足够好的强化学习模型来获得所有这些技能，它将是构建自动驾驶汽车的理想候选者。
- en: Self-driving cars need to respond to a large stream of data about the environment.
    For example, they need to detect cars, roads, road signs, and much more on a continuous
    video stream that is being filmed on multiple cameras, together with other sensors
    potentially.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶汽车需要响应大量关于环境的数据流。例如，它们需要在多个摄像头拍摄的视频流中连续检测汽车、道路、路标等等，以及其他可能的传感器。
- en: Real-time responses are key in this scenario. Retraining the model in real time
    might be more problematic, as you would want to make sure that the model is not
    applying a trial-and-error methodology while on the road.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，实时响应至关重要。在道路上，实时重新训练模型可能会更成问题，因为您希望确保模型在行驶过程中不是在应用试错方法。
- en: 'More information on this can be found at the following links:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息可以在以下链接中找到：
- en: '[https://arxiv.org/ftp/arxiv/papers/1901/1901.00569.pdf](https://arxiv.org/ftp/arxiv/papers/1901/1901.00569.pdf)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.ingentaconnect.com/contentone/ist/ei/2017/00002017/00000019/art00012?crawler=true&mimetype=application/pdf](https://www.ingentaconnect.com/contentone/ist/ei/2017/00002017/00000019/art00012?crawler=true&mimetype=application/pdf)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case four – chatbots
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another very different but also very advanced use case of machine learning is
    the development of chatbots. Intelligent chatbots are still rare, but we can expect
    to see chatbots become more intelligent in the near future.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Chatbots need to be able to generate a response to a person while treating
    the information that was given to it by a user. The chatbot is therefore performing
    a sort of action: replying to the human.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning in combination with other techniques from the domain
    of natural language processing can be a good solution for such problems. By letting
    the chatbot talk with users, a reward can be given by the human user in the form
    of, for example, an evaluation of the usefulness of their interaction. This reward
    can then help the reinforcement learning agent adapt its policy and make replies
    more appropriate in future interactions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots need to be able to respond in real time, as no one wants to wait for
    an answer from a chatbot interaction. Learning can be done in an online or an
    offline fashion, but reinforcement learning is definitely one of the suitable
    alternatives.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more on this use case here:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1709.02349](https://arxiv.org/abs/1709.02349)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1908.10331.pdf](https://arxiv.org/pdf/1908.10331.pdf)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case five – learning games
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a final use case example for reinforcement learning, let's talk about the
    use case of learning games. It may be less valuable for business, but it is still
    an interesting use case of reinforcement learning.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Over the past years, reinforcement learning agents have learned to play a number
    of games, including chess and Go. There is a clear set of moves that can be made
    at each step, and by playing many simulated (or real) games, the models can learn
    which policy (decision rules for the step to take) are the best.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: In the end, the agent has such a powerful policy that it can often beat the
    best human players in the world at such games.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more examples of this at the following links:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.science.org/doi/10.1126/science.aar6404](https://www.science.org/doi/10.1126/science.aar6404)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1912.10944.pdf](https://arxiv.org/pdf/1912.10944.pdf)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have explored some of the use cases for reinforcement learning,
    let's implement it using Python, in the next section.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Implementing reinforcement learning in Python
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now move on to an example in which streaming data is used for Q-Learning.
    The data that we will be using is simulated data of stock prices:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The data is generated in the following block of code.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The list of values that is first generated is a list of 30,000 consecutive values
    that represent stock prices. The data generating process is the starting point
    of 0 and at every time step, there is a random value added to this. The random
    normal values are centered around 0, which indicates that prices would go up or
    down by a step size based on a standard deviation of 1.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: This process is often referred to as a random walk and it can go quite far up
    or down. After that, the values are standardized to be within a normal distribution
    again.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 8-1
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The resulting plot can be seen in the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – The resulting plot from the preceding code block'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_08_5.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – The resulting plot from the preceding code block
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for a reinforcement problem, it is necessary to have a finite number of
    states. Of course, if we consider stock prices, we could collect up to an infinite
    number of decimals. The data is rounded to 1 decimal to limit the number of possible
    state data points:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 8-2
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The resulting graph is shown in the following figure:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – The graph resulting from the preceding code block'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_08_6.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – The graph resulting from the preceding code block
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: We can now set the states' potential values to all of the values that have happened
    in the past. We can also initiate a policy.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As seen in the theoretical part of this chapter, the policy represents the rules
    of the reinforcement learning agent. In some cases, there is a very specific ruleset,
    but in Q-learning, there is only a Q-value (quality) for each combination of state
    and action.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, let''s consider a stock trading bot that can only do two things
    at a time *t*. Either the trading bot buys at time *t* and sells at *t+1*, or
    it sells at time *t* and closes the sell position at time *t+1*. Without going
    into stock trading too much, the important things to understand about this are
    the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: When the agent buys, it should do so because it expects the stock market to
    go up.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the agent opens a sell order, it should do so because it expects the stock
    market to go down.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As information, our stock trader will be very limited. The only data point
    in the state is the price at time t. The goal here is not to make a great model,
    but to show the principles of building a reinforcement learning agent on a stock
    trading example. In reality, you''d need much more information in the state to
    decide on your action:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 8-3
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The function defined hereafter is how to obtain an action (sell or buy) based
    on the Q-table. It is not entirely correct to refer to the Q table as the policy,
    but it does make it more understandable.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The action chosen is that with the highest Q value for a given state (state
    is the current value of the stock):'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 8-4
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It is also necessary to define an update rule. In this example, the update
    rule is based on the Bellman equation that was explained earlier on. However,
    keep in mind that the agent is fairly simple, and the discounting part is not
    really relevant. Discounting is useful to make an agent prefer short-term gains
    over long-term gains. The current agent always makes its gains in one time step,
    so discounting is of no added value. In a real stock-trading bot, this would be
    very important: you wouldn''t put your money on a stock that will double over
    20 years if you could double it in 1 year instead:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 8-5
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We now get to the execution of the model. We start by setting `past_state` to
    0 and `past_action` to `buy`. The total reward is initialized at 0 and an accumulator
    list for rewards is instantiated.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code will then loop through the rounded values. This is a process that copies
    a data stream. If the data arrived one by one, the agent would be able to learn
    in exactly the same manner. The essence is an update of the Q table at every learning
    step.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Within each iteration, the model will execute the best action, where the best
    is based on the Q values of the Q values table (policy). The model will also receive
    the reward from time step t-1, as this was defined as the only option for the
    stock trading bot. Those rewards will be used to update the Q table so that the
    next round can have updated information:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 8-6
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the following plot, you see how the model is getting its rewards. In the
    beginning, total rewards are negative for a long time, and then they are positive
    at the end. Keep in mind that we are learning on input data that is hypothetical
    and that represents a random walk. If we wanted an actual intelligent stock trading
    bot, we''d need to give it much more and much better data:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 8-7
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The resulting graph is shown hereafter:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – The graph resulting from the preceding code block'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_08_7.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.7 – The graph resulting from the preceding code block
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows a heat map of the Q values against the policy. The
    values at the top of the table are the preferred action when stock prices are
    low, and the values at the bottom are preferred actions when stock prices are
    high. The color light yellow means high-quality actions, and the color black means
    low-quality actions:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 8-8
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting heatmap is shown here:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – The heatmap resulting from the preceding code block'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_08_8.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 – The heatmap resulting from the preceding code block
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'It is interesting to see that the model seems to have started to learn a basic
    rule in stock trading: buy low, sell high. This can be seen by more yellow in
    selling at high prices and more yellow in buying at low prices. Apparently, this
    rule is even true on simulated random walk data.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: To learn more advanced rules, the agent would need to have more data in the
    state, and therefore the Q table would also become much heavier. An example of
    what you could add is a rolling history of prices so that the agent knows whether
    you are in an uptrend or a downtrend. You could also add macro-economic factors,
    sentiment estimations, or any other data.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: You could also make the action structure much more advanced. Rather than having
    only one-day sell or buy trades, it would be much more interesting to have a model
    that can buy or sell any equity in its portfolio at any time that the agent decides
    to.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you would also need to provide enough data to allow the model to
    make estimations for all these scenarios. The more scenarios you take into account,
    the more time it will take the agent to learn how to behave correctly.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you were first introduced to the underlying foundations of
    reinforcement learning. You saw that reinforcement learning models are focused
    on taking actions rather than on making predictions.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: You also saw two widely known algorithms for reinforcement learning. This started
    with Q-learning, which is the foundational algorithm of reinforcement learning,
    and its more powerful improvement, Deep Q-learning.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is often used for more advanced use cases such as chatbots
    or self-driving cars, but can also be used for numerical data streams very well.
    Through a use case, you saw how to apply reinforcement learning to streaming data
    for finance.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter, you have come to the end of discovering the most relevant
    machine learning models for online learning. In the coming chapters, you will
    discover a number of additional tools that you will need to take into account
    in online learning and that have no real counterpart in traditional ML. You will
    first have a deep dive into all types of data and model drift and then discover
    how to deal with models that go totally in the wrong direction through catastrophic
    forgetting.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reinforcement learning applications: [https://neptune.ai/blog/reinforcement-learning-applications](https://neptune.ai/blog/reinforcement-learning-applications)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Q-learning: [https://en.wikipedia.org/wiki/Q-learning](https://en.wikipedia.org/wiki/Q-learning)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep Q-learning: [https://en.wikipedia.org/wiki/Deep_reinforcement_learning](https://en.wikipedia.org/wiki/Deep_reinforcement_learning)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
