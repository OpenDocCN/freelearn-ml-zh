["```py\n    > install.packages(\"car\")\n    > library(car)\n\n    ```", "```py\n    > data(Quartet)\n\n    ```", "```py\n    > str(Quartet)\n    'data.frame':   11 obs. of  6 variables:\n     $ x : int  10 8 13 9 11 14 6 4 12 7 ...\n     $ y1: num  8.04 6.95 7.58 8.81 8.33 ...\n     $ y2: num  9.14 8.14 8.74 8.77 9.26 8.1 6.13 3.1 9.13 7.26 ...\n     $ y3: num  7.46 6.77 12.74 7.11 7.81 ...\n     $ x4: int  8 8 8 8 8 8 8 19 8 8 ...\n     $ y4: num  6.58 5.76 7.71 8.84 8.47 7.04 5.25 12.5 5.56 7.91 ...\n\n    ```", "```py\n    > plot(Quartet$x, Quartet$y1)\n    > lmfit = lm(y1~x, Quartet) \n    > abline(lmfit, col=\"red\") \n\n    ```", "```py\n    > lmfit\n\n    Call:\n    lm(formula = y1 ~ x, data = Quartet)\n\n    Coefficients:\n    (Intercept)            x \n     3.0001       0.5001 \n\n    ```", "```py\n> plot(Quartet$x, Quartet$y1)\n> lmfit2 = lsfit(Quartet$x,Quartet$y1)\n> abline(lmfit2, col=\"red\")\n\n```", "```py\n    > summary(lmfit)\n\n    Call:\n    lm(formula = y1 ~ x)\n\n    Residuals:\n     Min       1Q   Median       3Q      Max \n    -1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\n    Coefficients:\n     Estimate Std. Error t value Pr(>|t|) \n    (Intercept)   3.0001     1.1247   2.667  0.02573 * \n    Quartet$x     0.5001     0.1179   4.241  0.00217 **\n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    Residual standard error: 1.237 on 9 degrees of freedom\n    Multiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \n    F-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\n    ```", "```py\n    > ?summary.lm\n\n    ```", "```py\n    >  coefficients(lmfit) # Extract model coefficients\n    >  confint(lmfit, level=0.95)  # Computes confidence intervals for model parameters.\n    >  fitted(lmfit) # Extract model fitted values\n    >  residuals(lmfit) # Extract model residuals \n    >  anova(lmfit) # Compute analysis of variance tables for fitted model object\n    >  vcov(lmfit) # Calculate variance-covariance matrix for a fitted model object\n    >  influence(lmfit) # Diagnose quality of regression fits\n\n    ```", "```py\n    > lmfit = lm(y1~x, Quartet)\n\n    ```", "```py\n    > newdata = data.frame(x = c(3,6,15))\n\n    ```", "```py\n    > predict(lmfit, newdata, interval=\"confidence\", level=0.95)\n     fit      lwr       upr\n    1  4.500364 2.691375  6.309352\n    2  6.000636 4.838027  7.163245\n    3 10.501455 8.692466 12.310443\n\n    ```", "```py\n    > predict(lmfit, newdata, interval=\"predict\")\n     fit      lwr       upr\n    1  4.500364 1.169022  7.831705\n    2  6.000636 2.971271  9.030002\n    3 10.501455 7.170113 13.832796\n\n    ```", "```py\n    > par(mfrow=c(2,2))\n    > plot(lmfit)\n\n    ```", "```py\n> ?plot.lm\n\n```", "```py\n> plot(cooks.distance(lmfit))\n\n```", "```py\n    > plot(Quartet$x, Quartet$y2)\n\n    ```", "```py\n    > lmfit = lm(Quartet$y2~poly(Quartet$x,2))\n    > lines(sort(Quartet$x), lmfit$fit[order(Quartet$x)], col = \"red\")\n\n    ```", "```py\n> plot(Quartet$x, Quartet$y2)\n> lmfit = lm(Quartet$y2~ I(Quartet$x)+I(Quartet$x^2))\n\n```", "```py\n    > plot(Quartet$x, Quartet$y3)\n\n    ```", "```py\n    > library(MASS)\n    > lmfit = rlm(Quartet$y3~Quartet$x)\n    > abline(lmfit, col=\"red\")\n\n    ```", "```py\n> plot(Quartet$x, Quartet$y3)\n> lmfit = lm(Quartet$y3~Quartet$x)\n> abline(lmfit, col=\"red\")\n\n```", "```py\n    > str(SLID)\n    'data.frame':  7425 obs. of  5 variables:\n     $ wages    : num  10.6 11 NA 17.8 NA ...\n     $ education: num  15 13.2 16 14 8 16 12 14.5 15 10 ...\n     $ age      : int  40 19 49 46 71 50 70 42 31 56 ...\n     $ sex      : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 1 1 1 2 1 ...\n     $ language : Factor w/ 3 levels \"English\",\"French\",..: 1 1 3 3 1 1 1 1 1 1 ..\n\n    ```", "```py\n    > par(mfrow=c(2,2))\n    > plot(SLID$wages ~ SLID$language)\n    > plot(SLID$wages ~ SLID$age)\n    > plot(SLID$wages ~ SLID$education)\n    > plot(SLID$wages ~ SLID$sex)\n\n    ```", "```py\n    > lmfit = lm(wages ~ ., data = SLID)\n\n    ```", "```py\n    > summary(lmfit)\n\n    Call:\n    lm(formula = wages ~ ., data = SLID)\n\n    Residuals:\n     Min      1Q  Median      3Q     Max \n    -26.062  -4.347  -0.797   3.237  35.908 \n\n    Coefficients:\n     Estimate Std. Error t value Pr(>|t|) \n    (Intercept)    -7.888779   0.612263 -12.885   <2e-16 ***\n    education       0.916614   0.034762  26.368   <2e-16 ***\n    age             0.255137   0.008714  29.278   <2e-16 ***\n    sexMale         3.455411   0.209195  16.518   <2e-16 ***\n    languageFrench -0.015223   0.426732  -0.036    0.972 \n    languageOther   0.142605   0.325058   0.439    0.661 \n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    Residual standard error: 6.6 on 3981 degrees of freedom\n     (3438 observations deleted due to missingness)\n    Multiple R-squared:  0.2973,\tAdjusted R-squared:  0.2964 \n    F-statistic: 336.8 on 5 and 3981 DF,  p-value: < 2.2e-16\n\n    ```", "```py\n    > lmfit = lm(wages ~ age + sex + education, data = SLID)\n    > summary(lmfit)\n\n    Call:\n    lm(formula = wages ~ age + sex + education, data = SLID)\n\n    Residuals:\n     Min      1Q  Median      3Q     Max \n    -26.111  -4.328  -0.792   3.243  35.892 \n\n    Coefficients:\n     Estimate Std. Error t value Pr(>|t|) \n    (Intercept) -7.905243   0.607771  -13.01   <2e-16 ***\n    age          0.255101   0.008634   29.55   <2e-16 ***\n    sexMale      3.465251   0.208494   16.62   <2e-16 ***\n    education    0.918735   0.034514   26.62   <2e-16 ***\n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    Residual standard error: 6.602 on 4010 degrees of freedom\n     (3411 observations deleted due to missingness)\n    Multiple R-squared:  0.2972,\tAdjusted R-squared:  0.2967 \n    F-statistic: 565.3 on 3 and 4010 DF,  p-value: < 2.2e-16\n\n    ```", "```py\n    > par(mfrow=c(2,2))\n    > plot(lmfit)\n\n    ```", "```py\n    > lmfit = lm(log(wages) ~ age + sex + education, data = SLID)\n    > plot(lmfit)\n\n    ```", "```py\n    > vif(lmfit)\n     age       sex education \n     1.011613  1.000834  1.012179 \n    > sqrt(vif(lmfit)) > 2\n     age       sex education \n     FALSE     FALSE     FALSE\n\n    ```", "```py\n    > install.packages(\"lmtest\")\n    > library(lmtest)\n    > bptest(lmfit)\n\n     studentized Breusch-Pagan test\n\n    data:  lmfit\n    BP = 29.0311, df = 3, p-value = 2.206e-06\n\n    ```", "```py\n    > install.packages(\"rms\")\n    > library(rms)\n    > olsfit = ols(log(wages) ~ age + sex + education, data= SLID, x= TRUE, y= TRUE)\n    > robcov(olsfit)\n\n    Linear Regression Model\n\n    ols(formula = log(wages) ~ age + sex + education, data = SLID, \n     x = TRUE, y = TRUE)\n\n    Frequencies of Missing Values Due to Each Variable\n    log(wages)        age        sex  education \n     3278          0          0        249 \n\n     Model Likelihood     Discrimination \n     Ratio Test           Indexes \n    Obs     4014    LR chi2   1486.08    R2       0.309 \n    sigma 0.4187    d.f.            3    R2 adj   0.309 \n    d.f.    4010    Pr(> chi2) 0.0000    g        0.315 \n\n    Residuals\n\n     Min       1Q   Median       3Q      Max \n    -2.36252 -0.27716  0.01428  0.28625  1.56588 \n\n     Coef   S.E.   t     Pr(>|t|)\n    Intercept 1.1169 0.0387 28.90 <0.0001 \n    age       0.0176 0.0006 30.15 <0.0001 \n    sex=Male  0.2244 0.0132 16.96 <0.0001 \n    education 0.0552 0.0022 24.82 <0.0001\n\n    ```", "```py\n    >  ?SLID\n\n    ```", "```py\n    > lmfit1 = glm(wages ~ age + sex + education, data = SLID, family=gaussian)\n    > summary(lmfit1)\n\n    Call:\n    glm(formula = wages ~ age + sex + education, family = gaussian, \n     data = SLID)\n\n    Deviance Residuals: \n     Min       1Q   Median       3Q      Max \n    -26.111   -4.328   -0.792    3.243   35.892 \n\n    Coefficients:\n     Estimate Std. Error t value Pr(>|t|) \n    (Intercept) -7.905243   0.607771  -13.01   <2e-16 ***\n    age          0.255101   0.008634   29.55   <2e-16 ***\n    sexMale      3.465251   0.208494   16.62   <2e-16 ***\n    education    0.918735   0.034514   26.62   <2e-16 ***\n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    (Dispersion parameter for Gaussian family taken to be 43.58492)\n\n     Null deviance: 248686  on 4013  degrees of freedom\n    Residual deviance: 174776  on 4010  degrees of freedom\n     (3411 observations deleted due to missingness)\n    AIC: 26549\n\n    Number of Fisher Scoring iterations: 2\n\n    ```", "```py\n    > lmfit2 = lm(wages ~ age + sex + education, data = SLID)\n    > summary(lmfit2)\n\n    Call:\n    lm(formula = wages ~ age + sex + education, data = SLID)\n\n    Residuals:\n     Min      1Q  Median      3Q     Max \n    -26.111  -4.328  -0.792   3.243  35.892 \n\n    Coefficients:\n     Estimate Std. Error t value Pr(>|t|) \n    (Intercept) -7.905243   0.607771  -13.01   <2e-16 ***\n    age          0.255101   0.008634   29.55   <2e-16 ***\n    sexMale      3.465251   0.208494   16.62   <2e-16 ***\n    education    0.918735   0.034514   26.62   <2e-16 ***\n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    Residual standard error: 6.602 on 4010 degrees of freedom\n     (3411 observations deleted due to missingness)\n    Multiple R-squared:  0.2972,\tAdjusted R-squared:  0.2967 \n    F-statistic: 565.3 on 3 and 4010 DF,  p-value: < 2.2e-16\n\n    ```", "```py\n    > anova(lmfit1, lmfit2)\n    Analysis of Deviance Table\n\n    Model: gaussian, link: identity\n\n    Response: wages\n\n    Terms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev\n    NULL                       4013     248686\n    age        1    31953      4012     216733\n    sex        1    11074      4011     205659\n    education  1    30883      4010     174776\n\n    ```", "```py\n    > data(warpbreaks)\n    > head(warpbreaks)\n     breaks wool tension\n    1     26    A       L\n    2     30    A       L\n    3     54    A       L\n    4     25    A       L\n    5     70    A       L\n    6     52    A       L\n\n    ```", "```py\n    > rs1 = glm(breaks ~ tension, data=warpbreaks, family=\"poisson\")\n    > summary(rs1)\n\n    Call:\n    glm(formula = breaks ~ tension, family = \"poisson\", data = warpbreaks)\n\n    Deviance Residuals: \n     Min       1Q   Median       3Q      Max \n    -4.2464  -1.6031  -0.5872   1.2813   4.9366 \n\n    Coefficients:\n     Estimate Std. Error z value Pr(>|z|) \n    (Intercept)  3.59426    0.03907  91.988  < 2e-16 ***\n    tensionM    -0.32132    0.06027  -5.332 9.73e-08 ***\n    tensionH    -0.51849    0.06396  -8.107 5.21e-16 ***\n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    (Dispersion parameter for Poisson family taken to be 1)\n\n     Null deviance: 297.37  on 53  degrees of freedom\n    Residual deviance: 226.43  on 51  degrees of freedom\n    AIC: 507.09\n\n    Number of Fisher Scoring iterations: 4\n\n    ```", "```py\n    > head(mtcars$vs)\n    [1] 0 0 1 1 0 1\n\n    ```", "```py\n    > lm1 = glm(vs ~ hp+mpg+gear,data=mtcars, family=binomial)\n    > summary(lm1)\n\n    Call:\n    glm(formula = vs ~ hp + mpg + gear, family = binomial, data = mtcars)\n\n    Deviance Residuals: \n     Min        1Q    Median        3Q       Max \n    -1.68166  -0.23743  -0.00945   0.30884   1.55688 \n\n    Coefficients:\n     Estimate Std. Error z value Pr(>|z|) \n    (Intercept) 11.95183    8.00322   1.493   0.1353 \n    hp          -0.07322    0.03440  -2.129   0.0333 *\n    mpg          0.16051    0.27538   0.583   0.5600 \n    gear        -1.66526    1.76407  -0.944   0.3452 \n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    (Dispersion parameter for binomial family taken to be 1)\n\n     Null deviance: 43.860  on 31  degrees of freedom\n    Residual deviance: 15.651  on 28  degrees of freedom\n    AIC: 23.651\n\n    Number of Fisher Scoring iterations: 7\n\n    ```", "```py\n    > lm1 = glm(vs ~ hp+mpg+gear,data=mtcars, family=binomial(link=\"probit\"))\n\n    ```", "```py\n     > ?family\n\n    ```", "```py\n    > install.packages(\"mgcv\")\n    > library(mgcv)\n\n    ```", "```py\n    > install.packages(\"MASS\")\n    > library(MASS)\n    > attach(Boston)\n    > str(Boston)\n\n    ```", "```py\n    > fit = gam(dis ~ s(nox))\n\n    ```", "```py\n    > summary(fit)\n    Family: gaussian \n    Link function: identity \n\n    Formula:\n    dis ~ s(nox)\n\n    Parametric coefficients:\n     Estimate Std. Error t value Pr(>|t|) \n    (Intercept)  3.79504    0.04507   84.21   <2e-16 ***\n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    Approximate significance of smooth terms:\n     edf Ref.df   F p-value \n    s(nox) 8.434  8.893 189  <2e-16 ***\n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    R-sq.(adj) =  0.768   Deviance explained = 77.2%\n    GCV = 1.0472  Scale est. = 1.0277    n = 506\n\n    ```", "```py\n     > ? bam\n\n    ```", "```py\n    > plot(nox, dis)\n\n    ```", "```py\n    > x = seq(0, 1, length = 500)\n    > y = predict(fit, data.frame(nox = x))\n    > lines(x, y, col = \"red\", lwd = 2)\n\n    ```", "```py\n    > plot(fit)\n\n    ```", "```py\n> fit2=gam(medv~crim+zn+crim:zn, data=Boston)\n> vis.gam(fit2)\n\n```", "```py\n    > gam.check(fit)\n\n    Method: GCV   Optimizer: magic\n    Smoothing parameter selection converged after 7 iterations.\n    The RMS GCV score gradient at convergence was 8.79622e-06 .\n    The Hessian was positive definite.\n    The estimated model rank was 10 (maximum possible: 10)\n    Model rank =  10 / 10 \n\n    Basis dimension (k) checking results. Low p-value (k-index<1) may\n    indicate that k is too low, especially if edf is close to k'.\n\n     k'   edf k-index p-value\n    s(nox) 9.000 8.434   0.397       0\n\n    ```", "```py\n> ?gam.check\n\n```", "```py\n> ?choose.k\n\n```"]