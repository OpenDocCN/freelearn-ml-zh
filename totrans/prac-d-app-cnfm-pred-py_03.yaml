- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fundamentals of Conformal Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will dive into **conformal prediction**, a powerful and versatile
    probabilistic prediction framework. Conformal prediction allows for effective
    quantification of uncertainty in machine learning applications. By learning and
    utilizing conformal prediction techniques, you will be able to make more informed
    decisions and manage risks associated with data-driven solutions more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will cover the mathematical underpinnings of conformal prediction.
    You will learn how to accurately measure the uncertainty that comes with your
    predictions. You will also become familiar with nonconformity measures, grasp
    the idea of prediction sets, and be able to evaluate your model’s performance
    in a thorough and meaningful manner. The abilities you will acquire through this
    chapter will be highly valuable in various academic and industrial fields where
    comprehending the uncertainty associated with predictions is essential.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentals of conformal prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic components of a conformal predictor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By mastering the concepts and techniques presented in this chapter, you will
    be well equipped to harness the power of conformal prediction and effectively
    apply it to your industrial applications.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentals of conformal prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the fundamentals of conformal prediction. There
    are two variants of conformal prediction – **inductive conformal prediction**
    (**ICP**) and **transductive conformal prediction** (**TCP**). We will discuss
    the benefits of the conformal prediction framework and learn about the basic components
    of conformal predictors and the different types of nonconformity measures. We
    will also learn how to use nonconformity measures to create probabilistic prediction
    sets in classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Definition and principles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Conformal prediction is a machine learning framework that quantifies uncertainty
    to produce probabilistic predictions. These predictions can be prediction sets
    for classification tasks or prediction intervals for regression tasks. Conformal
    prediction has significant advantages in equipping statistical, machine learning,
    and deep learning models with valuable additional features that instill confidence
    in their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it is the only uncertainty quantification framework that offers strong
    mathematical assurances that the error rate will never surpass the significance
    level determined by the user. Simply put, conformal prediction models always generate
    valid and unbiased prediction sets and prediction intervals, which is a crucial
    aspect of making informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: For comprehensive resources, visit *Awesome Conformal Prediction* ([https://github.com/valeman/awesome-conformal-prediction](https://github.com/valeman/awesome-conformal-prediction)).
    It is the most extensive, professionally curated resource on conformal prediction.
    Over time, conformal prediction has developed into an extensive framework suitable
    for use with any underlying point prediction model, regardless of the size of
    the dataset, the underlying point prediction model, or the data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, the principles of conformal prediction can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Validity**: The objective of conformal prediction is to create **prediction
    regions** (such as **prediction** **sets** for classification tasks or **prediction
    intervals** for regression tasks) that encompass the actual target value with
    a confidence level specified by the user. The goal is to attain a coverage probability
    at least as large as the user-defined confidence level. For example, if the user
    selects a 95% confidence level, the prediction regions (sets or intervals) should
    contain the correct target values at least 95% of the time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency**: Conformal prediction aims to generate prediction intervals
    or regions that are as small as possible while preserving the desired confidence
    level. This approach ensures the predictions are valid and precise while conveying
    useful information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptivity**: Conformal prediction aims to generate prediction sets that
    are adaptive to individual examples. For examples that are hard to predict, prediction
    sets are expected to be wider to account for uncertainty in predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distribution-free**: Conformal prediction is a versatile and robust framework
    that can be utilized with various data types and machine learning tasks since
    it does not depend on specific assumptions about the underlying data distribution.
    The only assumption made by conformal prediction is data exchangeability, which
    is a less restrictive requirement than that of **independent, identically distributed**
    (**IID**)data. Nevertheless, conformal prediction has succeeded in numerous applications
    beyond exchangeability, including time series and forecasting applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Online adaptivity**: Conformal prediction can operate in both online and
    offline scenarios. In online settings, a conformal predictor can adjust to new
    incoming data points and modify its predictions accordingly without retraining
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compatibility**: Conformal prediction is a flexible framework that can be
    seamlessly integrated with various statistical and machine learning techniques,
    including decision trees, neural networks, support vector machines, boosted trees
    (XGBoost/LightGBM/CatBoost), bagging trees (random forest), and deep learning
    models. This is achieved by defining an appropriate nonconformity measure that
    can be applied to any existing machine learning algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-intrusive**: Conformal prediction does not necessitate statistical, machine
    learning, or deep learning point prediction model changes. This is particularly
    important for models that have been deployed into production. Conformal prediction
    can be added as an uncertainty quantification layer on top of any deployed model
    without any modification or knowledge of how the prediction model functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretability**: Conformal prediction produces prediction sets and intervals
    that are easy to understand and provide a clear way to measure uncertainty. This
    makes it a valuable tool for industries such as finance, healthcare, and autonomous
    vehicles, where understanding prediction uncertainty is crucial.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will look at the basic components of a conformal predictor
    and learn about nonconformity measures.
  prefs: []
  type: TYPE_NORMAL
- en: Basic components of a conformal predictor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now look at the basic components of a conformal predictor:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nonconformity measure**: The nonconformity measure is a function that evaluates
    how much a new data point differs from the existing data points. It compares the
    new observation to either the entire dataset (in the full transductive version
    of conformal prediction) or the calibration set (in the most popular variant –
    **ICP**. The selection of the nonconformity measure is based on a particular machine
    learning task, such as classification, regression, or time series forecasting,
    as well as the underlying model. This chapter will examine several nonconformity
    measures suitable for classification and regression tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calibration set**: The calibration set is a portion of the dataset used to
    calculate nonconformity scores for the known data points. These scores are a reference
    for establishing prediction intervals or regions for new test data points. The
    calibration set should be a representative sample of the entire data distribution
    and is typically randomly selected. The calibration set should contain a sufficient
    number of data points (at least 500). If the dataset is small and insufficient
    to reserve enough data for the calibration set, the user should consider other
    variants of conformal prediction – including **TCP** (see, for example, *Mastering
    Classical Transductive Conformal Prediction in Action* – [https://medium.com/@valeman/how-to-use-full-transductive-conformal-prediction-7ed54dc6b72b](https://medium.com/@valeman/how-to-use-full-transductive-conformal-prediction-7ed54dc6b72b)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test set**: The test set contains new data points for generating predictions.
    For every data point in the test set, the conformal prediction model calculates
    a nonconformity score using the nonconformity measure and compares it to the scores
    from the calibration set. Using this comparison, the conformal predictor generates
    a prediction region that includes the target value with a user-defined confidence
    level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these components work in tandem to create a conformal prediction framework
    that facilitates valid and efficient uncertainty quantification in a wide range
    of machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we examined nonconformity measures, which are the fundamental
    element of any conformal prediction model. To recap, the primary role of the nonconformity
    measure is to enable the quantification of uncertainty for new data points by
    evaluating the extent to which they differ from previously observed data.
  prefs: []
  type: TYPE_NORMAL
- en: In the conformal prediction framework, any model provides valid prediction sets
    regardless of the chosen nonconformity measure. However, selecting the proper
    nonconformity measure is essential for creating more precise, informative, and
    adaptive prediction regions.
  prefs: []
  type: TYPE_NORMAL
- en: In conformal prediction, the size of these regions determines the effectiveness
    of predictive systems. Smaller regions are considered more efficient and informative.
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of a conformal prediction model can be influenced by the nonconformity
    measure that is chosen by the user. However, the best nonconformity measure for
    machine learning is dependent on the context.
  prefs: []
  type: TYPE_NORMAL
- en: For further understanding, you can explore the *no free l**unch* theorem, which
    discusses the limitations of universal optimization and learning algorithms. You
    can find more information on this theorem at ([https://en.wikipedia.org/wiki/No_free_lunch_theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem
    )).
  prefs: []
  type: TYPE_NORMAL
- en: Despite the theorem’s implications, research papers have provided valuable insights
    into selecting effective nonconformity measures. These papers have examined diverse
    datasets and offer guidance on choosing nonconformity measures that have demonstrated
    effectiveness across various scenarios. By leveraging these research findings,
    you can make informed decisions when selecting nonconformity measures for your
    specific machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the research paper titled *Model-Agnostic Nonconformity Functions for Conformal
    Classification* ([https://ieeexplore.ieee.org/abstract/document/7966105](https://ieeexplore.ieee.org/abstract/document/7966105)),
    the authors examined the efficiency of three model-agnostic nonconformity measures
    for classification problems. In the experiments on 21 multi-class datasets using
    neural networks and neural network ensembles as classifiers, the authors discovered
    that the choice of the nonconformity measure substantially influenced the efficiency
    of prediction sets. These findings highlight the importance of selecting an appropriate
    nonconformity measure to enhance the efficiency of conformal prediction in classification
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: The researchers concluded that choosing the optimal nonconformity measure depends
    on the efficiency metric most suitable for the use case. When evaluating the efficiency
    in terms of the proportion of single-label predictions (singletons), the margin-based
    nonconformity measure emerged as the preferred option. On the other hand, when
    assessing the average width of prediction sets, the hinge loss measure resulted
    in the narrowest prediction sets, indicating its effectiveness in producing more
    precise and focused predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In conformal prediction, the nonconformity measures are derived from the predictions
    generated by the underlying point prediction model. Instances more prone to misclassification
    or greater inherent uncertainty are assigned higher nonconformity scores. The
    efficiency of a conformal prediction model depends on both the accuracy of the
    underlying model and the quality of the chosen nonconformity measure. Selecting
    an appropriate nonconformity measure becomes particularly crucial when dealing
    with challenging datasets where the underlying point prediction model needs additional
    support to classify objects accurately.
  prefs: []
  type: TYPE_NORMAL
- en: Types of nonconformity measures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two types of nonconformity measures – **model-dependent** and **model-independent**
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Model-dependent nonconformity measures are specific to a particular type of
    underlying model used in conformal prediction. These measures rely on the internal
    workings or characteristics of the model to compute nonconformity scores. Unlike
    model-agnostic nonconformity measures, which can be applied to any type of point
    prediction model, model-dependent measures are tailored to the specific model
    used.
  prefs: []
  type: TYPE_NORMAL
- en: Model-dependent nonconformity measures take advantage of the unique features
    or properties of the underlying model to assess the deviation or uncertainty of
    a new data point from the training (in classical TCP) or calibration (in ICP)
    data. These measures can be customized based on the model’s output, such as the
    probability estimates or decision boundaries. They can also leverage model-specific
    attributes, such as the learned weights or parameters, to determine the nonconformity
    scores.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of model-dependent nonconformity measures include the distance to support
    vectors in support vector machines, the residual error in linear regression models,
    or the discrepancy between predicted and actual class probabilities in probabilistic
    classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Model-dependent nonconformity measures offer the advantage of potentially capturing
    model-specific information and characteristics, leading to tailored and potentially
    more accurate uncertainty quantification. However, they are limited to the specific
    model they are designed for and may not generalize well to other models. For this
    reason, we will only cover model-independent nonconformity measures.
  prefs: []
  type: TYPE_NORMAL
- en: In *Model-Agnostic Nonconformity Functions for Conformal Classification*, the
    authors investigated three popular loss functions – hinge loss, margin, and Brier
    score – as popular choices of model-independent nonconformity measures for predictive
    classification. Since these functions work with any classification model producing
    class estimates, they can be utilized with any classifier that generates class
    scores, making them model-agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that, in a conformal predictor, only the ordering of
    nonconformity scores matters. In two-class problems, all three loss functions
    – hinge loss, margin, and Brier score – will arrange the instances in the same
    order, resulting in the same efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: To compare these efficiency measures, the authors focused on multi-class problems.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the three nonconformity measures in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Hinge loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The hinge loss (also sometimes called **LAC loss** or **inverse probability**)
    can be described in the context of classification problems where we obtain class
    probabilities as outputs from a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a particular instance, let’s say the true label is *y* and the predicted
    probability of the model for that label is *P(y)*. Then, the hinge loss for this
    instance is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: Hinge loss = 1 − P(y)
  prefs: []
  type: TYPE_NORMAL
- en: Explanation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If the model is very confident and assigns a probability of 1 to the true label
    *y*, then the hinge loss is 0\. This indicates a perfect prediction.
  prefs: []
  type: TYPE_NORMAL
- en: If the model assigns a probability of 0 to the true label *y*, then the hinge
    loss is 1\. This indicates a completely incorrect prediction.
  prefs: []
  type: TYPE_NORMAL
- en: For probabilities between 0 and 1, the hinge loss will range between 0 and 1,
    with higher values indicating lower confidence in the correct label and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, the hinge loss gives a measure of how “off” the prediction is from
    the true label. Lower hinge loss values are better, indicating that the predicted
    probability for the true label is closer to 1\. Conversely, higher hinge loss
    values indicate greater disagreement between the predicted probability and the
    true label.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate how to compute the hinge loss nonconformity score, consider a
    classifier that produces three class scores: *class_0 = 0.5*, *class_1 = 0.3*,
    and *class_2 = 0.2*,and the actual label *y =* *1*.'
  prefs: []
  type: TYPE_NORMAL
- en: To compute the nonconformity score, take the probability score of the true class
    (in this case, 1) and subtract it from 1\. Thus, this example’s inverse probability
    (hinge) nonconformity score is 0.7.
  prefs: []
  type: TYPE_NORMAL
- en: Margin nonconformity measure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The margin nonconformity measure is defined as the difference between the predicted
    probability of the most likely incorrect class label and the predicted probability
    of the true label:'
  prefs: []
  type: TYPE_NORMAL
- en: Δ[h(x i), y i] = max y≠y i   ˆ P  h(y ∣ x i) −  ˆ P  h(y i ∣ x i)
  prefs: []
  type: TYPE_NORMAL
- en: Explanation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The measure captures the difference in probabilities between the highest probability
    given to any incorrect label and the probability of the true label *y*i for the
    instance *x*i. For a particular instance *x*i with true label *y*i, we first identify
    the probability of the most likely incorrect class and then subtract from this
    value the probability of the true label to get the margin.
  prefs: []
  type: TYPE_NORMAL
- en: If the margin is close to zero or negative, it means that the model is confident
    in its prediction for the true class label and there isn’t another class with
    a closely competing probability. This indicates a conforming example.
  prefs: []
  type: TYPE_NORMAL
- en: If the margin is positive and large, it indicates that there’s another class
    (an incorrect one) for which the model assigns a higher probability than the true
    class. This is a nonconforming example, indicating that the model is more confident
    in an incorrect class than in the true one.
  prefs: []
  type: TYPE_NORMAL
- en: The larger the margin, the more nonconforming the example is, as it suggests
    greater disagreement between the predicted probabilities for the true class and
    the most likely incorrect class.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, the margin-based nonconformity measure gives an indication of how
    “risky” a prediction is. If the measure is high, it indicates potential problems
    with the model’s prediction for that instance, signaling that the prediction might
    be unreliable. If the measure is close to zero or negative, it means the model
    is more confident in its prediction of the true class.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate how to compute the margin nonconformity score, consider a classifier
    that produces three class scores: *class_0 = 0.5*, *class_1 = 0.3*, *and class_2
    = 0.2*, and the actual label *y =* *1*.'
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the margin nonconformity score, one would take the probability
    of the most likely but incorrect class (in this case, 0) and subtract it from
    the probability of the true class (1) to get a margin nonconformity score of 0.2.
  prefs: []
  type: TYPE_NORMAL
- en: The Brier score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Brier score measures the accuracy of probability-based predictions in classification
    tasks. It calculates the squared difference between the predicted probabilities
    and the actual binary results. The score’s values can range from 0 (perfect accuracy)
    to 1 (complete inaccuracy).
  prefs: []
  type: TYPE_NORMAL
- en: The Brier score is an example of a **proper scoring rule** (another example
    of a proper scoring rule in classification problems is log loss).
  prefs: []
  type: TYPE_NORMAL
- en: A proper scoring rule is a metric used to evaluate the accuracy of probabilistic
    predictions. Specifically, it’s a rule that assigns a numerical score to each
    prediction in such a way that the most accurate (or calibrated) probabilistic
    forecast will, on average, receive a better (typically lower) score than any other
    biased or less accurate forecast.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind a proper scoring rule is to encourage honest reporting of probabilities.
    If a scoring rule is “proper,” then a forecaster has the best expected score when
    they report their true beliefs or true estimated probabilities, rather than exaggerating
    or downplaying their forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, a proper scoring rule ensures that forecasters are best rewarded,
    in terms of the score, when they provide their genuine assessments of the probabilities
    of events.
  prefs: []
  type: TYPE_NORMAL
- en: As a proper scoring rule, the Brier score promotes well-calibrated probability
    estimates. It uniquely captures both the calibration, which refers to the alignment
    between predicted probabilities and actual outcomes, and the discrimination, or
    the model’s ability to distinguish between the classes.
  prefs: []
  type: TYPE_NORMAL
- en: Glenn W. Brier, who worked in weather forecasting, invented the Brier score
    in the 1950s and described it in his paper *Verification of forecasts expressed
    in terms of probability* ([https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml](https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml)).
    Along with log loss, the Brier score is widely used today to evaluate the performance
    of probabilistic classifiers and understand the quality of predicted probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate how to compute the Brier score using our example, consider the
    same classifier, which produces three class scores: *class_0 = 0.5*, *class_1
    = 0.3*, *and class_2 = 0.2*, and the actual label *y = 1*. Here’s a step-by-step
    guide to how to compute the Brier score for the given example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Encode the* *actual class*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For a multi-class problem, you’ll want to use one-hot encoding for the actual
    labels:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*class_0*: 0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*class_1*: 1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*class_2*: 0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Compute the squared differences for* *each class*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the squared difference between predicted probabilities and the actual
    outcomes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*For class 0: (0-0.5)^2 =* *0.25*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*For class 1: (1-0.3)^2 =* *0.49*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*For class 2: (0-0.2)^2 =* *0.04*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Average the* *squared differences*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Brier score =  0.25 + 0.49 + 0.04  _____________ 3  = 0.26
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the Brier score for this example is 0.26\. A lower Brier score indicates
    better performance, with 0 being the best possible score.
  prefs: []
  type: TYPE_NORMAL
- en: Effectiveness of model-agnostic nonconformity measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the paper *Model-Agnostic Nonconformity Functions for Conformal Classification*,
    the authors assessed the effectiveness of three nonconformity measures using two
    criteria: **one-class classification** (**OneC**) and *AvgC*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*OneC* refers to the proportion of all predictions that consist of singleton
    sets containing only one label. These sets are desired because they provide the
    most informative predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**AvgC**, on the other hand, refers to the average number of class labels in
    the prediction set. A lower **AvgC** value indicates that the model is better
    at producing more specific and informative predictions by eliminating class labels
    that do not fit well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider a binary classification problem with the following predictions
    for five instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prediction Set* *1: {0}*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prediction Set* *2: {1}*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prediction Set 3: {**0, 1}*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prediction Set* *4: {1}*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prediction Set* *5: {0}*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this case, there are five predictions, four of which are singleton sets
    (prediction sets 1, 2, 4, and 5). To calculate **OneC**, we compute the proportion
    of singleton sets among all predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*OneC = (Number of Singleton Sets) / (Total Number of Prediction Sets) = (4)
    / (5) =* *0.8*'
  prefs: []
  type: TYPE_NORMAL
- en: A higher **OneC** value indicates that the conformal prediction model produces
    specific and informative predictions more efficiently. In this example, 80% of
    the prediction sets are singletons, reflecting a relatively efficient classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate **AvgC**, which represents the average number of class labels
    in the prediction set, we first compute the sum of the class labels in each prediction
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction Set 1**: 1 label'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction Set 2**: 1 label'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction Set 3**: 2 labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction Set 4**: 1 label'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction Set 5**: 1 label'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of the class labels is 1 + 1 + 2 + 1 + 1 = 6.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we divide this sum by the total number of prediction sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '*AvgC = (Sum of Class Labels) / (Total Number of Prediction Sets) = (6) / (5)
    =* *1.2*'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, **AvgC** is 1.2, indicating that, on average, each prediction
    set contains 1.2 class labels. A lower **AvgC** value signifies that the model
    is better at producing more specific and informative predictions. In this case,
    the **AvgC** value of 1.2 reflects a relatively efficient classifier, as it is
    close to the minimum possible value of 1, which would occur if all prediction
    sets were singleton sets.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have determined that the most effective approach is to use a margin-based
    nonconformity function to achieve a high rate of singleton predictions (**OneC**).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a nonconformity measure utilizing the hinge (inverse probability)
    nonconformity measure yielded the smallest label sets on average, as measured
    by **AvgC**.
  prefs: []
  type: TYPE_NORMAL
- en: What is the intuition behind such results?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To achieve a high **OneC** score, the predictions should predominantly be singleton
    sets that contain only one label. This means that high nonconformity scores must
    be assigned to all other labels. The margin-based nonconformity measure fosters
    this outcome, especially when the underlying model attributes a high probability
    to a single label. In this scenario, the probability associated with that single
    label is added to the nonconformity scores for all other labels, effectively promoting
    the selection of a singleton set and thereby improving the **OneC** performance.
  prefs: []
  type: TYPE_NORMAL
- en: However, the hinge loss function, which assesses each label on an individual
    basis, might only exclude some labels, leaving the high-probability ones intact
    in certain cases. This situation arises because all other labels must have inherently
    low probabilities, and the hinge loss function does not take into account how
    the remaining probability mass is distributed specifically to the high-probability
    label. Consequently, the hinge loss function’s inability to consider this distribution
    may lead to it only eliminating some labels and not necessarily focusing on the
    high-probability ones. This difference in how the two nonconformity measures assign
    scores leads to margin-based nonconformity measures being better suited for achieving
    a high proportion of singleton predictions.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, to obtain smaller label sets on average as measured by **AvgC**,
    a nonconformity measure based on the hinge (inverse probability) nonconformity
    measure is more effective. This is because hinge loss considers only the probability
    of the true class label. In contrast, the margin-based nonconformity measure considers
    both the probability of the true class label and the most likely incorrect class
    label. This leads to the margin-based nonconformity measure producing broader
    sets on average, whereas hinge loss is more likely to eliminate incorrect labels
    and produce smaller, more informative sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to classification, regression problems are relatively straightforward
    regarding the selection of nonconformity measures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Absolute error**: The absolute error nonconformity measure is the absolute
    difference between the predicted value and the true target value for a given data
    point. This measure can be used with any regression model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonconformity (x) = |y _ pred − y _ true|
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalized error**: The normalized error nonconformity measure is the absolute
    error divided by an estimate of the prediction error’s scale, such as the **mean
    absolute error** (**MAE**) or the standard deviation of the residuals. This measure
    can be used with any regression model and helps account for heteroscedasticity
    in the data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonconformity (x) = |y _ pred − y _ true| / scale
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s now consider the pros and cons of both nonconformity measures in the regression
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Absolute error
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The pros are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplicity**: It is straightforward to compute and understand, making it
    a go-to choice for many practitioners'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uniform interpretation**: Given that it is not scaled, the interpretation
    remains consistent across different datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The cons are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scale sensitivity**: The absolute error can be sensitive to the scale of
    the target variable. For datasets with large target values, the absolute error
    might be large, even if the predictions are relatively accurate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No consideration for data distribution**: It does not consider the variability
    or distribution of errors in the dataset, which might lead to overly optimistic
    or pessimistic conformal predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalized error
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The pros are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scale invariance**: By normalizing the error with respect to the error’s
    scale (e.g., MAE or standard deviation of residuals), it becomes less sensitive
    to the scale of the target variable, allowing for more consistent performance
    across datasets with varying scales.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accounts for heteroscedasticity**: This measure can be particularly useful
    for data that exhibits heteroscedasticity (i.e., where the variability of errors
    changes across the data). By normalizing with a measure of spread, it can give
    a more accurate representation of the prediction’s relative accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More adaptive**: The normalization factor can adapt to the local properties
    of the data, providing more meaningful error measurements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The cons are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complexity**: It introduces an additional layer of complexity as one needs
    to determine the best way to normalize the errors. The choice of normalization
    (e.g., using MAE versus standard deviation of residuals) can influence the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risk of misleading results**: If the normalization factor is not well chosen
    or if it is computed from a small sample, it might lead to misleading conformal
    predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Requires more data**: Estimating the scale of prediction error typically
    requires a sufficiently large sample size to be reliable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of conformal prediction for regression, the choice between these
    measures often depends on the properties of the data and the specific needs of
    the application. If heteroscedasticity is a concern, the normalized error might
    be more appropriate. Otherwise, the simplicity of the absolute error might be
    preferred.
  prefs: []
  type: TYPE_NORMAL
- en: The second component of a conformal predictor is the **calibration set**, which
    is used to compute nonconformity scores for the known data points. The calibration
    set is a feature of the most popular variant, ICP, while TCP does not require
    a calibration set. In contrast to ICP, TCP utilizes all available data, making
    it efficient regarding data usage. However, TCP is computationally inefficient,
    requiring retraining the underlying point prediction model for each new test object.
  prefs: []
  type: TYPE_NORMAL
- en: We will focus on ICP, the most popular and widely used variant in current research
    and open source libraries, to better understand conformal prediction. ICP was
    introduced in a 2002 paper *Inductive Confidence Machines for* *Regression* ([https://link.springer.com/chapter/10.1007/3-540-36755-1_29](https://link.springer.com/chapter/10.1007/3-540-36755-1_29)).
  prefs: []
  type: TYPE_NORMAL
- en: ICP has a significant advantage in terms of computational efficiency, as it
    is almost as fast as the underlying point prediction model. This is because ICP
    generates a single model, based on the training data, which can then be used to
    produce predictions for all test instances. Any predictive model can be combined
    with ICP to convert it into a conformal predictor.
  prefs: []
  type: TYPE_NORMAL
- en: When developing an ICP, remember that the training set is exclusively for training
    the base prediction model. Do not use it to construct the conformal predictor.
    Likewise, the calibration set should be reserved solely for the conformal predictor
    and not for training the base model.
  prefs: []
  type: TYPE_NORMAL
- en: The main objective of conformal prediction is to provide valid prediction sets
    for new, unseen examples. The ICP approach enables the model to learn about uncertainty
    by comparing predictions made by the underlying point prediction model with the
    actual labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ICP is constructed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide your training data into two disjoint subsets *I**T* and *I**C* where
    *T* and *C* denote the proper training and calibration sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train your point prediction model, *H*, using data exclusively from the appropriate
    proper training set, IT. As discussed in the previous chapter, H can be any point
    prediction model, including statistical, machine learning, deep learning, or even
    any model based on expert opinions, business rules, or heuristics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use an appropriate nonconformity measure for your classification or regression
    task to calculate nonconformity scores α1, α2, ..., αn, where *n* represents the
    total number of data points in the calibration dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tentatively assign a label *y* as the potential label for the new test point
    *x*, and compute the nonconformity score α for (x,y).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the p-value as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'p = |{z i : α i ≥ : α T}| + 1 _ n + 1 .'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s briefly discuss this important step of calculating the p-value for each
    test data point. In previous steps, we computed the nonconformity scores for all
    points in the calibration set. We then compute the nonconformity score of a new
    test point to determine how the test point’s nonconformity score compares to those
    in the calibration set. The p-value provides a measure of how different the test
    object is from the calibration data, based on its nonconformity score. It is calculated
    by first calculating a nonconformity score for the test object using the model,
    then calculating nonconformity scores for all objects in the calibration set using
    the same model. The number of calibration objects that have a nonconformity score
    greater than or equal to the test object’s score is then counted, and 1 is added
    to this count. This count plus 1 is then divided by the total number of calibration
    objects plus 1 to determine the p-value, which indicates the proportion of calibration
    objects that are at least as extreme as the test object. The resulting p-value
    provides a quantitative metric of how nonconforming the test object is to the
    pattern in the calibration data, with a low p-value meaning the test object is
    highly unusual compared to the calibration data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Conformal prediction’s core idea is to assign a tentative label y to the test
    point (a class label in classification problems or a real y value in regression
    problems) and evaluate how well the test object, including its features and assigned
    label, fits in with the observed objects from the calibration set. To measure
    this fit, we compute the p-value by comparing the test object’s “strangeness”
    using the test object’s nonconformity score to that of the calibration set objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is essential to note that when calculating the p-value, the numerator and
    denominator (n+1) include the test object in the same bag, together with the calibration
    set data. Due to the exchangeability assumption, data can be shuffled, making
    all data points equivalent in terms of their order.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once we have the p-value for the test point with a tentatively assigned label,
    we compare it to the significance level. If the p-value is lower than the significance
    level, it indicates that very few, if any, objects in the calibration set are
    as strange as our test point. This suggests that the proposed label y does not
    fit, and we exclude it from the prediction set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the other hand, if the p-value is equal to or higher than the significance
    level, assigning the potential label y would not make our test object particularly
    strange, given the observed data. In this sense, we use p-values to test the statistical
    hypothesis of whether each potential y value fits previously observed data given
    the exchangeability assumption.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We repeat the process mentioned above for each possible value of y. As a result,
    we obtain a prediction set that includes the true label with a probability of
    1 - ε, where ε is our chosen significance level (for example, 5%).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will now discuss the concepts of confidence and credibility, which help
    assess the quality of predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Confidence level**: The confidence level, denoted by (1 - ε), represents
    the probability with which the true label (or value) falls within the prediction
    set. A higher confidence level indicates that the predictions are more likely
    to be accurate. The confidence level is usually chosen in advance, and common
    values are 0.95 (95%) or 0.99 (99%). With a 95% confidence level, for example,
    one can expect that the true label will be included in the prediction set 95%
    of the time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Credibility level**: The credibility level, denoted by p, measures the likelihood
    of each element within the prediction set being the true label. We have discussed
    calculating p-values for each potential value of label ‘y’. In classification
    tasks, credibility levels can be interpreted as a normalized measure of confidence
    in each class label. Credibility levels help determine the most likely value(s)
    within the prediction interval for regression tasks. The credibility level can
    be used as a threshold to filter out less probable predictions, leading to a more
    precise, albeit smaller, prediction set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s consider a binary classification problem with two possible class labels:
    *A* and *B*. We’ll use a conformal prediction framework to calculate confidence
    and credibility levels for a test data point.'
  prefs: []
  type: TYPE_NORMAL
- en: Assume we have already trained a point prediction model and calculated the nonconformity
    scores for the calibration dataset and the test point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nonconformity scores for the calibration dataset are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Point 1 (Label* *A): 0.4*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Point 2 (Label* *A): 0.3*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Point 3 (Label* *B): 0.2*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Point 4 (Label* *B): 0.5*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nonconformity scores are calculated for a test point using the model’s predicted
    labels and probabilities. For example, if the model produces probability estimates
    of 0.75 for class A and 0.65 for class B, nonconformity scores would be computed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: For class A, the model assigns a probability of 0.75\. The nonconformity score
    is calculated by subtracting this probability from 1, giving 1–0.75 = 0.25.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For class B, the model assigns a probability of 0.65\. The nonconformity score
    is 1–0.65 = 0.35.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, the nonconformity score is 1 minus the model’s probability for the
    tentative label. This measures how much the prediction deviates from full confidence
    (a probability of 1). Higher nonconformity scores mean the model’s label assignments
    are less certain or conforming. Hinge loss is commonly used when the model outputs
    probability estimates for each class. Subtracting the probability from 1 provides
    an intuitive nonconformity measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we’ll calculate p-values for each tentative label:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Label A*: Count the number of calibration points with nonconformity scores
    greater than or equal to 0.25: 3 (Points 1, 2, and 4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: p−value = (3 + 1) / (4 + 1) = 4 / 5 = 0.8
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Label B*: Count the number of calibration points with nonconformity scores
    greater than or equal to 0.35: 2 (Points 1 and 4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: p−value = (2 + 1) / (4 + 1) = 3 / 5 = 0.6
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These p-values are the credibility levels for each label. Thus, the credibility
    of label A is 0.8, and the credibility of label B is 0.6.
  prefs: []
  type: TYPE_NORMAL
- en: Our desired confidence level is 95% (1–ε = 0.95, ε = 0.05). Since both credibility
    levels (p-values) are greater than the chosen significance level ε = 0.05, we
    include both labels A and B in the prediction set.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the confidence level is 95%, indicating that the true label
    is included in the prediction set with a 95% probability. The credibility levels
    are 0.8 for label A and 0.6 for label B, suggesting that label A is more likely
    to be the correct label than label B.
  prefs: []
  type: TYPE_NORMAL
- en: However, both labels are included in the prediction set because their credibility
    levels are higher than the significance level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Online and offline conformal prediction are two variants of conformal prediction
    that differ in how they process and incorporate new data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Offline conformal prediction**: In offline conformal prediction, a model
    is trained on a fixed dataset, and the nonconformity scores are calculated for
    a separate calibration dataset. The model does not update or change as new data
    points become available. This method is suitable when the dataset is static or
    when you have a large amount of data available for training and calibration. The
    disadvantage of offline conformal prediction is that it doesn’t adapt to new data
    or changes in data distribution over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Online conformal prediction**: In online conformal prediction, the model
    continuously updates as new data points become available. It incorporates new
    information by updating the nonconformity scores and adjusting the predictions
    accordingly. Online conformal prediction is particularly useful when working with
    streaming data or when the underlying data distribution changes over time. This
    method lets the model stay up to date with the most recent data, providing more
    accurate predictions in dynamic environments. However, online conformal prediction
    can be computationally more demanding due to the need for constant updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conditional and unconditional coverage are two criteria used to evaluate the
    performance of prediction intervals in forecasting models. They assess the coverage
    of the true values by the prediction intervals, but they focus on different aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unconditional coverage**: Unconditional coverage assesses the proportion
    of true values that fall within the prediction intervals without considering specific
    conditions or patterns. It measures the overall ability of the prediction intervals
    to capture the true values across the entire dataset. A model with good unconditional
    coverage will include the true values within the prediction intervals for a specified
    proportion (e.g., 95%) of the time. Unconditional coverage is useful for evaluating
    the general performance of a model, but it does not account for potential dependencies
    between observations or changes in data distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conditional coverage**: On the other hand, conditional coverage evaluates
    the performance of prediction intervals while accounting for specific conditions
    or patterns in the data. It examines how well the prediction intervals capture
    the true values when considering subsets of the data that share certain characteristics
    or dependencies (e.g., time periods, categories, etc.). A model with good conditional
    coverage will maintain the desired coverage rate for each specific condition or
    subset of the data. Conditional coverage provides a more nuanced evaluation of
    a model’s performance, helping to identify potential weaknesses or biases in the
    model’s predictions for certain data subsets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, unconditional coverage evaluates the overall ability of a model’s
    prediction intervals to include the true values. In contrast, conditional coverage
    assesses the performance of prediction intervals within specific conditions or
    data subsets. Both criteria are important for understanding the performance of
    forecasting models, but they focus on different aspects of a model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conformal prediction is a framework for producing reliable and valid predictions
    with quantifiable uncertainty. It can be applied to a wide range of machine learning,
    statistical, or deep learning models and other prediction methods. Here, we’ll
    discuss the relationship of conformal prediction with other frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Traditional machine learning frameworks**: Conformal prediction can be combined
    with traditional machine learning methods (e.g., linear regression, SVM, decision
    trees, etc.) to provide valid confidence or credibility measures for the predictions.
    By doing so, conformal prediction enhances these methods, giving users a better
    understanding of the uncertainty associated with each prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble methods**: Ensemble methods such as bagging, boosting, and random
    forests can also benefit from conformal prediction. By adding conformal prediction
    to these methods, the ensemble can produce a point estimate and a prediction interval
    or set with associated confidence levels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep learning frameworks**: Conformal prediction can be integrated with deep
    learning models, such as neural networks, to provide quantifiable uncertainty
    estimates for their predictions. This allows practitioners to better understand
    the reliability of the predictions produced by these complex models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian frameworks**: Bayesian methods inherently provide uncertainty quantification
    through probability distributions. However, conformal prediction can still be
    combined with Bayesian frameworks to offer a frequentist approach to uncertainty
    quantification. This combination can provide a complementary perspective on the
    uncertainty associated with predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model validation techniques**: Conformal prediction can be used alongside
    model validation techniques such as cross-validation or bootstrapping to assess
    the performance of a model. While these validation techniques evaluate the model’s
    accuracy and generalization, conformal prediction provides a complementary perspective
    on the model’s uncertainty quantification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B19925_04.xhtml#_idTextAnchor040), we will look into the concepts
    of validity and efficiency in the context of probabilistic prediction models,
    building upon the foundations laid in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have deep-dived into the fundamentals and mathematical foundations
    of conformal prediction, a powerful and versatile probabilistic prediction framework.
    We have learned about different measures of nonconformity used in classification
    and regression, building solid foundations for applying conformal prediction to
    your industry applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll cover the concepts of validity and efficiency in
    the context of probabilistic prediction models, building upon the foundations
    laid in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
