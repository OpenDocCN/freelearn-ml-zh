- en: Chapter 7. Working with Graph Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I'll delve into graph libraries and algorithm implementations
    in Scala. In particular, I will introduce Graph for Scala ([http://www.scala-graph.org](http://www.scala-graph.org)),
    an open source project that was started in 2011 in the EPFL Scala incubator. Graph
    for Scala does not support distributed computing yet—the distributed computing
    aspects of popular graph algorithms is available in GraphX, which is a part of
    MLlib library that is part of Spark project ([http://spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html)).
    Both, Spark and MLlib were started as class projects at UC Berkeley around or
    after 2009\. I considered Spark in [Chapter 3](ch03.xhtml "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib* and introduced an RDD.
    In GraphX, a graph is a pair of RDDs, each of which is partitioned among executors
    and tasks, represents vertices and edges in a graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring **Simple Build Tool** (**SBT**) to use the material in this chapter
    interactively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning basic operations on graphs supported by Graph for Scala
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to enforce graph constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to import/export graphs in JSON
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing connected components, triangle count, and strongly connected components
    running on Enron e-mail data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing PageRank computations on Enron e-mail data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to use SVD++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A quick introduction to graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a graph? A graph is a set of **vertices** where some pairs of these
    vertices are linked with **edges**. If every vertex is linked with every other
    vertex, we say the graph is a complete graph. On the contrary, if it has no edges,
    the graph is said to be empty. These are, of course, extremes that are rarely
    encountered in practice, as graphs have varying degrees of density; the more edges
    it has proportional to the number of vertices, the more dense we say it is.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on what algorithms we intend to run on a graph and how dense is it
    expected to be, we can choose how to appropriately represent the graph in memory.
    If the graph is really dense, it pays off to store it as a square *N x N* matrix,
    where *0* in the *n*th row and *m*th column means that the *n* vertex is not connected
    to the *m* vertex. A diagonal entry expresses a node connection to itself. This
    representation is called the adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: If there are not many edges and we need to traverse the whole edge set without
    distinction, often it pays off to store it as a simple container of pairs. This
    structure is called an **edge list**.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we can model many real-life situations and events as graphs. We
    could imagine cities as vertices and plane routes as edges. If there is no flight
    between two cities, there is no edge between them. Moreover, if we add the numerical
    costs of plane tickets to the edges, we say that the graph is **weighted**. If
    there are some edges where only travels in one direction exist, we can represent
    that by making a graph directed as opposed to an undirected graph. So, for an
    undirected graph, it is true that the graph is symmetric, that is, if *A* is connected
    to *B*, then *B* is also connected to *A*—that is not necessarily true for a directed
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs without cycles are called acyclic. Multigraph can contain multiple edges,
    potentially of different type, between the nodes. Hyperedges can connect arbitrary
    number of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular algorithm on the undirected graphs is probably **connected
    components**, or partitioning of a graph into subgraph, in which any two vertices
    are connected to each other by paths. Partitioning is important to parallelize
    the operations on the graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Google and other search engines made PageRank popular. According to Google,
    PageRank estimates of how important the website is by counting the number and
    quality of links to a page. The underlying assumption is that more important websites
    are likely to receive more links from other websites, especially more highly ranked
    ones. PageRank can be applied to many problems outside of websites ranking and
    is equivalent to finding eigenvectors and the most significant eigenvalue of the
    connectivity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The most basic, nontrivial subgraph, consists of three nodes. Triangle counting
    finds all the possible fully connected (or complete) triples of nodes and is another
    well-known algorithm used in community detection and CAD.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **clique** is a fully connected subgraph. A strongly connected component
    is an analogous notion for a directed graph: every vertex in a subgraph is reachable
    from every other vertex. GraphX provides an implementation for both.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a recommender graph is a graph connecting two types of nodes: users
    and items. The edges can additionally contain the strength of a recommendation
    or a measure of satisfaction. The goal of a recommender is to predict the satisfaction
    for potentially missing edges. Multiple algorithms have been developed for a recommendation
    engine, such as SVD and SVD++, which are considered at the end of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: SBT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everyone likes Scala REPL. REPL is the command line for Scala. It allows you
    to type Scala expressions that are evaluated immediately and try and explore things.
    As you saw in the previous chapters, one can simply type `scala` at the command
    prompt and start developing complex data pipelines. What is even more convenient
    is that one can press *tab* to have auto-completion, a required feature of any
    fully developed modern IDE (such as Eclipse or IntelliJ, *Ctrl +*. or *Ctrl +
    Space*) by keeping track of the namespace and using reflection mechanisms. Why
    would we need one extra tool or framework for builds, particularly that other
    builds management frameworks such as Ant, Maven, and Gradle exist in addition
    to IDEs? As the SBT authors argue, even though one might compile Scala using the
    preceding tools, all of them have inefficiencies, as it comes to interactivity
    and reproducibility of Scala builds (*SBT in Action* by *Joshua Suereth* and *Matthew
    Farwell*, Nov 2015).
  prefs: []
  type: TYPE_NORMAL
- en: One of the main SBT features for me is interactivity and the ability to seamlessly
    work with multiple versions of Scala and dependent libraries. In the end, what
    is critical for software development is the speed with which one can prototype
    and test new ideas. I used to work on mainframes using punch cards, where the
    programmers were waiting to execute their programs and ideas, sometimes for hours
    and days. The efficiency of the computers mattered more, as this was the bottleneck.
    These days are gone, and a personal laptop is probably having more computing power
    than rooms full of servers a few decades back. To take advantage of this efficiency,
    we need to utilize human time more efficiently by speeding up the program development
    cycle, which also means interactivity and more versions in the repositories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the ability to handle multiple versions and REPL, SBT''s main features
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Native support for compiling Scala code and integrating with many test frameworks,
    including JUnit, ScalaTest, and Selenium
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build descriptions written in Scala using a DSL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependency management using Ivy (which also supports Maven-format repositories)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous execution, compilation, testing, and deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with the Scala interpreter for rapid iteration and debugging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for mixed Java/Scala projects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for testing and deployment frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to complement the tool with custom plugins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel execution of tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SBT is written in Scala and uses SBT to build itself (bootstrapping or dogfooding).
    SBT became the de facto build tool for the Scala community, and is used by the
    **Lift** and **Play** frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'While you can download SBT directly from [http://www.scala-sbt.org/download](http://www.scala-sbt.org/download),
    the easiest way to install SBT on Mac is to run MacPorts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also run Homebrew:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'While other tools exist to create SBT projects, the most straightforward way
    is to run the `bin/create_project.sh` script in the GitHub book project repository
    provided for each chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create main and test source subdirectories (but not the code). The
    project directory contains project-wide settings (refer to `project/build.properties`).
    The target will contain compiled classes and build packages (the directory will
    contain different subdirectories for different versions of Scala, for example,
    2.10 and 2.11). Finally, any jars or libraries put into the `lib` directory will
    be available across the project (I personally recommend using the `libraryDependencies`
    mechanism in the `build.sbt` file, but not all libraries are available via centralized
    repositories). This is the minimal setup, and the directory structure may potentially
    contain multiple subprojects. The Scalastyle plugin will even check the syntax
    for you ([http://www.scalastyle.org/sbt.html](http://www.scalastyle.org/sbt.html)).
    Just add `project/plugin.sbt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the SBT creates Scaladoc documentation with the `sdbt doc` command.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Blank lines and other settings in build.sbt**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Probably most of the `build.sbt` files out there are double spaced: this is
    a remnant of old versions. You no longer need them. As of version 0.13.7, the
    definitions do not require extra lines.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many other settings that you can use on `build.sbt` or `build.properties`,
    the up-to-date documentation is available at [http://www.scala-sbt.org/documentation.html](http://www.scala-sbt.org/documentation.html).
  prefs: []
  type: TYPE_NORMAL
- en: When run from the command line, the tool will automatically download and use
    the dependencies, in this case, `graph-{core,constrained,json}` and `lift-json`.
    In order to run the project, simply type `sbt run`.
  prefs: []
  type: TYPE_NORMAL
- en: In continuous mode, SBT will automatically detect changes to the source file
    and rerun the command(s). In order to continuously compile and run the code, type
    `~~ run` after starting REPL with `sbt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get help on the commands, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: While SBT will be sufficient for our use even with a simple editor such as **vi**
    or **Emacs**, the `sbteclipse` project at [https://github.com/typesafehub/sbteclipse](https://github.com/typesafehub/sbteclipse)
    will create the necessary project files to work with your Eclipse IDE.
  prefs: []
  type: TYPE_NORMAL
- en: Graph for Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this project, I will create a `src/main/scala/InfluenceDiagram.scala` file.
    For demo purpose, I will just recreate the graph from [Chapter 2](ch02.xhtml "Chapter 2. Data
    Pipelines and Modeling"), *Data Pipelines and Modeling*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `~+>` operator is used to create a directed labeled edge between two nodes
    defined in `scalax/collection/edge/Implicits.scala`, which, in our case, are of
    the `String` type. The list of other edge types and operators is provided in the
    following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The following table shows* graph edges from `scalax.collection.edge.Implicits`
    (from [http://www.scala-graph.org/guides/core-initializing.html](http://www.scala-graph.org/guides/core-initializing.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '| Edge Class | Shortcut/Operator | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Hyperedges** |'
  prefs: []
  type: TYPE_TB
- en: '| `HyperEdge` | `~` | hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `WHyperEdge` | `~%` | weighted hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `WkHyperEdge` | `~%#` | key-weighted hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `LHyperEdge` | `~+` | labeled hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `LkHyperEdge` | `~+#` | key-labeled hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `WLHyperEdge` | `~%+` | weighted labeled hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `WkLHyperEdge` | `~%#+` | key-weighted labeled hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `WLkHyperEdge` | `~%+#` | weighted key-labeled hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `WkLkHyperEdge` | `~%#+#` | key-weighted key-labeled hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| **Directed hyperedges** |'
  prefs: []
  type: TYPE_TB
- en: '| `DiHyperEdge` | `~>` | directed hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `WDiHyperEdge` | `~%>` | weighted directed hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `WkDiHyperEdge` | `~%#>` | key-weighted directed hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `LDiHyperEdge` | `~+>` | labeled directed hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `LkDiHyperEdge` | `~+#>` | key-labeled directed hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `WLDiHyperEdge` | `~%+>` | weighted labeled directed hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `WkLDiHyperEdge` | `~%#+>` | key-weighted labeled directed hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `WLkDiHyperEdge` | `~%+#>` | weighted key-labeled directed hyperedge |'
  prefs: []
  type: TYPE_TB
- en: '| `WkLkDiHyperEdge` | `~%#+#>` | key-weighted key-labeled directed hyperedge
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Undirected edges** |'
  prefs: []
  type: TYPE_TB
- en: '| `UnDiEdge` | `~` | undirected edge |'
  prefs: []
  type: TYPE_TB
- en: '| `WUnDiEdge` | `~%` | weighted undirected edge |'
  prefs: []
  type: TYPE_TB
- en: '| `WkUnDiEdge` | `~%#` | key-weighted undirected edge |'
  prefs: []
  type: TYPE_TB
- en: '| `LUnDiEdge` | `~+` | labeled undirected edge |'
  prefs: []
  type: TYPE_TB
- en: '| `LkUnDiEdge` | `~+#` | key-labeled undirected edge |'
  prefs: []
  type: TYPE_TB
- en: '| `WLUnDiEdge` | `~%+` | weighted labeled undirected edge |'
  prefs: []
  type: TYPE_TB
- en: '| `WkLUnDiEdge` | `~%#+` | key-weighted labeled undirected edge |'
  prefs: []
  type: TYPE_TB
- en: '| `WLkUnDiEdge` | `~%+#` | weighted key-labeled undirected edge |'
  prefs: []
  type: TYPE_TB
- en: '| `WkLkUnDiEdge` | `~%#+#` | key-weighted key-labeled undirected edge |'
  prefs: []
  type: TYPE_TB
- en: '| **Directed edges** |'
  prefs: []
  type: TYPE_TB
- en: '| `DiEdge` | `~>` | directed edge |'
  prefs: []
  type: TYPE_TB
- en: '| `WDiEdge` | `~%>` | weighted directed edge |'
  prefs: []
  type: TYPE_TB
- en: '| `WkDiEdge` | `~%#>` | key-weighted directed edge |'
  prefs: []
  type: TYPE_TB
- en: '| `LDiEdge` | `~+>` | labeled directed edge |'
  prefs: []
  type: TYPE_TB
- en: '| `LkDiEdge` | `~+#>` | key-labeled directed edge |'
  prefs: []
  type: TYPE_TB
- en: '| `WLDiEdge` | `~%+>` | weighted labeled directed edge |'
  prefs: []
  type: TYPE_TB
- en: '| `WkLDiEdge` | `~%#+>` | key-weighted labeled directed edge |'
  prefs: []
  type: TYPE_TB
- en: '| `WLkDiEdge` | `~%+#>` | weighted key-labeled directed edge |'
  prefs: []
  type: TYPE_TB
- en: '| `WkLkDiEdge` | `~%#+#>` | key-weighted key-labeled directed edge |'
  prefs: []
  type: TYPE_TB
- en: 'You saw the power of graph for Scala: the edges can be weighted and we may
    potentially construct a multigraph (key-labeled edges allow multiple edges for
    a pair of source and destination nodes).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run SBT on the preceding project with the Scala file in the `src/main/scala`
    directory, the output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If continuous compilation is enabled, the main method will be run as soon as
    SBT detects that the file has changed (in the case of multiple classes having
    the main method, SBT will ask you which one to run, which is not great for interactivity;
    so you might want to limit the number of executable classes).
  prefs: []
  type: TYPE_NORMAL
- en: I will cover different output formats in a short while, but let's first see
    how to perform simple operations on the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Adding nodes and edges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we already know that the graph is directed and acyclic, which is a required
    property for all decision diagrams so that we know we did not make a mistake.
    Let''s say that I want to make the graph more complex and add a node that will
    indicate the likelihood of me recommending a vacation in Portland, Oregon to another
    person. The only thing I need to add is the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have continuous compilation/run enabled, you will immediately see the
    changes after pressing the **Save File** button:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we want to know the parents of the newly introduced node, we can simply
    run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us a set of parents for a specific node—and thus drive the decision
    making process. If we add a cycle, the acyclic method will automatically detect
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that you can create the graphs completely programmatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, the element computation provided as the second parameter to the fill method
    is repeated `45` times (the first parameter). The graph connects every node to
    all of its predecessors, which is also known as a clique in the graph theory.
  prefs: []
  type: TYPE_NORMAL
- en: Graph constraints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Graph for Scala enables us to set constraints that cannot be violated by any
    future graph update. This comes in handy when we want to preserve some detail
    in the graph structure. For example, a **Directed Acyclic Graph** (**DAG**) should
    not contain cycles. Two constraints are currently implemented as a part of the
    `scalax.collection.constrained.constraints` package—connected and acyclic, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the command to run the program that tries to add or remove nodes that
    violate the constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Adding or subtracting nodes that violate one of the constraints is rejected.
    The programmer can also specify a side effect if an attempt to add or subtract
    a node that violates the condition is made.
  prefs: []
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Graph for Scala supports importing/exporting graphs to JSON, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To produce a JSON representation for a sample graph, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: For more complex structures, one might need to write custom descriptors, serializers,
    and deserializers (refer to [http://www.scala-graph.org/api/json/api/#scalax.collection.io.json.package](http://www.scala-graph.org/api/json/api/#scalax.collection.io.json.package)).
  prefs: []
  type: TYPE_NORMAL
- en: GraphX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While graph for Scala may be considered a DSL for graph operations and querying,
    one should go to GraphX for scalability. GraphX is build on top of a powerful
    Spark framework. As an example of Spark/GraphX operations, I''ll use the CMU Enron
    e-mail dataset (about 2 GB). The actual semantic analysis of the e-mail content
    is not going to be important to us until the next chapters. The dataset can be
    downloaded from the CMU site. It has e-mail from mailboxes of 150 users, primarily
    Enron managers, and about 517,401 e-mails between them. The e-mails may be considered
    as an indication of a relation (edge) between two people: Each email is an edge
    between a source (`From:`) and a destination (`To:`) vertices.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since GraphX requires the data in RDD format, I''ll have to do some preprocessing.
    Luckily, it is extremely easy with Scala—this is why Scala is the perfect language
    for semi-structured data. Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: First, we use the `MurmurHash3` class to generate node IDs, which are of type
    `Long`, as they are required for each node in GraphX. The `emailRe` and `messageRe`
    are used to match the file content to find the required content. Scala allows
    you to parallelize the programs without much work.
  prefs: []
  type: TYPE_NORMAL
- en: Note the `par` call on line 50, `getFileTree(new File(args(0))).par.map`. This
    will make the loop parallel. If processing the whole Enron dataset can take up
    to an hour even on 3 GHz processor, adding parallelization reduces it by about
    8 minutes on a 32-core Intel Xeon E5-2630 2.4 GHz CPU Linux machine (it took 15
    minutes on an Apple MacBook Pro with 2.3 GHz Intel Core i7).
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the code will produce a set of JSON records that can be loaded into
    Spark (to run it, you''ll need to put **joda-time** and **lift-json** library
    jars on the classpath), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Nice! Spark was able to figure out the fields and types on it''s own. If Spark
    was not able to parse all the records, one would have a `_corrupt_record` field
    containing the unparsed records (one of them is the `[success]` line at the end
    of the dataset, which can be filtered out with a `grep -Fv [success]`). You can
    see them with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The nodes (people) and edges (relations) datasets can be extracted with the
    following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Node IDs in GraphX**'
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in Graph for Scala, specifying the edges is sufficient for defining
    the nodes and the graph. In Spark/GraphX, nodes need to be extracted explicitly,
    and each node needs to be associated with *n* id of the `Long` type. While this
    potentially limits the flexibility and the number of unique nodes, it enhances
    the efficiency. In this particular example, generating node ID as a hash of the
    e-mail string was sufficient as no collisions were detected, but the generation
    of unique IDs is usually a hard problem to parallelize.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first GraphX graph is ready!! It took a bit more work than Scala for Graph,
    but now it''s totally ready for distributed processing. A few things to note:
    first, we needed to explicitly convert the fields to `Long` and `String` as the
    `Edge` constructor needed help in figuring out the types. Second, Spark might
    need to optimize the number of partitions (likely, it created too many):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To repartition, there are two calls: repartition and coalesce. The latter tries
    to avoid shuffle, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this might limit parallelism if one performs computations over a large
    cluster. Finally, it''s a good idea to use `cache` method that pins the data structure
    in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It took a few more commands to construct a graph in Spark, but four is not
    too bad. Let''s compute some statistics (and show the power of Spark/GraphX, in
    the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: Computing basic statistics on Enron e-mail graph.
  prefs: []
  type: TYPE_NORMAL
- en: '| Statistics | Spark command | Value for Enron |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Total # of relations (pairwise communications) | `graph.numEdges` | 3,035,021
    |'
  prefs: []
  type: TYPE_TB
- en: '| Number of e-mails (message IDs) | `graph.edges.map( e => e.attr._1 ).distinct.count`
    | 371,135 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of connected pairs | `graph.edges.flatMap( e => List((e.srcId, e.dstId),
    (e.dstId, e.srcId))).distinct.count / 2` | 217,867 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of one-way communications | `graph.edges.flatMap( e => List((e.srcId,
    e.dstId), (e.dstId, e.srcId))).distinct.count - graph.edges.map( e => (e.srcId,
    e.dstId)).distinct.count` | 193,183 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of distinct subject lines | `graph.edges.map( e => e.attr._2 ).distinct.count`
    | 110,273 |'
  prefs: []
  type: TYPE_TB
- en: '| Total # of nodes | `graph.numVertices` | 23,607 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of destination-only nodes | `graph. numVertices - graph.edges.map(
    e => e.srcId).distinct.count` | 17,264 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of source-only nodes | `graph. numVertices - graph.edges.map( e =>
    e.dstId).distinct.count` | 611 |'
  prefs: []
  type: TYPE_TB
- en: Who is getting e-mails?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most straightforward ways to estimate people''s importance in an
    organization is to look at the number of connections or the number of incoming
    and outgoing communicates. The GraphX graph has built-in `inDegrees` and `outDegrees`
    methods. To rank the emails with respect to the number of incoming emails, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To rank the emails with respect to the number of egressing emails, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Let's apply some more complex algorithms to the Enron dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Connected components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Connected components determine whether the graph is naturally partitioned into
    several parts. In the Enron relationship graph, this would mean that two or several
    groups communicate mostly between each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We see 18 groups. Each one of the groups can be counted and extracted by filtering
    the ID. For instance, the group associated with `<[etc.survey@enron.com](mailto:etc.survey@enron.com)>`
    can be found by running a SQL query on DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This group is based on a single e-mail sent on September 19, 2000, from `<[survey.test@enron.com](mailto:survey.test@enron.com)>`
    to `<[etc.survey@enron](mailto:etc.survey@enron)>`. The e-mail is listed twice,
    only because it ended up in two different folders (and has two distinct message
    IDs). Only the first group, the largest subgraph, contains more than two e-mail
    addresses in the organization.
  prefs: []
  type: TYPE_NORMAL
- en: Triangle counting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The triangle counting algorithm is relatively straightforward and can be computed
    in the following three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the set of neighbors for each vertex.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each edge, compute the intersection of the sets and send the count to both
    vertices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the sum at each vertex and divide by two, as each triangle is counted
    twice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to convert the multigraph to an undirected graph with `srcId < dstId`,
    which is a precondition for the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: While there is no direct relationship between the triangle count and the importance
    of people in the organization, the people with higher triangle count arguably
    are more social—even though a clique or a strongly connected component count might
    be a better measure.
  prefs: []
  type: TYPE_NORMAL
- en: Strongly connected components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the mathematical theory of directed graphs, a subgraph is said to be strongly
    connected if every vertex is reachable from every other vertex. It could happen
    that the whole graph is just one strongly connected component, but on the other
    end of the spectrum, each vertex could be its own connected component.
  prefs: []
  type: TYPE_NORMAL
- en: If you contract each connected component to a single vertex, you get a new directed
    graph that has a property to be without cycles—acyclic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm for SCC detection is already built into GraphX:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: There are 18,200 strongly connected components with only an average 23,787/18,200
    = 1.3 users per group.
  prefs: []
  type: TYPE_NORMAL
- en: PageRank
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The PageRank algorithm gives us an estimate of how important a person by analysing
    the links, which are the emails in this case. For example, let''s run PageRank
    on Enron email graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Ostensibly, these are the go-to people. PageRank tends to emphasize the incoming
    edges, and Tana Jones returns to the top of the list compared to the 9th place
    in the triangle counting.
  prefs: []
  type: TYPE_NORMAL
- en: SVD++
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SVD++ is a recommendation engine algorithm, developed specifically for Netflix
    competition by Yahuda Koren and team in 2008—the original paper is still out there
    in the public domain and can be Googled as `kdd08koren.pdf`. The specific implementation
    comes from the .NET *MyMediaLite* library by ZenoGarther ([https://github.com/zenogantner/MyMediaLite](https://github.com/zenogantner/MyMediaLite)),
    who granted Apache 2 license to the Apache Foundation. Let''s assume I have a
    set of users (on the left) and items (on the right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![SVD++](img/B04935_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 07-1\. A graphical representation of a recommendation problem as a bipartite
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram is a graphical representation of the recommendation problem.
    The nodes on the left represent users. The nodes on the right represent items.
    User **1** recommends items **A** and **C**, while users **2** and **3** recommend
    only a single item **A**. The rest of the edges are missing. The common question
    is to find recommendation ranking of the rest of the items, the edges may also
    have a weight or recommendation strength attached to them. The graph is usually
    sparse. Such graph is also often called bipartite, as the edges only go from one
    set of nodes to another set of nodes (the user does not recommend another user).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the recommendation engine, we typically need two types of nodes—users and
    items. The recommendations are based on the rating matrix of (user, item, and
    rating) tuples. One of the implementation of the recommendation algorithm is based
    on **Singular Value Decomposition** (**SVD**) of the preceding matrix. The final
    scoring has four components: the baseline, which is the sum of average for the
    whole matrix, average for the users, and average for the items, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SVD++](img/B04935_07_01F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the ![SVD++](img/B04935_07_02F.jpg), ![SVD++](img/B04935_07_03F.jpg),
    and ![SVD++](img/B04935_07_04F.jpg) can be understood as the averages for the
    whole population, user (among all user recommendations), and item (among all the
    users). The final part is the Cartesian product of two rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SVD++](img/B04935_07_05F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The problem is posed as a minimization problem (refer to [Chapter 4](ch04.xhtml
    "Chapter 4. Supervised and Unsupervised Learning"), *Supervised and Unsupervised
    Learning*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![SVD++](img/B04935_07_06F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![SVD++](img/B04935_07_07F.jpg) is a regularization coefficient also
    discussed in [Chapter 4](ch04.xhtml "Chapter 4. Supervised and Unsupervised Learning"),
    *Supervised and Unsupervised Learning*. So, each user is associated with a set
    of numbers (![SVD++](img/B04935_07_08F.jpg), and each item with ![SVD++](img/B04935_07_09F.jpg),
    ![SVD++](img/B04935_07_10F.jpg). In this particlar implementation, the optimal
    coefficients are found by gradient descent. This is the basic of SVD optimization.
    In linear algebra, SVD takes an arbitrary ![SVD++](img/B04935_07_11F.jpg) matrix
    *A* and represents it as a product of an orthogonal ![SVD++](img/B04935_07_11F.jpg)
    matrix *U*, a diagonal ![SVD++](img/B04935_07_11F.jpg) matrix ![SVD++](img/B04935_07_12F.jpg),
    and a ![SVD++](img/B04935_07_11F.jpg) unitary matrix *V*, for example, the columns
    are mutually orthogonal. Arguably, if one takes the largest ![SVD++](img/B04935_07_13F.jpg)
    entries of the ![SVD++](img/B04935_07_12F.jpg) matrix, the product is reduced
    to the product of a very tall ![SVD++](img/B04935_07_14F.jpg) matrix and a wide
    ![SVD++](img/B04935_07_15F.jpg) matric, where ![SVD++](img/B04935_07_13F.jpg)
    is called the rank of decomposition. If the remaining values are small, the new
    ![SVD++](img/B04935_07_17F.jpg) numbers approximate the original ![SVD++](img/B04935_07_11F.jpg)
    numbers for the relation, *A*. If *m* and *n* are large to start with, and in
    practical online shopping situations, *m* is the items and can be in hundreds
    of thousands, and *n* is the users and can be hundreds of millions, the saving
    can be substantial. For example, for *r=10*, *m=100,000*, and *n=100,000,000*,
    the savings are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SVD++](img/B04935_07_18F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'SVD can also be viewed as PCA for matrices with ![SVD++](img/B04935_07_19F.jpg).
    In the Enron case, we can treat senders as users and recipients as items (we''ll
    need to reassign the node IDs), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `svdRanks` is the user-part of the ![SVD++](img/B04935_07_20F.jpg) prediction.
    The distribution lists take a priority as this is usually used for mass e-mailing.
    To get the user-specific part, we need to provide the user ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Here, we computed the top five recommended e-mail-to list for top in-degree
    and out-degree users.
  prefs: []
  type: TYPE_NORMAL
- en: SVD has only 159 lines of code in Scala and can be the basis for some further
    improvements. SVD++ includes a part based on implicit user feedback and item similarity
    information. Finally, the Netflix winning solution had also taken into consideration
    the fact that user preferences are time-dependent, but this part has not been
    implemented in GraphX yet.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While one can easily create their own data structures for graph problems, Scala's
    support for graphs comes from both semantic layer—Graph for Scala is effectively
    a convenient, interactive, and expressive language for working with graphs—and
    scalability via Spark and distributed computing. I hope that some of the material
    exposed in this chapter will be useful for implementing algorithms on top of Scala,
    Spark, and GraphX. It is worth mentioning that bot libraries are still under active
    development.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll step down from from our flight in the the skies and
    look at Scala integration with traditional data analysis frameworks such as statistical
    language R and Python, which are often used for data munching. Later, in [Chapter
    9](ch09.xhtml "Chapter 9. NLP in Scala"), *NLP in Scala*. I'll look at NLP Scala
    tools, which leverage complex data structures extensively.
  prefs: []
  type: TYPE_NORMAL
