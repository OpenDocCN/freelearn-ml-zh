<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Learning to Recognize Traffic Signs</h1>
                </header>
            
            <article>
                
<p><span>We have previously studied how to describe objects by means of key points and features, and how to find the correspondence points in two</span> <span>different images of the same physical object. However, our previous approaches were rather limited when it came to recognizing objects in real-world settings and assigning them to conceptual categories. For example, in </span><a href="0bab4fd2-6330-47d2-a2b2-339e8f879ed7.xhtml"><span class="ChapterrefPACKT">Chapter 2</span></a><span>, </span><em>Hand Gesture Recognition Using a Kinect Depth Sensor</em>, <span>the required object in the image was a hand, and it had to be nicely placed in the center of the screen. Wouldn't it be nice if we could remove these restrictions?</span></p>
<p>The goal of this chapter is to train a <strong>multiclass</strong> <strong>classifier</strong> to recognize traffic signs. In this chapter, we will cover the following concepts:</p>
<ul>
<li>Planning the app</li>
<li class="mce-root">Briefing on supervised learning concepts</li>
<li class="mce-root">Understanding the <strong>German Traffic Sign Recognition</strong> <strong>Benchmark</strong> (<strong>GTSRB</strong>) dataset</li>
<li class="mce-root">Learning about dataset feature extraction</li>
<li class="mce-root">Learning about <strong>support vector machines</strong> (<strong>SVMs</strong>)</li>
<li>Putting it all together</li>
<li>Improving results with neural networks</li>
</ul>
<p>In this chapter, you will learn how to apply machine learning models to real-world problems. You will learn how to use already available datasets for training models. You will also learn h<span>ow to use SVMs for multiclass classification and </span><span>how to train, test, and improve machine learning algorithms provided with OpenCV to achieve real-world tasks.</span></p>
<p>We will train an SVM to recognize all sorts of traffic signs. Although SVMs are binary classifiers (that is, they can be used to learn, at most, two categories<span>—</span>positives and negatives, animals and non-animals, and so on), they can be extended to be used in multiclass classification. In order to achieve good classification performance, we will explore a number of color spaces, as well as the <strong>Histogram of Oriented Gradients</strong> (<strong>HOG</strong>) feature. The end result will be a classifier that can distinguish more than 40<span> </span><span>different signs from the dataset,</span> with very high accuracy.</p>
<p>Learning the basics of machine learning will be very useful for the future when you would like to make your vision-related applications even smarter. This chapter will teach you the basics of machine learning, on which the following chapters will build.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started</h1>
                </header>
            
            <article>
                
<p><span>The GTSRB dataset can be freely obtained from </span><a href="http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=dataset"><span class="URLPACKT">http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=dataset</span></a><span> (see the <em>Dataset attribution</em> section for attribution details).</span></p>
<p>You can find the code that we present in this chapter at our GitHub repository: <a href="https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter7">https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter7</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Planning the app</h1>
                </header>
            
            <article>
                
<p>To arrive at such a multiclass classifier (<span>that can differentiate between more than 40</span><span> </span><span>different signs from the dataset</span>), we need to perform the following steps:</p>
<ol>
<li><strong>Preprocess the dataset</strong>: We need a way to load our dataset, extract the regions of interest, and split the data into appropriate training and test sets.</li>
<li><strong>Extract features</strong>: Chances are that raw pixel values are not the most informative representation of the data. We need a way to extract meaningful features from the data, such as features based on different color spaces and HOG.</li>
<li><strong>Train the classifier</strong>: We will train the multiclass classifier on the training data using a<em> one-versus-all</em><span> </span>strategy.</li>
<li><strong>Score the classifier</strong>: We will evaluate the quality of the trained ensemble classifier by calculating different performance metrics, such as<span> </span><strong>accuracy</strong>,<span> </span><strong>precision</strong>, and<span> </span><strong>recall</strong>.</li>
</ol>
<p><span>We will discuss all these steps in detail in the upcoming sections.</span></p>
<p>The final app will parse a dataset, train the ensemble classifier, assess its classification performance, and visualize the result. This will require the following components:</p>
<ul>
<li><kbd>main</kbd>: The main function routine (in <kbd>chapter7.py</kbd>) is required for starting the application.</li>
<li><kbd>datasets.gtsrb</kbd>: This is a script for parsing the GTSRB dataset. This script contains the following functions:
<ul>
<li><kbd>load_data</kbd>: This function is used to load the GTSRB dataset, extract a feature of choice, and split the data into training and test sets.</li>
<li><kbd>*_featurize</kbd>, <kbd>hog_featurize</kbd>: These functions are passed to <kbd>load_data</kbd> for extracting a feature of choice from the dataset. Example functions are as follows:
<ul>
<li><kbd>gray_featurize</kbd>: This is a function that creates features based on grayscale pixel values.</li>
<li><kbd>surf_featurize</kbd>: This is a function that creates features based on <strong>Speeded-Up-Robust Features</strong> (<strong>SURF</strong>).</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><span>The classification performance will be judged based on</span><span> </span><span>accuracy,</span><span> </span><span>precision, and</span><span> </span><span>recall. The following sections will explain all of these terms in detail.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Briefing on supervised learning concepts</h1>
                </header>
            
            <article>
                
<p>An important subfield of machine learning is <strong>supervised learning</strong>. In supervised learning, we try to learn from a set of labeled data—that is, every data sample has a desired target value or true output value. These target values could correspond to the continuous output of a function (such as <kbd>y</kbd> in <kbd>y = sin(x)</kbd>), or to more abstract and discrete categories (such as <em>cat</em> or <em>dog</em>).</p>
<p>A supervised learning algorithm uses the already labeled training data, analyzes it, and produces a mapping inferred function from features to a label, which can be used for mapping new examples. Ideally, the inferred algorithm will generalize well and give correct target values for new data.</p>
<p>We divide supervised learning tasks into two categories:</p>
<ul>
<li>If we are dealing with continuous output (for example, the probability of rain), the process is called <strong>regression</strong>.</li>
<li>If we are dealing with discrete output (for example, species of an animal), the process is called <strong>classification</strong>.</li>
</ul>
<p>In this chapter, we focus on the classification problem of labeling images of the GTSRB dataset, and we will use an algorithm called SVM to infer a mapping function between images and their labels.</p>
<p>Let's first understand how machine learning<span> </span>gives <em>machines</em><span> the ability to </span><em>learn</em><span><em> like humans</em>.</span> Here is a hint<span>—</span>we train them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The training procedure</h1>
                </header>
            
            <article>
                
<p>As an example, we may want to learn what cats and dogs look like. To make this a supervised learning task, first, we have to put it as a question that has either a categorical answer or a real-valued answer.</p>
<p>Here are some example questions:</p>
<ul>
<li>Which animal is shown in the given picture?</li>
<li>Is there a cat in the picture?</li>
<li>Is there a dog in the picture?</li>
</ul>
<p>After that, we have to gather an example picture with its corresponding correct answer<span>—</span><strong>training data</strong>.</p>
<p>Then, we have to pick a learning algorithm (<strong>learner</strong>) and start tweaking its parameters in some way (<strong>learning algorithm</strong>) so that the learner can tell the correct answers when presented with a datum from training data.</p>
<p>We repeat this process until we are satisfied with the learner's performance or <strong>score</strong> (which could be <strong>accuracy</strong>, <strong>precision</strong>, or some other <strong>cost function</strong>) on the training data. If we are not satisfied, we change the parameters of the learner in order to improve the score over time.</p>
<p>This procedure is outlined in the following screenshot:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/58a3b6a3-86fc-4fb9-a486-bed5734c4562.png" style="width:34.50em;height:16.75em;"/></p>
<p class="packt_figure">From the previous screenshot, <span class="packt_screen">Training data</span> is represented by a set of <span class="packt_screen">Features</span>. For real-life classification tasks, these features are rarely the raw pixel values of an image, since these tend not to represent the data well. Often, the process of finding the features that best describe the data is an essential part of the entire learning task (also referred to as <strong>feature selection</strong> or <strong>feature engineering</strong>).</p>
<p class="packt_figure">That is why it is always a good idea to deeply study the statistics and appearances of the training set that you are working with before even thinking about setting up a classifier.</p>
<p>As you are probably aware, there is an entire zoo of learners, cost functions, and learning algorithms out there. These make up the core of the learning procedure. The <span class="packt_screen">l</span><span class="packt_screen">earner</span> (for example, a linear classifier or SVM) defines how input features are converted into a score function (for example, mean-squared error), whereas the <span class="packt_screen">Learning algorithm</span> (for example, gradient descent) defines how the parameters of the <span class="packt_screen">l</span><span class="packt_screen">earner</span> are changed over time.</p>
<p>The training procedure in a classification task can also be thought of as finding an appropriate <strong>decision boundary</strong>, which is a line that best partitions the training set into two subsets, one for each class. For example, consider training samples with only two features (<span class="packt_screen">x</span> and <span class="packt_screen">y</span> values) and a corresponding class label (positive (<strong><span class="packt_screen">+</span></strong>), or negative (<strong><span class="packt_screen">–</span></strong>)).</p>
<p>At the beginning of the training procedure, the classifier tries to draw a line to separate all positives from all negatives. As the training progresses, the classifier sees more and more data samples. These are used to update the decision boundary, as illustrated in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/88288338-4a20-42f3-a19b-8dfd07d3edfb.png" style="width:38.00em;height:11.33em;"/></p>
<p>Compared to this simple illustration, an SVM tries to find the optimal decision boundary in a high-dimensional space, so the decision boundary can be more complex than a straight line.</p>
<p>We now move on to understand the testing procedure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The testing procedure</h1>
                </header>
            
            <article>
                
<p>In order for a trained classifier to be of any practical value, we need to know how it performs when applied to a data sample (also called <strong>generalization</strong>) that has never been seen before. To stick to our example shown earlier, we want to know which class the classifier predicts when we present it with a previously unseen picture of a cat or a dog.</p>
<p>More generally speaking, we want to know which class the <span class="packt_screen">?</span> sign, in the following screenshot, corresponds to, based on the decision boundary we learned during the training phase:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7d225b9d-2003-4164-9df6-cc599c2d1bc6.png" style="width:18.83em;height:11.17em;"/></p>
<p>From the preceding screenshot, you can see why this is a tricky problem. If the location of the question mark (<span class="packt_screen">?</span>) were more to the left, we would be certain that the corresponding class label is <span class="packt_screen">+</span>.</p>
<p>However, in this case, there are several ways to draw the decision boundary such that all the <span class="packt_screen"><span class="packt_screen">+</span></span> signs are to the left of it and all the <span class="packt_screen"><span>–</span> </span>signs are to the right of it, as illustrated in the following <span><span>screenshot</span></span>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6274c099-983b-4f31-b9b0-bbdbb196f24d.png" style="width:23.92em;height:14.17em;"/></p>
<p>The label of <span class="packt_screen"><strong>?</strong></span> thus depends on the exact decision boundary that was derived during training. If the <span class="packt_screen"><strong>?</strong></span> sign in the preceding screenshot is actually a <strong><span class="packt_screen">–</span></strong> sign, then only one decision boundary (the leftmost) would get the correct answer. A common problem is that <span>training can result in a decision boundary</span><span> </span><span>that works <em>too well</em> on the training set (also known as</span> <strong>overfitting</strong><span>) but also makes a lot of mistakes when applied to unseen data.</span></p>
<p>In that case, it is likely that the learner imprinted details that are specific to the training set on the decision boundary, instead of revealing general properties about the data that might also be true for unseen data.</p>
<div class="packt_infobox">A common technique for reducing the effect of overfitting is called <strong>regularization</strong>.</div>
<p>Long story short: the problem always comes back to finding the boundary that best splits not only the training set but also the test set. That is why the most important metric for a classifier is its generalization performance (that is, how well it classifies data not seen in the training phase).</p>
<p><span>In order to apply our classifier to traffic-sign recognition, we need a suitable dataset. A good choice might be the GTSRB dataset. Let us learn about it next.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the GTSRB dataset</h1>
                </header>
            
            <article>
                
<p>The GTSRB dataset contains more than 50,000 images of traffic signs belonging to 43 classes.</p>
<p class="mce-root">This dataset was used by professionals in a classification challenge during the <strong>International Joint Conference on Neural Networks</strong> (<strong>IJCNN</strong>) in 2011. The GTSRB dataset is perfect for our purposes because it is large, organized, open source, and annotated.</p>
<p>Although the actual traffic sign is not necessarily a square or is in the center of each image, the dataset comes with an annotation file that specifies the bounding boxes for each sign.</p>
<p>A good idea before doing any sort of machine learning is usually to get a feel of the dataset, its qualities, and its challenges. Some good ideas include manually going through the data and understanding what are some characteristics of it, reading a data description—if it's available on the page—to understand which models might work best, and so on.</p>
<p class="mce-root">Here, we present a snippet from <kbd>data/gtsrb.py</kbd> that loads and then plots a random-15 sample of the training dataset, and does that <kbd>100</kbd> times, so you can paginate through the data:</p>
<pre>if __name__ == '__main__':<br/>    train_data, train_labels = load_training_data(labels=None)<br/>    np.random.seed(75)<br/>    for _ in range(100):<br/>        indices = np.arange(len(train_data))<br/>        np.random.shuffle(indices)<br/>        for r in range(3):<br/>            for c in range(5):<br/>                i = 5 * r + c<br/>                ax = plt.subplot(3, 5, 1 + i)<br/>                sample = train_data[indices[i]]<br/>                ax.imshow(cv2.resize(sample, (32, 32)), cmap=cm.Greys_r)<br/>                ax.axis('off')<br/>        plt.tight_layout()<br/>        plt.show()<br/>        np.random.seed(np.random.randint(len(indices)))</pre>
<p><span>Another good strategy would be to plot 15 samples from each of the 43 classes and see how images change for the given class. </span>The following screenshot shows some examples of this dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f166f8b5-f216-4a84-ac1d-dcff00181016.png" style="width:33.58em;height:16.92em;"/></p>
<p>Even from this small data sample, it is immediately clear that this is a challenging dataset for any sort of classifier. The appearance of the signs changes drastically based on viewing angle (orientation), viewing distance (blurriness), and lighting conditions (shadows and bright spots).</p>
<p>For some of these signs—such as the second sign of the third row—it is difficult, even for humans (at least for me), to tell the correct class label right away. It's a good thing we are aspiring experts in machine learning!</p>
<p>Let's now learn to parse the dataset in order to convert to a format suitable for the SVM to use for training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parsing the dataset</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">The GTSRB dataset has 21 files that we can download. We choose to work with the raw data to make it more educational and download the official training data<span>—</span><strong>Images and annotations</strong> (<kbd>GTSRB_Final_Training_Images.zip</kbd>) for training, and the official training dataset that was used at the <strong>IJCNN</strong> <strong>2011 competition</strong><span>—</span><strong>Images and annotations</strong> (<kbd>GTSRB-Training_fixed.zip</kbd>) for scoring.</p>
<p class="CDPAlignLeft CDPAlign">The following screenshot shows the files from the dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4fa8121b-2ea6-41e7-8b5a-14ffa222433b.png" style="width:52.42em;height:44.17em;"/></p>
<p class="mce-root">We chose to download the train and test data separately instead of constructing our own train/test data from one of the datasets because, after exploring the data, there are usually 30 images of the same sign from different distances that look very much alike. Putting these 30 images in different datasets will skew the problem and lead to great results, even though our model might not generalize well.</p>
<p>The following code is a function that downloads the data from the <strong>University of Copenhagen Data Archive</strong>:</p>
<pre>ARCHIVE_PATH = 'https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/'<br/><br/>def _download(filename, *, md5sum=None):<br/>    write_path = Path(__file__).parent / filename<br/>    if write_path.exists() and _md5sum_matches(write_path, md5sum):<br/>        return write_path<br/>    response = requests.get(f'{ARCHIVE_PATH}/{filename}')<br/>    response.raise_for_status()<br/>    with open(write_path, 'wb') as outfile:<br/>        outfile.write(response.content)<br/>    return write_path</pre>
<p>The previous code takes a filename (you can see the files and their names from the previous screenshot) and checks if the file already exists or not (and checks whether the <kbd>md5sum</kbd> matches or not, if provided). This saves a lot of bandwidth and time by not having to download the files again and again. Then, it downloads the file and stores it in the same directory as the file that contains the code.</p>
<div class="packt_infobox">The annotation format can be viewed at <a href="http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=dataset#Annotationformat">http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=dataset#Annotationformat</a>.</div>
<p><span>After we have downloaded the file, we write a function that unzips and extracts the data using the annotation format provided with the data, as follows:</span></p>
<ol>
<li><span>First, we open the downloaded</span> <kbd>.zip</kbd> <span>file (this could be either the training or test data), and we iterate over all the files and only open</span> <kbd>.csv</kbd> <span>files, which contain the target information of each image in the corresponding class. This is shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px;">def _load_data(filepath, labels):<br/>    data, targets = [], []<br/><br/>    with ZipFile(filepath) as data_zip:<br/>        for path in data_zip.namelist():<br/>            if not path.endswith('.csv'):<br/>                continue<br/>            # Only iterate over annotations files<br/>            ...</pre>
<ol start="2">
<li>Then, we check if the label of the image is in the <kbd>labels</kbd> array that we are interested in. Then, we create a <kbd>csv.reader</kbd> that we will use to iterate over the <kbd>.csv</kbd> file contents, as follows:</li>
</ol>
<pre style="padding-left: 60px;">            ....<br/>            # Only iterate over annotations files<br/>            *dir_path, csv_filename = path.split('/')<br/>            label_str = dir_path[-1]<br/>            if labels is not None and int(label_str) not in labels:<br/>                continue<br/>            with data_zip.open(path, 'r') as csvfile:<br/>                reader = csv.DictReader(TextIOWrapper(csvfile), delimiter=';')<br/>                for img_info in reader:<br/>                    ... </pre>
<ol start="3">
<li>Every line of the file contains the annotation for one data sample. So, we extract the image path, read the data, and convert it to a NumPy array. Usually, the object in these samples is not perfectly cut out but is embedded in its surroundings. We cut the image using the boundary-box information provided in the archive, using a <kbd>.csv</kbd> file for each of the labels. I<span>n the following code,</span> we add the sign to <kbd>data</kbd> and add the label to <kbd>targets</kbd>:</li>
</ol>
<pre style="padding-left: 60px;">                    img_path = '/'.join([*dir_path, img_info['Filename']])<br/>                    raw_data = data_zip.read(img_path)<br/>                    img = cv2.imdecode(np.frombuffer(raw_data, np.uint8), 1)<br/><br/>                    x1, y1 = np.int(img_info['Roi.X1']), <br/>                    np.int(img_info['Roi.Y1'])<br/>                    x2, y2 = np.int(img_info['Roi.X2']), <br/>                    np.int(img_info['Roi.Y2'])<br/><br/>                    data.append(img[y1: y2, x1: x2])<br/>                    targets.append(np.int(img_info['ClassId']))</pre>
<p>Often, it is desirable to perform some form of feature extraction, because raw image data is rarely the best description of the data. We will defer this job to another function, which we will discuss in detail later.</p>
<p>As pointed out in the previous subsection, it is imperative to separate the samples that we use to train our classifier from the samples that we use to test it. For this, <span>the following code snippet shows us that</span> we have two different functions that download training and testing data and load them into memory:</p>
<pre>def load_training_data(labels):<br/>    filepath = _download('GTSRB-Training_fixed.zip',<br/>                         md5sum='513f3c79a4c5141765e10e952eaa2478')<br/>    return _load_data(filepath, labels)<br/><br/>def load_test_data(labels):<br/>    filepath = _download('GTSRB_Online-Test-Images-Sorted.zip',<br/>                         md5sum='b7bba7dad2a4dc4bc54d6ba2716d163b')<br/>    return _load_data(filepath, labels)</pre>
<p>Now that we know how to convert images into NumPy matrices, we can go on to more interesting parts, namely, we can feed the data to the SVM and train it to make predictions. So, let's move on to the next section, which covers feature extraction.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning about dataset feature extraction</h1>
                </header>
            
            <article>
                
<p>Chances are that raw pixel values are not the most informative way to represent the data, as we have already realized in <a href="905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Finding Objects via Feature Matching and Perspective Transforms</em>. Instead, we need to derive a measurable property of the data that is more informative for classification.</p>
<p>However, often, it is not clear which features would perform best. Instead, it is often necessary to experiment with different features that the practitioner finds appropriate. After all, the choice of features might strongly depend on the specific dataset to be analyzed or the specific classification task to be performed.</p>
<p>For example, if you have to distinguish between a stop sign and a warning sign, then the most <span>distinctive </span>feature might be the shape of the sign or the color scheme. However, if you have to distinguish between two warning signs, then color and shape will not help you at all, and you will be required to come up with more sophisticated features.</p>
<p>In order to demonstrate how the choice of features affects classification performance, we will focus on the following:</p>
<ul>
<li><strong>A few simple color transformations</strong> (such as grayscale; <strong>red, green, blue</strong> (<strong>RGB</strong>); and <strong>hue, saturation, value</strong> (<strong>HSV</strong>): Classification based on grayscale images will give us some baseline performance for the classifier. RGB might give us slightly better performance because of the distinct color schemes of some traffic signs.</li>
</ul>
<p style="padding-left: 60px;">Even better performance is expected from HSV. This is because it represents colors even more robustly than RGB. Traffic signs tend to have very bright, saturated colors that (ideally) are quite distinct from their surroundings.</p>
<ul>
<li><strong>SURF</strong>: This should appear very familiar to you by now. We have previously recognized SURF as an efficient and robust method of extracting meaningful features from an image. So, can't we use this technique to our advantage in a classification task?</li>
<li><strong>HOG</strong>: This is by far the most advanced feature descriptor to be considered in this chapter. The technique counts occurrences of gradient orientations along a dense grid laid out on the image and is well suited for use with SVMs.</li>
</ul>
<p>Feature extraction is performed by functions in the <kbd>data/process.py</kbd> file, from which we will call different functions to construct and compare different features.</p>
<p>Here is a nice blueprint, which—if you follow it—will enable you to easily write your own featurization functions and use with our code, and compare if your <kbd>your_featurize</kbd> function will yield better results:</p>
<pre>def your_featurize(data: List[np.ndarry], **kwargs) -&gt; np.ndarray: 
    ...</pre>
<p>The <kbd>*_featurize</kbd> functions take a list of images and return a matrix (as a 2D <kbd>np.ndarray</kbd>), where each row is a new sample and each column represents a feature.</p>
<div class="packt_infobox">For most of the following features, we will be using the (already suitable) default arguments in OpenCV. However, these values are not set in stone, and, even in real-world classification tasks, it is often necessary to search across the range of possible values for both features extracting and feature learning parameters in a process called <strong>hyperparameter exploration</strong>.</div>
<div>
<p>Now that we know what we are doing, let's take a look at some featurization functions that we have come up with that build on top of concepts from previous sections and also add some new concepts as well.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding common preprocessing</h1>
                </header>
            
            <article>
                
<p>Before we look at what we have come up with, let's take our time to look at the two most common forms of preprocessing that are almost always applied to any data before machine learning tasks—namely, <strong>mean subtraction</strong> and <strong>normalization</strong>.</p>
<p>Mean subtraction is the most common form of preprocessing (sometimes also referred to as <strong>zero centering</strong> or de-meaning), where the mean value of every feature dimension is calculated across all samples in a dataset. This feature-wise average is then subtracted from every sample in the dataset. You can think of this process as centering the <em>cloud</em> of data on the origin.</p>
<p>Normalization refers to the scaling of data dimensions so that they are of roughly the same scale. This can be achieved by either dividing each dimension by its standard deviation (once it has been zero-centered) or scaling each dimension to lie in the range of [-1, 1].</p>
<p>It makes sense to apply this step only if you have reason to believe that different input features have different scales or units. In the case of images, the relative scales of pixels are already approximately equal (and in the range of [0, 255]), so it is not strictly necessary to perform this additional preprocessing step.</p>
<p>Armed with these two concepts, let's take a look at our feature extractors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning about grayscale features</h1>
                </header>
            
            <article>
                
<p>The easiest feature to extract is probably the grayscale value of each pixel. Usually, grayscale values are not very indicative of the data they describe, but we will include them here for illustrative purposes (that is, to achieve baseline performance).</p>
<p>For each image in the input set, we are going to perform the following steps:</p>
<ol>
<li>Resize all images to have the same (usually smaller) size. We use <kbd>scale_size=(32, 32)</kbd> to make sure we don't make the images too small. At the same time, we want our data to be small enough to work on our personal computer. We can do this with the following code:</li>
</ol>
<pre style="padding-left: 60px;">resized_images = (cv2.resize(x, scale_size) for x in data)</pre>
<ol start="2">
<li>Convert the image to grayscale (values are still in 0-255 range), like this:</li>
</ol>
<pre style="padding-left: 60px;">gray_data = (cv2.cvtColor(x, cv2.COLOR_BGR2GRAY) for x in resized_images)</pre>
<ol start="3">
<li>Convert each image to have the pixel value in (0, 1) and flatten, so instead of a matrix of <kbd>(32, 32)</kbd> size for each image, we have a vector of size <kbd>1024</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px;">scaled_data = (np.array(x).astype(np.float32).flatten() / 255 for x in gray_data)</pre>
<ol start="4">
<li>Subtract the average pixel value of the flattened vector, like this:</li>
</ol>
<pre style="padding-left: 60px;">return np.vstack([x - x.mean() for x in scaled_data])</pre>
<p>We use the returned matrix as our training data for the machine learning algorithm.</p>
<p>Now, let's take a look at another example—<em>what would happen if we used information in the colors as well?</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding color spaces</h1>
                </header>
            
            <article>
                
<p>Alternatively, you might find that colors contain some information that raw grayscale values cannot capture. Traffic signs often have a distinct color scheme, and it might be indicative of the information it is trying to convey (that is, red for stop signs and forbidden actions; green for informational signs; and so on). We could opt to use the RGB images as input, but, in our case, we do not have to do anything since the dataset is already RGB.</p>
<p>However, even RGB might not be informative enough. For example, a stop sign in broad daylight might appear very bright and clear, but its colors might appear much less vibrant on a rainy or foggy day. A better choice might be the HSV color space, which represents colors using hue, saturation, and value (or brightness).</p>
<p>The most telling feature of traffic signs in this color space might be the hue (a more perceptually relevant description of color or chromaticity), provides an improved ability to distinguish between the color scheme of different sign types. Saturation and value could be equally important, however, as traffic signs tend to use relatively bright and saturated colors that do not typically appear in natural scenes (that is, their surroundings).</p>
<p>In OpenCV, the HSV color space is only a single call to <kbd>cv2.cvtColor</kbd> away, as shown in the following code:</p>
<pre>    hsv_data = (cv2.cvtColor(x, cv2.COLOR_BGR2HSV) for x in resized_images)</pre>
<p>So, to summarize, featurization is almost the same as for grayscale features. For each image, we carry out the following four steps:</p>
<ol>
<li>Resize all images to have the same (usually smaller) size.</li>
<li>Convert the image to HSV (values in the 0-255 range).</li>
<li>Convert each image to have the pixel value in (0, 1), and flatten it.</li>
<li>Subtract the average pixel value of the flattened vector.</li>
</ol>
<p>Now, let's try to look at a more complex example of a feature extractor that uses SURF.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using SURF descriptor</h1>
                </header>
            
            <article>
                
<p>But wait a minute! In <a href="905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Finding Objects via Feature Matching and Perspective Transforms</em>, you learned that the SURF descriptor is one of the best and most robust ways to describe images independent of scale or rotations. Can we use this technique to our advantage in a classification task?</p>
<p>Glad you asked! To make this work, we need to adjust SURF so that it returns a fixed number of features per image. By default, the SURF descriptor is only applied to a small list of <em>interesting</em> key points in the image, the number of which might differ on an image-by-image basis. This is unsuitable for our current purposes because we want to find a fixed number of feature values per data sample.</p>
<p>Instead, we need to apply SURF to a fixed dense grid laid out over the image, for which we create a key points array containing all pixels, as illustrated in the following code block:</p>
<pre>def surf_featurize(data, *, scale_size=(16, 16)):
    all_kp = [cv2.KeyPoint(float(x), float(y), 1)<br/>              for x, y in itertools.product(range(scale_size[0]),<br/>                                            range(scale_size[1]))]</pre>
<p>Then, it is possible to obtain SURF descriptors for each point on the grid and append that data sample to our feature matrix. We initialize SURF with a <kbd>hessianThreshold</kbd> value of <kbd>400</kbd>, as we did before, like this:</p>
<pre>    surf = cv2.xfeatures2d_SURF.create(hessianThreshold=400)</pre>
<p>The key points and descriptors can then be obtained via the following code:</p>
<pre>    kp_des = (surf.compute(x, kp) for x in data)</pre>
<p>Because <kbd>surf.compute</kbd> has two output arguments, <kbd>kp_des</kbd> will actually be a concatenation of both key points and descriptors. The second element in the <kbd>kp_des</kbd> array is the descriptor that we care about.</p>
<p>We select the first <kbd>num_surf_features</kbd> from each data sample and return it as a feature for the image, as follows:</p>
<pre>    return np.array([d.flatten()[:num_surf_features]<br/>                     for _, d in kp_des]).astype(np.float32)</pre>
<p>Now, let's take a look at a new concept that is very popular in the community—HOG.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mapping HOG descriptor</h1>
                </header>
            
            <article>
                
<p>The last feature descriptor to consider is the HOG. HOG features have previously been shown to work exceptionally well in combination with SVMs, especially when applied to tasks such as pedestrian recognition.</p>
<p>The essential idea behind HOG features is that the local shapes and appearance of objects within an image can be described by the distribution of edge directions. The image is divided into small connected regions, within which a histogram of gradient directions (or edge directions) is compiled.</p>
<p>The following screenshot shows such a histogram from a region in a picture. Angles are not directional; that's why the range is (<span class="packt_screen">-180</span>, <span class="packt_screen">180</span>):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/169da355-a9f3-4b2c-a7cf-551e233b1e8b.png" style="width:27.42em;height:20.92em;"/></p>
<p class="CDPAlignLeft CDPAlign"><span>As you</span> <span>can see, it has a lot of edge directions in the horizontal direction (angles around <span class="packt_screen">+180</span> and</span> <span class="packt_screen">-180</span> <span>degrees), so this seems like a good feature, especially when we are working with arrows and lines.</span></p>
<p>Then, the descriptor is assembled by concatenating the different histograms. For improved performance, the local histograms can be contrast normalized, which results in better invariance to changes in illumination and shadowing. You can see why this sort of preprocessing might be just the perfect fit for recognizing traffic signs under different viewing angles and lighting conditions.</p>
<p>The HOG descriptor is fairly accessible in OpenCV by means of <kbd>cv2.HOGDescriptor</kbd>, which takes the detection window size (32 x 32), the block size (16 x 16), the cell size (8 x 8), and the cell stride (8 x 8) as input arguments. For each of these cells, the HOG descriptor then calculates a HOG using nine bins, like this:</p>
<pre>def hog_featurize(data, *, scale_size=(32, 32)):<br/>    block_size = (scale_size[0] // 2, scale_size[1] // 2)<br/>    block_stride = (scale_size[0] // 4, scale_size[1] // 4)<br/>    cell_size = block_stride<br/>    hog = cv2.HOGDescriptor(scale_size, block_size, block_stride,<br/>                            cell_size, 9)<br/>    resized_images = (cv2.resize(x, scale_size) for x in data)<br/>    return np.array([hog.compute(x).flatten() for x in resized_images])</pre>
<p>Applying the HOG descriptor to every data sample is then as easy as calling <kbd>hog.compute</kbd>.</p>
<p>After we have extracted all the features we want, we return a flattened list for each of the images.</p>
<p>Now, we are finally ready to train the classifier on the preprocessed dataset. So, let's move on to the SVM.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning about SVMs</h1>
                </header>
            
            <article>
                
<p>An SVM is a learner for binary classification (and regression) that tries to separate examples from the two different class labels with a decision boundary that maximizes the margin between the two classes.</p>
<p>Let's return to our example of positive and negative data samples, each of which has exactly two features (<span class="packt_screen">x</span> and <span class="packt_screen">y</span>) and two possible decision boundaries, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/451b4435-7bdd-4dd9-b9b5-105450095b86.png" style="width:37.58em;height:11.33em;"/></p>
<p>Both of these decision boundaries get the job done. They partition all the samples of positives and negatives with zero misclassifications. However, one of them seems intuitively better. How can we quantify <em>better</em> and thus learn the <em>best</em> parameter settings?</p>
<p>This is where SVMs come into the picture. SVMs are also called <strong>maximal margin classifiers</strong> because they can be used to do exactly that—define the decision boundary so as to make those two clouds of <span class="packt_screen">+</span> and <span class="packt_screen">-</span> as far apart as possible; that is, as far apart from the decision boundary as possible.</p>
<p>For the preceding example, an SVM would find two parallel lines that pass through the data points on the class margins (the <em>dashed lines</em> in the following screenshot), and then make the line (that passes through the center of the margins) the decision boundary (the <em>bold black line</em> in the following screenshot):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1e826ff4-c743-4082-8bb1-73cebd9ae244.png" style="width:30.25em;height:18.00em;"/></p>
<p>It turns out that to find the maximal margin, it is only important to consider the data points that lie on the class margins. These points are sometimes also called <strong>support vectors</strong>.</p>
<div class="packt_infobox"><span>In addition to performing linear classification (that is, when the decision boundary is a straight line), SVMs can also perform a non-linear classification using what is called the</span><span> </span><strong>kernel trick</strong><span>, implicitly mapping their input to high-dimensional feature spaces.</span></div>
<p>Now, let's take a look at how we can turn this binary classifier into a multiclass classifier that is more appropriate for the 43-class classification problem we are trying to tackle.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using SVMs for multiclass classification</h1>
                </header>
            
            <article>
                
<p>Whereas some classification algorithms, such as neural networks, naturally lend themselves to using more than two classes, SVMs are binary classifiers by nature. They can, however, be turned into multiclass classifiers.</p>
<p>Here, we will consider two different strategies:</p>
<ul>
<li><strong>One-versus-all</strong>: The <em>one-versus-all</em> strategy involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives.</li>
</ul>
<p style="padding-left: 60px;">For the <kbd>k</kbd> classes, this strategy thus requires the training of <kbd>k</kbd> number of different SVMs. During testing, all classifiers can express a <em>+1</em> vote by predicting that an unseen sample belongs to their class.</p>
<p style="padding-left: 60px;">In the end, an unseen sample is classified by the ensemble as the class with the most votes. Usually, this strategy is used in combination with confidence scores instead of predicted labels so that, in the end, the class with the highest confidence score can be picked.</p>
<ul>
<li><strong>One-versus-one</strong>: The <em>one-versus-one</em> strategy involves training a single classifier per class pair, with the samples of the first class as positive samples and the samples of the second class as negative samples. For the <kbd>k</kbd> classes, this strategy requires the training of <kbd>k*(k-1)/2</kbd> classifiers.</li>
</ul>
<p style="padding-left: 60px;">However, the classifiers have to solve a significantly easier task, so there is a trade-off when considering which strategy to use. During testing, all classifiers can express a <em>+1</em> vote for either the first or the second class. In the end, an unseen sample is classified by the ensemble as the class with the most votes.</p>
<p>Usually, you would not have to write your own classification algorithms unless you really wanted to dive deep into the algorithms and squeeze the last bit of performance out of your model. And luckily, OpenCV already comes with a good machine learning toolkit that we will use in this chapter. OpenCV uses a one-versus-all approach, and we will focus on that approach.</p>
<p>Now, let's get our hands dirty, and see how we can code this up with OpenCV and get some real results.<br/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the SVM</h1>
                </header>
            
            <article>
                
<p>We are going to write the training method in a separate function; it's a good practice if we later wanted to change our training method. First, we define the signature of our function, as follows:</p>
<pre>def train(training_features: np.ndarray, training_labels: np.ndarray):</pre>
<p>Thus, we want a function that takes two arguments—<kbd>training_features</kbd> and <kbd>training_labels</kbd>—and the correct answers corresponding to each feature. Thus, the first argument will be a matrix in the form of a two-dimensional NumPy array, and the second argument will be a one-dimensional NumPy array.</p>
<p>Then, the function will return an object that should have a <kbd>predict</kbd> method, which takes new unseen data and labels it. So, let's get started and see how we could train an SVM with OpenCV.</p>
<p>We name our function <kbd>train_one_vs_all_SVM</kbd>, and do the following:</p>
<ol>
<li>Instantiate an SVM class instance using <kbd>cv2.ml.SVM_create</kbd>, which creates a multiclass SVM using the one-versus-all strategy, as follows:</li>
</ol>
<pre style="padding-left: 60px;">def train_one_vs_all_SVM(X_train, y_train):<br/>    svm = cv2.ml.SVM_create()</pre>
<ol start="2">
<li>Set the hyperparameters of the learner. These are called <strong>hyperparameters</strong> because these parameters are out of the control of the learner (versus parameters that the learner changes during the learning process). This can be done with the following code:</li>
</ol>
<pre style="padding-left: 60px;">    svm.setKernel(cv2.ml.SVM_LINEAR)<br/>    svm.setType(cv2.ml.SVM_C_SVC)<br/>    svm.setC(2.67)<br/>    svm.setGamma(5.383)</pre>
<ol start="3">
<li>Call the <kbd>train</kbd> method on the SVM instance, and OpenCV takes care of training (this takes a couple of minutes on a regular laptop computer with the GTSRB dataset), as follows:</li>
</ol>
<pre style="padding-left: 60px;">    svm.train(X_train, cv2.ml.ROW_SAMPLE, y_train)<br/>    return svm</pre>
<p>OpenCV will take care of the rest. What happens under the hood is that the SVM training uses <strong>Lagrange multipliers</strong> to optimize some constraints that lead to the maximum margin decision boundary.</p>
<p>The optimization process is usually performed until some termination criteria are met, which can be specified via the SVM's optional arguments.</p>
<p class="mce-root">Now that we have looked at training the SVM, let's look at testing it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the SVM</h1>
                </header>
            
            <article>
                
<p>There are many ways to evaluate a classifier, but most often, we are simply interested in the accuracy metric—that is, how many data samples from the test set were classified correctly.</p>
<p>In order to arrive at this metric, we need to get the prediction results out of the SVM—and again, OpenCV has us covered, by providing the <kbd>predict</kbd> method that takes a matrix of features and returns an array of predicted labels. We thus need to proceed as follows:</p>
<ol>
<li>So, we have to first featurize our testing data:</li>
</ol>
<pre style="padding-left: 60px;">        x_train = featurize(train_data)</pre>
<ol start="2">
<li>Then, we feed the featurized data to the classifier and get the predicted labels, like this:</li>
</ol>
<pre style="padding-left: 60px;">        y_predict = model.predict(x_test)</pre>
<ol start="3">
<li>After that, we can try to see how many of the labels the classifier got correctly, by running the following code:</li>
</ol>
<pre style="padding-left: 60px;">        num_correct = sum(y_predict == y_test)</pre>
<p>Now, we are ready to calculate the desired performance metrics, as described in detail in later sections. For the purpose of this chapter, we choose to calculate accuracy, precision, and recall.</p>
<p class="mce-root"><span>The </span><kbd>scikit-learn</kbd><span> machine learning package (which can be found at </span><a href="http://scikit-learn.org"><span class="URLPACKT">http:</span><span class="URLPACKT">//scikit-learn.org</span></a><span>) supports the three metrics—which are accuracy, precision, and recall (as well as others)—straight out of the box, and also comes with a variety of other useful tools. </span><span>For educational purposes (and to minimize software dependencies), we will derive the three metrics ourselves.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Accuracy</h1>
                </header>
            
            <article>
                
<p>The most straightforward metric to calculate is probably accuracy. This metric simply counts the number of test samples that have been predicted correctly, and returns the number as a fraction of the total number of test samples, as shown in the following code block:</p>
<pre>def accuracy(y_predicted, y_true):<br/>    return sum(y_predicted == y_true) / len(y_true)</pre>
<p>The previous code shows that we have extracted <kbd>y_predicted</kbd> by calling <kbd>model.predict(x_test)</kbd>. This was quite simple, but, again, to make things reusable, we put this inside a function that takes <kbd>predicted</kbd> and <kbd>true</kbd> labels. And now, we will go on to implement slightly more complicated metrics that are useful to measure classifier performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Confusion matrix</h1>
                </header>
            
            <article>
                
<p>A confusion matrix is a 2D matrix of size equal to <kbd>(num_classes, num_classes)</kbd>, where the rows correspond to the predicted class labels, and the columns correspond to the actual class labels. Then, the <kbd>[r,c]</kbd> matrix element contains the number of samples that were predicted to have label <kbd>r</kbd>, but in reality, have label <kbd>c</kbd>. Having access to a confusion matrix will allow us to calculate precision and recall.</p>
<p>Now, let's implement a very simple way to calculate the confusion matrix. Similar to accuracy, we create a function with the same arguments, so it's easy to reuse, by following the next steps:</p>
<ol>
<li>Assuming our labels are non-negative integers, we can figure out <kbd>num_classes</kbd> by taking the highest integer and adding <kbd>1</kbd> to account for zero, as follows:</li>
</ol>
<pre style="padding-left: 60px;">def confusion_matrix(y_predicted, y_true):<br/>    num_classes = max(max(y_predicted), max(y_true)) + 1<br/>    ...</pre>
<ol start="2">
<li>Next, we instantiate an empty matrix, where we will fill the counts, like this:</li>
</ol>
<pre style="padding-left: 60px;">    conf_matrix = np.zeros((num_classes, num_classes))</pre>
<ol start="3">
<li>Next, we iterate over all data, and, for each datum, we take predicted value <kbd>r</kbd> and actual value <kbd>c</kbd>, and we increment the appropriate value in the matrix. There are much faster ways to achieve this, but nothing is simpler than counting everything one by one. We do this with the following code:</li>
</ol>
<pre style="padding-left: 60px;">    for r, c in zip(y_predicted, y_true):<br/>        conf_matrix[r, c] += 1</pre>
<ol start="4">
<li>After we have accounted for all the data in the training set, we can return our confusion matrix, as follows:</li>
</ol>
<pre style="padding-left: 60px;">    return conf_matrix</pre>
<ol start="5">
<li>Here is our confusion matrix for the GTSRB dataset test data:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d1b2e14e-a3dc-4457-91b2-dad492d830fc.png" style="width:37.25em;height:33.00em;"/></p>
<p style="padding-left: 60px;">A<span>s you can see, most of the values are in the diagonal. This means that at first glance, our classifier is doing pretty well.</span></p>
<ol start="6">
<li>It is also easy to calculate the accuracy from the confusion matrix as well. We just take the number of elements in the diagonal, and divide by the number of elements overall, like this:</li>
</ol>
<pre style="padding-left: 60px;">    cm = confusion_matrix(y_predicted, y_true)<br/>    accuracy = cm.trace() / cm.sum()  # 0.95 in this case.</pre>
<p>Note that we have a different number of elements in each of the classes. Each class contributes to accuracy differently, and our next metric will focus on per-class performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Precision</h1>
                </header>
            
            <article>
                
<p>Precision in binary classification is a useful metric for measuring the fraction of retrieved instances that are relevant (also called the <strong>positive predictive value</strong>). In a classification task, the number of <strong>true positives</strong> is defined as the number of items correctly labeled as belonging to the positive class.</p>
<p>Precision is defined as the number of true positives divided by the total number of positives. In other words, out of all the pictures in the test set that a classifier thinks to contain a cat, precision is the fraction of pictures that actually do contain a cat.</p>
<div class="packt_infobox"><span>Note that here, we have a positive label; thus, precision is a per-class value. We usually talk about the precision of one class or the precision of cats, and so on.</span></div>
<p>The total number of positives can also be calculated as the sum of <strong>true positives</strong> and <strong>false positives</strong>, the latter being the number of samples incorrectly labeled as belonging to a particular class. This is where the confusion matrix comes in handy because it will allow us to quickly calculate the number of false positives and true positives by following the next steps:</p>
<ol>
<li>So, in this case, we have to change our function arguments, and add the positive class label, like this:</li>
</ol>
<pre style="padding-left: 60px;">def precision(y_predicted, y_true, positive_label):
    ...</pre>
<ol start="2">
<li>Let's use our confusion matrix, and calculate the number of true positives, which will be the element at <kbd>[positive_label, positive_label]</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px;">    cm = confusion_matrix(y_predicted, y_true)<br/>    true_positives = cm[positive_label, positive_label]</pre>
<ol start="3">
<li>Now, let's calculate the number of true and false positives, which will be the sum of all elements on the <kbd>positive_label</kbd> row since the row indicates the predicted class label, as follows:</li>
</ol>
<pre style="padding-left: 60px;">    total_positives = sum(cm[positive_label])</pre>
<ol start="4">
<li>And finally, return the ratio of true positives and all positives, like this:</li>
</ol>
<pre style="padding-left: 60px;">    return true_positives / total_positives</pre>
<p>Based on different classes, we get very different values of precision. Here is a histogram of the precision scores for all 43 classes:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9988c2b9-ae74-42c6-8586-54db08e2b2c2.png" style="width:26.08em;height:20.83em;"/></p>
<p>The class with lower precision is <span class="packt_screen">30</span>, which means that a lot of other signs are mistaken to be the sign shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d3c3138f-b7f6-4796-97a5-82469fb203be.png" style="width:26.33em;height:23.50em;"/></p>
<p>In this case, it's alright if we are extra cautious while driving on the icy road, but it's possible that we missed something important. So, let's look at recall values for different classes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recall</h1>
                </header>
            
            <article>
                
<p>Recall is similar to precision in the sense that it measures the fraction of relevant instances that are retrieved (as opposed to the fraction of retrieved instances that are relevant). Thus, it will tell us the<span> probability that we will not notice it</span> for a given positive class (given sign). </p>
<p>In a classification task, the number of false negatives is the number of items that are not labeled as belonging to the positive class but should have been labeled.</p>
<p>Recall is the number of true positives divided by the sum of true positives and false negatives. In other words, out of all the pictures of cats in the world, recall is the fraction of pictures that have been correctly identified as pictures of cats.</p>
<p>Here is how to calculate recall of a given positive label using true and predicted labels:</p>
<ol>
<li>Again, we have the same signature as for the precision, and we retrieve true positives the same way, as follows:</li>
</ol>
<pre style="padding-left: 60px;">def recall(y_predicted, y_true, positive_label):<br/>    cm = confusion_matrix(y_predicted, y_true)<br/>    true_positives = cm[positive_label, positive_label]</pre>
<p style="padding-left: 60px;">Now, notice that the sum of true positives and false negatives is the total number of points in the given data class.</p>
<ol start="2">
<li>Thus, we just have to count the number of elements in that class, which means we sum the <kbd>positive_label</kbd> column of the confusion matrix, as follows:</li>
</ol>
<pre style="padding-left: 60px;">    class_members = sum(cm[:, positive_label])</pre>
<ol start="3">
<li>Then, we return the ratio as for the precision function, like this:</li>
</ol>
<pre style="padding-left: 60px;">    return true_positives / class_members</pre>
<p style="padding-left: 60px;">Now, let's look at the distribution of recall values for all 43 classes of traffic signs, shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/15d10f43-3ea6-4eda-86c9-15cd242e0a64.png" style="width:26.17em;height:20.83em;"/></p>
<p style="padding-left: 60px;">The recall values are a lot more spread out, with class 21 having a value of 0.66. Let's check out which class has a value of 21:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f779bebe-02c8-40d0-9a43-9dcb761b09d0.png" style="width:23.08em;height:21.67em;"/></p>
<p class="mce-root">Now, this is not as harmful as driving on a road covered with snowflakes/ice, but it's very important not to miss dangerous curves ahead on the road. Missing this sign could have bad consequences.</p>
<p>The next section will demonstrate the <kbd>main()</kbd> function routine needed to run our app. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting it all together</h1>
                </header>
            
            <article>
                
<p>To run our app, we will need to execute the main function routine (in <kbd>chapter6.py</kbd>). This loads the data, trains the classifier, evaluates its performance, and visualizes the result:</p>
<ol>
<li>First, we need to import all the relevant modules and set up the <kbd>main</kbd> function, as follows:</li>
</ol>
<pre style="padding-left: 60px;">import cv2<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/>from data.gtsrb import load_training_data<br/>from data.gtsrb import load_test_data<br/>from data.process import grayscale_featurize, hog_featurize</pre>
<ol start="2">
<li>Then, the goal is to compare classification performance across feature extraction methods. This includes running the task using a list of different feature extraction approaches. So, we first load the data, and repeat the process for each of the featurizing functions, as follows:</li>
</ol>
<pre style="padding-left: 60px;">def main(labels):<br/>    train_data, train_labels = load_training_data(labels)<br/>    test_data, test_labels = load_test_data(labels)<br/>    y_train, y_test = np.array(train_labels), np.array(test_labels)<br/>    accuracies = {}<br/>    for featurize in [hog_featurize, grayscale_featurize, hsv_featurize, <br/>    surf_featurize]:<br/>       ...</pre>
<p style="padding-left: 60px;">For each of the <kbd>featurize</kbd> functions, we perform the following steps:</p>
<ul>
<li style="list-style-type: none;">
<ol>
<li><span><kbd>Featurize</kbd></span> <span>the data, so we have a matrix of features, like this:</span></li>
</ol>
</li>
</ul>
<pre style="padding-left: 90px;">        x_train = featurize(train_data)</pre>
<ol start="2">
<li style="list-style-type: none;">
<ol start="2">
<li>Train a model using our <kbd>train_one_vs_all_SVM</kbd> method, as follows:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 90px;">        model = train_one_vs_all_SVM(x_train, y_train)</pre>
<ol start="3">
<li style="list-style-type: none;">
<ol start="3">
<li>Predict test labels for the training data, by featurizing the test data and passing to the <kbd>predict</kbd> method (we have to featurize test data separately to make sure we don't have information leakage), as follows:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 90px;">        x_test = featurize(test_data)<br/>        res = model.predict(x_test)<br/>        y_predict = res[1].flatten()</pre>
<ol>
<li style="list-style-type: none;">
<ol start="4">
<li>We score the predicted labels against the true labels, using the <kbd>accuracy</kbd> function, and store the score in a dictionary, to plot after we have results for all the <kbd>featurize</kbd> functions, like this:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 90px;">        accuracies[featurize.__name__] = accuracy(y_predict, y_test)</pre>
<ol start="3">
<li>Now, it's time to plot the results, and for this, we choose the <kbd>bar</kbd> plot functionality of <kbd>matplotlib</kbd>. We also make sure to scale the bar plot accordingly to visually understand the scale of difference. Since the accuracy is a number between <kbd>0</kbd> <span>and</span> <kbd>1</kbd><span>, we limit the <em>y</em> axis to</span> <kbd>[0, 1]</kbd><span>, as follows:</span></li>
</ol>
<pre style="padding-left: 60px;">    plt.bar(accuracies.keys(), accuracies.values())<br/>    plt.ylim([0, 1])</pre>
<ol start="4">
<li>We add some nice formatting to the plot by rotating labels on the horizontal axis, adding a <kbd>grid</kbd> and a <kbd>title</kbd> to the plot, like this:</li>
</ol>
<pre style="padding-left: 60px;">    plt.axes().xaxis.set_tick_params(rotation=20)<br/>    plt.grid()<br/>    plt.title('Test accuracy for different featurize functions')<br/>    plt.show()</pre>
<ol start="5">
<li>And after the last line of <kbd>plt.show()</kbd> has executed, the plot shown in the following screenshot pops up in a separate window:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fdf5a433-8f27-4d8b-9100-683d862e9fc3.png" style="width:80.33em;height:45.83em;"/></p>
<p style="padding-left: 60px;">So, we see that <kbd>hog_featurize</kbd> is a winner on this dataset, but we are far from having perfect results<span>—</span>slightly above 95%. To understand how good a result it's possible to get, you could do a quick Google search, and you will come across a lot of papers achieving 99%+ accuracy. So, even though we are not getting cutting-edge results, we did pretty well with an off-the-shelf classifier and an easy <kbd>featurize</kbd> function.</p>
<p>Another interesting fact is that even though we thought that traffic signs having bright colors should lead to <span class="packt_screen">hsv_featurize</span> (it being more important than grayscale features), it turns out that's not the case.</p>
<p>So, a good takeaway is that you should experiment with your data to develop better intuition about which features work for your data and which don't.</p>
<p>Speaking of experimentation, let's use a neural network to increase the efficiency of our obtained results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving results with neural networks</h1>
                </header>
            
            <article>
                
<p>Let's do a quick teaser of how good we could get if we were to use some fancy <strong>deep neural networks</strong> (<strong>DNNs</strong>), and give you a sneak peek of what is to come in the future chapters of this book.</p>
<p>If we use the following "<em>not quite so deep"</em> neural network, which takes about 2 minutes to train on my laptop (where it takes 1<em> minute</em> to train the SVM), we get an accuracy of around 0.964!</p>
<p>Here is a snippet of the training method (you should be able to plug it into the preceding code, and play with some parameters to see if you could do it later):</p>
<pre>def train_tf_model(X_train, y_train):<br/>    model = tf.keras.models.Sequential([<br/>        tf.keras.layers.Conv2D(20, (8, 8),<br/>                               input_shape=list(UNIFORM_SIZE) + [3],<br/>                               activation='relu'),<br/>        tf.keras.layers.MaxPooling2D(pool_size=(4, 4), strides=4),<br/>        tf.keras.layers.Dropout(0.15),<br/>        tf.keras.layers.Flatten(),<br/>        tf.keras.layers.Dense(64, activation='relu'),<br/>        tf.keras.layers.Dropout(0.15),<br/>        tf.keras.layers.Dense(43, activation='softmax')<br/>    ])<br/><br/>    model.compile(optimizer='adam',<br/>                  loss='sparse_categorical_crossentropy',<br/>                  metrics=['accuracy'])<br/>    model.fit(x_train, np.array(train_labels), epochs=10)<br/>    return model</pre>
<p>The code uses the<span> high-level Keras API of</span> <span>TensorFlow (we will see more of this in the upcoming chapters) and creates a neural network with the</span> following<span>:</span></p>
<ul>
<li><strong>Convolutional layer</strong> with max pooling that is followed by a dropout<span>—</span>which is there only during the training.</li>
<li><strong>Hidden Dense layer</strong> that is followed by a dropout<span>—</span>which is there only during the training.</li>
<li><strong>Final Dense layer</strong> that spits out the final result; it should identify which class (<span>among the 43 classes)</span> the input data belongs to.</li>
</ul>
<p class="mce-root">Note that we only have one convolutional layer, which is very similar to HOG featurize. If we were to add more convolutional layers, the performance would improve quite a lot, but let's leave that for the next chapters to explore.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we trained a multiclass classifier to recognize traffic signs from the GTSRB database. We discussed the basics of supervised learning, explored the intricacies of feature extraction, and sneaked a peek into DNNs.</p>
<p>Using the approach we took in this chapter, you should be able to <span>formulate real-life problems as machine learning models, </span><span>use your Python skills to download a sample labeled dataset from the internet, </span><span>write your featurizing functions that convert images to feature vectors, </span><span>and use OpenCV for training off-the-shelf machine learning models that help you solve your real-life problems.</span></p>
<p>Notably, we left out some details along the way, such as attempting to fine-tune the hyperparameters of the learning algorithm (as they were out of the scope of this book). We only looked at accuracy scores and didn't do much feature engineering by trying to combine all sets of different features.</p>
<p>With this functional setup and a good understanding of the underlying methodology, you can now classify the entire GTSRB dataset to get accuracies higher than 0.97! How about 0.99? It is definitely worth taking a look at their website, where you will find classification results for a variety of classifiers. Maybe your own approach will soon be added to the list.</p>
<p>In the next chapter, we will move even deeper into the field of machine learning. Specifically, we will focus on recognizing emotional expressions in human faces using <strong>convolutional neural networks</strong> (<strong>CNNs</strong>). This time, we will combine the classifier with a framework for object detection, which will allow us to find a human face in an image, and then focus on identifying the emotional expression contained in that face.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset attribution</h1>
                </header>
            
            <article>
                
<p><span>J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, The German Traffic Sign Recognition Benchmark—A multiclass classification competition, in </span><em>Proceedings of the IEEE International Joint Conference on Neural Networks</em>, 2011<span>, pages 1453–1460</span><span>.</span></p>


            </article>

            
        </section>
    </body></html>