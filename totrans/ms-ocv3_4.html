<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Non-Rigid Face Tracking</h1>
            </header>

            <article>
                
<p>Non-rigid face tracking, which is the estimation of a quasi-dense set of facial features in each frame of a video stream, is a difficult problem for which modern approaches borrow ideas from a number of related fields, including Computer Vision, computational geometry, machine learning, and image processing. Non-rigidity here refers to the fact that relative distances between facial features vary between facial expression and across the population, and is distinct from face detection and tracking, which aims only to find the location of the face in each frame, rather than the configuration of facial features. Non-rigid face tracking is a popular research topic that has been pursued for over two decades, but it is only recently that various approaches have become robust enough, and processors fast enough, which makes the building of commercial applications possible.</p>
<p>Although commercial-grade face tracking can be highly sophisticated and pose a challenge even for experienced Computer Vision scientists, in this chapter we will see that a face tracker that performs reasonably well under constrained settings can be devised using modest mathematical tools and OpenCV's substantial functionality in linear algebra, image processing, and visualization. This is particularly the case when the person to be tracked is known ahead of time, and training data in the form of images and landmark annotations are available. The techniques described henceforth will act as a useful starting point and a guide for further pursuits towards a more elaborate face-tracking system.</p>
<p>An outline of this chapter is as follows:</p>
<ul>
<li><strong>Overview</strong>: This section covers a brief history of face tracking.</li>
<li><strong>Utilities</strong>: This section outlines the common structures and conventions used in this chapter. It includes object-oriented design, data storage and representation, and a tool for data collection and annotation.</li>
<li><strong>Geometrical constraints</strong>: This section describes how facial geometry and its variations are learned from the training data and utilized during tracking to constrain the solution. This includes modeling the face as a linear shape model and how global transformations can be integrated into its representation.</li>
<li><strong>Facial feature detectors</strong>: This section describes how to learn the appearance of facial features in order to detect them in an image where the face is to be tracked.</li>
<li><strong>Face detection and initialization</strong>: This section describes how to use face detection to initialize the tracking process.</li>
<li><strong>Face tracking</strong>: This section combines all components described previously into a tracking system through the process of image alignment. Discussion on the settings in which the system can be expected to work best.</li>
</ul>
<p>The following block diagram illustrates the relationships between the various components of the system:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="292" width="406" class="image-border" src="assets/image_05_001.jpg"/></div>
<div class="packt_infobox">Note that all methods employed in this chapter follow a data-driven paradigm whereby all models used are learned from data rather than designed by hand in a rule-based setting. As such, each component of the system will involve two components: training and testing. Training builds the models from data and testing employs these models on new unseen data.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Overview</h1>
            </header>

            <article>
                
<p>Non-rigid face tracking was first popularized in the early to mid-1990s with the advent of <strong>Active Shape Models</strong> (<strong>ASM</strong>) by Cootes and Taylor. Since then, a tremendous amount of research has been dedicated to solving the difficult problem of generic face tracking with many improvements over the original method that ASM proposed. The first milestone was the extension of ASM to <strong>Active Appearance Models</strong> (<strong>AAM</strong>) in 2001, also by Cootes and Taylor. This approach was later formalized though the principled treatment of image warps by Baker and colleges in the mid-2000s. Another strand of work along these lines was the <strong>3D morphable model</strong> (<strong>3DMM</strong>) by Blanz and Vetter, which like AAM, not only modeled image textures as opposed to profiles along object boundaries as in ASM, but took it one step further by representing the models with a highly dense 3D data learned from laser scans of faces. From the mid- to late 2000s, the focus of research on face tracking shifted away from how the face was parameterized to how the objective of the tracking algorithm was posed and optimized. Various techniques from the machine-learning community were applied with various degrees of success. Since the turn of the century, the focus has shifted once again, this time towards joint parameter and objective design strategies that guarantee global solutions.</p>
<p>Despite the continued intense research into face tracking, there have been relatively few commercial applications that use it. There has also been a lag in uptake by hobbyists and enthusiasts, despite there being a number of freely available source code packages for a number of common approaches. Nonetheless, in the past 2 years there has been a renewed interest in the public domain for the potential use of face tracking and commercial-grade products are beginning to emerge.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Utilities</h1>
            </header>

            <article>
                
<p>Before diving into the intricacies of face tracking, a number of book-keeping tasks and conventions common to all face-tracking methods must first be introduced. The rest of this section will deal with these issues. An interested reader may want to skip this section at the first reading and go straight to the section on geometrical constraints.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Object-oriented design</h1>
            </header>

            <article>
                
<p>As with face detection and recognition, programmatically, face tracking consists of two components: data and algorithms. The algorithms typically perform some kind of operation on the incoming (that is, online) data by referencing prestored (that is, offline) data as a guide. As such, an object-oriented design that couples algorithms with the data they rely on is a convenient design choice.</p>
<p>In OpenCV v2.x, a convenient XML/YAML file storage class was introduced that greatly simplifies the task of organizing offline data for use in the algorithms. To leverage this feature, all classes described in this chapter will implement read-and write-serialization functions. An example of this is shown as follows for an imaginary class <kbd>foo</kbd>:</p>
<pre>
    #include &lt;opencv2/opencv.hpp&gt; 
    using namespace cv; 
    class foo { 
      public: 
      Mat a; 
      type_b b; 
      void write(FileStorage &amp;fs) const{ 
        assert(fs.isOpened()); 
        fs&lt;&lt; "{" &lt;&lt; "a"  &lt;&lt; a &lt;&lt; "b"  &lt;&lt; b &lt;&lt; "}"; 
      } 
      void read(const FileNode&amp; node){ 
        assert(node.type() == FileNode::MAP); 
        node["a"] &gt;&gt; a; node["b"] &gt;&gt; b; 
      } 
    };
</pre>
<p>Here, <kbd>Mat</kbd> is OpenCV's matrix class and <kbd>type_b</kbd> is a (imaginary) user-defined class that also has the serialization functionality defined. The I/O functions <kbd>read</kbd> and <kbd>write</kbd> implement the serialization. The <kbd>FileStorage</kbd> class supports two types of data structures that can be serialized. For simplicity, in this chapter all classes will only utilize mappings, where each stored variable creates a <kbd>FileNode</kbd> object of type <kbd>FileNode::MAP</kbd>. This requires a unique key to be assigned to each element. Although the choice for this key is arbitrary, we will use the variable name as the label for consistency reasons. As illustrated in the preceding code snippet, the <kbd>read</kbd> and <kbd>write</kbd> functions take on a particularly simple form, whereby the streaming operators (<kbd>&lt;&lt;</kbd> and <kbd>&gt;&gt;</kbd>) are used to insert and extract data to the <kbd>FileStorage</kbd> object. Most OpenCV classes have implementations of the <kbd>read</kbd> and <kbd>write</kbd> functions, allowing the storage of the data that they contain to be done with ease.</p>
<p>In addition to defining the serialization functions, one must also define two additional functions for the serialization in the <kbd>FileStorage</kbd> class to work, as follows:</p>
<pre>
    void write(FileStorage&amp; fs, const string&amp;, const foo&amp; x) { 
      x.write(fs); 
    } 
    void read(const FileNode&amp; node, foo&amp; x,const foo&amp; default){ 
      if(node.empty())x = d; else x.read(node); 
    }
</pre>
<p>As the functionality of these two functions remains the same for all classes we describe in this section, they are templated and defined in the <kbd>ft.hpp</kbd> header file found in the source code pertaining to this chapter. Finally, to easily save and load user-defined classes that utilize the serialization functionality, templated functions for these are also implemented in the header file as follows:</p>
<pre>
    template&lt;class T&gt; 
    T load_ft(const char* fname){ 
      T x; FileStorage f(fname,FileStorage::READ); 
      f["ft object"] &gt;&gt; x; f.release(); return x; 
    } 
    template&lt;class T&gt; 
    void save_ft(const char* fname,const T&amp; x){ 
      FileStorage f(fname,FileStorage::WRITE); 
      f &lt;&lt; "ft object" &lt;&lt; x; f.release(); 
    }
</pre>
<p>Note that the label associated with the object is always the same (that is, <kbd>ft object</kbd>). With these functions defined, saving and loading object data is a painless process. This is shown with the help of the following example:</p>
<pre>
    #include "opencv_hotshots/ft/ft.hpp" 
    #include "foo.hpp" 
    int main() { 
      ... 
      foo A; save_ft&lt;foo&gt;("foo.xml",A); 
      ... 
      foo B = load_ft&lt;foo&gt;("foo.xml"); 
      ... 
    }
</pre>
<p>Note that the <kbd>.xml</kbd> extension results in an XML-formatted data file. For any other extension, it defaults to the (more human-readable) YAML format.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data collection - image and video annotation</h1>
            </header>

            <article>
                
<p>Modern face-tracking techniques are almost entirely data driven, that is, the algorithms used to detect the locations of facial features in the image rely on models of the appearance of the facial features and the geometrical dependencies between their relative locations from a set of examples. The larger the set of examples, the more robust the algorithms behave, as they become more aware of the gamut of variability that faces can exhibit. Thus, the first step in building a face-tracking algorithm is to create an image/video annotation tool, where the user can specify the locations of the desired facial features in each example image.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Training data types</h1>
            </header>

            <article>
                
<p>The data for training face tracking algorithms generally consists of four components:</p>
<ul>
<li><strong>Images</strong>: This component is a collection of images (still images or video frames) that contain an entire face. For best results, this collection should be specialized to the types of conditions (that is, identity, lighting, distance from camera, capturing device, among others) in which the tracker is later deployed. It is also crucial that the faces in the collection exhibit the range of head poses and facial expressions that the intended application expects.</li>
<li><strong>Annotations</strong>: This component has ordered hand-labeled locations in each image that correspond to every facial feature to be tracked. More facial features often lead to a more robust tracker as the tracking algorithm can use their measurements to reinforce each other. The computational cost of common tracking algorithms typically scales linearly with the number of facial features.</li>
<li><strong>Symmetry indices</strong>: This component has an index for each facial feature point that defines its bilaterally symmetrical feature. This can be used to mirror the training images, effectively doubling the training set size and symmetrizing the data along the <em>y</em> axis.</li>
<li><strong>Connectivity indices</strong>: This component has a set of index pairs of the annotations that define the semantic interpretation of the facial features. These connections are useful for visualizing the tracking results.</li>
</ul>
<p>A visualization of these four components is shown in the following image, <span>where from left to right we have the raw image, facial feature annotations, </span><span>color-coded bilateral symmetry points, mirrored image, and annotations </span><span>and facial feature connectivity:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_05_002.jpg"/></div>
<p>To conveniently manage such data, a class that implements storage and access functionality is a useful component. The <kbd>CvMLData</kbd> class in the <kbd>ml</kbd> module of OpenCV has the functionality for handling general data often used in machine-learning problems. However, it lacks the functionality required from the face-tracking data. As such, in this chapter, we will use the <kbd>ft_data</kbd> class, declared in the <kbd>ft_data.hpp</kbd> header file, which is designed specifically with the peculiarity of face-tracking data in mind. All data elements are defined as public members of the class, as follows:</p>
<pre>
    class ft_data{ 
      public: 
      vector&lt;int&gt; symmetry; 
      vector&lt;Vec2i&gt; connections; 
      vector&lt;string&gt; imnames; 
      vector&lt;vector&lt;Point2f&gt;&gt; points; 
      ... 
    }
</pre>
<p>The <kbd>Vec2i</kbd> and <kbd>Point2f</kbd> types are OpenCV classes for vectors of two integers and 2D floating-point coordinates respectively. The <kbd>symmetry</kbd> vector has as many components as there are feature points on the face (as defined by the user). Each of the <kbd>connections</kbd> define a zero-based index pair of connected facial features. As the training set can potentially be very large, rather than storing the images directly, the class stores the filenames of each image in the <kbd>imnames</kbd> member variable (note that this requires the images to be located in the same relative path for the filenames to remain valid). Finally, for each training image, a collection of facial feature locations are stored as vectors of floating-point coordinates in the <kbd>points</kbd> member variable.</p>
<p>The <kbd>ft_data</kbd> class implements a number of convenience methods for accessing the data. To access an image in the dataset, the <kbd>get_image</kbd> function loads the image at the specified index, <kbd>idx</kbd>, and optionally mirrors it around the y axis as follows:</p>
<pre>
    Mat 
    ft_data::get_image( 
      const int idx,   //index of image to load from file 
      const int flag) { //0=gray,1=gray+flip,2=rgb,3=rgb+flip 
        if((idx &lt; 0) || (idx &gt;= (int)imnames.size()))return Mat(); 
        Mat img,im; 
        if(flag &lt; 2) img = imread(imnames[idx],0); 
        else         img = imread(imnames[idx],1); 
        if(flag % 2 != 0) flip(img,im,1); 
        else              im = img; 
        return im; 
      }
</pre>
<p>The (<kbd>0</kbd>,<kbd>1</kbd>) flag passed to OpenCV's <kbd>imread</kbd> function specifies whether the image is loaded as a three-channel color image or as a single-channel grayscale image. The flag passed to OpenCV's <kbd>flip</kbd> function specifies the mirroring around the <em>y</em> axis.</p>
<p>To access a point set corresponding to an image at a particular index, the <kbd>get_points</kbd> function returns a vector of floating-point coordinates with the option of mirroring their indices as follows:</p>
<pre>
    vector&lt;Point2f&gt; 
    ft_data::get_points( 
    const int idx,        //index of image corresponding to points
    const bool flipped) { //is the image flipped around the y-axis? 
      if((idx &lt; 0) || (idx &gt;= (int)imnames.size())) 
      return vector&lt;Point2f&gt;(); 
      vector&lt;Point2f&gt; p = points[idx]; 
      if(flipped){ 
        Mat im = this-&gt;get_image(idx,0); int n = p.size(); 
        vector&lt;Point2f&gt; q(n); 
        for(int i = 0; i &lt; n; i++){       
          q[i].x = im.cols-1-p[symmetry[i]].x; 
          q[i].y = p[symmetry[i]].y; 
        } return q; 
      } else return p; 
    }
</pre>
<p>Note that when the mirroring flag is specified, this function calls the <kbd>get_image</kbd> function. This is required to determine the width of the image in order to correctly mirror the facial feature coordinates. A more efficient method could be devised by simply passing the image width as a variable. Finally, the utility of the <kbd>symmetry</kbd> member variable is illustrated in this function. The mirrored feature location of a particular index is simply the feature location at the index specified in the <kbd>symmetry</kbd> variable with its <em>x</em> coordinate flipped and biased.</p>
<p>Both the <kbd>get_image</kbd> and <kbd>get_points</kbd> functions return empty structures if the specified index is outside the one that exists for the dataset. It is also possible that not all images in the collection are annotated. Face-tracking algorithms can be designed to handle missing data; however, these implementations are often quite involved and are outside the scope of this chapter. The <kbd>ft_data</kbd> class implements a function for removing samples from its collection that do not have corresponding annotations, as follows:</p>
<pre>
    void ft_data::rm_incomplete_samples(){ 
      int n = points[0].size(),N = points.size(); 
      for(int i = 1; i &lt; N; i++)n = max(n,int(points[i].size())); 
      for(int i = 0; i &lt; int(points.size()); i++){ 
        if(int(points[i].size()) != n){ 
          points.erase(points.begin()+i); 
          imnames.erase(imnames.begin()+i); i--; 
        } else { 
          int j = 0; 
          for(; j &lt; n; j++) { 
            if((points[i][j].x &lt;= 0) || 
            (points[i][j].y &lt;= 0))break; 
          } 
          if(j &lt; n) { 
            points.erase(points.begin()+i); 
            imnames.erase(imnames.begin()+i); i--; 
          } 
        } 
      } 
    }
</pre>
<p>The sample instance that has the most number of annotations is assumed to be the canonical sample. All data instances that have a point set with less than that number of points are removed from the collection using the vector's <kbd>erase</kbd> function. Also notice that points with (<em>x, y</em>) coordinates less than 1 are considered missing in their corresponding image (possibly due to occlusion, poor visibility, or ambiguity).</p>
<p>The <kbd>ft_data</kbd> class implements the serialization functions <kbd>read</kbd> and <kbd>write</kbd>, and can thus be stored and loaded easily. For example, saving a dataset can be done as simply as:</p>
<pre>
    ft_data D;                          //instantiate data structure 
    ...                                 //populate data 
    save_ft&lt;ft_data&gt;("mydata.xml",D);   //save data
</pre>
<p>For visualizing the dataset, <kbd>ft_data</kbd> implements a number of drawing functions. Their use is illustrated in the <kbd>visualize_annotations.cpp</kbd> file. This simple program loads annotation data stored in the file specified in the command-line, removes the incomplete samples, and displays the training images with their corresponding annotations, symmetry, and connections superimposed. A few notable features of OpenCV's <kbd>highgui</kbd> module are demonstrated here. Although quite rudimentary and not well suited for complex user interfaces, the functionality in OpenCV's <kbd>highgui</kbd> module is extremely useful for loading and visualizing data and algorithmic outputs in Computer Vision applications. This is perhaps one of OpenCV's distinguishing qualities compared to other Computer Vision libraries.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Annotation tool</h1>
            </header>

            <article>
                
<p>To aid in generating annotations for use with the code in this chapter, a rudimentary annotation tool can be found in the <kbd>annotate.cpp</kbd> file. The tool takes as input a video stream, either from a file or from the camera. The procedure for using the tool is listed in the following four steps:</p>
<ol>
<li><strong>Capture images</strong>: In this first step, the image stream is displayed on the screen and the user chooses the images to annotate by pressing the <kbd><span class="KeyPACKT">S</span></kbd> key. The best set of features to annotate are those that maximally span the range of facial behaviors that the face-tracking system will be required to track.</li>
<li><strong>Annotate first image</strong>: In this second step, the user is presented with the first image selected in the previous stage. The user then proceeds to click on the image at the locations pertaining to the facial features that require tracking.</li>
<li><strong>Annotate connectivity</strong>: In this third step, to better visualize a shape, the connectivity structure of points needs to be defined. Here, the user is presented with the same image as in the previous stage, where the task now is to click a set of point pairs, one after the other, to build the connectivity structure for the face model.</li>
<li><strong>Annotate symmetry</strong>: In this step, still with the same image, the user selects pairs of points that exhibit bilateral symmetry.</li>
<li><strong>Annotate remaining images</strong>: In this final step, the procedure here is similar to that of <em>step 2</em>, except that the user can browse through the set of images and annotate them asynchronously.</li>
</ol>
<p>An interested reader may want to improve on this tool by improving its usability or may even integrate an incremental learning procedure, whereby a tracking model is updated after each additional image is annotated and is subsequently used to initialize the points to reduce the burden of annotation.</p>
<p>Although some publicly available datasets are available for use with the code developed in this chapter (see for example, the description in the following section), the annotation tool can be used to build person-specific face-tracking models, which often perform far better than their generic, person-independent, counterparts.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Pre-annotated data (the MUCT dataset)</h1>
            </header>

            <article>
                
<p>One of the hindering factors of developing face-tracking systems is the tedious and error-prone process of manually annotating a large collection of images, each with a large number of points. To ease this process for the purpose of following the work in this chapter, the publicly available MUCT dataset can be downloaded from<span class="URLPACKT"> <a href="http://www/milbo.org/muct" target="_blank">h t t p ://w w w /m i l b o . o r g /m u c t </a></span>.</p>
<p>The dataset consists of 3,755 face images annotated with 76 point landmarks. The subjects in the dataset vary in age and ethnicity and are captured under a number of different lighting conditions and head poses.</p>
<p>To use the MUCT dataset with the code in this chapter, perform the following steps:</p>
<ol>
<li><strong>Download the image set</strong>: In this step, all the images in the dataset can be obtained by downloading the files <kbd>muct-a-jpg-v1.tar.gz</kbd> to <kbd>muct-e-jpg-v1.tar.gz</kbd> and uncompressing them. This will generate a new folder in which all the images will be stored.</li>
<li><strong>Download the annotations</strong>: In this step, download the file containing the annotations <kbd>muct-landmarks-v1.tar.gz</kbd>. Save and uncompress this file in the same folder as the one in which the images were downloaded.</li>
<li><strong>Define connections and symmetry using the annotation tool</strong>: In this step, from the command-line, issue the command <kbd>./annotate -m $mdir -d $odir</kbd>, where <kbd>$mdir</kbd> denotes the folder where the MUCT dataset was saved and <kbd>$odir</kbd> denotes the folder to which the <kbd>annotations.yaml</kbd> file, containing the data stored as an <kbd>ft_data</kbd> object, will be written.</li>
</ol>
<div class="packt_tip">Usage of the MUCT dataset is encouraged to get a quick introduction to the functionality of the face-tracking code described in this chapter.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Geometrical constraints</h1>
            </header>

            <article>
                
<p>In face tracking, geometry refers to the spatial configuration of a predefined set of points that correspond to physically consistent locations on the human face (such as eye corners, nose tips, and eyebrow edges). A particular choice of these points is application dependent, with some applications requiring a dense set of over 100 points and others requiring only a sparser selection. However, the robustness of face-tracking algorithms generally improves with an increased number of points, as their separate measurements can reinforce each other through their relative spatial dependencies. For example, the location of an eye corner is a good indication of where to expect the nose to be located. However, there are limits to improvements in robustness gained by increasing the number of points, where performance typically plateaus after around 100 points. Furthermore, increasing the point set used to describe a face carries with it a linear increase in computational complexity. Thus, applications with strict constraints on computational load may fare better with fewer points.</p>
<p>It is also the case that faster tracking often leads to more accurate tracking in the online setting. This is because, when frames are dropped, the perceived motion between frames increases, and the optimization algorithm used to find the configuration of the face in each frame has to search a larger space of possible configurations of feature points; a process that often fails when displacement between frames becomes too large. In summary, although there are general guidelines on how to best design the selection of facial feature points, to get an optimal performance, this selection should be specialized to the application's domain.</p>
<p>Facial geometry is often parameterized as a composition of two elements: a <strong>global transformation</strong> (rigid) and a <strong>local deformation</strong> (non-rigid). The global transformation accounts for the overall placement of the face in the image, which is often allowed to vary without constraint (that is, the face can appear anywhere in the image). This includes the (<em>x, y</em>) location of the face in the image, the in-plane head rotation, and the size of the face in the image. Local deformations, on the other hand, account for differences between facial shapes across identities and between expressions. In contrast to the global transformation, these local deformations are often far more constrained largely due to the highly structured configuration of facial features. Global transformations are generic functions of 2D coordinates, applicable to any type of object, whereas local deformations are object specific and must be learned from a training dataset.</p>
<p>In this section, we will describe the construction of a geometrical model of a facial structure, hereby referred to as the shape model. Depending on the application, it can capture expression variations of a single individual, differences between facial shapes across a population, or a combination of both. This model is implemented in the <kbd>shape_model</kbd> class which can be found in the <kbd>shape_model.hpp</kbd> and <kbd>shape_model.cpp</kbd> files. The following code snippet is a part of the header of the <kbd>shape_model</kbd> class that highlights its primary functionality:</p>
<pre>
    class shape_model { //2d linear shape model 
      public: 
      Mat p; //parameter vector (kx1) CV_32F 
      Mat V; //linear subspace (2nxk) CV_32F 
      Mat e; //parameter variance (kx1) CV_32F 
      Mat C; //connectivity (cx2) CV_32S 
      ... 
      void calc_params( 
      const vector&lt;Point2f&gt;&amp;pts,  //points to compute parameters 
      const Mat &amp;weight = Mat(),    //weight/point (nx1) CV_32F 
      const float c_factor = 3.0); //clamping factor 
      ... 
      vector&lt;Point2f&gt;              //shape described by parameters 
      calc_shape(); 
      ... 
      void train( 
      const vector&lt;vector&lt;Point2f&gt;&gt;&amp;p, //N-example shapes 
      const vector&lt;Vec2i&gt;&amp;con = vector&lt;Vec2i&gt;(),//connectivity 
      const float frac = 0.95, //fraction of variation to retain 
      const int kmax = 10);   //maximum number of modes to retain 
      ... 
    }
</pre>
<p>The model that represents variations in face shapes is encoded in the subspace matrix <kbd>V</kbd> and variance vector <kbd>e</kbd>. The parameter vector <kbd>p</kbd> stores the encoding of a shape with respect to the model. The connectivity matrix <kbd>C</kbd> is also stored in this class as it pertains only to visualizing instances of the face's shape. The three functions of primary interest in this class are <kbd>calc_params</kbd>, <kbd>calc_shape</kbd>, and <kbd>train</kbd>. The <kbd>calc_params</kbd> function projects a set of points onto the space of plausible face shapes. It optionally provides separate confidence weights for each of the points to be projected. The <kbd>calc_shape</kbd> function generates a set of points by decoding the parameter vector <kbd>p</kbd> using the face model (encoded by <kbd>V</kbd> and <kbd>e</kbd>). The <kbd>train</kbd> function learns the encoding model from a dataset of face shapes, each of which consists of the same number of points. The parameters <kbd>frac</kbd> and <kbd>kmax</kbd> are parameters of the training procedure that can be specialized for the data at hand.</p>
<p>The functionality of this class will be elaborated in the sections that follow, where we begin by describing <strong>Procrustes analysis</strong>, a method for rigidly registering a point set, followed by the linear model used to represent local deformations. The programs in the <kbd>train_shape_model.cpp</kbd> and <kbd>visualize_shape_model.cpp</kbd> files train and visualize the shape model respectively. Their usage will be outlined at the end of this section.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Procrustes analysis</h1>
            </header>

            <article>
                
<p>In order to build a deformation model of face shapes, we must first process the raw annotated data to remove components pertaining to global rigid motion. When modeling geometry in 2D, a rigid motion is often represented as a similarity transform; this includes the scale, in-plane rotation, and translation. The following image illustrates the set of permissible motion types under a similarity transform. The process of removing global rigid motion from a collection of points is called <strong>Procrustes analysis</strong>.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="200" width="594" class="image-border" src="assets/image_05_003.jpg"/></div>
<p>Mathematically, the objective of Procrustes analysis is to simultaneously find a canonical shape and similarity, and transform each data instance that brings them into alignment with the canonical shape. Here, alignment is measured as the least-squares distance between each transformed shape with the canonical shape. An iterative procedure for fulfilling this objective is implemented in the <kbd>shape_model</kbd> class as follows:</p>
<pre>
    #define fl at&lt;float&gt; 
    Mat shape_model::procrustes ( 
    const Mat &amp;X,       //interleaved raw shape data as columns 
    const int itol,     //maximum number of iterations to try 
    const float ftol)   //convergence tolerance 
    { 
      int N = X.cols,n = X.rows/2; Mat Co,P = X.clone();//copy 
      for(int i = 0; i &lt; N; i++){ 
        Mat p = P.col(i);            //i'th shape 
        float mx = 0,my = 0;         //compute centre of mass... 
        for(int j = 0; j &lt; n; j++) { //for x and y separately 
          mx += p.fl(2*j); my += p.fl(2*j+1); 
        } 
        mx /= n; my /= n; 
        for(int j = 0; j &lt; n; j++) {  //remove center of mass 
          p.fl(2*j) -= mx; p.fl(2*j+1) -= my; 
        } 
      } 
      for(int iter = 0; iter &lt; itol; iter++) {     
        Mat C = P*Mat::ones(N,1,CV_32F)/N; //compute normalized... 
        normalize(C,C);                    //canonical shape 
        if(iter &gt; 0) { if(norm(C,Co) &lt; ftol) break; } //converged? 
        Co = C.clone();                               //remember current estimate 
        for(int i = 0; i &lt; N; i++){ 
          Mat R = this-&gt;rot_scale_align(P.col(i),C); 
          for(int j = 0; j &lt; n; j++) { //apply similarity transform 
            float x = P.fl(2*j,i), y = P.fl(2*j+1,i); 
            P.fl(2*j  ,i) = R.fl(0,0)*x + R.fl(0,1)*y; 
            P.fl(2*j+1,i) = R.fl(1,0)*x + R.fl(1,1)*y; 
          } 
        } 
      } return P; //returned procrustes aligned shapes 
    }
</pre>
<p>The algorithm begins by subtracting the center of mass of each shape's instance followed by an iterative procedure that alternates between computing the canonical shape, as the normalized average of all shapes, and rotating and scaling each shape to best match the canonical shape. The normalization step of the estimated canonical shape is necessary to fix the scale of the problem and prevent it from shrinking all the shapes to zero. The choice of this anchor scale is arbitrary; here, we have chosen to enforce the length of the canonical shape vector <kbd>C</kbd> to 1.0, as is the default behavior of OpenCV's <kbd>normalize</kbd> function. Computing the in-plane rotation and scaling that best aligns each shape's instance to the current estimate of the canonical shape is effected through the <kbd>rot_scale_align</kbd> function as follows:</p>
<pre>
    Mat shape_model::rot_scale_align( 
      const Mat &amp;src, //[x1;y1;...;xn;yn] vector of source shape 
      const Mat &amp;dst) //destination shape 
      { 
        //construct linear system 
        int n = src.rows/2; 
        float a=0, b=0, d=0; 
        for(int i = 0; i &lt; n; i++) { 
          d+= src.fl(2*i)*src.fl(2*i  )+src.fl(2*i+1)*src.fl(2*i+1); 
          a+= src.fl(2*i)*dst.fl(2*i  )+src.fl(2*i+1)*dst.fl(2*i+1); 
          b+= src.fl(2*i)*dst.fl(2*i+1)-src.fl(2*i+1)*dst.fl(2*i  ); 
        } 
        a /= d; b /= d;//solve linear system 
        return (Mat_&lt;float&gt;(2,2) &lt;&lt; a,-b,b,a); 
      }
</pre>
<p>This function minimizes the following least squares difference between the rotated and canonical shapes. Mathematically this can be written as:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/a7829_09_04-1.png"/></div>
<p>Here the solution to the least-squares problem takes on the closed-form solution shown in the following image on the right-hand side of the equation. Note that rather than solving for the scaling and in-plane rotation, which are nonlinearly related in the scaled 2D rotation matrix, we solve for the variables (<kbd>a</kbd>, <kbd>b</kbd>). These variables are related to the scale and rotation matrix as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="50" width="291" class="image-border" src="assets/a7829_09_05.png"/></div>
<p>A visualization of the effects of Procrustes analysis on raw annotated shape data is illustrated in the following image. Each facial feature is displayed with a unique color. After translation normalization, the structure of the face becomes apparent, where the locations of facial features cluster around their average locations. After the iterative scale and rotation normalization procedure, the feature clustering becomes more compact and their distribution becomes more representative of the variation induced by facial deformation. This last point is important as it is these deformations that we will attempt to model in the following section. Thus, the role of Procrustes analysis can be thought of as a preprocessing operation on the raw data that will allow better local deformation models of the face to be learned:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_05_004.jpg"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Linear shape models</h1>
            </header>

            <article>
                
<p>The aim of facial-deformation modeling is to find a compact parametric representation of how the face's shape varies across identities and between expressions. There are many ways of achieving this goal with various levels of complexity. The simplest of these is to use a linear representation of facial geometry. Despite its simplicity, it has been shown to accurately capture the space of facial deformations, particularly when the faces in the dataset are largely in a frontal pose. It also has the advantage that inferring the parameters of its representation is an extremely simple and cheap operation, in contrast to its nonlinear counterparts. This plays an important role when deploying it to constrain the search procedure during tracking.</p>
<p>The main idea of linearly modeling facial shapes is illustrated in the following image. Here, a face shape, which consists of <em>N</em> facial features, is modeled as a single point in a 2<em>N</em>-dimensional space. The aim of linear modeling is to find a low-dimensional hyperplane embedded within this 2<em>N</em>-dimensional space in which all the face shape points lie (that is, the green points in the image). As this hyperplane spans only a subset of the entire 2<em>N</em>-dimensional space, it is often referred to as the subspace. The lower the dimensionality of the subspace, the more compact the representation of the face is and the stronger the constraint that it places on the tracking procedure becomes. This often leads to more robust tracking. However, care should be taken in selecting the subspace's dimension so that it has enough capacity to span the space of all faces, but not so much that non-face shapes lie within its span (that is, the red points in the image). It should be noted that when modeling data from a single person, the subspace that captures the face's variability is often far more compact than the one that models multiple identities. This is one of the reasons why person-specific trackers perform much better than generic ones.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="306" width="411" class="image-border" src="assets/image_05_005.jpg"/></div>
<p>The procedure for finding the best low-dimensional subspace that spans a dataset is called <strong>Principal Component Analysis</strong> (<strong>PCA</strong>). OpenCV implements a class for computing PCA; however, it requires the number of preserved subspace dimensions to be prespecified. As this is often difficult to determine a priori, a common heuristic is to choose it based on the fraction of the total amount of variation it accounts for. In the <kbd>shape_model::train</kbd> function, PCA is implemented as follows:</p>
<pre>
    SVD svd(dY*dY.t()); 
    int m = min(min(kmax,N-1),n-1); 
    float vsum = 0; for(int i = 0; i &lt; m; i++)vsum += svd.w.fl(i); 
    float v = 0; int k = 0; 
    for(k = 0; k &lt; m; k++){ 
      v += svd.w.fl(k); if(v/vsum &gt;= frac){k++; break;} 
    } 
    if(k &gt; m)k = m; 
    Mat D = svd.u(Rect(0,0,k,2*n));
</pre>
<p>Here, each column of the <kbd>dY</kbd> variable denotes the mean-subtracted Procrustes-aligned shape. Thus, <strong>Singular Value Decomposition</strong> (<strong>SVD</strong>) is effectively applied to the covariance matrix of the shape data (that is, <kbd>dY.t()*dY</kbd>). The <kbd>w</kbd> member of OpenCV's <kbd>SVD</kbd> class stores the variance in the major directions of variability of the data, ordered from largest to smallest. A common approach to choose the dimensionality of the subspace is to choose the smallest set of directions that preserve a fraction <kbd>frac</kbd> of the total energy of the data, which is represented by the entries of <kbd>svd.w</kbd>. As these entries are ordered from largest to smallest, it suffices to enumerate the subspace selection by greedily evaluating the energy in the top <kbd>k</kbd> directions of variability. The directions themselves are stored in the <kbd>u</kbd> member of the <kbd>SVD</kbd> class. The <kbd>svd.w</kbd> and <kbd>svd.u</kbd> components are generally referred to as the eigen spectrum and eigen vectors respectively. A visualization of these two components is shown in the following figure:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_05_006.jpg"/></div>
<div class="packt_infobox">Note that the eigen spectrum decreases rapidly, which suggests that most of the variation contained in the data can be modeled with a low-dimensional subspace.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">A combined local-global representation</h1>
            </header>

            <article>
                
<p>A shape in the image frame is generated by the composition of a local deformation and a global transformation. Mathematically, this parameterization can be problematic, as the composition of these transformations results in a nonlinear function that does not admit a closed-form solution. A common way to circumvent this problem is to model the global transformation as a linear subspace and append it to the deformation subspace. For a fixed shape, a similarity transform can be modeled with a subspace as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="98" width="339" class="image-border" src="assets/a7829_09_09.png"/></div>
<p>In the <kbd>shape_model</kbd> class, this subspace is generated using the <kbd>calc_rigid_basis</kbd> function. The shape from which the subspace is generated (that is, the <kbd>x</kbd> and <kbd>y</kbd> components in the preceding equation) is the mean shape over the Procustes-aligned shape (that is, the canonical shape). In addition to constructing the subspace in the aforementioned form, each column of the matrix is normalized to unit length. In the <kbd>shape_model::train</kbd> function, the variable <kbd>dY</kbd> described in the previous section is computed by projecting out the components of the data that pertain to rigid motion, as follows:  </p>
<pre>
    Mat R = this-&gt;calc_rigid_basis(Y); //compute rigid subspace 
    Mat P = R.t()*Y; Mat dY = Y - R*P; //project-out rigidity
</pre>
<p>Note that this projection is implemented as a simple matrix multiplication. This is possible because the columns of the rigid subspace have been length normalized. This does not change the space spanned by the model, and means only that <kbd>R.t()*R</kbd> equals the identity matrix.</p>
<p>As the directions of variability stemming from rigid transformations have been removed from the data before learning the deformation model, the resulting deformation subspace will be orthogonal to the rigid transformation subspace. Thus, concatenating the two subspaces results in a combined local-global linear representation of facial shapes that is also orthonormal. Concatenation here can be performed by assigning the two subspace matrices to submatrices of the combined subspace matrix through the ROI extraction mechanism implemented in OpenCV's <kbd>Mat</kbd> class as follows:</p>
<pre>
    V.create(2*n,4+k,CV_32F);                  //combined subspace 
    Mat Vr = V(Rect(0,0,4,2*n)); R.copyTo(Vr); //rigid subspace  
    Mat Vd = V(Rect(4,0,k,2*n)); D.copyTo(Vd); //nonrigid subspace
</pre>
<p>The orthonormality of the resulting model means that the parameters describing a shape can be computed easily, as is done in the <kbd>shape_model::calc_params</kbd> function:</p>
<pre>
    p = V.t()*s;
</pre>
<p>Here <kbd>s</kbd> is a vectorized face shape and <kbd>p</kbd> stores the coordinates in the face subspace that represents it.</p>
<p>A final point to note about linearly modeling facial shapes is how to constrain the subspace coordinates such that shapes generated using it remain valid. In the following image, instances of face shapes that lie within the subspace are shown for an increasing value of the coordinates in one of the directions of variability in increments of four standard deviations. Notice that for small values, the resulting shape remains face-like, but deteriorates as the values become too large.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_05_007.jpg"/></div>
<p>A simple way to prevent such deformation is to clamp the subspace coordinate values to lie within a permissible region as determined from the dataset. A common choice for this is a box constraint within &amp;pm;3 standard deviations of the data, which accounts for 99.7 percent of variation in the data. These clamping values are computed in the <kbd>shape_model::train</kbd> function after the subspace is found, as follows:</p>
<pre>
    Mat Q = V.t()*X;               //project raw data onto subspace 
    for(int i = 0; i &lt; N; i++) {   //normalize coordinates w.r.t scale 
      float v = Q.fl(0,i); Mat q = Q.col(i); q /= v; 
    } 
    e.create(4+k,1,CV_32F); multiply(Q,Q,Q); 
    for(int i = 0; i &lt; 4+k; i++) { 
      if(i &lt; 4)  e.fl(i) = -1;     //no clamping for rigid coefficients 
      else       e.fl(i) = Q.row(i).dot(Mat::ones(1,N,CV_32F))/(N-1); 
    }
</pre>
<p>Notice that the variance is computed over the subspace coordinate <kbd>Q</kbd> after normalizing with respect to the coordinate of the first dimension (that is, scale). This prevents data samples that have relatively large scale from dominating the estimate. Also, notice that a negative value is assigned to the variance of the coordinates of the rigid subspace (that is, the first four columns of <kbd>V</kbd>). The clamping function <kbd>shape_model::clamp</kbd> checks to see if the variance of a particular direction is negative and only applies clamping if it is not, as follows:</p>
<pre>
    void shape_model::clamp(const float c) { 
      //clamping as fraction of standard deviation 
      double scale = p.fl(0);        //extract scale 
      for(int i = 0; i &lt; e.rows; i++) { 
        if(e.fl(i) &lt; 0)continue;       //ignore rigid components 
        float v = c*sqrt(e.fl(i));     //c*standard deviations box 
        if(fabs(p.fl(i)/scale) &gt; v) {  //preserve sign of coordinate 
          if(p.fl(i) &gt; 0) p.fl(i) =  v*scale; //positive threshold 
          else            p.fl(i) = -v*scale; //negative threshold 
        } 
      } 
    }
</pre>
<p>The reason for this is that the training data is often captured under contrived settings where the face is upright and centered in the image at a particular scale. Clamping the rigid components of the shape model to adhere to the configurations in the training set would then be too restrictive. Finally, as the variance of each deformable coordinate is computed in the scale-normalized frame, the same scaling must be applied to the coordinates during clamping.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Training and visualization</h1>
            </header>

            <article>
                
<p>An example program for training a shape model from the annotation data can be found in <kbd>train_shape_model.cpp</kbd>. With the command-line argument <kbd>argv[1]</kbd> containing the path to the annotation data, training begins by loading the data into memory and removing incomplete samples, as follows:</p>
<pre>
    ft_data data = load_ft&lt;ft_data&gt;(argv[1]); 
    data.rm_incomplete_samples();
</pre>
<p>The annotations for each example, and optionally their mirrored counterparts, are then stored in a vector before passing them to the training function as follows:</p>
<pre>
    vector&lt;vector&lt;Point2f&gt;&gt; points; 
    for(int i = 0; i &lt; int(data.points.size()); i++) { 
      points.push_back(data.get_points(i,false)); 
      if(mirror)points.push_back(data.get_points(i,true)); 
    }
</pre>
<p>The shape model is then trained by a single function call to <kbd>shape_model::train</kbd> as follows:</p>
<pre>
    shape_model smodel;       
    smodel.train(points,data.connections,frac,kmax);
</pre>
<p>Here, <kbd>frac</kbd> (that is, the fraction of variation to retain) and <kbd>kmax</kbd> (that is, the maximum number of eigen vectors to retain) can be optionally set through command-line options, although the default settings of 0.95 and 20, respectively, tend to work well in most cases. Finally, with the command-line argument <kbd>argv[2]</kbd> containing the path to save the trained shape model to, saving can be performed by a single function call as follows:</p>
<pre>
   save_ft(argv[2],smodel);
</pre>
<p>The simplicity of this step results from defining the <kbd>read</kbd> and <kbd>write</kbd> serialization functions for the <kbd>shape_model</kbd> class.</p>
<p>To visualize the trained shape model, the <kbd>visualize_shape_model.cpp</kbd> program animates the learned non-rigid deformations of each direction in turn. It begins by loading the shape model into memory as follows:</p>
<pre>
    shape_model smodel = load_ft&lt;shape_model&gt;(argv[1]);
</pre>
<p>The rigid parameters that place the model at the center of the display window are computed as follows:</p>
<pre>
    int n = smodel.V.rows/2; 
    float scale = calc_scale(smodel.V.col(0),200); 
    float tranx = 
      n*150.0/smodel.V.col(2).dot(Mat::ones(2*n,1,CV_32F)); 
    float trany = 
      n*150.0/smodel.V.col(3).dot(Mat::ones(2*n,1,CV_32F));
</pre>
<p>Here, the <kbd>calc_scale</kbd> function finds the scaling coefficient that would generate face shapes with a width of 200 pixels. The translation components are computed by finding the coefficients that generate a translation of 150 pixels (that is, the model is mean-centered and the display window is 300x300 pixels in size).</p>
<div class="packt_infobox">Note that the first column of <kbd>shape_model::V</kbd> corresponds to scale and the third and fourth columns to <kbd>x</kbd> and <kbd>y</kbd> translations respectively.</div>
<p>A trajectory of parameter values is then generated, which begins at zero, moves to the positive extreme, moves to the negative extreme, and then back to zero, as follows:</p>
<pre>
    vector&lt;float&gt; val; 
    for(int i = 0; i &lt; 50; i++)val.push_back(float(i)/50); 
    for(int i = 0; i &lt; 50; i++)val.push_back(float(50-i)/50); 
    for(int i = 0; i &lt; 50; i++)val.push_back(-float(i)/50); 
    for(int i = 0; i &lt; 50; i++)val.push_back(-float(50-i)/50);
</pre>
<p>Here, each phase of the animation is composed of 50 increments. This trajectory<br/>
is then used to animate the face model and render the results in a display window<br/>
as follows:</p>
<pre>
    Mat img(300,300,CV_8UC3); namedWindow("shape model"); 
    while(1) { 
      for(int k = 4; k &lt; smodel.V.cols; k++){ 
        for(int j = 0; j &lt; int(val.size()); j++){ 
          Mat p = Mat::zeros(smodel.V.cols,1,CV_32F); 
          p.at&lt;float&gt;(0) = scale; 
          p.at&lt;float&gt;(2) = tranx; 
          p.at&lt;float&gt;(3) = trany; 
          p.at&lt;float&gt;(k) = scale*val[j]*3.0* 
          sqrt(smodel.e.at&lt;float&gt;(k));   
          p.copyTo(smodel.p); img = Scalar::all(255); 
          vector&lt;Point2f&gt; q = smodel.calc_shape(); 
          draw_shape(img,q,smodel.C); 
          imshow("shape model",img); 
          if(waitKey(10) == 'q')return 0; 
        } 
      } 
    }
</pre>
<div class="packt_infobox">Note that the rigid coefficients (that is, those corresponding to the first four columns of <kbd>shape_model::V</kbd>) are always set to the values computed previously, to place the face at the center of the display window.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Facial feature detectors</h1>
            </header>

            <article>
                
<p>Detecting facial features in images bares a strong resemblance to general object detection. OpenCV has a set of sophisticated functions for building general object detectors, the most well-known of which is the cascade of Haar-based feature detectors used in their implementation of the well-known <strong>Viola-Jones</strong> <strong>face detector</strong>. There are, however, a few distinguishing factors that make facial feature detection unique. These are as follows:</p>
<ul>
<li><strong>Precision versus robustness</strong>: In generic object detection, the aim is to find the coarse position of the object in the image; facial feature detectors are required to give highly precise estimates of the location of the feature. An error of a few pixels is considered inconsequential in object detection but it can mean the difference between a smile and a frown in facial expression estimation through feature detections.</li>
<li><strong>Ambiguity from limited spatial support</strong>: It is common to assume that the object of interest in generic object detection exhibits sufficient image structure such that it can be reliably discriminated from image regions that do not contain the object. This is often not the case for facial features, which typically have limited spatial support. This is because image regions that do not contain the object can often exhibit a very similar structure to facial features. For example, a feature on the periphery of the face, seen from a small bounding box centered at the feature, can be easily confused with any other image patch that contains a strong edge through its center.</li>
<li><strong>Computational complexity</strong>: Generic object detection aims to find all instances of the object in an image. Face tracking, on the other hand, requires the locations of all facial features, which often ranges from around 20 to 100 features. Thus, the ability to evaluate each feature detector efficiently is paramount in building a face tracker that can run in real time.</li>
</ul>
<p>Due to these differences, the facial feature detectors used in face tracking are often specifically designed with that purpose in mind. There are, of course, many instances of generic object-detection techniques being applied to facial feature detectors in face tracking. However, there does not appear to be a consensus in the community about which representation is best suited for the problem.</p>
<p>In this section, we will build facial feature detectors using a representation that is perhaps the simplest model one would consider: a linear image patch. Despite its simplicity, with due care in designing its learning procedure, we will see that this representation can in fact give reasonable estimates of facial feature locations for use in a face-tracking algorithm. Furthermore, their simplicity enables an extremely rapid evaluation that makes real-time face tracking possible. Due to their representation as an image patch, the facial feature detectors are hereby referred to as patch models. This model is implemented in the <kbd>patch_model</kbd> class that can be found in the<br/>
<kbd>patch_model.hpp</kbd> and <kbd>patch_model.cpp</kbd> files. The following code snippet is<br/>
of the header of the <kbd>patch_model</kbd> class that highlights its primary functionality:</p>
<pre>
    class patch_model{ 
      public: 
      Mat P; //normalized patch 
      ... 
      Mat                          //response map 
      calc_response( 
      const Mat &amp;im,               //image patch of search region 
      const bool sum2one = false); //normalize to sum-to-one? 
      ... 
      void train(const vector&lt;Mat&gt;&amp;images, //training image patches 
      const Size psize,                    //patch size 
      const float var = 1.0,               //ideal response variance 
      const float lambda = 1e-6,           //regularization weight 
      const float mu_init = 1e-3,          //initial step size 
      const int nsamples = 1000,           //number of samples 
      const bool visi = false);            //visualize process? 
      ... 
    };
</pre>
<p>The patch model used to detect a facial feature is stored in the matrix <kbd>P</kbd>. The two functions of primary interest in this class are <kbd>calc_response</kbd> and <kbd>train</kbd>. The <kbd>calc_response</kbd> function evaluates the patch model's response at every integer displacement over the search region <kbd>im</kbd>. The <kbd>train</kbd> function learns the patch model <kbd>P</kbd> of size <kbd>psize</kbd> that, on an average, yields response maps over the training set that is as close as possible to the ideal response map. The parameters <kbd>var</kbd>, <kbd>lambda</kbd>, <kbd>mu_init</kbd>, and <kbd>nsamples</kbd> are parameters of the training procedure that can be tuned to optimize performance for the data at hand.</p>
<p>The functionality of this class will be elaborated in this section. We begin by discussing the correlation patch and its training procedure, which will be used to learn the patch model. Next, the <kbd>patch_models</kbd> class, which is a collection of the patch models for each facial feature and has functionality that accounts for global transformations will be described. The programs in <kbd>train_patch_model.cpp</kbd> and <kbd>visualize_patch_model.cpp</kbd> train and visualize the patch models, respectively, and their usage will be outlined at the end of this section on facial feature detectors.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Correlation-based patch models</h1>
            </header>

            <article>
                
<p>In learning detectors, there are two primary competing paradigms: generative and discriminative. Generative methods learn an underlying representation of image patches that can best generate the object appearance in all its manifestations. Discriminative methods, on the other hand, learn a representation that best discriminates instances of the object from other objects that the model will likely encounter when deployed. Generative methods have the advantage that the resulting model encodes properties specific to the object, allowing novel instances of the object to be visually inspected. A popular approach that falls within the paradigm of generative methods is the famous <kbd>Eigenfaces</kbd> method. Discriminative methods have the advantage that the full capacity of the model is geared directly towards the problem at hand; discriminating instances of the object from all others. Perhaps the most well-known of all discriminative methods is the support vector machine. Although both paradigms can work well in many situations, we will see that when modeling facial features as an image patch, the discriminative paradigm is far superior.</p>
<div class="packt_infobox">Note that the <kbd>Eigenfaces</kbd> and support vector machine methods were originally developed for classification rather than detection or image alignment. However, their underlying mathematical concepts have been shown to be applicable to the face-tracking domain.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Learning discriminative patch models</h1>
            </header>

            <article>
                
<p>Given an annotated dataset, the feature detectors can be learned independently from each other. The learning objective of a discriminative patch model is to construct an image patch that, when cross-correlated with an image region containing the facial feature, yields a strong response at the fease. Mathematically, this can be expressed as:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="61" width="415" class="aligncenter size-full wp-image-151 image-border" src="assets/a7829_09_11-1.png"/></div>
<p>Here, <strong>P</strong> denotes the patch model, <strong><span class="packt_screen">I</span></strong> denotes the i<sup>th</sup> training image, <strong>I</strong>(<em>a:b, c:d</em>) denotes the rectangular region whose top-left and bottom-right corners are located at <em>(a, c)</em> and <em>(b, d)</em>, respectively. The period symbol denotes the inner product operation and <strong>R</strong> denotes the ideal response map. The solution to this equation is a patch model that generates response maps that are, on average, closest to the ideal response map as measured using the least-squares criterion. An obvious choice for the ideal response map, <strong>R</strong>, is a matrix with zeros everywhere except at the center (assuming the training image patches are centered at the facial feature of interest). In practice, since the images are hand-labeled, there will always be an annotation error. To account for this, it is common to describe R as a decaying function of distance from the center. A good choice is the 2D-Gaussian distribution, which is equivalent to assuming the annotation error is Gaussian distributed. A visualization of this setup is shown in the following figure for the left outer eye corner:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/a7829_09_12.png"/></div>
<p>The learning objective as written previously is in a form commonly referred to as linear least squares. As such, it affords a closed-form solution. However, the degrees of freedom of this problem; that is, the number of ways the variables can vary to solve the problem, is equal to the number of pixels in the patch. Thus, the computational cost and memory requirements of solving for the optimal patch model can be prohibitive, even for a moderately sized patch; for example, a 40x40 patch model has 1,600 degrees of freedom.</p>
<p>An efficient alternative to solving the learning problem as a linear system of equations is a method called stochastic gradient descent. By visualizing the learning objective as an error terrain over the degrees of freedom of the patch model, stochastic gradient descent iteratively makes an approximate estimate of the gradient direction of the terrain and takes a small step in the opposite direction. For our problem, the approximation to gradient can be computed by considering only the gradient of the learning objective for a single, randomly chosen image from the training set:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/a7829_09_13.png"/></div>
<p>In the <kbd>patch_model</kbd> class, this learning process is implemented in the <kbd>train</kbd> function:</p>
<pre>
    void patch_model::train( 
      const vector&lt;Mat&gt;&amp;images, //featured centered training images 
      const Size psize,          //desired patch model size 
      const float var,           //variance of annotation error 
      const float lambda,        //regularization parameter 
      const float mu_init,       //initial step size 
      const int nsamples,        //number of stochastic samples 
      const bool visi) {         //visualise training process 
        int N = images.size(),n = psize.width*psize.height; 
        int dx = wsize.width-psize.width;      //center of response map 
        int dy = wsize.height-psize.height;    //... 
        Mat F(dy,dx,CV_32F);                   //ideal response map 
        for(int y = 0; y &lt; dy; y++) { 
          float vy = (dy-1)/2 - y; 
          for(int x = 0; x &lt; dx; x++) {
            float vx = (dx-1)/2 - x; 
            F.fl(y,x) = exp(-0.5*(vx*vx+vy*vy)/var); //Gaussian 
          } 
        } 
        normalize(F,F,0,1,NORM_MINMAX); //normalize to [0:1] range 

        //allocate memory 
        Mat I(wsize.height,wsize.width,CV_32F); 
        Mat dP(psize.height,psize.width,CV_32F); 
        Mat O = Mat::ones(psize.height,psize.width,CV_32F)/n; 
        P = Mat::zeros(psize.height,psize.width,CV_32F); 

        //optimise using stochastic gradient descent 
        RNG rn(getTickCount()); //random number generator 
        double mu=mu_init,step=pow(1e-8/mu_init,1.0/nsamples); 
        for(int sample = 0; sample &lt; nsamples; sample++){ 
          int i = rn.uniform(0,N); //randomly sample image index 
          I = this-&gt;convert_image(images[i]); dP = 0.0; 
          for(int y = 0; y &lt; dy; y++) { //compute stochastic gradient 
            for(int x = 0; x &lt; dx; x++){ 
              Mat Wi=I(Rect(x,y,psize.width,psize.height)).clone(); 
              Wi -= Wi.dot(O); normalize(Wi,Wi); //normalize 
              dP += (F.fl(y,x) - P.dot(Wi))*Wi; 
            } 
          }     
          P += mu*(dP - lambda*P); //take a small step 
          mu *= step;              //reduce step size 
          ... 
        } return; 
      }
</pre>
<p>The first highlighted code snippet in the preceding code is where the ideal response map is computed. Since the images are centered on the facial feature of interest, the response map is the same for all samples. In the second highlighted code snippet, the decay rate, <kbd>step</kbd>, of the step sizes is determined such that after <kbd>nsamples</kbd> iterations, the step size would have decayed to a value close to zero. The third highlighted code snippet is where the stochastic gradient direction is computed and used to update the patch model. There are two things to note here. First, the images used in training are passed to the <kbd>patch_model::convert_image</kbd> function, which converts the image to a single-channel image (if it is a color image) and applies the natural logarithm to the image pixel intensities:</p>
<pre>
    I += 1.0; log(I,I);
</pre>
<p>A bias value of 1 is added to each pixel before applying the logarithm since the logarithm of zero is undefined. The reason for performing this pre-processing on the training images is because log-scale images are more robust against differences in contrast and changes in illumination conditions. The following figure shows images of two faces with different degrees of contrast in the facial region. The difference between the images is much less pronounced in the log-scale images than it is in the raw images.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_05_009.jpg"/></div>
<p>The second point to note about the update equation is the subtraction of <kbd>lambda*P</kbd> from the update direction. This effectively regularizes the solution from growing too large; a procedure that is often applied in machine-learning algorithms to promote generalization to unseen data. The scaling factor <kbd>lambda</kbd> is user defined and is usually problem dependent. However, a small value typically works well for learning patch models for facial feature detection.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Generative versus discriminative patch models</h1>
            </header>

            <article>
                
<p>Despite the ease of which discriminative patch models can be learned as described previously, it is worth considering whether generative patch models and their corresponding training regimes are simple enough to achieve similar results. The generative counterpart of the correlation patch model is the average patch. The learning objective for this model is to construct a single image patch that is as close as possible to all examples of the facial feature as measured via the least-squares criterion:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="56" width="178" src="assets/a7829_09_15.png"/></div>
<p>The solution to this problem is exactly the average of all the feature-centered training image patches. Thus, in a way, the solution afforded by this objective is far simpler.</p>
<p>In the following figure, a comparison is shown for the response maps obtained by cross-correlating the average and correlation patch models with an example image. The respective average and correlation patch models are also shown, where the range of pixel values is normalized for visualization purposes. Although the two patch model types exhibit some similarities, the response maps they generate differ substantially. While the correlation patch model generates response maps that are highly peaked around the feature location, the response map generated by the average patch model is overly smooth and does not strongly distinguish the feature location from those close by. Inspecting the patch models' appearance, the correlation patch model is mostly gray, which corresponds to zero in the un-normalized pixel range, with strong positive and negative values strategically placed around prominent areas of the facial feature. Thus, it preserves only those components of the training patches, useful for discriminating it from misaligned configuration, which leads to highly peaked responses. In contrast, the average patch model encodes no knowledge of misaligned data. As a result, it is not well suited to the task of facial feature localization, where the task is to discriminate an aligned image patch from locally shifted versions of itself:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_05_010.jpg"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Accounting for global geometric transformations</h1>
            </header>

            <article>
                
<p>So far, we have assumed that the training images are centered at the facial feature and are normalized with respect to global scale and rotation. In practice, the face can appear at any scale and rotation within the image during tracking. Thus, a mechanism must be devised to account for this discrepancy between the training and testing conditions. One approach is to synthetically perturb the training images in scale and rotation within the ranges one expects to encounter during deployment. However, the simplistic form of the detector as a correlation patch model often lacks the capacity to generate useful response maps for that kind of data. On the other hand, the correlation patch model does exhibit a degree of robustness against small perturbations in scale and rotation. Since motion between consecutive frames in a video sequence is relatively small, one can leverage the estimated global transformation of the face in the previous frame to normalize the current image with respect to scale and rotation. All that is needed to enable this procedure is to select a reference frame in which the correlation patch models are learned.</p>
<p>The <kbd>patch_models</kbd> class stores the correlation patch models for each facial feature as well as the reference frame in which they are trained. It is the <kbd>patch_models</kbd> class, rather than the <kbd>patch_model</kbd> class, that the face tracker code interfaces with directly, to obtain the feature detections. The following code snippet of the declaration of this class highlights its primary functionality:</p>
<pre>
    class patch_models { 
      public: 
      Mat reference;      //reference shape [x1;y1;...;xn;yn] 
      vector&lt;patch_model&gt; patches; //patch model/facial feature 
      ... 
      void train(ft_data &amp;data,        //annotated image and shape data 
        const vector&lt;Point2f&gt;&amp;ref,       //reference shape 
        const Size psize,           //desired patch size 
        const Size ssize,           //training search window size 
        const bool mirror = false,  //use mirrored training data 
        const float var = 1.0,      //variance of annotation error 
        const float lambda = 1e-6,  //regularisation weight 
        const float mu_init = 1e-3, //initial step size 
        const int nsamples = 1000,  //number of samples 
        const bool visi = false);   //visualise training procedure? 
        ... 
        vector&lt;Point2f&gt;//location of peak responses/feature in image 
  calc_peaks( 
          const Mat &amp;im,    //image to detect features in 
          const vector&lt;Point2f&gt;&amp;points, //current estimate of shape 
          const Size ssize = Size(21,21)); //search window size 
          ... 
      };
</pre>
<p>The <kbd>reference</kbd> shape is stored as an interleaved set of (<em>x, y</em>) coordinates that are used to normalize the scale and rotation of the training images, and later, during deployment, that of the test images. In the <kbd>patch_models::train</kbd> function, this is done by first computing the similarity transform between the <kbd>reference</kbd> shape and the annotated shape for a given image using the <kbd>patch_models::calc_simil</kbd> function, which solves a similar problem to that in the <kbd>shape_model::procrustes</kbd> function, albeit for a single pair of shapes. Since the rotation and scale is common across all facial features, the image normalization procedure only requires adjusting this similarity transform to account for the centers of each feature in the image and the center of the normalized image patch. In <kbd>patch_models::train</kbd>, this is implemented as follows:</p>
<pre>
    Mat S = this-&gt;calc_simil(pt),A(2,3,CV_32F); 
    A.fl(0,0) = S.fl(0,0); A.fl(0,1) = S.fl(0,1); 
    A.fl(1,0) = S.fl(1,0); A.fl(1,1) = S.fl(1,1); 
    A.fl(0,2) = pt.fl(2*i  ) - (A.fl(0,0)*(wsize.width -1)/2 + 
    A.fl(0,1)*(wsize.height-1)/2); 
    A.fl(1,2) = pt.fl(2*i+1) - (A.fl(1,0)*(wsize.width -1)/2 + 
    A.fl(1,1)*(wsize.height-1)/2); 
    Mat I; warpAffine(im,I,A,wsize,INTER_LINEAR+WARP_INVERSE_MAP);
</pre>
<p>Here, <kbd>wsize</kbd> is the total size of the normalized training image, which is the sum of the patch size and the search region size. As just mentioned, the top-left (2x2) block of the similarity transform from the reference shape to the annotated shape <kbd>pt</kbd>, which corresponds to the scale and rotation component of the transformation, is preserved in the affine transform passed to OpenCV's <kbd>warpAffine</kbd> function. The last column of the affine transform <kbd>A</kbd> is an adjustment that will render the i<sup>th</sup> facial feature location centered in the normalized image after warping (that is, the normalizing translation). Finally, the <kbd>cv::warpAffine</kbd> function has the default setting of warping from the image to the reference frame. Since the similarity transform was computed for transforming the <kbd>reference</kbd> shape to the image-space annotations, the <kbd>pt</kbd>, the <kbd>WARP_INVERSE_MAP</kbd> flag needs to be set to ensure the function applies the warp in the desired direction. Exactly the same procedure is performed in the <kbd>patch_models::calc_peaks</kbd> function, with the additional step that the computed similarity transform between the reference and the current shape in the image-frame is re-used to un-normalize the detected facial features, placing them appropriately in the image:</p>
<pre>
    vector&lt;Point2f&gt; 
    patch_models::calc_peaks(const Mat &amp;im, 
    const vector&lt;Point2f&gt;&amp;points,const Size ssize){ 
    int n = points.size(); assert(n == int(patches.size())); 
    Mat pt = Mat(points).reshape(1,2*n); 
    Mat S = this-&gt;calc_simil(pt); 
    Mat Si = this-&gt;inv_simil(S); 
    vector&lt;Point2f&gt; pts = this-&gt;apply_simil(Si,points); 
    for(int i = 0; i &lt; n; i++){ 
      Size wsize = ssize + patches[i].patch_size(); 
      Mat A(2,3,CV_32F),I;      
      A.fl(0,0) = S.fl(0,0); A.fl(0,1) = S.fl(0,1); 
      A.fl(1,0) = S.fl(1,0); A.fl(1,1) = S.fl(1,1); 
      A.fl(0,2) = pt.fl(2*i  ) - (A.fl(0,0)*(wsize.width -1)/2 + 
      A.fl(0,1)*(wsize.height-1)/2); 
      A.fl(1,2) = pt.fl(2*i+1) - (A.fl(1,0)*(wsize.width -1)/2 + 
      A.fl(1,1)*(wsize.height-1)/2); 
      warpAffine(im,I,A,wsize,INTER_LINEAR+WARP_INVERSE_MAP); 
      Mat R = patches[i].calc_response(I,false); 
      Point maxLoc; minMaxLoc(R,0,0,0,&amp;maxLoc); 
      pts[i] = Point2f(pts[i].x + maxLoc.x - 0.5*ssize.width, 
      pts[i].y + maxLoc.y - 0.5*ssize.height); 
    } return this-&gt;apply_simil(S,pts);
</pre>
<p>In the first highlighted code snippet in the preceding code, both the forward and inverse similarity transforms are computed. The reason why the inverse transform is required here is so that the peaks of the response map for each feature can be adjusted according to the normalized locations of the current shape estimate. This must be performed before reapplying the similarity transform to place the new estimates of the facial feature locations back into the image frame using the<br/>
<kbd>patch_models::apply_simil</kbd> function.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Training and visualization</h1>
            </header>

            <article>
                
<p>An example program for training the patch models from the annotation data can be found in <kbd>train_patch_model.cpp</kbd>. With the command-line argument <kbd>argv[1]</kbd> containing the path to the annotation data, training begins by loading the data into memory and removing incomplete samples:</p>
<pre>
    ft_data data = load_ft&lt;ft_data&gt;(argv[1]); 
    data.rm_incomplete_samples();
</pre>
<p>The simplest choice for the reference shape in the <kbd>patch_models</kbd> class is the average shape of the training set, scaled to a desired size. Assuming that a shape model has previously been trained for this dataset, the reference shape is computed by first loading the shape model stored in <kbd>argv[2]</kbd> as follows:</p>
<pre>
    shape_model smodel = load_ft&lt;shape_model&gt;(argv[2]);
</pre>
<p>This is followed by the computation of the scaled-centered average shape:</p>
<pre>
    smodel.p = Scalar::all(0.0); 
    smodel.p.fl(0) = calc_scale(smodel.V.col(0),width); 
    vector&lt;Point2f&gt; r = smodel.calc_shape();
</pre>
<p>The <kbd>calc_scale</kbd> function computes the scaling factor to transform the average shape (that is, the first column of <kbd>shape_model::V</kbd>) to one with a width of <kbd>width</kbd>. Once the reference shape <kbd>r</kbd> is defined, training the set of patch models can be done with a single function call:</p>
<pre>
    patch_models pmodel;       
    pmodel.train(data,r,Size(psize,psize),Size(ssize,ssize));
</pre>
<p>The optimal choices for the parameters <kbd>width</kbd>, <kbd>psize</kbd>, and <kbd>ssize</kbd> are application dependent; however, the default values of 100, 11, and 11, respectively, give reasonable results in general.</p>
<p>Although the training process is quite simple, it can still take some time to complete. Depending on the number of facial features, the size of the patches, and the number of stochastic samples in the optimization algorithm, the training process can take anywhere from between a few minutes to over an hour. However, since the training of each patch can be performed independently of all others, this process can be sped<br/>
up substantially by parallelizing the training process across multiple processorcores or machines.</p>
<p>Once training has been completed, the program in <kbd>visualize_patch_model.cpp</kbd> can be used to visualize the resulting patch models. As with the <kbd>visualize_shape_model.cpp</kbd> program, the aim here is to visually inspect the results to verify if anything went wrong during the training process. The program generates a composite image of all the patch models, <kbd>patch_model::P</kbd>, each centered at their respective feature location in the reference shape, <kbd>patch_models::reference</kbd>, and displaying a bounding rectangle around the patch whose index is currently active. The <kbd>cv::waitKey</kbd> function is used to get user input for selecting the activee patch index and terminating the program. The following image shows three examples of composite patch images learned for patch models with varying spatial support. Despite using the same training data, modifying the spatial support of the patch model appears to change the structure of the patch models substantially. Visually inspecting the results in this way can lend intuition into how to modify the parameters of the training process, or even the training process itself, in order to optimize results for a particular application:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="153" width="515" class="image-border" src="assets/image_05_011.jpg"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Face detection and initialization</h1>
            </header>

            <article>
                
<p>The method for face tracking described thus far has assumed that the facial features in the image are located within a reasonable proximity to the current estimate. Although this assumption is reasonable during tracking, where face motion between frames is often quite small, we are still faced with the dilemma of how to initialize the model in the first frame of the sequence. An obvious choice for this is to use OpenCV's in-built cascade detector to find the face. However, the placement of the model within the detected bounding box will depend on the selection made for the facial features to track. In keeping with the data-driven paradigm we have followed so far in this chapter, a simple solution is to learn the geometrical relationship between the face detection's bounding box and the facial features.</p>
<p>The <kbd>face_detector</kbd> class implements exactly this solution. A snippet of its declaration that highlights its functionality is given as follows:</p>
<pre>
    class face_detector{ //face detector for initialisation 
      public: 
      string detector_fname; //file containing cascade classifier 
      Vec3f detector_offset; //offset from center of detection 
      Mat reference;         //reference shape 
      CascadeClassifier detector; //face detector 

      vector&lt;Point2f&gt;  //points describing detected face in image 
      detect(const Mat &amp;im,          //image containing face 
        const float scaleFactor = 1.1,//scale increment 
        const int minNeighbours = 2,  //minimum neighborhood size 
      const Size minSize = Size(30,30));//minimum window size 

      void train(ft_data &amp;data,         //training data 
        const string fname,             //cascade detector 
        const Mat &amp;ref,                 //reference shape 
        const bool mirror = false,      //mirror data? 
        const bool visi = false,        //visualize training? 
        const float frac = 0.8,       //fraction of points in detection 
        const float scaleFactor = 1.1,  //scale increment 
        const int minNeighbours = 2,    //minimum neighbourhood size 
      const Size minSize = Size(30,30)); //minimum window size 
      ... 
    };
</pre>
<p>The class has four public member variables: the path to an object of type <kbd>cv::CascadeClassifier</kbd> called <kbd>detector_fname</kbd>, a set of offsets from a detection bounding box to the location and scale of the face in the image <kbd>detector_offset</kbd>, a reference shape to place in the bounding box <kbd>reference</kbd>, and a face detector <kbd>detector</kbd>. The primary function of use to a face-tracking system is <kbd>face_detector::detect</kbd>, which takes an image as the input, along with standard options for the <kbd>cv::CascadeClassifier</kbd> class, and returns a rough estimate of the facial feature locations in the image. Its implementation is as follows:</p>
<pre>
    Mat gray; //convert image to grayscale and histogram equalize 
    if(im.channels() == 1)  gray = im; 
    else                    cvtColor(im,gray,CV_RGB2GRAY); 
    Mat eqIm; equalizeHist(gray,eqIm); 
    vector&lt;Rect&gt; faces; //detect largest face in image 
    detector.detectMultiScale(eqIm,faces,scaleFactor, minNeighbours,0 
      |CV_HAAR_FIND_BIGGEST_OBJECT 
      |CV_HAAR_SCALE_IMAGE,minSize); 
    if(faces.size() &lt; 1) { return vector&lt;Point2f&gt;(); } 

<strong>Rect R = faces[0]; Vec3f scale = detector_offset*R.width; </strong>
<strong>    int n = reference.rows/2; vector&lt;Point2f&gt; p(n); </strong>
<strong>    for(int i = 0; i &lt; n; i++){ //predict face placement </strong>
<strong>      p[i].x = scale[2]*reference.fl(2*i  ) + R.x + 0.5 * R.width  + </strong>
<strong>      scale[0]; </strong>
<strong>      p[i].y = scale[2]*reference.fl(2*i+1) + R.y + 0.5 * R.height + </strong>
<strong>      scale[1]; </strong>
<strong>    } return p;</strong>
</pre>
<p>The face is detected in the image in the usual way, except that the <kbd>CV_HAAR_FIND_BIGGEST_OBJECT</kbd> flag is set so as to enable tracking the most prominent face in the image. The highlighted code is where the reference shape is placed in the image in accordance with the detected face's bounding box. The <kbd>detector_offset</kbd> member variable consists of three components: an (x, y) offset of the center of the face from the center of the detection's bounding box, and the scaling factor that resizes the reference shape to best fit the face in the image. All three components are a linear function of the bounding box's width.</p>
<p>The linear relationship between the bounding box's width and the <kbd>detector_offset</kbd> variable is learned from the annotated dataset in the <kbd>face_detector::train</kbd> function. The learning process is started by loading the training data into memory and assigning the reference shape:</p>
<pre>
detector.load(fname.c_str()); detector_fname = fname; reference = ref.clone();
</pre>
<p>As with the reference shape in the <kbd>patch_models</kbd> class, a convenient choice for the reference shape is the normalized average face shape in the dataset. The <kbd>cv::CascadeClassifier</kbd> is then applied to each image (and optionally its mirrored counterpart) in the dataset and the resulting detection is checked to ensure that enough annotated points lie within the detected bounding box (see the figure towards the end of this section) to prevent learning from misdetections:</p>
<pre>
    if(this-&gt;enough_bounded_points(pt,faces[0],frac)){ 
      Point2f center = this-&gt;center_of_mass(pt); 
      float w = faces[0].width; 
      xoffset.push_back((center.x - 
        (faces[0].x+0.5*faces[0].width ))/w); 
      yoffset.push_back((center.y - 
        (faces[0].y+0.5*faces[0].height))/w); 
      zoffset.push_back(this-&gt;calc_scale(pt)/w); 
    }
</pre>
<p>If more than a fraction of <kbd>frac</kbd> of the annotated points lie within the bounding box, the linear relationship between its width and the offset parameters for that image are added as a new entry in an STL <kbd>vector</kbd> class object. Here, the <kbd>face_detector::center_of_mass</kbd> function computes the center of mass of the annotated point set for that image and the <kbd>face_detector::calc_scale</kbd> function computes the scaling factor for transforming the reference shape to the centered annotated shape. Once all images have been processed, the <kbd>detector_offset</kbd> variable is set to the median over all of the image-specific offsets:</p>
<pre>
    Mat X = Mat(xoffset),Xsort,Y = Mat(yoffset),Ysort,Z =    
      Mat(zoffset),Zsort; 
    cv::sort(X,Xsort,CV_SORT_EVERY_COLUMN|CV_SORT_ASCENDING); 
    int nx = Xsort.rows; 
    cv::sort(Y,Ysort,CV_SORT_EVERY_COLUMN|CV_SORT_ASCENDING); 
    int ny = Ysort.rows; 
    cv::sort(Z,Zsort,CV_SORT_EVERY_COLUMN|CV_SORT_ASCENDING); 
    int nz = Zsort.rows; 
    detector_offset = 
      Vec3f(Xsort.fl(nx/2),Ysort.fl(ny/2),Zsort.fl(nz/2));
</pre>
<p>As with the <em>shape and patch</em> models, the simple program in <kbd>train_face_detector.cpp</kbd> is an example of how a <kbd>face_detector</kbd> object can be built and saved for later use in the tracker. It first loads the annotation data and the shape model, and sets the reference shape as the mean-centered average of the training data (that is, the identity shape of the <kbd>shape_model</kbd> class):</p>
<pre>
    ft_data data = load_ft&lt;ft_data&gt;(argv[2]); 
    shape_model smodel = load_ft&lt;shape_model&gt;(argv[3]); 
    smodel.set_identity_params(); 
    vector&lt;Point2f&gt; r = smodel.calc_shape(); 
    Mat ref = Mat(r).reshape(1,2*r.size());
</pre>
<p>Training and saving the face detector, then, consists of two function calls:</p>
<pre>
    face_detector detector; 
    detector.train(data,argv[1],ref,mirror,true,frac); 
    save_ft&lt;face_detector&gt;(argv[4],detector);
</pre>
<p>To test the performance of the resulting shape-placement procedure, the program in <kbd>visualize_face_detector.cpp</kbd> calls the <kbd>face_detector::detect</kbd> function for each image in the video or camera input stream and draws the results on screen. An example of the results using this approach is shown in the following figure. Although the placed shape does not match the individual in the image, its placement is close enough so that face tracking can proceed using the approach described in the following section:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="231" width="529" src="assets/image_05_012.jpg"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Face tracking</h1>
            </header>

            <article>
                
<p>The problem of face tracking can be posed as that of finding an efficient and robust way to combine the independent detections of various facial features with the geometrical dependencies they exhibit in order to arrive at an accurate estimate of facial feature locations in each image of a sequence. With this in mind, it is perhaps worth considering whether geometrical dependencies are at all necessary. In the following figure, the results of detecting the facial features with and without geometrical constraints are shown. These results clearly highlight the benefit of capturing the spatial inter-dependencies between facial features. The relative performance of these two approaches is typical, whereby relying strictly on the detections leads to overly noisy solutions. The reason for this is that the response maps for each facial feature cannot be expected to always peak at the correct location. Whether due to image noise, lighting changes, or expression variation, the only way to overcome the limitations of facial feature detectors is by leveraging the geometrical relationship they share with each other:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_05_013.jpg"/></div>
<p>A particularly simple, but surprisingly effective, way to incorporate facial geometry into the tracking procedure is by projecting the output of the feature detections onto the linear shape model's subspace. This amounts to minimizing the distance between the original points and their closest plausible shape that lies on the subspace. Thus, when the spatial noise in the feature detections is close to being Gaussian distributed, the projection yields the most likely solution. In practice, the distribution of detection errors on occasion does not follow a Gaussian distribution and additional mechanisms need to be introduced to account for this.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Face tracker implementation</h1>
            </header>

            <article>
                
<p>An implementation of the face-tracking algorithm can be found in the <kbd>face_tracker</kbd> class (see <kbd>face_tracker.cpp</kbd> and <kbd>face_tracker.hpp</kbd>). The following code is a snippet of its header that highlights its primary functionality:</p>
<pre>
    class face_tracker{ 
      public: 
      bool tracking;          //are we in tracking mode? 
      fps_timer timer;        //frames/second timer 
      vector&lt;Point2f&gt; points; //current tracked points 
      face_detector detector; //detector for initialisation 
      shape_model smodel;     //shape model 
      patch_models pmodel;    //feature detectors 

      face_tracker(){tracking = false;} 

      int                             //0 = failure 
      track(const Mat &amp;im,            //image containing face 
      const face_tracker_params &amp;p =  //fitting parameters 
      face_tracker_params());     //default tracking parameters 

      void 
      reset(){                            
        //reset tracker 
        tracking = false; timer.reset(); 
      } 
      ... 
      protected: 
      ... 
      vector&lt;Point2f&gt;   //points for fitted face in image 
      fit(const Mat &amp;image,//image containing face 
      const vector&lt;Point2f&gt;&amp;init,   //initial point estimates 
      const Size ssize = Size(21,21),//search region size 
      const bool robust = false,     //use robust fitting? 
      const int itol = 10,    //maximum number of iterations 
      const float ftol = 1e-3);      //convergence tolerance 
    };
</pre>
<p>The class has public member instances of the <kbd>shape_model</kbd>, <kbd>patch_models</kbd>, and <kbd>face_detector</kbd> classes. It uses the functionality of these three classes to effect tracking. The <kbd>timer</kbd> variable is an instance of the <kbd>fps_timer</kbd> class that keeps track of the frame rate at which the <kbd>face_tracker::track</kbd> function is called and is useful for analyzing the effects patch and shape model configurations on the computational complexity of the algorithm. The <kbd>tracking</kbd> member variable is a flag to indicate the current state of the tracking procedure. When this flag is set to <kbd>false</kbd>, as it is in the constructor and the <kbd>face_tracker::reset</kbd> function, the tracker enters a detection mode whereby the <kbd>face_detector::detect</kbd> function is applied to the next incoming image to initialize the model. When in the tracking mode, the initial estimate used for inferring facial feature locations in the next incoming image is simply their location in the previous frame. The complete tracking algorithm is implemented simply as follows:</p>
<pre>
    int face_tracker:: 
    track(const Mat &amp;im,const face_tracker_params &amp;p) { 
      Mat gray; //convert image to grayscale 
      if(im.channels()==1)  gray=im; 
      else                  cvtColor(im,gray,CV_RGB2GRAY); 
      if(!tracking) //initialize 
      points = detector.detect(gray,p.scaleFactor, 
        p.minNeighbours,p.minSize); 
      if((int)points.size() != smodel.npts()) return 0; 
      for(int level = 0; level &lt; int(p.ssize.size()); level++) 
      points = this-&gt;fit(gray,points,p.ssize[level], 
        p.robust,p.itol,p.ftol); 
      tracking = true; timer.increment();  return 1; 
    }
</pre>
<p>Other than bookkeeping operations, such as setting the appropriate <kbd>tracking</kbd> state and incrementing the tracking time, the core of the tracking algorithm is the multi-level fitting procedure, which is highlighted in the preceding code snippet. The fitting algorithm, implemented in the <kbd>face_tracker::fit</kbd> function, is applied multiple times with the different search window sizes stored in <kbd>face_tracker_params::ssize</kbd>, where the output of the previous stage is used as input to the next. In its simplest setting, the <kbd>face_tracker_params::ssize</kbd> function performs the facial feature detection around the current estimate of the shape in the image:</p>
<pre>
    smodel.calc_params(init); 
    vector&lt;Point2f&gt; pts = smodel.calc_shape(); 
    vector&lt;Point2f&gt; peaks = pmodel.calc_peaks(image,pts,ssize);
</pre>
<p>It also projects the result onto the face shape's subspace:</p>
<pre>
    smodel.calc_params(peaks);         
    pts = smodel.calc_shape();
</pre>
<p>To account for gross outliers in the facial features' detected locations, a robust model's fitting procedure can be employed instead of a simple projection by setting the <kbd>robust</kbd> flag to <kbd>true</kbd>. However, in practice, when using a decaying search window size (that is, as set in <kbd>face_tracker_params::ssize</kbd>), this is often unnecessary as gross outliers typically remain far from its corresponding point in the projected shape, and will likely lie outside the search region of the next level of the fitting procedure. Thus, the rate at which the search region size is reduced acts as an incremental outlier rejection scheme.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Training and visualization</h1>
            </header>

            <article>
                
<p>Unlike the other classes detailed in this chapter, training a <kbd>face_tracker</kbd> object does not involve any learning process. It is implemented in <kbd>train_face_tracker.cpp</kbd> simply as:</p>
<pre>
    face_tracker tracker; 
    tracker.smodel = load_ft&lt;shape_model&gt;(argv[1]); 
    tracker.pmodel = load_ft&lt;patch_models&gt;(argv[2]); 
    tracker.detector = load_ft&lt;face_detector&gt;(argv[3]); 
    save_ft&lt;face_tracker&gt;(argv[4],tracker);
</pre>
<p>Here <kbd>arg[1]</kbd> to <kbd>argv[4]</kbd> contain the paths to the <kbd>shape_model</kbd>, <kbd>patch_model</kbd>, <kbd>face_detector</kbd>, and <kbd>face_tracker</kbd> objects, respectively. The visualization for the face tracker in <kbd>visualize_face_tracker.cpp</kbd> is equally simple. Obtaining its input image stream either from a camera or video file, through the <kbd>cv::VideoCapture</kbd> class, the program simply loops until the end of the stream or until the user presses the <span class="KeyPACKT">Q</span> key, tracking each frame as it comes in. The user also has the option of resetting the tracker by pressing the <span class="KeyPACKT">D</span> key at any time.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Generic versus person-specific models</h1>
            </header>

            <article>
                
<p>There are a number of variables in the training and tracking process that can be tweaked to optimize the performance for a given application. However, one of the primary determinants of tracking quality is the range of shape and appearance variability the tracker has to model. As a case in point, consider the generic versus person-specific case. A generic model is trained using annotated data from multiple identities, expressions, lighting conditions, and other sources of variability. In contrast, person-specific models are trained specifically for a single individual. Thus, the amount of variability it needs to account for is far smaller. As a result, person-specific tracking is often more accurate than its generic counter part by a large magnitude.</p>
<p>An illustration of this is shown in the following image. Here the generic model was trained using the MUCT dataset. The person-specific model was learned from data generated using the annotation tool described earlier in this chapter. The results clearly show a substantially better tracking offered by the person-specific model, capable of capturing complex expressions and head-pose changes, whereas the generic model appears to struggle even for some of the simpler expressions:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_05_014.jpg"/></div>
<p>It should be noted that the method for face tracking described in this chapter is a bare-bones approach that serves to highlight the various components utilized in most non-rigid face-tracking algorithms. The numerous approaches to remedy<br/>
some of the drawbacks of this method are beyond the scope of this book and<br/>
require specialized mathematical tools that are not yet supported by OpenCV's functionality. The relatively few commercial-grade face-tracking software<br/>
packages available are testament to the difficulty of this problem in the general<br/>
setting. Nonetheless, the simple approach described in this chapter can work remarkably well in constrained settings.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we have built a simple face tracker that can work reasonably in constrained settings using only modest mathematical tools and OpenCV's substantial functionality for basic image processing and linear algebraic operations. Improvements to this simple tracker can be achieved by employing more sophisticated techniques in each of the three components of the tracker: the shape model, the feature detectors, and the fitting algorithm. The modular design of the tracker described in this section should allow these three components to be modified without substantial disruptions to the functionality of the others.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">References</h1>
            </header>

            <article>
                
<ul>
<li><em>Procrustes Problems, Gower, John C. and Dijksterhuis, Garmt B, Oxford University Press, 2004</em>.</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>