- en: Feature-Based Object Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we understood the importance of and how to model deep
    layered feature extraction using **Convolutional Neural Networks** (**CNNs**).
    In this chapter, we will learn how to model a CNN to detect where the object in
    the image is and also classify the object in one of our pre-decided categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: We will begin with a general discussion on image recognition and what is object
    detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A working example of the popular techniques for face detection using OpenCV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection using two-stage models such as Faster-RCNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection using one-stage model such as SSD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The major part of this chapter will be discussing deep learning-based object
    detectors and explaining them using a code for the demo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin with object detection, we will first see an overview of image recognition
    as detection is one part of it. In the following figure, an overview of object
    recognition is described using an image from `Pascal VOC` dataset. The input is
    passes through a model which then produces information in four different styles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e87720aa-caa0-4632-b7b7-633f61ec8f82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The model in the previous image performs generic image recognition where we
    can predict the following  information:'
  prefs: []
  type: TYPE_NORMAL
- en: A class name for the object in the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object center pixel location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bounding box surrounding the object as output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In instance image where each pixel is classified into a class. The classes are
    for object as well as background
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we say object detection, we are usually referring to the first and third
    type of image recognition. Our goal is to estimate class names as well as bounding
    box surrounding target objects. Before we begin our discussion on object detection
    techniques, in the next section we shall see why detecting objects is a difficult
    computer vision task.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past, several approaches for object detection were proposed. However,
    these either perform well in a controlled environment or look for special objects
    in images like a human face. Even in the case of faces, the approaches suffer
    from issues like low light conditions, a highly occluded face or tiny face size
    compared to the image size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are several challenges that are faced by an object detector in real-world
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Occlusion**: Objects like dogs or cats can be hidden behind one another,
    as a result, the features that can be extracted from them are not strong enough
    to say that they are an object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Viewpoint changes**: In cases of different viewpoints of an object, the shape
    may change drastically and hence the features of the object will also change drastically.
    This causes a detector which is trained to see a given object from one viewpoint
    to fail on seeing it from other viewpoints. For example, in the case of person
    detection, if the detector is looking for a head, hands, and legs combination
    to find a person, will fail if we put the camera overhead to take vertical downward
    facing images. The only thing that the detector will see are heads and hence the
    results are drastically reduced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variation in sizes**: The same object can be far from a camera or near. As
    a result, the size of objects varies. The detector is therefore required to be
    size invariant as well as rotation invariant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-rigid objects**: If the shape of the object splits into parts or there
    is a fluid object, it becomes even more challenging to describe them using features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Motion-blur**: If we are detecting a moving body like a car, there might
    be cases where the camera captured image is blurred. This is another challenge
    for the object detectors, to provide a correct estimation, and making a detector
    robust is crucial when deployed in moving robots like self-driving cars or drones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset and libraries used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be using TensorFlow (v1.4.0) and OpenCV as our main
    library for detection. We show results on custom images. However, any colored
    image can be used as input for various models. Wherever required, there are links
    to pre-trained model files in the sections.
  prefs: []
  type: TYPE_NORMAL
- en: Methods for object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object detection is the problem of two steps. First, it should localize an object
    or multiple objects inside an image. Secondly, it gives out a predicted class
    for each of the localized objects. There have been several object detection methods
    that use a sliding window-based approach. One of the popular detection techniques
    is face detection approach, developed by Viola and Jones[1]. The paper exploited
    the fact that the human face has strong descriptive features such as regions near
    eyes which are darker than near the mouth. So there may be a significant difference
    between the rectangle area surrounding the eyes with respect to the rectangular
    area near the nose. Using this as one of the several pre-defined patterns of rectangle
    pairs, their method computed area difference between rectangles in each pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Detecting faces is a two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First is to create a classifier with parameters for specific object detection.
    In our case, it is face detection:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In second step, for each image, it face detection is done using previously
    loaded classifier parameters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In OpenCV we can code this to detect the face, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, we used a file `haarcascade_frontalface_default.xml` which contains classifier
    parameters available at [https://github.com/opencv/opencv/tree/master/data/haarcascades](https://github.com/opencv/opencv/tree/master/data/haarcascades).
    We have to download these cascade classifier files in order to run face detection.
    Also for detecting other objects like eyes, smiles, and so on, we require similar
    files for use with OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding face detector we saw became popular in several devices ranging
    from smartphones to digital cameras. However, recent advances in deep learning
    are creating better face detectors. We will see this in the next few sections
    on deep learning-based general object detectors.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning-based object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With recent advancements in CNNs and their performance in image classification,
    it was becoming intuitive to use the similar model style for object detection.
    This has been proven right, as in the last few years there are better object detectors
    proposed every year which increases overall accuracy on standard benchmarks. Some
    of the styles of detectors are already in use in smartphones, robot self-driving
    cars, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: A generic CNN outputs class probabilities, as in the case of image recognition.
    But in order to detect objects, these must be modified to output both the class
    probability as well as bounding box rectangle coordinates and shape. Early CNN-based
    object detection, computes possible windows from an input image and then computes
    features using a CNN model for each window. This output of the CNN feature extractor
    will then tell us if the chosen window is the target object or not. This is slow
    due to a large computation of each window through the CNN feature extractor. Intuitively,
    we would like to extract features from images and use those features for object
    detection. This not only enhances speed for detection but also filters unwanted
    noise in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'There have been several methods proposed to tackle such issues of speed and
    accuracy in object detection. These are in general divided into two major categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Two-stage detectors**: Here, the overall process is divided into two major
    steps, hence the name two-stage detectors. The most popular among these is **Faster
    R-CNN**. In the next section, we will see a detailed explanation of this method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-stage detectors**: While two-stage detectors increased accuracy for detection,
    they were still hard to train and they were slower for several real-time operations.
    One-stage detectors rectified these issues by making a network in single architecture
    which predicts faster. One of the popular models of this style is **Single Shot
    Multibox Detector** (**SSD**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will see both of these types of detectors with
    a demo that shows the quality of results from each.
  prefs: []
  type: TYPE_NORMAL
- en: Two-stage detectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As CNN show their performance in general image classification, researchers used
    the same CNNs to do better object detection. The initial approaches using deep
    learning for object detection can be described as two-stage detectors and one
    of the popular ones is Faster R-CNN by Shaoqing Ren, Kaiming He, Ross Girshick,
    and Jian Sun 2015 [https://arxiv.org/pdf/1506.01497.pdf](https://arxiv.org/pdf/1506.01497.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'The method is divided into two stages:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first stage, the features are extracted from an image and **Region of
    Interests** (**ROI**) are proposed. ROIs consists of a possible box where an object
    might be in the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second stage uses features and ROIs to compute final bounding boxes and
    class probabilities for each of the boxes. These together constitute the final
    output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'An overview of Faster-RCNN is as shown in the following figure. An input image
    is used to extract features and a region proposals. These extracted features and
    proposals are used together to compute predicted bounding boxes and class probabilities
    for each box:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f61d607d-20c7-4394-9ed3-c8ba39a8e4d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the previous figure, overall method is considered two-stage because
    during training the model will first learn to produce ROIs using a sub-model called
    **Region Proposal Network (RPN)**. It will then learn to produce correct class
    probabilities and bounding box locations using ROIs and features. An overview
    of RPN is as shown in the following figure . RPN layer uses feature layer as input
    creates a proposal for bounding boxes and corresponding probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcfcf94d-01e0-433a-ad98-d22603384731.png)'
  prefs: []
  type: TYPE_IMG
- en: The bounding box locations are usually normalized values for the top left coordinate
    of the box with width and height values, though this can change depending on the
    way the model is learnt. During prediction, the model outputs a set of class probabilities,
    class categories as well as the bounding box location in (x, y, w, h) format.
    This set is again passed through a threshold to filter out the bounding boxes
    with confidence scores less than the threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'The major advantage of using this style of the detector is that it gives better
    accuracy than one-stage detectors. These usually achieve state-of-the-art detection
    accuracy. However, they suffer from slower speeds during predictions. If for an
    application prediction, time plays a crucial role, then it is advised to either
    provide these networks with a high-performance system or use one-stage detectors.
    On the other hand, if the requirement is to get the best accuracy, it is highly
    recommended to use such a method for object detection. An example output of object
    detection is as shown in the following figure with the bounding box around detected
    objects. Each box has a label showing predicted class name and confidence for
    the box:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc24432c-3230-4ddb-a84e-27232070db7f.png)'
  prefs: []
  type: TYPE_IMG
- en: The detection in the previous screenshot uses Faster RCNN model and even for
    small objects, like a person on the right bottom, the model detects with a good
    confidence score. Overall detected objects are bus, car and person. The model
    doesn't detect other objects, such as trees, pole, traffic light, and so on because
    it has not been trained to detect those objects.
  prefs: []
  type: TYPE_NORMAL
- en: Demo – Faster R-CNN with ResNet-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It can be seen from the previous screenshot that even in the case of varying
    object sizes and also objects with small sizes, the two-stage model of Faster
    R-CNN predicts accurately. Now, we will show how to run a similar prediction using
    TensorFlow. Let''s begin by cloning a repository, as it will contain most of the
    required codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have cloned, we will set up the environment. We will first download
    a pre-trained model from TensorFlow `model-zoo`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For macOS X:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For Linux:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Keep the extracted folder by the name `faster_rcnn_resnet101_coco_2017_11_08` in `models/research/object_detection`*. *This
    completes the downloading of the pre-trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'These two steps have to be performed each time we launch a Terminal shell:'
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, we will compile `protobuf` files, as TensorFlow uses them to serialize
    structured data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, run in the research folder:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The environment and pre-trained models are set, now we will start with the
    prediction code. The following code stays and runs inside `models/research/object_detection` and
    the code style is like a Jupyter notebook. As we progress in this section, each
    of the further code blocks can be run inside a Jupyter notebook cell. If you are
    not familiar with Jupyter, you can still run complete Python scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin with loading libs that will be used here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to load a pre-trained model for prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'It can be used to load the model Faster R-CNN with the ResNet-101 feature extractor
    pre-trained on `MSCOCO` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s set up labels to display in our figure using `MSCOCO` labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Before final predictions, we will set up the utility function as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is utility function to display bounding boxes using `matplotib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this setup, we can do predictions on the input image. In the following
    snippet, we are doing predictions on the input image as well as displaying the
    results. We will launch a `Tensorflow` session and run the graph in `sess.run`
    to compute bounding boxes, scores for each box, the class prediction for boxes
    and number of detections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Using previous code, an example of prediction is as shown in the following
    screenshot. Each detected object is displayed with the bounding box. Each bounding
    box has a name of the predicted class as well as the confidence score for the
    object inside the box:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/172440dd-3f05-4611-b21c-5edfcd084ecb.png)'
  prefs: []
  type: TYPE_IMG
- en: One-stage detectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we saw that two-stage detectors suffer from the issue
    of slower prediction time and harder training by splitting the network into two.
    In recently proposed networks like **Single Shot Multibox Detectors (SSD)**[3],
    the prediction time is reduced by removing the intermediate stage and the training
    is always end-to-end. These networks have shown effectiveness by running on smartphones
    as well as low-end computation units:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2dc74647-a537-43e6-b0cc-d9463a8ccdd1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An abstract view of the network is shown in the preceding figure. The overall
    output of the network is same as two-stage, the class probability for the object
    and bounding box coordinates of the form **(x, y, w, h)**, where (x,y) is the
    top-left corner of the rectangle and (w, h) are the width and height of the box
    respectively. In order to use multiple resolutions, the model not only uses the
    final layer of feature extraction but also several intermediate feature layers.
    An abstract view is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7db77e68-b1ef-40c9-a317-72247f97d1f3.png)'
  prefs: []
  type: TYPE_IMG
- en: To further increase the speed for detection, the model also uses a technique
    called **non-maximal suppression**. This will suppress all the **Bounding Box**
    which do not have a maximum score in a given region and for a given category.
    As a result, the total output boxes from the **MultiBox Layer** are reduced significantly
    and thus we have only high scored detections per class in an image.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see TensorFlow-based SSD object detection. We will
    use some of the code from the previous section;  Reader does not need to install
    again if there is already an installation of the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Demo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following codes, we will load a pre-trained model and perform an object
    detection task on pre-defined 90 categories. Before we begin, check that there
    is a working TensorFlow (Version = 1.4.0) Python environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, our input is as shown in the image with people:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86cae04e-26c3-4f4d-a0df-4be87d55022c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will follow similar instructions as that of two-stage detectors and begin
    by cloning TensorFlow/models repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s download a pre-trained model from TensorFlow model-zoo. These are for
    one-stage detectors:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For macOS X:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'For Linux:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, keep the extracted folder by the name `ssd_inception_v2_coco_2017_11_17
    in models/research/object_detection`*.* We will set up the environment now. If
    this has already been done from the previous section, please skip this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will compile the `protobuf` files:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, run in the research folder:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s begin with loading libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code reads pre-trained model. In TensorFlow, these models are
    usually saved as `protobuf` in `.pb` format. Also, note that if there are other
    formats of pre-trained model files, then we may have to read accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'For using our input image, the following block reads an image from a given
    path to a file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The last utility function is for the output display of the bounding box around
    the predicted object with the class name and detection score for each box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be using an SSD model for object detection that uses Inception-v2 model
    for feature extraction. This model is pre-trained on the `MSCOCO` dataset. We
    saw earlier the code snippet to download the model and also to load. So let''s
    go ahead and read the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we start using the model to do predictions on the input image, we need
    our output to make sense. We will create a dictionary map of the class index to
    pre-defined class names. The following code will read a file `data/mscoco_label_map.pbtxt`
    which contains this index to class name mapping. The final index can be used to
    read our output as class names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We have set up everything necessary for prediction. In TensorFlow, the model
    is represented as a computational graph and is often referred to as graph in code
    snippets. This consists of various layers and operation on layers represented
    as a node the and connection between them is how the data will flow. For performing
    predictions, we need to know the input node name and output node names. There
    can be more than one nodes of a type. To start performing the computation, we
    will first create a session. A graph can only perform computation inside a session
    and we can create a session as we need it in the program. In the following code
    snippet, we create a session and get pre-defined input node and output nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code, the input node is `image_tensor:0` and four output nodes
    are `detection_boxes:0`, `detection_scores:0`, `detection_classes:0`, and `num_detections:0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run inference on a given image, the inference is as shown in the following
    figure. Each box color is according to the class, and the predicted class name,
    as well as the score for class prediction, is displayed in the top-left corner.
    Ideally, score one shows the model is 100% sure about the category of an object
    inside the box:'
  prefs: []
  type: TYPE_NORMAL
- en: This score is not for how correct the box is but only for the confidence for
    the category of the object inside.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a2601de-45f3-43b3-85a3-f6314b0eda24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we used only one image as input. We can use a list of images as input
    and correspondingly we will get a list of outputs for each image. To display the
    results, iterate simultaneously on images and outputs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To show the comparison with the two-stage detector, for the same input the
    following are the output prediction with the one-stage detector. We can easily
    notice that the one-stage detectors such as SSD is good for large objects but
    fail to recognize small objects such as people. Also, the prediction scores vary
    a lot between the two detectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbceb912-d626-4585-89b5-293a2796d0e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter gives an overview of object detection and several challenges in
    modeling a good detector. While there are many methods for detection using deep
    learning, common categories are one-stage and two-stage detectors. Each of the
    detectors has its own advantages, such as one-stage detectors are good for real-time
    applications while two-stage detectors are good for high accuracy output. The
    difference in accuracy between the models is shown using example figures. We can
    now understand the choice of object detector and run a pre-trained model using
    TensorFlow. The various output samples for each show the effectiveness of models
    in complex images.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn more about the image recognition problems
    of segmentation as well as tracking using deep learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Viola Paul and Michael J. Jones. *Robust real-time face detection*. International
    journal of computer vision 57, no. 2 (2004): 137-154.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. *Faster R-CNN: Towards
    real-time object detection with region proposal networks*. In Advances in neural
    information processing systems, pp. 91-99\. 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu Wei, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang
    Fu, and Alexander C. Berg. S*SD: Single Shot Multibox Detector*. In European conference
    on computer vision, pp. 21-37\. Springer, Cham, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al., *Microsoft COCO: Common Objects in Context*, [https://arxiv.org/pdf/1405.0312.pdf](https://arxiv.org/pdf/1405.0312.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
