- en: Chapter 3. Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've learned from previous chapters that regression problems *involve predicting
    a numerical output*. The simplest but most common type of regression is linear
    regression. In this chapter, we'll explore why linear regression is so commonly
    used, its limitations, and extensions, and then touch on *polynomial regression*,
    which you may consider when a linear relationship isn't a best fit for your circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In **linear regression**, the output variable is predicted by a linearly weighted
    combination of input features. Here is an example of a simple linear model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to linear regression](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding model essentially says that we are estimating one output, and
    this is a linear function of a single predictor variable (that is, a feature)
    denoted by the letter *x*. The terms involving the Greek letter *β* are the parameters
    of the model and are known as **regression coefficients**. Once we train the model
    and settle on values for these parameters, we can make a prediction on the output
    variable for any value of *x* by a simple substitution in our equation. Another
    example of a linear model, this time with three features and with values assigned
    to the regression coefficients, is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to linear regression](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this equation, just as with the previous one, we can observe that we have
    one more coefficient than the number of features. This additional coefficient,
    *β[0]*, is known as the **intercept** and is the expected value of the model when
    the value of all input features is zero. The other *β* coefficients can be interpreted
    as the expected change in the value of the output per unit increase of a feature.
    For example, in the preceding equation, if the value of the feature *x[1]* rises
    by one unit, the expected value of the output will rise by 1.91 units. Similarly,
    a unit increase in the feature *x[3]* results in a decrease in the output by 7.56
    units. In a simple one-dimensional regression problem, we can plot the output
    on the *y* axis of a graph and the input feature on the *x* axis. In this case,
    the model predicts a straight-line relationship between these two, where *β[0]*
    represents the point at which the straight line crosses or intercepts the *y*
    axis and *β[1]* represents the slope of the line. We often refer to the case of
    a single feature (hence, two regression coefficients) as **simple linear regression**
    and the case of two or more features as **multiple linear regression**.
  prefs: []
  type: TYPE_NORMAL
- en: Assumptions of linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we delve into the details of how to train a linear regression model
    and how it performs, we''ll look at model assumptions. Model assumptions essentially
    describe what the model believes about the output variable *y* that we are trying
    to predict. Specifically, linear regression models assume that the output variable
    is a weighted linear function of a set of feature variables. Additionally, the
    model assumes that for fixed values of the feature variables, the output is normally
    distributed with a constant variance. This is the same as saying that the model
    assumes that the true output variable *y* can be represented by an equation such
    as the following one, shown for two input features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assumptions of linear regression](img/00031.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *ε* represents an error term, which is normally distributed with a zero
    mean and a constant variance *σ²*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assumptions of linear regression](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We might come across the term **homoscedasticity** as a more formal way of
    describing the notion of constant variance. By homoscedasticity or constant variance,
    we are referring to the fact that the variance in the error component does not
    vary with the values or levels of the input features. In the following plot, we
    are visualizing a hypothetical example of a linear relationship with **heteroskedastic**
    errors, which are errors that do not have a constant variance. The data points
    lie close to the line at low values of the input feature, because the variance
    is low in this region of the plot, but lie farther away from the line at higher
    values of the input feature because of the higher variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assumptions of linear regression](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The *ε* term is an irreducible error component of the true function *y* and
    can be used to represent random errors, such as measurement errors in the feature
    values. When training a linear regression model, we always expect to observe some
    amount of error in our estimate of the output, even if we have all the right features,
    enough data, and the system being modeled really is linear.
  prefs: []
  type: TYPE_NORMAL
- en: Put differently, even with a true function that is linear, we still expect that
    once we find a line of best fit through our training examples, our line will not
    go through all, or even any, of our data points because of this inherent variance
    exhibited by the error component. The critical thing to remember, though, is that
    in this ideal scenario, because our error component has zero mean and constant
    variance, our training criterion will allow us to come close to the true values
    of the regression coefficients given a sufficiently large sample, as the errors
    will cancel out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important assumption relates to the independence of the error terms.
    This means that we do not expect the **residual** or error term associated with
    one particular observation to be somehow correlated with that of another observation.
    This assumption can be violated if observations are functions of each other, which
    is typically the result of an error in the measurement. If we were to take a portion
    of our training data, double all the values of the features and outputs, and add
    these new data points to our training data, we could create the illusion of having
    a larger dataset; however, there will be pairs of observations whose error terms
    will depend on each other as a result, and hence our model assumption would be
    violated. Incidentally, artificially growing our dataset in such a manner is never
    acceptable for any model. Similarly, correlated error terms may occur if observations
    are related in some way by an unmeasured variable. For example, if we are measuring
    the malfunction rate of parts from an assembly line, then parts from the same
    factory might have a correlation in the error: for example, due to different standards
    and protocols used in the assembly process. Therefore, if we don''t use the factory
    as a feature, we may see correlated errors in our sample among observations that
    correspond to parts from the same factory. The study of **experimental design**
    is concerned with identifying and reducing correlations in error terms, but this
    is beyond the scope of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, another important assumption concerns the notion that the features
    themselves are statistically independent of each other. It is worth clarifying
    here that in linear models, although the input features must be linearly weighted,
    they themselves may be the output of another function. To illustrate this, one
    might be surprised to see that the following is a linear model of three features,
    *sin(z[1]**)*, *ln(z[2]* *)*, and *exp(z*[3]*)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assumptions of linear regression](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that this is a linear model by making a few transformations on the
    input features and then making the replacements in our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assumptions of linear regression](img/00035.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we have an equation that is more recognizable as a linear regression model.
    If the previous example made us believe that nearly everything could be transformed
    into a linear model, then the following two examples will emphatically convince
    us that this is not in fact the case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assumptions of linear regression](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Both models are not linear models because of the first regression coefficient
    (*β[1]*). The first model is not a linear model because *β[1]* is acting as the
    exponent of the first input feature. In the second model, *β[1]* is inside a *sine*
    function. The important lesson to take away from these examples is that there
    are cases where we can apply transformations on our input features in order to
    fit our data to a linear model; however, we need to be careful that our regression
    coefficients are always the linear weights of the resulting new features.
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the code for the simple linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For our input feature, we randomly sample points from a uniform distribution.
    We used a uniform distribution to get a good spread of data points. Note that
    our final `df` data frame is meant to simulate a data frame that we would obtain
    in practice; as a result, we do not include the error terms, as these would be
    unavailable to us in a real-world setting.
  prefs: []
  type: TYPE_NORMAL
- en: When we train a linear model using some data such as the data in our data frame,
    we are essentially hoping to produce a linear model with the same coefficients
    as the ones from the underlying model of the data. Put differently, the original
    coefficients define a **population regression line**. In this case, the population
    regression line represents the true underlying model of the data. In general,
    we will find ourselves attempting to model a function that is not necessarily
    linear. In this case, we can still define the population regression line as the
    best possible linear regression line, but a linear regression model will obviously
    not perform equally well.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the regression coefficients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our simple linear regression model, the process of training the model amounts
    to an estimation of our two regression coefficients from our dataset. As we can
    see from our previously constructed data frame, our data is effectively a series
    of observations, each of which is a pair of values (*x[i]*, *y[i]*) where the
    first element of the pair is the input feature value and the second element of
    the pair is its output label. It turns out that for the case of simple linear
    regression, it is possible to write down two equations that can be used to compute
    our two regression coefficients. Instead of merely presenting these equations,
    we'll first take a brief moment to review some very basic statistical quantities
    that the reader has most likely encountered previously, as they will be featured
    very shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **mean** of a set of values is just the average of these values and is
    often described as a measure of location, giving a sense of where the values are
    centered on the scale in which they are measured. In statistical literature, the
    average value of a random variable is often known as the **expectation**, so we
    often find that the mean of a random variable *X* is denoted as *E(X)*. Another
    notation that is commonly used is bar notation, where we can represent the notion
    of taking the average of a variable by placing a bar over that variable. To illustrate
    this, the following two equations show the mean of the output variable *y* and
    input feature *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating the regression coefficients](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A second very common quantity, which should also be familiar, is the **variance**
    of a variable. The variance measures the average square distance that individual
    values have from the mean. In this way, it is a measure of dispersion, so that
    a low variance implies that most of the values are bunched up close to the mean,
    whereas a higher variance results in values that are spread out. Note that the
    definition of variance involves the definition of the mean, and for this reason
    we''ll see the use of the *x* variable with a bar on it in the following equation,
    which shows the variance of our input feature *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating the regression coefficients](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we''ll define the **covariance** between two random variables, *x*
    and *y*, using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating the regression coefficients](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From the previous equation, it should be clear that the variance, which we just
    defined previously, is actually a special case of the covariance where the two
    variables are the same. The covariance measures how strongly two variables are
    correlated with each other and can be positive or negative. A positive covariance
    implies a positive correlation; that is, when one variable increases, the other
    will increase as well. A negative covariance suggests the opposite; when one variable
    increases, the other will tend to decrease. When two variables are statistically
    independent of each other and hence uncorrelated, their covariance will be zero
    (although it should be noted that a zero covariance does not necessarily imply
    statistical independence).
  prefs: []
  type: TYPE_NORMAL
- en: 'Armed with these basic concepts, we can now present equations for the estimates
    of the two regression coefficients for the case of simple linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating the regression coefficients](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The first regression coefficient can be computed as the ratio of the covariance
    between the output and the input feature, and the variance of the input feature.
    Note that if the output feature were to be independent of the input feature, the
    covariance would be zero and therefore, our linear model would consist of a horizontal
    line with no slope. In practice, it should be noted that even when two variables
    are statistically independent, we will still typically see a small degree of covariance
    due to the random nature of the errors; thus, if we were to train a linear regression
    model to describe their relationship, our first regression coefficient would be
    nonzero in general. Later, we'll see how significance tests can be used to detect
    features we should not include in our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement linear regression in R, it is not necessary to perform these calculations
    as R provides us with the `lm()` function, which builds a linear regression model
    for us. The following code sample uses the `df` data frame we created previously
    and calculates the regression coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first line, we see that the usage of the `lm()` function involves first
    specifying a formula and then following up with the `data` parameter, which in
    our case is our data frame. In the case of simple linear regression, the syntax
    of the formula that we specify for the `lm()` function is the name of the output
    variable, followed by a tilde (*~*) and then by the name of the single input feature.
    We''ll see how to specify more complex formulas when we look at multiple linear
    regression further along in this chapter. Finally, the output shows us the values
    for the two regression coefficients. Note that the *β[0]* coefficient is labeled
    as the intercept, and the *β[1]* coefficient is labeled by the name of the corresponding
    feature (in this case, `x[1]`) in the equation of the linear model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows the population line and the estimated line on the
    same plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating the regression coefficients](img/00042.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the two lines are so close to each other that they are barely
    distinguishable, showing that the model has estimated the true population line
    very closely. From [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*,
    we know that we can formalize how closely our model matches our dataset, as well
    as how closely it would match an analogous test set using the mean square error.
    We'll examine this as well as several other metrics of model performance and quality
    in this chapter, but first we'll generalize our regression model to deal with
    more than one input feature.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Whenever we have more than one input feature and want to build a linear regression
    model, we are in the realm of multiple linear regression. The general equation
    for a multiple linear regression model with *k* input features is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple linear regression](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Our assumptions about the model and about the error component *ε* remain the
    same as with simple linear regression, remembering that, as we now have more than
    one input feature, we assume that these are independent of each other. Instead
    of using simulated data to demonstrate multiple linear regression, we will analyze
    two real-world datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting CPU performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our first real-world dataset was presented by the researchers *Dennis F. Kibler*,
    *David W. Aha*, and *Marc K. Albert* in a 1989 paper entitled *Instance-based
    prediction of real-valued attributes* and published in the *Journal of Computational
    Intelligence*. The data contains the characteristics of different CPU models,
    such as the cycle time and the amount of cache memory. When deciding between processors,
    we would like to take all of these things into account, but ideally, we''d like
    to compare processors on a single numerical scale. For this reason, we often develop
    programs to benchmark the relative performance of a CPU. Our dataset also comes
    with the published relative performance of our CPUs and our objective will be
    to use the available CPU characteristics as features to predict this. The dataset
    can be obtained online from the UCI Machine Learning Repository via this link:
    [http://archive.ics.uci.edu/ml/datasets/Computer+Hardware](http://archive.ics.uci.edu/ml/datasets/Computer+Hardware).'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The UCI Machine Learning Repository is a wonderful online resource that hosts
    a large number of datasets, many of which are often cited by authors of books
    and tutorials. It is well worth the effort to familiarize yourself with this website
    and its datasets. A very good way to learn predictive analytics is to practice
    using the techniques you learn in this book on different datasets, and the UCI
    repository provides many of these for exactly this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `machine.data` file contains all our data in a comma-separated format,
    with one line per CPU model. We''ll import this in R and label all the columns.
    Note that there are 10 columns in total, but we don''t need the first two for
    our analysis, as these are just the brand and model name of the CPU. Similarly,
    the final column is a predicted estimate of the relative performance that was
    produced by the researchers themselves; our actual output variable, PRP, is in
    column *9*. We''ll store the data that we need in a data frame called `machine`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset also comes with definitions of the data columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Column name | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `MYCT` | The machine cycle time in nanoseconds |'
  prefs: []
  type: TYPE_TB
- en: '| `MMIN` | The minimum main memory in kilobytes |'
  prefs: []
  type: TYPE_TB
- en: '| `MMAX` | The maximum main memory in kilobytes |'
  prefs: []
  type: TYPE_TB
- en: '| `CACH` | The cache memory in kilobytes |'
  prefs: []
  type: TYPE_TB
- en: '| `CHMIN` | The minimum channels in units |'
  prefs: []
  type: TYPE_TB
- en: '| `CHMAX` | The maximum channels in units |'
  prefs: []
  type: TYPE_TB
- en: '| `PRP` | The published relative performance (our output variable) |'
  prefs: []
  type: TYPE_TB
- en: 'The dataset contains no missing values, so no observations need to be removed
    or modified. One thing that we''ll notice is that we only have roughly 200 data
    points, which is generally considered a very small sample. Nonetheless, we will
    proceed with splitting our data into a training set and a test set, with an 85-15
    split, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our data set up-and-running, we''ll usually want to investigate
    further and check whether some of our assumptions for linear regression are valid.
    For example, we would like to know whether we have any highly correlated features.
    To do this, we can construct a correlation matrix with the `cor()`function and
    use the `findCorrelation()` function from the `caret` package to get suggestions
    for which features to remove:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Using the default cutoff of `0.9` for a high degree of correlation, we found
    that none of our features should be removed. When we reduce this cutoff to `0.75`,
    we see that `caret` recommends that we remove the third feature (MMAX). As the
    final line of the preceding code shows, the degree of correlation between this
    feature and MMIN is `0.768`. While the value is not very high, it is still high
    enough to cause us a certain degree of concern that this will affect our model.
    Intuitively, of course, if we look at the definitions of our input features, we
    will certainly tend to expect that a model with a relatively high value for the
    minimum main memory will also be likely to have a relatively high value for the
    maximum main memory. Linear regression can sometimes still give us a good model
    with correlated variables, but we would expect to get better results if our variables
    were uncorrelated. For now, we've decided to keep all our features for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the price of used cars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our second dataset is in the cars data frame included in the `caret` package
    and was collected by *Shonda Kuiper* in 2008 from the *Kelly Blue Book* website,
    [www.kbb.com](http://www.kbb.com). This is an online resource to obtain reliable
    prices for used cars. The dataset comprises 804 GM cars, all with the model year
    2005\. It includes a number of car attributes, such as the mileage and engine
    size as well as the suggested selling price. Many features are binary indicator
    variables, such as the Buick feature, which represents whether a particular car''s
    make is Buick. The cars were all in excellent condition and less than one year
    old when priced, so the car condition is not included as a feature. Our objective
    for this dataset is to build a model that will predict the selling price of a
    car using the values of these attributes. The definitions of the features are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Column name | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Price` | The suggested retail price in USD (our output variable) |'
  prefs: []
  type: TYPE_TB
- en: '| `Mileage` | The number of miles the car has been driven |'
  prefs: []
  type: TYPE_TB
- en: '| `Cylinder` | The number of cylinders in the car''s engine |'
  prefs: []
  type: TYPE_TB
- en: '| `Doors` | The number of doors |'
  prefs: []
  type: TYPE_TB
- en: '| `Cruise` | The indicator variable representing whether the car has cruise
    control |'
  prefs: []
  type: TYPE_TB
- en: '| `Sound` | The indicator variable representing whether the car has upgraded
    speakers |'
  prefs: []
  type: TYPE_TB
- en: '| `Leather` | The indicator variable representing whether the car has leather
    seats |'
  prefs: []
  type: TYPE_TB
- en: '| `Buick` | The indicator variable representing whether the make of the car
    is Buick |'
  prefs: []
  type: TYPE_TB
- en: '| `Cadillac` | The indicator variable representing whether the make of the
    car is Cadillac |'
  prefs: []
  type: TYPE_TB
- en: '| `Chevy` | The indicator variable representing whether the make of the car
    is Chevy |'
  prefs: []
  type: TYPE_TB
- en: '| `Pontiac` | The indicator variable representing whether the make of the car
    is Pontiac |'
  prefs: []
  type: TYPE_TB
- en: '| `Saab` | The indicator variable representing whether the make of the car
    is Saab |'
  prefs: []
  type: TYPE_TB
- en: '| `Saturn` | The indicator variable representing whether the make of the car
    is Saturn |'
  prefs: []
  type: TYPE_TB
- en: '| `convertible` | The indicator variable representing whether the type of the
    car is a convertible |'
  prefs: []
  type: TYPE_TB
- en: '| `coupe` | The indicator variable representing whether the type of the car
    is a coupe |'
  prefs: []
  type: TYPE_TB
- en: '| `hatchback` | The indicator variable representing whether the type of the
    car is a hatchback |'
  prefs: []
  type: TYPE_TB
- en: '| `sedan` | The indicator variable representing whether the type of the car
    is a sedan |'
  prefs: []
  type: TYPE_TB
- en: '| `wagon` | The indicator variable representing whether the type of the car
    is a wagon |'
  prefs: []
  type: TYPE_TB
- en: 'As with the machine dataset, we should investigate the correlation between
    the input features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Just as with the machine dataset, we have a correlation that shows up when we
    set `cutoff` to `0.75` in the `findCorrelation()` function of `caret`. By directly
    examining the correlation matrix, we found that there is a relatively high degree
    of correlation between the `Doors` feature and the `coupe` feature. By cross-tabulating
    these two, we can see why this is the case. If we know that the type of a car
    is a coupe, then the number of doors is always two. If the car is not a coupe,
    then it most likely has four doors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another problematic aspect of the cars data is that some features are exact
    linear combinations of other features. This is discovered using the `findLinearCombos()`
    function in the caret package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are advised to drop the `coupe` and `wagon` columns, which are the
    15^(th) and 18^(th) features, respectively, because they are exact linear combinations
    of other features. We will remove both of these from our data frame, thus also
    eliminating the correlation problem we saw previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll split our data into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our data ready, we'll build some models.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing linear regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll once again use the `lm()` function to fit linear regression models to
    our data. For both of our datasets, we''ll want to use all the input features
    that remain in our respective data frames. R provides us with a shorthand to write
    formulas that include all the columns of a data frame as features, excluding the
    one chosen as the output. This is done using a single period, as the following
    code snippets show:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Training a linear regression model may be a one-line affair once we have all
    our data prepared, but the important work comes straight after, when we study
    our model in order to determine how well we did. Fortunately, we can instantly
    obtain some important information about our model using the `summary()` function.
    The output of this function for our CPU dataset is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Following a repeat of the call we made to the `lm()` function itself, the information
    provided by the `summary()` function is organized into three distinct sections.
    The first section is a summary of the model residuals, which are the errors that
    our model makes on the observations in the data on which it was trained. The second
    section is a table containing the predicted values of the model coefficients as
    well as the results of their significance tests. The final few lines display overall
    performance metrics for our model. If we repeat the same process on our cars dataset,
    we will notice the following line in our model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This occurs because we still have a feature whose effect on the output is indiscernible
    from other features due to underlying dependencies. This phenomenon is known as
    **aliasing**. The `alias()` command shows the features we need to remove from
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the problematic feature is the `Saturn` feature, so we will
    remove this feature and retrain the model. To exclude a feature from a linear
    regression model, we include it in the formula after the period and prefix it
    with a minus sign:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Residual analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A residual is simply the error our model makes for a particular observation.
    Put differently, it is the difference between the actual value of the output and
    our prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Residual analysis](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Analyzing residuals is very important when building a good regression model,
    as residuals reveal various aspects of our model, from violated assumptions and
    the quality of the fit to other problems, such as outliers. To understand the
    metrics in the residual summary, imagine ordering residuals from the smallest
    to the largest. Besides the minimum and maximum values that occur at the extremes
    of this sequence, the summary shows the first and third quartiles, which are the
    values a quarter along the way in this sequence and three quarters, respectively.
    The **median** is the value in the middle of the sequence. The **interquartile
    range** is the portion of the sequence between the first and third quartiles,
    and by definition contains half of the data. Looking first at the residual summary
    from our CPU model, it is interesting to note that the first and third quartiles
    are quite small in value compared to the minimum and maximum value. This is a
    first indication that there might be a few points that have a large residual error.
    In an ideal scenario, our residuals will have a median of zero and will have small
    values for the quartiles. We can reproduce the residuals summary from the summary
    function by noting that the model produced by the `lm()` function has a `residuals`
    attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note that, in the preceding example for our cars model, we need to compare the
    value of the residuals against the average value of the output variable, in order
    to get a sense of whether the residuals are large or not. Thus, the previous results
    show that the average selling price of a car in our training data is around $21
    k, and 50% of our predictions are roughly within ± $1.6 k of the correct value,
    which seems fairly reasonable. Obviously, the residuals for our CPU model are
    all much smaller in the absolute value because the values of the output variable
    for that model, namely the published relative performance, are much smaller than
    the values for `Price` in the cars model.
  prefs: []
  type: TYPE_NORMAL
- en: In linear regression, we assume that the irreducible errors in our model are
    randomly distributed with a normal distribution. A diagnostic plot, known as the
    **Quantile-Quantile plot** (**Q-Q plot**), is useful in helping us visually gauge
    the extent to which this assumption holds. The key idea behind this plot is that
    we can compare two distributions by comparing the values at their **quantiles**.
    The quantiles of a distribution are essentially evenly spaced intervals of a random
    variable, such that each interval has the same probability; for example, quartiles
    are four-quantiles because they split up a distribution into four equally probable
    parts. If the two distributions are the same, then the graph should be a plot
    of the line *y = x*. To check whether our residuals are normally distributed,
    we can compare their distribution against a normal distribution and see how close
    to the *y = x* line we land.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many other ways to check whether the model residuals are normally
    distributed. A good place to look is the `nortest` R package, which implements
    a number of well-known tests for normality, including the Anderson-Darling test
    and the Lilliefors test. In addition, the `stats` package contains the `shapiro.test()`
    function for performing the Shapiro-Wilk normality test.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code generates Q-Q plots for our two datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram displays the Q-Q plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Residual analysis](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The residuals from both models seem to lie reasonably close to the theoretical
    quantiles of a normal distribution, although the fit isn't perfect, as is typical
    with most real-world data. A second very useful diagnostic plot for a linear regression
    is the so-called **residual plot**. This is a plot of residuals against corresponding
    fitted values for the observations in the training data. In other words, this
    is a plot of the pairs (*i*, *e[i]*). There are two important properties of the
    residual plot that interest us in particular. Firstly, we would like to confirm
    our assumption of constant variance by checking whether the residuals are not
    larger on average for a range of fitted values and smaller in a different range.
    Secondly, we should verify that there isn't some sort of pattern in the residuals.
    If a pattern is observed, however, it may be an indication that the underlying
    model is nonlinear in terms of the features involved or that there are additional
    features missing from our model that we have not included. In fact, one way of
    discovering new features that might be useful for our model is to look for new
    features that are correlated with our model's residuals.
  prefs: []
  type: TYPE_NORMAL
- en: '![Residual analysis](img/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Both plots show a slight pattern of decreasing residuals in the left part of
    the graph. Slightly more worrying is the fact that the variance of the residuals
    seems to be a little higher for higher values of both output variables, which
    could indicate that the errors are not homoscedastic. This is more pronounced
    in the second plot for the cars datasets. In the preceding two residual plots,
    we have also labeled some of the larger residuals (in absolute magnitude). We'll
    see shortly that these are potential candidates for outliers. Another way to obtain
    a residual plot is to use the `plot()` function on the model produced by the `lm()`
    function itself. This generates four diagnostic plots, including the residual
    plot and the Q-Q plot.
  prefs: []
  type: TYPE_NORMAL
- en: Significance tests for linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After scrutinizing the residual summaries, the next thing we should focus on
    is the table of coefficients that our models have produced. Here, every estimated
    coefficient is accompanied by an additional set of numbers, as well as a number
    of stars or a dot at the end. At first, this may seem confusing because of the
    barrage of numbers, but there is a good reason why all this information is included.
    When we collect measurements on some data and specify a set of features to build
    a linear regression model, it is often the case that one or more of these features
    are not actually related to the output we are trying to predict. Of course, this
    is something we are generally not aware of beforehand when we are collecting data.
    Ideally, we would want our model to not only find the best values for the coefficients
    that correspond to the features that our output does actually depend on, but also
    to tell us which of the features we don't need.
  prefs: []
  type: TYPE_NORMAL
- en: One possible approach for determining whether a particular feature is needed
    in our model is to train two models instead of one. The second model will have
    all the features of the first model, excluding the specific feature whose significance
    we are trying to ascertain. We can then test whether the two models are different
    by looking at their distributions of residuals. This is actually what R does for
    all of the features that we have specified in each model. For each coefficient,
    a **confidence interval** is constructed for the null hypothesis that its corresponding
    feature is unrelated to the output variable. Specifically, for each coefficient,
    we consider a linear model with all the other features included, except the feature
    that corresponds to this coefficient. Then, we test whether adding this particular
    feature to the model significantly changes the distribution of residual errors,
    which would be evidence of a linear relationship between this feature and the
    output and that its coefficient should be nonzero. R's `lm()` function automatically
    runs these tests for us.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In statistics, a confidence interval combines a point estimate with the precision
    of that estimate. This is done by specifying an interval in which the true value
    of the parameter that is being estimated is expected to lie under a certain degree
    of confidence. A 95% globally confidence interval for a parameter essentially
    tells us that, if we were to collect 100 samples of data from the same experiment
    and construct a 95 percent confidence interval for the estimated parameter in
    each sample, the real value of the target parameter would lie within its corresponding
    confidence interval for 95 of these data samples. Confidence intervals that are
    constructed for point estimates with high variance, such as when the estimate
    is being made with very few data points, will tend to define a wider interval
    for the same degree of confidence than estimates made with low variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a snapshot of the summary output for the CPU model, which shows
    the coefficient for the intercept and the MYCT feature in the CPU model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Focusing on the MYCT feature for the moment, the first number in its row is
    the estimate of its coefficient, and this number is roughly 0.05 (*5.210×10^(-2)*).
    The **standard error** is the standard deviation of this estimate, and this is
    given next as 0.01885\. We can gauge our confidence as to whether the value of
    our coefficient is really zero (indicating no linear relationship for this feature)
    by counting the number of standard errors between zero and our coefficient estimate.
    To do this, we can divide our coefficient estimate by our standard error, and
    this is precisely the definition of the **t-value**, the third value in our row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'So, our MYCT coefficient is almost three standard errors away from zero, which
    is a fairly good indicator that this coefficient is not likely to be zero. The
    higher the t-value, the more likely we should be including our feature in our
    linear model with a nonzero coefficient. We can turn this absolute value into
    a probability that tells us how likely it is that our coefficient should really
    be zero. This probability is obtained from Student''s t-distribution and is known
    as the **p-value**. For the MYCT feature, this probability is 0.006335, which
    is small. We can obtain this value for ourselves using the `pt()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `pt()` function is the distribution function for the t-distribution, which
    is symmetric. To understand why our p-value is computed this way, note that we
    are interested in the probability of the absolute value of the t-value being larger
    than the value we computed. To obtain this, we first obtain the probability of
    the upper or right tail of the t-distribution and multiply this by 2 in order
    to include the lower tail as well. Working with basic distribution functions is
    a very important skill in R, and we have included examples in our online tutorial
    chapter on R if this example seems overly difficult. The t-distribution is parameterized
    by the degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The number of degrees of freedom is essentially the number of variables that
    we can freely change when calculating a particular statistic, such as a coefficient
    estimate. In our linear regression context, this amounts to the number of observations
    in our training data minus the number of parameters in the model (the number of
    regression coefficients). For our CPU model, this number is *179 – 7 = 172*. For
    the cars model where we have more data points, this number is 664\. The name comes
    from its relation to the number of independent dimensions or pieces of information
    that are applied as input to a system, and hence reflects the extent to which
    the system can be freely configured without violating any constraints on the input.
  prefs: []
  type: TYPE_NORMAL
- en: As a general rule of thumb, we would like our p-values to be less than 0.05,
    which is the same as saying that we would like to have 95 percent confidence intervals
    for our coefficient estimates that do not include zero. The number of stars next
    to each coefficient provides us with a quick visual aid for what the confidence
    level is, and a single star corresponds to our 95 percent rule of thumb while
    two stars represent a 99 percent confidence interval. Consequently, every coefficient
    in our model summary that does not have any stars corresponds to a feature that,
    we are not confident we should include in our model using our rule of thumb. In
    the CPU model, the CHMIN feature is the only feature that is suspect, with the
    other p-values being very small. The situation is different with the cars model.
    Here, we have four features that are suspect as well as the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to properly understand the interpretation of p-values in the
    context of our linear regression model. Firstly, we cannot and should not compare
    p-values against each other in order to gauge which feature is the most important.
    Secondly, a high p-value does not necessarily indicate that there is no linear
    relationship between a feature and the output; it only suggests that in the presence
    of all the other model features, this feature does not provide any new information
    about the output variable. Finally, we should always remember that the 95 percent
    rule of thumb is not infallible and is only really useful when the number of features
    and hence coefficients is not very large. Under 95 percent confidence, if we have
    1,000 features in our model, we can expect to get the wrong result for 50 coefficients
    on average. Consequently, linear regression coefficient significance tests aren't
    as useful for problems in high dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The final test of significance actually appears at the very bottom of the summary
    of the `lm()` output and is on the last line. This line provides us with the **F
    statistic**, which gets its name from the F test, which checks whether there is
    a statistical significance between the variances of two (ideally normal) distributions.
    The F statistic in this case tries to assess whether the variance of the residuals
    from a model in which all coefficients are zero is significantly different from
    the variance of the residuals from our trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Put differently, the F test will tell us whether the trained model explains
    some of the variance in the output, and hence we know that at least one of the
    coefficients must be nonzero. While not as useful when we have many coefficients,
    this tests the significance of coefficients together and doesn''t suffer from
    the same problem as the t-test on the individual coefficients. The summary shows
    a tiny p-value for this, so we know that at least one of our coefficients is nonzero.
    We can reproduce the F test that was run using the `anova()` function, which stands
    for **analysis of variance**. This test compares the **null model**, which is
    the model built with just an intercept and none of the features, with our trained
    model. We''ll show this here for the CPU dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note that the formula of the null model is `PRP ~ 1`, where the 1 represents
    the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: Performance metrics for linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The final details in our summary are concerned with the performance of the
    model as a whole and the degree to which the linear model fits the data. To understand
    how we assess a linear regression fit, we should first point out that the training
    criterion of the linear regression model is to minimize the MSE on the data. In
    other words, fitting a linear model to a set of data points amounts to finding
    a line whose slope and position minimize the sum (or average) of the squared distances
    from these points. As we refer to the error between a data point and its predicted
    value on the line as the residual, we can define the **Residual Sum of Squares**
    (**RSS**) as the sum of all the squared residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance metrics for linear regression](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, RSS is just the **Sum of Squared Errors** (**SSE**), so we
    can relate to the MSE with which we are familiar via this simple equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance metrics for linear regression](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Beyond certain historic reasons, RSS is an important metric to be aware of because
    it is related to another important metric, known as the RSE, which we will talk
    about next. For this, we'll need to first build up an intuition about what happens
    when we train linear regression models. If we run our simple linear regression
    experiment with artificial data a number of times, each time changing the random
    seed so that we get a different random sample, we'll see that we will get a number
    of regression lines that are likely to be very close to the true population line,
    just as our single run showed us. This illustrates the fact that linear models
    are characterized by low variance in general. Of course, the unknown function
    we are trying to approximate may very well be nonlinear and as a result, even
    the population regression line is not likely to be a good fit to the data for
    nonlinear functions. This is because the linearity assumption is very strict,
    and consequently, linear regression is a method with high bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a metric known as the **Residual Standard Error** (**RSE**), which
    estimates the standard deviation of our model compared to the target function.
    That is to say, it measures roughly how far away from the population regression
    line on average our model will be. This is measured in the units of the output
    variable and is an absolute value. Consequently, it needs to be compared against
    the values of *y* in order to gauge whether it is high or not for a particular
    sample. The general RSE for a model with *k* input features is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance metrics for linear regression](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For simple linear regression, this is just with *k = 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance metrics for linear regression](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can compute the RSE for our two models using the preceding formula, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To interpret the RSE values for our two models, we need to compare them with
    the mean of our output variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note that, in the car model, the RSE of 61.3 is quite small compared to the
    RSE of the cars model, which is roughly 2,947\. When we look at these numbers
    in terms of how close they are to the means of their respective output variables,
    however, we learn that actually it is the cars model RSE that shows a better fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, although the RSE is useful as an absolute value in that one can compare
    it to the mean of the output variable, we often want a relative value that we
    can use to compare across different training scenarios. To this end, when evaluating
    the fit of linear regression models, we often also look at the **R2 statistic**.
    In the summary, this is denoted as multiple R-squared. Before we provide the equation,
    we''ll first present the notion of the **Total Sum of Squares** (**TSS**). The
    total sum of squares is proportional to the total variance in the output variable,
    and is designed to measure the amount of variability intrinsic to this variable
    before we perform our regression. The formula for TSS is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance metrics for linear regression](img/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The idea behind the R² statistic is that if a linear regression model is a
    close fit to the true population model, it should be able to completely capture
    all the variance in the output. In fact, we often refer to the R² statistic as
    the relative amount that shows us what proportion of the output variance is explained
    by the regression. When we apply our regression model to obtain an estimate of
    the output variable, we see that the errors in our observations are called residuals
    and the RSS is essentially proportional to the variance that is left between our
    prediction and the true values of the output function. Consequently, we can define
    the R² statistic, which is the amount of variance in our output *y* that our linear
    regression model explains, as the difference between our starting variance (TSS)
    and our ending variance (RSS) relative to our starting variance (TSS). As a formula,
    this is just:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance metrics for linear regression](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'From this equation, we can see that R² ranges between 0 and 1\. A value close
    to 1 is indicative of a good fit as it means that most of the variance in the
    output variable has been explained by the regression model. A low value, on the
    other hand, indicates that there is still significant variance in the errors in
    the model, indicating that our model is not a good fit. Let''s see how the R2
    statistic can be computed manually for our two models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We used the `fitted.values` attribute of the model trained by `lm()`, which
    is the predictions the model makes on the training data. Both values are quite
    high, with the cars model again indicating a slightly better fit. We've now seen
    two important metrics to assess a linear regression model, namely RSE and the
    R² statistic. At this point, we might consider whether there is a more general
    measure of the linear relationship between two variables that we could also apply
    to our case. From statistics, we might recall that the notion of correlation describes
    exactly that.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **correlation** between two random variables, *X* and *Y*, is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance metrics for linear regression](img/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It turns out that in the case of simple regression, the square of the correlation
    between the output variable and the input feature is the same as the R2 statistic,
    a result that further bolsters the importance of the latter as a useful metric.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing different regression models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we want to compare two different regression models that have been trained
    on the same set of input features, the R² statistic can be very useful. Often,
    however, we want to compare two models that don't have the same number of input
    features. For example, during the process of feature selection, we may want to
    know whether including a particular feature in our model is a good idea. One of
    the limitations of the R² statistic is that it tends to be higher for models with
    more input parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **adjusted R²** attempts to correct the fact that R² always tends to be
    higher for models with more input features and hence is susceptible to overfitting.
    The adjusted R² is generally lower than R² itself, as we can verify by checking
    the values in our model summaries. The formula for the adjusted R² is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing different regression models](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The definitions of *n* and *k* are the same as those for the R² statistic.
    Now, let''s implement this function in R and compute the adjusted R² for our two
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several other commonly used performance metrics designed to compare
    models with a different number of features. The **Akaike Information Criterion**
    (**AIC**) uses an information theoretic approach to assess the relative quality
    of a model by balancing model complexity and accuracy. For linear regression models
    trained by minimizing the squared error, this is proportional to another well-known
    statistic, **Mallow's Cp**, so these can be used interchangeably. A third metric
    is the **Bayesian Information Criterion** (**BIC**). This tends to penalize models
    with more variables more heavily, compared to the previous metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Test set performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we''ve looked at the performance of our models in terms of the training
    data. This is important in order to gauge whether a linear model can fit the data
    well, but doesn''t give us a good sense of predictive accuracy over unseen data.
    For this, we turn to our test datasets. To use our model to make predictions,
    we can use the `predict()` function. This is a general function in R that many
    packages extend. With models trained with `lm()`, we simply need to provide the
    model and a data frame with the observations that we want to predict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll define our own function for computing the MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: For each model, we've used our `compute_mse()` function to return the training
    and test MSE. It happens that in this case both test MSE values are smaller than
    the train MSE values. Whether the test MSE is slightly larger or smaller than
    the train MSE is not particularly important. The important issue is that the test
    MSE is not significantly larger than the train MSE as this would indicate that
    our model is overfitting the data. Note that, especially for the CPU model, the
    number of observations in the original data set is very small and this has resulted
    in a test set size that is also very small. Consequently, we should be conservative
    with our confidence in the accuracy of these estimates for the predictive performance
    of our models on unseen data, because predictions made using a small test set
    size will have a higher variance.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've already seen some examples where trying to build a linear
    regression model might run into problems. One big class of problems that we've
    talked about is related to our model assumptions of linearity, feature independence,
    and the homoscedasticity and normality of errors. In particular we saw methods
    of diagnosing these problems either via plots, such as the residual plot, or by
    using functions that identify dependent components. In this section, we'll investigate
    a few more issues that can arise with linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Multicollinearity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As part of our preprocessing steps, we were diligent in removing features that
    were linearly related to each other. In doing this, we were looking for an exact
    linear relationship and this is an example of **perfect collinearity**. **Collinearity**
    is the property that describes when two features are approximately in a linear
    relationship. This creates a problem for linear regression as we are trying to
    assign separate coefficients to variables that are almost linear functions of
    each other. This can result in a situation where the coefficients of two highly
    collinear features have high p-values that indicate they are not related to the
    output variable, but if we remove one of these and retrain a model, the one left
    in has a low p-value. Another classic indication of collinearity is an unusual
    sign on one of the coefficients; for example, a negative coefficient on educational
    background for a linear model that predicts income. Collinearity between two features
    can be detected through pairwise correlation. One way to deal with collinearity
    is to combine two features into a new one (for example, by averaging); another
    is by simply discarding one of the features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multicollinearity** occurs when the linear relationship involves more than
    two features. A standard method for detecting this is to calculate the **variance
    inflation factor** (**VIF**) for every input feature in a linear model. In a nutshell,
    the VIF tries to estimate the increase in variance that is observed in the estimate
    of a particular coefficient that is a direct result of that feature being collinear
    with other features. This is typically done by fitting a linear regression model
    in which we treat one of the features as the output feature and the remaining
    features as regular input features. We then compute the R2 statistic for this
    linear model and from this, the VIF for our chosen feature using the formula *1
    / (1 – R²)*. In R, the `car` package contains the `vif()` function, which conveniently
    calculates the VIF value for every feature in a linear regression model. A rule
    of thumb here is that a VIF score of 4 or more for a feature is suspect, and a
    score in excess of 10 indicates a strong likelihood of multicollinearity. Since
    we saw that our cars data had linearly dependent features that we had to remove,
    let''s investigate whether we have multicollinearity in those that remain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We see three values here that are slightly above `4` but no values above that.
    As an example, the following code shows how the VIF value for `sedan` was calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we looked at the residuals of our two models, we saw that there were certain
    observations that had a significantly higher residual than others. For example,
    referring to the residual plot for the CPU model, we can see that the observation
    200 has a very high residual. This is an example of an **outlier**, an observation
    whose predicted value is very far from its actual value. Due to the squaring of
    residuals, outliers tend to have a significant impact on the RSS, giving us a
    sense that we don't have a good model fit. Outliers can occur due to measurement
    errors and detecting them may be important, as they may denote data that is inaccurate
    or invalid.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, outliers may simply be the result of not having the right
    features or building the wrong kind of model.
  prefs: []
  type: TYPE_NORMAL
- en: As we generally won't know whether an outlier is an error or a genuine observation
    during data collection, handling outliers can be very tricky. Sometimes, especially
    when we have very few outliers, a common recourse is to remove them, because including
    them frequently has the effect of changing the predicted model coefficients significantly.
    We say that outliers are often points with high **influence**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Outliers are not the only observations that can have high influence. **High
    leverage points** are observations that have an extreme value for at least one
    of their features and thus, lie far away from most other observations. **Cook's
    distance** is a typical metric that combines the notions of outlier and high leverage
    to identify points that have high influence on the data. For a more thorough exploration
    of linear regression diagnostics, a wonderful reference is *An R Companion to
    Applied Regression*, *John Fox*, *Sage Publications*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the effect of removing an outlier, we will create a new CPU model
    by using our training data without observation number 200\. Then, we will see
    whether our model has an improved fit on the training data. Here, we''ve shown
    the steps taken and a truncated model summary with only the final three lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from the reduced RSE and improved R2, we have a better fit on
    our training data. Of course, the real measure of model accuracy is the performance
    on the test data, and there are no guarantees that our decision to label observation
    200 as a spurious outlier was the right one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We have a lower test MSE than before, which is usually a good sign that we made
    the right choice. Again, because we have a small test set, we cannot be certain
    of this fact despite the positive indication from the MSE.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our CPU model only came with six features. Often, we encounter real-world datasets
    that have a very large number of features arising from a diverse array of measurements.
    Alternatively, we may have to come up with a large number of features when we
    aren't really sure what features will be important in influencing our output variable.
    Moreover, we might have categorical variables with many possible levels from which
    we are forced to create a large number of new indicator variables, as we saw in
    [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7 "Chapter 1. Gearing
    Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*. When our scenario
    involves a large number of features, we often find that our output only depends
    on a subset of these. Given *k* input features, there are *2^k* distinct subsets
    that we can form, so for even a moderate number of features the space of subsets
    is too large for us to fully explore by fitting a model on each subset.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One easy way to understand why there are *2^k* possible feature subsets is
    this: we can assign a unique identifying code to every subset as a string of binary
    digits of length *k*, where the digit at a certain position *i* is 1 if we chose
    to include the *i^(th)* feature (features can be ordered arbitrarily) in the subset.
    For example, if we have three features, the string 101 corresponds to the subset
    that only includes the first and third features. In this way, we have formed all
    possible binary strings from a string of *k* zeros to a string of *k* ones; thus
    we have all the numbers from 0 to *2^(k-1)* and *2^k* total subsets.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature selection** refers to the process by which a subset of features in
    a model is chosen in order to form a new model with fewer features. This removes
    features that we deem unrelated to the output variable and consequently results
    in a simpler model, which is easier to train as well as interpret. There are a
    number of methods designed to do this, and they generally do not involve exhaustively
    searching the space of possible subsets, but perform a guided search through this
    space instead.'
  prefs: []
  type: TYPE_NORMAL
- en: One such method is **forward selection**, which is an example of **stepwise
    regression** that performs feature selection in a series of steps. With forward
    selection, the idea is to start out with an empty model that has no features selected.
    We then perform *k* simple linear regressions (one for every feature that we have)
    and pick the best one. Here, we are comparing models that have the same number
    of features so that we can use the R² statistic to guide our choice, although
    we can use metrics such as AIC as well. Once we have chosen our first feature
    to add, we then pick another feature to add from the remaining *k-1* features.
    Therefore, we now run *k-1* multiple regressions for every possible pair of features,
    where one of the features in the pair is the feature that we picked in the first
    step. We continue adding in features like this until we have evaluated the model
    with all the features included and stop. Note that, in every step, we make a hard
    choice about which feature to include for all future steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, models that have more than one feature in them and do not include
    the feature we chose in the first step of this process are never considered. Therefore,
    we do not exhaustively search our space. In fact, if we take into account that
    we also assess the null model, we can compute the total number of models we perform
    a linear regression on as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature selection](img/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The order of magnitude of this computation is on the scale of *k²*, which for
    even small values of *k* is already considerably less than *2^k*. At the end of
    the forward selection process, we have to choose between *k+1* models, corresponding
    to the subsets we obtained at the end of every step of the process. As the final
    part of the process involves comparing models with different numbers of features,
    we usually use a criterion such as the AIC or the adjusted R2 to make our final
    choice of model. We can demonstrate this process for our CPU dataset by running
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `step()` function implements the process of forward selection. We first
    provide it with the null model obtained by fitting a linear model with no features
    on our training data. For the `scope` parameter, we specify that we want our algorithm
    to step through from the null model all the way to our full model consisting of
    all six features. The effect of issuing these commands in R is an output that
    demonstrates which feature subset is specified at every step of the iteration.
    To conserve space, we present the results in the following table, along with the
    value of the AIC for each model. Note that the lower the AIC value, the better
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: '| Step | Features in subset | AIC value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | `{}` | 1839.13 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `{MMAX}` | 1583.38 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `{MMAX, CACH}` | 1547.21 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `{MMAX, CACH, MMIN}` | 1522.06 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | `{MMAX, CACH, MMIN, CHMAX}` | 1484.14 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | `{MMAX, CACH, MMIN, CHMAX, MYCT}` | 1478.36 |'
  prefs: []
  type: TYPE_TB
- en: The `step()` function uses an alternative specification for forward selection,
    which is to terminate when there is no feature from those remaining that can be
    added to the current feature subset and would improve our score. For our dataset,
    only one feature was left out from the final model, as adding it did not improve
    the overall score. It is interesting and somewhat reassuring that this feature
    was CHMIN, which was the only variable whose relatively high p-value indicated
    that we weren't confident that our output variable is related to this feature
    in the presence of the other features.
  prefs: []
  type: TYPE_NORMAL
- en: 'One might wonder whether we could perform variable selection in the opposite
    direction by starting off with a full model and removing features one by one based
    on which feature, when removed, will make the biggest improvement in the model
    score. This is indeed possible, and the process is known either as **backward
    selection** or **backward elimination**. This can be done in R with the `step()`
    function by specifying `backward` as the direction and starting from the full
    model. We''ll show this on our cars dataset and save the result into a new cars
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The formula for the final linear regression model on the cars dataset is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the final model has thrown away the `Cruise`, `Sound`, and `Chevy`
    features. Looking at our previous model summary, we can see that these three features
    had high p-values. The previous two approaches are examples of a **greedy algorithm**.
    This is to say that, once a choice about whether to include a variable has been
    made, it becomes final and cannot be undone later. To remedy this, a third method
    of variable selection known as **mixed selection** or **bidirectional elimination**
    starts as forward selection with forward steps to add variables, but also includes
    backward steps when these can improve the AIC. Predictably, the `step()` function
    does this when the `direction` is specified as `both`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have two new models, we can see how they perform on the test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: For the CPU model, we perform marginally better on the test set than our original
    model. A suitable next step might be to investigate whether this reduced set of
    features works better in combination with the removal of our outlier; this is
    left as an exercise for the reader. In contrast, for the cars model, we see that
    the test MSE has increased slightly as a result of removing all these features.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Variable selection is an important process, as it tries to make models simpler
    to interpret, easier to train, and free of spurious associations by eliminating
    variables unrelated to the output. This is one possible approach to dealing with
    the problem of overfitting. In general, we don't expect a model to completely
    fit our training data; in fact, the problem of overfitting often means that it
    may be detrimental to our predictive model's accuracy on unseen data if we fit
    our training data too well. In this section on **regularization**, we'll study
    an alternative to reducing the number of variables in order to deal with overfitting.
    Regularization is essentially the process of introducing an intentional bias or
    constraint in our training procedure that prevents our coefficients from taking
    large values. As this is a process that tries to shrink the coefficients, the
    methods we'll look at are also known as **shrinkage methods**.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the number of parameters is very large, particularly compared to the number
    of available observations, linear regression tends to exhibit very high variance.
    This is to say that small changes in a few of the observations will cause the
    coefficients to change substantially. **Ridge regression** is a method that introduces
    bias through its constraint but is effective at reducing the model''s variance.
    Ridge regression tries to minimize the sum of the residual sum of squares and
    uses a term that involves the sum of the squares of the coefficients multiplied
    by a constant for which we''ll use the Greek letter *λ*. For a model with *k*
    parameters, not counting the constant term *β[0]*, and a dataset with *n* observations,
    ridge regression minimizes the following quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ridge regression](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We are still minimizing the RSS but the second term is the penalty term, which
    is high when any of the coefficients is high. Thus, when minimizing, we are effectively
    pushing the coefficients to smaller values. The *λ* parameter is known as a **meta
    parameter**, which we need to select or tune. A very large value of *λ* will mask
    the RSS term and just push the coefficients to zero. An overly small value of
    *λ* will not be as effective against overfitting and a *λ* parameter of 0 just
    performs regular linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: When performing ridge regression, we often want to scale by dividing the values
    of all our features by their variance. This was not the case with regular linear
    regression because, if one feature is scaled by a factor of 10, then the coefficient
    will simply be scaled by a factor of a tenth to compensate. With ridge regression,
    the scale of a feature affects the computation of all other features through the
    penalty term.
  prefs: []
  type: TYPE_NORMAL
- en: Least absolute shrinkage and selection operator (lasso)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **lasso** is an alternative regularization method to ridge regression. The
    difference appears only in the penalty term, which involves minimizing the sum
    of the absolute values of the coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: '![Least absolute shrinkage and selection operator (lasso)](img/00057.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It turns out that this difference in the penalty term is very significant, as
    the lasso combines both shrinkage and selection because it shrinks some coefficients
    to exactly zero, which is not the case with ridge regression. Despite this, there
    is no clear winner between these two. Models that depend on a subset of the input
    features will tend to perform better with lasso; models that have a large spread
    in coefficients across many different variables will tend to perform better with
    ridge regression. It is usually worth trying both.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The penalty in ridge regression is often referred to as an *l[2]* penalty,
    whereas the penalty term in lasso is known as an *l[1]* penalty. This arises from
    the mathematical notion of a **norm** of a vector. A norm of a vector is a function
    that assigns a positive number to that vector to represent its length or size.
    There are many different types of norms. Both the *l[1]* and *l[2]* norms are
    examples of a family of norms known as **p-norms** and have the following general
    form for a vector *v* with *n* components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Least absolute shrinkage and selection operator (lasso)](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Implementing regularization in R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a number of different functions and packages that implement ridge
    regression, such as `lm.ridge()` from the `MASS` package and `ridge()` from the
    `genridge` package. For the lasso there is also the `lars` package. In this chapter,
    we are going to work with the `glmnet()` function from the `glmnet` package due
    to its consistent and friendly interface. The key to working with regularization
    is to determine an appropriate value of *λ* to use. The approach that the `glmnet()`
    function uses is to use a grid of different *λ* values and train a regression
    model for each value. Then, one can either pick a value manually or use a technique
    to estimate the best lambda. We can specify the sequence of *λ* values to try
    via the `lambda` parameter; otherwise, a default sequence with 100 values will
    be used. The first parameter to the `glmnet()` function must be a matrix of features,
    which we can build using the `model.matrix()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second parameter is a vector with the output variable. Finally, the `alpha`
    parameter is a switch between ridge regression (0) and lasso (1). We''re now ready
    to train some models on the cars dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As we provided a sequence of 250 *λ* values, we''ve actually trained 250 ridge
    regression models and another 250 lasso models. We can see the value of *λ* from
    the `lambda` attribute of the object that is produced by `glmnet()` and apply
    the `coef()` function on this object to retrieve the corresponding coefficients
    for the 100th model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We can use the `plot()` function to obtain a plot showing how the values of
    the coefficients change as the logarithm of *λ* changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown below, it is very helpful to show the corresponding plot for ridge
    regression and lasso side by side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![Implementing regularization in R](img/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The key difference between these two graphs is that lasso forces many coefficients
    to fall to zero exactly, whereas in ridge regression they tend to drop off smoothly
    and only become zero altogether at extreme values of *λ*. This is further evident
    by reading the values of the numbers on the top horizontal axis of both graphs,
    which show the number of non-zero coefficients as *λ* varies. In this way, lasso
    has a significant advantage in that it can often be used to perform feature selection
    (because a feature with a zero coefficient is essentially not included in the
    model) as well as providing regularization to minimize the issue of overfitting.
    We can obtain other useful plots by changing the value supplied to the `xvar`
    parameter. The value `norm` plots the *l[1]* norm of the coefficients on the x-axis
    and `dev` plots the percentage deviance explained. We will learn about deviance
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deal with the issue of finding a good value for *λ*, the `glmnet()` package
    offers the `cv.glmnet()` function. This uses a technique known as cross-validation
    (we''ll study this in [Chapter 5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 5. Neural Networks"), *Support Vector Machines*) on the training data
    to find an appropriate *λ* that minimizes the MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'If we plot the result produced by the `cv.glmnet()` function, we can see how
    the MSE changes over the different values of lambda:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing regularization in R](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The bars shown above and below each dot are the error bars showing one standard
    deviation above and below the estimate of the MSE for each plotted value of lambda.
    The plots also show two vertical dotted lines. The first vertical line shown corresponds
    to the value of `lambda.min`, which is the optimal value proposed by cross-validation.
    The second vertical line to the right is the value in the attribute `lambda.1se`.
    This corresponds to a value that is one standard error away from `lambda.min`
    and produces a more regularized model.
  prefs: []
  type: TYPE_NORMAL
- en: With the `glmnet` package, the `predict()` function now operates in a variety
    of contexts. We can, for example, obtain the coefficients of a model for a lambda
    value that was not in our original list.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that it seems that lasso has not forced any coefficients to zero in this
    case, indicating that, based on the MSE, it is not suggesting removing any of
    them for the cars dataset. Finally, using the `predict()` function again, we can
    make predictions with a regularized model using the `newx` parameter to provide
    a matrix of features for observations on which we want to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The lasso model performs best and, unlike ridge regression, in this case also
    slightly outperforms the regular model on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Polynomial regression is a *kind* of linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: While linear regression is when both the predictor and the response are each
    continuous and linearly-related, causing the response to increase or decrease
    at a constant ratio to the predictor (that is, in a straight line), with polynomial
    regression, *different powers* of the predictor are successively added to see
    if they adjust the response significantly. As these increases are added to the
    equation, the line of data points will change its shape, turning the linear regression
    model from a best fitted line into a best fitted curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, why should you bother with polynomial regression? The generally accepted
    answer or thought process is: when a linear model doesn''t seem to be the best
    model for your data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main conditions that indicate a linear relationship may not
    be a good model for a use:'
  prefs: []
  type: TYPE_NORMAL
- en: There will be some variable relationships in your data that you *assume* are
    curvilinear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During visual inspection of your variables, you *establish* (using a scatter
    plot is the most common method for this) a curvilinear relationship
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After you have actually created a linear regression model, an examination of
    residuals (using a scatterplot) shows numerous positive residual values in the
    middle, but patches of negative residual values at either end (or vice versa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note: In curvilinear relationships, values increase together up to a certain
    level (like a positive relationship) and then, as one value increases, the other
    decreases (negative relationship) or vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: So, here we consider an example like the one just mentioned involving user cars.
    In our vehicle data, we have information listing many attributes, including the
    number of options each vehicle has. Suppose we are interested in the relationship
    between the number of options a car has (such as air conditioning or heated seats)
    and the resell price.
  prefs: []
  type: TYPE_NORMAL
- en: One might assume that the more options a vehicle has, the higher the price the
    vehicle would sell for.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, upon closer analysis of this data, we see that is not the case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Polynomial regression](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we plot the data, we can see perhaps a text book example of a scenario that
    would benefit from a polynomial regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Polynomial regression](img/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this hypothetical scenario, the relationship between the independent variable
    *x* (the percentage increase in the resell price over the blue book value) and
    the dependent variable *y* (the number of options a vehicle has) can be modeled
    as an *n^(th)* degree polynomial in *x*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we studied linear regression, a method that allows us to fit
    a linear model in a supervised learning setting where we have a number of input
    features and a single numeric output. Simple linear regression is the name given
    to the scenario where we have only one input feature, and multiple linear regression
    describes the case where we have multiple input features. Linear regression is
    very commonly used as a first approach to solving a regression problem. It assumes
    that the output is a linear weighted combination of the input features in the
    presence of an irreducible error component that is normally distributed and has
    zero mean and constant variance. The model also assumes that the features are
    independent. The performance of linear regression can be assessed by a number
    of different metrics from the more standard MSE to others, such as the R² statistic.
    We explored several model diagnostics and significance tests designed to detect
    problems from violated assumptions to outliers. Finally, we also discussed how
    to perform feature selection with stepwise regression and perform regularization
    using ridge regression and lasso.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is a model with several advantages, which include fast and
    cheap parameter computation and a model that, by virtue of its simple form, is
    very easy to interpret and draw inferences from. There is a plethora of tests
    available to diagnose problems with the model fit and perform hypothesis testing
    to check the significance of the coefficients. In general, as a method, it is
    considered to be low variance because it is robust to small errors in the data.
    On the negative side, because it makes very strict assumptions, notably that the
    output function must be linear in the model parameters, it introduces a high degree
    of bias, and for general functions that are complex or highly nonlinear this approach
    tends to fare poorly. In addition, we saw that we cannot really rely on significance
    testing for coefficients when we move to a high number of input features. This
    fact, coupled with the independence assumption between features, renders linear
    regression a relatively poor choice to make when working in a higher dimensional
    feature space.
  prefs: []
  type: TYPE_NORMAL
- en: We also mentioned polynomial regression as an option for fitting data once a
    linear regression falls short, based upon the relationship of your data point
    values or when residuals from a linear regression model show certain positive-negative
    relationships.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study logistic regression, which is an important
    method used in classification problems.
  prefs: []
  type: TYPE_NORMAL
